Duplicates?   'diva2:408826' and 'diva2:512408'
----------------------------------------------------------------------
Duplicates: 'diva2:1221420', 'diva2:1229928' - note that one has the full
text.
----------------------------------------------------------------------
Duplicates: 'diva2:1221420', 'diva2:1229928' - note that one has the full
text.
----------------------------------------------------------------------
Duplicates?   'diva2:408826' and 'diva2:512408' 
Not duplicates - but an error in document uploaded
----------------------------------------------------------------------
Duplicates - ['diva2:509959', 'diva2:515592' both have full text but one also
has a compenfium
----------------------------------------------------------------------
Are these duplicates: diva2:1033230 diva2:1033189 ?
----------------------------------------------------------------------
Duplicates? 'diva2:1033239', 'diva2:1045115'
----------------------------------------------------------------------
===============================================================================
Above were all reported on or before 2024-10-11
----------------------------------------------------------------------
I get a error when trying to get the fill text for diva2:1900086 - via
https://kth.diva-portal.org/smash/get/diva2:1900086/FULLTEXT01.pdf - the file
will not open and it appears to have been truncated - as the usual PDF file
ending is not present.
----------------------------------------------------------------------
The full text fo diva2:1873390 
https://kth.diva-portal.org/smash/get/diva2:1873390/FULLTEXT01.pdf
seems to be blank pages - the file is truncated and cannot be repaired.
----------------------------------------------------------------------
diva2:736211 has the text of the summary and not that of the abstract, as the DiVA abstract
----------------------------------------------------------------------
Are 'diva2:754257' and  'diva2:753742' duplicates?
----------------------------------------------------------------------
The Swedish abstract for diva2:1595164 is actually in English.
----------------------------------------------------------------------
Are diva2:1045047 and diva2:1033220 duplicates?
----------------------------------------------------------------------
Are diva2:839893 and diva2:737929 duplicates?
The abstract for the second is the same except for two spaces.
----------------------------------------------------------------------
In diva2:644350 - missing symbols, missing ligatures, and colons rather than decimal points::

<p>In this thesis a detailed discussion of the topic percolation theory in squared lattices in</p><p>two dimensions will be conducted. To support this discussion numerical calculations will</p><p>be done. For the data analysis and simulations the Hoshen-Kopelman-Algorithm [2] will</p><p>be used. All concepts deduced will nally lead to the determination of the conductance's</p><p>exponent</p><p>t in random resistor networks. Using Derrida's transfer matrix program to</p><p>calculate the conductivity of random resistors in two and three dimensions [11] and</p><p>the nite-size scaling approach were used. In two dimensions</p><p>t= = 0:955 0:006 was</p><p>obtained. Were</p><p>is the exponent of the correlation length in innite lattices. This</p><p>value is in excellent agreement with Derrida (</p><p>t= = 0:960:02, [11]) and slightly smaller</p><p>than Sahimi (</p><p>t= = 0:97480:001, [21]). In three dimensions the same approach yielded</p><p>t=</p><p>= 2:155 0:012 which some what smaller than the value found by Sahimi t= =</p><p>2</p><p>:27 0:20 [21] and Gingold and Lobb t= = 2:276 0:012 [25].</p>

abstract should be:

diva2:644350: <p>In this thesis a detailed discussion of the topic percolation theory in squared lattices in two dimensions will be conducted. To support this discussion numerical calculations will be done. For the data analysis and simulations the Hoshen-Kopelman-Algorithm [2] will be used. All concepts deduced will nally lead to the determination of the conductance's exponent t in random resistor networks. Using Derrida's transfer matrix program to calculate the conductivity of random resistors in two and three dimensions [11] and the finite-size scaling approach were used. In two dimensions <em>t/&nu;</em> = 0.955&pm;0.006 was obtained. Were is the exponent of the correlation length in infinite lattices. This value is in excellent agreement with Derrida ( <em>t/&nu;</em> = 0.96&pm;0.02, [11]) and slightly smaller than Sahimi ( <em>t/&nu;</em> = 0.9748&pm;0.001, [21]). In three dimensions the same approach yielded <em>t/&nu;</em> = 2.155&pm;0.012 which some what smaller than the value found by Sahimi <em>t/&nu;</em> = 2.:27&pm;0.20 [21] and Gingold and Lobb <em>t/&nu;</em> = 2.276&pm;0.012 [25].</p>
----------------------------------------------------------------------
The full text for diva2:1210790 has a different thesis: https://kth.diva-portal.org/smash/get/diva2:1210790/FULLTEXT02.pdf
----------------------------------------------------------------------
In diva2:571089

<p>Curve fitting is used in a variety of fields, especially in physics, mathematics and economics.</p><p>The method is often used to smooth noisy data and for doing path planning. In this bachelor</p><p>thesis calculus of variations will be used to derive a formula for finding an optimal curve to fit a</p><p>set of data points. We evaluate a cost function (defined on the set of all curves</p><p></p><p>f on the interval</p><p>[</p><p></p><p>a; b]) given by F(f) =</p><p>R</p><p></p><p>b</p><p>a</p><p></p><p>(f00(x))2dx +</p><p>P</p><p></p><p>n</p><p>i</p><p></p><p>=1(f(xi) 􀀀 yi)2. The integral term represents the</p><p>smoothness of the curve, the interpolation error is given by the summation term and</p><p></p><p>&gt; 0 is</p><p>defined as the interpolation parameter. An ideal curve minimizes the interpolation error and</p><p>is relatively smooth. This is problematic since a smooth function generally has a large interpolation</p><p>error when doing curve fitting, and therefore the interpolation parameter</p><p></p><p>is needed</p><p>to decide how much consideration should be given to each attribute. For the cost function</p><p></p><p>F</p><p>a larger value of</p><p></p><p>decreases the interpolation error of the curve. The analytical calculations</p><p>performed made it possible to construct a</p><p></p><p>Matlab program, that could be used to solve the</p><p>minimization problem. In the result part some examples are presented for different values of</p><p></p><p>.</p><p>The conclusion is that a larger value of the interpolation parameter</p><p></p><p>is generally needed when</p><p>using more data points and if the points are closely placed on the x-axis. Further on, a method</p><p>called Ordinary Cross Validation (OCV) is evaluated to find an optimal value of</p><p></p><p>. This method</p><p>gave good results, except for the case when the points could almost be fitted with a straight line.</p>

corrected abstract:

<p>Curve fitting is used in a variety of fields, especially in physics, mathematics and economics. The method is often used to smooth noisy data and for doing path planning. In this bachelor thesis calculus of variations will be used to derive a formula for finding an optimal curve to fit a set of data points. We evaluate a cost function (defined on the set of all curves <em>f</em> on the interval [a, b]) given by <em>F(f) = &int;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup style="position: relative; top: -0.5rem; left: 0.05rem;">b</sup> <sub style="position: relative; bottom: -0.3rem; left: -0.1rem;">a</sub></span></span>(f&Prime;(x))<sup>2</sup> dx + &lambda; &sum;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup style="position: relative; top: -0.5rem; left: 0.05rem;">n</sup><sub style="position: relative; bottom: -0.5rem; left: 0.05rem;">i=1</sub></span></span>(f(x<sub>i</sub>) - y<sub>i</sub>)<sup>2</sup></em>. The integral term represents the smoothness of the curve, the interpolation error is given by the summation term and &lambda; &gt; 0 is defined as the interpolation parameter. An ideal curve minimizes the interpolation error and is relatively smooth. This is problematic since a smooth function generally has a large interpolation error when doing curve fitting, and therefore the interpolation parameter &lambda; is needed to decide how much consideration should be given to each attribute. For the cost function <em>F</em> a larger value of &lambda; decreases the interpolation error of the curve. The analytical calculations performed made it possible to construct a  Matlab program, that could be used to solve the minimization problem. In the result part some examples are presented for different values of &lambda;. The conclusion is that a larger value of the interpolation parameter &lambda; is generally needed when using more data points and if the points are closely placed on the x-axis. Further on, a method called Ordinary Cross Validation (OCV) is evaluated to find an optimal value of &lambda;. This method gave good results, except for the case when the points could almost be fitted with a straight line.</p>

If MathJax were installed one could replace the long HTML with:
 $F(f) = \int_{a}^{b}(f^{\prime\prime}(x))^2 dx + \lambda \sum_{i=1}^{n} (f(x_i) -y_i)^2$
----------------------------------------------------------------------
In diva2:557257

<p>For a field k and a grading of the polynomial ringk[t] with Hilbert functionh, we consider the Quot functor Quoth V , where V = ? di =1k[t] is a finitely generated and free k[t]-module. The Quot functor parametrizes, for any k-algebra B, homogeneous B [t]-submodulesN⊆B⊗kV such that the graded components of the quotient( B⊗kV)/Nare locally freeB-modules of rank given byh. We find that it is locallyrepresentable by a polynomial ring over kin a finite number of variables. Finally, weshow that there is a scheme that represents the Quot functor that is both smooth and irreducible.</p>


Corrected abstract:
<p>For a field <em>k</em> and a grading of the polynomial ring <em>k[t]</em> with Hilbert function <em>h</em>, we consider the Quot functor Quoth <em>V<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>h</sup><sub>V</sub></span></span></em> , where <em>V = &otimes; b<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>d</sup><sub>i=1</sub></span></span> k[t]</em> is a finitely generated and free <em>k[t]</em>-module. The Quot functor parametrizes, for any <em>k</em>-algebra <em>B</em>, homogeneous <em>B[t]</em>-submodules <em>N ⊆ B⊗<sub>k</sub>V</em> such that the graded components of the quotient (<em>B⊗<sub>k</sub>V</em>)/N are locally free <em>B</em>-modules of rank given by <em>h</em>. We find that it is locally representable by a polynomial ring over <em>k</em> in a finite number of variables. Finally, we show that there is a scheme that represents the Quot functor that is both smooth and irreducible.</p>

----------------------------------------------------------------------
In diva2:1576394 custom font encoding have been used - you cannot select or search for text.
----------------------------------------------------------------------
In diva2:1816881, many words were merged in the abstract:

<p>Gas turbines can experience various changes that affect their performance.Compressor fouling is one of the leading causes that deteriorate the gas turbineperformance. This research aims to investigate the impact of compressorfouling on the performance of gas turbines and the rotodynamic behaviorof gas turbines. Fouling was simulated as a reduction of mass flow and areduction of compressor isentropic efficiency by using Turbomatch software.A rotor–bearing model was created to analyze the vibration behavior dueto compressor fouling by using MADYN 2000 software and that particledeposition leads to rotor imbalance. The results show that the main variationsfor performance are power output, pressure ratio and EGT. For the rotodynamicmodel, the result illustrates an increase in vibration level for the first andsecond bearings and a decrease for the third bearing. The results also predictedthat parameters mass flow, compressor discharge temperature or specific fuelconsumption show a similar trend compared to the increase in vibrations. Thisresult can be used in conjunction with GPA analysis to predict the foulingcondition and help in identifying the severity of the fouling condition.</p>

Corrected abstract:

diva2:1816881: <p>Gas turbines can experience various changes that affect their performance. Compressor fouling is one of the leading causes that deteriorate the gas turbine performance. This research aims to investigate the impact of compressor fouling on the performance of gas turbines and the rotodynamic behavior of gas turbines. Fouling was simulated as a reduction of mass flow and a reduction of compressor is entropic efficiency by using Turbomatch software. A rotor–bearing model was created to analyze the vibration behavior dueto compressor fouling by using MADYN 2000 software and that particle deposition leads to rotor imbalance. The results show that the main variations for performance are power output, pressure ratio and EGT. For the rotodynamic model, the result illustrates an increase in vibration level for the first and second bearings and a decrease for the third bearing. The results also predicted that parameters mass flow, compressor discharge temperature or specific fuel consumption show a similar trend compared to the increase in vibrations. This result can be used in conjunction with GPA analysis to predict the fouling condition and help in identifying the severity of the fouling condition.</p>
----------------------------------------------------------------------
In diva2:1216708 merged words and innecessary text:

<p>In this project, we aim to find a method for obtainingthe factors in a bull/bear market factor model for asset returnand variance, given an optimal portfolio. The proposed methodwas derived using the Karush-Kuhn-Tucker (KKT) conditionsfor optimal solutions to the convex Markowitz portfolio selectionproblem. For synthetic data where all necessary parameters wereknown exactly, the method could give bounds on the factors. Theexact values of the factors were obtained when short selling wasallowed, and in some instances when short selling was forbidden.The method was evaluated on real-world data with varyingresults, possibly due to estimation errors and invalid assumptionsabout the model of the investor.I. INTRODUC</p>


Corrected abstract:

<p>In this project, we aim to find a method for obtainingthe factors in a bull/bear market factor model for asset returnand variance, given an optimal portfolio. The proposed method was derived using the Karush-Kuhn-Tucker (KKT) conditions for optimal solutions to the convex Markowitz portfolio selection problem. For synthetic data where all necessary parameters were known exactly, the method could give bounds on the factors. The exact values of the factors were obtained when short selling was allowed, and in some instances when short selling was forbidden. The method was evaluated on real-world data with varying results, possibly due to estimation errors and invalid assumptions about the model of the investor.</p>

----------------------------------------------------------------------
In diva2:1900086 there is an error in the PDF, it will fail to load - see
https://kth.diva-portal.org/smash/get/diva2:1900086/FULLTEXT01.pdf

The file seems to have been cut off after 14,680,064 bytes.
----------------------------------------------------------------------
In diva2:1348434, many equations are incorrect and the words are not correct:

<p>Better Shelter RHU is a social enterprise developing and providing temporary Refugee Housing Units to aid regions of crisis. The shelters are deployed worldwide and they are subjected to harsh weather conditions particularly to heavy wind loads. To maximise the amount of units deployed, the shelters have to be cost-efficient and material lead times need to be short. In order to achieve this, an evaluation to use lesser strength materials in the load bearing structure specifically the main joint named Joint1 - is assessed in this thesis. To assess the feasibility to change the material of the joint to an alternative steel with lower tensile strength and elongation ratio, the current performance is first analysed and then compared to the performance with an alternative cheaper material available for the production method. Modeling of the wind loads were made with fluid analysis and the resulting pressures were transferred on to the load bearing frame. From the frame, displacements were derived which were subsequently transferred to a subassembly with Joint1 in focus. From the sub-assembly, stresses for a wind load of 28 m/s could be evaluated for the joint. For the current material, which has a yield strength denoted RC shown in regions of about 1.09 · RC has a yield strength denoted  and a tensile strength denoted RC p02 or 0.6 · RC m, incipient plasticity were p02 and a tensile strength denoted RA m. The alternative material, which m, plasticity was shown in similar regions but also areas where the stresses reached tensile strength (1.03 · RA m) at the same wind speed. Conclusively, the alternative material appears as more hazardous because of the lower tensile strength compared to the current material. These results are based on conservative assumptions where minimum values of material data are used and the simulated models are simplified.</p><p> </p>

Corrected abstract:
<p><em>Better Shelter RHU</em> is a social enterprise developing and providing temporary Refugee Housing Units to aid regions of crisis. The shelters are deployed worldwide and they are subjected to harsh weather conditions particularly to heavy wind loads. To maximise the amount of units deployed, the shelters have to be cost-efficient and material lead times need to be short. In order to achieve this, an evaluation to use lesser strength materials in the load bearing structure specifically the main joint named <em>Joint1</em> - is assessed in this thesis.</p>
<p>To assess the feasibility to change the material of the joint to an alternative steel with lower tensile strength and elongation ratio, the current performance is first analyzed and then compared to the performance with an alternative cheaper material available for the production method.</p>
<p>Modeling of the wind loads were made with fluid analysis and the resulting pressures were transferred on to the load bearing frame. From the frame, displacements were derived which were subsequently transferred to a subassembly with Joint1 in focus. From the sub-assembly, stresses for a wind load of 28 m/s could be evaluated for the joint. For the current material, which has a yield strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>p02</sub></span></span></em> and a tensile strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>m</sub></span></span></em>, incipient plasticty were shown in regions of about <em>1.09 &middot; R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>p02</sub></span></span></em> or <em>0.6 &middot; R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>m</sub></span></span></em>. The alternative material, which has a yield strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>A</sup><sub>p02</sub></span></span></em> and a tensile strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>A</sup><sub>m</sub></span></span></em>, plasticity was shown in similar regions but also areas where the stresses reached tensile strength (<em>1.03 &middot; R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>A</sup><sub>m</sub></span></span></em>) at the same wind speed. Conclusively, the alternative material appears as more hazardous because of the lower tensile strength compared to the current material. These results are based on conservative assumptions where minimum values of material data are used and the simulated models are simplified.</p>
----------------------------------------------------------------------
Possible duplicates:
diva2:1045047: <p>In this project we are implementing group formations in to the game engine Unity3D. Groups</p><p>of people move in a certain way when avoiding other groups and obstacles. With the use of the</p><p>steering package UnitySteer we have made some example scenes using di erent steering behaviors.</p><p>We have made an implementation of the RVO2 package in Unity. With this implementation we</p><p>can simulate various scenarios with agents avoiding each other. Human pedestrians tend to walk</p><p>in groups. We adapted this behavior in our program to make our simulations as real as possible.</p><p>Finally we evaluated the performance of our simulation by checking the frames per seconds when</p><p>increasing the number of agents.</p><p>Keywords: Agent, Steering, Obstacle Avoidance, RVO, Groups, Formations, Real-Time, Unity.</p>
diva2:1033220: <p>In this project we are implementing group formations in to the game engine Unity3D. Groups</p><p>of people move in a certain way when avoiding other groups and obstacles. With the use of the</p><p>steering package UnitySteer we have made some example scenes using di erent steering behaviors.</p><p>We have made an implementation of the RVO2 package in Unity. With this implementation we</p><p>can simulate various scenarios with agents avoiding each other. Human pedestrians tend to walk</p><p>in groups. We adapted this behavior in our program to make our simulations as real as possible.</p><p>Finally we evaluated the performance of our simulation by checking the frames per seconds when</p><p>increasing the number of agents.</p><p>Keywords: Agent, Steering, Obstacle Avoidance, RVO, Groups, Formations, Real-Time, Unity.</p>
----------------------------------------------------------------------
In diva2:516126 missing equations:

<p>Uncertainties in radiative effects of the quarks in -background in the form of final state radiation (FSR) are significant when it comes to reducing all forms of systematics that can arise from measuring the jets energy. Analysis on FSR is in general conducted on different simulated samples where one has included the radiative effect using algorithms such as PYTHIA[29]. The hypothesis is that through the re-weighting of the -background nominal sample one could add a better representation of the FSR effect. Finding a simple way to include a better description of FSR would not only save time in the simulation process but it would also be a way to reduce the systematic errors originating from limited MC statistics. Due to statistical effects coming from the simulations one cannot use the basic approach to define the effect of FSR as simply the difference between nominal and FSR. Two methods are tested to estimate the FSR effects; the first method uses a set of efficiency factors to represent the signal regions, the second method is to add a weight to the events of the nominal sample. The first method show positive results, especially in SR2, compared to a basic analysis, with an uncertainty of the FSR effect of: SR1:±29% SR2: ±51% SR3: ±37%. While a basic analysis gave an uncertainty of ±42%, ±122% and 36%. The second method shows positive signs where the re-weighted sample moves closer to the behaviour of the FSR sample. However, both methods are based on insufficient amount of statistics to draw any absolute conclusions.</p>

Corrected abstract:

diva2:516126: <p>Uncertainties in radiative effects of the quarks in <em>t<span style="text-decoration: overline;">t</span></em>-background in the form of final state radiation (FSR) are significant when it comes to reducing all forms of systematics that can arise from measuring the jets energy. Analysis on FSR is in general conducted on different simulated samples where one has included the radiative effect using algorithms such as PYTHIA[29]. The hypothesis is that through the re-weighting of the <em>t<span style="text-decoration: overline;">t</span></em>-background nominal sample one could add a better representation of the FSR effect. Finding a simple way to include a better description of FSR would not only save time in the simulation process but it would also be a way to reduce the systematic errors originating from limited MC statistics. Due to statistical effects coming from the simulations one cannot use the basic approach to define the effect of FSR as simply the difference between nominal and FSR. Two methods are tested to estimate the FSR effects; the first method uses a set of efficiency factors to represent the signal regions, the second method is to add a weight to the events of the nominal sample. The first method show positive results, especially in SR2, compared to a basic analysis, with an uncertainty of the FSR effect of: SR1:±29% SR2: ±51% SR3: ±37%. While a basic analysis gave an uncertainty of ±42%, ±122% and 36%. The second method shows positive signs where the re-weighted sample moves closer to the behaviour of the FSR sample. However, both methods are based on insufficient amount of statistics to draw any absolute conclusions.</p>
----------------------------------------------------------------------
In diva2:783990 abstract is:
<p>During construction of large sandwich structures one of or perhaps the greatest difficulty lies increating good joints. The joint is often the first component to fail and constitute a large part of theweight of the structure and production workload. The study is focused on investigating if noveljoining methods could affect how military vessels are able to handle an internal blast withoutcatastrophic damage to the sections surrounding the area of the explosion. If the conventionalmethod of joining sandwich panels was to be used the result of an internal blast would mostprobably be a complete failure of the surrounding joints. This is based on that the conventional Xjointslacks continuous fibers through the joint in at least one direction and are therefore weak whensubjected to tensile forces in that direction. The belief is that a joint with continuous fibers wouldhave a smaller risk of failure or at least the failure would be less severe than with the conventionaljoints and could therefore maintain its structural integrity until reparations can be made.The purpose of this study is to manufacture small scale samples of novel X-joint design concepts.These concepts are then compared by manufacturability, tensile strength in either direction of thejoint and the joints flexibility. How well the promising novel concepts handle an internal blast cannotbe seen in this study, since full scale samples and advanced blast trials would be required.Six novel methods for joining of composite sandwich panels and a reference conventional joint,referred to as the standard joint, were manufactured and evaluated through three tensile tests. Outof the six concepts, all except the 3D-woven joint was found to have advantages over the standardtype, which only surpassed the novel joints in the aspect of bulkhead strength, and the lath and thebundle joint concepts were found to be the most promising. The tensile strength tests gave that theultimate strength of lath joint was 86% of the reference value in its deck direction and at least 75%in the bulkhead direction when reinforced with laths (49% unreinforced), where the reference value(100%) was the ultimate strength of the standard joint bulkhead. The tensile test results for thebundle joint, which did not have a reinforced bulkhead, reached 77% and 66% (deck respectivelybulkhead) of the reference strength value. These results supports the theory that the circular holesof the bundle concept has less impact on the strength of the bulkhead than the rectangular holes ofthe lath concept. The fiber bundles, however, are difficult and time consuming to work with,especially since the fiber bundles needs to be flattened and spread out to increase the area ofattachment when adhered to the sandwich panels. In contrast, the lath pieces are easy tomanufacture and work with during the final assembly and therefore more suitable to use whenreinforcement of the bulkhead is required.The joint flexibility test indicated that the most flexible joint was, as might be expected, the taperedfinger joint. If the tapered panel could be designed to counteract or avoid delamination of thetapering area and if the panels of the joint could be prevented from bending to the point fracture ofthe single skin laminates, this concept could be a suitable solution for confining a blast in terms ofrapidly developing membrane forces. The lath and the bundle joints were once again the strongestand reacted very similar. Unfortunately no difference could be found between the two jointconcepts because the hinges used to attach the samples in the test broke before the samples werebroken.Since the fiber properties and amounts were not compared it is not definite that either of the lath orthe bundle concept is better than the other in terms of mechanical properties. The conclusion is thatboth these two concepts work, are relatively easy to produce and have a far greater potential thanthat of the standard joint, in the internal blast situation and as a joining method in general.</p>

corrected abstract:
<p>During construction of large sandwich structures one of or perhaps the greatest difficulty lies in creating good joints. The joint is often the first component to fail and constitute a large part of the weight of the structure and production workload. The study is focused on investigating if novel joining methods could affect how military vessels are able to handle an internal blast without catastrophic damage to the sections surrounding the area of the explosion. If the conventional method of joining sandwich panels was to be used the result of an internal blast would most probably be a complete failure of the surrounding joints. This is based on that the conventional X-joints lacks continuous fibers through the joint in at least one direction and are therefore weak when subjected to tensile forces in that direction. The belief is that a joint with continuous fibers would have a smaller risk of failure or at least the failure would be less severe than with the conventional joints and could therefore maintain its structural integrity until reparations can be made.</p><p>The purpose of this study is to manufacture small scale samples of novel X-joint design concepts. These concepts are then compared by manufacturability, tensile strength in either direction of the joint and the joints flexibility. How well the promising novel concepts handle an internal blast cannot be seen in this study, since full scale samples and advanced blast trials would be required.</p><p>Six novel methods for joining of composite sandwich panels and a reference conventional joint, referred to as the standard joint, were manufactured and evaluated through three tensile tests. Out of the six concepts, all except the 3D-woven joint was found to have advantages over the standard type, which only surpassed the novel joints in the aspect of bulkhead strength, and the lath and the bundle joint concepts were found to be the most promising. The tensile strength tests gave that the ultimate strength of lath joint was 86% of the reference value in its deck direction and at least 75% in the bulkhead direction when reinforced with laths (49% unreinforced), where the reference value (100%) was the ultimate strength of the standard joint bulkhead. The tensile test results for the bundle joint, which did not have a reinforced bulkhead, reached 77% and 66% (deck respectively bulkhead) of the reference strength value. These results supports the theory that the circular holes of the bundle concept has less impact on the strength of the bulkhead than the rectangular holes of the lath concept. The fiber bundles, however, are difficult and time consuming to work with, especially since the fiber bundles needs to be flattened and spread out to increase the area of attachment when adhered to the sandwich panels. In contrast, the lath pieces are easy to manufacture and work with during the final assembly and therefore more suitable to use when reinforcement of the bulkhead is required.</p><p>The joint flexibility test indicated that the most flexible joint was, as might be expected, the tapered finger joint. If the tapered panel could be designed to counteract or avoid delamination of the tapering area and if the panels of the joint could be prevented from bending to the point fracture of the single skin laminates, this concept could be a suitable solution for confining a blast in terms of rapidly developing membrane forces. The lath and the bundle joints were once again the strongest and reacted very similar. Unfortunately no difference could be found between the two joint concepts because the hinges used to attach the samples in the test broke before the samples were broken.</p><p>Since the fiber properties and amounts were not compared it is not definite that either of the lath or the bundle concept is better than the other in terms of mechanical properties. The conclusion is that both these two concepts work, are relatively easy to produce and have a far greater potential than that of the standard joint, in the internal blast situation and as a joining method in general.</p>
----------------------------------------------------------------------
In diva2:1110812 abstract is:
<p>Automotive development has always been need-based and the product of today is an evolutionover several decades and a diversied technology application to deliver better products to theend users. Steady increase in the deployment of on-board electronics and software is characterizedby the demand and stringent regulations. Today, almost every function on-board a modernvehicle is either monitored or controlled electronically.One such specic demand for AB Volvo arose out of construction trucks in the US market. Usersseldom have/had a view of the operational boundaries of the drivetrain components, resultingin inappropriate use causing damage, poor traction and steering performance. Also, AB Volvo'sstand-alone traction assistance functions were not suciently capable to handle the vehicle useconditions. Hence, the goal was set to automate and synchronize the traction assistance devicesand software functions to improve the traction and steerability under a variety of road conditions.The rst steps in this thesis involved understanding the drivetrain components from design andoperational boundary perspective. The function descriptions of the various traction softwarefunctions were reviewed and a development/integration plan drafted. A literature survey wascarried out seeking potential improvement in traction from dierential locking and also its eectson steerability. A benchmarking exercise was carried out to identify competitor and suppliertechnologies available for the traction device automation task.The focus was then shifted to developing and validating the traction controller in a simulationenvironment. Importance was given to modeling of drivetrain components and renement ofvehicle behavior to study and understand the eects of dierential locking and develop a differentiallock control strategy. The modeling also included creating dierent road segments toreplicate use environment and simulating vehicle performance in the same, to reduce test timeand costs. With well-correlated vehicle performance results, a dierential lock control strategywas developed and simulated to observe traction improvement. It was then implemented onan all-wheel drive construction truck using dSPACE Autobox to test, validate and rene thecontroller.Periodic test sessions carried out at Hallered proving ground, Sweden were important to re-ne the control strategy. Feedback from test drivers and inputs from cross-functional teamswere essential to develop a robust controller and the same was tested for vehicle suitability andrepeatability of results. When comparing with the existing traction software functions, the integrateddierential lock and transfer case lock controller showed signicantly better performanceunder most test conditions. Repeatable results proved the reliability of developed controller.The correlation between vehicle test scenarios and simulation environment results indicated theaccuracy of software models and control strategy, bi-directionally.Finally, the new traction assistance device controller function was demonstrated within ABVolvo to showcase the traction improvement and uncompromising steerability.</p>

corrected abstract:
<p>Automotive development has always been need-based and the product of today is an evolution over several decades and a diversified technology application to deliver better products to the end users. Steady increase in the deployment of on-board electronics and software is characterized by the demand and stringent regulations. Today, almost every function on-board a modern vehicle is either monitored or controlled electronically.</p><p>One such specific demand for AB Volvo arose out of construction trucks in the US market. Users seldom have/had a view of the operational boundaries of the drivetrain components, resulting in inappropriate use causing damage, poor traction and steering performance. Also, AB Volvo's stand-alone traction assistance functions were not sufficiently capable to handle the vehicle use conditions. Hence, the goal was set to automate and synchronize the traction assistance devices and software functions to improve the traction and steerability under a variety of road conditions.</p><p>The first steps in this thesis involved understanding the drivetrain components from design and operational boundary perspective. The function descriptions of the various traction software functions were reviewed and a development/integration plan drafted. A literature survey was carried out seeking potential improvement in traction from differential locking and also its effects on steerability. A benchmarking exercise was carried out to identify competitor and supplier technologies available for the traction device automation task.</p><p>The focus was then shifted to developing and validating the traction controller in a simulation environment. Importance was given to modeling of drivetrain components and refinement of vehicle behavior to study and understand the effects of differential locking and develop a differential lock control strategy. The modeling also included creating different road segments to replicate use environment and simulating vehicle performance in the same, to reduce test time and costs. With well-correlated vehicle performance results, a differential lock control strategy was developed and simulated to observe traction improvement. It was then implemented on an all-wheel drive construction truck using dSPACE Autobox to test, validate and refine the controller.</p><p>Periodic test sessions carried out at Hällered proving ground, Sweden were important to refine the control strategy. Feedback from test drivers and inputs from cross-functional teams were essential to develop a robust controller and the same was tested for vehicle suitability and repeatability of results. When comparing with the existing traction software functions, the integrated differential lock and transfer case lock controller showed significantly better performance under most test conditions. Repeatable results proved the reliability of developed controller. The correlation between vehicle test scenarios and simulation environment results indicated the accuracy of software models and control strategy, bi-directionally.</p><p>Finally, the new traction assistance device controller function was demonstrated within AB Volvo to showcase the traction improvement and uncompromising steerability.</p>
----------------------------------------------------------------------
In diva2:492776 abstract is:
<p>In recent years the interest for smaller, cheaper and more energy efficient vehicles hasincreased significantly. These vehicles are intended to be used in urban areas, where theactual need of large heavy cars is generally minor. The travelled distance is on average lessthan 56km during a day and most often there is only one person travelling in the vehicle. Manyof the established car manufacturers have recently started to take interest into this marketsegment, but the majority of these small vehicles are still manufactured by smaller companiesat a low cost and with little or no research done on vehicle traffic safety. This may be becausethere are still no legal requirements on crash testing of this type of vehicles.This report will examine road safety for Urban Light-weight Vehicle (ULV) to find criticalcrash scenarios from which future crash testing methods for urban vehicles can be derived.The term ULV is specific to this report and is the title for all engine powered three- and fourwheeledvehicles categorized by the European Commission. Other attributes than the wheelgeometry is engine power and the vehicles unladen mass. The maximum allowed weight for athree-wheeled ULV is 1 000kg and 400kg for a four-wheeled one.By studying current crash test methods used in Europe by Euro NCAP it has beenconcluded that these tests are a good way of assessing car safety. For light-weight urbanvehicles it has been concluded that some of these tests need to be changed and that some newtest scenarios should be added when assessing road safety. The main reasons for this is linkedto that vehicle’s with a weight difference of more than 150kg cannot be compared withcurrent test methods, and that crash tests are performed with crash objects with similar orequal mass in current safety assessment programs. This correlates poorly to the trafficsituation for light-weight urban vehicles since it would most likely collide with a far heaviervehicle than itself in an accident event.To verify the actual traffic situation in urban areas, accident statistics have beenexamined closely. The research has shown that there are large differences between rural andurban areas. For instance; 66% of all severe and fatal traffic accident occurs in rural areaseven though they are less populated. Even the distribution of accident categories has showndifferent in rural and urban areas. The United Nations Economic Commission for Europe(UNECE) has defined accident categories in their database which is widely used within theEuropean Union. By comparing each accident category’s occurrence, injury and fatality rate,the most critical urban accident categories were found in the following order.</p>
<p>1. Collision due to crossing or turning</p>
<p>2. Vehicle and pedestrian collision</p>
<p>3. Rear-end collision</p>
<p>4. Single-vehicle accident</p>
<p>5. Other collisions</p>
<p>6. Head-on collision</p>
<p>Statistics also show that of all fatally injured crash victims in urban trafficapproximately; one third is travelling by car; one third by motorcycle, moped or pedal-cycle;and one third are pedestrians. This means that unprotected road travelers correspond to twothirds of all fatal urban traffic accidents, a fact that has to be taken into account in future crashtesting of urban vehicles. With all the information gathered a total of four new crash testscenarios for light-weight urban vehicles have been presented:</p>
<p>• Vehicle-to-vehicle side impact at 40km/h with a 1 300kg striking vehicle to evaluate theoccupant protection level of the light-weight vehicle.</p>
<p>• Vehicle-to-motorcycle side impact at 40km/h with motorcycle rider protection evaluation.</p>
<p>• Pedestrian protection assessment at 40km/h over the whole vehicle front and roof area.</p>
<p>• Rigid barrier impact at 40km/h corresponding to an urban single vehicle accident with aroad side object or a collision with a heavier or similar sized vehicle.</p>

corrected abstract:
<p>In recent years the interest for smaller, cheaper and more energy efficient vehicles has increased significantly. These vehicles are intended to be used in urban areas, where the actual need of large heavy cars is generally minor. The travelled distance is on average less than <em>56km</em> during a day and most often there is only one person travelling in the vehicle. Many of the established car manufacturers have recently started to take interest into this market segment, but the majority of these small vehicles are still manufactured by smaller companies at a low cost and with little or no research done on vehicle traffic safety. This may be because there are still no legal requirements on crash testing of this type of vehicles.</p><p>This report will examine road safety for <em>Urban Light-weight Vehicle (ULV)</em> to find critical crash scenarios from which future crash testing methods for urban vehicles can be derived. The term <em>ULV</em> is specific to this report and is the title for all engine powered three- and four-wheeled vehicles categorized by the <em>European Commission</em>. Other attributes than the wheel geometry is engine power and the vehicles unladen mass. The maximum allowed weight for a three-wheeled <em>ULV</em> is <em>1 000kg</em> and <em>400kg</em> for a four-wheeled one.</p><p>By studying current crash test methods used in Europe by <em>Euro NCAP</em> it has been concluded that these tests are a good way of assessing car safety. For light-weight urban vehicles it has been concluded that some of these tests need to be changed and that some new test scenarios should be added when assessing road safety. The main reasons for this is linked to that vehicle’s with a weight difference of more than <em>150kg</em> cannot be compared with current test methods, and that crash tests are performed with crash objects with similar or equal mass in current safety assessment programs. This correlates poorly to the traffic situation for light-weight urban vehicles since it would most likely collide with a far heavier vehicle than itself in an accident event.</p><p>To verify the actual traffic situation in urban areas, accident statistics have been examined closely. The research has shown that there are large differences between rural and urban areas. For instance; 66% of all severe and fatal traffic accident occurs in rural areas even though they are less populated. Even the distribution of accident categories has shown different in rural and urban areas. The <em>United Nations Economic Commission for Europe (UNECE)</em> has defined accident categories in their database which is widely used within the <em>European Union</em>. By comparing each accident category’s occurrence, injury and fatality rate, the most critical urban accident categories were found in the following order.</p>
<em><ol>
<li>Collision due to crossing or turning</li>
<li>Vehicle and pedestrian collision</li>
<li>Rear-end collision</li>
<li>Single-vehicle accident</li>
<li>Other collisions</li>
<li>Head-on collision</li>
</ol></em>
<p>Statistics also show that of all fatally injured crash victims in urban traffic approximately; one third is travelling by <em>car</em>; one third by <em>motorcycle</em>, moped or pedal-cycle; and one third are <em>pedestrians</em>. This means that unprotected road travelers correspond to two thirds of all fatal urban traffic accidents, a fact that has to be taken into account in future crash testing of urban vehicles. With all the information gathered a total of four new crash test scenarios for light-weight urban vehicles have been presented:</p>
<em><ul>
<li>Vehicle-to-vehicle side impact at 40km/h with a 1 300kg striking vehicle to evaluate the occupant protection level of the light-weight vehicle.</li>
<li>Vehicle-to-motorcycle side impact at 40km/h with motorcycle rider protection evaluation.</li>
<li>Pedestrian protection assessment at 40km/h over the whole vehicle front and roof area.</li>
<li>Rigid barrier impact at 40km/h corresponding to an urban single vehicle accident with a road side object or a collision with a heavier or similar sized vehicle.</li>
</ul><em>
----------------------------------------------------------------------
In diva2:895396 abstract is:
<p>A lattice physics code is a vital tool, forming a base of reactor coreanalysis. It enables the neutronic properties of the fuel assembly to becalculated and generates a proper set of data to be used by a 3-D full coresimulator. Due to advancement and complexity of modern Boiling WaterReactor assembly designs, a new deterministic lattice physics codeis being developed at Westinghouse Sweden AB, namely PHOENIX5.Each time a new code is written, its methodology of solving the neutrontransport equation, has to be validated to make sure it providesreliable output. In a wake of preparation for PHOENIX5 release andconsecutive validation efforts, a set of reference Monte Carlo calculationswas prepared, using the code Serpent. A depletion calculation with achosen type of branch cases was conducted. Methods implemented inPHOENIX5 are based on the Current Coupling Collision Probabilitymethod used in older versions of the code HELIOS. Therefore, a comparisonbetween reference Monte Carlo simulations and HELIOS 1.8.1is made, in order to discover problems inherent to the said method ofsolving the neutron transport equation. A special care should be givenduring PHOENIX5 validation, to issues highlighted in this work.Discrepancies in results of Serpent and HELIOS are attributed mostlyto disparities in the basic nuclear data used by the codes, as well as arange of approximations and corrections adopted by the deterministiccode.Serpent and HELIOS showed a good agreement in a typical voidrange (up to 90 % void) and ‘less’ challenging branches (coolant void,fuel temperature and spacer grid branches). More significant discrepanciesappeared for extreme cases with a very high void and control rodpresence (k1 differences as high as 1000 pcm) and rather pronouncedconcentrations of the natural boron dissolved in coolant (absolute differencesroughly at a level of 900 pcm). The issues do not seem to stemsolely from discrepancies in the nuclear data libraries used by Serpentand HELIOS.Moreover, a coolant void bias was consistently found in the resultsof branch calculation at changing coolant void. This confirms the analogousphenomenon found in previous studies of the CCCP based deterministiccodes. It most probably stems from the assumptions used bythe method while tackling the neutron transport equation, such as theflat source approximation, the isotropic scattering assumption and thetransport correction. An alternative transport correction approximationis proposed to alleviate this issue.</p>

corrected abstract:
<p>A lattice physics code is a vital tool, forming a base of reactor core analysis. It enables the neutronic properties of the fuel assembly to be calculated and generates a proper set of data to be used by a 3-D full core simulator. Due to advancement and complexity of modern Boiling Water Reactor assembly designs, a new deterministic lattice physics code is being developed at Westinghouse Sweden AB, namely PHOENIX5. Each time a new code is written, its methodology of solving the neutron transport equation, has to be validated to make sure it provides reliable output. In a wake of preparation for PHOENIX5 release and consecutive validation efforts, a set of reference Monte Carlo calculations was prepared, using the code Serpent. A depletion calculation with a chosen type of branch cases was conducted. Methods implemented in PHOENIX5 are based on the Current Coupling Collision Probability method used in older versions of the code HELIOS. Therefore, a comparison between reference Monte Carlo simulations and HELIOS 1.8.1 is made, in order to discover problems inherent to the said method of solving the neutron transport equation. A special care should be given during PHOENIX5 validation, to issues highlighted in this work.</p><p>Discrepancies in results of Serpent and HELIOS are attributed mostly to disparities in the basic nuclear data used by the codes, as well as a range of approximations and corrections adopted by the deterministic code.</p><p>Serpent and HELIOS showed a good agreement in a typical void range (up to 90 % void) and ‘less’ challenging branches (coolant void, fuel temperature and spacer grid branches). More significant discrepancies appeared for extreme cases with a very high void and control rod presence (k<sub>&infin;</sub> differences as high as 1000 pcm) and rather pronounced concentrations of the natural boron dissolved in coolant (absolute differences roughly at a level of 900 pcm). The issues do not seem to stem solely from discrepancies in the nuclear data libraries used by Serpent and HELIOS.</p><p>Moreover, a coolant void bias was consistently found in the results of branch calculation at changing coolant void. This confirms the analogous phenomenon found in previous studies of the CCCP based deterministic codes. It most probably stems from the assumptions used by the method while tackling the neutron transport equation, such as the flat source approximation, the isotropic scattering assumption and the transport correction. An alternative transport correction approximation is proposed to alleviate this issue.</p>
----------------------------------------------------------------------
In diva2:1740195 abstract is:
<p>Carbon fibers submitted to high temperatures (&gt;2000 °C) experience a permanent increasein their thermal conductivity. This change has been attributed to a change in the molecularstructure due to graphitisation. Graphitisation occurs when amorphous carbons are exposed tohigh temperatures (&gt; 1000°C) for a prolonged period of time and describes the process in whichcarbon atoms are rearranged from their amorphous form into structured hexagonal ringed latticesheets. To characterise the extent of this process, one needs to determine certain ring statisticswhich provide information on the bonding structure. In this work, we develop and verify a ringstatistics tool that can be used to analyze the resulting structure of atomistic simulations, and useit in a novel approach to characterise the extent of graphitisation in Molecular Dynamics (MD)simulations of carbon. Different ring definitions, such as Franzblau, Leroux, Hybrid and King arecompared to determine the most appropriate definition for the investigation of carbon structures.A new ring definition, Hybrid, is introduced as an extension of Leroux’s definition, exploiting theefficiency of Leroux’s definition while making the definition more appropriate for carbon systemsby removing shortcuts of length 1. It was found that Franzblau rings most accurately capturecarbon structures, and are most optimal for the investigation of amorphous and graphitisedcarbons. We then apply this tool to two MD simulations of amorphous carbons undergoing anannealing process at 4000K for 300 ps to characterise the extent of graphitisation. We found aprevalence of ∼0.1 hexagonal rings per atom in amorphous carbons prior to annealing, comparedto ∼0.33 hexagonal rings per atom in graphitised carbon after annealing. The likelihood of a ringbeing hexagonal in amorphous carbon was ∼30%, as opposed to ∼75% in graphitised samples.Calculating the ratio in the number of hexagonal rings per atom to the number of hexagonalrings per atom in a fully graphitised system, the extent of graphitisation can be quantified. Sincethis value is normalized by the number of atoms in the simulation this method can be appliedto any domain size. This successful application of the ring statistics tool opens the door toapply it to more realistic and complex systems. The tool has already been expanded to considermulti-component systems and molecule identification. Hence, the tool could already be appliedto more complex cases, such as doped or contaminated systems, investigating the effects on bondstructure. In its current state, the tool could also be used to investigate how the extent andrate of graphitisation changes at different depths in a system. Potentially characterising therate at which graphitisation penetrates a system under various conditions. The tool also hasthe potential to be expanded to consider localisation and identification of defects, bond angles,bond creation and destruction and the structural classification and identification of systems.Combining this tool with MDSuite, a software in development by the Institute for ComputationalPhysics (ICP) at the University of Stuttgart with the collaboration of the von Karman Institutefor Fluid Dynamics (VKI) to analyse MD trajectories, could offer a package that can providedeep system information for minimal cost.</p>

corrected abstract:
<p>Carbon fibers submitted to high temperatures (&gt;2000 °C) experience a permanent increase in their thermal conductivity. This change has been attributed to a change in the molecular structure due to graphitisation. Graphitisation occurs when amorphous carbons are exposed to high temperatures (&gt; 1000°C) for a prolonged period of time and describes the process in which carbon atoms are rearranged from their amorphous form into structured hexagonal ringed lattice sheets. To characterise the extent of this process, one needs to determine certain ring statistics which provide information on the bonding structure. In this work, we develop and verify a ring statistics tool that can be used to analyze the resulting structure of atomistic simulations, and use it in a novel approach to characterise the extent of graphitisation in Molecular Dynamics (MD) simulations of carbon. Different ring definitions, such as Franzblau, Leroux, Hybrid and King are compared to determine the most appropriate definition for the investigation of carbon structures. A new ring definition, Hybrid, is introduced as an extension of Leroux’s definition, exploiting the efficiency of Leroux’s definition while making the definition more appropriate for carbon systems by removing shortcuts of length 1. It was found that Franzblau rings most accurately capture carbon structures, and are most optimal for the investigation of amorphous and graphitised carbons. We then apply this tool to two MD simulations of amorphous carbons undergoing an annealing process at 4000K for 300 ps to characterise the extent of graphitisation. We found a prevalence of ∼0.1 hexagonal rings per atom in amorphous carbons prior to annealing, compared to ∼0.33 hexagonal rings per atom in graphitised carbon after annealing. The likelihood of a ring being hexagonal in amorphous carbon was ∼30%, as opposed to ∼75% in graphitised samples. Calculating the ratio in the number of hexagonal rings per atom to the number of hexagonal rings per atom in a fully graphitised system, the extent of graphitisation can be quantified. Since this value is normalized by the number of atoms in the simulation this method can be applied to any domain size. This successful application of the ring statistics tool opens the door to apply it to more realistic and complex systems. The tool has already been expanded to consider multi-component systems and molecule identification. Hence, the tool could already be applied to more complex cases, such as doped or contaminated systems, investigating the effects on bond structure. In its current state, the tool could also be used to investigate how the extent and rate of graphitisation changes at different depths in a system. Potentially characterising the rate at which graphitisation penetrates a system under various conditions. The tool also has the potential to be expanded to consider localisation and identification of defects, bond angles, bond creation and destruction and the structural classification and identification of systems. Combining this tool with MDSuite, a software in development by the Institute for Computational Physics (ICP) at the University of Stuttgart with the collaboration of the von Karman Institute for Fluid Dynamics (VKI) to analyse MD trajectories, could offer a package that can provide deep system information for minimal cost.</p>
----------------------------------------------------------------------
In diva2:1078086 abstract is:
<p>There is a renewed interest in the wind estimate over and around forest areas dueto the increasing demand of wind-energy resources. Many researches have been donewith simplied forest models. However, the introduction of a simple two-dimensionalclearing adds further parameters, such as the width of the clearing, which furthercomplicates the analysis. The main purpose of the present experimental and nu-merical efforts is, therefore, to characterize the ow over the forest clearing and tosuggest the suitable location for the wind-power generation over the forest clearing.The experiments were performed in the Minimum Turbulence Level (MTL) windtunnel at KTH in Stockholm, and PIV data evaluation and analysis were carried out.The canopy model consists of several wooden at plates, and to each of the plateswooden cylindrical pins were clamped in a staggered layout to mimic a homogeneoushigh-density forest. The total length of the forest model is 40hc, where hc indicatesthe canopy height. Two cases were experimentally investigated, one with a fullforest conguration and the other with the presence of a clearing that starts fromx=hc = 20 and ends at x=hc = 30, where x is a streamwise coordinate that startsat the forest windward edge. Particle Image Velocimetry (PIV) was performed, andplanar velocity snapshots were taken at the downwind edge of the clearing.Large Eddy Simulations (LES) were also conducted to complement the experi-mental information. The present LES code was developed by modifying the DirectNumerical Simulation (DNS) code of the turbulent boundary layer ow by Kametani&amp; Fukagata (2011), by adding the subgrid scale model part and the empirical canopymodel part into the DNS code.Both the experimental and the numerical results indicate that the clearing is as-sociated to a streamwise velocity defect in the mean prole mainly due to the strongturbulent diffusion into the clearing region. The turbulence is redistributed amongstthe various velocity components so that the streamwise velocity variance is reduced,while the vertical velocity variance is enhanced. The streamwise velocity varianceis in fact damped due to the absence of the canopy drag from x=hc = 20, whileenhanced vertical-velocity uctuations can be observed at the end of the clearing.However, the effects are immediately weakened both by a ow re-acceleration andby a new surface layer development right after passing the downwind clearing edge.The clearing effect seems to be dominant in the roughness sublayer at least for theneutral atmospheric conditions. The clearing perturbation seems to be associatedto turbulent mixing at its initial stage near y hc, followed by a rapid distorsionnear the clearing trailing edge. This phenomenon is highlighted by the low valueof the vertical correlation length scale that, after the clearing trailing edge, risesagain towards to homogenous forest condition. The LES results further show thata suitable area for the wind-turbine operation is close to the upwind clearing edgewhere the energy contents is the highest, while the turbulent intensity is lowestbetween the clearing. They also indicate that wind-speed enhancement can be ex-pected downstream of the short forest edge, implying that the ow can be optimizedfor wind-power generation just by changing the forest conguration.</p>

corrected abstract:
<p>There is a renewed interest in the wind estimate over and around forest areas due to the increasing demand of wind-energy resources. Many researches have been done with simplified forest models. However, the introduction of a simple two-dimensional clearing adds further parameters, such as the width of the clearing, which further complicates the analysis. The main purpose of the present experimental and numerical efforts is, therefore, to characterize the flow over the forest clearing and to suggest the suitable location for the wind-power generation over the forest clearing.</p><p>The experiments were performed in the Minimum Turbulence Level (MTL) wind tunnel at KTH in Stockholm, and PIV data evaluation and analysis were carried out. The canopy model consists of several wooden flat plates, and to each of the plates wooden cylindrical pins were clamped in a staggered layout to mimic a homogeneous high-density forest. The total length of the forest model is 40hc, where hc indicates the canopy height. Two cases were experimentally investigated, one with a full forest configuration and the other with the presence of a clearing that starts from <em>x/h<sub>c<sub> = 20</em> and ends at <em>x/h<sub>c</sub> = 30</em>, where <em>x</em> is a streamwise coordinate that starts at the forest windward edge. Particle Image Velocimetry (PIV) was performed, and planar velocity snapshots were taken at the downwind edge of the clearing.</p><p>Large Eddy Simulations (LES) were also conducted to complement the experimental information. The present LES code was developed by modifying the Direct Numerical Simulation (DNS) code of the turbulent boundary layer flow by Kametani &amp; Fukagata (2011), by adding the subgrid scale model part and the empirical canopy model part into the DNS code.</p><p>Both the experimental and the numerical results indicate that the clearing is associated to a streamwise velocity defect in the mean profile mainly due to the strong turbulent diffusion into the clearing region. The turbulence is redistributed amongst the various velocity components so that the streamwise velocity variance is reduced, while the vertical velocity variance is enhanced. The streamwise velocity variance is in fact damped due to the absence of the canopy drag from <em>x/h<sub>c</sub> = 20</em>, while enhanced vertical-velocity fluctuations can be observed at the end of the clearing. However, the effects are immediately weakened both by a flow re-acceleration and by a new surface layer development right after passing the downwind clearing edge. The clearing effect seems to be dominant in the roughness sublayer at least for the neutral atmospheric conditions. The clearing perturbation seems to be associated to turbulent mixing at its initial stage near <em>y ≈ h<sub>c</sub></em>, followed by a rapid distorsion near the clearing trailing edge. This phenomenon is highlighted by the low value of the vertical correlation length scale that, after the clearing trailing edge, rises again towards to homogenous forest condition. The LES results further show that a suitable area for the wind-turbine operation is close to the upwind clearing edge where the energy contents is the highest, while the turbulent intensity is lowest between the clearing. They also indicate that wind-speed enhancement can be expected downstream of the short forest edge, implying that the flow can be optimized for wind-power generation just by changing the forest configuration.</p>
----------------------------------------------------------------------
In diva2:1247161 abstract is:
<p>Nowadays, increasing pressure from legislation and customer demands in the automotive industryare forcing manufacturers to produce greener vehicles with lower emissions and fuel consumption.As a result, electrified and hybrid vehicles are a growing popular alternative to traditional internalcombustion engines (ICE). The noise from an electric vehicle comes mainly from contact betweentyres and road, wind resistance and driveline. The noise emitted from the driveline is for the mostpart related to the gearbox. When developing a driveline, it is a factor of importance to estimatethe noise radiating from the gearbox to achieve an acceptable design.Gears are used extensively in the driveline of electric vehicles. As the gears are in mesh, a mainintrusive concern is known as gear whine noise. Gear whine noise is an undesired vibroacousticphenomenon and is likely to originate through the gear contacts and be transferred through themechanical components to the housing where the vibrations are converted into airborne andstructure-borne noise. The gear whine noise originates primarily from the excitation coming fromtransmission error (TE). Transmission error is defined as the difference between the ideal smoothtransfer of motion of a gear and what is in practice due to lack of smoothness.The main objective of this study is to simulate the vibrations generated by the gear whine noise inan electric powertrain line developed by AVL Vicura. The electric transmission used in this studyprovides only a fixed overall gear ratio, i.e. 9.59, under all operation conditions. It is assumed thatthe system is excited only by the transmission error and the mesh stiffness of the gear contacts. Inorder to perform NVH analysis under different operating conditions, a multibody dynamics modelaccording to the AVL Excite program has been developed. The dynamic simulations are thencompared with previous experimental measurements provided by AVL Vicura.Two validation criteria have been used to analyse the dynamic behaviour of the AVL Excite model:signal processing using the FFT method and comparison with the experimental measurements.The results from the AVL Excite model show that the FFT criterion is quite successful and allexcitation frequencies are properly observed in FFT plots. Nevertheless, when it comes to thesecond criterion, as long as not all dynamic parameters of the system such as damping or stiffnesscoefficients are provided with certainty in the model, it is too difficult to investigate the accuracy ofthe AVL Excite model.Another investigation is a numerical design study to analyses how the damping coefficientsinfluence the response. After reducing the damping parameters, the results show that the housingand bearings have the highest influence on the response. If more acceptable results are desired,future studies must be concentrated on these to obtain more acceptable damping values.</p>


corrected abstract:
<p>Nowadays, increasing pressure from legislation and customer demands in the automotive industry are forcing manufacturers to produce greener vehicles with lower emissions and fuel consumption. As a result, electrified and hybrid vehicles are a growing popular alternative to traditional internal combustion engines (ICE). The noise from an electric vehicle comes mainly from contact between tyres and road, wind resistance and driveline. The noise emitted from the driveline is for the most part related to the gearbox. When developing a driveline, it is a factor of importance to estimate the noise radiating from the gearbox to achieve an acceptable design.</p><p>Gears are used extensively in the driveline of electric vehicles. As the gears are in mesh, a main intrusive concern is known as gear whine noise. Gear whine noise is an undesired vibroacoustic phenomenon and is likely to originate through the gear contacts and be transferred through the mechanical components to the housing where the vibrations are converted into airborne and structure-borne noise. The gear whine noise originates primarily from the excitation coming from transmission error (TE). Transmission error is defined as the difference between the ideal smooth transfer of motion of a gear and what is in practice due to lack of smoothness.</p><p>The main objective of this study is to simulate the vibrations generated by the gear whine noise in an electric powertrain line developed by AVL Vicura. The electric transmission used in this study provides only a fixed overall gear ratio, i.e. 9.59, under all operation conditions. It is assumed that the system is excited only by the transmission error and the mesh stiffness of the gear contacts. In order to perform NVH analysis under different operating conditions, a multibody dynamics model according to the AVL Excite program has been developed. The dynamic simulations are then compared with previous experimental measurements provided by AVL Vicura.</p><p>Two validation criteria have been used to analyse the dynamic behaviour of the AVL Excite model: signal processing using the FFT method and comparison with the experimental measurements. The results from the AVL Excite model show that the FFT criterion is quite successful and all excitation frequencies are properly observed in FFT plots. Nevertheless, when it comes to the second criterion, as long as not all dynamic parameters of the system such as damping or stiffness coefficients are provided with certainty in the model, it is too difficult to investigate the accuracy of the AVL Excite model.</p><p>Another investigation is a numerical design study to analyses how the damping coefficients influence the response. After reducing the damping parameters, the results show that the housing and bearings have the highest influence on the response. If more acceptable results are desired, future studies must be concentrated on these to obtain more acceptable damping values.</p>
----------------------------------------------------------------------
In diva2:1078069 - missing space in title:
"Partially Premixed Combustion (PPC) for low loadconditions in marine engines using computationaland experimental techniques"
==>
"Partially Premixed Combustion (PPC) for low loadconditions in marine engines using computational and experimental techniques"

abstract is:
<p>Diesel Engine has been the most powerful and relevant source of power in the automobile industryfor decades due to their excellent performance, efficiency and power. On the contrary, there arenumerous environmental issues of the diesel engines hampering the environment. It has been agreat challenge for the researchers and scientists to minimize these issues. In the recent years, severalstrategies have been introduced to eradicate the emissions of the diesel engines. Among them,Partially Premixed Combustion (PPC) is one of the most emerging and reliable strategies. PPC is acompression ignited combustion process in which ignition delay is controlled. PPC is intended toendow with better combustion with low soot and NOx emission.The engine used in the present study is a single-cylinder research engine, installed in Aalto UniversityInternal Combustion Engine Laboratory with the bore diameter of 200 mm. The thesis presentsthe validation of the measurement data with the simulated cases followed by the study of the sprayimpingement and fuel vapor mixing in PPC mode for different injection timing. A detailed study ofthe correlation of early injection with the fuel vapor distribution and wall impingement has beenmade.The simulations are carried out with the commercial CFD software STAR CD. Different injectionparameters have been considered and taken into an account to lower the wall impingement and toproduce better air-fuel mixing with the purpose of good combustion and reduction of the emissions.The result of the penetration length of the spray and the fuel vapor distribution for different earlyinjection cases have been illustrated in the study. Comparisons of different thermodynamic propertiesand spray analysis for different injection timing have been very clearly illustrated to get insightof effect of early injection. The parameters like injection timing, injection period, injection pressure,inclusion angle of the spray have an influence the combustion process in PPC mode. Extensivestudy has been made for each of these parameters to better understand their effects in the combustionprocess. Different split injection profiles have been implemented for the study of better fuelvapor distribution in the combustion chamber.The final part of the thesis includes the study of the combustion and implementation of EGR tocontrol the temperature so as to get more prolonged ignition delay to accompany the PPC strategyfor standard piston top and deep bowl piston top. With the injection optimization and implementationof EGR, NOx has been reduced by around 44%, CO by 60% and Soot by 66% in the standardpiston top. The piston optimization resulted in more promising result with 58% reduction in NOx,55% reduction in CO and 67% reduction in Soot. In both cases the percentage of fuel burnt wasincreased by around 8%.</p>



corrected abstract:
<p>Diesel Engine has been the most powerful and relevant source of power in the automobile industry for decades due to their excellent performance, efficiency and power. On the contrary, there are numerous environmental issues of the diesel engines hampering the environment. It has been a great challenge for the researchers and scientists to minimize these issues. In the recent years, several strategies have been introduced to eradicate the emissions of the diesel engines. Among them, Partially Premixed Combustion (PPC) is one of the most emerging and reliable strategies. PPC is a compression ignited combustion process in which ignition delay is controlled. PPC is intended to endow with better combustion with low soot and NOx emission.</p><p>The engine used in the present study is a single-cylinder research engine, installed in Aalto University Internal Combustion Engine Laboratory with the bore diameter of 200 mm. The thesis presents the validation of the measurement data with the simulated cases followed by the study of the spray impingement and fuel vapor mixing in PPC mode for different injection timing. A detailed study of the correlation of early injection with the fuel vapor distribution and wall impingement has been made.</p><p>The simulations are carried out with the commercial CFD software STAR CD. Different injection parameters have been considered and taken into an account to lower the wall impingement and to produce better air-fuel mixing with the purpose of good combustion and reduction of the emissions. The result of the penetration length of the spray and the fuel vapor distribution for different early injection cases have been illustrated in the study. Comparisons of different thermodynamic properties and spray analysis for different injection timing have been very clearly illustrated to get insight of effect of early injection. The parameters like injection timing, injection period, injection pressure, inclusion angle of the spray have an influence the combustion process in PPC mode. Extensive study has been made for each of these parameters to better understand their effects in the combustion process. Different split injection profiles have been implemented for the study of better fuel vapor distribution in the combustion chamber.</p><p>The final part of the thesis includes the study of the combustion and implementation of EGR to control the temperature so as to get more prolonged ignition delay to accompany the PPC strategy for standard piston top and deep bowl piston top. With the injection optimization and implementation of EGR, NOx has been reduced by around 44%, CO by 60% and Soot by 66% in the standard piston top. The piston optimization resulted in more promising result with 58% reduction in NOx, 55% reduction in CO and 67% reduction in Soot. In both cases the percentage of fuel burnt was increased by around 8%.</p>
----------------------------------------------------------------------
In diva2:1816751 abstract is:
<p>This thesis investigates reasons for signiﬁcant uncertainties in added wave resistance predictionsand how wave conditions can potentially aﬀect the design of RoPax ferries. The objectiveis to ﬁnd a suitable prediction method of added wave resistance for the RoPax ferry designapplication. Furthermore, the wave environment on the route strongly inﬂuences this delicateand complex phenomenon. Thus, the emphasis is to understand the added wave resistancethrough a case study with a probabilistic wave environment.The fast transition into decarbonization and regulations regarding energy-eﬃcient ship designhave ramped up the awareness of the inﬂuence of seaways. For lower speeds, the addedresistance becomes a more signiﬁcant part of total resistance, with concerns regarding minimumpropulsion power and safe maneuvering in adverse sea conditions. Consequently, the demandhas rocketed for profound insight and accurate prediction methods of added wave resistance. Inaddition, with new larger ships, added wave resistance domain for short waves becomes essentialand an additional challenge regarding predictions. Nevertheless, added wave resistancepredictions are complex and contain many pitfalls, so accurate estimations of the ship’s addedwave resistance response and wave environment are crucial. In addition, added wave resistanceis very ship-speciﬁc, and published research for RoPax ferries is rare.Due to signiﬁcant uncertainties for general numerical methods, the study investigates a new(modiﬁed NTUA) semi-empirical method reﬁned for ships with a large beam-to-draft ratio.In addition, a realistic wave environment is included by selecting relevant wave spectra forconditions on the route.The study shows that signiﬁcant variances of added wave resistance predictions arise fromselecting prediction methods beyond the range of applicability and rough assumptions of waveconditions and spectra. The case study discovered that errors might also be introduced bythe classiﬁcation society deﬁnitions, which gives reasons to rethink the applied deﬁnition of"average BF 8" wave conditions for Safe Return to Port (SRtP) assessments. This can causea false illusion of the ship’s performance and safety in waves. Only the misjudgment ofthe most critical peak period resulted in a rough underestimation (&gt; 40%) of mean addedwave resistance. The error corresponded to 215% of the still water resistance for the SRtPassessment. In addition, the nature of added wave resistance is very ship speciﬁc. Therefore, theauthor emphasizes caution when selecting the prediction method, especially for semi-empiricalmethods. Despite the ﬁrst promising glance of the applied semi-empirical method, it appearsthat the ship database correlates poorly for RoPax ferries. Reliability for the method is weak forshort waves, with a tendency to large overestimations. The lack of references of RoPax vesselsfor validations, accident statics in adverse sea conditions and recent insight into nonlinear eﬀectsrequest further research on added wave resistance for modern hull shapes.</p>

corrected abstract:
<p>This thesis investigates reasons for significant uncertainties in added wave resistance predictions and how wave conditions can potentially affect the design of RoPax ferries. The objective is to find a suitable prediction method of added wave resistance for the RoPax ferry design application. Furthermore, the wave environment on the route strongly influences this delicate and complex phenomenon. Thus, the emphasis is to understand the added wave resistance through a case study with a probabilistic wave environment.</p><p>The fast transition into decarbonization and regulations regarding energy-efficient ship design have ramped up the awareness of the influence of seaways. For lower speeds, the added resistance becomes a more significant part of total resistance, with concerns regarding minimum propulsion power and safe maneuvering in adverse sea conditions. Consequently, the demand has rocketed for profound insight and accurate prediction methods of added wave resistance. In addition, with new larger ships, added wave resistance domain for short waves becomes essential and an additional challenge regarding predictions. Nevertheless, added wave resistance predictions are complex and contain many pitfalls, so accurate estimations of the ship’s added wave resistance response and wave environment are crucial. In addition, added wave resistance is very ship-specific, and published research for RoPax ferries is rare.</p><p>Due to significant uncertainties for general numerical methods, the study investigates a new (modified NTUA) semi-empirical method refined for ships with a large beam-to-draft ratio. In addition, a realistic wave environment is included by selecting relevant wave spectra for conditions on the route.</p><p>The study shows that significant variances of added wave resistance predictions arise from selecting prediction methods beyond the range of applicability and rough assumptions of wave conditions and spectra. The case study discovered that errors might also be introduced by the classification society definitions, which gives reasons to rethink the applied definition of "average BF 8" wave conditions for Safe Return to Port (SRtP) assessments. This can cause a false illusion of the ship’s performance and safety in waves. Only the misjudgment of the most critical peak period resulted in a rough underestimation (&gt; 40%) of mean added wave resistance. The error corresponded to 215% of the still water resistance for the SRtP assessment. In addition, the nature of added wave resistance is very ship specific. Therefore, the author emphasizes caution when selecting the prediction method, especially for semi-empirical methods. Despite the first promising glance of the applied semi-empirical method, it appears that the ship database correlates poorly for RoPax ferries. Reliability for the method is weak for short waves, with a tendency to large overestimations. The lack of references of RoPax vessels for validations, accident statics in adverse sea conditions and recent insight into nonlinear effects request further research on added wave resistance for modern hull shapes.</p>
----------------------------------------------------------------------
In diva2:1465518 - missing space in title:
"Accelerated test for vehicle body durability basedon vehicle dynamics simulations using pseudo damage"
==>
"Accelerated test for vehicle body durability based on vehicle dynamics simulations using pseudo damage"

abstract is: <p>Vehicle body durability evaluation strongly relies on the possibility of testing areal prototype on different testing surfaces, such as proving grounds, test rigsand real roads. Although many efforts have been made to reduce time requiredfor testing, this still remains one of the main resource-demanding phases in avehicle development. As direct consequence, more and more companies aim tooptimise and to improve vehicle development by means of CAE tools.This master thesis is a step towards virtual testing of a vehicle body, aimingto investigate and to select the most important measurements for a bodydurability evaluation and to reproduce an invariant excitation that could beapplied to other vehicle models for new durability assessments without theneed of real measurements from other models. Moreover, a comparison betweenfrequency-based methods and time-based methods and their differences werehighlighted and the validity of ISO8608:2016 discussed.The method relied on small sets of simple and easy-to-get internal measurements,called desired signal, that allowed back-calculation of wheel hubdisplacements and other excitations by means of the product of the model’stransfer function and desired signal. Then, the iteration procedure allowedto drastically reduce the error between the desired signal and the computedone and it proved to be essential in the process. Thanks to this procedure,damage information of also the not-measured signals could be computed andtheir damage assessed and thus available for durability purposes. Moreover, thechance of applying the same measurements taken from a real vehicle to a modelof a newer generation was investigated. This would avoid the need of buildinga running prototype, allowing a more accurate durability assessment alreadyavailable in the pre-design phase.As a result, using a specific set of 8 measurements, other 22 other forces,displacements and velocities of several components were precisely reproduced,showing that not all measurement are equally valuable in a durability evaluation.A method for the measurement selection, called Observability Method, wasthen developed and compared with a set of measurements selected by meansof experience, showing a better convergence of the pseudo damage and similarpseudo damage values. Eventually a small set of measures from an older carwas applied to a newer version. It was proved that a detailed knowledge ofthe car model is needed, in order to successfully back-calculate the relevantmeasurements.</p>



corrected abstract:
<p>Vehicle body durability evaluation strongly relies on the possibility of testing a real prototype on different testing surfaces, such as proving grounds, test rigs and real roads. Although many efforts have been made to reduce time required for testing, this still remains one of the main resource-demanding phases in a vehicle development. As direct consequence, more and more companies aim to optimise and to improve vehicle development by means of CAE tools.</p><p>This master thesis is a step towards virtual testing of a vehicle body, aiming to investigate and to select the most important measurements for a body durability evaluation and to reproduce an invariant excitation that could be applied to other vehicle models for new durability assessments without the need of real measurements from other models. Moreover, a comparison between frequency-based methods and time-based methods and their differences were highlighted and the validity of ISO8608:2016 discussed.</p><p>The method relied on small sets of simple and easy-to-get internal measurements, called desired signal, that allowed back-calculation of wheel hub displacements and other excitations by means of the product of the model’s transfer function and desired signal. Then, the iteration procedure allowed to drastically reduce the error between the desired signal and the computed one and it proved to be essential in the process. Thanks to this procedure, damage information of also the not-measured signals could be computed and their damage assessed and thus available for durability purposes. Moreover, the chance of applying the same measurements taken from a real vehicle to a model of a newer generation was investigated. This would avoid the need of building a running prototype, allowing a more accurate durability assessment already available in the pre-design phase.</p><p>As a result, using a specific set of 8 measurements, other 22 other forces, displacements and velocities of several components were precisely reproduced, showing that not all measurement are equally valuable in a durability evaluation. A method for the measurement selection, called Observability Method, was then developed and compared with a set of measurements selected by means of experience, showing a better convergence of the pseudo damage and similar pseudo damage values. Eventually a small set of measures from an older car was applied to a newer version. It was proved that a detailed knowledge of the car model is needed, in order to successfully back-calculate the relevant measurements.</p>
----------------------------------------------------------------------
In diva2:915628 abstract is:
<p>Resistance spot welding (RSW) is the dominant joining technology in the automotive industry. This is due to its low costs and high efficiency. Other advantages with RSW is the high ability for automation,low consumption of energy, lack of need for added materials and low degree of pollution,no expensive equipment or education of personal compared to arc welding and laser welding. A modern automobile contains approximately 4000 resistance spot welds,which is why it is of great interest to be ableto predict the properties of the resistance spotwelds. The most important measurement used to ensure the quality of the weld is thenugget size, as it correlates to the weldsmechanical strength. This is usually measured by destructive testing, and the most common method is the coach peel test. Thistest is performed by manually peeling the specimen and then measuring the largest and smallest nugget diameter. It is also possibleto perform non-destructive testing on resistance spot welds with both ultrasonic and x-ray tests, however none of these methods have the same accuracy as the destructive methods and they are cumbersome to use in large-scale. Toimprove the efficiency and lower the cost forthe optimization of the welding parameters,simulation tools have been developed. Thereare both 2D- and 3D-simulation software tomodel the RSW process. When the spotwelds are simulated with 2D or 2D axis symmetry,the number of elements is lowercompared to a full 3D model, which reducesthe computation times. The disadvantageswith the 2D model are the inabilities tomodel misalignments or other asymmetricalgeometries. In contrast, the 3D-simulationsare not limited by these factors, and they arealso capable of modeling the shunt effectsoccurring when a weld is placed close to aprevious weld.The aims of this thesis was to evaluate such a3D-simulation software, Sorpas 3D, and itspotential to be used in industrial processplanning, and to assess the software’s usefulness for both simple and more complexcases.The results from this work show a good correspondence between the simulations andthe physical tests. However, in order to achieve these results a number of modifications in the material properties were  required. Another critical limitation in the software is that no expulsion criterion isimplemented. Considering the possibility that the problems can be solved with a number ofupdates in the software, Sorpas 3D can be auseful tool in the process planning industry inorder to decrease process times and materialcosts and improve the weld quality in thefuture.</p>

corrected abstract:
<p>Resistance spot welding (RSW) is the dominant joining technology in the automotive industry. This is due to its low costs and high efficiency. Other advantages with RSW is the high ability for automation, low consumption of energy, lack of need for added materials and low degree of pollution, no expensive equipment or education of personal compared to arc welding and laser welding. A modern automobile contains approximately 4000 resistance spot welds, which is why it is of great interest to be able to predict the properties of the resistance spot welds. The most important measurement used to ensure the quality of the weld is the nugget size, as it correlates to the welds mechanical strength. This is usually measured by destructive testing, and the most common method is the coach peel test. This test is performed by manually peeling the specimen and then measuring the largest and smallest nugget diameter. It is also possible to perform non-destructive testing on resistance spot welds with both ultrasonic and x-ray tests, however none of these methods have the same accuracy as the destructive methods and they are cumbersome to use in large-scale. To improve the efficiency and lower the cost for the optimization of the welding parameters, simulation tools have been developed. There are both 2D- and 3D-simulation software to model the RSW process. When the spot welds are simulated with 2D or 2D axis-symmetry, the number of elements is lower compared to a full 3D model, which reduces the computation times. The disadvantages with the 2D model are the inabilities to model misalignments or other asymmetrical geometries. In contrast, the 3D-simulations are not limited by these factors, and they are also capable of modeling the shunt effects occurring when a weld is placed close to a previous weld.</p><p>The aims of this thesis was to evaluate such a 3D-simulation software, Sorpas 3D, and its potential to be used in industrial process planning, and to assess the software’s usefulness for both simple and more complex cases.</p><p>The results from this work show a good correspondence between the simulations and the physical tests. However, in order to achieve these results a number of modifications in the material properties were required. Another critical limitation in the software is that no expulsion criterion is implemented. Considering the possibility that the problems can be solved with a number of updates in the software, Sorpas 3D can be a useful tool in the process planning industry in order to decrease process times and material costs and improve the weld quality in the future.</p>
----------------------------------------------------------------------
In diva2:1880451 abstract is:
<p>Positron emission tomography (PET) is a nuclear medicine imaging techniquethat uses radiotracers to visualize processes like metabolism and perfusion. Theradiotracer emits positrons, which collide with shell electrons of the atomsthat make up the surrounding tissue. Such a collision produces two gammarayphotons, emitted roughly 180 degrees apart [1]. PET captures thesephotons using a cylindrical arrangement of detectors. When two photons aredetected simultaneously by different detectors, it registers as a line of response(LOR). These LORs are then pre-processed into a sinogram. A mathematicalreconstruction method is used to computationally recover the 3D distribution ofthe radiotracer (activity map) from the sinogram. However, genuine LORs can becorrupted by false LORs that come from scattering, random events, and spuriousevents. Mitigating these in reconstruction algorithms is essential for improvingPET imaging accuracy and reliability.This paper explores the theoretical foundation of the Time of Flight (TOF) SingleScatter Simulation (SSS) model by Watson (2007) [2]. It also includes a Pythonimplementation of the MATLAB code associated with [2]. The model modelsCompton scattering to accurately estimate scattered photons in PET.Incorporating TOF data into the SSS model improves estimation accuracy, albeitat the cost of increased computational time. To expedite computations, thealgorithm was simplified by restricting operations to a subset of rings anddetectors and by pre-processing images through cropping and downscaling.Interpolation fills in missing data, ensuring complete estimation.The outcome of this project is a Python implementation that exhibited a strongcorrelation with the estimates obtained using the MATLAB implementation. Anotable issue arose during the comparison between the main components ofthe SSS algorithm in Python and MATLAB. The Euclidean norm between theresults from these two implementations was significant, indicating that they wereon different scales. Nevertheless, both implementations accurately predictedthe scatter in the same locations and relative magnitudes, despite the scalediscrepancy. Investigation into the discrepancy’s cause is ongoing, but theproject demonstrates the feasibility of implementing the TOF SSS algorithm inPython.</p>


corrected abstract:
<p>Positron emission tomography (PET) is a nuclear medicine imaging technique that uses radiotracers to visualize processes like metabolism and perfusion. The radiotracer emits positrons, which collide with shell electrons of the atoms that make up the surrounding tissue. Such a collision produces two gamma-ray photons, emitted roughly 180 degrees apart [1]. PET captures these photons using a cylindrical arrangement of detectors. When two photons are detected simultaneously by different detectors, it registers as a line of response (LOR). These LORs are then pre-processed into a sinogram. A mathematical reconstruction method is used to computationally recover the 3D distribution of the radiotracer (activity map) from the sinogram. However, genuine LORs can be corrupted by false LORs that come from scattering, random events, and spurious events. Mitigating these in reconstruction algorithms is essential for improving PET imaging accuracy and reliability.</p><p>This paper explores the theoretical foundation of the Time of Flight (TOF) Single Scatter Simulation (SSS) model by Watson (2007) [2]. It also includes a Python implementation of the MATLAB code associated with [2]. The model models Compton scattering to accurately estimate scattered photons in PET.</p><p>Incorporating TOF data into the SSS model improves estimation accuracy, albeit at the cost of increased computational time. To expedite computations, the algorithm was simplified by restricting operations to a subset of rings and detectors and by pre-processing images through cropping and downscaling. Interpolation fills in missing data, ensuring complete estimation.</p><p>The outcome of this project is a Python implementation that exhibited a strong correlation with the estimates obtained using the MATLAB implementation. A notable issue arose during the comparison between the main components of the SSS algorithm in Python and MATLAB. The Euclidean norm between the results from these two implementations was significant, indicating that they were on different scales. Nevertheless, both implementations accurately predicted the scatter in the same locations and relative magnitudes, despite the scale discrepancy. Investigation into the discrepancy’s cause is ongoing, but the project demonstrates the feasibility of implementing the TOF SSS algorithm in Python.</p>
----------------------------------------------------------------------
In diva2:1110752 - error in title:
"Identification and modelling of noise sources on a realisticnose landing gear using phased array methods applied tocomputational data"
==>
"Identification and modelling of noise sources on a realistic nose landing gear using phased array methods applied to computational data"


abstract is:
<p>Due to the recent development of quieter turbofan engines, airframe noise has started to emerge asthe most important noise source. This is particularly true during the approach/landing phase, whenthe engines are operated at low-thrust levels. In order to meet future noise level regulations, thecharacterization and subsequent reduction of landing gear induced noise is necessary. Wind-tunnelaeroacoustic tests have always been the favoured method for assessing and studying the noise generatedby landing gears, but their prohibitive cost has steered the attention towards numerical methods.Since direct flow noise simulations are still too demanding in computer resources, there is astrong interest in developing coupled CFD-CAA simulations as a tool to model and identify flownoise sources. More recently, they have been coupled with phased array methods in order to conductaeroacoustic studies on scaled-down, or simplified, aircraft components. This project investigates theaerodynamic sound sources on a realistic nose landing gear using numerical phased array methods,based on array data extracted from compressible Detached Eddy Simulations of the flow. Assumingmonopole and dipole modes of propagation, the sound sources are identified in the source regionthrough beamforming approaches: conventional beamforming, dual linear programming (dual-LP)deconvolution, orthogonal beamforming and CLEAN-SC. To assess the accuracy of the employedmethods, beamforming maps from flyover, sideline and forward point of views are obtained andcompared to experimental ones originating from wind-tunnel experiments performed on the samenose landing gear configuration by industrial and academic partners of the ALLEGRA project. Anarray design metric is defined to quantitatively assess the fitness of the employed arrays with respectto the different frequencies and distances separating the beamforming and array planes. A geneticalgorithm based on the Differential Evolution method is used to generate optimized arrays for selectedfrequencies in order to reduce the computational size of the problems solved. The modelledsources are used to generate far-field spectra which are subsequently compared to the ones obtainedwith the FfowcsWilliams and Hawkings acoustic analogy. The results show a good concordance betweenthe numerical phased array beamforming maps and the experimental ones, and a good matchbetween the far-field spectra up to a certain frequency threshold corresponding to the quality of themesh used. The presence of specific noise sources has been validated and their contribution to theoverall generated noise has been quantified. The results obtained demonstrate the potential of numericalphased array methods as a legitimate tool for aeroacoustic simulations in general and as atool to gain insight into the noise generation mechanisms of landing gear components in particular.</p>



corrected abstract:
<p>Due to the recent development of quieter turbofan engines, airframe noise has started to emerge as the most important noise source. This is particularly true during the approach/landing phase, when the engines are operated at low-thrust levels. In order to meet future noise level regulations, the characterization and subsequent reduction of landing gear induced noise is necessary. Wind-tunnel aeroacoustic tests have always been the favoured method for assessing and studying the noise generated by landing gears, but their prohibitive cost has steered the attention towards numerical methods. Since direct flow noise simulations are still too demanding in computer resources, there is a strong interest in developing coupled CFD-CAA simulations as a tool to model and identify flow noise sources. More recently, they have been coupled with phased array methods in order to conduct aeroacoustic studies on scaled-down, or simplified, aircraft components. This project investigates the aerodynamic sound sources on a realistic nose landing gear using numerical phased array methods, based on array data extracted from compressible Detached Eddy Simulations of the flow. Assuming monopole and dipole modes of propagation, the sound sources are identified in the source region through beamforming approaches: conventional beamforming, dual linear programming (dual-LP) deconvolution, orthogonal beamforming and CLEAN-SC. To assess the accuracy of the employed methods, beamforming maps from flyover, sideline and forward point of views are obtained and compared to experimental ones originating from wind-tunnel experiments performed on the same nose landing gear configuration by industrial and academic partners of the ALLEGRA project. An array design metric is defined to quantitatively assess the fitness of the employed arrays with respect to the different frequencies and distances separating the beamforming and array planes. A genetic algorithm based on the Differential Evolution method is used to generate optimized arrays for selected frequencies in order to reduce the computational size of the problems solved. The modelled sources are used to generate far-field spectra which are subsequently compared to the ones obtained with the Ffowcs Williams and Hawkings acoustic analogy. The results show a good concordance between the numerical phased array beamforming maps and the experimental ones, and a good match between the far-field spectra up to a certain frequency threshold corresponding to the quality of the mesh used. The presence of specific noise sources has been validated and their contribution to the overall generated noise has been quantified. The results obtained demonstrate the potential of numerical phased array methods as a legitimate tool for aeroacoustic simulations in general and as a tool to gain insight into the noise generation mechanisms of landing gear components in particular.</p>
----------------------------------------------------------------------
In diva2:618564 merged words and missing ligatures - note the corrected abstract needs to be checked agains the original abstract in the thesis - but there is no full text in DiVA.

abstract is:
<p>Due to progress in CFD (Computational Fluid Dynamics), it is now possible to compute and analyzesteady ow and phenomena for turbomachine design. Unsteady instability predictions are important tocertify that a turbomachine will not encounter high vibration levels in operation. Flutter is one of the mostcommon fan instabilities. Thus, the fan design is bound to respect a given Flutter Margin. It guaranteesa certain operation envelope for the engine and its fan, refered as operability. The operation envelope ofan engine is dened in the fan map mass ow rate - pressure ratio as the space in which the engine can berun in during operation. The fan map is made of isovelocities. An isovelocity is a line described by varyingmass ow rate and keeping constant the rotational speed. This domain is bounded by phenomena such asrotating stall, surge, utter, etc which are hazardous for engine mechanical integrity. Fig. 1 highlights howan operating envelope is bounded. When operation envelope is too small, the fan blade geometry needsto be modied to improve its utter behavior and therefore increase the size of the envelope. Eciency,pressure ratio and operability can be strongly impacted by the changes made. Therefore design parametersinuencing utter must be precisely spotted. This can be done if mechanisms which trigger utter are wellunderstood. Thus the blade can be reshaped to lower the contribution of a given phenomenon. However,when a phenomenon is identied, one should quantify its contribution to the globally stable or unstablebehavior. In fact, to reduce the geometrical changes and therefore consequences on operability, one shouldact on the most critical phenomenon.The study has been performed on a single fan blade with dierent congurations of back pressure androtational speed. Consequently, two kinds of utter are investigated : stall utter and transonic utter. Therst one occurs at low rotational speed. It corresponds to zone 1 in Fig. 1. It is commonly driven by owseparation. As described in,1 separation on the suction side of a blade can be responsible for utter. Article1shows that unsteady pressure and blade motion are out of phase in the separated zone. Furthermore, studiesin2 reveal that traveling Mach waves on the blade surface can destabilize it. It depends on the phase betweenthe waves and the blade motion. Acoustic interference is also studied in.1 Transonic utter occurs at higherrotational speed when the blade is shocked. It corresponds to zone 2 in Fig. 1. There are two main sourcesthat destabilize the blade: interaction between the shock waves and the boundary layer and the shock waveoscillation as presented in.3 That study divides the prole into four parts. Each part corresponds to a givenmechanism: supersonic part (stabilizing), shocked part (both stabilizing and destabilizing if the shock wavesoscillates), downstream the shock (destabilizing due to separation) on the suction side and the pressure side(stabilizing).The objective of this paper is not only to identify mechanisms responsible for utter at low and high rotationalspeed but also to follow their evolution along an isovelocity. A global approach from the mechanismsidentication to the quantication of the phenomenon is then described.The critical mechanisms responsiblefor fan blade utter for a given conguration can be pointed out.</p>

corrected abstract:
<p>Due to progress in CFD (Computational Fluid Dynamics), it is now possible to compute and analyze steady flow and phenomena for turbomachine design. Unsteady instability predictions are important to certify that a turbomachine will not encounter high vibration levels in operation. Flutter is one of the most common fan instabilities. Thus, the fan design is bound to respect a given Flutter Margin. It guarantees a certain operation envelope for the engine and its fan, refered as operability. The operation envelope of an engine is defined in the fan map mass flow rate - pressure ratio as the space in which the engine can be run in during operation. The fan map is made of isovelocities. An isovelocity is a line described by varying mass flow rate and keeping constant the rotational speed. This domain is bounded by phenomena such as rotating stall, surge, flutter, etc which are hazardous for engine mechanical integrity. Fig. 1 highlights how an operating envelope is bounded. When operation envelope is too small, the fan blade geometry needs to be modified to improve its flutter behavior and therefore increase the size of the envelope. Efficiency, pressure ratio and operability can be strongly impacted by the changes made. Therefore design parameters influencing flutter must be precisely spotted. This can be done if mechanisms which trigger flutter are well understood. Thus the blade can be reshaped to lower the contribution of a given phenomenon. However, when a phenomenon is identied, one should quantify its contribution to the globally stable or unstable behavior. In fact, to reduce the geometrical changes and therefore consequences on operability, one should act on the most critical phenomenon. The study has been performed on a single fan blade with different configurations of back pressure and rotational speed. Consequently, two kinds of flutter are investigated : stall flutter and transonic flutter. The first one occurs at low rotational speed. It corresponds to zone 1 in Fig. 1. It is commonly driven by flow separation. As described in, 1 separation on the suction side of a blade can be responsible for flutter. Article 1 shows that unsteady pressure and blade motion are out of phase in the separated zone. Furthermore, studies in 2 reveal that traveling Mach waves on the blade surface can destabilize it. It depends on the phase between the waves and the blade motion. Acoustic interference is also studied in .1 Transonic flutter occurs at higher rotational speed when the blade is shocked. It corresponds to zone 2 in Fig. 1. There are two main sources that destabilize the blade: interaction between the shock waves and the boundary layer and the shock wave oscillation as presented in.3 That study divides the prole into four parts. Each part corresponds to a given mechanism: supersonic part (stabilizing), shocked part (both stabilizing and destabilizing if the shock waves oscillates), downstream the shock (destabilizing due to separation) on the suction side and the pressure side(stabilizing). The objective of this paper is not only to identify mechanisms responsible for flutter at low and high rotational speed but also to follow their evolution along an isovelocity. A global approach from the mechanisms identication to the quantication of the phenomenon is then described. The critical mechanisms responsible for fan blade flutter for a given configuration can be pointed out.</p>
----------------------------------------------------------------------
In diva2:1528140 abstract is:
<p>Due to the accelerating need for decarbonization in the shipping sector, wind-assisted cargo shipsare able to play a key role in achieving the IMO 2050 targets on reducing the total annual GHGemissions from international shipping by at least 50%. The aim of this Master’s Thesis project is todevelop a Performance Prediction Program for wind-assisted cargo ships to contribute knowledgeon the performance of this technology. The three key characteristics of this model are its genericstructure, the small number of input data needed and its ability to predict the performance of threepossible Wind-Assisted Propulsion Systems (WAPS): Rotor Sails, Rigid Wing Sails and DynaRigs.It is a fast and easy tool able to predict, to a good level of accuracy and really low computationaltime, the performance of any commercial ship with these three WAPS options installed with onlythe main particulars and general dimensions as input data.The hull and WAPS models predict the forces and moments, which the program balances in 6degrees of freedom to predict the theoretical sailing performance of the wind-assisted cargo shipwith the specified characteristics for various wind conditions. The model is able to play with differentoptimization objectives. This includes maximizing sailing speed if a VPP is run or maximizingtotal power savings if it is a PPP. The program is based on semi-empirical methods and a WAPSaerodynamic database created from published data on lift and drag coefficients. All WAPS datacan be interpolated with the aim to scale to different sizes and configurations such as number ofunits and different aspect ratios.A model validation is carried out to evaluate its reliability. The model results are compared withthe real sailing data of the Long Range 2 (LR2) class tanker vessel, the Maersk Pelican, whichwas recently fitted with two 30 meter high Rotor Sails; and results from another performanceprediction program. In general, the two performance prediction programs and some of the realsailing measurements show good agreement. However, for some downwind sailing conditions, theperformance predictions are more conservative than the measured values.Results showing and comparing power savings, thrust and side force coefficients for the differentWAPS are also presented and discussed. The results of this Master’s Thesis project show howWind-Assisted Propulsion Systems have high potential in playing a key role in the decarbonizationof the shipping sector. WAPS can prove substantial power, fuel, cost, and emissions savings.Tankers and bulk-carriers are specially suitable for wind propulsion thanks to their available deckspace and relatively low design speeds.The Performance Prediction Program for wind-assisted cargo ships developed in this Master’sThesis shows promising results with a good level of accuracy despite its generic and small numberof input data. It can be a useful tool in early project stages to quickly and accurately assess thepotential and performance of WAPS systems.</p>


corrected abstract:
<p>Due to the accelerating need for decarbonization in the shipping sector, wind-assisted cargo ships are able to play a key role in achieving the IMO 2050 targets on reducing the total annual GHG emissions from international shipping by at least 50%. The aim of this Master’s Thesis project is to develop a Performance Prediction Program for wind-assisted cargo ships to contribute knowledge on the performance of this technology. The three key characteristics of this model are its generic structure, the small number of input data needed and its ability to predict the performance of three possible Wind-Assisted Propulsion Systems (WAPS): Rotor Sails, Rigid Wing Sails and DynaRigs. It is a fast and easy tool able to predict, to a good level of accuracy and really low computational time, the performance of any commercial ship with these three WAPS options installed with only the main particulars and general dimensions as input data.</p><p>The hull and WAPS models predict the forces and moments, which the program balances in 6 degrees of freedom to predict the theoretical sailing performance of the wind-assisted cargo ship with the specified characteristics for various wind conditions. The model is able to play with different optimization objectives. This includes maximizing sailing speed if a VPP is run or maximizing total power savings if it is a PPP. The program is based on semi-empirical methods and a WAPS aerodynamic database created from published data on lift and drag coefficients. All WAPS data can be interpolated with the aim to scale to different sizes and configurations such as number of units and different aspect ratios.</p><p>A model validation is carried out to evaluate its reliability. The model results are compared with the real sailing data of the Long Range 2 (LR2) class tanker vessel, the Maersk Pelican, which was recently fitted with two 30 meter high Rotor Sails; and results from another performance prediction program. In general, the two performance prediction programs and some of the real sailing measurements show good agreement. However, for some downwind sailing conditions, the performance predictions are more conservative than the measured values.</p><p>Results showing and comparing power savings, thrust and side force coefficients for the different WAPS are also presented and discussed. The results of this Master’s Thesis project show how Wind-Assisted Propulsion Systems have high potential in playing a key role in the decarbonization of the shipping sector. WAPS can prove substantial power, fuel, cost, and emissions savings. Tankers and bulk-carriers are specially suitable for wind propulsion thanks to their available deck space and relatively low design speeds.</p><p>The Performance Prediction Program for wind-assisted cargo ships developed in this Master’s Thesis shows promising results with a good level of accuracy despite its generic and small number of input data. It can be a useful tool in early project stages to quickly and accurately assess the potential and performance of WAPS systems.</p>
----------------------------------------------------------------------
In diva2:1083484 abstract is:
<p>Cable-stayed bridges have become very popular over the last ve decades due totheir aesthetic appeal, structural eciency, the limited amount of material usageand nancial benets. The rapid increase of new techniques creating longer spans,slender decks and more spectacular design has given rise to a major concern ofthe dynamic behavior of cable-stayed bridges. This has resulted in a more carefulmodelling procedure that will represent the reality in the most particular way. Amodel is simply an approximation of the reality, thus it is important to establishwhat simplications and approximations that are reasonable to make in order forthe model to be as accurate as possible.The Millau Viaduct is a cable-stayed bridge unique of its kind. At the time thatit was built it was breaking many records: span length, height of deck above thefoundations and the short construction time in just three years. Due to the slendernessof the structure, the extreme height and the location in a deep valley, theviaduct is naturally subjected to external loads. This thesis attempts to describea performed dynamic nonlinear analysis of two models of the Millau Viaduct usingthe FEA packages SAP2000 and BRIGADE/Plus. The models have been renedin order to be compared between the programs and to the reality i.e. the measuredmode shapes and frequencies obtained from reports.The viaduct required many specically designed solutions in order to obtain theelegance and the aesthetic appeal. Approximations in geometry has been essentialdue to the many details that the viaduct consists of, but the details are nonethelessimportant to capture to get the structural mechanics correct. The support conditionshas been considered as important as these were designed to allow for movementthat were caused by a combination of the external loads and the slenderness of thestructure. The most critical support conditions were the deck-pier connection inwhich the piers are split into two columns equipped with spherical bearings allowingfor angular rotation. The two shafts were modelled by one single column and thespherical bearings were simulated by creating two alternative models; one assignedwith a pinned constraint to allow for the angular rotation and the second, since thissupport condition is in fact rigid has been assigned as xed.The SAP and BRIGADE models showed to be consistent with each other, thoughthe beam theories, Euler-Bernoulli were applied to the SAP model and Timoshenkoin BRIGADE. The alternative models with the dierent constraints generated fairresults yet diers signicantly from each other. Alternative approaches towards themodelling have been addressed in the conclusions.</p>


corrected abstract:
<p>Cable-stayed bridges have become very popular over the last five decades due to their aesthetic appeal, structural efficiency, the limited amount of material usage and financial benefits. The rapid increase of new techniques creating longer spans, slender decks and more spectacular design has given rise to a major concern of the dynamic behavior of cable-stayed bridges. This has resulted in a more careful modelling procedure that will represent the reality in the most particular way. A model is simply an approximation of the reality, thus it is important to establish what simplifications and approximations that are reasonable to make in order for the model to be as accurate as possible.</p><p>The Millau Viaduct is a cable-stayed bridge unique of its kind. At the time that it was built it was breaking many records: span length, height of deck above the foundations and the short construction time in just three years. Due to the slenderness of the structure, the extreme height and the location in a deep valley, the viaduct is naturally subjected to external loads. This thesis attempts to describe a performed dynamic nonlinear analysis of two models of the Millau Viaduct using the FEA packages SAP2000 and BRIGADE/Plus. The models have been refined in order to be compared between the programs and to the reality i.e. the measured mode shapes and frequencies obtained from reports.</p><p>The viaduct required many specifically designed solutions in order to obtain the elegance and the aesthetic appeal. Approximations in geometry has been essential due to the many details that the viaduct consists of, but the details are nonetheless important to capture to get the structural mechanics correct. The support conditions has been considered as important as these were designed to allow for movement that were caused by a combination of the external loads and the slenderness of the structure. The most critical support conditions were the deck-pier connection in which the piers are split into two columns equipped with spherical bearings allowing for angular rotation. The two shafts were modelled by one single column and the spherical bearings were simulated by creating two alternative models; one assigned with a pinned constraint to allow for the angular rotation and the second, since this support condition is in fact rigid has been assigned as fixed.</p><p>The SAP and BRIGADE models showed to be consistent with each other, though the beam theories, Euler-Bernoulli were applied to the SAP model and Timoshenko in BRIGADE. The alternative models with the different constraints generated fair results yet differs significantly from each other. Alternative approaches towards the modelling have been addressed in the conclusions.</p>
----------------------------------------------------------------------
In diva2:876188 abstract is:
<p>Every computational fluid dynamics engineer deals with a never ending story – limitedcomputer resources. In computational fluid dynamics there is practically never enoughcomputer power. Limited computer resources lead to long calculation times which result inhigh costs and one of the main reasons is that large quantity of elements are needed in acomputational mesh in order to obtain accurate and reliable results.Although there exist established meshing approaches for the Siemens 4th generation DLEburner, mesh dependency has not been fully evaluated yet. The main goal of this work istherefore to better optimize accuracy versus cell count for this particular burner intended forsimulation of air/gas mixing where eddy-viscosity based turbulence models are employed.Ansys Fluent solver was used for all simulations in this work. For time effectivisationpurposes a 30° sector model of the burner was created and validated for the meshconvergence study. No steady state solutions were found for this case therefore timedependent simulations with time statistics sampling were employed. The mesh convergencestudy has shown that a coarse computational mesh in air casing of the burner does not affectflow conditions downstream where air/gas mixing process is taking place and that a majorpart of the combustion chamber is highly mesh independent. A large reduction of cell count inthose two parts is therefore allowed. On the other hand the RPL (Rich Pilot Lean) and thepilot burner turned out to be highly mesh density dependent. The RPL and the Pilot burnerneed to have significantly more refined mesh as it has been used so far with the establishedmeshing approaches. The mesh optimization has finally shown that at least as accurate resultsof air/gas mixing results may be obtained with 3x smaller cell count. Furthermore it has beenshown that significantly more accurate results may be obtained with 60% smaller cell count aswith the established meshing approaches.A short mesh study of the Siemens 3rd generation DLE burner in ignition stage of operationwas also performed in this work. This brief study has shown that the established meshingapproach for air/gas mixing purposes is sufficient for use with Ansys Fluent solver whilecertain differences were discovered when comparing the results obtained with Ansys Fluentagainst those obtained with Ansys CFX solver. Differences between Fluent and CFX solverwere briefly discussed in this work as identical simulation set up in both solvers producedslightly different results. Furthermore the obtained results suggest that Fluent solver is lessmesh dependent as CFX solver for this particular case.</p>


corrected abstract:
<p>Every computational fluid dynamics engineer deals with a never ending story – limited computer resources. In computational fluid dynamics there is practically never enough computer power. Limited computer resources lead to long calculation times which result in high costs and one of the main reasons is that large quantity of elements are needed in a computational mesh in order to obtain accurate and reliable results.</p><p>Although there exist established meshing approaches for the Siemens 4<sup>th</sup> generation DLE burner, mesh dependency has not been fully evaluated yet. The main goal of this work is therefore to better optimize accuracy versus cell count for this particular burner intended for simulation of air/gas mixing where eddy-viscosity based turbulence models are employed. Ansys Fluent solver was used for all simulations in this work. For time effectivisation purposes a 30° sector model of the burner was created and validated for the mesh convergence study. No steady state solutions were found for this case therefore time dependent simulations with time statistics sampling were employed. The mesh convergence study has shown that a coarse computational mesh in air casing of the burner does not affect flow conditions downstream where air/gas mixing process is taking place and that a major part of the combustion chamber is highly mesh independent. A large reduction of cell count in those two parts is therefore allowed. On the other hand the RPL (Rich Pilot Lean) and the pilot burner turned out to be highly mesh density dependent. The RPL and the Pilot burner need to have significantly more refined mesh as it has been used so far with the established meshing approaches. The mesh optimization has finally shown that at least as accurate results of air/gas mixing results may be obtained with 3x smaller cell count. Furthermore it has been shown that significantly more accurate results may be obtained with 60% smaller cell count as with the established meshing approaches.</p><p>A short mesh study of the Siemens 3<sup>rd</sup> generation DLE burner in ignition stage of operation was also performed in this work. This brief study has shown that the established meshing approach for air/gas mixing purposes is sufficient for use with Ansys Fluent solver while certain differences were discovered when comparing the results obtained with Ansys Fluent against those obtained with Ansys CFX solver. Differences between Fluent and CFX solver were briefly discussed in this work as identical simulation set up in both solvers produced slightly different results. Furthermore the obtained results suggest that Fluent solver is less mesh dependent as CFX solver for this particular case.</p>
----------------------------------------------------------------------
In diva2:854573 abstract is:
<p>Populations in big cities keep a constant inflation. It is estimated that 60% of thepopulation will move into a big city in the next 20 years, regarding this reason there arehigh demands for new solutions to the modern transportation system. A means ofmeasure that a lot of industrialized countries have implemented are high speed railwaytrains. The railway train covers the transportation needs. What would happen if amaglev train would be implemented instead?</p><p>The purpose with this report is to get an estimate if maglev trains can be a superiorsolution than the conventional railway train. In order to proceed this task, an analyszeswith respect to the Shinkansen N700A and ICE3 within railway as well as TransrapidTR09 and SCMaglev MLX01 within Maglev train was carried out. Aspects such asSafety, energy consumption, environmental impact and cost were the four models thatwere investigated.</p><p>Concerning safety, you could establish that both systems keep a high security standard,both systems have been involved in accidents, which is in a way a positive trait, keepsthe companies developing and improving the security measures for their trains, keepingthem safer. There is no guarantee that collisions or derailing occurs. Regarding energyconsumption, the results were that TR09 and ICE3 consume the same amount of energyat maximum allowed speed as well as Shinkansen N700A consumes considerably lessenergy than MLX01. Considering that energy consumption is proportional with thecarbon dioxide emissions results that N700A contributes to less carbon dioxideemissions than MLX01 as well as the TR09 and ICE3 contributed equally.</p><p>None of this aspects were decisive to demonstration if one system was moreadvantageous than the other, but how does the cost portion differentiate with thedifferent train systems? The result shows that the infrastructure cost for the maglev trainwere extremely high compared to the railway train. Germany (pioneers within themaglev technology) have shut down there maglev projects. Difference regardingoperation cost were not significant for the Transrapid and the Intercity-Express trains.Observing the discovered information on vehicle cost, a complete vehicle cost analyzescould not be established due to the lack of information on the vehicle cost on SCMaglevML0X1. With respect to the other three high speed trains, the Shinkasen N700A was themost expensive with a vehicle cost on $44 millions per unit, followed by the Transrapidwith a vehicle cost on $12.9 millions and cheapest train is the ICE3, costing $3.7millions per vehicle.</p><p>Keeping in thought all gathered information and data, the conclusion drawn is that it isnot profitable to construct maglev train with modern technology. If technical aspectsimproves further so that specifically the infrastructure cost decreases considerably thena maglev system could be a worthy solution for the foreseeable future.</p>

corrected abstract:
<p>Populations in big cities keep a constant inflation. It is estimated that 60% of the population will move into a big city in the next 20 years, regarding this reason there are high demands for new solutions to the modern transportation system. A means of measure that a lot of industrialized countries have implemented are high speed railway trains. The railway train covers the transportation needs. What would happen if a maglev train would be implemented instead?</p><p>The purpose with this report is to get an estimate if maglev trains can be a superior solution than the conventional railway train. In order to proceed this task, an analyszes with respect to the Shinkansen N700A and ICE3 within railway as well as Transrapid TR09 and SCMaglev MLX01 within Maglev train was carried out. Aspects such as Safety, energy consumption, environmental impact and cost were the four models that were investigated.</p><p>Concerning safety, you could establish that both systems keep a high security standard, both systems have been involved in accidents, which is in a way a positive trait, keeps the companies developing and improving the security measures for their trains, keeping them safer. There is no guarantee that collisions or derailing occurs. Regarding energy consumption, the results were that TR09 and ICE3 consume the same amount of energy at maximum allowed speed as well as Shinkansen N700A consumes considerably less energy than MLX01. Considering that energy consumption is proportional with the carbon dioxide emissions results that N700A contributes to less carbon dioxide emissions than MLX01 as well as the TR09 and ICE3 contributed equally.</p><p>None of this aspects were decisive to demonstration if one system was more advantageous than the other, but how does the cost portion differentiate with the different train systems? The result shows that the infrastructure cost for the maglev train were extremely high compared to the railway train. Germany (pioneers within the maglev technology) have shut down there maglev projects. Difference regarding operation cost were not significant for the Transrapid and the Intercity-Express trains. Observing the discovered information on vehicle cost, a complete vehicle cost analyzes could not be established due to the lack of information on the vehicle cost on SCMaglev ML0X1. With respect to the other three high speed trains, the Shinkasen N700A was the most expensive with a vehicle cost on $44 millions per unit, followed by the Transrapid with a vehicle cost on $12.9 millions and cheapest train is the ICE3, costing $3.7 millions per vehicle.</p><p>Keeping in thought all gathered information and data, the conclusion drawn is that it is not profitable to construct maglev train with modern technology. If technical aspects improves further so that specifically the infrastructure cost decreases considerably then a maglev system could be a worthy solution for the foreseeable future.</p>
----------------------------------------------------------------------
In diva2:1741184 abstract is:
<p>Accurate estimations of the reverberation time of furnished office spacesis essential as an acoustic consultant. A time efficient way to predict thefurniture’s effect is to investigate it by software modelling. The room acousticsoftware Odeon is suitable for doing this. In today’s geometrical room acousticmodellers (such as Odeon) a parameter called the scattering factor was usedwhich was of large importance in this thesis. This thesis set out to investigatehow well Odeon predicts the reverberation time in smaller and larger officespaces given that the correct scattering factor for each type of furniturecould be established. The method for the investigation was to perform fieldmeasurements in two office spaces of different sizes and geometry. Thenuse different setups of furniture to examine the furniture’s effect on thereverberation time connected to their sound scattering properties. The modelwas designed with 2D objects in SketchUp and exported to Odeon. A referencevalue for a pair of measurement setups was obtained by using the ratio of thetotal reverberation time (octave bands 125 Hz to 4 kHz) of the rooms. Thisratio was used as a target in Odeon for the same simulated pair of room setups.The scattering factor was adjusted in increments of the specific furniture usedin the setup until an optimized fit was reached. These steps were carried outfor each combination of setups. Afterwards the simulations were comparedto the measured and calculated reverberation times using Sabine’s formulaand Arau-Puchades formula. It was possible to establish specific scatteringfactors for the furniture types within Odeon although their accuracy was hardto determine. The resulting reverberation times from the Odeon simulationsof the large and small spaces were not closer to the measured reverberationtime than the calculated ones to any distinct degree. It’s worth noting that allof these calculations are dependant on ideally diffuse circumstances which theactual rooms rarely are. This is why they tend to underestimate the room’sreverberation time. The main goal of this thesis is to an extent fulfilled,although maybe not with as great future utility as hoped. The scattering factorscorrespond to each other independently of what room was being modelled. Inan isolated framework of Odeon, the attained scattering factors may be of use,especially if the measured room can be assumed ideally diffuse.</p>

corrected abstract:
o<p>Accurate estimations of the reverberation time of furnished office spaces is essential as an acoustic consultant. A time efficient way to predict the furniture’s effect is to investigate it by software modelling. The room acoustic software Odeon is suitable for doing this. In today’s geometrical room acoustic modellers (such as Odeon) a parameter called the scattering factor was used which was of large importance in this thesis. This thesis set out to investigate how well Odeon predicts the reverberation time in smaller and larger office spaces given that the correct scattering factor for each type of furniture could be established. The method for the investigation was to perform field measurements in two office spaces of different sizes and geometry. Then use different setups of furniture to examine the furniture’s effect on the reverberation time connected to their sound scattering properties. The model was designed with 2D objects in SketchUp and exported to Odeon. A reference value for a pair of measurement setups was obtained by using the ratio of the total reverberation time (octave bands 125 Hz to 4 kHz) of the rooms. This ratio was used as a target in Odeon for the same simulated pair of room setups. The scattering factor was adjusted in increments of the specific furniture used in the setup until an optimized fit was reached. These steps were carried out for each combination of setups. Afterwards the simulations were compared to the measured and calculated reverberation times using Sabine’s formula and Arau-Puchades formula. It was possible to establish specific scattering factors for the furniture types within Odeon although their accuracy was hard to determine. The resulting reverberation times from the Odeon simulations of the large and small spaces were not closer to the measured reverberation time than the calculated ones to any distinct degree. It’s worth noting that all of these calculations are dependant on ideally diffuse circumstances which the actual rooms rarely are. This is why they tend to underestimate the room’s reverberation time. The main goal of this thesis is to an extent fulfilled, although maybe not with as great future utility as hoped. The scattering factors correspond to each other independently of what room was being modelled. In an isolated framework of Odeon, the attained scattering factors may be of use, especially if the measured room can be assumed ideally diffuse.</p>
----------------------------------------------------------------------
In diva2:1644922 abstract is:
<p>This thesis studies the feasibility of integrating the novelStructural Battery (SB)[1] into the airframe of a UnmannedAerial Vehicle (UAV). The potential advantages in terms ofmass, range and endurance are studied.The aircraft performance is analysed using conventionalflight mechanics, modelled in Matlab and Xfoil. The structureis designed and analysed using composite laminate theoryand beam theory in conjunction with verification in AnsysMechanical. An iterative procedure was used to arriveat a design that satisfied the set structural- and flight requirements.The currently demonstrated structural battery has a specificenergy density of 23.8Wh/kg, an elastic modulus of25GPa and tensile strength of at least 300MPa.[1]The laminae properties used in this master thesis were estimatedusing the Reuss and Voigt model combined with theRule of Mixtures (RoM). A quasi isotropic SB laminate wasmodelled according to the previous structural requirementsand assumed material properties. It yielded an elastic modulusof 54GPa. In order to simplify the analysis the energyand stiffness were decoupled. The SB was assigned a specificenergy of 23.8Wh/kg and 60.6Wh/kg according to thevalues measured and estimated previously[1].A SB with a tensile modulus of 54GPa and specific energyof 24Wh/kg was shown not to be beneficial to integrate intothe primary aircraft structure. The designed SB yieldeda reduction in flight range of 5.8%. This was shown bycomparing the designed SB with a reference aircraft configuration.The reference configuration uses a conventionalbattery that has a specific energy density of 160Wh/kg andconventional Carbon Fibre Composite (CFC) with an elasticmodulus of 71GPa.It was shown that the integration of the SB modelled wouldbecome beneficial compared to the reference aircraft configurationwhen the SB specific energy exceeds 33Wh/kg.The integration of a structural battery with a specific energyof 60.6Wh/kg yielded a flight range improvement of16.9% compared to the reference aircraft.</p>

corrected abstract:
<p>This thesis studies the feasibility of integrating the novel Structural Battery (SB)[1] into the airframe of a Unmanned Aerial Vehicle (UAV). The potential advantages in terms of mass, range and endurance are studied. The aircraft performance is analysed using conventional flight mechanics, modelled in Matlab and Xfoil. The structure is designed and analysed using composite laminate theory and beam theory in conjunction with verification in Ansys Mechanical. An iterative procedure was used to arrive at a design that satisfied the set structural- and flight requirements.</p><p>The currently demonstrated structural battery has a specific energy density of 23.8 Wh/kg, an elastic modulus of 25GPa and tensile strength of at least 300 MPa.[1]</p><p>The laminae properties used in this master thesis were estimated using the Reuss and Voigt model combined with the Rule of Mixtures (RoM). A quasi isotropic SB laminate was modelled according to the previous structural requirements and assumed material properties. It yielded an elastic modulus of 54 GPa. In order to simplify the analysis the energy and stiffness were decoupled. The SB was assigned a specific energy of 23.8 Wh/kg and 60.6 Wh/kg according to the values measured and estimated previously[1].</p><p>A SB with a tensile modulus of 54 GPa and specific energy of 24 Wh/kg was shown not to be beneficial to integrate into the primary aircraft structure. The designed SB yielded a reduction in flight range of 5.8%. This was shown by comparing the designed SB with a reference aircraft configuration. The reference configuration uses a conventional battery that has a specific energy density of 160 Wh/kg and conventional Carbon Fibre Composite (CFC) with an elastic modulus of 71 GPa.i</p><p>It was shown that the integration of the SB modelled would become beneficial compared to the reference aircraft configuration when the SB specific energy exceeds 33 Wh/kg. The integration of a structural battery with a specific energy of 60.6 Wh/kg yielded a flight range improvement of 16.9% compared to the reference aircraft.</p>
----------------------------------------------------------------------
In diva2:1541213 abstract is:
<p>The objectives of the present project were to set up, optimise and characterise a digitalholographic microscopy (DHM) laboratory set-up designed for the study of eyetissue and to implement and optimise digital data processing and noise reductionroutines. This work is part of a collaborative project aiming to provide quantitativemethods for the in vitro and in vivo characterisation of human corneal transparency.The laboratory set-up is based on a commercial laboratory microscope with zoomfunction (a “macroscope”). In continuation of previous work, we completed and optimised,and extended a software for holographic signal processing and numericalpropagation of the wavefront.To characterise the set-up and quantify its performances for standard operationand in its DHM configuration, we compare the magnification and resolution to theoreticalvalues for a given set of parameters. We determined the magnification factorand the rotation angle between the object and camera planes. With a laser wavelengthof 532 nm, a x1 objective and a zoom setting of x2.9 (which corresponds to aplane sample wavefront), we measured a magnification of 1.68. With the same parameters,we measure a holographic resolution of about 11 m. The wavefront phasecould be determined with a precision of a fraction of the wavelength.We subsequently performed analysis of the relative contribution of coherent noiseand implemented and evaluated several noise reduction routines. While the impactof coherent noise remained visible in the amplitude image, interferometric precisionwas obtained for the phase of the wavefront and the set-up was considered qualifiedfor its intended use for corneal characterisation.A first test measurement was performed on primate cornea.Subsequent work will address the further quantitative characterisation of the setupfor the full set of parameters (objectives, zoom positions, wavelengths), test measurementson samples with known transmission and light scattering properties (e.g.solutions of PMMA beads) and the comparison of the results with the predictions ofa theoretical model, and measurements on animal and human tissue.</p>


corrected abstract:
<p>The objectives of the present project were to set up, optimise and characterise a digital holographic microscopy (DHM) laboratory set-up designed for the study of eye tissue and to implement and optimise digital data processing and noise reduction routines. This work is part of a collaborative project aiming to provide quantitative methods for the in vitro and in vivo characterisation of human corneal transparency.</p><p>The laboratory set-up is based on a commercial laboratory microscope with zoom function (a “macroscope”). In continuation of previous work, we completed and optimised, and extended a software for holographic signal processing and numerical propagation of the wavefront.</p><p>To characterise the set-up and quantify its performances for standard operation and in its DHM configuration, we compare the magnification and resolution to theoretical values for a given set of parameters. We determined the magnification factor and the rotation angle between the object and camera planes. With a laser wavelength of 532 nm, a x1 objective and a zoom setting of x2.9 (which corresponds to a plane sample wavefront), we measured a magnification of 1.68. With the same parameters, we measure a holographic resolution of about 11 µm. The wavefront phase could be determined with a precision of a fraction of the wavelength.</p><p>We subsequently performed analysis of the relative contribution of coherent noise and implemented and evaluated several noise reduction routines. While the impact of coherent noise remained visible in the amplitude image, interferometric precision was obtained for the phase of the wavefront and the set-up was considered qualified for its intended use for corneal characterisation.</p><p>A first test measurement was performed on primate cornea.</p><p>Subsequent work will address the further quantitative characterisation of the setup for the full set of parameters (objectives, zoom positions, wavelengths), test measurements on samples with known transmission and light scattering properties (e.g. solutions of PMMA beads) and the comparison of the results with the predictions of a theoretical model, and measurements on animal and human tissue.</p>
----------------------------------------------------------------------
In diva2:1334020 abstract is:
<p>In order to keep up with the increasing demand of fuel-efficiency in the transportationindustry, the interest of making the vehicles as lightweight as possible is steadilyincreasing. One of the ways of reducing the weight is to introduce an anisotropicmaterial as Short Fibre Reinforced Polymers (SFRP) as a replacement for structuralparts made out of metals. To meet the modern vehicle design process which strivestowards a more simulation driven workflow, the need for accurate simulations offibre reinforced composites is of importance.This thesis aims to evaluate and find a working process for fatigue analysis of injectionmoulded SFRP components. To evaluate the fatigue analysis procedure anexisting SFRP component has been studied. The component is the front bracket thatmounts the roof air deflector to the roof on Scania trucks. To correlate the fatigue lifeestimation from the fatigue analysis, experiments were performed at ÅF Test Centerin Borlänge.The anisotropic behaviour is modelled using the commercial software Digimat togetherwith an injection simulation provided by Scania, to estimate the fibre orientationand thereby the material behaviour of the SFRP component. The fatigue analysiswas conducted by performing a coupled structural analysis between Digimat-Abaqus and then import the resulting stress- and strain-fields into the fatigue postprocessornCode DesignLife. The stress is then cyclic tested towards experimentallydetermined S-N curves determined in Digimat.Due to restriction of available fatigue data for the plastic in the front bracket, a fatiguematerial model for a plastic containing the same fibres and matrix but witha different fibre amount was implemented. The fatigue data were scaled using theUTS method to get a good characterisation of the real-life material behaviour of theplastic of the front bracket component.From the correlation between the fatigue analysis and performed experiments, itwas shown that the simulated fatigue life was conservative compared to the fatiguelife determined from the experiments. However, the correlation between the fatigueanalysis and experiments is not fully captured but gives a better estimation of thefatigue life compared to performing the fatigue analysis using an isotropic materialmodel.</p>

corrected abstract:
<p>In order to keep up with the increasing demand of fuel-efficiency in the transportation industry, the interest of making the vehicles as lightweight as possible is steadily increasing. One of the ways of reducing the weight is to introduce an anisotropic material as Short Fibre Reinforced Polymers (SFRP) as a replacement for structural parts made out of metals. To meet the modern vehicle design process which strives towards a more simulation driven workflow, the need for accurate simulations of fibre reinforced composites is of importance.</p><p>This thesis aims to evaluate and find a working process for fatigue analysis of injection moulded SFRP components. To evaluate the fatigue analysis procedure an existing SFRP component has been studied. The component is the front bracket that mounts the roof air deflector to the roof on Scania trucks. To correlate the fatigue life estimation from the fatigue analysis, experiments were performed at ÅF Test Center in Borlänge.</p><p>The anisotropic behaviour is modelled using the commercial software Digimat together with an injection simulation provided by Scania, to estimate the fibre orientation and thereby the material behaviour of the SFRP component. The fatigue analysis was conducted by performing a coupled structural analysis between Digimat-Abaqus and then import the resulting stress- and strain-fields into the fatigue post-processor nCode DesignLife. The stress is then cyclic tested towards experimentally determined S-N curves determined in Digimat.</p><p>Due to restriction of available fatigue data for the plastic in the front bracket, a fatigue material model for a plastic containing the same fibres and matrix but with a different fibre amount was implemented. The fatigue data were scaled using the UTS method to get a good characterisation of the real-life material behaviour of the plastic of the front bracket component.</p><p>From the correlation between the fatigue analysis and performed experiments, it was shown that the simulated fatigue life was conservative compared to the fatigue life determined from the experiments. However, the correlation between the fatigue analysis and experiments is not fully captured but gives a better estimation of the fatigue life compared to performing the fatigue analysis using an isotropic material model.</p>
----------------------------------------------------------------------
In diva2:1110758 abstract is:
<p>In the frame of the improvement of the performances for Ariane 5, an analysis iscarried out to explain the pressure drop observed in the ascent phase of some flights inthe liquid hydrogen (LH2) tank of the upper stage. This stage is mainly idle until therocket is out of the atmosphere but is submitted to important excitation throughoutthe ascent phase in the atmosphere. Due to excitation, the liquid contained in thetank moves and breaks the thermodynamic equilibrium. This movement, sloshing isidentified as the most likely cause of the pressure drop observed. It is investigated inthis thesis to understand how exactly it impacts the thermodynamic equilibrium inthe tank.The pressure drop called creux PGRH can be explained by the mixing of the topof the liquid with the liquid bulk, colder than the top, when the liquid is sloshing.This movement changes the saturation conditions and yields pressure and temperatureevolutions in the ullage volume of the tank. Observations on Ariane 5 flightsshowed that the first asymmetric mode was mainly excited during this first phase ofascension. Simple models such as the pendulum model are used to simulate the dynamicbehaviour of this mode. Its stability is also investigated through lateral andlongitudinal excitations.The thermodynamics of the system in the tank can be modelled by a one-dimensionalmodel. Based on an experiment with liquid nitrogen in a cylindrical tank, the heatfluxes are calculated and plugged in the model. The pressurisation phase is first simulatedthrough self and active pressurisation to estimate the importance of the thicknessof the thermal boundary layer. Sloshing is included in the model thermodynamicallyby considering a more important conductive coefficient in the sloshing layer. The amplitudeof sloshing can be linked to the new conduction term thanks to a literaturerelation but it underestimates the actual magnitude of the pressure drop. The modelis extended to a two-dimensional model to take into account the sloshing mechanically,knowing the velocities from the pendulum model. It is found to be not accurate mostlydue to the turbulence of the sloshing layer not considered in the model.The models give in any case important results regarding the influence of some tankparameters such as the ullage volume, the importance of the pressurisation phase anda necessary distinction between chaotic and stable sloshing. From this, the data ofAriane 5 flights is analysed. The flights are divided in three families according to themagnitude of the pressure drop measured. A fault tree analysis is performed to ruleout possible influences and put forward a theory on the creux eventually.</p>

corrected abstract:
<p>In the frame of the improvement of the performances for Ariane 5, an analysis is carried out to explain the pressure drop observed in the ascent phase of some flights in the liquid hydrogen (LH<sub>2</sub>) tank of the upper stage. This stage is mainly idle until the rocket is out of the atmosphere but is submitted to important excitation throughout the ascent phase in the atmosphere. Due to excitation, the liquid contained in the tank moves and breaks the thermodynamic equilibrium. This movement, sloshing is identified as the most likely cause of the pressure drop observed. It is investigated in this thesis to understand how exactly it impacts the thermodynamic equilibrium in the tank.</p><p>The pressure drop called <em>creux PGRH</em> can be explained by the mixing of the top of the liquid with the liquid bulk, colder than the top, when the liquid is sloshing. This movement changes the saturation conditions and yields pressure and temperature evolutions in the ullage volume of the tank. Observations on Ariane 5 flights showed that the first asymmetric mode was mainly excited during this first phase of ascension. Simple models such as the pendulum model are used to simulate the dynamic behaviour of this mode. Its stability is also investigated through lateral and longitudinal excitations.</p><p>The thermodynamics of the system in the tank can be modelled by a one-dimensional model. Based on an experiment with liquid nitrogen in a cylindrical tank, the heat fluxes are calculated and plugged in the model. The pressurisation phase is first simulated through self and active pressurisation to estimate the importance of the thickness of the thermal boundary layer. Sloshing is included in the model thermodynamically by considering a more important conductive coefficient in the sloshing layer. The amplitude of sloshing can be linked to the new conduction term thanks to a literature relation but it underestimates the actual magnitude of the pressure drop. The model is extended to a two-dimensional model to take into account the sloshing mechanically, knowing the velocities from the pendulum model. It is found to be not accurate mostly due to the turbulence of the sloshing layer not considered in the model.</p><p>The models give in any case important results regarding the influence of some tank parameters such as the ullage volume, the importance of the pressurisation phase and a necessary distinction between chaotic and stable sloshing. From this, the data of Ariane 5 flights is analysed. The flights are divided in three families according to the magnitude of the pressure drop measured. A fault tree analysis is performed to rule out possible influences and put forward a theory on the creux eventually.</p>
----------------------------------------------------------------------
In diva2:783982 - spaces missing in title:
"Evaluating the effectiveness of collisionavoidance functions using state-of-the-artsimulation tools for vehicle dynamics"
==>
"Evaluating the effectiveness of collision avoidance functions using state-of-the-art simulation tools for vehicle dynamics"

abstract is:
<p>The main goal of this work is to gain knowledge of how and to what extent state-of-the-artsimulation tools can be used in a conceptual development phase for vehicle dynamics control atVolvo Car Corporation (VCC).The first part of the thesis deals with an evaluation of vehicle dynamics simulation tools and theiruses. The three simulation tools selected for the study, namely Mechanical Simulation CarSim 8.2.1,IPG CarMaker 4.0.5, and VI-Grade CarRealTime V14, are briefly described and discussed. In order toevaluate and compare these tools with respect to application for vehicle dynamics control, a criterialist is developed covering aspects such as tool requirements and intended usage. Based on thecriteria list and certain identified drawbacks, a ranking of the tools is made possible. Furthermore,the process of developing vehicle models for the different tools is discussed in detail, along with theprocedure of validating the vehicle models.In the second part, the concept of Collision Avoidance Driver Assistance (CADA) function isintroduced and possible approaches for developing CADA functions are discussed in brief. It isimportant to note that the CADA functions in this work are based on cornering the vehicle i.e.maneuvering around the threat, rather than solely reducing vehicle speed. A number ofimplementations of the functions are developed in Simulink. A frequency analysis of a simplifiedlinear vehicle model is performed to investigate the influence of steering, differential braking, andtheir combination on the resultant lateral displacement of the vehicle during an evasive maneuver.The developed CADA functions are then simulated using the vehicle simulation tools. Two specificmetrics - Lateral Displacement gain and DeltaX - are formulated to evaluate the effectiveness of theCADA functions. Based on these metrics, the assistance obtained due to the functions for a specificevasive maneuver is compared.From the evaluation process of the three tools, two were considered suitable for the purpose ofsimulating collision avoidance functions. The evaluation of the CADA functions demonstrates thatcombined assistive steering with differential braking provides considerable assistance in order toavoid collisions. The simulation results also present interesting trends which provide a usefuldirection regarding the conditions for intervention by such collision avoidance functions during anevasive maneuver. The use of simulation tools makes it possible to observe these trends and utilizethem in the development process of the functions.</p>

corrected abstract:
<p>The main goal of this work is to gain knowledge of how and to what extent state-of-the-art simulation tools can be used in a conceptual development phase for vehicle dynamics control at Volvo Car Corporation (VCC).</p><p>The first part of the thesis deals with an evaluation of vehicle dynamics simulation tools and their uses. The three simulation tools selected for the study, namely Mechanical Simulation CarSim 8.2.1, IPG CarMaker 4.0.5, and VI-Grade CarRealTime V14, are briefly described and discussed. In order to evaluate and compare these tools with respect to application for vehicle dynamics control, a criteria list is developed covering aspects such as tool requirements and intended usage. Based on the criteria list and certain identified drawbacks, a ranking of the tools is made possible. Furthermore, the process of developing vehicle models for the different tools is discussed in detail, along with the procedure of validating the vehicle models.</p><p>In the second part, the concept of Collision Avoidance Driver Assistance (CADA) function is introduced and possible approaches for developing CADA functions are discussed in brief. It is important to note that the CADA functions in this work are based on cornering the vehicle i.e. maneuvering around the threat, rather than solely reducing vehicle speed. A number of implementations of the functions are developed in Simulink. A frequency analysis of a simplified linear vehicle model is performed to investigate the influence of steering, differential braking, and their combination on the resultant lateral displacement of the vehicle during an evasive maneuver. The developed CADA functions are then simulated using the vehicle simulation tools. Two specific metrics - Lateral Displacement gain and DeltaX - are formulated to evaluate the effectiveness of the CADA functions. Based on these metrics, the assistance obtained due to the functions for a specific evasive maneuver is compared.</p><p>From the evaluation process of the three tools, two were considered suitable for the purpose of simulating collision avoidance functions. The evaluation of the CADA functions demonstrates that combined assistive steering with differential braking provides considerable assistance in order to avoid collisions. The simulation results also present interesting trends which provide a useful direction regarding the conditions for intervention by such collision avoidance functions during an evasive maneuver. The use of simulation tools makes it possible to observe these trends and utilize them in the development process of the functions.</p>
----------------------------------------------------------------------
In diva2:1527803 abstract is:
<p>Multi-body simulations are given more emphasis over physical tests owing toenvironmental, financial, and time requirements in the competitive automotive industry. Thus,it is imperative to develop models to accurately predict and analyse the system's behaviour.This thesis focuses on developing an air suspension model with Electronic Level Control thathas the ability to communicate with other air springs in a pneumatic circuit thus replicating thepneumatic connection in actual truck and regulate the ride height of the vehicle.To accomplish this, a comprehensive literature study is performed to identify an effectivecontrol variable to manipulate the air springs. This is done by understanding the working andthermodynamic principles of air suspension, understanding various Scania pneumaticconfigurations, and decrypting the working of the Electronic Level Control.Different methods for implementing the model through the identified control variable arediscussed. A brief explanation of the necessary physical tests performed to validate the modelis given. An extensive description of implementation of the static and dynamic model inADAMS through command batch script coding is provided.The developed static model is validated by comparing the results from simulations and the testdata. The axle weights have an error of maximum 6% and the pressure in the air springs havean error of maximum 9% which can be owed to neglection of hysteresis in the air springcharacteristics and using mean values to compare the data. The dynamic model is validated byaltering the ride height level and observing the response of the model. The results obtainedindicate the developed Electronic Level Control is able to regulate the ride height at the desiredlevel.The robustness of the model is validated by modifying the developed model for longitudinalpneumatic connection and for a truck with trailer model. The results indicate the developedmodel is capable to perform satisfactorily and conform to the Scania tolerance limits.Thus, an appropriate control variable for the air springs model is identified. Static and dynamicmodel to identify the suitable pressure in the air springs and thus, the force in the air springs isdeveloped which helped in drastically reducing the manual iterative work that was required.</p>

corrected abstract:
<p>Multi-body simulations are given more emphasis over physical tests owing to environmental, financial, and time requirements in the competitive automotive industry. Thus, it is imperative to develop models to accurately predict and analyse the system's behaviour.</p><p>This thesis focuses on developing an air suspension model with Electronic Level Control that has the ability to communicate with other air springs in a pneumatic circuit thus replicating the pneumatic connection in actual truck and regulate the ride height of the vehicle.</p><p>To accomplish this, a comprehensive literature study is performed to identify an effective control variable to manipulate the air springs. This is done by understanding the working and thermodynamic principles of air suspension, understanding various Scania pneumatic configurations, and decrypting the working of the Electronic Level Control.</p><p>Different methods for implementing the model through the identified control variable are discussed. A brief explanation of the necessary physical tests performed to validate the model is given. An extensive description of implementation of the static and dynamic model in ADAMS through command batch script coding is provided.</p><p>The developed static model is validated by comparing the results from simulations and the test data. The axle weights have an error of maximum 6% and the pressure in the air springs have an error of maximum 9% which can be owed to neglection of hysteresis in the air spring characteristics and using mean values to compare the data. The dynamic model is validated by altering the ride height level and observing the response of the model. The results obtained indicate the developed Electronic Level Control is able to regulate the ride height at the desired level.</p><p>The robustness of the model is validated by modifying the developed model for longitudinal pneumatic connection and for a truck with trailer model. The results indicate the developed model is capable to perform satisfactorily and conform to the Scania tolerance limits.</p><p>Thus, an appropriate control variable for the air springs model is identified. Static and dynamic model to identify the suitable pressure in the air springs and thus, the force in the air springs is developed which helped in drastically reducing the manual iterative work that was required.</p>
----------------------------------------------------------------------
In diva2:1465517 - missing space in the tttle:
"Multi-Objective Optimization of Torque Distribution inHybrid Vehicles"
==>
"Multi-Objective Optimization of Torque Distribution in Hybrid Vehicles"

abstract is:
<p>Electrification is one of the mega-trends in the transportation and automotive industry today. Boththe alarming environmental conditions and the ever decreasing fuel reserves are driving the shifttowards hybrid, all electric and alternative fuel source vehicles. This thesis work is another smallstep towards studying, addressing and handling this issue while also laying the groundwork for developingand moving towards more efficient and commercially viable vehicles.This thesis work aims at investigating the trade-off offered by optimal control techniques betweenenergy consumption and reference tracking for torque allocation to the various actuators available topropel a hybrid electric vehicle. The particular vehicle under consideration has two electric motorsat the rear wheels and an internal combustion engine along with an integrator starter generatordriving the front wheels. The torque allocation problem is originally solved by proposing a one stageoptimization strategy (OSOS) that takes into account actuator limits, losses, and objectives throughconstraints. The performance of this formulation is presented over two simulated test tracks on apareto front where the advantage on relaxing complete reference tracking becomes visible. Next,two new formulations each as a two stage optimization strategies (TSOS) are proposed, the mainobjective being to split the original formulation into two parts. One addressing energy optimalityand the other addressing reference tracking of total wheel torque and yaw moment request fulfilment.These formulations are then similarly investigated and presented in comparison with the originalformulation. In developing the formulations, an assumption about the loss models is made andthe problem size of the second stage quadratic program is significantly reduced. The problems areappropriately scaled and made mathematically robust to handle the constraints and inputs in theoperating range. As reference tracking for the vehicle is split into lateral and longitudinal torquerequests from the vehicle, this becomes a multi-objective optimization problem. To further studythe behaviour of these formulations, they are given constant inputs and simulated over a single timestep. The effect of changing hybridization level, i.e, the amount of electrical energy used comparedto fuel energy on the behaviour of these formulations is also explored. One of the effects of the twostage formulations was the confinement of solutions within a reasonable error for the majority ofchosen weights due to the energy considerations in the first stage. The proposed formulations wereable to generate results close but not equal to the original formulation on the pareto front. Anotherfinding was that due to the implementation of two actuators at the rear of the vehicle, a desired yawrate could be achieved at no additional energy cost because of regenerative and propulsive torquesgenerated respectively on either side of rear axle for torque vectoring. Furthermore with a dedicatedsolver, the TSOS could present an interesting alternative to enhance independent development invehicle dynamics control and energy management of the vehicle.</p>


corrected abstract:
<p>Electrification is one of the mega-trends in the transportation and automotive industry today. Both the alarming environmental conditions and the ever decreasing fuel reserves are driving the shift towards hybrid, all electric and alternative fuel source vehicles. This thesis work is another small step towards studying, addressing and handling this issue while also laying the groundwork for developing and moving towards more efficient and commercially viable vehicles.</p><p>This thesis work aims at investigating the trade-off offered by optimal control techniques between energy consumption and reference tracking for torque allocation to the various actuators available to propel a hybrid electric vehicle. The particular vehicle under consideration has two electric motors at the rear wheels and an internal combustion engine along with an integrator starter generator driving the front wheels. The torque allocation problem is originally solved by proposing a one stage optimization strategy (OSOS) that takes into account actuator limits, losses, and objectives through constraints. The performance of this formulation is presented over two simulated test tracks on a pareto front where the advantage on relaxing complete reference tracking becomes visible. Next, two new formulations each as a two stage optimization strategies (TSOS) are proposed, the main objective being to split the original formulation into two parts. One addressing energy optimality and the other addressing reference tracking of total wheel torque and yaw moment request fulfilment.</p><p>These formulations are then similarly investigated and presented in comparison with the original formulation. In developing the formulations, an assumption about the loss models is made and the problem size of the second stage quadratic program is significantly reduced. The problems are appropriately scaled and made mathematically robust to handle the constraints and inputs in the operating range. As reference tracking for the vehicle is split into lateral and longitudinal torque requests from the vehicle, this becomes a multi-objective optimization problem. To further study the behaviour of these formulations, they are given constant inputs and simulated over a single time step. The effect of changing hybridization level, i.e, the amount of electrical energy used compared to fuel energy on the behaviour of these formulations is also explored. One of the effects of the two stage formulations was the confinement of solutions within a reasonable error for the majority of chosen weights due to the energy considerations in the first stage. The proposed formulations were able to generate results close but not equal to the original formulation on the pareto front. Another finding was that due to the implementation of two actuators at the rear of the vehicle, a desired yaw rate could be achieved at no additional energy cost because of regenerative and propulsive torques generated respectively on either side of rear axle for torque vectoring. Furthermore with a dedicated solver, the TSOS could present an interesting alternative to enhance independent development in vehicle dynamics control and energy management of the vehicle.</p>
----------------------------------------------------------------------
In diva2:1327792 - note: no full text in DiVA

abstract is:
<p>Capacitive deionization is an emerging environmentally friendly technique for waterdesalination that has been getting increasing attention in recent years. In thistechnique, water passes through a cell with nanostructured porous carbon electrodeswhich have a high surface area. When a potential is applied to these electrodes, theelectrodes adsorb the salt ion in the water stream, which results in the production offresh water. While the technique is promising, it still needs to be developed further tosee more widespread use, and modeling can be an essential tool during investigationsto expedite these developments. To make modeling more accessible, it is crucial thatmodels are developed that can predict the process performance transparently andstraightforwardly. This master thesis encompasses three submitted papers. A model,termed the Dynamic Langmuir model, is developed based on a few fundamentalmacroscopic principles. The model is more straightforward and more transparent thanprevious models yet could accurately describe key concepts including ion adsorptionand charge efficiency, in both equilibrium and dynamic settings and for variouselectrode materials and cell structures. The model is shown to accurately predictperformance over crucial parameters including the applied voltage, flow rate of thewater through the cell, inlet concentration, mixtures of ions in the water, varyingelectrode asymmetry and electrode pre-charging. The usefulness of the model isfurther demonstrated by using it to optimize the time spent absorbing salt versuscleaning the cell. This improved ion-removal efficiency by 31 % compared to doing fullsaturation/regeneration. To further aid the goal of making modeling more accessible,a software program has been developed and provided as open source that can bedirectly used to implement the model, without requiring extensive knowledge aboutthe theory, or lots of experiments to set up. Also, an automated experimental setup forcontinuous and stable CDI operation was developed that could provide data for futuremodeling. Finally, this thesis additionally includes an extensive theory section to givea comprehensive introduction to the capacitive-deionization field. In conclusion, asimple and transparent model has been developed, able to accurately describe howcritical concepts in capacitive deionization vary over a wide range of operationalparameters. It is hoped that this work can make the modeling of capacitivedeionization more accessible.</p>

corrected abstract:
<p>Capacitive deionization is an emerging environmentally friendly technique for water desalination that has been getting increasing attention in recent years. In this technique, water passes through a cell with nanostructured porous carbon electrodes which have a high surface area. When a potential is applied to these electrodes, the electrodes adsorb the salt ion in the water stream, which results in the production of fresh water. While the technique is promising, it still needs to be developed further to see more widespread use, and modeling can be an essential tool during investigations to expedite these developments. To make modeling more accessible, it is crucial that models are developed that can predict the process performance transparently and straightforwardly. This master thesis encompasses three submitted papers. A model, termed the Dynamic Langmuir model, is developed based on a few fundamental macroscopic principles. The model is more straightforward and more transparent than previous models yet could accurately describe key concepts including ion adsorption and charge efficiency, in both equilibrium and dynamic settings and for various electrode materials and cell structures. The model is shown to accurately predict performance over crucial parameters including the applied voltage, flow rate of the water through the cell, inlet concentration, mixtures of ions in the water, varying electrode asymmetry and electrode pre-charging. The usefulness of the model is further demonstrated by using it to optimize the time spent absorbing salt versus cleaning the cell. This improved ion-removal efficiency by 31 % compared to doing fullsaturation/regeneration. To further aid the goal of making modeling more accessible, a software program has been developed and provided as open source that can be directly used to implement the model, without requiring extensive knowledge about the theory, or lots of experiments to set up. Also, an automated experimental setup for continuous and stable CDI operation was developed that could provide data for future modeling. Finally, this thesis additionally includes an extensive theory section to give a comprehensive introduction to the capacitive-deionization field. In conclusion, a simple and transparent model has been developed, able to accurately describe how critical concepts in capacitive deionization vary over a wide range of operational parameters. It is hoped that this work can make the modeling of capacitive deionization more accessible.</p>
----------------------------------------------------------------------
In diva2:412700 abstract is:
<p>This diploma thesis captures the three-dimensional implementation of noise-reducing high-liftsystems. A parametric CAD model is developed for the FNG aircraft and different high-lift configurationsare built up. In the course of research, these configurations are designed based onformerly obtained two-dimensional results of DLR’s LEISA project featuring the design of a verylong chord slat (VLCS), whose slat shape resulted in a favourable aeroacoustic behaviour at noiserelevantapproach conditions. The high-lift systems derived in this thesis differ in the spanwisevariation of the slat geometry planform as well as in the applied high-lift settings described bygap, overlap and deflection angle.The aerodynamic performance is computed via CFD RANS simulations and the results arecompared to a reference high-lift system of the FNG aircraft, which has been designed in previousstudies. The observed CFD results are further evaluated in the reference wing section of the FNGaircraft in order to display the agreement between the implemented 3D high-lift configurationsand the 2D LEISA reference data. Besides the aerodynamic performance, aeroacoustic aspects arealso considered in this diploma thesis. By means of the obtained CFD results, indirect statementsabout the success of the 3D low-noise implementation approach are made.The geometrical concordance of the derived reference wing section of the 3D CAD model isfound in general to be very high in comparison to the 2D-optimized LEISA design wing section.With regard to the observed pressure distributions of the initial four designed high-lift systemshowever, small geometry deviations are noticed to affect the obtained pressure distributions in asignificantly unintended way. The requirements of a low-noise high-lift system are thus not metfor these high-lift configurations. In the 3D implementation, the 2D-optimized slat settings haveto be modified in order to maintain the favourable aeroacoustic behaviour of the 2D considerations.Based on a reduced slat deflection angle, a further derived 3D VLCS high-lift system isobtained to match the 2D-optimized pressure distributions in the reference wing section more accurately.A significant pressure increase at the VLCS trailing edge is noticed for this configuration,which shows the noise-reducing potential of the derived VLCS device. However, the aerodynamicdegradations obtained for the designed low-noise high-lift system are found to be too high in orderto still provide improved aeroacoustic behaviour during conditions of increased approachspeed. A 3D noise-reducing high-lift system is therefore not achieved, although the 2D-optimizedLEISA pressure distributions are well captured in the reference wing section of the implemented3D high-lift system featuring modified high-lift setting parameters.</p>


corrected abstract:
<p>This diploma thesis captures the three-dimensional implementation of noise-reducing high-lift systems. A parametric CAD model is developed for the FNG aircraft and different high-lift configurations are built up. In the course of research, these configurations are designed based on formerly obtained two-dimensional results of DLR’s LEISA project featuring the design of a very long chord slat (VLCS), whose slat shape resulted in a favourable aeroacoustic behaviour at noise relevant approach conditions. The high-lift systems derived in this thesis differ in the spanwise variation of the slat geometry plan form as well as in the applied high-lift settings described by gap, overlap and deflection angle.</p><p>The aerodynamic performance is computed via CFD RANS simulations and the results are compared to a reference high-lift system of the FNG aircraft, which has been designed in previous studies. The observed CFD results are further evaluated in the reference wing section of the FNG aircraft in order to display the agreement between the implemented 3D high-lift configurations and the 2D LEISA reference data. Besides the aerodynamic performance, aeroacoustic aspects are also considered in this diploma thesis. By means of the obtained CFD results, indirect statements about the success of the 3D low-noise implementation approach are made.</p><p>The geometrical concordance of the derived reference wing section of the 3D CAD model is found in general to be very high in comparison to the 2D-optimized LEISA design wing section. With regard to the observed pressure distributions of the initial four designed high-lift systems however, small geometry deviations are noticed to affect the obtained pressure distributions in a significantly unintended way. The requirements of a low-noise high-lift system are thus not met for these high-lift configurations. In the 3D implementation, the 2D-optimized slat settings have to be modified in order to maintain the favourable aeroacoustic behaviour of the 2D considerations. Based on a reduced slat deflection angle, a further derived 3D VLCS high-lift system is obtained to match the 2D-optimized pressure distributions in the reference wing section more accurately. A significant pressure increase at the VLCS trailing edge is noticed for this configuration, which shows the noise-reducing potential of the derived VLCS device. However, the aerodynamic degradations obtained for the designed low-noise high-lift system are found to be too high in order to still provide improved aeroacoustic behaviour during conditions of increased approach speed. A 3D noise-reducing high-lift system is therefore not achieved, although the 2D-optimized LEISA pressure distributions are well captured in the reference wing section of the implemented 3D high-lift system featuring modified high-lift setting parameters.</p>
----------------------------------------------------------------------
In diva2:408831 abstract is:
<p>This thesis is about teachers who share knowledge through lektion.se. Lektion.se is anInternet site and started in 2003 by three teachers. It gives its members an opportunity toshare ideas for lessons, discuss interesting topics and give advice to other members. Thethesis focuses on how the teachers define and use lection.se for sharing knowledge and howthe site can be improved concerning knowledge sharing. To investigate this, seven interviewshave been conducted with teachers in and around Stockholm and a questionnaire has beenpublished on lektion.se. I have also done an observation on how the members act on the site’sforum and the lesson database.The teachers in the study define the site depending on which functions they use and how theyuse them. The teachers who only use the lessons database or forum to get access to otherteacher’s ideas, often see the site as a collection of tips. The teachers who use the lessondatabase or forum for both getting other teacher’s ideas and sharing their own ideas often seethe site as a community of practice for teachers.The use of the functions on the site varies. It is mainly the lesson database and the forum thatis used. Only 1 % of the members share their knowledge with other members. But almostevery participant in the study has used some other teacher’s advice or lesson plan. A numberof barriers and motives for sharing knowledge on the site have been identified in the study.The barriers are both personal and organizational. Cowardice, fear and a feeling ofinadequate ideas are some of the personal barriers identified in the study. A lack of time andtechnical problems are examples of organizational barriers. The knowledge sharing amongthe teachers are motivated by own needs and altruism. Through knowledge sharing, they getfeedback on their ideas and facilitate other teacher’s work.To maintain and, hopefully, increase the sharing of knowledge, the number of barriers forsharing knowledge must be minimized. It can be done by creating more opportunities forcollaboration and that the functions in the site will encourage sharing knowledge. It is alsorequired that the teachers create a reflecting culture where knowledge sharing are consideredto be something natural.</p>


corrected abstract:
<p>This thesis is about teachers who share knowledge through lektion.se. Lektion.se is an Internet site and started in 2003 by three teachers. It gives its members an opportunity to share ideas for lessons, discuss interesting topics and give advice to other members. The thesis focuses on how the teachers define and use lection.se for sharing knowledge and how the site can be improved concerning knowledge sharing. To investigate this, seven interviews have been conducted with teachers in and around Stockholm and a questionnaire has been published on lektion.se. I have also done an observation on how the members act on the site’s forum and the lesson database.</p><p>The teachers in the study define the site depending on which functions they use and how they use them. The teachers who only use the lessons database or forum to get access to other teacher’s ideas, often see the site as a collection of tips. The teachers who use the lesson database or forum for both getting other teacher’s ideas and sharing their own ideas often see the site as a community of practice for teachers.</p><p>The use of the functions on the site varies. It is mainly the lesson database and the forum that is used. Only 1 % of the members share their knowledge with other members. But almost every participant in the study has used some other teacher’s advice or lesson plan. A number of barriers and motives for sharing knowledge on the site have been identified in the study. The barriers are both personal and organizational. Cowardice, fear and a feeling of inadequate ideas are some of the personal barriers identified in the study. A lack of time and technical problems are examples of organizational barriers. The knowledge sharing among the teachers are motivated by own needs and altruism. Through knowledge sharing, they get feedback on their ideas and facilitate other teacher’s work.</p><p>To maintain and, hopefully, increase the sharing of knowledge, the number of barriers for sharing knowledge must be minimized. It can be done by creating more opportunities for collaboration and that the functions in the site will encourage sharing knowledge. It is also required that the teachers create a reflecting culture where knowledge sharing are considered to be something natural.</p>
----------------------------------------------------------------------
In diva2:1804716 abstract is:
<p>The thesis examines the prospects of using the superconductor NbN as the gatemetal for an InP HEMT. A HEMT or High Electron Mobility Transistor is aheterostructure transistor engineered to reach very high electron mobility. InPHEMTs are used as cryogenic Low Noise Amplifiers (LNAs), which have increasedin demand as quantum computing is scaling up. A superconducting NbN gate isof interest as it has the potential to decrease the amount of noise generated by theHEMT LNAs.A gate width dependence for both the transconductance (gm) and the large-signal HEMT channel resistance (RON ) of the NbN HEMTs at room temperaturehas been observed, and the first goal pf the thesis is to determine the originof the dependence. Moreover, the measured RF characteristics of the NbNdevices tend to deviate from the norm of a standard HEMT, and the secondgoal is to understand why. The third goal is to determine if the NbN gate stayssuperconducting at cryogenic temperatures or if self-heating from the channelduring DC operations will break superconductivity.In the thesis, it was possible to recreate the observed gate width dependence withnew devices, and additionally, a gate width dependence in the threshold voltageis observed. The origin of width dependence is most likely related to the straincreated by the NbN gate. At DC, extremely high peaks in the transconductanceare observed, which is most likely related to impact ionization and a subsequentincrease in hole trapping caused by the introduction of the NbN gate.Using simulations, it was possible to accurately recreate the observed deviantbehaviour, likely associated with the NbN gate’s high capacitance, inductance andresistance at room temperature. The high capacitance is likely partly related tosome NbN gates of the HEMTs being broken. Finally, the HEMT can operatein DC at 2 K with VG = 0.3 V and a maximum VD = 0.1 V before self-heatingfrom the channel will break the NbN superconductivity of the gate. This is oneof the critical conclusions of the work because it shows that a superconductinggate electrode can be implemented and functional in a high-performance HEMTdevice structure and under realistic operating bias conditions. As long as it can bedemonstrated that the superconductivity does not break when operating in RF, aNbN gate is a promising avenue to increase the noise performance of the cryogenicHEMT.</p>


corrected abstract:
<p>The thesis examines the prospects of using the superconductor NbN as the gate metal for an InP HEMT. A HEMT or High Electron Mobility Transistor is a heterostructure transistor engineered to reach very high electron mobility. InP HEMTs are used as cryogenic Low Noise Amplifiers (LNAs), which have increased in demand as quantum computing is scaling up. A superconducting NbN gate is of interest as it has the potential to decrease the amount of noise generated by the HEMT LNAs.</p><p>A gate width dependence for both the transconductance (<em>g<sub>m</sub></em>) and the large-signal HEMT channel resistance (<em>R<sub>ON</sub></em>) of the NbN HEMTs at room temperature has been observed, and the first goal pf the thesis is to determine the origin of the dependence. Moreover, the measured RF characteristics of the NbN devices tend to deviate from the norm of a standard HEMT, and the second goal is to understand why. The third goal is to determine if the NbN gate stays superconducting at cryogenic temperatures or if self-heating from the channel during DC operations will break superconductivity.</p><p>In the thesis, it was possible to recreate the observed gate width dependence with new devices, and additionally, a gate width dependence in the threshold voltage is observed. The origin of width dependence is most likely related to the strain created by the NbN gate. At DC, extremely high peaks in the transconductance are observed, which is most likely related to impact ionization and a subsequent increase in hole trapping caused by the introduction of the NbN gate.</p><p>Using simulations, it was possible to accurately recreate the observed deviant behaviour, likely associated with the NbN gate’s high capacitance, inductance and resistance at room temperature. The high capacitance is likely partly related to some NbN gates of the HEMTs being broken. Finally, the HEMT can operate in DC at 2 K with <em>V<sub>G</sub> = 0.3</em> V and a maximum <em>V<sub>D</sub> = 0.1</em> V before self-heating from the channel will break the NbN superconductivity of the gate. This is one of the critical conclusions of the work because it shows that a superconducting gate electrode can be implemented and functional in a high-performance HEMT device structure and under realistic operating bias conditions. As long as it can be demonstrated that the superconductivity does not break when operating in RF, a NbN gate is a promising avenue to increase the noise performance of the cryogenic HEMT.</p>
----------------------------------------------------------------------
In diva2:1698151 abstract is:
<p>An extensive technological shift is currently taking place to mitigate climate changeand this trend is particularly noticeable in the transport sector. This is interestingfrom an acoustical perspective, since it changes the noise environment in society.For example, a study in Gothenburg has shown that a complete electrification ofthe road traffic would reduce the noise levels by between 2 and 5 dB(A). In Sweden,noise emissions are calculated with a calculation model from 1996, called the Nordiskberäkningsmodell (Nordiska). Given the age of the model it is reasonable to investigate whether Sweden should change completely to the EU-common calculationmodel Common NOise aSSessment methOdS (CNOSSOS), since it is mandatory touse for national noise mapping.</p><p>This master thesis has performed a computation analysis to compare and discussdifferences between CNOSSOS and Nordiska, to contribute to answering the question whether Sweden should change to CNOSSOS (or perhaps another model). Theresults show that CNOSSOS overall computes higher noise levels than Nordiska andthat the differences between them increase linearly with distance. Farthest from thenoise source the differences are up to 5 dB(A) for the road case and 9 dB(A) forthe railway case. In other words, the differences are larger for the railway trafficmodels than they are for the road traffic models, which is thought to be a result ofthe complexity of the CNOSSOS railway model. Another interesting phenomenonis that the differences behind buildings between the models are different for roadand railway traffic, which can be explained by the fact that the screening effects inNordiska’s road and railway models are different.</p><p>My conclusion is that CNOSSOS is unsuitable for domestic calculations of noiseemissions. The model does not align with Swedish legislation and there is uncertaintydue to the fact that the differences between the CNOSSOS and Nordiska road andrailway models are different in size. Moreover, CNOSSOS railway model requires alot of computational power, which can delay and increase the costs of noise mappingor reduce the accuracy of the results. However, additional work is needed in whicheach calculation model is compared with measurements in situ to see which modelbest describes reality. If the conclusion thereafter is that CNOSSOS still is not asuitable option, it could be examined whether it is possible to create an updatedversion of Nord2000 (another Nordic calculation model used e.g. in Denmark) toobtain a calculation model that is more suitable for future traffic conditions.</p>


corrected abstract:
<p>An extensive technological shift is currently taking place to mitigate climate change and this trend is particularly noticeable in the transport sector. This is interesting from an acoustical perspective, since it changes the noise environment in society. For example, a study in Gothenburg has shown that a complete electrification of the road traffic would reduce the noise levels by between 2 and 5 dB(A). In Sweden, noise emissions are calculated with a calculation model from 1996, called the <em>Nordisk beräkningsmodell</em> (Nordiska). Given the age of the model it is reasonable to investigate whether Sweden should change completely to the EU-common calculation model <em>Common NOise aSSessment methOdS</em> (CNOSSOS), since it is mandatory to use for national noise mapping.</p><p>This master thesis has performed a computation analysis to compare and discuss differences between CNOSSOS and Nordiska, to contribute to answering the question whether Sweden should change to CNOSSOS (or perhaps another model). The results show that CNOSSOS overall computes higher noise levels than Nordiska and that the differences between them increase linearly with distance. Farthest from the noise source the differences are up to 5 dB(A) for the road case and 9 dB(A) for the railway case. In other words, the differences are larger for the railway traffic models than they are for the road traffic models, which is thought to be a result of the complexity of the CNOSSOS railway model. Another interesting phenomenon is that the differences behind buildings between the models are different for road and railway traffic, which can be explained by the fact that the screening effects in Nordiska’s road and railway models are different.</p><p>My conclusion is that CNOSSOS is unsuitable for domestic calculations of noise emissions. The model does not align with Swedish legislation and there is uncertainty due to the fact that the differences between the CNOSSOS and Nordiska road and railway models are different in size. Moreover, CNOSSOS railway model requires a lot of computational power, which can delay and increase the costs of noise mapping or reduce the accuracy of the results. However, additional work is needed in which each calculation model is compared with measurements in situ to see which model best describes reality. If the conclusion thereafter is that CNOSSOS still is not a suitable option, it could be examined whether it is possible to create an updated version of Nord2000 (another Nordic calculation model used e.g. in Denmark) to obtain a calculation model that is more suitable for future traffic conditions.</p>
----------------------------------------------------------------------
In diva2:1465539 abstract is:
<p>Throughout the history of motor vehicles, the tyres have always been consideredas one of the most important components of the vehicle due to their interactionwith the road. One important aspect is the wheel alignment, with the purposeto adjust the static wheel angles that are essential for many reasons, such assafety and fuel consumption for instance. Despite the numerous methods forwheel angle measurements, there seems to be no existing technical solutionbased on computer vision, that is suitable for residential use, regarding bothcost and size of the equipment. The study aims to investigate the feasibility ofsuch a system.The proposed system is based on planar fiducial markers called ArUco.From images or video frames of the marker, the pose of the marker can beestimated. Thus, by placing such markers on the ground, on the wheel andon the vehicle, the estimated pose of the markers can be used to measure andcalculate the wheel alignment parameters. Only toe and camber angles aremeasured within the scope of this thesis, even if the system has the potential tomeasure other wheel alignment parameters as well.After camera calibration, simplified ArUco marker tests were done by measuringthe known displacement and inclination of a marker with respect to areference marker. The mean absolute error was 030400 and 0:024mm for theinclination angle and displacement, respectively. Furthermore, the toe and camberangles of a vehicle were measured and compared to reference measurementsperformed with a commercial wheel alignment system, giving mean absoluteerrors of 0520 and 0280 for the camber and toe angles, respectively. Despitethe relatively large errors for the toe and camber angle measurements, theresults from the initial inclination and displacement tests show the potential ofthe system. In addition, several error sources and suggestions for improvementcan be identified.As a conclusion, the proposed system can be considered a working firstprototype, which after improvement and optimisation has the potential tobecome a feasible alternative, especially for residential use and for mobileworkshops due to the low cost, size and usability of the system.</p>


corrected abstract:
<p>Throughout the history of motor vehicles, the tyres have always been considered as one of the most important components of the vehicle due to their interaction with the road. One important aspect is the wheel alignment, with the purpose to adjust the static wheel angles that are essential for many reasons, such as safety and fuel consumption for instance. Despite the numerous methods for wheel angle measurements, there seems to be no existing technical solution based on computer vision, that is suitable for residential use, regarding both cost and size of the equipment. The study aims to investigate the feasibility of such a system.</p><p>The proposed system is based on planar fiducial markers called ArUco. From images or video frames of the marker, the pose of the marker can be estimated. Thus, by placing such markers on the ground, on the wheel and on the vehicle, the estimated pose of the markers can be used to measure and calculate the wheel alignment parameters. Only toe and camber angles are measured within the scope of this thesis, even if the system has the potential to measure other wheel alignment parameters as well.</p><p>After camera calibration, simplified ArUco marker tests were done by measuring the known displacement and inclination of a marker with respect to a reference marker. The mean absolute error was 0&deg;3&prime;4&Prime; and 0.024 mm for the inclination angle and displacement, respectively. Furthermore, the toe and camber angles of a vehicle were measured and compared to reference measurements performed with a commercial wheel alignment system, giving mean absolute errors of 0&deg;52&prime; and 0&deg;28&prime; for the camber and toe angles, respectively. Despite the relatively large errors for the toe and camber angle measurements, the results from the initial inclination and displacement tests show the potential of the system. In addition, several error sources and suggestions for improvement can be identified.</p><p>As a conclusion, the proposed system can be considered a working first prototype, which after improvement and optimisation has the potential to become a feasible alternative, especially for residential use and for mobile workshops due to the low cost, size and usability of the system.</p>
----------------------------------------------------------------------
In diva2:1189528 abstract is:
<p>Road accidents have been a persistent cause of death worldwide, and claim millions of lives everyyear. Recent developments in the active safety systems like Electronic Stability Control (ESC) havehelped in reducing these numbers quite signicantly over the years. However, a major challenge forthese systems is to know the friction coecient between the tire and the road, as this value limits theamount of force the tires can generate. Knowledge of the coecient of friction can be used to adaptthe driving style, thereby avoiding interventions by stability control at the limit, making vehiclessafer. However, it is a major challenge within the automotive industry to estimate the coecientof friction accurately, and with sucient availability, as that requires high levels of tire utilization,such that the tire is forced to reach the non-linear range of operation. Such events are very rarein everyday driving, and requires a system induced active excitation of the tires. One such methodthat has been proposed earlier, to carry out an active tire excitation, is by using a simultaneouspropulsive and brake force on front and the rear the axles. However, applying an equal magnitudeof propulsive and brake force results in a force neutral situation at the vehicle level, which forcesthe velocity to be constant, overriding driver acceleration requests. Thus, an active tire excitationmethod was proposed by Volvo Cars, which is able to apply an unequal propulsive and brake forceto the front and the rear axle, such that the driver's acceleration demand can be met, during frictionestimation. However, such an excitation can be dangerous to carry out, if it leads to instability ofthe vehicle.Several methods have been developed to analyze and quantify stability of a vehicle, but detailedanalysis about the stability under forced excitation, for friction estimation, is very rare. This thesiswork investigates the lateral stability of a vehicle undergoing an active tire excitation for frictionestimation. The objective is to understand which vehicle and tire models can be used to quantifythe lateral stability of a vehicle under forced excitation, and how phase portrait methods can beused to develop a stability monitor that is able to indicate the lateral stability of the vehicle undera forced excitation.The results of using a stability monitor during active tire excitation clearly show that it is able toindicate when the vehicle becomes unstable and looses control. It also shows that for slow speedsteady-state maneuvers and straight line maneuvers, the stability monitor does not indicate instability.A comparison between phase portrait based and conventional side-slip based stability monitorsshow the eectiveness and generality of the phase portrait based monitor, which is able to detectinstability earlier than the conventional side-slip based method.</p>

corrected abstract:
<p>Road accidents have been a persistent cause of death worldwide, and claim millions of lives every year. Recent developments in the active safety systems like Electronic Stability Control (ESC) have helped in reducing these numbers quite significantly over the years. However, a major challenge for these systems is to know the friction coefficient between the tire and the road, as this value limits the amount of force the tires can generate. Knowledge of the coefficient of friction can be used to adapt the driving style, thereby avoiding interventions by stability control at the limit, making vehicles safer. However, it is a major challenge within the automotive industry to estimate the coefficient of friction accurately, and with sufficient availability, as that requires high levels of tire utilization, such that the tire is forced to reach the non-linear range of operation. Such events are very rare in everyday driving, and requires a system induced active excitation of the tires. One such method that has been proposed earlier, to carry out an active tire excitation, is by using a simultaneous propulsive and brake force on front and the rear the axles. However, applying an equal magnitude of propulsive and brake force results in a force neutral situation at the vehicle level, which forces the velocity to be constant, overriding driver acceleration requests. Thus, an active tire excitation method was proposed by Volvo Cars, which is able to apply an unequal propulsive and brake force to the front and the rear axle, such that the driver's acceleration demand can be met, during friction estimation. However, such an excitation can be dangerous to carry out, if it leads to instability of the vehicle.</p><p>Several methods have been developed to analyze and quantify stability of a vehicle, but detailed analysis about the stability under forced excitation, for friction estimation, is very rare. This thesis work investigates the lateral stability of a vehicle undergoing an active tire excitation for friction estimation. The objective is to understand which vehicle and tire models can be used to quantify the lateral stability of a vehicle under forced excitation, and how phase portrait methods can be used to develop a stability monitor that is able to indicate the lateral stability of the vehicle under a forced excitation.</p><p>The results of using a stability monitor during active tire excitation clearly show that it is able to indicate when the vehicle becomes unstable and looses control. It also shows that for slow speed steady-state maneuvers and straight line maneuvers, the stability monitor does not indicate instability. A comparison between phase portrait based and conventional side-slip based stability monitors show the effectiveness and generality of the phase portrait based monitor, which is able to detect instability earlier than the conventional side-slip based method.</p>
----------------------------------------------------------------------
In diva2:1087251 - space missing in title:
"Numerical study on hydraulic verticallift gate during shutdown process"
==>
"Numerical study on hydraulic vertical lift gate during shutdown process"

abstract is:
<p>China is undergoing a rapid increase in their development of hydropower.Due to this rapid increase, China has become one of theleading countries in technological solutions regarding the constructionof the hydropower plant. The hydro resources in China are extensivebut building a new power plant is laborious and costly. Upgrading anexisting power plant is therefore of interest. Increasing the volume flowis one way, but this can bring problems to the hydraulic structures.The design of hydraulic gates is crucial for operating a hydropowerplant safely. An emergency gate is especially important as it protectsthe turbine situated downstream of the gate. In this study, a numericalsimulation of the shutdown process of a hydraulic vertical lift gatewas conducted. The simulation was done in two dimensions using theReynolds Navier Stokes Equations (RANS), together with the RNGk ≠ ‘ turbulence model and the Volume of Fluid method (VOF). Thegoal was to extract the pressure distribution around the gate, subsequently,attaining the hydrodynamic forces and also to observe andanalyze the flow surrounding the gate. The simulation was comparedwith existing experimental data, from a 1/18 scale model, for validation.Once the model was validated, eight different cases were tested toimprove the operating conditions. The closing speed of the gate andthe gate bottom angle was altered in order to reduce the down-pullforce and undesirable flow phenomena. It was found that lowering thegate speed to 8.1 m/min would have positive effect. As the gate closesrelatively fast with reduced forces compared to a faster speed, and withless induced vibrations than with a slower speed. Changing the gatebottom angle from 9¶ to 30¶, would also have a considerable positiveinfluence of the lowered gate vibrations. However changing the bottomangle needs to be more thoroughly studied concerning structuraleffects.</p>

corrected abstract:
<p>China is undergoing a rapid increase in their development of hydropower. Due to this rapid increase, China has become one of the leading countries in technological solutions regarding the construction of the hydropower plant. The hydro resources in China are extensive but building a new power plant is laborious and costly. Upgrading an existing power plant is therefore of interest. Increasing the volume flow is one way, but this can bring problems to the hydraulic structures. The design of hydraulic gates is crucial for operating a hydropower plant safely. An emergency gate is especially important as it protects the turbine situated downstream of the gate. In this study, a numerical simulation of the shutdown process of a hydraulic vertical lift gate was conducted. The simulation was done in two dimensions using the Reynolds Navier Stokes Equations (RANS), together with the RNG <em>k - &epsilon;</em> turbulence model and the Volume of Fluid method (VOF). The goal was to extract the pressure distribution around the gate, subsequently, attaining the hydrodynamic forces and also to observe and analyze the flow surrounding the gate. The simulation was compared with existing experimental data, from a 1/18 scale model, for validation. Once the model was validated, eight different cases were tested to improve the operating conditions. The closing speed of the gate and the gate bottom angle was altered in order to reduce the down-pull force and undesirable flow phenomena. It was found that lowering the gate speed to 8.1 m/min would have positive effect. As the gate closes relatively fast with reduced forces compared to a faster speed, and with less induced vibrations than with a slower speed. Changing the gate bottom angle from 9&deg; to 30&deg;, would also have a considerable positive influence of the lowered gate vibrations. However changing the bottom angle needs to be more thoroughly studied concerning structural effects.</p>
----------------------------------------------------------------------
In diva2:1078078 - missing space and ligature in title:
"An Experimental Study on Global TurbineArray Eects in Large Wind Turbine Clusters"
==>
"An Experimental Study on Global Turbine Array Effects in Large Wind Turbine Clusters"

abstract is:
<p>It is well known that the layout of a large wind turbine cluster aects the energyoutput of the wind farm. The individual placement and distances betweenturbines will in uence the wake spreading and the wind velocity decit. Manyanalytical models and simulations have been made trying to calculate this, butstill there is a lack of experimental data to conrm the models. This thesis isdescribing the preparations and the execution of an experiment that has beenconducted using about 250 small rotating turbine models in a wind tunnel. Theturbine models were developed before the experiment and the characteristicswere investigated. The main focus was laid on special eects occurring in largewind turbine clusters, which were named Global Turbine Array Eects.It was shown that the upstream wind was little aected by a large windfarm downstream, even though there existed a small dierence in wind speedbetween the undisturbed free stream and the wind that arrived to the rstturbines in the wind farm. The dierence in wind speed was shown to beunder 1% of the undisturbed free stream. It was also shown that the densityof the wind farm was related to the reduced wind velocity, with a more densefarm the reduction could get up to 2.5% of the undisturbed free stream at theupstream center turbine. Less velocity decit was observed at the upstreamcorner turbines in the wind farm.When using small rotating turbine models some scaling requirements hadto be considered to make the experiment adaptable to reality. It was concludedthat the thrust coecient of the turbine models was the most important parameterwhen analysing the eects. One problem discussed was the low Reynoldsnumber, an eect always present in wind tunnel studies on small wind turbinemodels.A preliminary investigation of a photo measuring technique was also performed,but the technique was not fully developed. The idea was to take oneor a few photos instantaneously and then calculate the individual rotationalspeed of all the turbine models. It was dicult to apply the technique becauseof uctuations in rotational speed during the experiment, therefore thecalculated values could not represent the mean value over a longer time period.</p>

corrected abstract:
<p>It is well known that the layout of a large wind turbine cluster affects the energy output of the wind farm. The individual placement and distances between turbines will influence the wake spreading and the wind velocity deficit. Many analytical models and simulations have been made trying to calculate this, but still there is a lack of experimental data to confirm the models. This thesis is describing the preparations and the execution of an experiment that has been conducted using about 250 small rotating turbine models in a wind tunnel. The turbine models were developed before the experiment and the characteristics were investigated. The main focus was laid on special effects occurring in large wind turbine clusters, which were named Global Turbine Array Effects.</p><p>It was shown that the upstream wind was little affected by a large wind farm downstream, even though there existed a small difference in wind speed between the undisturbed free stream and the wind that arrived to the first turbines in the wind farm. The difference in wind speed was shown to be under 1% of the undisturbed free stream. It was also shown that the density of the wind farm was related to the reduced wind velocity, with a more dense farm the reduction could get up to 2.5% of the undisturbed free stream at the upstream center turbine. Less velocity deficit was observed at the upstream corner turbines in the wind farm.</p><p>When using small rotating turbine models some scaling requirements had to be considered to make the experiment adaptable to reality. It was concluded that the thrust coefficient of the turbine models was the most important parameter when analysing the effects. One problem discussed was the low Reynolds number, an effect always present in wind tunnel studies on small wind turbine models.</p><p>A preliminary investigation of a photo measuring technique was also performed, but the technique was not fully developed. The idea was to take one or a few photos instantaneously and then calculate the individual rotational speed of all the turbine models. It was difficult to apply the technique because of fluctuations in rotational speed during the experiment, therefore the calculated values could not represent the mean value over a longer time period.</p>
----------------------------------------------------------------------
In diva2:783994 abstract is:
<p>The performance and sound emission of a fan is strongly influenced by the installationeffect, which can be defined as the difference between the performance of a fan in ainstallation and the ideal configuration of the same fan. The factors that one should keepin mind while designing a fan system are many, but if some ground rules are followed thenoise can be drastically reduced. The choice of location for the equipment in the buildingis a critical decision and a less ideal location can result in expensive reconstructions and,or that spaces around the fan room can not be used for its initial purpose. A large fan withlower rotation speed will have lower sound emissions then a smaller fan with a higherrotation speed, for the same air flow. The sound and vibration emissions, as well as theenergy consumption of the fan will be at its lowest values when it is at its point ofmaximum efficiency. The outlet configuration of the duct from the fan should be straightand without dampers or ducts silencers that can create turbulence or a higher staticpressure close to the fan, which will decrease the fans performance drastically.The vibration isolation of the fan should be created and specified for the specificinstallation and not solely the fan characteristics. Proposals to predict and measure thestructure-borne sound pressure and transmissions in buildings have recently beenreleased. With a standard over the structure borne sound, the manufactures can declarethe source data for the fans under different operations. This brings that more accuratepredictions and calculations of the structure borne sound from installations can be done.Earlier calculation methods show big deviances between measured and calculated soundpressure in several cases. Above all the spread of the results is large, which makes themethod somewhat unreliable when sound rating spaces, regarding fan room noise.Calculations and predictions of the sound pressure in a fan room can, after proposals ofchange, be done with a deviation of 10 dBfor all frequencies between 63 and 4000 Hz.The method shows a tendency to overrate the sound pressure with a relatively smallspread of the results. It also shows signs to be able to predict the sound pressure in fanrooms with smaller fans then big fan units.Calculations of the increase of sound pressure that occur in the cavity between the floorstructure and the fan unit show big deviations it if is done for specific frequencies.However results show that calculations of the total sound pressure can be done with abetter accuracy.</p>


corrected abstract:
<p>The performance and sound emission of a fan is strongly influenced by the installation effect, which can be defined as the difference between the performance of a fan in a installation and the ideal configuration of the same fan. The factors that one should keep in mind while designing a fan system are many, but if some ground rules are followed the noise can be drastically reduced. The choice of location for the equipment in the building is a critical decision and a less ideal location can result in expensive reconstructions and, or that spaces around the fan room can not be used for its initial purpose. A large fan with lower rotation speed will have lower sound emissions then a smaller fan with a higher rotation speed, for the same air flow. The sound and vibration emissions, as well as the energy consumption of the fan will be at its lowest values when it is at its point of maximum efficiency. The outlet configuration of the duct from the fan should be straight and without dampers or ducts silencers that can create turbulence or a higher static pressure close to the fan, which will decrease the fans performance drastically.</p><p>The vibration isolation of the fan should be created and specified for the specific installation and not solely the fan characteristics. Proposals to predict and measure the structure-borne sound pressure and transmissions in buildings have recently been released. With a standard over the structure borne sound, the manufactures can declare the source data for the fans under different operations. This brings that more accurate predictions and calculations of the structure borne sound from installations can be done.</p><p>Earlier calculation methods show big deviances between measured and calculated sound pressure in several cases. Above all the spread of the results is large, which makes the method somewhat unreliable when sound rating spaces, regarding fan room noise.</p><p>Calculations and predictions of the sound pressure in a fan room can, after proposals of change, be done with a deviation of ±10 dB for all frequencies between 63 and 4000 Hz. The method shows a tendency to overrate the sound pressure with a relatively small spread of the results. It also shows signs to be able to predict the sound pressure in fan rooms with smaller fans then big fan units. Calculations of the increase of sound pressure that occur in the cavity between the floor structure and the fan unit show big deviations it if is done for specific frequencies. However results show that calculations of the total sound pressure can be done with abetter accuracy.</p>
----------------------------------------------------------------------
In diva2:618588 error in title
"Modeling And Analysis Of Fault Conditions In Avehicle With Four In-Wheel Motors"
==>
"Modeling And Analysis Of Fault Conditions In A Vehicle With Four In-Wheel Motors"

abstract is:
<p>A vast expansion is found in the field of automotive electronic systems. The expansion iscoupled with a related increase in the demands of power and design. Now, this is goodarena of engineering opportunities and challenges. One of the challenges faced, isdeveloping fault tolerant systems, which increases the overall automotive and passengersafety. The development in the field of automotive electronics has led to the innovationof some very sophisticated technology. However, with increasing sophistication intechnology also rises the requirement to develop fault tolerant solutions.As one of many steps towards developing a fault tolerant system, this thesis presents anexhaustive fault analysis. The modeling and fault analysis is carried out for a vehicle withfour in-wheel motors. The primary goal is to collect as many of the possible failuremodes that could occur in a vehicle. A database of possible failure modes is retrievedfrom the Vehicle Dynamics research group at KTH. Now with further inputs to thisdatabase the individual faults are factored with respect to change in parameters of vehicleperformance. The factored faults are grouped with respect to similar outputcharacterization.The fault groups are modeled and integrated into a vehicle model developed earlier inMatlab/Simulink. All the fault groups are simulated under specific conditions and theresults are obtained. The dynamic behavior of the vehicle under such fault conditions isanalyzed. Further, in particular the behavior of the vehicle with electronic stabilitycontrol (ESC) under the fault conditions is tested. The deviation in the vital vehicleperformance parameters from nominal is computed.Finally based on the results obtained, a ranking system termed Severity Ranking System(SeRS) is presented. The severity ranking is presented based on three essential vehicleperformance parameters, such as longitudinal acceleration ( ), lateral acceleration ( )and yaw rate ( ̇ ). The ranking of the faults are classified as low severity S1, mediumseverity S2, high severity S3 and very high severity S4. A fault tolerant system must beable to successfully detect the fault condition, isolate the fault and provide correctiveaction. Hence, this database would serve as an effective input in developing fault tolerantsystems.</p>

corrected abstract:
<p>A vast expansion is found in the field of automotive electronic systems. The expansion is coupled with a related increase in the demands of power and design. Now, this is good arena of engineering opportunities and challenges. One of the challenges faced, is developing fault tolerant systems, which increases the overall automotive and passenger safety. The development in the field of automotive electronics has led to the innovation of some very sophisticated technology. However, with increasing sophistication in technology also rises the requirement to develop fault tolerant solutions.</p><p>As one of many steps towards developing a fault tolerant system, this thesis presents an exhaustive fault analysis. The modeling and fault analysis is carried out for a vehicle with four in-wheel motors. The primary goal is to collect as many of the possible failure modes that could occur in a vehicle. A database of possible failure modes is retrieved from the Vehicle Dynamics research group at KTH. Now with further inputs to this database the individual faults are factored with respect to change in parameters of vehicle performance. The factored faults are grouped with respect to similar output characterization.</p><p>The fault groups are modeled and integrated into a vehicle model developed earlier in Matlab/Simulink. All the fault groups are simulated under specific conditions and the results are obtained. The dynamic behavior of the vehicle under such fault conditions is analyzed. Further, in particular the behavior of the vehicle with electronic stability control (ESC) under the fault conditions is tested. The deviation in the vital vehicle performance parameters from nominal is computed.</p><p>Finally based on the results obtained, a ranking system termed Severity Ranking System (SeRS) is presented. The severity ranking is presented based on three essential vehicle performance parameters, such as longitudinal acceleration (<strong>a<sub>x</sub></strong>), lateral acceleration (<strong>a<sub>y</sub></strong>) and yaw rate (<strong>&psi;&#x307;</strong>). The ranking of the faults are classified as low severity S1, medium severity S2, high severity S3 and very high severity S4. A fault tolerant system must be able to successfully detect the fault condition, isolate the fault and provide corrective action. Hence, this database would serve as an effective input in developing fault tolerant systems.</p>
----------------------------------------------------------------------
In diva2:1670946 - error in title:
"Mechanical Design,Analysis, andManufacturing of Wind Tunnel Modeland support structure"
==>
"Mechanical Design, Analysis, and Manufacturing of Wind Tunnel Model and Support Structure"

abstract is:
<p>This volume covers the phases from design to manufacturing of a wind tunnel testsupport structure for a conceptual blended wingbodyUAV designed by KTH GreenRaven Project students. The innovative aircraft design demonstrates sustainabilitywithin aviation by utilizing a hybrid electricfuelcell propulsion system. The windtunnel test to be conducted at Bristol University will produce data to evaluate theaerodynamic properties of the model for design verification. The wind tunnel modelis a smallscaled1.5mspanmodel supported by struts that change the pitch andyaw angles during testing. An external force balance provided by Bristol Universitymeasures the loads and moments experienced by the model. The main requirementsfor the structure are to withstand the aerodynamic loads imposed by the model andto change the model’s orientation while maintaining wind speed during the test. Themaximum aerodynamic loads were provided in a matrix, the largest of which was usedas the load condition for the support equating to a 512N lift at 14◦ AOA. Trade studieswere conducted to determine the mechanisms to satisfy the requirements while stayingwithin budget. The chosen design for the support structure includes a circular baseplate constrained by a locking ring with positioning pins to change the yaw angle. Themain strut is mounted at the the center of the circular base plate. A hinge bracketat the top of the strut interfaces with another hinge bracket within the model viaa clevis pin. An electric linear actuator mounted downstream of the main strut isused to vary the pitch angle, with the center of rotation at the clevis pin. Once thedesign was finalized, finite element analysis was done to verify the structural stabilityof the design. The FEA results were compared to EulerBernoulliapproximations fordeflection. Manufacturing of the components was outsourcedwhile assembly andprogramming of the actuator was done inhouse.</p>


corrected abstract:
<p>This volume covers the phases from design to manufacturing of a wind tunnel test support structure for a conceptual blended wing-body UAV designed by KTH Green Raven Project students. The innovative aircraft design demonstrates sustainability within aviation by utilizing a hybrid electric-fuel cell propulsion system. The wind tunnel test to be conducted at Bristol University will produce data to evaluate the aerodynamic properties of the model for design verification. The wind tunnel model is a small-scaled 1.5m-span model supported by struts that change the pitch and yaw angles during testing. An external force balance provided by Bristol University measures the loads and moments experienced by the model. The main requirements for the structure are to withstand the aerodynamic loads imposed by the model and to change the model’s orientation while maintaining wind speed during the test. The maximum aerodynamic loads were provided in a matrix, the largest of which was used as the load condition for the support equating to a 512N lift at 14&deg; AOA. Trade studies were conducted to determine the mechanisms to satisfy the requirements while staying within budget. The chosen design for the support structure includes a circular base plate constrained by a locking ring with positioning pins to change the yaw angle. The main strut is mounted at the the center of the circular base plate. A hinge bracket at the top of the strut interfaces with another hinge bracket within the model via a clevis pin. An electric linear actuator mounted downstream of the main strut is used to vary the pitch angle, with the center of rotation at the clevis pin. Once the design was finalized, finite element analysis was done to verify the structural stability of the design. The FEA results were compared to Euler-Bernoulli approximations for deflection. Manufacturing of the components was out-sourced while assembly and programming of the actuator was done in-house.</p>
----------------------------------------------------------------------
In diva2:1547559 abstract is:
<p>A hydraulic damper plays an important role in tuning the handling and comfort characteristicsof a vehicle. Tuning and selecting a damper based on subjective evaluation, by considering theopinions of various users, would be an inefficient method since the comfort requirements of usersvary a lot. Instead, mathematical models of damper and simulation of these models in variousoperating conditions are preferred to standardize the tuning procedure, quantify the comfortlevels and reduce cost of testing. This would require a model, which is good enough to capture thebehaviour of damper in various operating and extreme conditions.The Force-Velocity (FV) curve is one of the most widely used model of a damper. This curve isimplemented either as an equation or as a look-up table. It is a plot between the maximum forceat each peak velocity point. There are certain dynamic phenomena like hysteresis and dependencyon the displacement of damper, which cannot be captured with a FV curve model, but are requiredfor better understanding of the vehicle behaviour.This thesis was conducted in cooperation with Volvo Cars with an aim to improve the existingdamper model which is a Force-Velocity curve. This work focuses on developing a damper model,which is complex enough to capture the phenomena discussed above and simple enough to beimplemented in real time simulations. Also, the thesis aims to establish a standard method toparameterise the damper model and generate the Force-Velocity curve from the tests performedon the damper test rig. A test matrix which includes the standard tests for parameterising andthe extreme test cases for the validation of the developed model will be developed. The final focusis to implement the damper model in a multi body simulation (MBS) software.The master thesis starts with an introduction, where the background for the project is described and then the thesis goals are set. It is followed by a literature review in which fewadvanced damper models are discussed in brief. Then, a step-by-step process of developing thedamper model is discussed along with few more possible options. Later, the construction of a testmatrix is discussed in detail followed by the parameter identification process. Next, the validationof the developed damper model is discussed using the test data from Volvo Hällered ProvingGround (HPG). After validation, implementation of the model in VI CarRealTime and Adams Caralong with the results are presented. Finally the thesis is concluded and the recommendations forfuture work are made on further improving the model.</p>


corrected abstract:
<p>A hydraulic damper plays an important role in tuning the handling and comfort characteristics of a vehicle. Tuning and selecting a damper based on subjective evaluation, by considering the opinions of various users, would be an inefficient method since the comfort requirements of users vary a lot. Instead, mathematical models of damper and simulation of these models in various operating conditions are preferred to standardize the tuning procedure, quantify the comfort levels and reduce cost of testing. This would require a model, which is good enough to capture the behaviour of damper in various operating and extreme conditions.</p><p>The Force-Velocity (FV) curve is one of the most widely used model of a damper. This curve is implemented either as an equation or as a look-up table. It is a plot between the maximum force at each peak velocity point. There are certain dynamic phenomena like hysteresis and dependency on the displacement of damper, which cannot be captured with a FV curve model, but are required for better understanding of the vehicle behaviour.</p><p>This thesis was conducted in cooperation with Volvo Cars with an aim to improve the existing damper model which is a Force-Velocity curve. This work focuses on developing a damper model, which is complex enough to capture the phenomena discussed above and simple enough to be implemented in real time simulations. Also, the thesis aims to establish a standard method to parameterise the damper model and generate the Force-Velocity curve from the tests performed on the damper test rig. A test matrix which includes the standard tests for parameterising and the extreme test cases for the validation of the developed model will be developed. The final focus is to implement the damper model in a multi body simulation (MBS) software.</p><p>The master thesis starts with an introduction, where the background for the project is described and then the thesis goals are set. It is followed by a literature review in which few advanced damper models are discussed in brief. Then, a step-by-step process of developing the damper model is discussed along with few more possible options. Later, the construction of a test matrix is discussed in detail followed by the parameter identification process. Next, the validation of the developed damper model is discussed using the test data from Volvo Hällered Proving Ground (HPG). After validation, implementation of the model in VI CarRealTime and Adams Car along with the results are presented. Finally the thesis is concluded and the recommendations for future work are made on further improving the model.</p>
----------------------------------------------------------------------
In diva2:1465540 abstract is:
<p>There are many applications where 3D models of landscapes can be used, suchas determining volume of objects, inspecting buildings and planning of infrastructure.One common way of creating 3D models of a geographical area is totake overlapping geotagged photos with a drone and then perform an aerotriangulation(AT). The aerotriangulating software find common key points in theimages and create a 3D surface model of the area. This process requires the use ofground control points (GCPs), which are used to map the 3D model onto a globalcoordinate system. These GCPs consume a lot of time to place at the location,measure accurately with total station and manually pinpoint in several photos.The purpose of this study is to compare models created by images taken with differentGNSS-based drone positioning systems and investigate for example howmany GCPs are needed, how the GCPs should be placed, and how the accuracyof models vary between the drone positioning systems considering both relativeand absolute accuracy.Two data acquisition sessions were done where images from two differentlocations were collected. In the first session the use of regular GPS and the use ofa local reference station are used as positioning system for the drone, and in thesecond session network real time kinematics (RTK) is also used as a third kind ofpositioning system.From the produced 3D models there is no significant difference between modelsusing a local reference station and network RTK, but when only using GPS thevertical accuracy drops significantly which means that more GCPs are required inorder for the model to be accurate. The standard deviation for points in createdmodels is calculated in easting, northing and vertical for the coordinate differencesfor the three positioning methods. When no GCPs are used the absoluteaccuracy is drastically lowered to meter level accuracy. In conclusion, by usingnetwork RTK or a local reference station the same accuracy for the 3D model canbe acquired with much fewer GCPs than if stand-alone GPS is used. None ofthe positioning systems can fully replace GCPs when a high absolute accuracy isneeded. With a relative accuracy requirement of 10 cm or more, both networkRTK and use of a local reference station has the potential to provide such qualitythat a 3D model would not need GCPs.</p>


corrected abstract:
<p>There are many applications where 3D models of landscapes can be used, such as determining volume of objects, inspecting buildings and planning of infrastructure. One common way of creating 3D models of a geographical area is to take overlapping geotagged photos with a drone and then perform an aerotriangulation (AT). The aerotriangulating software find common key points in the images and create a 3D surface model of the area. This process requires the use of ground control points (GCPs), which are used to map the 3D model onto a global coordinate system. These GCPs consume a lot of time to place at the location, measure accurately with total station and manually pinpoint in several photos. The purpose of this study is to compare models created by images taken with different GNSS-based drone positioning systems and investigate for example how many GCPs are needed, how the GCPs should be placed, and how the accuracy of models vary between the drone positioning systems considering both relative and absolute accuracy.</p><p>Two data acquisition sessions were done where images from two different locations were collected. In the first session the use of regular GPS and the use of a local reference station are used as positioning system for the drone, and in the second session network real time kinematics (RTK) is also used as a third kind of positioning system.</p><p>From the produced 3D models there is no significant difference between models using a local reference station and network RTK, but when only using GPS the vertical accuracy drops significantly which means that more GCPs are required in order for the model to be accurate. The standard deviation for points in created models is calculated in easting, northing and vertical for the coordinate differences for the three positioning methods. When no GCPs are used the absolute accuracy is drastically lowered to meter level accuracy. In conclusion, by using network RTK or a local reference station the same accuracy for the 3D model can be acquired with much fewer GCPs than if stand-alone GPS is used. None of the positioning systems can fully replace GCPs when a high absolute accuracy is needed. With a relative accuracy requirement of 10 cm or more, both network RTK and use of a local reference station has the potential to provide such quality that a 3D model would not need GCPs.</p>
----------------------------------------------------------------------
In diva2:1142969 abstract is:
<p>Standardized information and mathematicalmodels, which model the characteristics of the power generationand power transmission systems, are requirements for futuredevelopment and maintenance of different applications tooperate the electrical grid. Available databases such as Nordpoolprovides large amounts of data for power supply and demand [1].The typical misconception with open availability of data is thatexisting power system software tools can interact and process thisdata. Difficulties occur mainly because of two reasons. The firston is the amount of data produced. When the topology of theelectrical grid changes e.g. when a switch opens or closes, the flowof electrical power changes. This event produce changes ingeneration, transmission and distribution of the energy anddifferent data sets are produced. The second problem is therepresentation of information [2]. There are a limited number ofsoftware tools that can analyze this data, but each software toolrequires a specific data format structure to run. Dealing withthese difficulties requires an effective way to transform theprovided data representation into new data structures that canbe used in different execution platforms. This project aims tocreate a generic Model-to-Text (M2T) transformation capable oftransforming standardized power system information modelsinto input files executable by the Power System Analysis Tool(PSAT). During this project, a working M2T transformation wasnever achieved. However, missing functionality in someprograms connected to sub processes resulted in unexpectedproblems. This led to a new task of updating the informationmodel interpreter PyCIM. This task is partially completed andcan load basic power system information models.</p>

corrected abstract:
<p>Standardized information and mathematical models, which model the characteristics of the power generation and power transmission systems, are requirements for future development and maintenance of different applications to operate the electrical grid. Available databases such as Nordpool provides large amounts of data for power supply and demand [1]. The typical misconception with open availability of data is that existing power system software tools can interact and process this data. Difficulties occur mainly because of two reasons. The first on is the amount of data produced. When the topology of the electrical grid changes e.g. when a switch opens or closes, the flow of electrical power changes. This event produce changes in generation, transmission and distribution of the energy and different data sets are produced. The second problem is the representation of information [2]. There are a limited number of software tools that can analyze this data, but each software tool requires a specific data format structure to run. Dealing with these difficulties requires an effective way to transform the provided data representation into new data structures that can be used in different execution platforms. This project aims to create a generic Model-to-Text (M2T) transformation capable of transforming standardized power system information models into input files executable by the Power System Analysis Tool (PSAT). During this project, a working M2T transformation was never achieved. However, missing functionality in some programs connected to sub processes resulted in unexpected problems. This led to a new task of updating the information model interpreter PyCIM. This task is partially completed and can load basic power system information models.</p>
----------------------------------------------------------------------
In diva2:405988 abstract is:
<p>Recently, Volvo Construction Equipment AB has developed a weld class system forimperfections in welded joints, which contains demands for the toe radii, cold laps, undercutsetc. and where root defects are treated as requirements on the drawing. In this master thesis, thetoe radius has been studied more carefully along with the selection of reliable measurementsystems which are able to measure the toe radius along the weld. A computerized vision systemhas been evaluated by performing a measurement system analysis. FE-simulations anddestructive fatigue testing has also been carried out to determine which radial geometry beingcritical to the fatigue life.The results show that the currently used methods and gauges do not provide the requiredaccuracy when measuring the toe radius. The gauges are handled differently by differentoperators – even when using the vision system – which makes the methods subjective andtherefore unreliable. There are measuring systems that can gather surface data along the weldwith high accuracy, but there is no reliable method to assess the data. Therefore, the authors havedeveloped an algorithm – named STELIN – that assess the gathered surface data andautomatically identifies and calculates the toe radius and the toe angle along the weld. Using thatinformation an opportunity to improve the process control when welding is possible.The performed FE-calculations show that the surface roughness in the weld toe probably has aninfluence at the fatigue life of the joint. A more precise separate study should be made todetermine the impact of the surface roughness on the fatigue life. Those results should serve as abase when reviewing the theory used when predicting the fatigue life. Currently, stress averagingapproach is used in the notches of the root and the weld toe. In the future though, there might beanother stress condition to be taken into account, if the goal of reducing weight of the finishedproduct shall be achieved. Regarding measuring the surface roughness in the weld toe, theevaluated vision system has enough accuracy to deliver reliable data.More work remains with the STELIN-algorithm. The method used when assessing the calculatedtoe radii should be based on the conclusions from the performed FE-calculations. Integrating theSTELIN-algorithm in a fast feedback measurement system – for instance, on a laser – willprobably provide good opportunities for a better process control in order to achieve higherfatigue life of the welded joint.</p>

corrected abstract:
<p>Recently, Volvo Construction Equipment AB has developed a weld class system for imperfections in welded joints, which contains demands for the toe radii, cold laps, undercuts etc. and where root defects are treated as requirements on the drawing. In this master thesis, the toe radius has been studied more carefully along with the selection of reliable measurement systems which are able to measure the toe radius along the weld. A computerized vision system has been evaluated by performing a measurement system analysis. FE-simulations and destructive fatigue testing has also been carried out to determine which radial geometry being critical to the fatigue life.</p><p>The results show that the currently used methods and gauges do not provide the required accuracy when measuring the toe radius. The gauges are handled differently by different operators – even when using the vision system – which makes the methods subjective and therefore unreliable. There are measuring systems that can gather surface data along the weld with high accuracy, but there is no reliable method to assess the data. Therefore, the authors have developed an algorithm – named STELIN – that assess the gathered surface data and automatically identifies and calculates the toe radius and the toe angle along the weld. Using that information an opportunity to improve the process control when welding is possible.</p><p>The performed FE-calculations show that the surface roughness in the weld toe probably has an influence at the fatigue life of the joint. A more precise separate study should be made to determine the impact of the surface roughness on the fatigue life. Those results should serve as a base when reviewing the theory used when predicting the fatigue life. Currently, stress averaging approach is used in the notches of the root and the weld toe. In the future though, there might be another stress condition to be taken into account, if the goal of reducing weight of the finished product shall be achieved. Regarding measuring the surface roughness in the weld toe, the evaluated vision system has enough accuracy to deliver reliable data.</p><p>More work remains with the STELIN-algorithm. The method used when assessing the calculated toe radii should be based on the conclusions from the performed FE-calculations. Integrating the STELIN-algorithm in a fast feedback measurement system – for instance, on a laser – will probably provide good opportunities for a better process control in order to achieve higher fatigue life of the welded joint.</p>
----------------------------------------------------------------------
In diva2:1800339 abstract is:
<p>Coil supports are integral load-bearing components employed in generators andmotors. They serve the purpose of preventing excessive deformation and maintaininga stable position of the coils responsible for generating power and magnetic fieldswhen rotating. However, a problem with these coil supports is that they block theairflow aimed to cool the coils. Thus, this master thesis aimed to conduct a topologyoptimization to develop a cooling-air permeable coil support and select a suitablematerial. The new design was required to withstand 30,000 operational cycles andan overspeed test running at 120% speed without plastic deformation or failure.</p><p>The material selection process was initiated and based on mechanical and physicalproperties requirements. One of these was that the material should be non-magnetic.Utilizing Ansys Granta EduPack, two materials were suggested, the reference materialcurrently used for the coil support, and a titanium alloy, Ti-6Al-4V. The subsequentstep was to create a CAD model of the original design based on technical drawingsprovided by ABB. With the generated design, finite element analysis (FEA) simulationand the topology optimization could be performed. The generated topology optimizedmodel was modified and two new models were created, one with smaller central cutoutsand one with larger central cutouts and a top surface cutout. Furthermore, a thirdmodel was created based on the fundamentals of fluid mechanics, the Rounded originalmodel. Computational Fluid Dynamics (CFD) simulations of the four models wereexecuted.</p><p>The findings indicate that the design with larger central cutouts exhibited the mostsubstantial increase in airflow through and in between the coil supports, achieving a122 % improvement compared to the original design. The model satisfied the fatiguerequirement and successfully passed the overspeed test. Both the current referencematerial and the Ti-6Al4V alloy are suitable to use for coil support. However, theutilization of a titanium alloy might be deemed excessive in terms of its mechanicalproperties and cost.</p>

corrected abstract:
<p>Coil supports are integral load-bearing components employed in generators and motors. They serve the purpose of preventing excessive deformation and maintaining a stable position of the coils responsible for generating power and magnetic fields when rotating. However, a problem with these coil supports is that they block the airflow aimed to cool the coils. Thus, this master thesis aimed to conduct a topology optimization to develop a cooling-air permeable coil support and select a suitable material. The new design was required to withstand 30,000 operational cycles and an overspeed test running at 120% speed without plastic deformation or failure.</p><p>The material selection process was initiated and based on mechanical and physical properties requirements. One of these was that the material should be non-magnetic. Utilizing Ansys Granta EduPack, two materials were suggested, the reference material currently used for the coil support, and a titanium alloy, Ti-6Al-<sub>4</sub>V. The subsequent step was to create a CAD model of the original design based on technical drawings provided by ABB. With the generated design, finite element analysis (FEA) simulation and the topology optimization could be performed. The generated topology optimized model was modified and two new models were created, one with smaller central cutouts and one with larger central cutouts and a top surface cutout. Furthermore, a third model was created based on the fundamentals of fluid mechanics, the Rounded original model. Computational Fluid Dynamics (CFD) simulations of the four models were executed.</p><p>The findings indicate that the design with larger central cutouts exhibited the most substantial increase in airflow through and in between the coil supports, achieving a 122 % improvement compared to the original design. The model satisfied the fatigue requirement and successfully passed the overspeed test. Both the current reference material and the Ti-6Al<sub>4</sub>V alloy are suitable to use for coil support. However, the utilization of a titanium alloy might be deemed excessive in terms of its mechanical properties and cost.</p>
----------------------------------------------------------------------
In diva2:1701306 abstract is:
<p>A big focus of the Automotive Industry’s work is now on the development ofAutonomous Vehicles (AVs). In order to be able to release them on the market,they need to be tested and validated in a safe and efficient way. That is whycompanies working on the development of AVs use simulation to test the workthey are completing. Before putting an Autonomous Vehicle on any road, itwould be ideal to make sure, it will be able to navigate through the given roadsafely and react to everything that has ever happened on this road. What ismore, for the Autonomous Vehicle to be safely on the roads, it should also beable to react to uncommon situations, not seen exactly in the data it was trainedon before. In this thesis, the focus is on trajectories and their variations. Theaim of this work is to develop a framework, which would allow, having discretedata of traffic participants from chosen locations, to model the trajectories ofthose vehicles and the variations of those trajectories. This is to help withthe testing of Autonomous Vehicles in a simulation environment. The data,which is used to develop this method are from an intersection in Denmark,however, it is believed the method can be applied to data from anywhere,as long as it contains information about x and y coordinates of the vehiclesand the corresponding times of the vehicles being at those positions. In thiswork, only trajectories of cars are considered, but again other vehicles can betaken into account in the future. First, vehicle trajectories from given data aremodelled with the use of B-splines. The routine is set up as a constrainedoptimization problem with seven different constraints developed for a car.The constraints are highly nonlinear and therefore a constrained nonlinearoptimization problem is solved. The chosen method for this is the interior-pointmethod. After obtaining the approximation of the trajectory in the Bsplineform, a variation of it is achieved through the change of the speed of thevehicle and its initial position. A projection of the required velocity change onthe derivative of B-spline basis space is calculated and then a new variation ofthe original approximated trajectory in B-spline form is obtained. The methodwas implemented in Matlab and successfully used to approximate and varytrajectories from a dataset from an intersection in Denmark, Aalborg.</p>

corrected abstract:
<p>A big focus of the Automotive Industry’s work is now on the development of Autonomous Vehicles (AVs). In order to be able to release them on the market, they need to be tested and validated in a safe and efficient way. That is why companies working on the development of AVs use simulation to test the work they are completing. Before putting an Autonomous Vehicle on any road, it would be ideal to make sure, it will be able to navigate through the given road safely and react to everything that has ever happened on this road. What is more, for the Autonomous Vehicle to be safely on the roads, it should also be able to react to uncommon situations, not seen exactly in the data it was trained on before. In this thesis, the focus is on trajectories and their variations. The aim of this work is to develop a framework, which would allow, having discrete data of traffic participants from chosen locations, to model the trajectories of those vehicles and the variations of those trajectories. This is to help with the testing of Autonomous Vehicles in a simulation environment. The data, which is used to develop this method are from an intersection in Denmark, however, it is believed the method can be applied to data from anywhere, as long as it contains information about x and y coordinates of the vehicles and the corresponding times of the vehicles being at those positions. In this work, only trajectories of cars are considered, but again other vehicles can be taken into account in the future. First, vehicle trajectories from given data are modelled with the use of B-splines. The routine is set up as a constrained optimization problem with seven different constraints developed for a car. The constraints are highly nonlinear and therefore a constrained nonlinear optimization problem is solved. The chosen method for this is the interior-point method. After obtaining the approximation of the trajectory in the B-spline form, a variation of it is achieved through the change of the speed of the vehicle and its initial position. A projection of the required velocity change on the derivative of B-spline basis space is calculated and then a new variation of the original approximated trajectory in B-spline form is obtained. The method was implemented in Matlab and successfully used to approximate and vary trajectories from a dataset from an intersection in Denmark, Aalborg.</p>
----------------------------------------------------------------------
In diva2:1350191 abstract is:
<p>Bending stiness is one of the most important mechanical properties in paperboard making,giving rigidity to panels and boxes. This property is currently only possible to measure bydestructive measure o the production line. The current quality control method is decient byassuming a non-realistic consistency of the paperboard properties along the machine direction.The objective of this thesis is to predict the thickness and bending stiness of the nal boardsfrom process data.Two modelling approaches are used: the rst model calculates the bending stiness from acalculated thickness, while the other one uses the measured baseboard thickness. Both modelsuse common inputs such as material properties and grammage measurement. The grammage istaken from the online baseboard measurement. The material properties come from laboratorymeasurements and assumptions. It is assumed that the density ratio between the outer andmiddle plies is constant for all product lines, at all times. The TSI of each ply is dened fromtensile testing experiments and nominal bending stiness. It is also assumed that the coatingdoes not contribute to bending stiness. The two models use equations based on laminatetheory assuming orthotropic layers and neglecting the interlaminar shear forces. The modelsuse data of two dierent natures: i.e. laboratory data and online data. Laboratory data is usedas a comparative to evaluate the models' performance of calculated values from online data.The results show various levels of prediction accuracy for dierent paperboard grades. Theaverage thickness predictions are all underestimations within a 5% error while the bendingstiness estimations vary much more from product to product; varying from 9% underestimationto 32% overestimation. The bending stiness prediction for CD is consistently higher thanfor MD for both models. Most product lines have better results with the calculated thickness,approach 1. The calculated thickness is always underestimated and bending stiness is overestimated,hence the better results with the rst approach.The most important conclusion from the models' results is the spread of laboratory measurements,when compared to the predicted values. The large variation most likely comes fromproduction, implying inconsistencies in the manufacturing process that are not accounted forby the models. These modelling approaches have failed to capture the production variationsbecause of the lack of input parameters.</p>

corrected abstract:
<p>Bending stiffness is one of the most important mechanical properties in paperboard making, giving rigidity to panels and boxes. This property is currently only possible to measure by destructive measure off the production line. The current quality control method is deficient by assuming a non-realistic consistency of the paperboard properties along the machine direction. The objective of this thesis is to predict the thickness and bending stiffness of the final boards from process data.</p><p>Two modelling approaches are used: the first model calculates the bending stiffness from a calculated thickness, while the other one uses the measured baseboard thickness. Both models use common inputs such as material properties and grammage measurement. The grammage is taken from the online baseboard measurement. The material properties come from laboratory measurements and assumptions. It is assumed that the density ratio between the outer and middle plies is constant for all product lines, at all times. The TSI of each ply is defined from tensile testing experiments and nominal bending stiffness. It is also assumed that the coating does not contribute to bending stiffness. The two models use equations based on laminate theory assuming orthotropic layers and neglecting the interlaminar shear forces. The models use data of two different natures: i.e. laboratory data and online data. Laboratory data is used as a comparative to evaluate the models' performance of calculated values from online data.</p><p>The results show various levels of prediction accuracy for different paperboard grades. The average thickness predictions are all underestimations within a 5% error while the bending stiffness estimations vary much more from product to product; varying from 9% underestimation to 32% overestimation. The bending stiffness prediction for CD is consistently higher than for MD for both models. Most product lines have better results with the calculated thickness, approach 1. The calculated thickness is always underestimated and bending stiffness is overestimated, hence the better results with the first approach.</p><p>The most important conclusion from the models' results is the spread of laboratory measurements, when compared to the predicted values. The large variation most likely comes from production, implying inconsistencies in the manufacturing process that are not accounted for by the models. These modelling approaches have failed to capture the production variations because of the lack of input parameters.</p>
----------------------------------------------------------------------
In diva2:1183391 abstract is:
<p>Transportation underlines the vehicle industry's critical role in a country's economic future.The amount of goods moved, specically by trucks, is only expected to increase inthe near future. This work attempts to tackle the problem of optimizing fuel consumptionin Volvo trucks, when there are hard constraints on the delivery time and speed limits.Knowledge of the truck such as position, state, conguration etc., along with the completeroute information of the transport mission is used for fuel optimization.Advancements in computation, storage, and communication on cloud based systems, hasmade it possible to easily incorporate such systems in assisting modern eet. In this work,an algorithm is developed in a cloud based system to compute a speed plan for the completemission for achieving fuel minimization. This computation is decoupled from thelocal control operations on the truck such as prediction control, safety, cruise control, etc.;and serves as a guide to the truck driver to reach the destination on time by consumingminimum fuel.To achieve fuel minimization under hard constraints on delivery (or arrival) time andspeed limits, a non-linear optimization problem is formulated for the high delity modelestimated from real-time drive cycles. This optimization problem is solved using a Nonlinearprogramming solver in Matlab.The optimal policy was tested on two drive cycles provided by Volvo. The policy wascompared with two dierent scenarios, where the mission demands hard constraints ontravel time and the speed limits in addition to no trac uncertainties (deterministic). with a cruise controller running at a constant set speed throughout the mission. Itis observed that there is no signicant fuel savings. with maximum possible fuel consumption; achieved without the help of optimalspeed plan (worst case). It is seen that there is a notable improvement in fuelsaving.In a real world scenario, a transport mission is interrupted by uncertainties such as trac ow, road blocks, re-routing, etc. To this end, a stochastic optimization algorithm is proposedto deal with the uncertainties modeled using historical trac ow data. Possiblesolution methodologies are suggested to tackle this stochastic optimization problem.</p>


corrected abstract:
<p>Transportation underlines the vehicle industry's critical role in a country's economic future. The amount of goods moved, specifically by trucks, is only expected to increase in the near future. This work attempts to tackle the problem of optimizing fuel consumption in Volvo trucks, when there are hard constraints on the delivery time and speed limits. Knowledge of the truck such as position, state, configuration etc., along with the complete route information of the transport mission is used for fuel optimization.</p><p>Advancements in computation, storage, and communication on cloud based systems, has made it possible to easily incorporate such systems in assisting modern fleet. In this work, an algorithm is developed in a cloud based system to compute a speed plan for the complete mission for achieving fuel minimization. This computation is decoupled from the local control operations on the truck such as prediction control, safety, cruise control, etc.; and serves as a guide to the truck driver to reach the destination on time by consuming minimum fuel.</p><p>To achieve fuel minimization under hard constraints on delivery (or arrival) time and speed limits, a non-linear optimization problem is formulated for the high fidelity model estimated from real-time drive cycles. This optimization problem is solved using a Nonlinear programming solver in Matlab.</p><p>The optimal policy was tested on two drive cycles provided by Volvo. The policy was compared with two different scenarios, where the mission demands hard constraints on travel time and the speed limits in addition to no traffic uncertainties (deterministic).</p><ul><li>with a cruise controller running at a constant set speed throughout the mission. It is observed that there is no significant fuel savings.</li><li>with maximum possible fuel consumption; achieved without the help of optimal speed plan (worst case). It is seen that there is a notable improvement in fuel saving.</li></ul><p>In a real world scenario, a transport mission is interrupted by uncertainties such as traffic flow, road blocks, re-routing, etc. To this end, a stochastic optimization algorithm is proposed to deal with the uncertainties modeled using historical traffic flow data. Possible solution methodologies are suggested to tackle this stochastic optimization problem.</p>
----------------------------------------------------------------------
In diva2:1142952 abstract is:
<p>Sending bacteria to space is a further step withinthe framework of transporting humans to distant locations inspace. This can build a knowledge platform of how the bacteriabehaves in the space environments, in order to be able to functionin the long term as a LLS (long term life support system), i.ea mini ecology for the space station that handles waste (gas,liquid and solid) and transforms it into food, water and oxygen.By constructing a bacterial experiment (MoreBac) in a smallsatellite and thermally simulating it in space environment, itcan aid future projects performed in similar but larger scales.To visualize the experiment in presentations, a CAD-model ofthe experiment will be designed and constructed in SIEMENSSolid Edge. The thermal analysis is made in Airbus SYSTEMAThermica and will help show on the critical problem, which isto maintain suitable temperature conditions on the microfluidicchip inside the experiment. By performing the simulations, onecan assure that the design is suitable and that the heat gradientis in required intervals for different components. The CADmodelwas designed in a sandwich layout and consist of twoprinted circuit boards, one microfluidic chip and one reservoir.Not specified components of the experiment was not used in theCAD- model since they where still in early development. Thethermal analysis of the experiment was studied in a steady stateenvironment, with boundary conditions of 5˝C in the cold caseand 30˝C in the hot case, which means that the time variablewas not considered. Three configurations of heat dissipation weremade; 16 nodes at the illumination board with 0,05 W each, 16nodes at the detection board with 0,05 W each and finally 36nodes on both PCBs together with 0,025 W each. In the hot case,the microfluidic chip reaches temperatures between 34, 16˝C and42, 15˝C when 0,8 W is equally divided to both PCBs. In thecold case, the microfluidic chip reaches temperatures between13, 82˝C and 22, 32˝C with the same heat distribution as thehot case.</p>

corrected abstract:
<p>Sending bacteria to space is a further step within the framework of transporting humans to distant locations in space. This can build a knowledge platform of how the bacteria behaves in the space environments, in order to be able to function in the long term as a LLS (long term life support system), i.e a mini ecology for the space station that handles waste (gas, liquid and solid) and transforms it into food, water and oxygen. By constructing a bacterial experiment (MoreBac) in a small satellite and thermally simulating it in space environment, it can aid future projects performed in similar but larger scales. To visualize the experiment in presentations, a CAD-model of the experiment will be designed and constructed in SIEMENS Solid Edge. The thermal analysis is made in Airbus SYSTEMA Thermica and will help show on the critical problem, which is to maintain suitable temperature conditions on the microfluidic chip inside the experiment. By performing the simulations, one can assure that the design is suitable and that the heat gradient is in required intervals for different components. The CAD-model was designed in a sandwich layout and consist of two printed circuit boards, one microfluidic chip and one reservoir. Not specified components of the experiment was not used in the CAD- model since they where still in early development. The thermal analysis of the experiment was studied in a steady state environment, with boundary conditions of 5˝C in the cold case and 30˝C in the hot case, which means that the time variable was not considered. Three configurations of heat dissipation were made; 16 nodes at the illumination board with 0,05 W each, 16 nodes at the detection board with 0,05 W each and finally 36 nodes on both PCBs together with 0,025 W each. In the hot case, the microfluidic chip reaches temperatures between 34,16˝C and 42,15˝C when 0,8 W is equally divided to both PCBs. In the cold case, the microfluidic chip reaches temperatures between 13,82˝C and 22,32˝C with the same heat distribution as the hot case.</p>
----------------------------------------------------------------------
In diva2:1083778 error in title:
"Pressure measurements in pulsatingflows"
==>
"Pressure measurements in pulsating flows"

abstract is:
<p>Due to confidentiality several axis in the figures and large parts of the specifics of the resultsand of the experimental setups have been replaced by symbols. Also one section of the report,concerning a prototype sensor, has been removed completely due to the sensitive nature of theresults.Measuring the exhaust gas pressure and the boost pressure at the air intake manifold isconsidered a standard procedure in modern cars and trucks. Although how to measure thepressure accurately for steady flows is well known, the pressure measurements in pulsatingflows is not a trivial task. This theses shows, experimentally, how well the characteristics of apressure measurement systems, using different dimensions of straight pneumatic tubing, canbe predicted using the Helmholtz resonator model. Also how much this resonance influencethe pressure measurements for different pressure transducers used in trucks today. This thesisalso demonstrates the effects that the sampling frequency and the averaging time has on theaccuracy of measuring an average pressure in pulsating gas flows and how clogging of thepneumatic tubes influence the measurements. This was done using two types of experiments;a step response experiment to properly show the characteristics of the measuring system and apulse rig experiment that shows the impact, of the tubing, on the measurements for typicalfrequencies found in medium sized trucks. The experiments shows that the response time andresonance frequency of a measurement system can be predicted with an accuracy of 𝜇! % fortubes longer than 725 mm. It also that the average absolute pressure measurement keeps anaccuracy of 𝜇! % for all tube dimensions, including clogging of the tube with a decrease ofdiameter up to 𝜇! %. It does however show that if the sensor has some internal resonance that4matches the Helmholtz resonance the measurement can be overestimated by over 𝜇! %. Testsof the sampling frequency shows that if the sampling frequency is chosen as a divisor or amultiple of the pulse frequency the error due to averaging is increased by one order ofmagnitude. Using the information given in this thesis it is possible to avoid unnecessary errorswhen performing pressure measurements in a pulsating flow.</p>


corrected abstract:
<p>Due to confidentiality several axis in the figures and large parts of the specifics of the results and of the experimental setups have been replaced by symbols. Also one section of the report, concerning a prototype sensor, has been removed completely due to the sensitive nature of the results.</p><p>Measuring the exhaust gas pressure and the boost pressure at the air intake manifold is considered a standard procedure in modern cars and trucks. Although how to measure the pressure accurately for steady flows is well known, the pressure measurements in pulsating flows is not a trivial task. This theses shows, experimentally, how well the characteristics of a pressure measurement systems, using different dimensions of straight pneumatic tubing, can be predicted using the Helmholtz resonator model. Also how much this resonance influence the pressure measurements for different pressure transducers used in trucks today. This thesis also demonstrates the effects that the sampling frequency and the averaging time has on the accuracy of measuring an average pressure in pulsating gas flows and how clogging of the pneumatic tubes influence the measurements. This was done using two types of experiments; a step response experiment to properly show the characteristics of the measuring system and a pulse rig experiment that shows the impact, of the tubing, on the measurements for typical frequencies found in medium sized trucks. The experiments shows that the response time and oresonance frequency of a measurement system can be predicted with an accuracy of µ<sub>1</sub> % for tubes longer than 725 mm. It also that the average absolute pressure measurement keeps an accuracy of µ<sub>2</sub> % for all tube dimensions, including clogging of the tube with a decrease of diameter up to µ<sub>3</sub> %. It does however show that if the sensor has some internal resonance that matches the Helmholtz resonance the measurement can be overestimated by over µ<sub>4</sub> %. Tests of the sampling frequency shows that if the sampling frequency is chosen as a divisor or a multiple of the pulse frequency the error due to averaging is increased by one order of magnitude. Using the information given in this thesis it is possible to avoid unnecessary errors when performing pressure measurements in a pulsating flow.</p>
----------------------------------------------------------------------
In diva2:860546 abstract is:
<p>Ankylosing Spondylitis (AS), or Bechterew’s disease, is an inflammatory rheumaticdisease that through the formation of additional bone tissue in the spine eventuallyleads to the complete fusion of the vertebrae, in effect turning the spine into one longbone. Due to the reduced flexibility of the spine with the long lever arms, spinalfractures in AS-patients are relatively common even after minor trauma.</p><p>The aim of this thesis was to use an existing finite element model of a healthy spineand adapt it to the conditions of AS, thus gaining some insight into the effects ofsurgical stabilization of cervical fractures, using posterior screws and rods. Althoughthis type of surgery is often performed, it has not been previously investigated in abiomechanical model. This thesis should be considered as a starting point for how afinite element model of the spine could be used to investigate the effect of spinalimplants in the case of a fracture in the ankylosed spine.</p><p>An existing FE-model was modified to some of the conditions of AS: The vertebraewere fused by adding ossifications at the intervertebral discs (with the Head-C1 andC1-C2 joints left mobile). A fracture was simulated at the C6C7 disc level. Fourdifferent implant configurations were tested: Short instrumentation C6C7, mediuminstrumentation C5toT1, long instrumentation C3toT3, and a long instrumentationC3C6C7T3 with skipped intermediate levels. Three loads (1.5g, 3.0g, 4.5g) wereapplied according to a specific load curve. Kinematic data such as the gap distance inthe fracture site were obtained. Furthermore the stresses in the ossified parts of thediscs were evaluated.</p><p>It was shown that the chosen methods of adapting the model to the AS conditions, andmodeling the fracture and implant, changed the kinematics so that less movementoccurred between the vertebra, which is typical for AS. Measured as fracture gap, alltested implant configurations were equally good at stabilizing the fracture, althoughthey all allowed more movement than the non-fractured AS-model did. All implantconfigurations were also able to stabilize the fracture in terms of the horizontal translation in the fracture. The disc ossifications were somewhat shielded from stress for those ossifications that were within the range of the implant. This was so for all implant configurations. No increased stress was observed in the ossifications immediately outside the range for the implants, relative the non-fractured AS-model.</p><p>For the C6C7 and C5toT1 implant configurations as well as the non-fractured ASmodel,the stresses were highest at the T1T2 level. Stresses in the ossifications in the thoracic spine were generally low, apart from the T1T2 level. The results show that the chosen AS-adaptations and the modeled implant seem reasonable for testing some of the considerations of cervical fractures in the ankylosed spine as well as for some implant configurations. The results also make it possible to speculate about the optimal type of implant. The effects of screw placement and anchoring, osteoporosis, muscle activation and possible spinal deformity on the implant stability were not investigated, and should be a matter for further studies.</p>

corrected abstract:
<p>Ankylosing Spondylitis (AS), or Bechterew’s disease, is an inflammatory rheumatic disease that through the formation of additional bone tissue in the spine eventually leads to the complete fusion of the vertebrae, in effect turning the spine into one long bone. Due to the reduced flexibility of the spine with the long lever arms, spinal fractures in AS-patients are relatively common even after minor trauma.</p><p>The aim of this thesis was to use an existing finite element model of a healthy spine and adapt it to the conditions of AS, thus gaining some insight into the effects of surgical stabilization of cervical fractures, using posterior screws and rods. Although this type of surgery is often performed, it has not been previously investigated in a biomechanical model. This thesis should be considered as a starting point for how a finite element model of the spine could be used to investigate the effect of spinal implants in the case of a fracture in the ankylosed spine.</p><p>An existing FE-model was modified to some of the conditions of AS: The vertebrae were fused by adding ossifications at the intervertebral discs (with the Head-C1 and C1-C2 joints left mobile). A fracture was simulated at the C6C7 disc level. Four different implant configurations were tested: Short instrumentation C6C7, medium instrumentation C5toT1, long instrumentation C3 to T3, and a long instrumentation C3C6C7T3 with skipped intermediate levels. Three loads (1.5g, 3.0g, 4.5g) were applied according to a specific load curve. Kinematic data such as the gap distance in the fracture site were obtained. Furthermore the stresses in the ossified parts of the discs were evaluated.</p><p>It was shown that the chosen methods of adapting the model to the AS conditions, and modeling the fracture and implant, changed the kinematics so that less movement occurred between the vertebra, which is typical for AS. Measured as fracture gap, all tested implant configurations were equally good at stabilizing the fracture, although they all allowed more movement than the non-fractured AS-model did. All implant configurations were also able to stabilize the fracture in terms of the horizontal translation in the fracture. The disc ossifications were somewhat shielded from stress for those ossifications that were within the range of the implant. This was so for all implant configurations. No increased stress was observed in the ossifications immediately outside the range for the implants, relative the non-fractured AS-model.</p><p>For the C6C7 and C5toT1 implant configurations as well as the non-fractured AS-model, the stresses were highest at the T1T2 level. Stresses in the ossifications in the thoracic spine were generally low, apart from the T1T2 level.</p><p>The results show that the chosen AS-adaptations and the modeled implant seem reasonable for testing some of the considerations of cervical fractures in the ankylosed spine as well as for some implant configurations. The results also make it possible to speculate about the optimal type of implant. The effects of screw placement and anchoring, osteoporosis, muscle activation and possible spinal deformity on the implant stability were not investigated, and should be a matter for further studies.</p>
----------------------------------------------------------------------
In diva2:643818 abstract is:
<p>This master thesis, which has been carried out in collaboration with Fairtrade,investigates how sustainable development can be integrated in the mathematicaldiscipline in the Swedish upper secondary school. The study includedfindings on design and disposition, with sustainability themes relatedto Fairtrade. This since Fairtrade is going to use the study as a basis forthe development of an educational material. The results of the study areof interest to teachers and organizations that want to develop educationalmaterial that integrate sustainable development in mathematics education.The main focus of the study has been the teachers’ preferences on dispositionand design.Semi-structured interviews were conducted with seven mathematics teachersin the Stockholm area. A workshop was also conducted with three mathematicsstudent teachers at the end of their training, and two people workingat Fairtrade. The result of the study shows that it is important that theeducational material corresponds to the achievement goals of the mathematicscourses. It also shows the importance of making the material differsfrom ordinary learning milieu, as this would inspire teachers to vary theirteaching. The results also indicate that mathematics education in the exerciseparadigm is not suitable for integrating sustainable development inmathematics education. Instead an investigative approach and working withtasks referring to real-life situations where the students are encouraged totake a position on questions of sustainable development is preferred. For instance,child labor and poverty can be investigated using mathematics andby linking those problems to consumptions the students can question theirown role as consumers.</p>

corrected abstract:
<p>This master thesis, which has been carried out in collaboration with Fairtrade, investigates how sustainable development can be integrated in the mathematical discipline in the Swedish upper secondary school. The study included findings on design and disposition, with sustainability themes related to Fairtrade. This since Fairtrade is going to use the study as a basis for the development of an educational material. The results of the study are of interest to teachers and organizations that want to develop educational material that integrate sustainable development in mathematics education. The main focus of the study has been the teachers’ preferences on disposition and design.</p><p>Semi-structured interviews were conducted with seven mathematics teachers in the Stockholm area. A workshop was also conducted with three mathematics student teachers at the end of their training, and two people working at Fairtrade. The result of the study shows that it is important that the educational material corresponds to the achievement goals of the mathematics courses. It also shows the importance of making the material differs from ordinary learning milieu, as this would inspire teachers to vary their teaching. The results also indicate that mathematics education in the exercise paradigm is not suitable for integrating sustainable development in mathematics education. Instead an investigative approach and working with tasks referring to real-life situations where the students are encouraged to take a position on questions of sustainable development is preferred. For instance, child labor and poverty can be investigated using mathematics and by linking those problems to consumptions the students can question their own role as consumers.</p>
----------------------------------------------------------------------
In diva2:405926 - missing space in title:
"Development of a dynamic calculation tool forsimulation of ditching"
==>
"Development of a dynamic calculation tool for simulation of ditching"

abstract is:
<p>The present document is the final master thesis report written by Marc PILORGET,student at SUPAERO (home institution) and KTH (Royal Institute of Technology,Exchange University). This six months internship was done at DASSAULT AVIATION(Airframe engineering department) based in Saint-Cloud, France. It spanned from the 5thof July to the 23rd of December. The thesis work aims at developing an SPH (SmoothParticle Hydrodynamics) calculation method for ditching and implementing it in the finiteelement software ELFINI® developed by DASSAULT. Ditching corresponds to a phasewhen the aeroplane is touching the water. The problematic of ditching has always beenan area of interest for DASSAULT and the whole aeronautical industry. So far, only testsand simple analytical calculations have been performed. Most of the work was carried bythe NACA (National Advisory Committee for Aeronautics) in the late 70's. However in thepast decade, a new method for fluid-structure coupling problems has been developed. Itis called SPH. The basic principle is the following: the domain is represented by means ofparticles and each particle of fluid is treated separately and submitted to the Navier-Stokes equations. The particle is influenced by the neighbouring particles with a weightfunction depending on the distance between the two particles. Particles are also placed atthe interface solid-fluid: they are called limit particles. The final purpose of this SPHmethod is to access to the structural response of an aircraft when ditching. The crucialinterest of such a method compared to methods used so far is the absence of mesh. Theanalysis of large deformation problems by the finite element method may require thecontinuous remeshing of the domain to avoid the breakdown of the calculation due toexcessive mesh distortion. When considering ditching or other large deformationsproblems, the mesh generation is a far more time-consuming task than the constructionand solution of a discrete set of equations. For DASSAULT-AVIATION, the long termobjective is to get a numerical tool able to model ditching. The SPH method is used tosolve the equations for the fluid and is coupled with a finite element method for thestructure. So far, the compressible solver for 2D geometries has been implemented.Tests are going to be performed to ensure the program’s robustness. Then theincompressible solver for 2D geometries will be studied both theoretically andnumerically.</p>

corrected abstract:
<p>The present document is the final master thesis report written by Marc PILORGET, student at SUPAERO (home institution) and KTH (Royal Institute of Technology, Exchange University). This six months internship was done at DASSAULT AVIATION (Airframe engineering department) based in Saint-Cloud, France. It spanned from the 5<sup>th</sup> of July to the 23<sup>rd</sup> of December. The thesis work aims at developing an SPH (Smooth Particle Hydrodynamics) calculation method for ditching and implementing it in the finite element software ELFINI® developed by DASSAULT. Ditching corresponds to a phase when the aeroplane is touching the water. The problematic of ditching has always been an area of interest for DASSAULT and the whole aeronautical industry. So far, only tests and simple analytical calculations have been performed. Most of the work was carried by the NACA (National Advisory Committee for Aeronautics) in the late 70's. However in the past decade, a new method for fluid-structure coupling problems has been developed. It is called SPH. The basic principle is the following: the domain is represented by means of particles and each particle of fluid is treated separately and submitted to the Navier-Stokes equations. The particle is influenced by the neighbouring particles with a weight function depending on the distance between the two particles. Particles are also placed at the interface solid-fluid: they are called limit particles. The final purpose of this SPH method is to access to the structural response of an aircraft when ditching. The crucial interest of such a method compared to methods used so far is the absence of mesh. The analysis of large deformation problems by the finite element method may require the continuous remeshing of the domain to avoid the breakdown of the calculation due to excessive mesh distortion. When considering ditching or other large deformations problems, the mesh generation is a far more time-consuming task than the construction and solution of a discrete set of equations. For DASSAULT-AVIATION, the long term objective is to get a numerical tool able to model ditching. The SPH method is used to solve the equations for the fluid and is coupled with a finite element method for the structure. So far, the compressible solver for 2D geometries has been implemented. Tests are going to be performed to ensure the program’s robustness. Then the incompressible solver for 2D geometries will be studied both theoretically and numerically.
----------------------------------------------------------------------
In diva2:1707348 abstract is:
<p>This thesis studies the implementation of an Explicit Algebraic Reynolds-Stress Model(EARSM) for Atmospheric Boundary Layer (ABL) in an open source ComputationalFluid Dynamics (CFD) software, OpenFOAM, following the guidance provided by thewind company ENERCON that aims to make use of this novel model to improvesites’ wind-field predictions. After carefully implementing the model in OpenFOAM,the EARSM implementation is verified and validated by testing it with a stratifiedCouette flow case. The former was done by feeding mean flow properties, takenfrom OpenFOAM, in a python tool containing the full EARSM system of equationsand constants, and comparing the resulting flux profiles with the ones extracted bythe OpenFOAM simulations. Subsequently, the latter was done by comparing theprofiles of the two universal functions used by Monin-Obukhov Similarity Theory(MOST) for mean velocity and temperature to the results obtained by Želi et al. intheir study of the EARSM applied to a single column ABL, in “Modelling of stably-stratified, convective and transitional atmospheric boundary layers using the explicitalgebraic Reynolds-stress model” (2021). The verification of the model showed minordifferences between the flux profiles from the python tool and OpenFOAM thus, themodel’s implementation was deemed verified, while the validation step showed nodifference in the unstable and neutral stratification cases, but a significant discrepancyfor stably stratified flow. Nonetheless, the reason behind the inconsistency is believedto be related to the choice of boundary conditions thus, the model’s implementationitself is considered validated.</p><p>Finally, the comparison between the EARSM and the k − ε model showed thatthe former is able to capture the physics of the flow properties where the latter failsto. In particular, the diagonal momentum fluxes resulting from the EARSM reflectthe observed behaviour of being different from each other, becoming isotropic withaltitude in the case of unstable stratification, and having magnitude u′u′ &gt; v′v′ &gt; w′w′ for stably stratified flows. On the other hand, the eddy viscosity assumption used bythe k − ε model computes the diagonal momentum fluxes as being equal to each other.Moreover, the EARSM captures more than one non-zero heat flux component in theCouette flow case, which has been observed to be the case in literature, while the eddydiffusivity assumption used by the k − ε model only accounts for one non-zero heat fluxcomponent.</p><p> </p>

corrected abstract:
<p>This thesis studies the implementation of an Explicit Algebraic Reynolds-Stress Model (EARSM) for Atmospheric Boundary Layer (ABL) in an open source Computational Fluid Dynamics (CFD) software, OpenFOAM, following the guidance provided by the wind company ENERCON that aims to make use of this novel model to improve sites’ wind-field predictions. After carefully implementing the model in OpenFOAM, the EARSM implementation is verified and validated by testing it with a stratified Couette flow case. The former was done by feeding mean flow properties, taken from OpenFOAM, in a python tool containing the full EARSM system of equations and constants, and comparing the resulting flux profiles with the ones extracted by the OpenFOAM simulations. Subsequently, the latter was done by comparing the profiles of the two universal functions used by Monin-Obukhov Similarity Theory (MOST) for mean velocity and temperature to the results obtained by Želi et al. in their study of the EARSM applied to a single column ABL, in “Modelling of stably-stratified, convective and transitional atmospheric boundary layers using the explicit algebraic Reynolds-stress model” (2021). The verification of the model showed minor differences between the flux profiles from the python tool and OpenFOAM thus, the model’s implementation was deemed verified, while the validation step showed no difference in the unstable and neutral stratification cases, but a significant discrepancy for stably stratified flow. Nonetheless, the reason behind the inconsistency is believed to be related to the choice of boundary conditions thus, the model’s implementation itself is considered validated.</p><p>Finally, the comparison between the EARSM and the <em>k − ϵ</em> model showed that the former is able to capture the physics of the flow properties where the latter fails to. In particular, the diagonal momentum fluxes resulting from the EARSM reflect the observed behaviour of being different from each other, becoming isotropic with altitude in the case of unstable stratification, and having magnitude <em><span style="border-top: 1px solid black; padding: 0.2rem;">u&prime;u&prime;</span></em> &gt; <em><span style="border-top: 1px solid black; padding: 0.2rem;">v&prime;v&prime;</span></em> &gt; <em><span style="border-top: 1px solid black; padding: 0.2rem;">w&prime;w&prime;</span></em> for stably stratified flows. On the other hand, the eddy viscosity assumption used by the <em>k − ϵ</em> model computes the diagonal momentum fluxes as being equal to each other. Moreover, the EARSM captures more than one non-zero heat flux component in the Couette flow case, which has been observed to be the case in literature, while the eddy diffusivity assumption used by the <em>k − ϵ</em> model only accounts for one non-zero heat flux component.</p>


Note: In the corrected abstract I could not use an overline, as the primes punctured the overline; therefore, I used the border-top echanism and raised by adding more padding.
----------------------------------------------------------------------
In diva2:1228966 abstract is:
<p>Much research is done today on how to make vehicles autonomous. But the main focuslies in how to make the techniques and safety sufficient. This means that the comfort forthe passengers has fallen behind. Studies show that approximately 25 % of the users ofautonomous vehicles would experience motion sickness.The purpose of this thesis is to use existing hypotheses about what causes motion sicknessto analyse different technical solutions that could decrease motion sickness inautonomous vehicles.To investigate this a literature study is done. Only existing research and experiments areused.The report is based on the theories about the sensor conflict and postural instability. Thetheory about the sensor conflict means that a person gets symptoms of motion sicknesswhen the visual impression doesn’t match with the ones from the balance organs. Thetheory about postural instability says that the motion sickness is caused when the braindoesn’t have control over the posture of the body.The different solutions analysed were active suspension, installing a screen on which thepassengers do all the activities and a Virtual Reality headset where the passengers gets apreview of the road to make their body prepare for the movement of the cars so that theycan make a countermovement. The conclusions are that the VR-headset can’t be usedbecause it limits the user possibilities to do other activities while traveling. The screen inthe middle of the passenger’s view can work for the activities that can be done using ascreen but doesn’t work for other activities. The solution with active suspension is apromising solution but perhaps a bit expensive.Finally a new solution to the problem is presented by the authors. The new solution isactive suspension of the car seats and is a mix of the two analysed solutions VirtualRealityheadset and active suspension.</p>


corrected abstract:
<p>Much research is done today on how to make vehicles autonomous. But the main focus lies in how to make the techniques and safety sufficient. This means that the comfort for the passengers has fallen behind. Studies show that approximately 25 % of the users of autonomous vehicles would experience motion sickness.</p><p>The purpose of this thesis is to use existing hypotheses about what causes motion sickness to analyse different technical solutions that could decrease motion sickness in autonomous vehicles.</p><p>To investigate this a literature study is done. Only existing research and experiments are used.</p><p>The report is based on the theories about the sensor conflict and postural instability. The theory about the sensor conflict means that a person gets symptoms of motion sickness when the visual impression doesn’t match with the ones from the balance organs. The theory about postural instability says that the motion sickness is caused when the brain doesn’t have control over the posture of the body.</p><p>The different solutions analysed were active suspension, installing a screen on which the passengers do all the activities and a Virtual Reality headset where the passengers gets a preview of the road to make their body prepare for the movement of the cars so that they can make a countermovement. The conclusions are that the VR-headset can’t be used because it limits the user possibilities to do other activities while traveling. The screen in the middle of the passenger’s view can work for the activities that can be done using a screen but doesn’t work for other activities. The solution with active suspension is a promising solution but perhaps a bit expensive.</p><p>Finally a new solution to the problem is presented by the authors. The new solution is active suspension of the car seats and is a mix of the two analysed solutions Virtual Reality headset and active suspension.</p>
----------------------------------------------------------------------
In diva2:408836 abstract is:
<p>This study illuminates how the science center as a concept can be developed and how asociocultural perspective on learning influences the design of an interactive exhibit. The aimof the study is to propose ideas on how a science center can be designed and developed withthe purpose of creating good conditions for learning.The work was divided in three parts. In the first part literature was studied with theaim of highlighting aspects important for learning from a sociocultural perspective. In thesecond part an educational model was formulated based on the result from the literature study,interviews and study visits. The educational model was then used to guide the design of aninteractive exhibit on hydro power. The third part consists of an evaluation of the exhibitbased on observation of the visitors’ interaction with the exhibit.In this thesis the work and the result of the three parts are presented leading to a finaldiscussion about the key question of the study: How can learning possibilities be createdthrough the design of an interactive exhibition?Creating possibilities for learning is about creating possibilities for activities thatmakes learning possible. Through the design and in the choice of content of an exhibition it ispossible to create more or less good conditions for learning. Therefore it is important to havea clear picture of what type of activities it is desirable that an exhibition invite the visitor to,for example cooperation and conversation and the visitors’ possibility to influence the resultof the activity.The traditional science center is often criticized for presenting science and technologyas something static and finished. To create interest and engagement for the subject area it isinstead needed to be presented from a wide range of perspectives. Therefore throughout thework of developing an interactive exhibition or a science center it is important to discuss howthe subject area can be presented to fulfil this aim.Creating and developing an exhibition is a work that involves a number of people withdifferent backgrounds, knowledge and ideas. Just as in any other project that involves manypeople a clear ambition with clear goals is needed and makes a shared vision possible that canlead and steer all parts of the work.</p>

corrected abstract:
<p>This study illuminates how the science center as a concept can be developed and how a sociocultural perspective on learning influences the design of an interactive exhibit. The aim of the study is to propose ideas on how a science center can be designed and developed with the purpose of creating good conditions for learning.</p><p>The work was divided in three parts. In the first part literature was studied with the aim of highlighting aspects important for learning from a sociocultural perspective. In the second part an educational model was formulated based on the result from the literature study, interviews and study visits. The educational model was then used to guide the design of an interactive exhibit on hydro power. The third part consists of an evaluation of the exhibit based on observation of the visitors’ interaction with the exhibit.</p><p>In this thesis the work and the result of the three parts are presented leading to a final discussion about the key question of the study: How can learning possibilities be created through the design of an interactive exhibition?</p><p>Creating possibilities for learning is about creating possibilities for activities that makes learning possible. Through the design and in the choice of content of an exhibition it is possible to create more or less good conditions for learning. Therefore it is important to have a clear picture of what type of activities it is desirable that an exhibition invite the visitor to, for example cooperation and conversation and the visitors’ possibility to influence the result of the activity.</p><p>The traditional science center is often criticized for presenting science and technology as something static and finished. To create interest and engagement for the subject area it is instead needed to be presented from a wide range of perspectives. Therefore throughout the work of developing an interactive exhibition or a science center it is important to discuss how the subject area can be presented to fulfil this aim.</p><p>Creating and developing an exhibition is a work that involves a number of people with different backgrounds, knowledge and ideas. Just as in any other project that involves many people a clear ambition with clear goals is needed and makes a shared vision possible that can lead and steer all parts of the work.</p>
----------------------------------------------------------------------
In diva2:401129 abstract is:
<p>CFD use has increased signi cantly in airplane conception, and the industry demands more andmore precise and reliable tools. This was the goal of the SimSAC project. The result is CEASIOM,a computerized environment made of several modules for the design and prediction of the aircraft'scharacteristics. It constructs aerodynamic tables used in the prediction of the characteristics of anaircraft. In simple ight conditions, simple computation methods are used, whereas in complex ightconditions,involving turbulences, more advanced methods are used. This reduces the computationalcost, but the tables resulting from di erent delity sources must be fused to obtain a coherent tablecovering the whole ight envelope.The goal of this project was to realize the fusion. Additionally, a lter and a custom-made mapping toenhance the accuracy of the results from the fusion were required. The addition of helpful visualizationtools was suggested. The whole should be integrated in the CEASIOM interface as a Fusion module.For this, 6 functions were coded. The rst one loads the data sets. The second, myplot, allows theengineer by plotting the data in a coherent way, to spot any big mistakes or incompatibility in thedata sets. The third, myvisual, displays the elements spotted as outliers or potentially out of pattern.This is used by the next function, my ltermap, to lter out the erroneous data. This function alsorealizes the custom-made mapping.The fth function, myfusion, fuses the data and saves it in a .xmlCEASIOM formatted structure to be used by the next CEASIOM module. The sixth function ltersout, in the same way as my ltermap, the outliers from the fused data, and saves the ltered fused dataset in a .xml CEASIOM formatted structure. Finally, a Matlab GUI was implemented and integratedinto the main CEASIOM interface.The module works perfectly, except for the mapping part, that needs a few readjustments.</p>


corrected abstract:
<p>CFD use has increased significantly in airplane conception, and the industry demands more and more precise and reliable tools. This was the goal of the SimSAC project. The result is CEASIOM, a computerized environment made of several modules for the design and prediction of the aircraft's characteristics. It constructs aerodynamic tables used in the prediction of the characteristics of an aircraft. In simple flight conditions, simple computation methods are used, whereas in complex flight conditions, involving turbulences, more advanced methods are used. This reduces the computational cost, but the tables resulting from different fidelity sources must be fused to obtain a coherent table covering the whole flight envelope.</p><p>The goal of this project was to realize the fusion. Additionally, a filter and a custom-made mapping to enhance the accuracy of the results from the fusion were required. The addition of helpful visualization tools was suggested. The whole should be integrated in the CEASIOM interface as a Fusion module. For this, 6 functions were coded. The first one loads the data sets. The second, myplot, allows the engineer by plotting the data in a coherent way, to spot any big mistakes or incompatibility in the data sets. The third, myvisual, displays the elements spotted as outliers or potentially out of pattern. This is used by the next function, myfiltermap, to filter out the erroneous data. This function also realizes the custom-made mapping. The fifth function, myfusion, fuses the data and saves it in a .xml CEASIOM formatted structure to be used by the next CEASIOM module. The sixth function filters out, in the same way as myfiltermap, the outliers from the fused data, and saves the filtered fused data set in a .xml CEASIOM formatted structure. Finally, a Matlab GUI was implemented and integrated into the main CEASIOM interface.</p><p>The module works perfectly, except for the mapping part, that needs a few readjustments.</p>
----------------------------------------------------------------------
In diva2:1818051 abstract is:
<p>The oceans are a key element in our society, economy and environmental system.They cover over 70% of the worlds surface and contribute substantially to ecosystemservices such as climate management as well as to economic sectors such as foodproduction and tourism. While the importance of the oceans for climate changeand the society is generally acknowledged in science and literature, it is often notreflected in policy. Integrated Assessment Models (IAMs) which are used to advicepolicy on carbon prices often systematically omit process and damages related tothe ocean such as ocean acidification, loss of biodiversity and changes in oceancurrents.The aim of this study is to give a more detailed perspective on ocean related processesand their role and importance for the economy under climate change and to testassumptions made in the development of IAMs - and more precisely the DynamicIntegrated Climate-Economy model also referred to as the DICE model. The initialresults of the DICE model resulted in a optimal temperature trajectory with amaximum of 4 ◦C contradicting the goals set with the Paris Agreement.This thesis is the first of its kind attempt in reviewing the most recentbiophysical evidence on climate change impacts with a focus on marine systemsand incorporating these damages to market and non-market sectors into the DICEmodel. The impacts from climate change are implemented into the DICE modelthrough economic valuation of the damages and an update of the damage function.The analysis is based on the damage function used in the original DICE2016R2model as well as the suggested update presented by Hänsel et al. (2020)The results show, that incorporating marine damages into the model yields in amajor increase in economic damages particularly in the temperature range up to 2◦C.These increased damages influence the results of the optimal temperature trajectoryand give a clear indication for a more stringent climate policy, drastically limitingthe maximum temperature increase compared to the original DICE model.</p>


corrected abstract:
<p>The oceans are a key element in our society, economy and environmental system. They cover over 70% of the worlds surface and contribute substantially to ecosystem services such as climate management as well as to economic sectors such as food production and tourism. While the importance of the oceans for climate change and the society is generally acknowledged in science and literature, it is often not reflected in policy. Integrated Assessment Models (IAMs) which are used to advice policy on carbon prices often systematically omit process and damages related to the ocean such as ocean acidification, loss of biodiversity and changes in ocean currents.</p><p>The aim of this study is to give a more detailed perspective on ocean related processes and their role and importance for the economy under climate change and to test assumptions made in the development of IAMs - and more precisely the Dynamic Integrated Climate-Economy model also referred to as the DICE model. The initial results of the DICE model resulted in a optimal temperature trajectory with a maximum of 4 ˚C contradicting the goals set with the Paris Agreement.</p><p>This thesis is the first of its kind attempt in reviewing the most recent biophysical evidence on climate change impacts with a focus on marine systems and incorporating these damages to market and non-market sectors into the DICE model. The impacts from climate change are implemented into the DICE model through economic valuation of the damages and an update of the damage function. The analysis is based on the damage function used in the original DICE2016R2 model as well as the suggested update presented by Hänsel et al. (2020)</p><p>The results show, that incorporating marine damages into the model yields in a major increase in economic damages particularly in the temperature range up to 2˚C. These increased damages influence the results of the optimal temperature trajectory and give a clear indication for a more stringent climate policy, drastically limiting the maximum temperature increase compared to the original DICE model.</p>
----------------------------------------------------------------------
In diva2:1800176 abstract is:
<p>Bolted joints are important due to their energy dissipation property in structures,but the damping mechanism is also highly nonlinear and localized. The goal ofthis thesis is to develop an accurate method for modeling bolted joint dampingin large structures using finite element (FE) software. To model bolted jointdamping, the first step is to study the mechanism and define the terms like slip,micro-slip, and macro-slip. An extensive literature review identified the necessarymethods: detailed contact model, thin-layer elements, and connector elements.These methods are compared based on parameters such as computation time,modeling time, etc. The thin-layer method was used for modeling bolted jointdamping in large structures.</p><p>To evaluate parameters for thin-layer element modeling, a local joint model wasbuilt using contact formulation of an engine housing and ladder frame assembly.The computed parameters include normal stiffness, tangential stiffness, and lossfactor. Analysis reveals that the loss factor depends on pre-load and amplitudeload. The micro-slip is the region of interest where the loss factor was computed. Using curve-fitting, a range of amplitude-dependent loss factors was calculated.</p><p>Finally, the thin-layer elements are used in the engine housing and ladder frameassembly to model bolted joint damping. The parameters estimated using the localjoint model are used to define the properties of the thin-layer elements such thatthe elements are a phenomenological representation of bolted joint. A mode-basedsteady-state analysis has been performed to estimate the loss factor on a systemlevel. The frequency response of such an analysis accurately captures the frequencyresponse curves of structures with bolted joints. The two important behaviors thathave been captured are the shifting of the resonance peak to a lower value and thewidening of the frequency response curve as the applied load increases. However,the resonance frequency shifting to a lower frequency (softening) has not beencaptured due to modeling limitations in the FE software. A substructure couplingmodel using the Craig-Bampton formulation of the engine housing and ladderframe assembly has been analyzed using a constant loss factor. The frequencyresponse of such a system appears to give an approximate behavior of a structurewith bolted joint damping.</p>

corrected abstract:
<p>Bolted joints are important due to their energy dissipation property in structures, but the damping mechanism is also highly nonlinear and localized. The goal of this thesis is to develop an accurate method for modeling bolted joint damping in large structures using finite element (FE) software. To model bolted joint damping, the first step is to study the mechanism and define the terms like slip, micro-slip, and macro-slip. An extensive literature review identified the necessary methods: detailed contact model, thin-layer elements, and connector elements. These methods are compared based on parameters such as computation time, modeling time, etc. The thin-layer method was used for modeling bolted joint damping in large structures.</p><p>To evaluate parameters for thin-layer element modeling, a local joint model was built using contact formulation of an engine housing and ladder frame assembly. The computed parameters include normal stiffness, tangential stiffness, and loss factor. Analysis reveals that the loss factor depends on pre-load and amplitude load. The micro-slip is the region of interest where the loss factor was computed. Using curve-fitting, a range of amplitude-dependent loss factors was calculated.</p><p>Finally, the thin-layer elements are used in the engine housing and ladder frame assembly to model bolted joint damping. The parameters estimated using the local joint model are used to define the properties of the thin-layer elements such that the elements are a phenomenological representation of bolted joint. A mode-based steady-state analysis has been performed to estimate the loss factor on a system level. The frequency response of such an analysis accurately captures the frequency response curves of structures with bolted joints. The two important behaviors that have been captured are the shifting of the resonance peak to a lower value and the widening of the frequency response curve as the applied load increases. However, the resonance frequency shifting to a lower frequency (softening) has not been captured due to modeling limitations in the FE software. A substructure coupling model using the Craig-Bampton formulation of the engine housing and ladder frame assembly has been analyzed using a constant loss factor. The frequency response of such a system appears to give an approximate behavior of a structure with bolted joint damping.</p>
----------------------------------------------------------------------
In diva2:1698414 abstract is:
<p>The impact of air travel on the climate, along with its increasing share in CO2 emissions haveraised the demand for sustainable air travel solutions. The current aircraft technologies haveseen significant improvement throughout the years. Although, the rate at which new aircrafttechnologies are developed can not keep up with the increased demand for air travel. Hence, adifferent approach to reduce the aviation’s impact on climate can be achieved by optimizing thevertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming.Upon departure, an optimization of the vertical flight path is initiated and an optimal flight planis suggested to the flight crew.</p><p>The fuel saving produced by the optimal flight plan is a potential saving that can only be fullyachieved if the flight crew chose to fly according to the optimized flight path. However, restrictionsfrom the Air Traffic Control, as well as the flight crew’s willingness to follow theoptimized flight path can affect the achieved saving. Hence, a tool is developed in order tocompute trip fuel consumption from post-flight data obtained from the Automatic DependentSurveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start andend positions of cruise segments is successfully implemented. Two methods of calculating thefuel are implemented and compared. The first method is based on simulating the actual flight,which uses the same performance model as for the simulation of the operational flight plantrip and optimized trip. The second method is based on utilizing the ADS-B data to obtain theaircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft,hence the trip is not simulated. The results reveals that the simulation method produces flighttrajectories that are comparable to the operational and optimized flight plans since they use thesame model structure. However, using ADS-B data to obtain fuel consumption represents theactual flight trajectory more accurately.</p><p>Furthermore, an optimization algorithm based on the on-board Flight Management Computeris implemented. According to the results, the FMC optimization offers a sufficient optimizationof the cruise phase, when compared to the OFP trip, however performs worse than the dynamicprogramming, which provides a global optimal solution</p>


corrected abstract:
<p>The impact of air travel on the climate, along with its increasing share in CO2 emissions have raised the demand for sustainable air travel solutions. The current aircraft technologies have seen significant improvement throughout the years. Although, the rate at which new aircraft technologies are developed can not keep up with the increased demand for air travel. Hence, a different approach to reduce the aviation’s impact on climate can be achieved by optimizing the vertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming. Upon departure, an optimization of the vertical flight path is initiated and an optimal flight plan is suggested to the flight crew.</p><p>The fuel saving produced by the optimal flight plan is a potential saving that can only be fully achieved if the flight crew chose to fly according to the optimized flight path. However, restrictions from the Air Traffic Control, as well as the flight crew’s willingness to follow the optimized flight path can affect the achieved saving. Hence, a tool is developed in order to compute trip fuel consumption from post-flight data obtained from the Automatic Dependent Surveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start and end positions of cruise segments is successfully implemented. Two methods of calculating the fuel are implemented and compared. The first method is based on simulating the actual flight, which uses the same performance model as for the simulation of the operational flight plan trip and optimized trip. The second method is based on utilizing the ADS-B data to obtain the aircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft, hence the trip is not simulated. The results reveals that the simulation method produces flight trajectories that are comparable to the operational and optimized flight plans since they use the same model structure. However, using ADS-B data to obtain fuel consumption represents the actual flight trajectory more accurately.</p><p>Furthermore, an optimization algorithm based on the on-board Flight Management Computer is implemented. According to the results, the FMC optimization offers a sufficient optimization of the cruise phase, when compared to the OFP trip, however performs worse than the dynamic programming, which provides a global optimal solution</p>
----------------------------------------------------------------------
In diva2:1216784 missing space in title:
"Mobile Network trafficprediction: Based on machine learning"
==>
"Mobile Network traffic prediction: Based on machine learning"

The following abstract is in DiVA - but it belongs to another thesis. Compare to the Swedish abstract!

abstract is:
<p>The investing market can be a cold ruthless placefor the layman. In order to get the chance of making money inthis business one must place countless hours on research, withmany different parameters to handle in order to reach success.To reduce the risk, one must look to many different companiesoperating in multiple fields and industries. In other words, it canbe a hard task to manage this feat.With modern technology, there is now lots of potential tohandle this tedious analysis autonomously using machine learningand clever algorithms. With this approach, the amount ofanalyzes is only limited by the capacity of the computer. Resultingin a number far greater than if done by hand.This study aims at exploring the possibilities to modify andimplement efficient algorithms in the field of finance. The studyutilizes the power of kernel methods in order to algorithmicallyanalyze the patterns found in financial data efficiently. Bycombining the powerful tools of change point detection andnonlinear regression the computer can classify the differenttrends and moods in the market.The study culminates to a tool for analyzing data from thestock market in a way that minimizes the influence from shortspikes and drops, and instead is influenced by the underlying pattern.But also, an additional tool for predicting future movementsin the price.</p>

corrected abstract:
<p>The amount of data traffic sent through mobile networks varies throughout the day and week. Thus, the network experiences varying demand and therefore, the load on all the back end systems in the core network is far from constant. By being able to predict the load, the back end system capacity can be optimized during the day, reducing maintenance costs and energy consumption, affecting the environment positively. The predictions may also be used for network planning.</p><p>The aim of this project was to predict the mobile network data traffic based on two weeks of data aggregated into five minute intervals. The data was treated as a time series and time series forecasting methods were used, the ARIMA model using external regressors based on a polynomial model and a Fourier series as well as the TBATS model. Also, a recurrent neural network based on a method called Long Short Term Memory was used.</p><p>The results show that the seasonal components of the time series are modelled well using simple methods such as a polynomial model or Fourier series. However, modelling the dynamics of the stationary time series is very difficult and the ARIMA model did not perform well in this situation due to the long time predictions made. Neither did the neural network or TBATS model manage to model the stationary dynamics and were only able to capture the seasonal components.</p>
----------------------------------------------------------------------
In diva2:1799891 abstract is:
<p>This thesis examines water mixing and exchange in a drinking water reservoir operated by themunicipal association Norrvatten. Recent water samples from the reservoir’s outgoing waterhave shown an increase in culturable bacteria during late summer and fall. This thesis utilizesComputational Fluid Dynamics (CFD) modeling and analysis in OpenFOAM to simulatereservoir inflow and outflow, analyzing mixing processes and their relationship to operationalstrategies. The objective is to understand the correlation between the residence time of waterand microbial growth and propose operational improvements to increase the exchange of waterin order to achieve improved water quality. A trace element was implemented in the CFDmodel to simulate the residence time of water. Initial simulations were based on the reservoir’shistorical operational data, utilizing temperature and water level measurements providedby Norrvatten. After the initial simulations, four alternative simulations were performed,comparing different operational strategies by modifying inflow parameters. Inflow parametersthat were changed were the volumetric inflow rate, water level variation, and the temperatureof the inflowing water. The post­processing in ParaView focused on the thermal stratificationand residence time distribution near the outlet during each mixing process. The study revealeda complex relationship between flow conditions and microbial growth, making it challengingto identify a clear pattern. However, based on the simulations with the alternative operationalstrategies it was concluded that the set of operational strategies called ”Strategy 1” generated themost optimal flow conditions. This strategy involves a three times larger volumetric inflow rate(an increase from 0.05 to 0.15 m^3/s) and a water level that is kept at the same values comparedto the original simulation. Strategy 1 resulted in a 3.6 % higher water exchange compared to theoriginal simulation. In comparison to the other simulated strategies, Strategy 1 generates thehighest water exchange, with a 63.6 % increase compared to the worst­-case scenario involvingcolder inflow. The conclusion that could be drawn is that the most favorable operationalstrategies involve higher volumetric inflow rates, lower water levels, and an incoming watertemperature that is higher than the initial reservoir temperature.</p><p> </p>

corrected abstract:
<p>This thesis examines water mixing and exchange in a drinking water reservoir operated by the municipal association Norrvatten. Recent water samples from the reservoir’s outgoing water have shown an increase in culturable bacteria during late summer and fall. This thesis utilizes Computational Fluid Dynamics (CFD) modeling and analysis in OpenFOAM to simulate reservoir inflow and outflow, analyzing mixing processes and their relationship to operational strategies. The objective is to understand the correlation between the residence time of water and microbial growth and propose operational improvements to increase the exchange of water in order to achieve improved water quality. A trace element was implemented in the CFD model to simulate the residence time of water. Initial simulations were based on the reservoir’s historical operational data, utilizing temperature and water level measurements provided by Norrvatten. After the initial simulations, four alternative simulations were performed, comparing different operational strategies by modifying inflow parameters. Inflow parameters that were changed were the volumetric inflow rate, water level variation, and the temperature of the inflowing water. The post­processing in ParaView focused on the thermal stratification and residence time distribution near the outlet during each mixing process. The study revealed a complex relationship between flow conditions and microbial growth, making it challenging to identify a clear pattern. However, based on the simulations with the alternative operational strategies it was concluded that the set of operational strategies called ”Strategy 1” generated the most optimal flow conditions. This strategy involves a three times larger volumetric inflow rate (an increase from 0.05 to 0.15 m<sup>3</sup>/s) and a water level that is kept at the same values compared to the original simulation. Strategy 1 resulted in a 3.6 % higher water exchange compared to the original simulation. In comparison to the other simulated strategies, Strategy 1 generates the highest water exchange, with a 63.6 % increase compared to the worst-case scenario involving colder inflow. The conclusion that could be drawn is that the most favorable operational strategies involve higher volumetric inflow rates, lower water levels, and an incoming water temperature which is higher than the initial reservoir temperature.</p>
----------------------------------------------------------------------
In diva2:1740181 abstract is:
<p>This project is about the design process of a resonancemitigating algorithm for a large solar sail. The solar sail isa structure made up by four long booms in a cross patternwith suspended sails between the edges of the booms. Thespacecraft in this report is controlled using smaller rotatingsails at each tip of the booms. Since the booms are longthey will experience significant bending moment even thoughthe actual force from the control sails will be small. Thisbending has a high risk of exciting the spacecraft’s resonancemodes, which will in turn complicate the control of theentire spacecraft. Because of this it is necessary to designan algorithm to mitigate resonance excitation. To design thisalgorithm three main steps were taken, an approximation ofspacecraft resonance modes, a valid mathematical model ofthe system and robustness building to handle model error.The resonance modes were approximated through analyticalformulas. The boom model was determined by combiningdifferent approaches from previous similar works. Finally,the controller chosen was a simple PID controller with abuilt-in saturation limiter to make sure the controller stayswithin the spacecraft’s operating bounds. To ensure robustness,multiple test simulations were made on systems with differentresonance modes compared to the true system. The chosencontroller passed these tests. Ultimately the chosen controllerreduced the settling time of the resonance oscillations by 75%.</p>

corrected abstract:
<p>This project is about the design process of a resonance mitigating algorithm for a large solar sail. The solar sail is a structure made up by four long booms in a cross pattern with suspended sails between the edges of the booms. The spacecraft in this report is controlled using smaller rotating sails at each tip of the booms. Since the booms are long they will experience significant bending moment even though the actual force from the control sails will be small. This bending has a high risk of exciting the spacecraft’s resonance modes, which will in turn complicate the control of the entire spacecraft. Because of this it is necessary to design an algorithm to mitigate resonance excitation. To design this algorithm three main steps were taken, an approximation of spacecraft resonance modes, a valid mathematical model of the system and robustness building to handle model error. The resonance modes were approximated through analytical formulas. The boom model was determined by combining different approaches from previous similar works. Finally, the controller chosen was a simple PID controller with a built-in saturation limiter to make sure the controller stays within the spacecraft’s operating bounds. To ensure robustness, multiple test simulations were made on systems with different resonance modes compared to the true system. The chosen controller passed these tests. Ultimately the chosen controller reduced the settling time of the resonance oscillations by 75%.</p>
----------------------------------------------------------------------
In diva2:1571119 abstract is:
<p>Shaped charges (SC) have been used as a means of explosives in military andcivilian use for decades. Thus, there is a substantial amount of research behindthis area. However, as this is a sensitive subject much of this research is notpublicly available.</p><p>This thesis will look at how one can use asymmetries in SC’s to velocity compensatethe jet formation. Velocity compensation is required when the SC is perpendicularto the projectile direction, hence, leading to an angled jet which decreases thepenetration potential.</p><p>The asymmetries that were investigated are• off-­center detonation• angled liner• displaced wave shaper• displaced wave shaper &amp; angled liner.</p><p>The 3D explosive simulation was conducted in IMPETUS AFEA solver and tocompare the performance of these asymmetries the position and velocity of thejet were measured. To create a baseline a simulation without any asymmetrieswas used.</p><p>The off­-center detonation showed some velocity compensating characteristicsat the tip of the jet. However, as the jet progressed it converged towards thereference.</p><p>Angled liner simulations were conducted with an angle of 0.5 degrees and 1 degreeand these asymmetries behaved vastly differently. Angled Liner 0.5 degrees hada greater jet angle but a greater quantity of the jet particles were concentratedaround one point increasing the penetration potential. A general characteristicthat angled liner displaced was the fact that it had desirable velocity compensatingtraits all through the jet.</p><p>Displaced Wave Shaper, like off­-center detonation, showed promising velocitycompensating attributes at the tip of the jet, however, it too converged towardsthe reference on the later part of the jet.</p><p>When combining the displaced wave shaper and angled liner asymmetries thedesire was to also combine their velocity compensating traits, i.e achievingthe displaced wave shaper’s tip compensation with the angled liner’s totalcompensation. Unfortunately, this was not achieved. The tip, again, showedpromising velocity compensating attributes but the rest of the jet convergedtowards the reference.</p><p>Conclusively, angled liner shows the highest potential for compensating thevelocity and allowed the most amount of jet particles to be concentrated aroundone point increasing the penetration potential.</p><p> </p>

corrected abstract:
<p>Shaped charges (SC) have been used as a means of explosives in military and civilian use for decades. Thus, there is a substantial amount of research behind this area. However, as this is a sensitive subject much of this research is not publicly available.</p><p>This thesis will look at how one can use asymmetries in SC’s to velocity compensate the jet formation. Velocity compensation is required when the SC is perpendicular to the projectile direction, hence, leading to an angled jet which decreases the penetration potential.</p><p>The asymmetries that were investigated are<ul><li>off-center detonation</li><li>angled liner</li><li>displaced wave shaper</li><li>displaced wave shaper &amp; angled liner.</li></ul></p><p>The 3D explosive simulation was conducted in IMPETUS AFEA solver and to compare the performance of these asymmetries the position and velocity of the jet were measured. To create a baseline a simulation without any asymmetries was used.</p><p>The off-center detonation showed some velocity compensating characteristics at the tip of the jet. However, as the jet progressed it converged towards the reference.</p><p>Angled liner simulations were conducted with an angle of 0.5 degrees and 1 degree and these asymmetries behaved vastly differently. Angled Liner 0.5 degrees had a greater jet angle but a greater quantity of the jet particles were concentrated around one point increasing the penetration potential. A general characteristic that angled liner displaced was the fact that it had desirable velocity compensating traits all through the jet.</p><p>Displaced Wave Shaper, like off-center detonation, showed promising velocity compensating attributes at the tip of the jet, however, it too converged towards the reference on the later part of the jet.</p><p>When combining the displaced wave shaper and angled liner asymmetries the desire was to also combine their velocity compensating traits, i.e achieving the displaced wave shaper’s tip compensation with the angled liner’s total compensation. Unfortunately, this was not achieved. The tip, again, showed promising velocity compensating attributes but the rest of the jet converged towards the reference.</p><p>Conclusively, angled liner shows the highest potential for compensating the velocity and allowed the most amount of jet particles to be concentrated around one point increasing the penetration potential.</p>
----------------------------------------------------------------------
In diva2:1527916 - missing space in title:
"Connected Tyres: Real-time Tyre Monitoring System for Fleet& Autonomous Vehicles with Tyre WearEstimation through Sensor Fusion"
==>
"Connected Tyres: Real-time Tyre Monitoring System for Fleet & Autonomous Vehicles with Tyre Wear Estimation through Sensor Fusion"

abstract is:
<p>Tyres are one crucial part for vehicles, as they are the only contact pointbetween the vehicle and the road. Intelligent tyres are a trending new subjectin the tyre industry. They are designed to monitor various tyre states and sendthis information to both drivers and remote servers. The master thesis focuseson the proposal of a real-time tyre monitoring system for fleet and autonomousvehicles. It includes developing a tyre wear model and analysis of the currenttyre pressure monitoring functionality by leveraging the connectivity of fleetvehicles equipped with a Volvo web cloud service. The tyre wear model indirectlymonitors the tread depth of the vehicles all four tyres by identifyingcharacteristics between worn and fresh tyres. The two characteristics are identifiedby monitoring and analyzing vehicle speed and braking signals. The twocharacteristics is input to a voting scheme which decides when a worn tyre isdetected. The test vehicle was a Volvo XC40 with three types of tyres: wintertyres, summer tyres and worn summer tyres. The wear model gives 90 %accuracy to 10 set of test data, randomly selected from all dataset at HälleredProving Ground (Sweden). The connectivity realizes the data transmissionfrom the raw data of onCAN and FlexRay signals stored in a Volvo web cloudservice to the tyre monitoring fleet system. The signals are filtered and resampled,leaving the required signals of the tyre pressure monitor system andthe tyre wear model. Two signals, Calibration Status and iTPMS Status, areused to perform a statistical analysis on tyre pressure by categorizing the calibrationstatus and the tyre pressure conditions.The project outcome is an interfacebuilt on MATLAB GUI for demonstration of vehicle identification andtyre health conditions, with the embedded tyre wear model and connectivity.</p>

corrected abstract:
<p>Tyres are one crucial part for vehicles, as they are the only contact point between the vehicle and the road. Intelligent tyres are a trending new subject in the tyre industry. They are designed to monitor various tyre states and send this information to both drivers and remote servers. The master thesis focuses on the proposal of a real-time tyre monitoring system for fleet and autonomous vehicles. It includes developing a tyre wear model and analysis of the current tyre pressure monitoring functionality by leveraging the connectivity of fleet vehicles equipped with a Volvo web cloud service. The tyre wear model indirectly monitors the tread depth of the vehicles all four tyres by identifying characteristics between worn and fresh tyres. The two characteristics are identified by monitoring and analyzing vehicle speed and braking signals. The two characteristics is input to a voting scheme which decides when a worn tyre is detected. The test vehicle was a Volvo XC40 with three types of tyres: winter tyres, summer tyres and worn summer tyres. The wear model gives 90 % accuracy to 10 set of test data, randomly selected from all dataset at Hällered Proving Ground (Sweden). The connectivity realizes the data transmission from the raw data of on CAN and FlexRay signals stored in a Volvo web cloud service to the tyre monitoring fleet system. The signals are filtered and resampled, leaving the required signals of the tyre pressure monitor system and the tyre wear model. Two signals, Calibration Status and iTPMS Status, are used to perform a statistical analysis on tyre pressure by categorizing the calibration status and the tyre pressure conditions. The project outcome is an interface built on MATLAB GUI for demonstration of vehicle identification and tyre health conditions, with the embedded tyre wear model and connectivity.</p>
----------------------------------------------------------------------
In diva2:1142785 - possible duplicate 'diva2:1120402

abstract is:
<p>Due to demographic changes, the transportationdemand is predicted to increase significantly in the next decades.Considering the transport sector’s impact on society and theenvironment, the development of a sustainable transport systemis of great importance. Two possible building blocks in such asystem are connectivity and automation, and this project aims tostudy a way of combining these two.The purpose of this project is to investigate how the introductionof autonomous minibuses to a pre-existing bus systemwould affect its operational cost and environmental impact. Thisis done using a linear programming model that finds the optimalcombination of conventional buses and autonomous minibuseswith respect to cost. The model is implemented in the modellingsystem GAMS for bus lines 1–4 in Stockholm using data ontravel demand. Two scenarios are analysed; the first allowingan arbitrary number of minibuses, and the second being morerealistic and restricting the number of minibuses. The solutionsare then compared to the corresponding solutions using onlyconventional buses.In both cases, the results indicate that considerable savingscan be obtained while maintaining or even improving availability.From this, we draw the conclusion that when such technologyis truly available, it would be advisable to investigate if thesesavings can weigh up the costs related to necessary investments.</p>

corrected abstract:
<p>Due to demographic changes, the transportation demand is predicted to increase significantly in the next decades. Considering the transport sector’s impact on society and the environment, the development of a sustainable transport system is of great importance. Two possible building blocks in such a system are connectivity and automation, and this project aims to study a way of combining these two.</p><p>The purpose of this project is to investigate how the introduction of autonomous minibuses to a pre-existing bus system would affect its operational cost and environmental impact. This is done using a linear programming model that finds the optimal combination of conventional buses and autonomous minibuses with respect to cost. The model is implemented in the modelling system GAMS for bus lines 1–4 in Stockholm using data on travel demand. Two scenarios are analysed; the first allowing an arbitrary number of minibuses, and the second being more realistic and restricting the number of minibuses. The solutions are then compared to the corresponding solutions using only conventional buses.</p><p>In both cases, the results indicate that considerable savings can be obtained while maintaining or even improving availability. From this, we draw the conclusion that when such technology is truly available, it would be advisable to investigate if these savings can weigh up the costs related to necessary investments.</p>
----------------------------------------------------------------------
In diva2:1110767 abstract is:
<p>The present work reports the first systematic results obtained in wind tunnel and at full scale with theexperimental apparatus developed within the joint project among Politecnico di Milano, North Sails andCSEM. The steady state upwind aerodynamics of sailing yachts are investigated through the contemporarymeasurement of global forces, distributed pressures and sail flying shapes, and with numerical simulationsbased on Potential Theory and Navier-Stokes equations.The wind tunnel of Politecnico di Milano and the Sailing Yacht Lab (SYL), used for full scale investigations,are described together with the measurement systems adopted for recording forces, pressures and sailshapes. The aerodynamic loads are obtained through dedicated arrangements of load cells; the pressuredistributions on sail sections are evaluated with integrated systems of customized local measurementsolutions and MEMS sensors; the sail flying shapes are detected thanks to two laser scanners based of theTime of Flight technology. The experimental procedures adopted during tests are presented and discussedin relation with the aim of the work.Numerical simulations of selected wind tunnel cases are performed in order to assess the capabilities of theempirical techniques to validate Computational Fluid Dynamics (CFD) codes and to propose solid numericalset-ups for investigating sailing yacht aerodynamics. A Vortex Lattice Method (VLM) code, written inMatlab, is used for quick preliminary analyses of the global forces developed by the sail plan, whereas theopen source environment OpenFOAM is adopted to perform 3D simulations for investigating in details thelocal flow patterns.The experimental apparatus, both at model and full scale, proved to be extremely well suited for thepurposes of the study, giving remarkable results regarding, in particular, pressures and sail shapes. Theexpected distributions are obtained during wind tunnel tests and interesting considerations arise from thecomparison with the full scale outcomes. Numerically, the results of simulations meet the preliminaryintuitions with the VLM code capable of accurately predicting global forces up to certain wind angles andthe RANS-based computations providing notable agreement with the measured local pressures.</p>

corrected abstract:
<p>The present work reports the first systematic results obtained in wind tunnel and at full scale with the experimental apparatus developed within the joint project among Politecnico di Milano, North Sails and CSEM. The steady state upwind aerodynamics of sailing yachts are investigated through the contemporary measurement of global forces, distributed pressures and sail flying shapes, and with numerical simulations based on Potential Theory and Navier-Stokes equations.</p><p>The wind tunnel of Politecnico di Milano and the Sailing Yacht Lab (SYL), used for full scale investigations, are described together with the measurement systems adopted for recording forces, pressures and sail shapes. The aerodynamic loads are obtained through dedicated arrangements of load cells; the pressure distributions on sail sections are evaluated with integrated systems of customized local measurement solutions and MEMS sensors; the sail flying shapes are detected thanks to two laser scanners based of the Time of Flight technology. The experimental procedures adopted during tests are presented and discussed in relation with the aim of the work.</p><p>Numerical simulations of selected wind tunnel cases are performed in order to assess the capabilities of the empirical techniques to validate Computational Fluid Dynamics (CFD) codes and to propose solid numerical set-ups for investigating sailing yacht aerodynamics. A Vortex Lattice Method (VLM) code, written in Matlab, is used for quick preliminary analyses of the global forces developed by the sail plan, whereas the open source environment OpenFOAM is adopted to perform 3D simulations for investigating in details the local flow patterns.</p><p>The experimental apparatus, both at model and full scale, proved to be extremely well suited for the purposes of the study, giving remarkable results regarding, in particular, pressures and sail shapes. The expected distributions are obtained during wind tunnel tests and interesting considerations arise from the comparison with the full scale outcomes. Numerically, the results of simulations meet the preliminary intuitions with the VLM code capable of accurately predicting global forces up to certain wind angles and the RANS-based computations providing notable agreement with the measured local pressures.</p>
----------------------------------------------------------------------
In diva2:1083779 abstract is:
<p>Carbonated sparkling water has been widely used from ancient age [1]. The original ideacame from natural sparkling water and people believed that taking baths at carbonatedhot springs was good for health and healed their sicknesses. This fact led people to startthinking that sparkling water could have more effective uses. Joseph Priestley success-fully produced artificial carbonated water in 1767 and sparkling water quickly becamewidely spread because it gives people refreshing feeling. The bottled and canned beverageindustry has grown from the 19th century and has become one of the biggest markets inthe world. According to Bloomberg Intelligence and Euromonitor, the global market ofthe carbonated beverages is around 350 billion dollar. One main drawback was that itwas not possible to re-cork the bottle to save the carbonation so that once it was opened,fizz was kept only for a short time. In 1813, the method to dispense a portion of carbon-ated water was invented by Charles Plinth[2]. This was the origin of the Soda Syphon.As the demand of sparkling water increased, the machine with which people could makesparkling water by themselves was introduced. Recently, it has become a very popularhome appliance, especially in Europe and North America. The most common way tocarbonate water is by injecting high-pressure CO2 into a water bottle. However, currentsystems waste a lot of CO2 during this carbonating process. In this thesis, the flow insidethe bottle during the injection of CO2 into water was studied in order to determine the pa-rameters that had most influence on the carbonation process. CFD (Computational FluidDynamics) simulations were performed in STAR-CCM+ of an axisymmetric 2D modeland a 3D model that was a 30 degree wedge of the real bottle shape. The Volume of Fluidmethod was used to solve the multiphase flow of gas and liquid. The RANS approachwas used with k 􀀀ϵ model and implicit time marching. To validate the simulations, axialpropagation of the volume fraction of CO2 was compared with the experimental visual-ization of the CO2 and H2O distribution. At the beginning of the phenomena, the gaspropagation was reasonably predicted and the results capture the features of the bubbleshape. However the results did not perfectly match with the experimental visualization.To seek the reason for the unrealistic results, the grid sensitivity study was performedand to consider the 3D effect the results with the 2D and the 3D model were compared.In addition, the bubble breakup process was deeply investigated.</p>

corrected abstract:
<p>Carbonated sparkling water has been widely used from ancient age [1]. The original idea came from natural sparkling water and people believed that taking baths at carbonated hot springs was good for health and healed their sicknesses. This fact led people to start thinking that sparkling water could have more effective uses. Joseph Priestley successfully produced artificial carbonated water in 1767 and sparkling water quickly became widely spread because it gives people refreshing feeling. The bottled and canned beverage industry has grown from the 19th century and has become one of the biggest markets in the world. According to Bloomberg Intelligence and Euromonitor, the global market of the carbonated beverages is around 350 billion dollar. One main drawback was that it was not possible to re-cork the bottle to save the carbonation so that once it was opened, fizz was kept only for a short time. In 1813, the method to dispense a portion of carbonated water was invented by Charles Plinth[2]. This was the origin of the Soda Syphon. As the demand of sparkling water increased, the machine with which people could make sparkling water by themselves was introduced. Recently, it has become a very popular home appliance, especially in Europe and North America. The most common way to carbonate water is by injecting high-pressure CO<sub>2</sub> into a water bottle. However, current systems waste a lot of CO<sub>2</sub> during this carbonating process. In this thesis, the flow inside the bottle during the injection of CO<sub>2</sub> into water was studied in order to determine the parameters that had most influence on the carbonation process. CFD (Computational Fluid Dynamics) simulations were performed in STAR-CCM+ of an axisymmetric 2D model and a 3D model that was a 30 degree wedge of the real bottle shape. The Volume of Fluid method was used to solve the multiphase flow of gas and liquid. The RANS approach was used with <em>k - ϵ</em> model and implicit time marching. To validate the simulations, axial propagation of the volume fraction of CO<sub>2</sub> was compared with the experimental visualization of the CO<sub>2</sub> and H<sub>2</sub>O distribution. At the beginning of the phenomena, the gas propagation was reasonably predicted and the results capture the features of the bubble shape. However the results did not perfectly match with the experimental visualization. To seek the reason for the unrealistic results, the grid sensitivity study was performed and to consider the 3D effect the results with the 2D and the 3D model were compared. In addition, the bubble breakup process was deeply investigated.</p>
----------------------------------------------------------------------
In diva2:1081137 - missing space in title:
"Revisiting Hot-Wire AnemometryClose to Solid Walls"
==>
"Revisiting Hot-Wire Anemometry Close to Solid Walls"

abstract is:
<p>A well-known problem of hot-wire anemometry (HWA), is the “wall effect”, namely theoverestimation of the measured velocity near a wall. The overestimation occurs due toadditional heat loss from the heated wire-sensor to the wall. The extra heat loss dependson parameters such as the heat conductivity of the wall material, the overheat ratio ofthe wire, and the sensor geometry. This problem has been studied for quite some timeand there are several suggestions with regard to the effect of these parameters for meanflow corrections, however the effect on measurements of turbulent fluctuation has notbeen investigated. The present work aims at providing further insight on this topic, byelucidating how these parameters affect measurements of both the mean and fluctuatingvelocity. Furthermore, the present study proposes a theoretical model on the total heattransfer from hot-wire sensor to explain the phenomenon.In the experimental part of the study, the measurements under both no flow and flowconditions are carried out to consider natural convection and forced convection separately.The results showed that the effect of the parameters is consistent with what is agreedwidely: higher wall conductivity, higher overheat ratio, and larger wire exposed area leadto higher output from an anemometer. On the other hand, it is observed that the conduc-tion under natural convection can be scaled with the overheat ratio. Velocity fluctuationsare found to decrease by employing higher overheat ratio and for walls with higher heatconductivity.In the numerical part of the study, a two-dimensional steady calculation using Open-FOAM is performed and the parameter dependency with respect to the overheat ratio andwall heat conductivity is investigated. The results qualitatively agree with the experi-mental results. Moreover, the inner scaling commonly employed in wall-turbulence isfound to be inadequate to resolve the wall effect of HWA when various sensor heights areconcerned.Lastly, a theoretical model on the total heat transfer from the wire close to solid wallsis established based on a superposition of the convection and the conduction contributions.The proposed model with the empirically determined coefficients is found to be capableof capturing the qualitative behaviours found in the experiment and numerical analysis, howewer for more practical use it leaves several issues to be further analysed.</p>

corrected abstract:
<p>A well-known problem of hot-wire anemometry (HWA), is the “wall effect”, namely the overestimation of the measured velocity near a wall. The overestimation occurs due to additional heat loss from the heated wire-sensor to the wall. The extra heat loss depends on parameters such as the heat conductivity of the wall material, the overheat ratio of the wire, and the sensor geometry. This problem has been studied for quite some time and there are several suggestions with regard to the effect of these parameters for mean flow corrections, however the effect on measurements of turbulent fluctuation has not been investigated. The present work aims at providing further insight on this topic, by elucidating how these parameters affect measurements of both the mean and fluctuating velocity. Furthermore, the present study proposes a theoretical model on the total heat transfer from hot-wire sensor to explain the phenomenon.</p><p>In the experimental part of the study, the measurements under both no flow and flow conditions are carried out to consider natural convection and forced convection separately. The results showed that the effect of the parameters is consistent with what is agreed widely: higher wall conductivity, higher overheat ratio, and larger wire exposed area lead to higher output from an anemometer. On the other hand, it is observed that the conduction under natural convection can be scaled with the overheat ratio. Velocity fluctuations are found to decrease by employing higher overheat ratio and for walls with higher heat conductivity.</p><p>In the numerical part of the study, a two-dimensional steady calculation using OpenFOAM is performed and the parameter dependency with respect to the overheat ratio and wall heat conductivity is investigated. The results qualitatively agree with the experimental results. Moreover, the inner scaling commonly employed in wall-turbulence is found to be inadequate to resolve the wall effect of HWA when various sensor heights are concerned.</p><p>Lastly, a theoretical model on the total heat transfer from the wire close to solid walls is established based on a superposition of the convection and the conduction contributions. The proposed model with the empirically determined coefficients is found to be capable of capturing the qualitative behaviours found in the experiment and numerical analysis, however for more practical use it leaves several issues to be further analysed.</p>
----------------------------------------------------------------------
In diva2:1894689 abstract is:
<p>This thesis evaluates the performance of four equity funds managed by SEBInvestment Management using the Fama-French three-factor model whichconsiders market risk, size (SMB), and value (HML) factors in addition to thegeneral market movement. The study aims to understand how these factorscontribute to the fund’s returns and to examine potential investment biasesthat could influence the funds performance.The analysis revealed varying degrees of sensitivity to these factors amongthe funds, overall reflecting their distinct investment strategies. For instance,the small cap fund displayed a significant positive relationship with SMB,indicating a tendency to benefit from investments in smaller companies.Conversely, the value fund showed a positive HML coeﬀicient, suggestinga preference for value stocks. However, several of the funds saw correlationwith development of small cap stocks.Results indicate that while market risk remains a dominant factor, size andvalue significantly contribute to fund performance. This is offering insightsbeyond the traditional CAPM model. The study’s findings are significant,suggesting that actively managed funds can exhibit distinct behavioral patternswhich could be systematically explored to enhance investment strategies anddecision making.The thesis concludes with recommendations for extending this research toinclude additional factors like momentum and liquidity which could providea deeper understanding of the influences affecting fund performance. Theadoption of multi factor models may offer a more comprehensive frameworkfor predicting stock returns and assisting portfolio management.</p>

corrected abstract:
<p>This thesis evaluates the performance of four equity funds managed by SEB Investment Management using the Fama-French three-factor model which considers market risk, size (SMB), and value (HML) factors in addition to the general market movement. The study aims to understand how these factors contribute to the fund’s returns and to examine potential investment biases that could influence the funds performance.</p><p>The analysis revealed varying degrees of sensitivity to these factors among the funds, overall reflecting their distinct investment strategies. For instance, the small cap fund displayed a significant positive relationship with SMB, indicating a tendency to benefit from investments in smaller companies. Conversely, the value fund showed a positive HML coefficient, suggesting a preference for value stocks. However, several of the funds saw correlation with development of small cap stocks.</p><p>Results indicate that while market risk remains a dominant factor, size and value significantly contribute to fund performance. This is offering insights beyond the traditional CAPM model. The study’s findings are significant, suggesting that actively managed funds can exhibit distinct behavioral patterns which could be systematically explored to enhance investment strategies and decision making.</p><p>The thesis concludes with recommendations for extending this research to include additional factors like momentum and liquidity which could provide a deeper understanding of the influences affecting fund performance. The adoption of multi factor models may offer a more comprehensive framework for predicting stock returns and assisting portfolio management.</p>
----------------------------------------------------------------------
In diva2:1881360 abstract is:
<p>ArtEmis is an EU project that today consists of 14 differentinstitutions collectively working towards the final goal of buildinga system that can make trustworthy earthquake predictions withthe help of radon gas. The purpose of this report is to analyse theconcentration of radon measured by four out of six prototype sensorsinstalled by the ArtEmis project.</p><p>In the occurrence of an earthquake, tectonic plates slide together,causing stress levels to rise within the Earth’s crust and microcracksbegin to form. When these microcracks form, specific elements suchas radon gas can ascend toward the surface and reach groundwater.Once in groundwater, the concentration of radon can be measuredby analysing the amount of γ rays at certain energies using γ-rayspectroscopy.</p><p>With energy spectra measured by these sensors, an energy intervalcorresponding to the presence of the isotope 222Rn could be identified,namely the interval around 609 keV. This stems from a daughterisotope of 222Rn. Further analysis on the activity over time, andcomparisons to instances when earthquakes occurred, could thenbe done. These measurements were also tested against a statisticalmodel based on Gaussian distribution, showing correlation in severalcases.</p><p>One sensor location had an extra interesting find. Here it wasable to see, on two different occasions, a distinct increase in 222Rnconcentration roughly 10 days prior to an earthquake.Considering that these sensors are active for the very first timeduring the time span of this report, unintended behaviour occurredon several occasions. A large focus of the project currently lies onfixing these issues. This leads to limited conclusions being able tobe drawn from such a short time span, but could give indication ofpositive results moving forward.</p>

corrected abstract:
<p>ArtEmis is an EU project that today consists of 14 different institutions collectively working towards the final goal of building a system that can make trustworthy earthquake predictions with the help of radon gas. The purpose of this report is to analyse the concentration of radon measured by four out of six prototype sensors installed by the ArtEmis project.</p><p>In the occurrence of an earthquake, tectonic plates slide together, causing stress levels to rise within the Earth’s crust and microcracks begin to form. When these microcracks form, specific elements such as radon gas can ascend toward the surface and reach groundwater. Once in groundwater, the concentration of radon can be measured by analysing the amount of γ rays at certain energies using γ-ray spectroscopy.</p><p>With energy spectra measured by these sensors, an energy interval corresponding to the presence of the isotope <sup>222</sup><em>Rn</em> could be identified, namely the interval around 609 keV. This stems from a daughter isotope of <sup>222</sup><em>Rn</em>. Further analysis on the activity over time, and comparisons to instances when earthquakes occurred, could then be done. These measurements were also tested against a statistical model based on Gaussian distribution, showing correlation in several cases.</p><p>One sensor location had an extra interesting find. Here it was able to see, on two different occasions, a distinct increase in <sup>222</sup><em>Rn</em> concentration roughly 10 days prior to an earthquake.</p><p>Considering that these sensors are active for the very first time during the time span of this report, unintended behaviour occurred on several occasions. A large focus of the project currently lies on fixing these issues. This leads to limited conclusions being able to be drawn from such a short time span, but could give indication of positive results moving forward.</p>
----------------------------------------------------------------------
In diva2:1878576 abstract is:
<p>Hematology analyzers can be used for screening patients for blood abnor-malities. The techniques used in a hematology analyzer include impedanceanalysis, flow cytometry and spectroscopy, which allow for measuring of forexample absolute count, sizes and concentration of different cells in a patient’sblood sample. Hyperlipidemia, which refers to elevated blood lipid levels, isthe primary cause of heart-related illness and fatalities in today’s developedor developing countries. Currently, blood lipid levels are not measured as aparameter with hematology analyzers. Since hematology analyzers allow for arapid general screening of blood parameters, an area of interest is therefore tobe able to measure blood lipids with a hematology analyzer. Thus, this studyaims to investigate the possibility of detecting and measuring blood lipids witha hematology analyzer, using flow cytometry and/or spectrophotometry.</p><p>In order to investigate this possibility, two simulating methods were conductedwhere in the first method Intralipid 20% was mixed with saline into sampleswith different lipid concentrations. In the second method, diluent wasused instead of saline. Lastly a Correlation study was performed whereIntralipid 20% was mixed with donor blood to prepare samples with differentlipid concentrations. All samples were then analyzed in a hematologyanalyser and scatter plots from flow cytometry and light absorption datafrom spectrophotometry measurements were obtained. The methods showedthat there is a strong correlation between number of detected pulse countsfrom the scatter plots and lipid concentration. Same applies to lightabsorption compared to the lipid concentration of the samples, measured withspectrophotometry.</p><p>The results from this study show that it is in fact possible to detect andmeasure blood lipid levels with a hematology analyser using flow cytometryand spectrophotometry. Further development within this area could thereforeenable simple screening of this additional parameter and early detection ofindications of hyperlipidemia.</p>


corrected abstract:
<p>Hematology analyzers can be used for screening patients for blood abnormalities. The techniques used in a hematology analyzer include impedance analysis, flow cytometry and spectroscopy, which allow for measuring of for example absolute count, sizes and concentration of different cells in a patient’s blood sample. Hyperlipidemia, which refers to elevated blood lipid levels, is the primary cause of heart-related illness and fatalities in today’s developed or developing countries. Currently, blood lipid levels are not measured as a parameter with hematology analyzers. Since hematology analyzers allow for a rapid general screening of blood parameters, an area of interest is therefore to be able to measure blood lipids with a hematology analyzer. Thus, this study aims to investigate the possibility of detecting and measuring blood lipids with a hematology analyzer, using flow cytometry and/or spectrophotometry.</p><p>In order to investigate this possibility, two simulating methods were conducted where in the first method Intralipid 20% was mixed with saline into samples with different lipid concentrations. In the second method, diluent was used instead of saline. Lastly a Correlation study was performed where Intralipid 20% was mixed with donor blood to prepare samples with different lipid concentrations. All samples were then analyzed in a hematology analyser and scatter plots from flow cytometry and light absorption data from spectrophotometry measurements were obtained. The methods showed that there is a strong correlation between number of detected pulse counts from the scatter plots and lipid concentration. Same applies to light absorption compared to the lipid concentration of the samples, measured with spectrophotometry.</p><p>The results from this study show that it is in fact possible to detect and measure blood lipid levels with a hematology analyser using flow cytometry and spectrophotometry. Further development within this area could therefore enable simple screening of this additional parameter and early detection of indications of hyperlipidemia.</p>
----------------------------------------------------------------------
In diva2:1817475 abstract is:
<p>Both commuting to work and long-distance travelling with one's own bicycle have been intrend for years and a large market has emerged. While the reasons on the customer side aremainly sustainability and sporting activity, the companies offer customised products for manydifferent use-cases.In this master's thesis, a transport box for bicycles is being developed that can be taken on aplane, for example, and converted into a cargo trailer at the destination. A market researchshows that such a product is already available for certain folding bikes, whereas the goal is auniversal solution for utility bikes as well as mountain bikes.The methodological development follows a standard that divides the process into four phases.After the market research, main functions of the product are identified, which are "Transport abicycle as luggage" and "Carry goods during the bicycle ride". These are then divided intosub-functions in order to find different design variants for each function, combine them intodifferent drafts with the help of a morphological box and finally evaluate according totechnical and economic criteria, so that a final draft is determined at the end of this phase. It isa hard case providing enough space for different types of bicycles, that can be converted intoa trailer by mounting two wheels on the sides and a tow bar.In phase 3, the wheel size is set at 12 inches due to small space requirements and weight, andthe material is set to ABS-plastic for reasons of sustainability and mechanical properties.The final phase involves 3D design using Fusion360 with drawing derivation and validationof the model. The model shows that the requirements regarding geometry and weight havebeen implemented and that a practical transport option for bicycles has been found. In order topotentially launch the product on the market, further investigations such as FEM analysis ordynamic simulations are necessary.</p>

corrected abstract:
<p>Both commuting to work and long-distance travelling with one's own bicycle have been in trend for years and a large market has emerged. While the reasons on the customer side are mainly sustainability and sporting activity, the companies offer customised products for many different use-cases.</p><p>In this master's thesis, a transport box for bicycles is being developed that can be taken on a plane, for example, and converted into a cargo trailer at the destination. A market research shows that such a product is already available for certain folding bikes, whereas the goal is a universal solution for utility bikes as well as mountain bikes.</p><p>The methodological development follows a standard that divides the process into four phases. After the market research, main functions of the product are identified, which are "Transport a bicycle as luggage" and "Carry goods during the bicycle ride". These are then divided into sub-functions in order to find different design variants for each function, combine them into different drafts with the help of a morphological box and finally evaluate according to technical and economic criteria, so that a final draft is determined at the end of this phase. It is a hard case providing enough space for different types of bicycles, that can be converted into a trailer by mounting two wheels on the sides and a tow bar.</p><p>In phase 3, the wheel size is set at 12 inches due to small space requirements and weight, and the material is set to ABS-plastic for reasons of sustainability and mechanical properties. The final phase involves 3D design using Fusion360 with drawing derivation and validation of the model. The model shows that the requirements regarding geometry and weight have been implemented and that a practical transport option for bicycles has been found. In order to potentially launch the product on the market, further investigations such as FEM analysis or dynamic simulations are necessary.</p>
----------------------------------------------------------------------
In diva2:1345189 abstract is:
<p>-aminobutyric acid receptors of type A (GABAARs) are the majorinhibitory neurotransmitter receptors in the human brain, andare modulated by a vast range of exogenous molecules, such assedatives and anesthetics. In the last year, the first cryo-electronmicroscopy (cryo-EM) images of the closed and desensitized statesof the GABAAR were released, enabling fruitful research throughsimulations of these complex proteins. This report investigatesthe characteristics of the two structures. Specifically, pore hydration,radius, and hydrophobicity is compared, and a major focuslies in the general anesthetic (GA) binding pockets in the transmembranedomain, as well as the ligands propofol, etomidate,and pentobarbital. Furthermore, different models for the missingstructure of the intracellular domain (ICD) are compared. Thestructures were simulated for 1 μs using GROMACS. Using multiplesequence alignment as the basis of different models with theheptapeptid SQPARAA in the place of the ICD, resulted in stablestructures with a backbone RMSD close to 2 Å after 1 μs. Thepores are shown to exhibit significant differences between the twostates, with heavier constriction at the 9’ site of the closed state,but also suspected faulty expansion of the pore near the top in thedesensitized state after equilibration. Two of the pockets in thedesensitized state further deviates from expectation, by being tooconstricted. The other pockets were large enough to bind ligandsin the desensitized state, but not in the closed state, as expected.The binding analysis of the GAs suggests that etomidate bindswith the phenyl ring pointing towards the ICD, and that pentobarbitalbinds with the head group pointing towards the pore. Italso suggests that the GAs can bind to every GA pocket, but thatmodulatory activity is dependent on consistently low binding energies,which varies between the ligands for the different pockets.</p>

corrected abstract:
<p>γ-aminobutyric acid receptors of type A (GABA<sub>A</sub>Rs) are the major inhibitory neurotransmitter receptors in the human brain, and are modulated by a vast range of exogenous molecules, such as sedatives and anesthetics. In the last year, the first cryo-electron microscopy (cryo-EM) images of the closed and desensitized states of the GABA<sub>A</sub>R were released, enabling fruitful research through simulations of these complex proteins. This report investigates the characteristics of the two structures. Specifically, pore hydration, radius, and hydrophobicity is compared, and a major focus lies in the general anesthetic (GA) binding pockets in the transmembrane domain, as well as the ligands propofol, etomidate, and pentobarbital. Furthermore, different models for the missing structure of the intracellular domain (ICD) are compared. The structures were simulated for 1 µs using GROMACS. Using multiple sequence alignment as the basis of different models with the heptapeptid SQPARAA in the place of the ICD, resulted in stable structures with a backbone RMSD close to 2 Å after 1 µs. The pores are shown to exhibit significant differences between the two states, with heavier constriction at the 9’ site of the closed state, but also suspected faulty expansion of the pore near the top in the desensitized state after equilibration. Two of the pockets in the desensitized state further deviates from expectation, by being too constricted. The other pockets were large enough to bind ligands in the desensitized state, but not in the closed state, as expected. The binding analysis of the GAs suggests that etomidate binds with the phenyl ring pointing towards the ICD, and that pentobarbital binds with the head group pointing towards the pore. It also suggests that the GAs can bind to every GA pocket, but that modulatory activity is dependent on consistently low binding energies, which varies between the ligands for the different pockets.</p>
----------------------------------------------------------------------
In diva2:1307667 abstract is:
<p>The industrial robot is a flexible and cheap standard component that can becombined with a milling head to complete low accuracy milling tasks. Thefuture goal for researchers and industry is to increase the milling accuracy, suchthat it can be introduced to more high value added operations.The serial build up of an industrial robot bring non-linear compliance andchallenges in vibration mitigation due to the member and reducer design. WithAdditive Manufacturing (AM), the traditional cast aluminum structure couldbe revised and, therefore, milling accuracy gain could be made possible due tostructural changes.This thesis proposes the structural changes that would improve the millingaccuracy for a specific trajectory. To quantify the improvement, first the robothad to be reverse engineered and a kinematic simulation model be built. Nextthe kinematic simulation process was automated such that multiple input parameterscould be varied and a screening conducted that proposed the mostprofitable change.It was found that a mass decrease in any member did not affect the millingaccuracy and a stiffness increase in the member of the second axis would increasethe milling accuracy the most, without changing the design concept. To changethe reducer in axis 1 would reduce the mean position error by 7.5 % and themean rotation error by 4.5 % approximately, but also reduces the maximumspeed of the robot. The best structural change would be to introduce twosupport bearings for axis two and three, which decreased the mean positioningerror and rotation error by approximately 8 % and 13 % respectively.</p>

corrected abstract:
<p>The industrial robot is a flexible and cheap standard component that can be combined with a milling head to complete low accuracy milling tasks. The future goal for researchers and industry is to increase the milling accuracy, such that it can be introduced to more high value added operations.</p><p>The serial build up of an industrial robot bring non-linear compliance and challenges in vibration mitigation due to the member and reducer design. With Additive Manufacturing (AM), the traditional cast aluminum structure could be revised and, therefore, milling accuracy gain could be made possible due to structural changes.</p><p>This thesis proposes the structural changes that would improve the milling accuracy for a specific trajectory. To quantify the improvement, first the robot had to be reverse engineered and a kinematic simulation model be built. Next the kinematic simulation process was automated such that multiple input parameters could be varied and a screening conducted that proposed the most profitable change.</p><p>It was found that a mass decrease in any member did not affect the milling accuracy and a stiffness increase in the member of the second axis would increase the milling accuracy the most, without changing the design concept. To change the reducer in axis 1 would reduce the mean position error by 7.5 % and the mean rotation error by 4.5 % approximately, but also reduces the maximum speed of the robot. The best structural change would be to introduce two support bearings for axis two and three, which decreased the mean positioning error and rotation error by approximately 8 % and 13 % respectively.</p>
----------------------------------------------------------------------
In diva2:1142911 abstract is:
<p>Recommender systems can be seen everywhere today,having endless possibilities of implementation. However,operating in the background, they can easily be passed withoutnotice. Essentially, recommender systems are algorithms thatgenerate predictions by operating on a certain data set. Eachcase of recommendation is environment sensitive and dependenton the condition of the data at hand. Consequently, it is difficultto foresee which method, or combination of methods, to apply in aparticular situation for obtaining desired results. The area of recommendersystems that this thesis is delimited to is Collaborativefiltering (CF) and can be split up into three different categories,namely memory based, model based and hybrid algorithms. Thisthesis implements a CF algorithm for each of these categoriesand sets focus on comparing their prediction accuracy and theirdependency on the amount of available training data (i.e. asa function of sparsity). The results show that the model basedalgorithm clearly performs better than the memory based, bothin terms of overall accuracy and sparsity dependency. With anincreasing sparsity level, the problem of having users without anyratings is encountered, which greatly impacts the accuracy forthe memory based algorithm. A hybrid between these algorithmsresulted in a better accuracy than the model based algorithmitself but with an insignificant improvement.</p>

corrected abstract:
<p>Recommender systems can be seen everywhere today, having endless possibilities of implementation. However, operating in the background, they can easily be passed without notice. Essentially, recommender systems are algorithms that generate predictions by operating on a certain data set. Each case of recommendation is environment sensitive and dependent on the condition of the data at hand. Consequently, it is difficult to foresee which method, or combination of methods, to apply in a particular situation for obtaining desired results. The area of recommender systems that this thesis is delimited to is Collaborative filtering (CF) and can be split up into three different categories, namely memory based, model based and hybrid algorithms. This thesis implements a CF algorithm for each of these categories and sets focus on comparing their prediction accuracy and their dependency on the amount of available training data (i.e. as a function of sparsity). The results show that the model based algorithm clearly performs better than the memory based, both in terms of overall accuracy and sparsity dependency. With an increasing sparsity level, the problem of having users without any ratings is encountered, which greatly impacts the accuracy for the memory based algorithm. A hybrid between these algorithms resulted in a better accuracy than the model based algorithm itself but with an insignificant improvement.</p>
----------------------------------------------------------------------
In diva2:1078073 spaces missing in title:
"An Experimental Study of Fibre SuspensionFlows in Pipes using Nuclear MagneticResonance Imaging"
==>
"An Experimental Study of Fibre Suspension Flows in Pipes using Nuclear Magnetic Resonance Imaging"

abstract is:
<p>This study deals with fibre suspension flows through cylindrical pipes. Thepresent work aims at measurements of opaque flows, which are common inindustries. Nuclear magnetic resonance imaging (NMRI) and ultrasound velocimetryprofiling (UVP) were employed as non-invasive and optic-independenttools to measure the velocity profiles. As a first experiment, a paper-pulp suspensionflow through a sudden contraction and expansion was investigated.The results show the NMRI technique can be used to measure the stronglyunsteady flow such as separated regions though the MR signal is attenuateddue to the turbulence in the flow. The flow loop had however an insufficientinlet length which caused asymmetric profiles at the test section. As a secondexperiment, a flow loop which provided fully developed flows at the test sectionwas designed. After that, the velocity profiles of rayon-fibre and micro-spheresuspension flows were measured by the NMRI and the UVP independently.In principle, these two techniques measure the different velocities of the fibresuspensionflows, i.e. the velocity of the water and the fibre. In dilute suspensionflows, where the velocities of the two phases were assumed to be thesame, the velocity profiles were in good agreement. This shows the validityof the two measurement techniques. However, it should be pointed out thatthere is a limitation of the current UVP method for highly concentrated flows.The velocity profiles obtained by the UVP at high concentrations seems notto represent physics while the NMRI is not affected by the concentrations. Itis argued that the advances of the NMRI for the measurement of the highlyconcentrated flows.</p>

corrected abstract:
<p>This study deals with fibre suspension flows through cylindrical pipes. The present work aims at measurements of opaque flows, which are common in industries. Nuclear magnetic resonance imaging (NMRI) and ultrasound velocimetry profiling (UVP) were employed as non-invasive and optic-independent tools to measure the velocity profiles. As a first experiment, a paper-pulp suspension flow through a sudden contraction and expansion was investigated. The results show the NMRI technique can be used to measure the strongly unsteady flow such as separated regions though the MR signal is attenuated due to the turbulence in the flow. The flow loop had however an insufficient inlet length which caused asymmetric profiles at the test section. As a second experiment, a flow loop which provided fully developed flows at the test section was designed. After that, the velocity profiles of rayon-fibre and micro-sphere suspension flows were measured by the NMRI and the UVP independently. In principle, these two techniques measure the different velocities of the fibresuspension flows, i.e. the velocity of the water and the fibre. In dilute suspension flows, where the velocities of the two phases were assumed to be the same, the velocity profiles were in good agreement. This shows the validity of the two measurement techniques. However, it should be pointed out that there is a limitation of the current UVP method for highly concentrated flows. The velocity profiles obtained by the UVP at high concentrations seems not to represent physics while the NMRI is not affected by the concentrations. It is argued that the advances of the NMRI for the measurement of the highly concentrated flows.</p>
----------------------------------------------------------------------
 diva2:854657 error in title:
 "Analysis oft yre wear using the expanded brush tyre model"
 ==>
 ""Analysis of tyre wear using the expanded brush tyre model"
 
abstract is:
<p>Approximately 60 000 tonnes of tyres are produced annually in Sweden to meet thedemand in the market. It is believed that 10 000 tonnes of rubber particles contaminatesthe Swedish roads every year. Some of the elements in the emitted particles cannegatively impact the environment. These elements can lead to leaching in water thatcan cause serious problems to aquatic organisms. Furthermore worn out tyres negativelyinfluence the driving dynamics. It increases the risk of aquaplaning which can have fatalconsequences. Innovative ways of recycling tyres are constantly being developed butstill faces major challenges. It is therefore important to understand tyre wear, whatinfluences it and how to reduce it.</p><p>The aim of the project is to acquire knowledge related to tyre wear, its environmentalimpacts, use a mathematical model to simulate tyre wear and study how the differentparameters influences wear.</p><p>First a literature survey was performed to acquire knowledge related to tyre wear. It wasfound that tyre wear is mainly due to adhesive and hysteresis wear. Several factors werefound to affect tyre wear for example velocity, slip angle and the type of road surface.The environment impact was also studied and the results show the composition of theparticles emitted to the environment. Some of the emitted particles negatively affect theaquatic organism and human beings.</p><p>In the second part of the project a mathematical model based on the well-known brushtyre model was used to simulate how wear changes with different parameters. Themodel created at KTH Vehicle Dynamics is named the expanded brush tyre model(EBM). The wear model chosen for this evaluation was the Archards wear law. Thismodel was used to be able to quantify wear and study how it is influenced by differentfactors.</p><p>The result of the mathematical models shows clearly an exponential increase in thevolume of wear with increases in velocity, slip angle and vertical load. The analysis wasdone using zero camber angle.</p><p>For future work it is recommended to investigate camber angle as it is also one of themajor factors that affects wear. Temperature is also another factor that was not taken into account in the study. It can also be studied in future work.</p>

corrected abstract:
<p>Approximately 60 000 tonnes of tyres are produced annually in Sweden to meet the demand in the market. It is believed that 10 000 tonnes of rubber particles contaminates the Swedish roads every year. Some of the elements in the emitted particles can negatively impact the environment. These elements can lead to leaching in water that can cause serious problems to aquatic organisms. Furthermore worn out tyres negatively influence the driving dynamics. It increases the risk of aquaplaning which can have fatal consequences. Innovative ways of recycling tyres are constantly being developed but still faces major challenges. It is therefore important to understand tyre wear, what influences it and how to reduce it.</p><p>The aim of the project is to acquire knowledge related to tyre wear, its environmental impacts, use a mathematical model to simulate tyre wear and study how the different parameters influences wear.</p><p>First a literature survey was performed to acquire knowledge related to tyre wear. It was found that tyre wear is mainly due to adhesive and hysteresis wear. Several factors were found to affect tyre wear for example velocity, slip angle and the type of road surface. The environment impact was also studied and the results show the composition of the particles emitted to the environment. Some of the emitted particles negatively affect the aquatic organism and human beings.</p><p>In the second part of the project a mathematical model based on the well-known brushf tyre model was used to simulate how wear changes with different parameters. The model created at KTH Vehicle Dynamics is named the expanded brush tyre model (EBM). The wear model chosen for this evaluation was the Archards wear law. This model was used to be able to quantify wear and study how it is influenced by different factors.</p><p>The result of the mathematical models shows clearly an exponential increase in the volume of wear with increases in velocity, slip angle and vertical load. The analysis was done using zero camber angle.</p><p>For future work it is recommended to investigate camber angle as it is also one of the major factors that affects wear. Temperature is also another factor that was not taken in to account in the study. It can also be studied in future work.</p>
----------------------------------------------------------------------
In diva2:1890520 abstract is:
<p>The observed cases of increased radon emissions from the earth precedingheightened seismic activity have given rise to numerous papers exploringthe viability of using radon concentration levels in soil gas and groundwateras a precursor in earthquake forecasting. In this paper, these methods areexplored further through a statistical analysis of the initial data collected by thegamma detectors installed within the scope of ArtEmis, a project funded bythe European Union with the goal of producing a reliable model for earthquakeforecasting using measured radioactivity as a precursor. A Gaussian RandomWalk model is implemented using Integrated Nested Laplace Approximationto infer a set of points defining the hidden distribution from which the observeddata are drawn. The model is implemented and trained on data sets recordedby five gamma detectors. The inferences made by the model imply thatthe model is applicable to the data collected by four of the detectors. Theinferred distributions are compared to seismic data collected by 142 seismicobservatories in Greece. No significant correlations between the inferredchanges in radon levels and seismic activity were found. The impact ofchanging the sampling frequency of radon data is investigated, with theconclusion that the created model infers a distribution corresponding to the rawdata obtained with a lower sampling frequency. It is concluded that the modelshould be improved further for more advanced analyses, with some of the mostimportant developments suggested being the inclusion of meteorological dataand upgrading from a Gaussian Random Walk of order 1 to order 2. Moreseismic and radiation data is also deemed necessary for meaningful analysesof the correlation between the two. More advanced analyses of the availableseismic data are required for improved classification of the events expected togive rise to precursory phenomena observable in the radiation data collectedby the various detectors.</p>


corrected abstract:
<p>The observed cases of increased radon emissions from the earth preceding heightened seismic activity have given rise to numerous papers exploring the viability of using radon concentration levels in soil gas and ground water as a precursor in earthquake forecasting. In this paper, these methods are explored further through a statistical analysis of the initial data collected by the gamma detectors installed within the scope of ArtEmis, a project funded by the European Union with the goal of producing a reliable model for earthquake forecasting using measured radioactivity as a precursor. A Gaussian Random Walk model is implemented using Integrated Nested Laplace Approximation to infer a set of points defining the hidden distribution from which the observed data are drawn. The model is implemented and trained on data sets recorded by five gamma detectors. The inferences made by the model imply that the model is applicable to the data collected by four of the detectors. The inferred distributions are compared to seismic data collected by 142 seismic observatories in Greece. No significant correlations between the inferred changes in radon levels and seismic activity were found. The impact of changing the sampling frequency of radon data is investigated, with the conclusion that the created model infers a distribution corresponding to the raw data obtained with a lower sampling frequency. It is concluded that the model should be improved further for more advanced analyses, with some of the most important developments suggested being the inclusion of meteorological data and upgrading from a Gaussian Random Walk of order 1 to order 2. More seismic and radiation data is also deemed necessary for meaningful analyses of the correlation between the two. More advanced analyses of the available seismic data are required for improved classification of the events expected to give rise to precursory phenomena observable in the radiation data collected by the various detectors.</p>
----------------------------------------------------------------------
In diva2:1881327
abstract is:
<p>Formula Student is Europe’s most established engineering competition, with teamsall over the world. Practical problem solving in combination with applyingacademic knowledge, give students the opportunity to explore their field of study inan exciting and meaningful way.</p><p>Aerodynamic development of race cars have seen significant results in competitionsince its introduction in the 1960s. Initial designs were adaptations of aerospaceconcepts for ground vehicles. Development relied solely on track- and wind tunneltesting but despite their rudimentary designs, significant performance increaseswere made. The purpose of aerodynamic development of race cars is to balance thecar, getting it to behave as desired. As a consequence of the forces generated, thevehicle corners faster at the cost of acceleration and top speed. With more powerfulcomputers, earlier unsolvable equations started to get numerically solved andcomputational fluid dynamics was born. CFD introduced the possibility for rapiditeration and exploration of more intricate designs. This report will solely utilizeCFD as a simulation tool, recognising its limitations in accuracy and real worldcorrelation.</p><p>The aim of this study is to increase downforce on the front wing, whilst beingcautious of downstream impact. The goal set by the team is an adjustable frontwing that generates as much downforce as possible, whilst allowing for adjustmentsto shift the center of pressure by promoting more air to the side-structure. Toachieve this, an iterative design process based on literature is the chosen method.Continuous cross evaluations with other parts of the design team is of the highestimportance to avoid poor interaction between aerodynamic devices.</p><p>The (negative) lift coefficient was increased from 4.7 to 5.7 for the entire vehicle, byonly improving the front wing. This was very satisfactory as increases upstreamoften lead do degraded performance downstream. An increased lift coefficient ofover 20%, with improvements to front wheel drag and similar side-structureperformance, demonstrate the quality and effectiveness of the design.</p>

corrected abstract:
<p>Formula Student is Europe’s most established engineering competition, with teams all over the world. Practical problem solving in combination with applying academic knowledge, give students the opportunity to explore their field of study in an exciting and meaningful way.</p><p>Aerodynamic development of race cars have seen significant results in competition since its introduction in the 1960s. Initial designs were adaptations of aerospace concepts for ground vehicles. Development relied solely on track- and wind tunnel testing but despite their rudimentary designs, significant performance increases were made. The purpose of aerodynamic development of race cars is to balance the car, getting it to behave as desired. As a consequence of the forces generated, the vehicle corners faster at the cost of acceleration and top speed. With more powerful computers, earlier unsolvable equations started to get numerically solved and computational fluid dynamics was born. CFD introduced the possibility for rapid iteration and exploration of more intricate designs. This report will solely utilize CFD as a simulation tool, recognising its limitations in accuracy and real world correlation.</p><p>The aim of this study is to increase downforce on the front wing, whilst being cautious of downstream impact. The goal set by the team is an adjustable front wing that generates as much downforce as possible, whilst allowing for adjustments to shift the center of pressure by promoting more air to the side-structure. To achieve this, an iterative design process based on literature is the chosen method. Continuous cross evaluations with other parts of the design team is of the highest importance to avoid poor interaction between aerodynamic devices.</p><p>The (negative) lift coefficient was inc
reased from 4.7 to 5.7 for the entire vehicle, by only improving the front wing. This was very satisfactory as increases upstream often lead do degraded performance downstream. An increased lift coefficient of over 20%, with improvements to front wheel drag and similar side-structure performance, demonstrate the quality and effectiveness of the design.</p>
----------------------------------------------------------------------
In diva2:1880362 

abstract is:

<p>Measurements of radon in groundwater before, during and after the 1995 Kobe earth-quake in Japan indicated that there might be a correlation between levels of 222Rn ingroundwater and seismological activity. The artEmis project investigates this possibleconnection with the goal of building a network of detectors in seismically active parts ofEurope. The detectors will be placed in groundwater and measure many factors, one ofthem being the radon level by measuring gamma radiation. The original vision for thedetectors also included alpha detection. The obtained data is analyzed with artificialintelligence.</p><p>This thesis investigates a possible method for alpha detection under water. Specif-ically by seeing if it is possible for radon dissolved in water to diffuse from the water,through silicone tubes and into the air inside of the silicone tubes. There is a possibilityfor alpha detection of the radon decay if the radon gas could get into the air. This wasinvestigated by submerging an air-filled silicone construction in water with high levelsof radon. The level of radon in the water was increased by placing pieces of lightweightconcrete in the water. The construction was removed after a period of time and itsgamma-ray spectrum was measured. A statistically significant increase in radon levelscompared to the background radiation would indicate that diffusion happened.</p><p>Measurements of the silicone construction with a germanium detector resulted ingamma spectra that were analyzed with a Python program to determine the activity of222Rn over time. Short measurements, around 1 hour long, showed a significant increaseof radon compared to the background. For longer measurements however, around one ortwo days, this effect was no longer apparent. The conclusion is that radon diffused intothe silicone construction, either into the silicone material itself or into the air inside theconstruction, but it comes out again quickly. If the radon diffused into the air inside ofthe silicone, the use of alpha detection to measure radon levels in groundwater is muchless far-fetched than before. Therefore, the artEmis project might be one step closer tousing alpha detection in their detector network.</p>


corrected abstract:
<p>Measurements of radon in groundwater before, during and after the 1995 Kobe earth-quake in Japan indicated that there might be a correlation between levels of <sup>222</sup>Rn in groundwater and seismological activity. The artEmis project investigates this possible connection with the goal of building a network of detectors in seismically active parts of Europe. The detectors will be placed in groundwater and measure many factors, one of them being the radon level by measuring gamma radiation. The original vision for the detectors also included alpha detection. The obtained data is analyzed with artificial intelligence.</p><p>This thesis investigates a possible method for alpha detection under water. Specifically by seeing if it is possible for radon dissolved in water to diffuse from the water, through silicone tubes and into the air inside of the silicone tubes. There is a possibility for alpha detection of the radon decay if the radon gas could get into the air. This was investigated by submerging an air-filled silicone construction in water with high levels of radon. The level of radon in the water was increased by placing pieces of lightweight concrete in the water. The construction was removed after a period of time and its gamma-ray spectrum was measured. A statistically significant increase in radon levels compared to the background radiation would indicate that diffusion happened.</p><p>Measurements of the silicone construction with a germanium detector resulted in gamma spectra that were analyzed with a Python program to determine the activity of <sup>222</supZRn over time. Short measurements, around 1 hour long, showed a significant increase of radon compared to the background. For longer measurements however, around one or two days, this effect was no longer apparent. The conclusion is that radon diffused into the silicone construction, either into the silicone material itself or into the air inside the construction, but it comes out again quickly. If the radon diffused into the air inside of the silicone, the use of alpha detection to measure radon levels in groundwater is much less far-fetched than before. Therefore, the artEmis project might be one step closer to using alpha detection in their detector network.</p>
----------------------------------------------------------------------
In diva2:1779369 -error in title:
"Computational Fluid Dynamics of the flow in a diffuser: - like geometry"
==>
"Computational Fluid Dynamics of the flow in a diffuser - like geometry"

If you want to include the subtitle, it is:
"Computational Fluid Dynamics of the flow in a diffuser - like geometry:
A study of flow separation using Computational Fluid Dynamics"

abstract is:
<p>Simulations were performed to investigate flow separation of an asymmetricdiffuser - like geometry. The geometry used for the simulations was modeledafter an experimental setup with recorded flow data, which was compared tothe simulated data. For all simulations, steady state flow at the inlet was usedwith the assumption of a 2D flow.A grid convergence study consisting of three different grids was performed.From this study no apparent change in simulation results were observed forfiner grids. This is caused by the fact that the coarse grid had a high enoughresolution to fully capture the flow, meaning that the higher resolution gridsyielded small improvements.Additionally, two different turbulence models RN G k − ε and SST k − ωwere used for evaluating which model was best suited to model flow separation.The simulations showed that the RN G k − ε model could not capture the flowseparation and had a poor accuracy when predicting the turbulent kinetic energy(TKE). Simulation results from SST k − ω gave good results in capturing flowseparation and predicting both the velocity and TKE when compared to theexperimental data.Finally, a turbulence intensity study was made for the mid grid with theSST k − ω model. The turbulent intensity was set to 5%, 10%, 15% and 20%at the inlet. This resulted in the point of separation moving further down thegeometry to x/H ≈ [17.68, 18.71, 19.58, 20.72] for respective intensity. The pointof reattachment also moves to x/H ≈ [44.85, 43.60, 42.67, 41.67] for respectiveintensity.In summary for simulating flow separation in turbulent flows the SST k − ωmodel is optimal and an increase in turbulent intensity reduces the recirculationzone.</p>


corrected abstract:
<p>Simulations were performed to investigate flow separation of an asymmetric diffuser - like geometry. The geometry used for the simulations was modeled after an experimental setup with recorded flow data, which was compared to the simulated data. For all simulations, steady state flow at the inlet was used with the assumption of a 2D flow.</p><p>A grid convergence study consisting of three different grids was performed. From this study no apparent change in simulation results were observed for finer grids. This is caused by the fact that the coarse grid had a high enough resolution to fully capture the flow, meaning that the higher resolution grids yielded small improvements.</p><p>Additionally, two different turbulence models <em>RNG k − ε</em> and <em>SST k − ω</em>  were used for evaluating which model was best suited to model flow separation. The simulations showed that the <em>RNG k − ε</em> model could not capture the flow separation and had a poor accuracy when predicting the turbulent kinetic energy (TKE). Simulation results from <em>SST k − ω</em> gave good results in capturing flow separation and predicting both the velocity and TKE when compared to the experimental data.</p><p>Finally, a turbulence intensity study was made for the mid grid with the <em>SST k − ω</em> model. The turbulent intensity was set to 5%, 10%, 15% and 20% at the inlet. This resulted in the point of separation moving further down the geometry to <em>x/H</em> ≈ [17.68, 18.71, 19.58, 20.72] for respective intensity. The point of reattachment also moves to <em>x/H</em> ≈ [44.85, 43.60, 42.67, 41.67] for respective intensity.</p><p>In summary for simulating flow separation in turbulent flows the <em>SST k − ω</em> model is optimal and an increase in turbulent intensity reduces the recirculation zone.</p>
----------------------------------------------------------------------
In diva2:1576894 correct the title:
"Tensile strength reduction for insufficient thread engagement A FEM study of a wall-shoe assembly"
==>
"Tensile strength reduction for insufficient thread engagement: A FEM study of a wall-shoe assembly"

abstract is:
<p>The purpose of this master thesis is to determine whether or not it exist, a model that can describethe reduction in strength, due to missing threads in a bond between a bolt and nut. And how thereduction in strength might effect a wall-shoe assembly, used to connect a wall to another wall or aconcrete base plate. This is done by firstly considering the strength of the entire wall-shoe assembly andthe strength of the bond between the nut and the bolt is then considered.</p><p>The strength of the entire assembly is calculated using some simple analytical models. The strengthof the bolt is the limiting factor for the assembly given the analytical models. Four FEM-models arethen created, three to evaluate the strength of the bolt and nut assembly, for bolt sized from M6to M60 with thread engagement from one thread to seven threads. </p><p>An elastic model without a defined tensile stress limit is proposed. The maximum stress at the sharpgeometry changes (stress concentrations) are used to dimension the maximum allowed load accordingto the von Mises yield criterion. </p><p>The yield limit is implemented by introducing a elastic-plastic (without hardening) for three different materials. Where the maximum yield force is determined as the maximum reactionforce on the frictionless support boundary condition, when a displacement is applied.</p><p>Material hardening is applied according to a bi-linear material model (with hardening). Thereaction force is evaluated and the maximum force and displacement can be determined using thedefinition of property class 8.8 that propose a yield limit of 80 percent of maximum load.</p><p>The behavior of the anchor bolt when it is pulled out of the concrete was also modeled. To obtain anunderstanding of the pull-out behavior.</p><p>The FEM-models makes it possible to formulate a simple reduction model, where the bolt failure loadis reduced by the calculated reduction factors. The reduction factors are dependent on the amountof missing thread due to damages or insufficient bolt height. The reduction factors are also highlydependent on the ratio between the rise and bolt diameter. The reduction factors are significantlysmaller when a perfect plastic material model is applied.</p><p>The failure mode is dependent on the material model, when hardening is applied fewer threads areneeded to achieve bolt failure rather then thread failure, compared to when a ideal plastic materialmodel is applied.</p><p>The reduction factors are not affected by the material yield limit, the maximal load is however highlydependent on the yield limit.</p><p>The placement of the missing thread does not effect the reduction factors. They can therefor beused regardless of if threads are missing due to damage or due to partial thread engagement.</p><p>A test of an M8 bolt was performed to attempt to validate the FEM-model. Due to some inherent flawsin the test procedure no clear conclusion, about the validity of the model can be made. It is howeverclear that missing threads induces risk of failure when tightening the assembly, since the bolt or nutcan be damaged without any clear signs that the assembly is compromised, leading to catastrophicfailure. Reducing the load with the reduction factor model should be done with caution.</p>


corrected abstract:
<p>The purpose of this master thesis is to determine whether or not it exist, a model that can describe the reduction in strength, due to missing threads in a bond between a bolt and nut. And how the reduction in strength might effect a wall-shoe assembly, used to connect a wall to another wall or a concrete base plate. This is done by firstly considering the strength of the entire wall-shoe assembly and the strength of the bond between the nut and the bolt is then considered.</p><p>The strength of the entire assembly is calculated using some simple analytical models. The strength of the bolt is the limiting factor for the assembly given the analytical models. Four FEM-models are then created, three to evaluate the strength of the bolt and nut assembly, for bolt sized from M6 to M60 with thread engagement from one thread to seven threads.</p><ul><li>An elastic model without a defined tensile stress limit is proposed. The maximum stress at the sharp geometry changes (stress concentrations) are used to dimension the maximum allowed load according to the von Mises yield criterion.</li><li>The yield limit is implemented by introducing a elastic-plastic (without hardening) for three different materials. Where the maximum yield force is determined as the maximum reaction force on the frictionless support boundary condition, when a displacement is applied.</li><li><p>Material hardening is applied according to a bi-linear material model (with hardening). The reaction force is evaluated and the maximum force and displacement can be determined using the definition of property class 8.8 that propose a yield limit of 80 percent of maximum load.</p><p>The behavior of the anchor bolt when it is pulled out of the concrete was also modeled. To obtain an understanding of the pull-out behavior.</p></li></ul><p>The FEM-models makes it possible to formulate a simple reduction model, where the bolt failure load is reduced by the calculated reduction factors. The reduction factors are dependent on the amount of missing thread due to damages or insufficient bolt height. The reduction factors are also highly dependent on the ratio between the rise and bolt diameter. The reduction factors are significantly smaller when a perfect plastic material model is applied.</p><p>The failure mode is dependent on the material model, when hardening is applied fewer threads are needed to achieve bolt failure rather then thread failure, compared to when a ideal plastic material model is applied.</p><p>The reduction factors are not affected by the material yield limit, the maximal load is however highly dependent on the yield limit.</p><p>The placement of the missing thread does not effect the reduction factors. They can therefor be used regardless of if threads are missing due to damage or due to partial thread engagement.</p><p>A test of an M8 bolt was performed to attempt to validate the FEM-model. Due to some inherent flaws in the test procedure no clear conclusion, about the validity of the model can be made. It is however clear that missing threads induces risk of failure when tightening the assembly, since the bolt or nut can be damaged without any clear signs that the assembly is compromised, leading to catastrophic failure. Reducing the load with the reduction factor model should be done with caution.</p>
----------------------------------------------------------------------
In diva2:1527832 abstract is:
<p>This master’s thesis covers the structuring and implementation of a digital testbench for the air brake system of freight trains. The test bench will serveto further improve the existing brake models at Transrail Sweden AB. Theseare used for the optimised calculation of train speed profiles by the DriverAdvisory System CATO. This work is based on the research of the technicalbackground, as well as the methodical approach to physical modelling anda modular implementation of the test bench. It gives full flexibility for thesimulation of customised train configurations using the European UIC brakesystem. Train length and vehicle arrangement can be adapted to the user’sspecific needs. For example, the test bench could be used for the simulation ofa train with distributed power. The system parameters are stored in a vehiclelibrary for the convenient generation of train configurations. This vehiclelibrary is freely expandable.The simulation is based on an equivalent electric circuit model which iscompleted with nozzle flow modelling. This model involves monitoring themain pipe, brake cylinder and reservoir pressure. Linear approximation is usedto obtain braking forces for the individual wagons and for the whole train. Thedepiction of the brake system behaviour is mostly accurate in the operationalscenarios, which is validated with measurement data. Additional calibrationis required for further reduction of the simulation errors and an extension ofthe model’s domain of validity. The test bench is developed by incrementaland iterative modelling and prepared for further improvements and variations,for example the adaption to the American AAR system variant.The presented work can also be used as a basis for similar implementationssuch as driving simulators. The methods are transferable to other applicationsof modular simulation.</p>

corrected abstract:
<p>This master’s thesis covers the structuring and implementation of a digital test bench for the air brake system of freight trains. The test bench will serve to further improve the existing brake models at Transrail Sweden AB. These are used for the optimised calculation of train speed profiles by the Driver Advisory System CATO. This work is based on the research of the technical background, as well as the methodical approach to physical modelling and a modular implementation of the test bench. It gives full flexibility for the simulation of customised train configurations using the European UIC brake system. Train length and vehicle arrangement can be adapted to the user’s specific needs. For example, the test bench could be used for the simulation of a train with distributed power. The system parameters are stored in a vehicle library for the convenient generation of train configurations. This vehicle library is freely expandable.</p><p>The simulation is based on an equivalent electric circuit model which is completed with nozzle flow modelling. This model involves monitoring the main pipe, brake cylinder and reservoir pressure. Linear approximation is used to obtain braking forces for the individual wagons and for the whole train. The depiction of the brake system behaviour is mostly accurate in the operational scenarios, which is validated with measurement data. Additional calibration is required for further reduction of the simulation errors and an extension of the model’s domain of validity. The test bench is developed by incremental and iterative modelling and prepared for further improvements and variations, for example the adaption to the American AAR system variant.</p><p>The presented work can also be used as a basis for similar implementations such as driving simulators. The methods are transferable to other applications of modular simulation.</p>
----------------------------------------------------------------------
In diva2:1500046 abstract is:
<p>The reliability of a mechanical system containing electronic packages is highly affectedby the environment the system is stationed in. The difference and fluctuationsbetween the ambient temperature and the operating temperature of the electronicpackage cause accumulation of inelastic strains in the package components thusdecreasing the service life. The most common failure modes of an electronic packagehas been identified from inspection of malfunctioning machines as cracks in the solderjoint and delamination between the glue and the die. Knowledge regarding therelationships between parameters affecting these failure modes, which are importantand which are not, is of high interest when developing new and existing products. SAAB AB would like to develop a methodology using design exploration to allow forevaluation of electronic packages using nonlinear finite element methods.</p><p>A surrogate model was created and parameterized with HyperMorph to be used forthree linear static variations of design of experiments, where both the performance ofthe methods themselves and the relative importance of the parameters were ofinterest. A connectivity condition was also implemented to allow for relativemovement between components while keeping the mesh intact. The designexploration was executed using a Taguchi design, a Modified extensive latticesequence design and a fractional factorial design where the three methods werecompared as well as the parameter significance analysed. An optimization was thenperformed to find the optimal parameter settings within the allowed bounds to beused where a nominal model and an optimized model are evaluated with animplemented creep law. The fatigue life of the two models were then estimated.</p>

corrected abstract:
<p>The reliability of a mechanical system containing electronic packages is highly affected by the environment the system is stationed in. The difference and fluctuations between the ambient temperature and the operating temperature of the electronic package cause accumulation of inelastic strains in the package components thus decreasing the service life. The most common failure modes of an electronic package has been identified from inspection of malfunctioning machines as cracks in the solder joint and delamination between the glue and the die. Knowledge regarding the relationships between parameters affecting these failure modes, which are important and which are not, is of high interest when developing new and existing products. SAAB AB would like to develop a methodology using design exploration to allow for evaluation of electronic packages using nonlinear finite element methods.</p><p>A surrogate model was created and parameterized with HyperMorph to be used for three linear static variations of design of experiments, where both the performance of the methods themselves and the relative importance of the parameters were of interest. A connectivity condition was also implemented to allow for relative movement between components while keeping the mesh intact. The design exploration was executed using a Taguchi design, a Modified extensive lattice sequence design and a fractional factorial design where the three methods were compared as well as the parameter significance analysed. An optimization was then performed to find the optimal parameter settings within the allowed bounds to be used where a nominal model and an optimized model are evaluated with an implemented creep law. The fatigue life of the two models were then estimated.</p>
----------------------------------------------------------------------
In diva2:1229161 missing hyphen in title:
"Direct optimization of dose-volume histogram metrics in intensity modulated radiation therapy treatment planning"
==>
"Direct optimization of dose-volume histogram metrics in intensity-modulated radiation therapy treatment planning"

abstract is:
<p>In optimization of intensity-modulated radiation therapy treatment plans, dose-volumehistogram (DVH) functions are often used as objective functions to minimize the violationof dose-volume criteria. Neither DVH functions nor dose-volume criteria, however,are ideal for gradient-based optimization as the former are not continuously differentiableand the latter are discontinuous functions of dose, apart from both beingnonconvex. In particular, DVH functions often work poorly when used in constraintsdue to their being identically zero when feasible and having vanishing gradients on theboundary of feasibility.In this work, we present a general mathematical framework allowing for direct optimizationon all DVH-based metrics. By regarding voxel doses as sample realizations ofan auxiliary random variable and using kernel density estimation to obtain explicit formulas,one arrives at formulations of volume-at-dose and dose-at-volume which are infinitelydifferentiable functions of dose. This is extended to DVH functions and so calledvolume-based DVH functions, as well as to min/max-dose functions and mean-tail-dosefunctions. Explicit expressions for evaluation of function values and corresponding gradientsare presented. The proposed framework has the advantages of depending on onlyone smoothness parameter, of approximation errors to conventional counterparts beingnegligible for practical purposes, and of a general consistency between derived functions.Numerical tests, which were performed for illustrative purposes, show that smoothdose-at-volume works better than quadratic penalties when used in constraints and thatsmooth DVH functions in certain cases have significant advantage over conventionalsuch. The results of this work have been successfully applied to lexicographic optimizationin a fluence map optimization setting.</p>

corrected abstract:
<p>In optimization of intensity-modulated radiation therapy treatment plans, dose-volume histogram (DVH) functions are often used as objective functions to minimize the violation of dose-volume criteria. Neither DVH functions nor dose-volume criteria, however, are ideal for gradient-based optimization as the former are not continuously differentiable and the latter are discontinuous functions of dose, apart from both being nonconvex. In particular, DVH functions often work poorly when used in constraints due to their being identically zero when feasible and having vanishing gradients on the boundary of feasibility.</p><p>In this work, we present a general mathematical framework allowing for direct optimization on all DVH-based metrics. By regarding voxel doses as sample realizations of an auxiliary random variable and using kernel density estimation to obtain explicit formulas, one arrives at formulations of volume-at-dose and dose-at-volume which are infinitely differentiable functions of dose. This is extended to DVH functions and so called volume-based DVH functions, as well as to min/max-dose functions and mean-tail-dose functions. Explicit expressions for evaluation of function values and corresponding gradients are presented. The proposed framework has the advantages of depending on only one smoothness parameter, of approximation errors to conventional counterparts being negligible for practical purposes, and of a general consistency between derived functions.</p><p>Numerical tests, which were performed for illustrative purposes, show that smooth dose-at-volume works better than quadratic penalties when used in constraints and that smooth DVH functions in certain cases have significant advantage over conventional such. The results of this work have been successfully applied to lexicographic optimization in a fluence map optimization setting.</p>
----------------------------------------------------------------------
In diva2:1120314 abstract is:
<p>A better understanding of turbine performance and its sensitivity to variations in the inletboundary conditions is crucial in the quest of further improving the efficiency of aero engines.Within the research efforts to reach this goal, a high-pressure turbine test rig has been designedby Rolls-Royce Deutschland in cooperation with the Deutsches Zentrum für Luft- und Raumfahrt(DLR), the German Aerospace Center. The scope of the test rig is high-precision measurement ofaerodynamic efficiency including the effects of film cooling and secondary air flows as well as theimprovement of numerical prediction tools, especially 3D Computational Fluid Dynamics (CFD).A sensitivity analysis of the test rig based on detailed 3D CFD computations was carried outwith the aim to quantify the influence of inlet boundary condition variations occurring in the testrig on the outlet capacity of the first stage nozzle guide vane (NGV) and the turbine efficiency.The analysis considered variations of the cooling and rimseal leakage mass flow rates as well asfluctuations in the inlet distributions of total temperature and pressure. The influence of anincreased rotor tip clearance was also studied.This thesis covers the creation, calibration and validation of the steady state 3D CFD modelof the full turbine domain. All relevant geometrical details of the blades, walls and the rimsealcavities are included with the exception of the film cooling holes that are replaced by a volumesource term based cooling strip model to reduce the computational cost of the analysis. Thehigh-fidelity CFD computation is run only on a sample of parameter combinations spread overthe entire input parameter space determined using the optimal latin hypercube technique. Thesubsequent sensitivity analysis is based on a Kriging response surface model fit to the sampledata. The results are discussed with regard to the planned experimental campaign on the test rigand general conclusions concerning the impacts of the studied parameters on turbine performanceare deduced.</p>


corrected abstract:
<p>A better understanding of turbine performance and its sensitivity to variations in the inlet boundary conditions is crucial in the quest of further improving the efficiency of aero engines. Within the research efforts to reach this goal, a high-pressure turbine test rig has been designed by Rolls-Royce Deutschland in cooperation with the Deutsches Zentrum für Luft- und Raumfahrt (DLR), the German Aerospace Center. The scope of the test rig is high-precision measurement of aerodynamic efficiency including the effects of film cooling and secondary air flows as well as the improvement of numerical prediction tools, especially 3D Computational Fluid Dynamics (CFD).</p><p>A sensitivity analysis of the test rig based on detailed 3D CFD computations was carried out with the aim to quantify the influence of inlet boundary condition variations occurring in the test rig on the outlet capacity of the first stage nozzle guide vane (NGV) and the turbine efficiency. The analysis considered variations of the cooling and rimseal leakage mass flow rates as well as fluctuations in the inlet distributions of total temperature and pressure. The influence of an increased rotor tip clearance was also studied.</p><p>This thesis covers the creation, calibration and validation of the steady state 3D CFD model of the full turbine domain. All relevant geometrical details of the blades, walls and the rimseal cavities are included with the exception of the film cooling holes that are replaced by a volume source term based cooling strip model to reduce the computational cost of the analysis. The high-fidelity CFD computation is run only on a sample of parameter combinations spread over the entire input parameter space determined using the optimal latin hypercube technique. The subsequent sensitivity analysis is based on a Kriging response surface model fit to the sample data. The results are discussed with regard to the planned experimental campaign on the test rig and general conclusions concerning the impacts of the studied parameters on turbine performance are deduced.</p>
----------------------------------------------------------------------
In diva2:618595 missing space in title:
"Future fuel for worldwide tankershipping in spot market"
==>
"Future fuel for worldwide tanker shipping in spot market"

abstract is:
<p>Ship exhausts contain high levels of sulphur oxides, nitrogen oxides, carbon dioxide and particles dueto the heavy fuel oil, HFO, used for combustion and the combustion characteristics of the engine.As a result of upcoming stricter regulations for shipping pollution, as well as growing attentionto greenhouse gas emissions, air pollution and uncertainty of future petroleum oil supply, a shifttowards a cleaner burning fuel is needed.This work explores potential alternative fuels, both conventional and unconventional, and abatementtechnologies, to be used by tankers in the worldwide spot market to comply with upcomingenvironmental regulations in the near and coming future. As a reference the product tanker M/TGotland Marieann is used and recommendations for which fuel that shall be used by the referenceship in 2015 and 2020 are presented.The environmental assessment and evaluation of the fuels are done from a life cycle perspective usingresults from Life Cycle Assessment, LCA, studies.This study illustrates that, of the various alternatives, methanol appears to be the best candidatefor long-term, widespread replacement of petroleum-based fuels within tanker shipping. It does notemit any sulphur oxides nor particles and the nitrogen oxides are shown to be lower than those ofmarine gas oil, MGO. The global warming potential of the natural gas produced methanol is notlower than that of MGO, but when gradually switching to bio-methanol the greenhouse gas emissionsare decreasing and with methanol the vision of a carbon free society can be reached.For 2015 a switch towards methanol is not seen as realistic. Further research and establishment ofregulations and distribution systems are needed, however there are indications that a shift will bepossible sometime between 2015 and 2020. For 2015 a shift towards MGO is suggested as it involveslow investment costs and there is no need for infrastructure changes. As MGO is more expensivethan methanol, a shift is preferable as soon as the market, technology and infrastructure are ready.</p>

corrected abstract:
<p>Ship exhausts contain high levels of sulphur oxides, nitrogen oxides, carbon dioxide and particles due to the heavy fuel oil, HFO, used for combustion and the combustion characteristics of the engine. As a result of upcoming stricter regulations for shipping pollution, as well as growing attention to greenhouse gas emissions, air pollution and uncertainty of future petroleum oil supply, a shift towards a cleaner burning fuel is needed.</p><p>This work explores potential alternative fuels, both conventional and unconventional, and abatement technologies, to be used by tankers in the worldwide spot market to comply with upcoming environmental regulations in the near and coming future. As a reference the product tanker M/T Gotland Marieann is used and recommendations for which fuel that shall be used by the reference ship in 2015 and 2020 are presented.</p><p>The environmental assessment and evaluation of the fuels are done from a life cycle perspective using results from Life Cycle Assessment, LCA, studies.</p><p>This study illustrates that, of the various alternatives, methanol appears to be the best candidate for long-term, widespread replacement of petroleum-based fuels within tanker shipping. It does not emit any sulphur oxides nor particles and the nitrogen oxides are shown to be lower than those of marine gas oil, MGO. The global warming potential of the natural gas produced methanol is not lower than that of MGO, but when gradually switching to bio-methanol the greenhouse gas emissions are decreasing and with methanol the vision of a carbon free society can be reached.</p><p>For 2015 a switch towards methanol is not seen as realistic. Further research and establishment of regulations and distribution systems are needed, however there are indications that a shift will be possible sometime between 2015 and 2020. For 2015 a shift towards MGO is suggested as it involves low investment costs and there is no need for infrastructure changes. As MGO is more expensive than methanol, a shift is preferable as soon as the market, technology and infrastructure are ready.</p>
----------------------------------------------------------------------
In diva2:401149 abstract is:
<p> </p>
<p> </p>
<p>During the 2000s, the ship owners have become more and more concerned thattheir ships save fuel. Several projects have been undertaken to exploit the resourcesavailable on board today’s vessels to reduce fuel consumption. As a stepin this the Swedish Meteorological and Hydrological Institute (SMHI) today offera Weather Routing service to ships. By planning your route more effectivelymuch fuel can be saved.This thesis has been about developing a fuel prediction program (FPP) forhow much fuel a ship consumes in different sea conditions. The model takes intoaccount the ship’s loading condition, winds, wind waves and swell. Any othereffects are pooled in one term. This makes it possible to also consider how muchfuel the ship consumes on the various route options in the planning process.The model will also be a useful tool to retrospectively evaluate how a ship hasperformed in relation to the contract.On the ships in this report the prediction program was able to calculate thefuel consumption with an error of only 1% of the reported fuel consumption.This requires that the data about the vessel is accurate and up to date. If not,the model can still, with thoughtful assumptions, reach an error of less than10% of the reported consumption, which is better than the strategy that SMHIuses today.</p>



corrected abstract:
<p>During the 2000s, the ship owners have become more and more concerned that their ships save fuel. Several projects have been undertaken to exploit the resources available on board today’s vessels to reduce fuel consumption. As a step in this the Swedish Meteorological and Hydrological Institute (SMHI) today offer a Weather Routing service to ships. By planning your route more effectively much fuel can be saved.</p><p>This thesis has been about developing a fuel prediction program (FPP) for how much fuel a ship consumes in different sea conditions. The model takes into account the ship’s loading condition, winds, wind waves and swell. Any other effects are pooled in one term. This makes it possible to also consider how much fuel the ship consumes on the various route options in the planning process. The model will also be a useful tool to retrospectively evaluate how a ship has performed in relation to the contract.</p><p>On the ships in this report the prediction program was able to calculate the fuel consumption with an error of only 1% of the reported fuel consumption. This requires that the data about the vessel is accurate and up to date. If not, the model can still, with thoughtful assumptions, reach an error of less than 10% of the reported consumption, which is better than the strategy that SMHI uses today.</p>
----------------------------------------------------------------------
In diva2:1877617 - Note: no full text in DiVA

abstract is:
<p>This thesis investigates the optimization of GaAs-based single-line defect photoniccrystal waveguides (PCWs) for mid-infrared (MIR) gas sensing applications. Photoniccrystals (PhCs) are materials with a periodic dielectric constant variation, structuredto create photonic bandgaps for precise light propagation control. This work focuseson the design of PCWs in GaAs membranes and development of processes to fabricateair-holes with smooth and straight sidewalls, which are crucial for optical PhC deviceperformance.The project is structured into two phases. In the first phase, AnsysLumerical is used for the simulation of PCWs, which includes designing a 2D hexagonallattice structure and optimizing parameters such as radius and lattice constant toachieve desired bandstructure, light confinement, and single-mode propagation.Simulation results show that the optimal PCW configuration effectively confines light,provides evanescent fields in both lateral and vertical directions, and supports singlemodepropagation at the target wavelength of 4.26 μm, at which CO2 has a majorabsorption peak. The second phase focuses on the fabrication process, utilizingcharged colloidal lithography and Inductively Coupled Plasma Reactive Ion Etching(ICP-RIE) with Ar/Cl2 chemistry to create air holes in GaAs substrates. The aim is toexplore the effects of varying hole diameters and process conditions to achieve goodhole profiles at a depth of 600 nm — the designed thickness of the GaAs membrane foreffective vertical light confinement. Impact of various parameters on the etch rate andhole profile is studied. A feature-size dependent etching phenomenon, the lag effect,is observed, with a significant variation in etch depths for different hole diameters.The findings provide clear guidelines for optimizing conditions to achieve suitable holedepths and profiles for fabricating MIR PhC devices in GaAs membranes.</p>

corrected abstract:
<p>This thesis investigates the optimization of GaAs-based single-line defect photoniccrystal waveguides (PCWs) for mid-infrared (MIR) gas sensing applications. Photonic crystals (PhCs) are materials with a periodic dielectric constant variation, structured to create photonic bandgaps for precise light propagation control. This work focuses on the design of PCWs in GaAs membranes and development of processes to fabricate air-holes with smooth and straight sidewalls, which are crucial for optical PhC device performance. The project is structured into two phases. In the first phase, Ansys Lumerical is used for the simulation of PCWs, which includes designing a 2D hexagonal lattice structure and optimizing parameters such as radius and lattice constant to achieve desired band structure, light confinement, and single-mode propagation. Simulation results show that the optimal PCW configuration effectively confines light, provides evanescent fields in both lateral and vertical directions, and supports single mode propagation at the target wavelength of 4.26 μm, at which CO<sub>2</sub> has a major absorption peak. The second phase focuses on the fabrication process, utilizing charged colloidal lithography and Inductively Coupled Plasma Reactive Ion Etching(ICP-RIE) with Ar/Cl<<sub>2</sub> chemistry to create air holes in GaAs substrates. The aim is to explore the effects of varying hole diameters and process conditions to achieve good hole profiles at a depth of 600 nm — the designed thickness of the GaAs membrane for effective vertical light confinement. Impact of various parameters on the etch rate and hole profile is studied. A feature-size dependent etching phenomenon, the lag effect, is observed, with a significant variation in etch depths for different hole diameters. The findings provide clear guidelines for optimizing conditions to achieve suitable hole depths and profiles for fabricating MIR PhC devices in GaAs membranes.</p>
----------------------------------------------------------------------
In diva2:1876088 abstract is:
<p>A key component of biological research is cell culture technology, which allows researchersto examine the behavior and functionality of cells in controlled environments. Conventionalcell culture monitoring frequently necessitates taking the cultures out of their incubators tomake observations under a microscope. This exposes them to pollutants and changes in thesurrounding environment and may jeopardize the integrity of the experiment.This thesis presents the development of a cost-effective, miniaturized microscope designedfor imaging of cell cultures directly within incubators. By integrating simple, inexpensiveglass lenses and 3D-printed components and focusing on the ESP32-CAM module for digitalimaging, the project explores various optical setups to optimize image quality while minimizingdisruption to cell environments.Central to the research was the identification and testing of diverse optical configurations todetermine the most effective arrangement for both brightfield and fluorescence microscopy.The design features a baseplate for stability, a filter plate for fluorescence imaging, and afocus adjustment mechanism using magnets. Iterative enhancements led to a side illuminationtechnique using an economical LED, removing the need for a beamsplitter and simplifying theoptical path.The final microscope demonstrated successful brightfield imaging and weak fluorescenceimaging of Madin-Darby Canine Kidney (MDCK) II cell cultures marked with Green FluorescentProtein (GFP), using a magnification ratio of 2.5:1 through an infinity-corrected optical system.The findings illustrate the potential of developing an economical, functional microscope thatcan be readily replicated and scaled for use in cell culture technology.</p><p> </p>

corrected abstract:
<p>A key component of biological research is cell culture technology, which allows researchers to examine the behavior and functionality of cells in controlled environments. Conventional cell culture monitoring frequently necessitates taking the cultures out of their incubators to make observations under a microscope. This exposes them to pollutants and changes in the surrounding environment and may jeopardize the integrity of the experiment.</p><p>This thesis presents the development of a cost-effective, miniaturized microscope designed for imaging of cell cultures directly within incubators. By integrating simple, inexpensive glass lenses and 3D-printed components and focusing on the ESP32-CAM module for digital imaging, the project explores various optical setups to optimize image quality while minimizing disruption to cell environments.</p><p>Central to the research was the identification and testing of diverse optical configurations to determine the most effective arrangement for both brightfield and fluorescence microscopy. The design features a baseplate for stability, a filter plate for fluorescence imaging, and a focus adjustment mechanism using magnets. Iterative enhancements led to a side illumination technique using an economical LED, removing the need for a beamsplitter and simplifying the optical path.</p><p>The final microscope demonstrated successful brightfield imaging and weak fluorescence imaging of Madin-Darby Canine Kidney (MDCK) II cell cultures marked with Green Fluorescent Protein (GFP), using a magnification ratio of 2.5:1 through an infinity-corrected optical system. The findings illustrate the potential of developing an economical, functional microscope that can be readily replicated and scaled for use in cell culture technology.</p>
----------------------------------------------------------------------
In diva2:1787386 abstract is:
<p>In this thesis, an innovative coarse grid CFD approach is developed that aims toexploit the capabilities of sub-channel codes and CFD methods while overcoming theirlimitations. In the approach, a very coarse mesh is implemented in the CFD softwareOpenFOAM and a new wall treatment, based on the traditional concept of the wallfunction, is applied to the wall boundary conditions of the domain to take into accountthe low resolution of the grid which does not allow to effectively capture the effect of thesolid walls on the thermo-hydraulics of the flow. To investigate the performance of thenew approach, the method is implemented first in three simple test cases for whichthe sub-channel codes are the state-of-the-art thermo-hydraulic analysis since theyare single-phase flow problems in which there are no prevailing 3D flow conditions.An additional test case representing a 2x2 fuel bundle with three full-length rods andone half-length rod is investigated to verify the behavior of the new approach in caseswhere secondary flows are present. The results for the pressure fields are comparedwith the analytical pressure profiles for the four test cases that well represent the onesthat would be obtained with sub-channel code analysis, while the results for the wallshear stresses obtained in the four test cases are compared with the ones obtained witha more refined mesh in which the traditional wall function approach is implementedsince they should be the best estimation of the actual wall shear stresses at the walldomain. For the first two cases, the developed approach produces reasonable resultswith a good agreement to the analytical pressure profiles while the other two testcases show that the methodology has a limited applicability and, before proceedingwith the extension of the new approach to single-phase problems with 3D prevailingphenomena and two-phase problems, it is necessary to solve the issues that emerge forsome types of cases.</p>

corrected abstract:
<p>In this thesis, an innovative coarse grid CFD approach is developed that aims to exploit the capabilities of sub-channel codes and CFD methods while overcoming their limitations. In the approach, a very coarse mesh is implemented in the CFD software OpenFOAM and a new wall treatment, based on the traditional concept of the wall function, is applied to the wall boundary conditions of the domain to take into account the low resolution of the grid which does not allow to effectively capture the effect of the solid walls on the thermo-hydraulics of the flow. To investigate the performance of the new approach, the method is implemented first in three simple test cases for which the sub-channel codes are the state-of-the-art thermo-hydraulic analysis since they are single-phase flow problems in which there are no prevailing 3D flow conditions. An additional test case representing a 2x2 fuel bundle with three full-length rods and one half-length rod is investigated to verify the behavior of the new approach in cases where secondary flows are present. The results for the pressure fields are compared with the analytical pressure profiles for the four test cases that well represent the ones that would be obtained with sub-channel code analysis, while the results for the wall shear stresses obtained in the four test cases are compared with the ones obtained with a more refined mesh in which the traditional wall function approach is implemented since they should be the best estimation of the actual wall shear stresses at the wall domain. For the first two cases, the developed approach produces reasonable results with a good agreement to the analytical pressure profiles while the other two test cases show that the methodology has a limited applicability and, before proceeding with the extension of the new approach to single-phase problems with 3D prevailing phenomena and two-phase problems, it is necessary to solve the issues that emerge for some types of cases.</p>
----------------------------------------------------------------------
In diva2:1782728 abstract is:
<p>Composite monocoque frames are becoming increasingly more popular inperformance cars. Compared to their steel and aluminum counterparts theyprovide additional torsional stiffness at the cost of less weight. This thesiscovers the complex optimization process of a monocoque applied within theregulations of a Formula Student competition. It aims to give the reader a goodunderstanding of the rules and how they affect the optimization process whilegenerating an optimized design used in the competition of Formula StudentGermany -21 by KTH Formula Student.</p><p>The rules of Formula Student dictate the structural requirements on themonocoque based on a steel space frame. All materials except low carbon steelused in the structure require proof of equivalence through regulated testingmethods. However, this thesis shows that the regulated setup can severelyaffect results through a deep analysis of the testing methodology.The torsional stiffness of the monocoque is analyzed and optimized accordingto the results of a free-size optimization. Both through slight adjustmentsin chassis geometry and the laminate, resulting in a theoretical torsionalstiffness of 9.9 kNm/deg, more than five times as much as the old space frame.Weighing in at 20 kg, a significant weight reduction of about 10 kg, eventhough it was larger, with a surface area of about 4.2 m2.</p><p>This design will be the first monocoque manufactured within KTH FormulaStudent since 2010. Therefore, a lot of focus was put on analyzing the rulesand lay the ground for future development by conducting tests on optimizedpanels. These results have the potential to further reduce the weight of a futuremonocoque with a different geometry.</p>

corrected abstract:
<p>Composite monocoque frames are becoming increasingly more popular in performance cars. Compared to their steel and aluminum counterparts they provide additional torsional stiffness at the cost of less weight. This thesis covers the complex optimization process of a monocoque applied within the regulations of a Formula Student competition. It aims to give the reader a good understanding of the rules and how they affect the optimization process while generating an optimized design used in the competition of Formula Student Germany -21 by KTH Formula Student.</p><p>The rules of Formula Student dictate the structural requirements on the monocoque based on a steel space frame. All materials except low carbon steel used in the structure require proof of equivalence through regulated testing methods. However, this thesis shows that the regulated setup can severely affect results through a deep analysis of the testing methodology. The torsional stiffness of the monocoque is analyzed and optimized according to the results of a free-size optimization. Both through slight adjustments in chassis geometry and the laminate, resulting in a theoretical torsional stiffness of 9.9 kNm/deg, more than five times as much as the old space frame. Weighing in at 20 kg, a significant weight reduction of about 10 kg, even though it was larger, with a surface area of about 4.2 m<sup>2</sup>.</p><p>This design will be the first monocoque manufactured within KTH Formula Student since 2010. Therefore a lot of focus was put on analyzing the rules and lay the ground for future development by conducting tests on optimized panels. These results have the potential to further reduce the weight of a future monocoque with a different geometry.</p>
----------------------------------------------------------------------
In diva2:1780558 abstract is:
<p>To mitigate climate change a proposed space-based geoengineering solutionis to screen off solar irradiance by placing a membrane in between the Earthand the Sun. The feasibility of such a project largely depends on minimizingthe mass of the shading screen and as an extension to the Sunshade projectthis thesis investigated how such a low-mass membrane could be designed.Because of the acting forces at location in space, minimizing the mass impliesthat the material ought to have a low reflection coeﬀicient and surface densityand therefore the highly transparent material of artificial spider silk was chosenas the proposed material. The only possibility to block light is then byrefraction or diffraction and, since the presence of apertures might lower thesurface density, the structure of the suggested membrane is a grid patternof wires. Such a diffraction grating was investigated while applying twomethods. Method 1 optimized the dimensions of the structure to lower thetotal transmission on Earth when placed on the direct transmission axis ofthe membrane and method 2 tilted the membrane in order to place Earth ata diffraction minimum. This resulted in three suggested designs A, B, andC with surface densities varying from that of 0.00867 to 0.228 gm−2. Theresults were compared with two previous design proposals where the lowestareal density was 0.34g/m2, which is 3/2 to 40 times larger than the densitiesproposed in this paper. The reflectivities for A and B were 12.5 and 3.75 timeslarger than that of the smallest previously achieved reflectivity. The reflectivityof C could not be determined exactly but ought to have a reflectivity at leastas low as B at 3%, making it the most promising candidate for a membranedesign of the three.</p>

corrected abstract:
<p>To mitigate climate change a proposed space-based geoengineering solution is to screen off solar irradiance by placing a membrane in between the Earth and the Sun. The feasibility of such a project largely depends on minimizing the mass of the shading screen and as an extension to the Sunshade project this thesis investigated how such a low-mass membrane could be designed. Because of the acting forces at location in space, minimizing the mass implies that the material ought to have a low reflection coefficient and surface density and therefore the highly transparent material of artificial spider silk was chosen as the proposed material. The only possibility to block light is then by refraction or diffraction and, since the presence of apertures might lower the surface density, the structure of the suggested membrane is a grid pattern of wires. Such a diffraction grating was investigated while applying two methods. Method 1 optimized the dimensions of the structure to lower the total transmission on Earth when placed on the direct transmission axis of the membrane and method 2 tilted the membrane in order to place Earth at a diffraction minimum. This resulted in three suggested designs A, B, and C with surface densities varying from that of 0.00867 to 0.228 gm<sup>−2</sup>. The results were compared with two previous design proposals where the lowest areal density was 0.34gm<sup>-2</sup>, which is 3/2 to 40 times larger than the densities proposed in this paper. The reflectivities for A and B were 12.5 and 3.75 times larger than that of the smallest previously achieved reflectivity. The reflectivity of C could not be determined exactly but ought to have a reflectivity at least as low as B at 3%, making it the most promising candidate for a membrane design of the three.</p>
----------------------------------------------------------------------
In diva2:1761916 abstract is:
<p>Since its introduction by Max Otto Lorenz, the Lorenz curve has been utilizedin several financial contexts. By using regression analysis to approximate theclaim cost of policyholders, a vector consisting of policyholder characteristics canbe obtained. The ordered Lorenz curve can subsequently be used to understandwhat commonalities are shared between profitable policyholders. This allows forbetter management of the insurance portfolio and thus better customer relationstowards both the policyholders and the insurer, which is important for an insuranceconsultancy agency. The aim of this thesis was to investigate which attributesapproximate the policyholder claim costs and consequently obtain insight into whatattributes are shared among profitable portfolio clients. The results presented inthis thesis show that a multi-linear regression model, transformed using the Box-Cox method is insuﬀicient to approximate the claim costs in a convincing manner.The model obtained in the thesis was capable of identifying significant regressorsbut the overall result displayed uncertainties in regards to overall goodness of fit.This means that the variability explained by the regression model only represents4.95% of the variability in the claim cost data. Thus, the relativity measureintroduced in section 2.1.1 was deemed uninterruptible in a meaningful way.Consequently, the empirical distribution functions presented in section 1.1 wouldbe based on a faulty order statistic, and in turn the visualization of an orderedLorenz curve with such a relativity measure is unnecessary.</p>

corrected abstract:
<p>Since its introduction by Max Otto Lorenz, the Lorenz curve has been utilized in several financial contexts. By using regression analysis to approximate the claim cost of policyholders, a vector consisting of policyholder characteristics can be obtained. The ordered Lorenz curve can subsequently be used to understand what commonalities are shared between profitable policyholders. This allows for better management of the insurance portfolio and thus better customer relations towards both the policyholders and the insurer, which is important for an insurance consultancy agency. The aim of this thesis was to investigate which attributes approximate the policyholder claim costs and consequently obtain insight into what attributes are shared among profitable portfolio clients. The results presented inthis thesis show that a multi-linear regression model, transformed using the Box-Cox method is insufficient to approximate the claim costs in a convincing manner. The model obtained in the thesis was capable of identifying significant regressors but the overall result displayed uncertainties in regards to overall goodness of fit. This means that the variability explained by the regression model only represents 4.95% of the variability in the claim cost data. Thus, the relativity measure introduced in section 2.1.1 was deemed uninterruptible in a meaningful way. Consequently, the empirical distribution functions presented in section 1.1 would be based on a faulty order statistic, and in turn the visualization of an ordered Lorenz curve with such a relativity measure is unnecessary.</p>
----------------------------------------------------------------------
In diva2:1583521 abstract is:
<p>With the increasing number of satellites operating in orbit and the development of nanosatelliteconstellations, it has become more and more arduous for operators to keep track of every satellitestate, and perform corrective or avoidance manoeuvres. That is why CNES, the French space agency,is developing new algorithms, which aimed at making satellites more self-su cient. More especially,these algorithms are in charge of autonomous orbit control, collision risk calculations and satellitestatus monitoring. In this thesis, we present the architecture of these three algorithms and how theyinteract between them to deal with the autonomous control of a satellite. In addition, this paper studiestheir integration within the OPS-SAT nanosatellite, which is an in-orbit demonstrator developed bythe European Space Agency (ESA) and opened to worldwide experimenters. By analysing the dataused by the numerical propagators, the size of the input configuration files sent to the nanosatellitewas optimised. Thanks to this optimisation, the size of telecommands sent during each OPS-SATflyby above the ESOC ground station meets the requirements.</p><p>Due to some issues encountered with the nanosatellite’s GPS, a solution was found to update thecurrent orbit on-board, and thus allow the proper algorithms’ operation. This thesis also introduceshow the tests were carried out in order to validate these algorithms, both on flat-sat and on the realsatellite. The results demonstrate that their integration on the OPS-SAT numerical environment issuccessful, meaning that the algorithms and their dependences are correctly packaged, sent and uploaded,and that they work as expected. Their execution time are of course longer due to the limitedcalculation capacity of the on-board computer, but are still compatible with real operations, except forthe collision risk computation, which can exceed the orbital period depending on the initial conditions.Finally, the thesis presents the process of real operations for one of the three algorithms developed byCNES, the di culties encountered and the solutions considered.</p>

corrected abstract:
<p>With the increasing number of satellites operating in orbit and the development of nanosatellite constellations, it has become more and more arduous for operators to keep track of every satellite state, and perform corrective or avoidance manoeuvres. That is why CNES, the French space agency, is developing new algorithms, which aimed at making satellites more self-sufficient. More especially, these algorithms are in charge of autonomous orbit control, collision risk calculations and satellite status monitoring. In this thesis, we present the architecture of these three algorithms and how they interact between them to deal with the autonomous control of a satellite. In addition, this paper studies their integration within the OPS-SAT nanosatellite, which is an in-orbit demonstrator developed by the European Space Agency (ESA) and opened to worldwide experimenters. By analysing the data used by the numerical propagators, the size of the input configuration files sent to the nanosatellite was optimised. Thanks to this optimisation, the size of telecommands sent during each OPS-SAT flyby above the ESOC ground station meets the requirements.</p><p>Due to some issues encountered with the nanosatellite’s GPS, a solution was found to update the current orbit on-board, and thus allow the proper algorithms’ operation. This thesis also introduces how the tests were carried out in order to validate these algorithms, both on flat-sat and on the real satellite. The results demonstrate that their integration on the OPS-SAT numerical environment is successful, meaning that the algorithms and their dependences are correctly packaged, sent and uploaded, and that they work as expected. Their execution time are of course longer due to the limited calculation capacity of the on-board computer, but are still compatible with real operations, except for the collision risk computation, which can exceed the orbital period depending on the initial conditions. Finally, the thesis presents the process of real operations for one of the three algorithms developed by CNES, the difficulties encountered and the solutions considered.</p>
----------------------------------------------------------------------
In diva2:1465506 abstract is:
<p>Silencers are used in vehicles to reduce the noise in the engine system. However,silencers themselves may produce break-out noise due to the interactionwith the exhaust gas ow and structure. In this Master thesis project, thenumerical simulation of vibrational behavior of housing plates of silencers isdeveloped.The housing plate is composed of two steel plates and a brous materiallayer. Measurement results show that the brous material has good dampingeect to decrease the vibration and radiated sound of steel plates. Steel platesare connected by spot welding. Modeling of spot welds can improve themodal assurance criterion between simulation and measurements. Interfacedamping is introduced into the simulation models to simulate the contacteect between two steel plates so that the simulated amplitude can have agood agreement with measurement result.Several numerical models of brous material are investigated. The Mikimodel is not chosen for the nal result due to the limit of range of frequencies.The rigid frame model can simulate the sound absorption but is unfeasible forthe simulation of vibration. The limp frame model can simulate the vibrationof light glass wool but cannot simulate the vibration of heavy glass wool.Finally, the Biot-Allard model which is a poro-elastic model is investigatedfor the nal result. The simulation results show good agreement with themeasurement result.</p>


corrected abstract:
<p>Silencers are used in vehicles to reduce the noise in the engine system. However, silencers themselves may produce break-out noise due to the interaction with the exhaust gas flow and structure. In this Master thesis project, the numerical simulation of vibrational behavior of housing plates of silencers is developed.</p><p>The housing plate is composed of two steel plates and a fibrous material layer. Measurement results show that the fibrous material has good damping effect to decrease the vibration and radiated sound of steel plates. Steel plates are connected by spot welding. Modeling of spot welds can improve the modal assurance criterion between simulation and measurements. Interface damping is introduced into the simulation models to simulate the contact effect between two steel plates so that the simulated amplitude can have a good agreement with measurement result.</p><p>Several numerical models of fibrous material are investigated. The Miki model is not chosen for the final result due to the limit of range of frequencies. The rigid frame model can simulate the sound absorption but is unfeasible for the simulation of vibration. The limp frame model can simulate the vibration of light glass wool but cannot simulate the vibration of heavy glass wool. Finally, the Biot-Allard model which is a poro-elastic model is investigated for the final result. The simulation results show good agreement with the measurement result.</p>
----------------------------------------------------------------------
In diva2:1244326 missing space in title:
"Transonic Flutter for aGeneric Fighter Configuration"
==>
"Transonic Flutter for a Generic Fighter Configuration"

abstract is:
<p>A hazardous and not fully understood aeroelastic phenomenon is the transonic dip,the decrease in flutter dynamic pressure that occurs for most aircraft configurationsin transonic flows. The difficulty of predicting this phenomenon forces aircraft manufacturersto run long and costly flight test campaigns to demonstrate flutter-free behaviourof their aircraft at transonic Mach numbers.In this project, subsonic and transonic flutter calculations for the KTH-NASA genericfighter research model have been performed and compared to existing experimentalflutter data from wind tunnel tests performed at NASA Langley in 2016. For the fluttercalculations, industry-standard linear panel methods have been used together with afinite element model from NASTRAN.Further, an alternative approach for more accurate transonic flutter predictions usingthe full-potential solver Phi has been investigated. To predict flutter using this newmethodology a simplified structural model has been used together with aerodynamicmeshes of the main wing. The purpose of the approach was to see if it was possibleto find a method that was more accurate than panel methods in the transonic regimewhilst still being suitable for use during iterative design processes.The results of this project demonstrated that industry-standard linear panel methodssignificantly over-predict the flutter boundary in the transonic regime. It was alsoseen that the flutter predictions using Phi showed potential, being close to the linearresults for the same configuration as tested in Phi. For improved transonic accuracy inPhi, an improved transonic flow finite element formulation could possibly help .Another challenge with Phi is the requirement of an explicit wake from all liftingsurfaces in the aerodynamic mesh. Therefore, a method for meshing external storeswith blunt trailing edges needs to be developed. One concept suggested in this projectis to model external stores in "2.5D", representing external stores using airfoils withsharp trailing edges.</p>

corrected abstract:
<p>A hazardous and not fully understood aeroelastic phenomenon is the transonic dip, the decrease in flutter dynamic pressure that occurs for most aircraft configurations in transonic flows. The difficulty of predicting this phenomenon forces aircraft manufacturers to run long and costly flight test campaigns to demonstrate flutter-free behaviour of their aircraft at transonic Mach numbers.</p><p>In this project, subsonic and transonic flutter calculations for the KTH-NASA generic fighter research model have been performed and compared to existing experimental flutter data from wind tunnel tests performed at NASA Langley in 2016. For the flutter calculations, industry-standard linear panel methods have been used together with a finite element model from NASTRAN.</p><p>Further, an alternative approach for more accurate transonic flutter predictions using the full-potential solver Phi has been investigated. To predict flutter using this new methodology a simplified structural model has been used together with aerodynamic meshes of the main wing. The purpose of the approach was to see if it was possible to find a method that was more accurate than panel methods in the transonic regime whilst still being suitable for use during iterative design processes.</p><p>The results of this project demonstrated that industry-standard linear panel methods significantly over-predict the flutter boundary in the transonic regime. It was also seen that the flutter predictions using Phi showed potential, being close to the linear results for the same configuration as tested in Phi. For improved transonic accuracy in Phi, an improved transonic flow finite element formulation could possibly help.</p><p>Another challenge with Phi is the requirement of an explicit wake from all lifting surfaces in the aerodynamic mesh. Therefore, a method for meshing external stores with blunt trailing edges needs to be developed. One concept suggested in this project is to model external stores in "2.5D", representing external stores using airfoils with sharp trailing edges.</p>
----------------------------------------------------------------------
In diva2:1083220 abstract is:
<p>The current pollution policies in all European and American countries are forcing the industry to movetowards a more efficient and environmentally friendly engines. On the other hand, customers requiremaintaining the power and fuel consumption. Lowering mainly nitrous oxides (NOx) and carbon particles(Soot) is therefore a challenging task with a very strong impact on mainly the automotive andaeronautical market.The purpose of the current work is to research the pollution production of automotive diesel enginesand optimize the fuel injection and piston geometry to lower the emissions. The interaction betweenfuel and air as well as the combustion are the two main physical and chemical processes governing thepollutants formation. Converged-CFD will be the CFD tool employed during the analysis of the previousproblems.The fuel-air interaction is related to jet break up, vaporization and turbulence. The strong dependenceon the surrounding flow field of the previous processes require the equations to be solved numericallywithin a CFD code. The fuel is to be placed in a combustion chamber (piston) where the spray will affectthe surrounding flow field and ultimately the combustion process.In order to accurately represent the nature of the processes, the current work is divided into two mainchapters. Spray modelling and Combustion Modelling. The first will help to accurately model the discretephase (fuel spray) and the vapour entrainment. The second chapter, combustion modelling willretrieve the knowledge gain in the first part to accurately represent the fuel injection in the chamber aswell as the combustion process to ultimately model the pollutants emissions.Finally, a piston bowl optimization will be performed using the previous analysed models and give theindustry a measure of the potential improvement by just adjusting the fuel injection or by modifyingthe piston bowl geometry.</p>


corrected abstract:
<p>The current pollution policies in all European and American countries are forcing the industry to move towards a more efficient and environmentally friendly engines. On the other hand, customers require maintaining the power and fuel consumption. Lowering mainly nitrous oxides (NOx) and carbon particles (Soot) is therefore a challenging task with a very strong impact on mainly the automotive and aeronautical market.</p><p>The purpose of the current work is to research the pollution production of automotive diesel engines and optimize the fuel injection and piston geometry to lower the emissions. The interaction between fuel and air as well as the combustion are the two main physical and chemical processes governing the pollutants formation. Converged-CFD will be the CFD tool employed during the analysis of the previous problems.</p><p>The fuel-air interaction is related to jet break up, vaporization and turbulence. The strong dependence on the surrounding flow field of the previous processes require the equations to be solved numerically within a CFD code. The fuel is to be placed in a combustion chamber (piston) where the spray will affect the surrounding flow field and ultimately the combustion process.</p><p>In order to accurately represent the nature of the processes, the current work is divided into two main chapters. Spray modelling and Combustion Modelling. The first will help to accurately model the discrete phase (fuel spray) and the vapour entrainment. The second chapter, combustion modelling will retrieve the knowledge gain in the first part to accurately represent the fuel injection in the chamber as well as the combustion process to ultimately model the pollutants emissions.</p><p>Finally, a piston bowl optimization will be performed using the previous analysed models and give the industry a measure of the potential improvement by just adjusting the fuel injection or by modifying the piston bowl geometry.</p>
----------------------------------------------------------------------
In diva2:1078083 missing space in title:
"Wind tunnel blockage corrections forwind turbine measurements"
==>
"Wind tunnel blockage corrections for wind turbine measurements"

abstract is:
<p>Wind-tunnel measurements are an important step during the windturbinedesign process. The goal of wind-tunnel tests is to estimate theoperational performance of the wind turbine, for example by measuringthe power and thrust coecients. Depending on the sizes of both thewind turbine and the test section, the eect of blockage can be substantial.Correction schemes for the power and thrust coecients havebeen proposed in the literature, but for high blockage and highly loadedrotors these correction schemes become less accurate.A new method is proposed here to calculate the eect a cylindricalwind-tunnel test section has on the performance of the wind turbine.The wind turbine is modeled with a simplied vortex model. Usingvortices of constant circulation to model the wake vortices, the performancecharacteristics are estimated. The test section is modeled witha panel method, adapted for this specic situation. It uses irrotationalaxisymmetric source panels to enforce the solid-wall boundary condition.Combining both models in an iterative scheme allows for thesimulation of the eect of the presence of the test-section walls on windturbines performace.Based on the proposed wind-tunnel model, a more general empirical correlationscheme is proposed to estimate the performance characteristicsof a wind turbine operating under unconned conditions by correctingthe performance measured in the conned wind-tunnel conguration.The proposed correction scheme performs better than the existing correctionschemes, including cases with high blockage and highly loadedrotors.</p>


corrected abstract:
<p>Wind-tunnel measurements are an important step during the windturbine design process. The goal of wind-tunnel tests is to estimate the operational performance of the wind turbine, for example by measuring the power and thrust coefficients. Depending on the sizes of both the wind turbine and the test section, the effect of blockage can be substantial. Correction schemes for the power and thrust coefficients have been proposed in the literature, but for high blockage and highly loaded rotors these correction schemes become less accurate.</p><p>A new method is proposed here to calculate the effect a cylindrical wind-tunnel test section has on the performance of the wind turbine. The wind turbine is modeled with a simplified vortex model. Using vortices of constant circulation to model the wake vortices, the performance characteristics are estimated. The test section is modeled with a panel method, adapted for this specific situation. It uses irrotational axisymmetric source panels to enforce the solid-wall boundary condition. Combining both models in an iterative scheme allows for the simulation of the effect of the presence of the test-section walls on wind turbines performace.</p><p>Based on the proposed wind-tunnel model, a more general empirical correlation scheme is proposed to estimate the performance characteristics of a wind turbine operating under unconfined conditions by correcting the performance measured in the confined wind-tunnel configuration. The proposed correction scheme performs better than the existing correction schemes, including cases with high blockage and highly loaded rotors.</p>
----------------------------------------------------------------------
In diva2:1817116 abstract is:
<p>Composite structures are commonly joined using adhesive or mechanical joints, withmechanical joints being preferred when components need to be removable for maintenancepurposes. However, the presence of mechanical joints introduces a discontinuity in theload path, which can serve as an initiation point for failure and needs to be taken intoaccount in the design of the joint. Additionally, delaminations may occur around thefastener hole during the manufacturing and assembly processes, further impacting thestrength of the laminate under compressive loading. While some studies have assessedthe residual strength of open-hole specimens, limited information exists regarding theresidual bearing strength in delaminated composite joints. This study aims to assessthe significance of delaminations of varying sizes on the bearing strength of single-bolt,single-lap shear joints under static loading using numerical analysis methods. The effectsof countersinking and bolt size are also examined. Stress and progressive failure analysisare utilized to evaluate different parameters and account for the nonlinear behavior of thematerials. The study reveals that the presence of delamination leads to degradation ofthe bearing strength of approximately five percent when bolt pretension is applied and15 percent in the absence of pretension. Countersinking increases maximum and averagestresses on the cylindrical section of the hole, while a larger bolt size enhances bearingstrength by reducing bolt bending in single-lap shear joints.</p>

corrected abstract:
<p>Composite structures are commonly joined using adhesive or mechanical joints, with mechanical joints being preferred when components need to be removable for maintenance purposes. However, the presence of mechanical joints introduces a discontinuity in the load path, which can serve as an initiation point for failure and needs to be taken into account in the design of the joint. Additionally, delaminations may occur around the fastener hole during the manufacturing and assembly processes, further impacting the strength of the laminate under compressive loading. While some studies have assessed the residual strength of open-hole specimens, limited information exists regarding the residual bearing strength in delaminated composite joints. This study aims to assess the significance of delaminations of varying sizes on the bearing strength of single-bolt, single-lap shear joints under static loading using numerical analysis methods. The effects of countersinking and bolt size are also examined. Stress and progressive failure analysis are utilized to evaluate different parameters and account for the nonlinear behavior of the materials. The study reveals that the presence of delamination leads to degradation of the bearing strength of approximately five percent when bolt pretension is applied and 15 percent in the absence of pretension. Countersinking increases maximum and average stresses on the cylindrical section of the hole, while a larger bolt size enhances bearing strength by reducing bolt bending in single-lap shear joints.</p>
----------------------------------------------------------------------
In diva2:1817018 abstract is:
<p>Space industry has been booming in the recent times, investments have not justbeen made on satellites and launch vehicles but also on space sustainability. Spaceindustry has its users in national agencies, private commercial agencies, academia andexperimental organisations. Smallsats and cubesats are one of the most interestingdomains in space industry today. This has led to pure research and astonishinginnovations in this domain to enable lower costs, lower mass, increased orbital period,better accessibility and global impact. Development of sustainable products requiresthe system to qualify a certain standard set up in the industry. This standard ensuresthat the system safely completes its mission up in space. The problem described byGomSpace Sweden concerns one of their ongoing products, a cold gas propulsionunit which is suitable for a typical 6U cubesat called ESA6DOF. In large, the productconsists of a structure, two propellant tanks, one plenum tank, piping, electronics andsix thrusters. Qualification of this propulsion system module involves the system toundergo a random vibration test according to ECSS standards. This thesis work shallbe focused on setting up the structure needed to perform random vibration simulationsin COMSOL. This step is done to primarily iterate the design to make it robust enoughto sustain loads during flight and also to avoid any physical damages during testingcampaign. Followed by performing the actual random vibration test at a facility usingthe assembled ESA6DOF propulsion module. Finally, this ends with validating thesimulation results with that of testing.</p>

corrected abstract:
<p>Space industry has been booming in the recent times, investments have not just been made on satellites and launch vehicles but also on space sustainability. Space industry has its users in national agencies, private commercial agencies, academia and experimental organisations. Smallsats and cubesats are one of the most interesting domains in space industry today. This has led to pure research and astonishing innovations in this domain to enable lower costs, lower mass, increased orbital period, better accessibility and global impact. Development of sustainable products requires the system to qualify a certain standard set up in the industry. This standard ensures that the system safely completes its mission up in space. The problem described by GomSpace Sweden concerns one of their ongoing products, a cold gas propulsion unit which is suitable for a typical 6U cubesat called ESA6DOF. In large, the product consists of a structure, two propellant tanks, one plenum tank, piping, electronics and six thrusters. Qualification of this propulsion system module involves the system to undergo a random vibration test according to ECSS standards. This thesis work shall be focused on setting up the structure needed to perform random vibration simulations in COMSOL. This step is done to primarily iterate the design to make it robust enough to sustain loads during flight and also to avoid any physical damages during testing campaign. Followed by performing the actual random vibration test at a facility using the assembled ESA6DOF propulsion module. Finally, this ends with validating the simulation results with that of testing.</p>
----------------------------------------------------------------------
In diva2:1745587 abstract is:
<p>This project’s idea revolved around utilizing the most recent techniques in MachineLearning, Neural Networks, and Data processing to construct a model to be used asa tool to determine stability during core design work. This goal will be achieved bycollecting distribution profiles describing the core state from different steady statesin five burn-up cycles in a reactor to serve as the dataset for training the model. Anadditional cycle will be reserved as a blind testing dataset for the trained model topredict. The variables that will be the target for the predictions are the decay ratioand the frequency since they describe the core stability.The distribution profiles extracted from the core simulator POLCA7 were subjectedto many different Data processing techniques to isolate the most relevant variablesto stability. The processed input variables were merged with the decay ratio andfrequency for those cases, as calculated with POLCA-T. Two different MachineLearning models, one for each output parameter, were designed with Pytorch toanalyze those labeled datasets. The goal of the project was to predict the outputvariables with an error lower than 0.1 for decay ratio and 0.05 for frequency. Themodels were able to predict the testing data with an RMSE of 0.0767 for decay ratioand 0.0354 for frequency.Finally, the trained models were saved and tasked with predicting the outputparameters for a completely unknown cycle. The RMSE was even better forthe unknown cycle, with 0.0615 for decay ratio and 0.0257 for frequency,respectively.</p>

corrected abstract:
<p>This project’s idea revolved around utilizing the most recent techniques in Machine Learning, Neural Networks, and Data processing to construct a model to be used as a tool to determine stability during core design work. This goal will be achieved by collecting distribution profiles describing the core state from different steady states in five burn-up cycles in a reactor to serve as the dataset for training the model. An additional cycle will be reserved as a blind testing dataset for the trained model to predict. The variables that will be the target for the predictions are the decay ratio and the frequency since they describe the core stability.</p><p>The distribution profiles extracted from the core simulator POLCA7 were subjected to many different Data processing techniques to isolate the most relevant variables to stability. The processed input variables were merged with the decay ratio and frequency for those cases, as calculated with POLCA-T. Two different Machine Learning models, one for each output parameter, were designed with Pytorch to analyze those labeled datasets. The goal of the project was to predict the output variables with an error lower than 0.1 for decay ratio and 0.05 for frequency. The models were able to predict the testing data with an RMSE of 0.0767 for decay ratio and 0.0354 for frequency.</p><p>Finally, the trained models were saved and tasked with predicting the output parameters for a completely unknown cycle. The RMSE was even better for the unknown cycle, with 0.0615 for decay ratio and 0.0257 for frequency, respectively.</p>
----------------------------------------------------------------------
In diva2:1679005 abstract is:
<p>As an alternative solution to global warming, this thesis explores the possibility of aspace-based geoengineering scheme that may prove worthwhile to implement in parallel toother environmental efforts that help mitigate impact of climate change. One suggestionof a geoengineering solution is deploying a large number of sunshades in the vicinity ofthe first Lagrange point of the Sun-Earth system, and this prospective sunshade projectwould serve to shield Earth from incident solar radiation. This thesis is an extension ofa feasibility study for the implementation of this large-scale mission, and has a focus oncomparing electric thrusters to solar sailing as a means of propulsion. Background onelectric propulsion systems and spaceflight mechanics is provided. The investigation wasperformed by defining the spacecraft configurations, and then computing trajectories toa point of escape from Earth and from there to the final equilibrium point.Our results show that in order to meet the propellant demands of the electric thrusters,the launch mass would need to increase by around 15-25 % compared to the solar sailingimplementation, equating to around 1010 kg. Nevertheless, electric propulsion could stillbe a beneficial choice since it would allow shorter transfer times for each shade whichreduces the radiation exposure and subsequent degradation of the spacecraft’s systems.It was found that the transfer time with electric propulsion would be about one-half orone-fifth that of solar sailing, depending on spacecraft parameters. Additionally, electricpropulsion allows a much lower initial parking orbit, and while this would increase the ra-diation exposure it would also reduce the launch costs due to the higher payload capacityto lower altitudes. However, electric propulsion of this scale require prior advancementsin xenon or other inert propellant extraction methods and possibly a wide-scale construc-tion of air separation plants.</p>


corrected abstract:
<p>As an alternative solution to global warming, this thesis explores the possibility of a space-based geoengineering scheme that may prove worthwhile to implement in parallel to other environmental efforts that help mitigate impact of climate change. One suggestion of a geoengineering solution is deploying a large number of sunshades in the vicinity of the first Lagrange point of the Sun-Earth system, and this prospective sunshade project would serve to shield Earth from incident solar radiation. This thesis is an extension of a feasibility study for the implementation of this large-scale mission, and has a focus on comparing electric thrusters to solar sailing as a means of propulsion. Background on electric propulsion systems and spaceflight mechanics is provided. The investigation was performed by defining the spacecraft configurations, and then computing trajectories to a point of escape from Earth and from there to the final equilibrium point.</p><p>Our results show that in order to meet the propellant demands of the electric thrusters, the launch mass would need to increase by around 15-25 % compared to the solar sailing implementation, equating to around 10<sup>10</sup> kg. Nevertheless, electric propulsion could still be a beneficial choice since it would allow shorter transfer times for each shade which reduces the radiation exposure and subsequent degradation of the spacecraft’s systems. It was found that the transfer time with electric propulsion would be about one-half or one-fifth that of solar sailing, depending on spacecraft parameters. Additionally, electric propulsion allows a much lower initial parking orbit, and while this would increase the radiation exposure it would also reduce the launch costs due to the higher payload capacity to lower altitudes. However, electric propulsion of this scale require prior advancements in xenon or other inert propellant extraction methods and possibly a wide-scale construction of air separation plants.</p>
----------------------------------------------------------------------
In diva2:1583422 abstract is:
<p>Highly automated driving is approaching reality at a high speed. BMW is planningto put its first autonomous driving vehicle on the road already by 2021. The path torealising this new technology is however, full of challenges. Not only the transverseand longitudinal dynamic vehicle motion play an important role in experiencedcomfort but also the requirements and expectations of the occupants regarding thevertical dynamic vibration behaviour. Especially during long trips on the motorwaywhere the so far active driver becomes the chauffeured passenger, who reads, worksor sleeps in his newly gained time. These new use-cases create new requirements forthe future design of driving comfort which are yet to be fully discovered.This work was carried out at the BMW headquarters and had the aim to usedifferent machine learning models to investigate and identify patterns between thesubjective comfort values reported by participants in a study, on a comfort scale of 1-7 and the mechanical vibrations that they experienced, measured inm/s2. The datawas collected in a previous independent study and statistical methods were used toinsure the quality of the data. A comparison of the ISO 2631-1 comfort ratings andthe study’s findings is done to understand the need for a more sophisticated model to predict comfort in autonomous driving. The work continued by investigating different dimensionality reduction methods and their influence on the performance of the models. The process used to build, optimise and validate neural networks and other models is included in the method chapter and the results are presented. The work ends with a discussion of both the prediction results and the modelsre-usability. The machine learning models investigated in this thesis have shown great po-tential for detecting complex pattern that link feelings and thoughts to mechanical variables. The models were able to predict the correct level of comfort with up to50% precision when trying to predict 6 or 7 levels of comfort. When divided into high versus low discomfort, i.e. predicting one of two comfort levels, the models were able to achieve a precision of up to 75.4%.Excluded from this thesis is the study of differences in attentive vs inattentive state when being driven in an autonomous driving vehicle. It became clear shortly before the start of this work, that the experiment that yielded the data used for it failed to find a statistically significant difference between the two states.</p><p> </p>

corrected abstract:
<p>Highly automated driving is approaching reality at a high speed. BMW is planning to put its first autonomous driving vehicle on the road already by 2021. The path to realising this new technology is however, full of challenges. Not only the transverse and longitudinal dynamic vehicle motion play an important role in experienced comfort but also the requirements and expectations of the occupants regarding the vertical dynamic vibration behaviour. Especially during long trips on the motorway where the so far active driver becomes the chauffeured passenger, who reads, works or sleeps in his newly gained time. These new use-cases create new requirements for the future design of driving comfort which are yet to be fully discovered.</p><p>This work was carried out at the BMW headquarters and had the aim to use different machine learning models to investigate and identify patterns between the subjective comfort values reported by participants in a study, on a comfort scale of 1-7 and the mechanical vibrations that they experienced, measured in <em>m/s<sup>2</sup></em>. The data was collected in a previous independent study and statistical methods were used to insure the quality of the data. A comparison of the ISO 2631-1 comfort ratings and the study’s findings is done to understand the need for a more sophisticated model to predict comfort in autonomous driving. The work continued by investigating different dimensionality reduction methods and their influence on the performance of the models. The process used to build, optimise and validate neural networks and other models is included in the method chapter and the results are presented. The work ends with a discussion of both the prediction results and the models re-usability.</p><p>The machine learning models investigated in this thesis have shown great potential for detecting complex pattern that link feelings and thoughts to mechanical variables. The models were able to predict the correct level of comfort with up to 50% precision when trying to predict 6 or 7 levels of comfort. When divided into high versus low discomfort, i.e. predicting one of two comfort levels, the models were able to achieve a precision of up to 75.4%.</p><p>Excluded from this thesis is the study of differences in attentive vs inattentive state when being driven in a autonomous driving vehicle. It became clear shortly before the start of this work, that the experiment that yielded the data used for it failed to find a statistically significant difference between the two states.</p>
----------------------------------------------------------------------
In diva2:1579558 - Note: no full text in DiVA
abstract is:
<p>Ion channels are essential for numerous functions in the human body e.g.they control electrical signaling, absorb salts and regulate the vital osmoticgradient. They are embedded in cell membranes and the interaction thatoccurs between molecules of the membranes and the channel proteins playsa role in the regulation of the protein structure and function. KcsA is aprototypical K+-channel whose inactivation is suppressed when the bindingto anionic lipids is inhibited. However, the biophysical phenomenon that liesbehind this effect on channel inactivation is not yet discovered. Here, wehave trained several random forest classifiers trained with MD simulation datato distinguish between datasets with lipids bound to the protein and datasetwithout lipids bound. When KcsA is in the Open state, there is a significantlipid impact on the intracellular end of the TM2 helix. We show that thereis a potential favouring of the Fully Open state when the KcsA channel isbound to the anionic DOPG lipid. This was seen in all random forest modelset ups. Our results suggest that a potential acceleration of the opening of theKcsA activation gate, which allosterically controls the selectivity filter, mightexplain why the binding to anionic lipids is necessary for efficient inactivation.We believe that using machine learning together with further biophysical,biochemical analysis of MD simulations is a promising method for findingnew interaction patterns between proteins and other molecules or stimuli.</p>


corrected abstract:
<p>Ion channels are essential for numerous functions in the human body e.g. they control electrical signaling, absorb salts and regulate the vital osmotic gradient. They are embedded in cell membranes and the interaction that occurs between molecules of the membranes and the channel proteins plays a role in the regulation of the protein structure and function. KcsA is a prototypical K+-channel whose inactivation is suppressed when the binding to anionic lipids is inhibited. However, the biophysical phenomenon that lies behind this effect on channel inactivation is not yet discovered. Here, we have trained several random forest classifiers trained with MD simulation data to distinguish between datasets with lipids bound to the protein and dataset without lipids bound. When KcsA is in the Open state, there is a significant lipid impact on the intracellular end of the TM2 helix. We show that there is a potential favouring of the Fully Open state when the KcsA channel is bound to the anionic DOPG lipid. This was seen in all random forest model set ups. Our results suggest that a potential acceleration of the opening of the KcsA activation gate, which allosterically controls the selectivity filter, might explain why the binding to anionic lipids is necessary for efficient inactivation. We believe that using machine learning together with further biophysical, biochemical analysis of MD simulations is a promising method for finding new interaction patterns between proteins and other molecules or stimuli.</p>
----------------------------------------------------------------------
In diva2:1334283 abstract is:
<p>The development of chisels is currently mainly based on experiments and empirical researcheswithin the company. Recently it was decided to develop a simulation tool aiming at predictingthe performances and the capabilities of its chisels. The simulation of concrete during thechiselling process is based on a Cohesive Particle Model for concrete developed in partnershipwith external universities and implemented in the open-source software YADE using the DiscreteElement Method.The goal of this thesis is first to continue the development of the simulation tool by improvingthe calibration method of the material parameters in order to better describe the concretebehaviour under static loading. Afterward a validation phase, aiming at evaluating the real capabilitiesof the simulation tool to predict the demolition process, is performed by comparingsimulations and experiments results. The last objective is to define a simulation method in orderto evaluate the clamping phenomenon during chiselling in an acceptable amount of time.The new calibration algorithm has produced significant improvements in the determinationof the material parameters. Moreover, it was discovered that the ratio between the ultimatetensile and compressive strengths as well as the concrete brittleness are key parameters forthe material calibration accuracy. On the other hand, the validation phase was performed byevaluating the influence of seven parameters, such as the impact energy, the chisel length, onthe demolition process for both experiments and simulations. The procedure and the key resultsare presented in the thesis report. Concerning the clamping phenomenon, it was discovered thatthere is a relation between the pull-out force and the contact force during chiselling. This resultoffers a new and quick possibility for the evaluation of chisels sticking behaviour.</p>

corrected abstract:
<p>The development of chisels is currently mainly based on experiments and empirical researches within the company. Recently it was decided to develop a simulation tool aiming at predicting the performances and the capabilities of its chisels. The simulation of concrete during the chiselling process is based on a Cohesive Particle Model for concrete developed in partnership with external universities and implemented in the open-source software YADE using the Discrete Element Method. The goal of this thesis is first to continue the development of the simulation tool by improving the calibration method of the material parameters in order to better describe the concrete behaviour under static loading. Afterward a validation phase, aiming at evaluating the real capabilities of the simulation tool to predict the demolition process, is performed by comparing simulations and experiments results. The last objective is to define a simulation method in orderto evaluate the clamping phenomenon during chiselling in an acceptable amount of time. The new calibration algorithm has produced significant improvements in the determination of the material parameters. Moreover, it was discovered that the ratio between the ultimate tensile and compressive strengths as well as the concrete brittleness are key parameters for the material calibration accuracy. On the other hand, the validation phase was performed by evaluating the influence of seven parameters, such as the impact energy, the chisel length, onthe demolition process for both experiments and simulations. The procedure and the key results are presented in the thesis report. Concerning the clamping phenomenon, it was discovered that there is a relation between the pull-out force and the contact force during chiselling. This result offers a new and quick possibility for the evaluation of chisels sticking behaviour.</p>
----------------------------------------------------------------------
In diva2:1321182 abstract is:
<p>A method to evaluate technical solutions to handle external loads on subsea wellheadshas been developed. The solutions, or concepts, are compared with respect to load relief,cost and operation. As a basis, a Pugh matrix was used. It is well proven and commonlyused amongst engineers to evaluate concepts. However, it has some major cons due toits simplicity.Two more layers were added trying to solve or minimize these cons. This made up a totalof three layers.I. Evaluation - gather concept data, answer questions with valuesII. Transformation - transforms gathered values to a [1-5] scalingIII. Comparison - scaled values are presented in a Pugh matrixIn layer I, questions are to be answered by analyses and expert knowledge, carried outby developers.As for layer II, a value-scaling relationship should be set by developers and decisionmakers. They decide what is a good difference compared with a reference, and what isnot. The values for layer I can then be translated to layer III.Lastly, in layer III the performances of concepts with respect to different criteria are statedin a Pugh matrix. A scaling [1-5] is used for this. The decision maker decides what criteriaare most important by weighting them.Besides that, everything could be made automated. So when the method was carried outon two concepts, a winner could be decided immediately in layer III when the questionsin layer I had been answered.A simple and straightforward method to compare concepts have been done. Visualizingthe concept evaluation process and the connection between developers and decisionmakers. Making it easier for them to understand one another.The method can continuously be improved over time and might have the potential to makethe development process in many companies leaner.</p>

corrected abstract:
<p>A method to evaluate technical solutions to handle external loads on subsea wellheads has been developed. The solutions, or concepts, are compared with respect to load relief, cost and operation. As a basis, a Pugh matrix was used. It is well proven and commonly used amongst engineers to evaluate concepts. However, it has some major cons due to its simplicity.</p><p>Two more layers were added trying to solve or minimize these cons. This made up a total of three layers.</p><ol type="I"><li>Evaluation - gather concept data, answer questions with values</li><li>Transformation - transforms gathered values to a [1-5] scaling</li><li>Comparison - scaled values are presented in a Pugh matrix</li></ol><p>In layer I, questions are to be answered by analyses and expert knowledge, carried out by developers.</p><p>As for layer II, a value-scaling relationship should be set by developers and decision makers. They decide what is a good difference compared with a reference, and what is not. The values for layer I can then be translated to layer III. Lastly, in layer III the performances of concepts with respect to different criteria are stated in a Pugh matrix. A scaling [1-5] is used for this. The decision maker decides what criteria are most important by weighting them.</p><p>Besides that, everything could be made automated. So when the method was carried out on two concepts, a winner could be decided immediately in layer III when the questions in layer I had been answered.</p><p>A simple and straightforward method to compare concepts have been done. Visualizing the concept evaluation process and the connection between developers and decision makers. Making it easier for them to understand one another.</p><p>The method can continuously be improved over time and might have the potential to make the development process in many companies leaner.</p>
----------------------------------------------------------------------
In diva2:1142912 abstract is:
<p>The purpose of this report is to examine the combinationof an Extreme Learning Machine (ELM) with the KernelMethod. Kernels lies at the core of Support Vector Machines successin classifying non-linearly separable datasets. The hypothesisis that by combining ELM with a kernel we will utilize featuresin the ELM-space otherwise unused. The report is intended asa proof of concept for the idea of using kernel methods in anELM setting. This will be done by running the new algorithmagainst five image datasets for a classification accuracy and timecomplexity analysis.Results show that our extended ELM algorithm, which we havenamed Extreme Kernel Machine (EKM), improve classificationaccuracy for some datasets compared to the regularised ELM,in the best scenarios around three percentage points. We foundthat the choice of kernel type and parameter values had greateffect on the classification performance. The implementation ofthe kernel does however add computational complexity, but wherethat is not a concern EKM does have an advantage. This tradeoffmight give EKM a place between other neural networks andregular ELMs.</p>

corrected abstract:
<p>The purpose of this report is to examine the combination of an <em>Extreme Learning Machine</em> (ELM) with the <em>Kernel Method</em>. Kernels lies at the core of <em>Support Vector Machines</em> success in classifying non-linearly separable datasets. The hypothesis is that by combining ELM with a kernel we will utilize features in the ELM-space otherwise unused. The report is intended as a proof of concept for the idea of using kernel methods in an ELM setting. This will be done by running the new algorithm against five image datasets for a classification accuracy and time complexity analysis.</p><p>Results show that our extended ELM algorithm, which we have named <em>Extreme Kernel Machine</e> (EKM), improve classification accuracy for some datasets compared to the regularised ELM, in the best scenarios around three percentage points. We found that the choice of kernel type and parameter values had great effect on the classification performance. The implementation of the kernel does however add computational complexity, but where that is not a concern EKM does have an advantage. This tradeoff might give EKM a place between other neural networks and regular ELMs.</p>
----------------------------------------------------------------------
In diva2:1139499 
Note: There was soe text missing in the DiVA abstract.

abstract is:
<p>The entropy noise in modern engines is mainly originating from two types of mechanisms.First, chemical reactions in the combustion chamber lead to unsteady heat releasewhich is responsible of the direct combustion noise. Second, hot and cold blobsof air coming from the combustion chamber are advected and accelerated throughturbine stages, giving rise to the so-called entropy noise (or indirect combustionnoise). In the present work, numerical characterization of indirect combustion noiseof a Nozzle Guide Vane passage was assessed using three-dimensional Large EddySimulations. The study was conducted on a simplified topology of a real turbinestator passage, for which experimental data were available in transonic operatingconditions. First, a baseline case was reproduced to validate a numerical finite volumesolver against the experimental measurements. Then, the same solver is used toreproduce the effects of incoming entropy waves from the combustion chamber andto characterize the additional generated acoustic power. Periodic temperature fluctuationsare imposed at the inlet, permitting to simulate hot and cold packets of aircoming from the unsteady combustion. The incoming waves are characterized bytheir characteristic wavelength; therefore, a parametric study has been conductedvarying the inlet temperature of the passage, generating entropy waves of greaterwavelengths. The study proves that the generated indirect combustion noise canbe significant. Moreover, the generated indirect combustion noise increases as thewavelength of the incoming disturbances increases. Finally, the present work suggeststhat, in transonic conditions, there might be flow features which enhance theindirect combustion noise generation mechanism.</p>

corrected abstract:
<p>The entropy noise in modern engines is mainly originating from two types of mechanisms. First, chemical reactions in the combustion chamber lead to unsteady heat release which is responsible of the direct combustion noise. Second, hot and cold blobs of air coming from the combustion chamber are advected and accelerated through turbine stages, giving rise to the so-called entropy noise (or indirect combustion noise). In the present work, numerical characterization of indirect combustion noise of a Nozzle Guide Vane passage was assessed using three-dimensional Large Eddy Simulations. The study was conducted on a simplified topology of a real turbine stator passage, for which experimental data were available in transonic operating conditions. First, a baseline case was reproduced to validate a numerical finite volume solver against the experimental measurements. Then, the same solver is used to reproduce the effects of incoming entropy waves from the combustion chamber and to characterize the additional generated acoustic power. Periodic temperature fluctuations are imposed at the inlet, permitting to simulate hot and cold packets of air coming from the unsteady combustion. The incoming waves are characterized by their characteristic wavelength; therefore, a parametric study has been conducted varying the inlet temperature of the passage, generating entropy waves of greater wavelengths. The study proves that the generated indirect combustion noise can be significant for transonic operating conditions. Moreover, the generated indirect combustion noise increases as the wavelength of the incoming disturbances increases. Finally, the present work suggests that, in transonic conditions, there might be flow features which enhance the indirect combustion noise generation mechanism.</p>
----------------------------------------------------------------------
In diva2:623694 abstract is:
<p>Companiesissuing insurance cover, in return for insurance premiums, face the payments ofclaims occurring according to a loss distribution. Hence, capital must be heldby the companies so that they can guarantee the fulfilment of the claims ofeach line of insurance. The increased incidence of insurance insolvencymotivates the birth of new legislations as the European Solvency II Directive.Companies have to determine the required amount of capital and the optimalcapital allocation across the different lines of insurance in order to keep therisk of insolvency at an adequate level. The capital allocation problem may betreated in different ways, starting from the insurance company balance sheet.Here, the running process and efficiency of four methods are evaluated andcompared so as to point out the characteristics of each of the methods. TheValue-at-Risk technique is straightforward and can be easily generated for anyloss distribution. The insolvency put option principle is easily implementableand is sensitive to the degree of default. The capital asset pricing model isone of the oldest reliable methods and still provides very helpful intermediateresults. The Myers and Read marginal capital allocation approach encouragesdiversification and introduces the concept of default value. Applications ofthe four methods to some fictive and real insurance companies are provided. Thethesis further analyses the sensitivity of those methods to changes in the economiccontext and comments how insurance companies can anticipate those changes.</p>


corrected abstract:
<p>Companies issuing insurance cover, in return for insurance premiums, face the payments of claims occurring according to a loss distribution. Hence, capital must be held by the companies so that they can guarantee the fulfilment of the claims of each line of insurance. The increased incidence of insurance insolvency motivates the birth of new legislations as the European Solvency II Directive. Companies have to determine the required amount of capital and the optimal capital allocation across the different lines of insurance in order to keep the risk of insolvency at an adequate level. The capital allocation problem may be treated in different ways, starting from the insurance company balance sheet. Here, the running process and efficiency of four methods are evaluated and compared so as to point out the characteristics of each of the methods. The Value-at-Risk technique is straightforward and can be easily generated for any loss distribution. The insolvency put option principle is easily implementable and is sensitive to the degree of default. The capital asset pricing model is one of the oldest reliable methods and still provides very helpful intermediate results. The Myers and Read marginal capital allocation approach encourages diversification and introduces the concept of default value. Applications of the four methods to some fictive and real insurance companies are provided. The thesis further analyses the sensitivity of those methods to changes in the economic context and comments how insurance companies can anticipate those changes.</p>
----------------------------------------------------------------------
In diva2:618335 - missing spaces in title:
"Structural strength of work boatsand high speed crafts with floatingframes"
==>
"Structural strength of work boats and high speed crafts with floating frames"

abstract is:
<p>This thesis investigates the usage of floating frames in boats. A floating frame is a transverse frame fittedto the longitudinal stiffener flanges without contact with the shell plating, as opposed to the traditionalfixed frame which is welded to the shell plating with the stiffeners most commonly fitted through cut outsin the frame.To study the floating frame structure in a bigger perspective a finite element analysis is performed on amid ship compartment of an existing 60 m high speed catamaran ferry. The analysis is performed on amodel with scantlings as the original craft but with introduced floating frames. Stresses are analysed withrespect to maximum allowable stress as given in the DNV-rules for HAZ.High stresses are found in the bottom of the frames due to the reduced bending stiffness without effectiveflange from the shell plating. A large deformation in the shell plating relative the transverse frames isfound, creating high stresses in the stiffener webs. This deformation is induced by a large verticaldeformation of the frames.It is concluded that the transverse frames requires an increased stiffness to achieve acceptable stress levels.Possible solutions to increase stiffness are discussed, further studies are required to achieve an acceptablestructure.A design criterion for stiffeners in floating frame constructions is evaluated. The criterion considers theinteraction between a concentrated contact force and a bending moment with the purpose of simplifyingthe design process of stiffeners. The criterion is a combination of design methods from DNV HSLC andEurocode 9.The design criterion is found to give conservative results, although not unreasonably conservative. Thecriterion is suitable for the design of smaller work boats where the scantlings traditionally are not veryoptimized.</p>

corrected abstract:
<p>This thesis investigates the usage of floating frames in boats. A floating frame is a transverse frame fitted to the longitudinal stiffener flanges without contact with the shell plating, as opposed to the traditional fixed frame which is welded to the shell plating with the stiffeners most commonly fitted through cut outs in the frame.</p><p>To study the floating frame structure in a bigger perspective a finite element analysis is performed on a mid ship compartment of an existing 60 m high speed catamaran ferry. The analysis is performed on a model with scantlings as the original craft but with introduced floating frames. Stresses are analysed with respect to maximum allowable stress as given in the DNV-rules for HAZ.</p><p>High stresses are found in the bottom of the frames due to the reduced bending stiffness without effective flange from the shell plating. A large deformation in the shell plating relative the transverse frames is found, creating high stresses in the stiffener webs. This deformation is induced by a large vertical deformation of the frames.</p><p>It is concluded that the transverse frames requires an increased stiffness to achieve acceptable stress levels. Possible solutions to increase stiffness are discussed, further studies are required to achieve an acceptable structure.</p><p>A design criterion for stiffeners in floating frame constructions is evaluated. The criterion considers the interaction between a concentrated contact force and a bending moment with the purpose of simplifying the design process of stiffeners. The criterion is a combination of design methods from DNV HSLC and Eurocode 9.</p><p>The design criterion is found to give conservative results, although not unreasonably conservative. The criterion is suitable for the design of smaller work boats where the scantlings traditionally are not very optimized.</p>
----------------------------------------------------------------------
In diva2:618217 - title is missing spaces:
"Driver Assistance Systemswith focus onAutomatic Emergency Brake"
==>
"Driver Assistance Systems with focus on Automatic Emergency Brake"

abstract is:
<p>This thesis work aims at performing a survey of those technologies generally called DriverAssistance Systems (DAS). This thesis work focuses on gathering information in terms ofaccident statistics, sensors and functions and analyzing this information and shall thruaccessible information match functions with accidents, functions with sensors etc.This analysis, based on accidents in United States and Sweden during the period 1998 – 2002and two truck accident studies, shows that of all accidents with fatalities or sever injuriesinvolving a heavy truck almost half are the result of a frontal impact. About one fourth of theaccidents are caused by side impact, whereas single vehicle and rear impact collisions causesaround 14 % each. Of these, about one fourth is collision with unprotected (motorcycles,mopeds, bicycles, and pedestrians) whereas around 60 % are collision with other vehicles.More than 90 % of all accidents are partly the result of driver error and about 75 % aredirectly the result of driver error. Hence there exist a great opportunity to reduce the numberof accidents by introducing DAS.In this work, an analysis of DAS shows that six of the systems discussed today have thepotential to prevent 40 – 50 % of these accidents, whereas 20 – 40 % are estimated to actuallyhaving the chance to be prevented.One of these DAS, automatic emergency brake (AEB), has been analyzed in more detail.Decision models for an emergency brake capable to mitigate rear-end accidents has beendesigned and evaluated. The results show that this model has high capabilities to mitigatecollisions.</p>

corrected abstract:
<p>This thesis work aims at performing a survey of those technologies generally called <em>Driver Assistance Systems (DAS)</em>. This thesis work focuses on gathering information in terms of accident statistics, sensors and functions and analyzing this information and shall thru accessible information match functions with accidents, functions with sensors etc.</p><p>This analysis, based on accidents in United States and Sweden during the period 1998 – 2002 and two truck accident studies, shows that of all accidents with fatalities or sever injuries involving a heavy truck almost half are the result of a frontal impact. About one fourth of the accidents are caused by side impact, whereas single vehicle and rear impact collisions causes around 14 % each. Of these, about one fourth is collision with unprotected (motorcycles, mopeds, bicycles, and pedestrians) whereas around 60 % are collision with other vehicles.</p><p>More than 90 % of all accidents are partly the result of driver error and about 75 % are directly the result of driver error. Hence there exist a great opportunity to reduce the number of accidents by introducing <em>DAS</em>.</p><p>In this work, an analysis of <em>DAS</em> shows that six of the systems discussed today have the potential to prevent 40 – 50 % of these accidents, whereas 20 – 40 % are estimated to actually having the chance to be prevented.</p><p>One of these <em>DAS</em>, <em>automatic emergency brake (AEB)</em>, has been analyzed in more detail. Decision models for an emergency brake capable to mitigate rear-end accidents has been designed and evaluated. The results show that this model has high capabilities to mitigate collisions.</p>
----------------------------------------------------------------------
In diva2:503940 abstract is:
<p>The wheel corner module represents a new technology for controlling the motion of avehicle. It is based on a modular design around the geometric boundaries of a conventionalwheel. The typical WCM consists of a wheel containing an electrical in-wheel propulsion motor, a friction brake, a steering system and a suspension system. Generally, the braking,steering and suspension systems are controlled by means of electrical actuators. The WCMis designed to easily, by means of bolted connections and a power connector, attach toa vehicle platform constructed for the specic purpose. All functions are controlled viaan electrical system, connecting the steering column to the module. A WCM vehicle cancontain two or four wheel corner modules.The purpose of this thesis is to serve as an introduction to wheel corner module technology.The technology itself, as well as advantages and disadvantages related to wheelcorner modules are discussed. An analysis of a variety of wheel corner module concepts iscarried out. In addition, simulations are conducted in order to estimate how an increasedunsprung mass aects the ride comfort and handling performance of a vehicle.Longitudinal translation over two types of road disturbance proles, a curb and a bump,is simulated. A quarter car model as well as a full car model is utilized. The obtainedresults indicate that handling performance is deteriorated in connection to an increase dunsprung mass. The RMS value of the tire force uctuation increases with up to 18%,when 20 kg is added to each of the rear wheels of the full car model. Ride comfort is deteriorated or enhanced in connection to an increased unsprung mass, depending on the disturbance frequency of the road. When subjected to a road disturbance frequency below the eigenfrequency of the unsprung mass, ride comfort deterioration is indicated. The RMS vertical acceleration of the sprung mass increases with up to 6%, in terms of the full car model. When subjected to a road disturbance frequency above the eigenfrequency ofthe unsprung mass, decreased RMS vertical acceleration of up to 25% is noted, indicatinga signicantly enhanced ride comfort. Implementation of wheel corner module technology enables improved handling performance,safety and ride comfort compared to conventional vehicle technology. Further development, e.g. in terms of in-wheel motors and alternative power sources, is however required. In addition, major investments related to manufacturing equipment andtechnology is regarded as a signicant obstacle in terms of serial production.</p>

corrected abstract:
<p>The wheel corner module represents a new technology for controlling the motion of a vehicle. It is based on a modular design around the geometric boundaries of a conventional wheel. The typical WCM consists of a wheel containing an electrical in-wheel propulsion motor, a friction brake, a steering system and a suspension system. Generally, the braking, steering and suspension systems are controlled by means of electrical actuators. The WCM is designed to easily, by means of bolted connections and a power connector, attach to a vehicle platform constructed for the specific purpose. All functions are controlled via an electrical system, connecting the steering column to the module. A WCM vehicle can contain two or four wheel corner modules.</p><p>The purpose of this thesis is to serve as an introduction to wheel corner module technology. The technology itself, as well as advantages and disadvantages related to wheel corner modules are discussed. An analysis of a variety of wheel corner module concepts is carried out. In addition, simulations are conducted in order to estimate how an increased unsprung mass affects the ride comfort and handling performance of a vehicle.</p><p>Longitudinal translation over two types of road disturbance profiles, a curb and a bump, is simulated. A quarter car model as well as a full car model is utilized. The obtained results indicate that handling performance is deteriorated in connection to an increased unsprung mass. The RMS value of the tire force fluctuation increases with up to 18%, when 20 kg is added to each of the rear wheels of the full car model. Ride comfort is deteriorated or enhanced in connection to an increased unsprung mass, depending on the disturbance frequency of the road. When subjected to a road disturbance frequency below the eigenfrequency of the unsprung mass, ride comfort deterioration is indicated. The RMS vertical acceleration of the sprung mass increases with up to 6%, in terms of the full car model. When subjected to a road disturbance frequency above the eigenfrequency of the unsprung mass, decreased RMS vertical acceleration of up to 25% is noted, indicating a significantly enhanced ride comfort.</p><p>Implementation of wheel corner module technology enables improved handling performance, safety and ride comfort compared to conventional vehicle technology. Further development, e.g. in terms of in-wheel motors and alternative power sources, is however required. In addition, major investments related to manufacturing equipment and technology is regarded as a significant obstacle in terms of serial production.</p>
----------------------------------------------------------------------
In diva2:411684 abstract is:
<p>The aim of this master thesis is to characterize the fluid forces applied to a fuel assembly inthe core of a nuclear power plant in case of seism. The forces are studied with a simplifiedtwo-dimensional model constituted of an array of 3 by 3 infinite cylinders oscillating in aclosed box. The axial flow of water, which convects the heat in the core of a nuclear powerplant, is also taken into account. The velocity of the axial flow reaches 4m/s in the middle ofthe assembly and modifies the forces features when the cylinders move laterally.The seism is modeled as a lateral displacement with high amplitude (several cylinderdiameters) and low frequencies (below 20 Hz). In order to study the effects of the amplitudeand of the frequency of the displacement, the displacement taken is a sine function withboth controlled amplitude and frequency. Four degrees of freedom of the system will bestudied: the amplitude of the displacement, its frequency, the axial velocity amplitude andthe confinement (due to the closed box).The fluid forces exerted on the cylinders can be seen as a combination of three terms: anadded mass, related to the acceleration of cylinders, a drift force, related to the damping ofthe fluid and a force due to the interaction of the cylinder with residual vortices. The firsttwo components will be characterized through the Morison expansion, and their evolutionwith the variation of the degree of freedom of the system will be quantified. The effect ofthe interaction with the residual vortices will be observed in the plots of the forces vs. timebut also in the velocity and vorticity map of the fluid.The fluid forces are calculated with the CFD code Code_Saturne, which uses a second orderaccurate finite volume method. Unsteady Reynolds Averaged Navier Stokes simulations arerealized with a k-epsilon turbulence model. The Arbitrary Lagrange Euler model is used todescribe the structure displacement. The domain is meshed with hexahedra with thesoftware gmsh [1] and the flow is visualized with Paraview [2]. The modeling techniquesused for the simulations are described in the first part of this master thesis.</p>

corrected abstract:
<p>The aim of this master thesis is to characterize the fluid forces applied to a fuel assembly in the core of a nuclear power plant in case of seism. The forces are studied with a simplified two-dimensional model constituted of an array of 3 by 3 infinite cylinders oscillating in a closed box. The axial flow of water, which convects the heat in the core of a nuclear power plant, is also taken into account. The velocity of the axial flow reaches 4m/s in the middle of the assembly and modifies the forces features when the cylinders move laterally.</p><p>The seism is modeled as a lateral displacement with high amplitude (several cylinder diameters) and low frequencies (below 20 Hz). In order to study the effects of the amplitude and of the frequency of the displacement, the displacement taken is a sine function with both controlled amplitude and frequency. Four degrees of freedom of the system will be studied: the amplitude of the displacement, its frequency, the axial velocity amplitude and the confinement (due to the closed box).</p><p>The fluid forces exerted on the cylinders can be seen as a combination of three terms: an added mass, related to the acceleration of cylinders, a drift force, related to the damping of the fluid and a force due to the interaction of the cylinder with residual vortices. The first two components will be characterized through the Morison expansion, and their evolution with the variation of the degree of freedom of the system will be quantified. The effect of the interaction with the residual vortices will be observed in the plots of the forces vs. time but also in the velocity and vorticity map of the fluid.</p><p>The fluid forces are calculated with the CFD code Code_Saturne, which uses a second order accurate finite volume method. Unsteady Reynolds Averaged Navier Stokes simulations are realized with a k-epsilon turbulence model. The Arbitrary Lagrange Euler model is used to describe the structure displacement. The domain is meshed with hexahedra with the software gmsh [1] and the flow is visualized with Paraview [2]. The modeling techniques used for the simulations are described in the first part of this master thesis.</p>
----------------------------------------------------------------------
In diva2:1808437 - Note: The French and ENglish abstracts are at the end of the PDF file. The French abstracts is not in DiVA.


abstract is:
<p>The PTR system allows the EPR2 fuel pool to be cooled. The evacuation of the residual power fromthe pool is ensured by several heat exchangers and pumps, which have to be dimensioned in order to meetdifferent requirements.In order to dimension them, the worst-case scenario of the components must first be determined.Sensitivity to external conditions and efficiency studies enable to propose a heat exchanger design tomeet the requirements. A parametric study then allows to study more precisely the influence of thegeometry of the exchanger on the heat transfer. This allows to guide the conception of a CFD study ofthe design on the Comsol software in order to validate it. The proposed design can then be integratedinto the PTR cooling train. The train is modeled with FloMaster, in order to compute the head losses inthe hydraulic system and to propose a pump altimetry preventing cavitation.The dimensioning case of the exchangers corresponds to the operating case of the PTR trains duringunit shutdown, while the scenario that facilitates cavitation corresponds to the boiling of the fuel pool.The temperature of the cold source RRI is a sensitive data for the operation of the exchangers. In addition,the placement of the baffles and the space between the tubes play a determining role in the heat removal.It was difficult to construct the desired exchanger geometry in CFD. A compromise model was thusidentified and studied in CFD. The FloMaster study showed that the pressure drop in the PTR network isabout 15.5 mCE at the considered flow rate. Cavitation in a main train is not a problem if the pumps arelowered by at least 1.8 meters from the pool suction point.The sizing study therefore allowed us to propose a heat exchanger design close to the specifications,but this could not be precisely studied in CFD. The pressure drop study allowed to propose a pumpaltimetry preventing cavitation.</p>

corrected abstract:
<p>The PTR system allows the EPR2 fuel pool to be cooled. The evacuation of the residual power from the pool is ensured by several heat exchangers and pumps, which have to be dimensioned in order to meet different requirements.</p><p>In order to dimension them, the worst-case scenario of the components must first be determined. Sensitivity to external conditions and efficiency studies enable to propose a heat exchanger design to meet the requirements. A parametric study then allows to study more precisely the influence of the geometry of the exchanger on the heat transfer. This allows to guide the conception of a CFD study of the design on the Comsol software in order to validate it. The proposed design can then be integrated into the PTR cooling train. The train is modeled with FloMaster, in order to compute the head losses in the hydraulic system and to propose a pump altimetry preventing cavitation.</p><p>The dimensioning case of the exchangers corresponds to the operating case of the PTR trains during unit shutdown, while the scenario that facilitates cavitation corresponds to the boiling of the fuel pool. The temperature of the cold source RRI is a sensitive data for the operation of the exchangers. In addition, the placement of the baffles and the space between the tubes play a determining role in the heat removal. It was difficult to construct the desired exchanger geometry in CFD. A compromise model was thus identified and studied in CFD. The FloMaster study showed that the pressure drop in the PTR network is about 15.5 mCE at the considered flow rate. Cavitation in a main train is not a problem if the pumps are lowered by at least 1.8 meters from the pool suction point.</p><p>The sizing study therefore allowed us to propose a heat exchanger design close to the specifications, but this could not be precisely studied in CFD. The pressure drop study allowed to propose a pump altimetry preventing cavitation.</p>

French abstract:

<p>Le système PTR permet le refroidissement de la piscine combustible de l’EPR2. L’évacuation de la puissance résiduelle de la piscine est assurée par plusieurs échangeurs de chaleur et pompes, qui doivent être dimensionnés afin de respecter plusieurs critères.</p><p>Pour les dimensionner, les conditions de fonctionnement les plus défavorable des composants doivent d’abord être déterminées. Des études de sensibilité au conditions extérieures, ainsi que d’efficacité permettent de proposer un type d’échangeur pour répondre aux exigences. Une étude paramétrique permet ensuite d’étudier plus précisément l’influence de la géométrie de l’échangeur sur le transfert de chaleur. Celle-ci permet d’orienter la conception d’une étude CFD du design sur le logiciel Comsol permettant de le valider. Le design proposé peut être intégré au train de refroidissement PTR. Le train est modélisé sous FloMaster, afin de calculer les pertes de charges du réseau et proposer une altimétrie de pompe prévenant la cavitation.</p><p>Le cas dimensionnant des échangeurs correspond au cas de fonctionnement des trains PTR en arrêt de tranche, tandis que celui favorisant la cavitation correspond au scenario d’ébullition de la piscine combustible. La température de la source froide RRI est une donnée sensible pour le fonctionnement des échangeurs. De plus, le positionnement de chicanes et l’espace entre les tubes jouent un rôle déterminant dans l’évacuation de la chaleur. Il a été difficile de construire la géométrie d’échangeur souhaitée en CFD. Un modèle compromis a été trouvé et étudié en CFD. L’étude <em>FloMaster</em> a montré que les pertes de charge dans le réseau PTR sont de l’ordre de 15.5 mCE au débit considéré. La cavitation dans un train principal n’est pas un problème en abaissant de 2 mètres minimum les pompes par rapport au point d’aspiration piscine.</p><p>L’étude de dimensionnement a donc permis de proposer un design d’échangeur de chaleur se rapprochant du cahier des charges, mais celui-ci n’a pas pu être précisément étudié en CFD. L’étude de perte de charge a permis de proposer une altimétrie de pompe prévenant la cavitation.</p>
----------------------------------------------------------------------
In diva2:1795177 abstract is:
<p>Since the global financial crisis of 2008, regulatory bodies worldwide have implementedincreasingly stringent requirements for measuring and pricing default risk in financialderivatives. Counterparty Credit Risk (CCR) serves as the measure for default risk infinancial derivatives, and Credit Valuation Adjustment (CVA) is the pricing method used toincorporate this default risk into derivatives prices. To calculate the CVA, one needs the risk-neutral Probability of Default (PD) for the counterparty, which is the centre in this type ofderivative.The traditional method for calculating risk-neutral probabilities of default involves constructingcredit curves, calibrated using the credit derivative Credit Default Swap (CDS). However,liquidity issues in CDS trading present a major challenge, as the majority of counterpartieslack liquid CDS spreads. This poses the difficult question of how to model risk-neutral PDwithout liquid CDS spreads.The current method for generating proxy credit curves, introduced by the Japanese BankNomura in 2013, involves a cross-sectional linear regression model. Although this model issufficient in most cases, it often generates credit curves unsuitable for larger counterpartiesin more volatile times. In this thesis, we introduce two Long Short-Term Memory (LSTM)models trained on similar entities, which use CDS spreads as input. Our introduced modelsshow some improvement in generating proxy credit curves compared to the Nomura model,especially during times of higher volatility. While the result were more in line with the tradedCDS-market, there remains room for improvement in the model structure by using a moreextensive dataset.</p>

corrected abstract:
<p>Since the global financial crisis of 2008, regulatory bodies worldwide have implemented increasingly stringent requirements for measuring and pricing default risk in financial derivatives. Counterparty Credit Risk (CCR) serves as the measure for default risk in financial derivatives, and Credit Valuation Adjustment (CVA) is the pricing method used to incorporate this default risk into derivatives prices. To calculate the CVA, one needs the risk-neutral Probability of Default (PD) for the counterparty, which is the centre in this type of derivative.</p><p>The traditional method for calculating risk-neutral probabilities of default involves constructing credit curves, calibrated using the credit derivative Credit Default Swap (CDS). However, liquidity issues in CDS trading present a major challenge, as the majority of counterparties lack liquid CDS spreads. This poses the difficult question of how to model risk-neutral PD without liquid CDS spreads.</p><p>The current method for generating proxy credit curves, introduced by the Japanese bank Nomura in 2013, involves a cross-sectional linear regression model. Although this model is sufficient in most cases, it often generates credit curves unsuitable for larger counterparties in more volatile times.</p><p>In this thesis, we introduce two Long Short-Term Memory (LSTM) models trained on similar entities, which use CDS spreads as input. Our introduced models show some improvement in generating proxy credit curves compared to the Nomura model, especially during times of higher volatility. While the result were more in line with the traded CDS-market, there remains room for improvement in the model structure by using a more extensive dataset.</p>
----------------------------------------------------------------------
In diva2:1781520 - missing space in title:
"Extraction and optimization for modeling ofdesalination by capacitive deionization"
==>
"Extraction and optimization for modeling of desalination by capacitive deionization"

Note there were many wording differences between the abstract in the thesis and the abstract in DiVA. It seems that someone tried to clean up the grammer.

abstract is:
<p>Water scarcity is set to become a big challenge in the 21st century and more efficient desalinationtechnologies will be needed in the future. In this project, one desalination method called capacitivedeionization (CDI) is explored and we used a model called the ELC model to simulate CDI withComsol. The goal of this project focuses on evaluating the performance of CDI and how changingdifferent operational parameters of the process affects other aspects of desalination. Some examplesare power consumption, desalination rate and water usage. With the gathered information, the process of CDI can be optimized in some way. Even though our project simulates a specific model ofCDI, the hope is to have come to general conclusions regarding CDI so that the results can be usedfor other models. If the correlations between parameters are known, it will be easier to calibrate anysetup of CDI. The gathered data is exported, stored, processed, and plotted using Matlab functionsintegrated with Comsol. The results consist of two sets, the first for constant voltage and the secondfor constant current. Both have results on how desalination rate and energy efficiency are related toparameters such as internal voltage intervals controlling how long the desalination cycle is running,external voltage, and inflow salt concentration in the water. The key conclusions drawn are as thefollowing for constant voltage. High external voltages are effective in increasing both desalinationrate and energy efficiency but will degrade the CDI electrodes. The internal voltage span should bepretty long with high max internal voltage and the minimum internal voltage the same as the external voltage. The energy efficiency increase with lower salt concentrations in the inflow water up toa point. The best setup for the desalination rate is at quite a high maximum internal voltage withvaried low minimum internal voltage. For constant current, low current is generally efficient, whilethe maximum external voltage depends on the current. Avoid a high current with a low externalvoltage. By relating all these parameters, we get more insights into what an energy-efficient and fastadsorbing CDI setup looks like.</p>


corrected abstract:
<p>Water scarcity is set to become a significant challenge in the 21st century and more efficient desalination technologies will be needed in the future. In this project, a desalination method called capacitive deionization (CDI) is explored and we used the ELC model to simulate CDI with Comsol. The goal of this project focuses on evaluating the performance of CDI and how changing different operational parameters of the process affects other aspects of desalination. Some examples are power consumption, desalination rate and water usage. With the gathered information, the process of CDI can be optimized in some way. Even though our project simulates a specific model of CDI, the hope is to have come to general conclusions regarding CDI so that the results can be used for other models. If the correlations between parameters are known, it will be easier to calibrate any setup of CDI. The gathered data is exported, stored, processed, and plotted using Matlab functions integrated with Comsol. The results consist of two sets, the first for constant voltage and the second for constant current. Both show how desalination rate and energy efficiency are related to parameters such as internal voltage intervals controlling how big the desalination cycle is running, external voltage, and salinity of inlet water. The key conclusions drawn are as the following for constant voltage. High external voltages effectively increase both desalination rate and energy efficiency but will degrade the CDI electrodes. The internal voltage span should be as big as possible with high max internal voltage and a minimum internal voltage the same as the external voltage. The energy efficiency increase with lower salt concentrations in the inflow water up to a point. The best setup for the desalination rate is at a high maximum internal voltage with varied low minimum internal voltage. For constant current, low current is generally efficient, while the maximum external voltage depends on the current. Avoid a high current with a low external voltage. By relating all these parameters, we get more insights into what an energy-efficient and fast-adsorbing CDI configuration should be set up.</p>
----------------------------------------------------------------------
In diva2:1781504 abstract is:
<p>Computed x-ray tomography is one of the most common medical imaging modalities andas such ways of improving the images are of high relevance. Applying deep learningmethods to denoise CT images has been of particular interest in recent years. In thisstudy, rather than using traditional denoising metrics such as MSE or PSNR for evaluation, we use a radiomic approach combined with 3D printed phantoms as a "groundtruth" to compare with. Our approach of having a ground truth ensures that we withabsolute certainty can say what a scanned tumor is supposed to look like and compareour results to a true value. This performance metric is better suited for evaluation thanMSE since we want to maintain structures and edges in tumors and MSE-evaluationrewards over-smoothing.</p><p>Here we apply U-Net networks to images of 3D printed tumors. The 4 tumors and alung phantom were printed with PLA filament and 80% fill rate with a gyroidal patternto mimic soft tissue in a CT-scan while maintaining isotropicity. CT images of the 3Dprinted phantom and tumors were taken with a GE revolution DE scanner at KarolinskaUniversity Hospital. The networks were trained on the 2016 NIH-AAPM-Mayo ClinicLow Dose CT Grand Challenge dataset, mapping Low Dose CT images to Normal DoseCT images using three different loss functions, l1, vgg16, and vgg16_l1.</p><p>Evaluating the networks on RadiomicsShape features from SlicerRadiomics® we findcompetitive performance with TrueFidelityTM Deep Learning Image Reconstruction (DLIR)by GE HealthCareTM. With one of our networks (UNet_alt, vgg16_l1 loss function with32 features, and batch size 16 in training.) outperforming TrueFidelity in 63% of caseswhen evaluated by counting if a radiomic feature has a lower relative error comparedto ground truth after our own denoising for four different kind of tumors. The samenetwork outperformed FBP in 84% of cases which in combination with the majority ofour networks performing substantially better against FBP than TrueFidelity shows theviability of DLIR compared to older methods such as FBP.</p><p> </p>

corrected abstract:
<p>Computed x-ray tomography is one of the most common medical imaging modalities and as such ways of improving the images are of high relevance. Applying deep learning methods to denoise CT images has been of particular interest in recent years. In this study, rather than using traditional denoising metrics such as MSE or PSNR for evaluation, we use a radiomic approach combined with 3D printed phantoms as a "ground truth" to compare with. Our approach of having a ground truth ensures that we with absolute certainty can say what a scanned tumor is supposed to look like and compare our results to a true value. This performance metric is better suited for evaluation than MSE since we want to maintain structures and edges in tumors and MSE-evaluation rewards over-smoothing.</p><p>Here we apply U-Net networks to images of 3D printed tumors. The 4 tumors and a lung phantom were printed with PLA filament and 80% fill rate with a gyroidal pattern to mimic soft tissue in a CT-scan while maintaining isotropicity. CT images of the 3D printed phantom and tumors were taken with a GE revolution DE scanner at Karolinska University Hospital. The networks were trained on the 2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge dataset, mapping Low Dose CT images to Normal Dose CT images using three different loss functions, l1, vgg16, and vgg16_l1.</p><p>Evaluating the networks on RadiomicsShape features from SlicerRadiomics® we find competitive performance with TrueFidelity™ Deep Learning Image Reconstruction (DLIR) by GE HealthCare™. With one of our networks (UNet_alt, vgg16_l1 loss function with 32 features, and batch size 16 in training.) outperforming TrueFidelity in 63% of cases when evaluated by counting if a radiomic feature has a lower relative error compared to ground truth after our own denoising for four different kind of tumors. The same network outperformed FBP in 84% of cases which in combination with the majority of our networks performing substantially better against FBP than TrueFidelity shows the viability of DLIR compared to older methods such as FBP.</p>
----------------------------------------------------------------------
In diva2:1698135 abstract is:
<p>Wear condition modelling is a research topic which lately has increased in popularitydue to its significance to the railway industry. Large sums are spent on maintenanceof the network every year, which has generated a demand for understanding and predictingwear mechanisms in a multitude of components within both railway vehiclesand infrastructure. To acquire this knowledge, using a simulation software to predictwear and maintenance needs, is a cheap and dependable option. This thesis sets outto create a wear modelling software for the catenary-pantograph interaction, which intheory could enlighten railway operators of the condition of their infrastructure.</p><p>This research contains an extensive literature review on the subject of wear modelling,analysing different strategies and theories of the wear mechanism and desired modelproperties. Thereafter, an attempt to recreate a model for a representative Swedishcase was conducted; implementing parameters of a typical Swedish rail vehicle to awear rate formula, recreating a passage of a pantograph sliding along the catenary.The output was a wear rate as a function of time, a determination of life expectancyfor the infrastructure and a worn profile of the catenary wire after one year of operation.</p><p>The model, named AWEAR, was deemed functional for determining wear mechanismtendencies and behaviour for a few cases of alternating parameters. However, sinceno validation could be performed due to a lack of resources, the validity of the outputvalues could not be confirmed - thus leaving calibration of the model to future work.In conclusion, AWEAR needs to be calibrated for future research but does contain amultitude of enhancement opportunities proposed in this thesis. While not completelyfunctional, the software was deemed a useful foundation for future projects and mightresult in a product that aids operators maintaining their infrastructure.Keywords: Wear modelling, catenary pantograph interaction, mechanism,</p>

corrected abstract:
<p>Wear condition modelling is a research topic which lately has increased in popularity due to its significance to the railway industry. Large sums are spent on maintenance of the network every year, which has generated a demand for understanding and predicting wear mechanisms in a multitude of components within both railway vehicles and infrastructure. To acquire this knowledge, using a simulation software to predict wear and maintenance needs, is a cheap and dependable option. This thesis sets out to create a wear modelling software for the catenary-pantograph interaction, which in theory could enlighten railway operators of the condition of their infrastructure.</p><p>This research contains an extensive literature review on the subject of wear modelling, analysing different strategies and theories of the wear mechanism and desired model properties. Thereafter, an attempt to recreate a model for a representative Swedish case was conducted; implementing parameters of a typical Swedish rail vehicle to a wear rate formula, recreating a passage of a pantograph sliding along the catenary. The output was a wear rate as a function of time, a determination of life expectancy for the infrastructure and a worn profile of the catenary wire after one year of operation.</p><p>The model, named AWEAR, was deemed functional for determining wear mechanism tendencies and behaviour for a few cases of alternating parameters. However, since no validation could be performed due to a lack of resources, the validity of the output values could not be confirmed - thus leaving calibration of the model to future work.  In conclusion, AWEAR needs to be calibrated for future research but does contain a multitude of enhancement opportunities proposed in this thesis. While not completely functional, the software was deemed a useful foundation for future projects and might result in a product that aids operators maintaining their infrastructure.</p>
----------------------------------------------------------------------
In diva2:1595652 abstract is:
<p>A comparative study is carried out to investigate the most promising route towardsthe lightweight construction of a retractable mast for a sailing cargo vessel.Four design families are developed and compared. The primary criteria forjudgment are the structural mass, strength, and stiffness in relation to a providedbenchmark design. Additional evaluation criteria are the capital costsfor raw materials and manufacturing.The design space includes isotropic materials as well as fiber-reinforced polymer(FRP) solutions and is navigated by employing analytical evaluation methodssupported by finite element analysis (FEA). Restrictions to the designspace are given by a general arrangement of the benchmark design. This includesthe limitation to the ULS loads and the overall mast geometry.A review of relevant Det Norske Veritas (DNV) rules for classification is performedand the guidelines for wind turbine blades and wind-powered units(WPU) are judged most suitable to the design challenge. Relevant design principlesare implemented in the structural analysis.It is concluded that pure metal constructions imply an unreasonably large weightpenalty. Local buckling is found to disqualify FRP single-skin solutions as successfulcandidates. Secondary to that, strength concerns are the major driversfor the structural mass.The report presents two designs that are judged fit for the purpose, one is ahybrid truss structure from high strength low alloy steel (HSLA steel) and carbonfiber-reinforced polymer (CFRP). The second design is a sandwich constructionwith CFRP face sheets, a PVC foam core, and additional stiffeningmembers in steel.</p>

corrected abstract:
<p>A comparative study is carried out to investigate the most promising route towards lightweight construction of a retractable mast for a sailing cargo vessel. Four design families are developed and compared. The primary criteria for judgement are the structural mass, strength and stiffness in relation to a provided benchmark design. Additional evaluation criteria are the capital costs for raw materials and manufacturing.</p><p>The design space includes isotropic materials as well as fiber reinforced polymer (FRP) solutions and is navigated by employing analytical evaluation methods supported by finite element analysis (FEA). Restrictions to the design space are given by a general arrangement of the benchmark design. This includes the limitation to the ULS loads and the overall mast geometry.</p><p>A review of relevant Det Norske Veritas (DNV) rules for classification is performed and the guidelines for wind turbine blades and wind powered units (WPU) are judged most suitable to the design challenge. Relevant design principles are implemented in the structural analysis.</p><p>It is concluded that pure metal constructions imply an unreasonably large weight penalty. Local buckling is found to disqualify FRP single skin solutions as successful candidates. Secondary to that, strength concerns are the major drivers for the structural mass.</p><p>The report presents two designs that are judged fit for the purpose, one is a hybrid truss structure from high strength low alloy steel (HSLA steel) and carbon fiber reinforced polymer (CFRP). The second design is a sandwich construction with CFRP face sheets, a PVC foam core and additional stiffening members in steel.</p>
----------------------------------------------------------------------
In diva2:1527800 abstract is:
<p>With the advent of hybrids and electric vehicles, the need for lightweight and highperformancematerials is growing. Sheet molding compound (SMC) is a compositemade of short and randomized  bers that o ers a substantial weight reduction andgood mechanical properties while meeting the demand for large volume production.This thesis aims to develop a constitutive FE model of the SMC used in the bodyin black of an autonomous vehicle.To extract its properties, several physical tests were performed on specimens madeof the above-mentioned material. Both the tensile and three point bending testsresults show that the material is not homogeneous and that its properties vary fordi erent directions. The damping ratio extracted from the vibration test is muchlower than in conventional structural materials like aluminum and steel.In the FE analysis, the material was modeled both as isotropic and orthotropic.After adjusting the Young's modulus, the isotropic model shows accurate resultsuntil 1200 Hz. On the other hand, without knowing in which directions the propertiesoccur, the orthotropic model is very limited.In conclusion, even though the properties were tailored speci cally for the specimen,the model might not correctly represent the material's behavior, being itsproperties not the same for di erent components. Therefore, it is more reasonableto use average data instead.</p>

corrected abstract:
<p>With the advent of hybrids and electric vehicles, the need for lightweight and high performance materials is growing. Sheet molding compound (SMC) is a composite made of short and randomized fibers that offers a substantial weight reduction and good mechanical properties while meeting the demand for large volume production. This thesis aims to develop a constitutive FE model of the SMC used in the body in black of an autonomous vehicle.</p><p>To extract its properties, several physical tests were performed on specimens made of the above-mentioned material. Both the tensile and three point bending tests results show that the material is not homogeneous and that its properties vary for different directions. The damping ratio extracted from the vibration test is much lower than in conventional structural materials like aluminum and steel.</p><p>In the FE analysis, the material was modeled both as isotropic and orthotropic. After adjusting the Young's modulus, the isotropic model shows accurate results until 1200 Hz. On the other hand, without knowing in which directions the properties occur, the orthotropic model is very limited.</p><p>In conclusion, even though the properties were tailored specifically for the specimen, the model might not correctly represent the material's behavior, being its properties not the same for different components. Therefore, it is more reasonable to use average data instead.</p>
----------------------------------------------------------------------
In diva2:1285510 - missing space and hyphen in title:
"Method for rebuilding gaspoweredtrucks"
==>
"Method for rebuilding gas-powered trucks"

abstract is:
<p>This master thesis investigates the possibility to rebuild heavy-duty trucks poweredby liquefied natural gas (LNG) to other fuel options such as compressednatural gas (CNG), diesel or ethanol. The background is that the second handvalue for a LNG truck is lower than a similar diesel truck due to an undevelopedinfrastructure with few refuelling stations. This results in a small market for secondhand LNG trucks.Both tractor and rigid trucks are evaluated from a technical perspective to determinewhich components that need to be changed when switching from LNG toanother type of fuel. When that is completed each fuel alternative is evaluatedbased on cost and market interest. Also certification and other legislations areinvestigated to determine if they will affect the rebuilding process.The result shows that the rebuild faces different technical complexity dependingon the target fuel alternative. It is concluded that a rebuild to diesel or ethanolis expensive due to many changes needed for the engine and aftertreatment andtherefore these alternatives are not a good choice for a rebuild. A rebuild to CNGis still expensive but can be of interest for rigid trucks, but not for tractor truckssince they usually have a demand for longer range. In order to get the final cost ofthe rebuild to CNG a commercial assessment has to be made and the rebuild willdepend on in which country the rebuild is performed due to different legislationsfor re-registration which may be a an obstacle.</p>

corrected abstract:
<p>This master thesis investigates the possibility to rebuild heavy-duty trucks powered by liquefied natural gas (LNG) to other fuel options such as compressed natural gas (CNG), diesel or ethanol. The background is that the second hand value for a LNG truck is lower than a similar diesel truck due to an undeveloped infrastructure with few refuelling stations. This results in a small market for second hand LNG trucks.</p><p>Both tractor and rigid trucks are evaluated from a technical perspective to determine which components that need to be changed when switching from LNG to another type of fuel. When that is completed each fuel alternative is evaluated based on cost and market interest. Also certification and other legislations are investigated to determine if they will affect the rebuilding process.</p><p>The result shows that the rebuild faces different technical complexity depending on the target fuel alternative. It is concluded that a rebuild to diesel or ethanol is expensive due to many changes needed for the engine and aftertreatment and therefore these alternatives are not a good choice for a rebuild. A rebuild to CNG is still expensive but can be of interest for rigid trucks, but not for tractor trucks since they usually have a demand for longer range. In order to get the final cost of the rebuild to CNG a commercial assessment has to be made and the rebuild will depend on in which country the rebuild is performed due to different legislations for re-registration which may be a an obstacle.</p>
----------------------------------------------------------------------
In diva2:1184068

abstract is:
<p>In this project, the design of a transverse leaf spring for an automotive vehicle isinvestigated. A transverse leaf spring is a concept for implementing the traditionalcoil spring for the vehicle, into a spring operating through beam bending. There aredifferent constructions and layouts of said leaf spring developed previously. Onesolution is where the spring is spanning from one side to the other of the vehicle,making it a transverse leaf spring. This solution has an extra gain; it is also providingan anti-roll bar action to the ride characteristics of the vehicle.The design of the transverse leaf spring is made for an automotive research vehicle atRoyal Institute of Technology (KTH). This vehicle is designed to represent a smallcity vehicle, weighing approximately 600 𝑘𝑔. The design of the original suspensionsystem is of the type Double Wishbone with push rod and coil springs with damper.The system is modular and exactly the same for the front and rear of the vehicle.Original mounting positions on the vehicle are to be kept intact. The design of thetransverse leaf spring is made in order to mimic the exact characteristics of theoriginal suspension system.First analytical optimizations are made in order to find an initial solution. This designis then implemented in FEM-software in order to further investigate thecharacteristics and design. A final design is found that is fulfilling the requirementsand a full scale version of the transverse leaf spring is built and examined withregards to its fulfilment of requirements.</p>

corrected abstract:
<p>In this project, the design of a transverse leaf spring for an automotive vehicle is investigated. A transverse leaf spring is a concept for implementing the traditional coil spring for the vehicle, into a spring operating through beam bending. There are different constructions and layouts of said leaf spring developed previously. One solution is where the spring is spanning from one side to the other of the vehicle, making it a transverse leaf spring. This solution has an extra gain; it is also providing an anti-roll bar action to the ride characteristics of the vehicle.</p><p>The design of the transverse leaf spring is made for an automotive research vehicle at <em>Royal Institute of Technology</em> (KTH). This vehicle is designed to represent a small city vehicle, weighing approximately 600 <em>kg</em>. The design of the original suspension system is of the type <em>Double Wishbone</em> with push rod and coil springs with damper. The system is modular and exactly the same for the front and rear of the vehicle. Original mounting positions on the vehicle are to be kept intact. The design of the transverse leaf spring is made in order to mimic the exact characteristics of the original suspension system.</p><p>First analytical optimizations are made in order to find an initial solution. This design is then implemented in FEM-software in order to further investigate the characteristics and design. A final design is found that is fulfilling the requirements and a full scale version of the transverse leaf spring is built and examined with regards to its fulfilment of requirements.</p>
----------------------------------------------------------------------
In diva2:1141679 - title missing spaces:
"Swing check valvecharacterization: 3D CFDvalidation of one dimensionalmodels used in RELAP5"
==>
"Swing check valve characterization: 3D CFD validation of one dimensional models used in RELAP5"

abstract is:
<p>In a previous thesis work a swing check valve was studied with CFD analysisin order to nd correlations that could provide a good input for a onedimensionalmodel of the same. In this document, starting from the previousthesis results and using the model by Li and Liou as the reference work, a checkvalve is investigated and the hydraulic torque coecients identied. In this wayit becomes possible to analyze the behavior of the same valve with a 1D codecalled RELAP5.The rst part of the work was dedicated to understanding the dynamics lyingbehind the movement of swing check vale, and to the construction of a suitable3D CFD model being able to nd the required coecients. The results weresubsequently elaborated and implemented in the RELAP5 code, in order to runthe 1D simulations. In the second part of the job, several transient simulations ofdierent pipelines were conducted with the 1D model, monitoring in particularthe closure of the valve over time.In the end, the data obtained in RELAP5 were compared to those fromequivalent 3D CFD analysis and from an alternative 1D approach by Adamkowski.Although not completely matching, the results showed that the 1D model byLi and Liou has a good ability to accurately simulate the valve. Further workcould be surely done on particular topics in order to improve and tune thismodel, making it a consistent and cheaper alternative to 3D simulations, ableto accurately simulate a wide range of cases.</p>

corrected abstract:
<p>In a previous thesis work a swing check valve was studied with CFD analysis in order to find correlations that could provide a good input for a one-dimensional model of the same. In this document, starting from the previous thesis results and using the model by Li and Liou as the reference work, a check valve is investigated and the hydraulic torque coefficients identified. In this way it becomes possible to analyze the behavior of the same valve with a 1D code called RELAP5.</p><p>The first part of the work was dedicated to understanding the dynamics lying behind the movement of swing check vale, and to the construction of a suitable 3D CFD model being able to find the required coefficients. The results were subsequently elaborated and implemented in the RELAP5 code, in order to run the 1D simulations. In the second part of the job, several transient simulations of different pipelines were conducted with the 1D model, monitoring in particular the closure of the valve over time.</p><p>In the end, the data obtained in RELAP5 were compared to those from equivalent 3D CFD analysis and from an alternative 1D approach by Adamkowski. Although not completely matching, the results showed that the 1D model by Li and Liou has a good ability to accurately simulate the valve. Further work could be surely done on particular topics in order to improve and tune this model, making it a consistent and cheaper alternative to 3D simulations, able to accurately simulate a wide range of cases.</p>
----------------------------------------------------------------------
In diva2:1120505 abstract is:
<p>This paper presents a locomotion system suitable for interactive usethat can plan realistic paths for small numbers of bipedal charactersin virtual worlds. Earlier approaches are extended by allowing animationsto be arbitrarily blended to increase the range of motions thatthe character can produce and our system also achieves greater performancecompared to the earlier approaches. The system uses a graphof valid footprints in the world in which is searched for a path thatthe character should traverse. The resulting sequence of footprints aresmoothed and refined to make them more similar to the character’soriginal animations. To make the motion smoother the curvature andother parameters of the path are estimated and those estimates areused to interpolate between different sets of similar animation clips. Asthe system is based on footprints it allows characters to navigate evenacross regions which are not directly connected, for example by jumpingover the gaps between disconnected regions. We have implementedthe system in C# using the Unity Game Engine and we evaluate it bymaking the character perform various actions such as walking, runningand jumping and study the visual result.Accompanying material can be found at http://arongranberg.com/research/thesis2017.</p>


corrected abstract:
<p>This paper presents a locomotion system suitable for interactive use that can plan realistic paths for small numbers of bipedal characters in virtual worlds. Earlier approaches are extended by allowing animations to be arbitrarily blended to increase the range of motions that the character can produce and our system also achieves greater performance compared to the earlier approaches. The system uses a graph of valid footprints in the world in which is searched for a path that the character should traverse. The resulting sequence of footprints are smoothed and refined to make them more similar to the character's original animations. To make the motion smoother the curvature and other parameters of the path are estimated and those estimates are used to interpolate between different sets of similar animation clips. As the system is based on footprints it allows characters to navigate even across regions which are not directly connected, for example by jumping over the gaps between disconnected regions. We have implemented the system in C# using the Unity Game Engine and we evaluate it by making the character perform various actions such as walking, running and jumping and study the visual result. Accompanying material can be found at http://arongranberg.com/research/thesis2017.</p>
----------------------------------------------------------------------
In diva2:1120039 abstract is:
<p>Recommender systems can be seen everywheretoday, having endless possibilities of implementation. However, operating inthe background, they can easily be passed without notice. Essentially, recommendersystems are algorithms that generate predictions by operating on a certain dataset. Each case of recommendation is environment sensitive and dependent on thecondition of the data at hand. Consequently, it is difficult to foresee whichmethod, or combination of methods, to apply in a particular situation forobtaining desired results. The area of recommender systems that this thesis isdelimited to is Collaborative filtering (CF) and can be split up into threedifferent categories, namely memory based, model based and hybrid algorithms.This thesis implements a CF algorithm for each of these categories and setsfocus on comparing their prediction accuracy and their dependency on the amountof available training data (i.e. as a function of sparsity). The results showthat the model based algorithm clearly performs better than the memory based,both in terms of overall accuracy and sparsity dependency. With an increasingsparsity level, the problem of having users without any ratings is encountered,which greatly impacts the accuracy for the memory based algorithm. A hybridbetween these algorithms resulted in a better accuracy than the model basedalgorithm itself but with an insignificant improvement.</p>

corrected abstract:
<p>Recommender systems can be seen everywhere today, having endless possibilities of implementation. However, operating in the background, they can easily be passed without notice. Essentially, recommender systems are algorithms that generate predictions by operating on a certain data set. Each case of recommendation is environment sensitive and dependent on the condition of the data at hand. Consequently, it is difficult to foresee which method, or combination of methods, to apply in a particular situation for obtaining desired results. The area of recommender systems that this thesis is delimited to is Collaborative filtering (CF) and can be split up into three different categories, namely memory based, model based and hybrid algorithms. This thesis implements a CF algorithm for each of these categories and sets focus on comparing their prediction accuracy and their dependency on the amount of available training data (i.e. as a function of sparsity). The results show that the model based algorithm clearly performs better than the memory based, both in terms of overall accuracy and sparsity dependency. With an increasing sparsity level, the problem of having users without any ratings is encountered, which greatly impacts the accuracy for the memory based algorithm. A hybrid between these algorithms resulted in a better accuracy than the model based algorithm itself but with an insignificant improvement.</p>
----------------------------------------------------------------------
