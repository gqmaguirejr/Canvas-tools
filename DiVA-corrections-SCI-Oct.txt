Duplicates?   'diva2:408826' and 'diva2:512408'
----------------------------------------------------------------------
Duplicates: 'diva2:1221420', 'diva2:1229928' - note that one has the full
text.
----------------------------------------------------------------------
Duplicates: 'diva2:1221420', 'diva2:1229928' - note that one has the full
text.
----------------------------------------------------------------------
Duplicates?   'diva2:408826' and 'diva2:512408' 
Not duplicates - but an error in document uploaded
----------------------------------------------------------------------
Duplicates - ['diva2:509959', 'diva2:515592' both have full text but one also
has a compenfium
----------------------------------------------------------------------
Are these duplicates: diva2:1033230 diva2:1033189 ?
----------------------------------------------------------------------
Duplicates? 'diva2:1033239', 'diva2:1045115'
----------------------------------------------------------------------
===============================================================================
Above were all reported on or before 2024-10-11
----------------------------------------------------------------------
I get a error when trying to get the fill text for diva2:1900086 - via
https://kth.diva-portal.org/smash/get/diva2:1900086/FULLTEXT01.pdf - the file
will not open and it appears to have been truncated - as the usual PDF file
ending is not present.
----------------------------------------------------------------------
The full text fo diva2:1873390 
https://kth.diva-portal.org/smash/get/diva2:1873390/FULLTEXT01.pdf
seems to be blank pages - the file is truncated and cannot be repaired.
----------------------------------------------------------------------
diva2:736211 has the text of the summary and not that of the abstract, as the DiVA abstract
----------------------------------------------------------------------
Are 'diva2:754257' and  'diva2:753742' duplicates?
----------------------------------------------------------------------
The Swedish abstract for diva2:1595164 is actually in English.
----------------------------------------------------------------------
Are diva2:1045047 and diva2:1033220 duplicates?
----------------------------------------------------------------------
Are diva2:839893 and diva2:737929 duplicates?
The abstract for the second is the same except for two spaces.
----------------------------------------------------------------------
In diva2:644350 - missing symbols, missing ligatures, and colons rather than decimal points::

<p>In this thesis a detailed discussion of the topic percolation theory in squared lattices in</p><p>two dimensions will be conducted. To support this discussion numerical calculations will</p><p>be done. For the data analysis and simulations the Hoshen-Kopelman-Algorithm [2] will</p><p>be used. All concepts deduced will nally lead to the determination of the conductance's</p><p>exponent</p><p>t in random resistor networks. Using Derrida's transfer matrix program to</p><p>calculate the conductivity of random resistors in two and three dimensions [11] and</p><p>the nite-size scaling approach were used. In two dimensions</p><p>t= = 0:955 0:006 was</p><p>obtained. Were</p><p>is the exponent of the correlation length in innite lattices. This</p><p>value is in excellent agreement with Derrida (</p><p>t= = 0:960:02, [11]) and slightly smaller</p><p>than Sahimi (</p><p>t= = 0:97480:001, [21]). In three dimensions the same approach yielded</p><p>t=</p><p>= 2:155 0:012 which some what smaller than the value found by Sahimi t= =</p><p>2</p><p>:27 0:20 [21] and Gingold and Lobb t= = 2:276 0:012 [25].</p>

abstract should be:

diva2:644350: <p>In this thesis a detailed discussion of the topic percolation theory in squared lattices in two dimensions will be conducted. To support this discussion numerical calculations will be done. For the data analysis and simulations the Hoshen-Kopelman-Algorithm [2] will be used. All concepts deduced will nally lead to the determination of the conductance's exponent t in random resistor networks. Using Derrida's transfer matrix program to calculate the conductivity of random resistors in two and three dimensions [11] and the finite-size scaling approach were used. In two dimensions <em>t/&nu;</em> = 0.955&pm;0.006 was obtained. Were is the exponent of the correlation length in infinite lattices. This value is in excellent agreement with Derrida ( <em>t/&nu;</em> = 0.96&pm;0.02, [11]) and slightly smaller than Sahimi ( <em>t/&nu;</em> = 0.9748&pm;0.001, [21]). In three dimensions the same approach yielded <em>t/&nu;</em> = 2.155&pm;0.012 which some what smaller than the value found by Sahimi <em>t/&nu;</em> = 2.:27&pm;0.20 [21] and Gingold and Lobb <em>t/&nu;</em> = 2.276&pm;0.012 [25].</p>
----------------------------------------------------------------------
The full text for diva2:1210790 has a different thesis: https://kth.diva-portal.org/smash/get/diva2:1210790/FULLTEXT02.pdf
----------------------------------------------------------------------
In diva2:571089

<p>Curve fitting is used in a variety of fields, especially in physics, mathematics and economics.</p><p>The method is often used to smooth noisy data and for doing path planning. In this bachelor</p><p>thesis calculus of variations will be used to derive a formula for finding an optimal curve to fit a</p><p>set of data points. We evaluate a cost function (defined on the set of all curves</p><p></p><p>f on the interval</p><p>[</p><p></p><p>a; b]) given by F(f) =</p><p>R</p><p></p><p>b</p><p>a</p><p></p><p>(f00(x))2dx +</p><p>P</p><p></p><p>n</p><p>i</p><p></p><p>=1(f(xi) 􀀀 yi)2. The integral term represents the</p><p>smoothness of the curve, the interpolation error is given by the summation term and</p><p></p><p>&gt; 0 is</p><p>defined as the interpolation parameter. An ideal curve minimizes the interpolation error and</p><p>is relatively smooth. This is problematic since a smooth function generally has a large interpolation</p><p>error when doing curve fitting, and therefore the interpolation parameter</p><p></p><p>is needed</p><p>to decide how much consideration should be given to each attribute. For the cost function</p><p></p><p>F</p><p>a larger value of</p><p></p><p>decreases the interpolation error of the curve. The analytical calculations</p><p>performed made it possible to construct a</p><p></p><p>Matlab program, that could be used to solve the</p><p>minimization problem. In the result part some examples are presented for different values of</p><p></p><p>.</p><p>The conclusion is that a larger value of the interpolation parameter</p><p></p><p>is generally needed when</p><p>using more data points and if the points are closely placed on the x-axis. Further on, a method</p><p>called Ordinary Cross Validation (OCV) is evaluated to find an optimal value of</p><p></p><p>. This method</p><p>gave good results, except for the case when the points could almost be fitted with a straight line.</p>

corrected abstract:

<p>Curve fitting is used in a variety of fields, especially in physics, mathematics and economics. The method is often used to smooth noisy data and for doing path planning. In this bachelor thesis calculus of variations will be used to derive a formula for finding an optimal curve to fit a set of data points. We evaluate a cost function (defined on the set of all curves <em>f</em> on the interval [a, b]) given by <em>F(f) = &int;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup style="position: relative; top: -0.5rem; left: 0.05rem;">b</sup> <sub style="position: relative; bottom: -0.3rem; left: -0.1rem;">a</sub></span></span>(f&Prime;(x))<sup>2</sup> dx + &lambda; &sum;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup style="position: relative; top: -0.5rem; left: 0.05rem;">n</sup><sub style="position: relative; bottom: -0.5rem; left: 0.05rem;">i=1</sub></span></span>(f(x<sub>i</sub>) - y<sub>i</sub>)<sup>2</sup></em>. The integral term represents the smoothness of the curve, the interpolation error is given by the summation term and &lambda; &gt; 0 is defined as the interpolation parameter. An ideal curve minimizes the interpolation error and is relatively smooth. This is problematic since a smooth function generally has a large interpolation error when doing curve fitting, and therefore the interpolation parameter &lambda; is needed to decide how much consideration should be given to each attribute. For the cost function <em>F</em> a larger value of &lambda; decreases the interpolation error of the curve. The analytical calculations performed made it possible to construct a  Matlab program, that could be used to solve the minimization problem. In the result part some examples are presented for different values of &lambda;. The conclusion is that a larger value of the interpolation parameter &lambda; is generally needed when using more data points and if the points are closely placed on the x-axis. Further on, a method called Ordinary Cross Validation (OCV) is evaluated to find an optimal value of &lambda;. This method gave good results, except for the case when the points could almost be fitted with a straight line.</p>

If MathJax were installed one could replace the long HTML with:
 $F(f) = \int_{a}^{b}(f^{\prime\prime}(x))^2 dx + \lambda \sum_{i=1}^{n} (f(x_i) -y_i)^2$
----------------------------------------------------------------------
In diva2:557257

<p>For a field k and a grading of the polynomial ringk[t] with Hilbert functionh, we consider the Quot functor Quoth V , where V = ? di =1k[t] is a finitely generated and free k[t]-module. The Quot functor parametrizes, for any k-algebra B, homogeneous B [t]-submodulesN⊆B⊗kV such that the graded components of the quotient( B⊗kV)/Nare locally freeB-modules of rank given byh. We find that it is locallyrepresentable by a polynomial ring over kin a finite number of variables. Finally, weshow that there is a scheme that represents the Quot functor that is both smooth and irreducible.</p>


Corrected abstract:
<p>For a field <em>k</em> and a grading of the polynomial ring <em>k[t]</em> with Hilbert function <em>h</em>, we consider the Quot functor Quoth <em>V<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>h</sup><sub>V</sub></span></span></em> , where <em>V = &otimes; b<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>d</sup><sub>i=1</sub></span></span> k[t]</em> is a finitely generated and free <em>k[t]</em>-module. The Quot functor parametrizes, for any <em>k</em>-algebra <em>B</em>, homogeneous <em>B[t]</em>-submodules <em>N ⊆ B⊗<sub>k</sub>V</em> such that the graded components of the quotient (<em>B⊗<sub>k</sub>V</em>)/N are locally free <em>B</em>-modules of rank given by <em>h</em>. We find that it is locally representable by a polynomial ring over <em>k</em> in a finite number of variables. Finally, we show that there is a scheme that represents the Quot functor that is both smooth and irreducible.</p>

----------------------------------------------------------------------
In diva2:1576394 custom font encoding have been used - you cannot select or search for text.
----------------------------------------------------------------------
In diva2:1816881, many words were merged in the abstract:

<p>Gas turbines can experience various changes that affect their performance.Compressor fouling is one of the leading causes that deteriorate the gas turbineperformance. This research aims to investigate the impact of compressorfouling on the performance of gas turbines and the rotodynamic behaviorof gas turbines. Fouling was simulated as a reduction of mass flow and areduction of compressor isentropic efficiency by using Turbomatch software.A rotor–bearing model was created to analyze the vibration behavior dueto compressor fouling by using MADYN 2000 software and that particledeposition leads to rotor imbalance. The results show that the main variationsfor performance are power output, pressure ratio and EGT. For the rotodynamicmodel, the result illustrates an increase in vibration level for the first andsecond bearings and a decrease for the third bearing. The results also predictedthat parameters mass flow, compressor discharge temperature or specific fuelconsumption show a similar trend compared to the increase in vibrations. Thisresult can be used in conjunction with GPA analysis to predict the foulingcondition and help in identifying the severity of the fouling condition.</p>

Corrected abstract:

diva2:1816881: <p>Gas turbines can experience various changes that affect their performance. Compressor fouling is one of the leading causes that deteriorate the gas turbine performance. This research aims to investigate the impact of compressor fouling on the performance of gas turbines and the rotodynamic behavior of gas turbines. Fouling was simulated as a reduction of mass flow and a reduction of compressor is entropic efficiency by using Turbomatch software. A rotor–bearing model was created to analyze the vibration behavior dueto compressor fouling by using MADYN 2000 software and that particle deposition leads to rotor imbalance. The results show that the main variations for performance are power output, pressure ratio and EGT. For the rotodynamic model, the result illustrates an increase in vibration level for the first and second bearings and a decrease for the third bearing. The results also predicted that parameters mass flow, compressor discharge temperature or specific fuel consumption show a similar trend compared to the increase in vibrations. This result can be used in conjunction with GPA analysis to predict the fouling condition and help in identifying the severity of the fouling condition.</p>
----------------------------------------------------------------------
In diva2:1216708 merged words and innecessary text:

<p>In this project, we aim to find a method for obtainingthe factors in a bull/bear market factor model for asset returnand variance, given an optimal portfolio. The proposed methodwas derived using the Karush-Kuhn-Tucker (KKT) conditionsfor optimal solutions to the convex Markowitz portfolio selectionproblem. For synthetic data where all necessary parameters wereknown exactly, the method could give bounds on the factors. Theexact values of the factors were obtained when short selling wasallowed, and in some instances when short selling was forbidden.The method was evaluated on real-world data with varyingresults, possibly due to estimation errors and invalid assumptionsabout the model of the investor.I. INTRODUC</p>


Corrected abstract:

<p>In this project, we aim to find a method for obtainingthe factors in a bull/bear market factor model for asset returnand variance, given an optimal portfolio. The proposed method was derived using the Karush-Kuhn-Tucker (KKT) conditions for optimal solutions to the convex Markowitz portfolio selection problem. For synthetic data where all necessary parameters were known exactly, the method could give bounds on the factors. The exact values of the factors were obtained when short selling was allowed, and in some instances when short selling was forbidden. The method was evaluated on real-world data with varying results, possibly due to estimation errors and invalid assumptions about the model of the investor.</p>

----------------------------------------------------------------------
In diva2:1900086 there is an error in the PDF, it will fail to load - see
https://kth.diva-portal.org/smash/get/diva2:1900086/FULLTEXT01.pdf

The file seems to have been cut off after 14,680,064 bytes.
----------------------------------------------------------------------
In diva2:1348434, many equations are incorrect and the words are not correct:

<p>Better Shelter RHU is a social enterprise developing and providing temporary Refugee Housing Units to aid regions of crisis. The shelters are deployed worldwide and they are subjected to harsh weather conditions particularly to heavy wind loads. To maximise the amount of units deployed, the shelters have to be cost-efficient and material lead times need to be short. In order to achieve this, an evaluation to use lesser strength materials in the load bearing structure specifically the main joint named Joint1 - is assessed in this thesis. To assess the feasibility to change the material of the joint to an alternative steel with lower tensile strength and elongation ratio, the current performance is first analysed and then compared to the performance with an alternative cheaper material available for the production method. Modeling of the wind loads were made with fluid analysis and the resulting pressures were transferred on to the load bearing frame. From the frame, displacements were derived which were subsequently transferred to a subassembly with Joint1 in focus. From the sub-assembly, stresses for a wind load of 28 m/s could be evaluated for the joint. For the current material, which has a yield strength denoted RC shown in regions of about 1.09 · RC has a yield strength denoted  and a tensile strength denoted RC p02 or 0.6 · RC m, incipient plasticity were p02 and a tensile strength denoted RA m. The alternative material, which m, plasticity was shown in similar regions but also areas where the stresses reached tensile strength (1.03 · RA m) at the same wind speed. Conclusively, the alternative material appears as more hazardous because of the lower tensile strength compared to the current material. These results are based on conservative assumptions where minimum values of material data are used and the simulated models are simplified.</p><p> </p>

Corrected abstract:
<p><em>Better Shelter RHU</em> is a social enterprise developing and providing temporary Refugee Housing Units to aid regions of crisis. The shelters are deployed worldwide and they are subjected to harsh weather conditions particularly to heavy wind loads. To maximise the amount of units deployed, the shelters have to be cost-efficient and material lead times need to be short. In order to achieve this, an evaluation to use lesser strength materials in the load bearing structure specifically the main joint named <em>Joint1</em> - is assessed in this thesis.</p>
<p>To assess the feasibility to change the material of the joint to an alternative steel with lower tensile strength and elongation ratio, the current performance is first analyzed and then compared to the performance with an alternative cheaper material available for the production method.</p>
<p>Modeling of the wind loads were made with fluid analysis and the resulting pressures were transferred on to the load bearing frame. From the frame, displacements were derived which were subsequently transferred to a subassembly with Joint1 in focus. From the sub-assembly, stresses for a wind load of 28 m/s could be evaluated for the joint. For the current material, which has a yield strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>p02</sub></span></span></em> and a tensile strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>m</sub></span></span></em>, incipient plasticty were shown in regions of about <em>1.09 &middot; R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>p02</sub></span></span></em> or <em>0.6 &middot; R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>m</sub></span></span></em>. The alternative material, which has a yield strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>A</sup><sub>p02</sub></span></span></em> and a tensile strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>A</sup><sub>m</sub></span></span></em>, plasticity was shown in similar regions but also areas where the stresses reached tensile strength (<em>1.03 &middot; R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>A</sup><sub>m</sub></span></span></em>) at the same wind speed. Conclusively, the alternative material appears as more hazardous because of the lower tensile strength compared to the current material. These results are based on conservative assumptions where minimum values of material data are used and the simulated models are simplified.</p>
----------------------------------------------------------------------
Possible duplicates:
diva2:1045047: <p>In this project we are implementing group formations in to the game engine Unity3D. Groups</p><p>of people move in a certain way when avoiding other groups and obstacles. With the use of the</p><p>steering package UnitySteer we have made some example scenes using di erent steering behaviors.</p><p>We have made an implementation of the RVO2 package in Unity. With this implementation we</p><p>can simulate various scenarios with agents avoiding each other. Human pedestrians tend to walk</p><p>in groups. We adapted this behavior in our program to make our simulations as real as possible.</p><p>Finally we evaluated the performance of our simulation by checking the frames per seconds when</p><p>increasing the number of agents.</p><p>Keywords: Agent, Steering, Obstacle Avoidance, RVO, Groups, Formations, Real-Time, Unity.</p>
diva2:1033220: <p>In this project we are implementing group formations in to the game engine Unity3D. Groups</p><p>of people move in a certain way when avoiding other groups and obstacles. With the use of the</p><p>steering package UnitySteer we have made some example scenes using di erent steering behaviors.</p><p>We have made an implementation of the RVO2 package in Unity. With this implementation we</p><p>can simulate various scenarios with agents avoiding each other. Human pedestrians tend to walk</p><p>in groups. We adapted this behavior in our program to make our simulations as real as possible.</p><p>Finally we evaluated the performance of our simulation by checking the frames per seconds when</p><p>increasing the number of agents.</p><p>Keywords: Agent, Steering, Obstacle Avoidance, RVO, Groups, Formations, Real-Time, Unity.</p>
----------------------------------------------------------------------
In diva2:516126 
missing equations:

<p>Uncertainties in radiative effects of the quarks in -background in the form of final state radiation (FSR) are significant when it comes to reducing all forms of systematics that can arise from measuring the jets energy. Analysis on FSR is in general conducted on different simulated samples where one has included the radiative effect using algorithms such as PYTHIA[29]. The hypothesis is that through the re-weighting of the -background nominal sample one could add a better representation of the FSR effect. Finding a simple way to include a better description of FSR would not only save time in the simulation process but it would also be a way to reduce the systematic errors originating from limited MC statistics. Due to statistical effects coming from the simulations one cannot use the basic approach to define the effect of FSR as simply the difference between nominal and FSR. Two methods are tested to estimate the FSR effects; the first method uses a set of efficiency factors to represent the signal regions, the second method is to add a weight to the events of the nominal sample. The first method show positive results, especially in SR2, compared to a basic analysis, with an uncertainty of the FSR effect of: SR1:±29% SR2: ±51% SR3: ±37%. While a basic analysis gave an uncertainty of ±42%, ±122% and 36%. The second method shows positive signs where the re-weighted sample moves closer to the behaviour of the FSR sample. However, both methods are based on insufficient amount of statistics to draw any absolute conclusions.</p>

Corrected abstract:

diva2:516126: <p>Uncertainties in radiative effects of the quarks in <em>t<span style="text-decoration: overline;">t</span></em>-background in the form of final state radiation (FSR) are significant when it comes to reducing all forms of systematics that can arise from measuring the jets energy. Analysis on FSR is in general conducted on different simulated samples where one has included the radiative effect using algorithms such as PYTHIA[29]. The hypothesis is that through the re-weighting of the <em>t<span style="text-decoration: overline;">t</span></em>-background nominal sample one could add a better representation of the FSR effect. Finding a simple way to include a better description of FSR would not only save time in the simulation process but it would also be a way to reduce the systematic errors originating from limited MC statistics. Due to statistical effects coming from the simulations one cannot use the basic approach to define the effect of FSR as simply the difference between nominal and FSR. Two methods are tested to estimate the FSR effects; the first method uses a set of efficiency factors to represent the signal regions, the second method is to add a weight to the events of the nominal sample. The first method show positive results, especially in SR2, compared to a basic analysis, with an uncertainty of the FSR effect of: SR1:±29% SR2: ±51% SR3: ±37%. While a basic analysis gave an uncertainty of ±42%, ±122% and 36%. The second method shows positive signs where the re-weighted sample moves closer to the behaviour of the FSR sample. However, both methods are based on insufficient amount of statistics to draw any absolute conclusions.</p>
----------------------------------------------------------------------
In diva2:783990 - missing spaces in title:
"Manufacturing and Evaluation ofNovel Composite Sandwich X-jointDesigns for Naval Applications"
==>
"Manufacturing and Evaluation of Novel Composite Sandwich X-joint Designs for Naval Applications"

abstract is:
<p>During construction of large sandwich structures one of or perhaps the greatest difficulty lies increating good joints. The joint is often the first component to fail and constitute a large part of theweight of the structure and production workload. The study is focused on investigating if noveljoining methods could affect how military vessels are able to handle an internal blast withoutcatastrophic damage to the sections surrounding the area of the explosion. If the conventionalmethod of joining sandwich panels was to be used the result of an internal blast would mostprobably be a complete failure of the surrounding joints. This is based on that the conventional Xjointslacks continuous fibers through the joint in at least one direction and are therefore weak whensubjected to tensile forces in that direction. The belief is that a joint with continuous fibers wouldhave a smaller risk of failure or at least the failure would be less severe than with the conventionaljoints and could therefore maintain its structural integrity until reparations can be made.The purpose of this study is to manufacture small scale samples of novel X-joint design concepts.These concepts are then compared by manufacturability, tensile strength in either direction of thejoint and the joints flexibility. How well the promising novel concepts handle an internal blast cannotbe seen in this study, since full scale samples and advanced blast trials would be required.Six novel methods for joining of composite sandwich panels and a reference conventional joint,referred to as the standard joint, were manufactured and evaluated through three tensile tests. Outof the six concepts, all except the 3D-woven joint was found to have advantages over the standardtype, which only surpassed the novel joints in the aspect of bulkhead strength, and the lath and thebundle joint concepts were found to be the most promising. The tensile strength tests gave that theultimate strength of lath joint was 86% of the reference value in its deck direction and at least 75%in the bulkhead direction when reinforced with laths (49% unreinforced), where the reference value(100%) was the ultimate strength of the standard joint bulkhead. The tensile test results for thebundle joint, which did not have a reinforced bulkhead, reached 77% and 66% (deck respectivelybulkhead) of the reference strength value. These results supports the theory that the circular holesof the bundle concept has less impact on the strength of the bulkhead than the rectangular holes ofthe lath concept. The fiber bundles, however, are difficult and time consuming to work with,especially since the fiber bundles needs to be flattened and spread out to increase the area ofattachment when adhered to the sandwich panels. In contrast, the lath pieces are easy tomanufacture and work with during the final assembly and therefore more suitable to use whenreinforcement of the bulkhead is required.The joint flexibility test indicated that the most flexible joint was, as might be expected, the taperedfinger joint. If the tapered panel could be designed to counteract or avoid delamination of thetapering area and if the panels of the joint could be prevented from bending to the point fracture ofthe single skin laminates, this concept could be a suitable solution for confining a blast in terms ofrapidly developing membrane forces. The lath and the bundle joints were once again the strongestand reacted very similar. Unfortunately no difference could be found between the two jointconcepts because the hinges used to attach the samples in the test broke before the samples werebroken.Since the fiber properties and amounts were not compared it is not definite that either of the lath orthe bundle concept is better than the other in terms of mechanical properties. The conclusion is thatboth these two concepts work, are relatively easy to produce and have a far greater potential thanthat of the standard joint, in the internal blast situation and as a joining method in general.</p>

corrected abstract:
<p>During construction of large sandwich structures one of or perhaps the greatest difficulty lies in creating good joints. The joint is often the first component to fail and constitute a large part of the weight of the structure and production workload. The study is focused on investigating if novel joining methods could affect how military vessels are able to handle an internal blast without catastrophic damage to the sections surrounding the area of the explosion. If the conventional method of joining sandwich panels was to be used the result of an internal blast would most probably be a complete failure of the surrounding joints. This is based on that the conventional X-joints lacks continuous fibers through the joint in at least one direction and are therefore weak when subjected to tensile forces in that direction. The belief is that a joint with continuous fibers would have a smaller risk of failure or at least the failure would be less severe than with the conventional joints and could therefore maintain its structural integrity until reparations can be made.</p><p>The purpose of this study is to manufacture small scale samples of novel X-joint design concepts. These concepts are then compared by manufacturability, tensile strength in either direction of the joint and the joints flexibility. How well the promising novel concepts handle an internal blast cannot be seen in this study, since full scale samples and advanced blast trials would be required.</p><p>Six novel methods for joining of composite sandwich panels and a reference conventional joint, referred to as the standard joint, were manufactured and evaluated through three tensile tests. Out of the six concepts, all except the 3D-woven joint was found to have advantages over the standard type, which only surpassed the novel joints in the aspect of bulkhead strength, and the lath and the bundle joint concepts were found to be the most promising. The tensile strength tests gave that the ultimate strength of lath joint was 86% of the reference value in its deck direction and at least 75% in the bulkhead direction when reinforced with laths (49% unreinforced), where the reference value (100%) was the ultimate strength of the standard joint bulkhead. The tensile test results for the bundle joint, which did not have a reinforced bulkhead, reached 77% and 66% (deck respectively bulkhead) of the reference strength value. These results supports the theory that the circular holes of the bundle concept has less impact on the strength of the bulkhead than the rectangular holes of the lath concept. The fiber bundles, however, are difficult and time consuming to work with, especially since the fiber bundles needs to be flattened and spread out to increase the area of attachment when adhered to the sandwich panels. In contrast, the lath pieces are easy to manufacture and work with during the final assembly and therefore more suitable to use when reinforcement of the bulkhead is required.</p><p>The joint flexibility test indicated that the most flexible joint was, as might be expected, the tapered finger joint. If the tapered panel could be designed to counteract or avoid delamination of the tapering area and if the panels of the joint could be prevented from bending to the point fracture of the single skin laminates, this concept could be a suitable solution for confining a blast in terms of rapidly developing membrane forces. The lath and the bundle joints were once again the strongest and reacted very similar. Unfortunately no difference could be found between the two joint concepts because the hinges used to attach the samples in the test broke before the samples were broken.</p><p>Since the fiber properties and amounts were not compared it is not definite that either of the lath or the bundle concept is better than the other in terms of mechanical properties. The conclusion is that both these two concepts work, are relatively easy to produce and have a far greater potential than that of the standard joint, in the internal blast situation and as a joining method in general.</p>
----------------------------------------------------------------------
In diva2:1110812 abstract is:
<p>Automotive development has always been need-based and the product of today is an evolutionover several decades and a diversied technology application to deliver better products to theend users. Steady increase in the deployment of on-board electronics and software is characterizedby the demand and stringent regulations. Today, almost every function on-board a modernvehicle is either monitored or controlled electronically.One such specic demand for AB Volvo arose out of construction trucks in the US market. Usersseldom have/had a view of the operational boundaries of the drivetrain components, resultingin inappropriate use causing damage, poor traction and steering performance. Also, AB Volvo'sstand-alone traction assistance functions were not suciently capable to handle the vehicle useconditions. Hence, the goal was set to automate and synchronize the traction assistance devicesand software functions to improve the traction and steerability under a variety of road conditions.The rst steps in this thesis involved understanding the drivetrain components from design andoperational boundary perspective. The function descriptions of the various traction softwarefunctions were reviewed and a development/integration plan drafted. A literature survey wascarried out seeking potential improvement in traction from dierential locking and also its eectson steerability. A benchmarking exercise was carried out to identify competitor and suppliertechnologies available for the traction device automation task.The focus was then shifted to developing and validating the traction controller in a simulationenvironment. Importance was given to modeling of drivetrain components and renement ofvehicle behavior to study and understand the eects of dierential locking and develop a differentiallock control strategy. The modeling also included creating dierent road segments toreplicate use environment and simulating vehicle performance in the same, to reduce test timeand costs. With well-correlated vehicle performance results, a dierential lock control strategywas developed and simulated to observe traction improvement. It was then implemented onan all-wheel drive construction truck using dSPACE Autobox to test, validate and rene thecontroller.Periodic test sessions carried out at Hallered proving ground, Sweden were important to re-ne the control strategy. Feedback from test drivers and inputs from cross-functional teamswere essential to develop a robust controller and the same was tested for vehicle suitability andrepeatability of results. When comparing with the existing traction software functions, the integrateddierential lock and transfer case lock controller showed signicantly better performanceunder most test conditions. Repeatable results proved the reliability of developed controller.The correlation between vehicle test scenarios and simulation environment results indicated theaccuracy of software models and control strategy, bi-directionally.Finally, the new traction assistance device controller function was demonstrated within ABVolvo to showcase the traction improvement and uncompromising steerability.</p>

corrected abstract:
<p>Automotive development has always been need-based and the product of today is an evolution over several decades and a diversified technology application to deliver better products to the end users. Steady increase in the deployment of on-board electronics and software is characterized by the demand and stringent regulations. Today, almost every function on-board a modern vehicle is either monitored or controlled electronically.</p><p>One such specific demand for AB Volvo arose out of construction trucks in the US market. Users seldom have/had a view of the operational boundaries of the drivetrain components, resulting in inappropriate use causing damage, poor traction and steering performance. Also, AB Volvo's stand-alone traction assistance functions were not sufficiently capable to handle the vehicle use conditions. Hence, the goal was set to automate and synchronize the traction assistance devices and software functions to improve the traction and steerability under a variety of road conditions.</p><p>The first steps in this thesis involved understanding the drivetrain components from design and operational boundary perspective. The function descriptions of the various traction software functions were reviewed and a development/integration plan drafted. A literature survey was carried out seeking potential improvement in traction from differential locking and also its effects on steerability. A benchmarking exercise was carried out to identify competitor and supplier technologies available for the traction device automation task.</p><p>The focus was then shifted to developing and validating the traction controller in a simulation environment. Importance was given to modeling of drivetrain components and refinement of vehicle behavior to study and understand the effects of differential locking and develop a differential lock control strategy. The modeling also included creating different road segments to replicate use environment and simulating vehicle performance in the same, to reduce test time and costs. With well-correlated vehicle performance results, a differential lock control strategy was developed and simulated to observe traction improvement. It was then implemented on an all-wheel drive construction truck using dSPACE Autobox to test, validate and refine the controller.</p><p>Periodic test sessions carried out at Hällered proving ground, Sweden were important to refine the control strategy. Feedback from test drivers and inputs from cross-functional teams were essential to develop a robust controller and the same was tested for vehicle suitability and repeatability of results. When comparing with the existing traction software functions, the integrated differential lock and transfer case lock controller showed significantly better performance under most test conditions. Repeatable results proved the reliability of developed controller. The correlation between vehicle test scenarios and simulation environment results indicated the accuracy of software models and control strategy, bi-directionally.</p><p>Finally, the new traction assistance device controller function was demonstrated within AB Volvo to showcase the traction improvement and uncompromising steerability.</p>
----------------------------------------------------------------------
In diva2:492776 abstract is:
<p>In recent years the interest for smaller, cheaper and more energy efficient vehicles hasincreased significantly. These vehicles are intended to be used in urban areas, where theactual need of large heavy cars is generally minor. The travelled distance is on average lessthan 56km during a day and most often there is only one person travelling in the vehicle. Manyof the established car manufacturers have recently started to take interest into this marketsegment, but the majority of these small vehicles are still manufactured by smaller companiesat a low cost and with little or no research done on vehicle traffic safety. This may be becausethere are still no legal requirements on crash testing of this type of vehicles.This report will examine road safety for Urban Light-weight Vehicle (ULV) to find criticalcrash scenarios from which future crash testing methods for urban vehicles can be derived.The term ULV is specific to this report and is the title for all engine powered three- and fourwheeledvehicles categorized by the European Commission. Other attributes than the wheelgeometry is engine power and the vehicles unladen mass. The maximum allowed weight for athree-wheeled ULV is 1 000kg and 400kg for a four-wheeled one.By studying current crash test methods used in Europe by Euro NCAP it has beenconcluded that these tests are a good way of assessing car safety. For light-weight urbanvehicles it has been concluded that some of these tests need to be changed and that some newtest scenarios should be added when assessing road safety. The main reasons for this is linkedto that vehicle’s with a weight difference of more than 150kg cannot be compared withcurrent test methods, and that crash tests are performed with crash objects with similar orequal mass in current safety assessment programs. This correlates poorly to the trafficsituation for light-weight urban vehicles since it would most likely collide with a far heaviervehicle than itself in an accident event.To verify the actual traffic situation in urban areas, accident statistics have beenexamined closely. The research has shown that there are large differences between rural andurban areas. For instance; 66% of all severe and fatal traffic accident occurs in rural areaseven though they are less populated. Even the distribution of accident categories has showndifferent in rural and urban areas. The United Nations Economic Commission for Europe(UNECE) has defined accident categories in their database which is widely used within theEuropean Union. By comparing each accident category’s occurrence, injury and fatality rate,the most critical urban accident categories were found in the following order.</p>
<p>1. Collision due to crossing or turning</p>
<p>2. Vehicle and pedestrian collision</p>
<p>3. Rear-end collision</p>
<p>4. Single-vehicle accident</p>
<p>5. Other collisions</p>
<p>6. Head-on collision</p>
<p>Statistics also show that of all fatally injured crash victims in urban trafficapproximately; one third is travelling by car; one third by motorcycle, moped or pedal-cycle;and one third are pedestrians. This means that unprotected road travelers correspond to twothirds of all fatal urban traffic accidents, a fact that has to be taken into account in future crashtesting of urban vehicles. With all the information gathered a total of four new crash testscenarios for light-weight urban vehicles have been presented:</p>
<p>• Vehicle-to-vehicle side impact at 40km/h with a 1 300kg striking vehicle to evaluate theoccupant protection level of the light-weight vehicle.</p>
<p>• Vehicle-to-motorcycle side impact at 40km/h with motorcycle rider protection evaluation.</p>
<p>• Pedestrian protection assessment at 40km/h over the whole vehicle front and roof area.</p>
<p>• Rigid barrier impact at 40km/h corresponding to an urban single vehicle accident with aroad side object or a collision with a heavier or similar sized vehicle.</p>

corrected abstract:
<p>In recent years the interest for smaller, cheaper and more energy efficient vehicles has increased significantly. These vehicles are intended to be used in urban areas, where the actual need of large heavy cars is generally minor. The travelled distance is on average less than <em>56km</em> during a day and most often there is only one person travelling in the vehicle. Many of the established car manufacturers have recently started to take interest into this market segment, but the majority of these small vehicles are still manufactured by smaller companies at a low cost and with little or no research done on vehicle traffic safety. This may be because there are still no legal requirements on crash testing of this type of vehicles.</p><p>This report will examine road safety for <em>Urban Light-weight Vehicle (ULV)</em> to find critical crash scenarios from which future crash testing methods for urban vehicles can be derived. The term <em>ULV</em> is specific to this report and is the title for all engine powered three- and four-wheeled vehicles categorized by the <em>European Commission</em>. Other attributes than the wheel geometry is engine power and the vehicles unladen mass. The maximum allowed weight for a three-wheeled <em>ULV</em> is <em>1 000kg</em> and <em>400kg</em> for a four-wheeled one.</p><p>By studying current crash test methods used in Europe by <em>Euro NCAP</em> it has been concluded that these tests are a good way of assessing car safety. For light-weight urban vehicles it has been concluded that some of these tests need to be changed and that some new test scenarios should be added when assessing road safety. The main reasons for this is linked to that vehicle’s with a weight difference of more than <em>150kg</em> cannot be compared with current test methods, and that crash tests are performed with crash objects with similar or equal mass in current safety assessment programs. This correlates poorly to the traffic situation for light-weight urban vehicles since it would most likely collide with a far heavier vehicle than itself in an accident event.</p><p>To verify the actual traffic situation in urban areas, accident statistics have been examined closely. The research has shown that there are large differences between rural and urban areas. For instance; 66% of all severe and fatal traffic accident occurs in rural areas even though they are less populated. Even the distribution of accident categories has shown different in rural and urban areas. The <em>United Nations Economic Commission for Europe (UNECE)</em> has defined accident categories in their database which is widely used within the <em>European Union</em>. By comparing each accident category’s occurrence, injury and fatality rate, the most critical urban accident categories were found in the following order.</p>
<em><ol>
<li>Collision due to crossing or turning</li>
<li>Vehicle and pedestrian collision</li>
<li>Rear-end collision</li>
<li>Single-vehicle accident</li>
<li>Other collisions</li>
<li>Head-on collision</li>
</ol></em>
<p>Statistics also show that of all fatally injured crash victims in urban traffic approximately; one third is travelling by <em>car</em>; one third by <em>motorcycle</em>, moped or pedal-cycle; and one third are <em>pedestrians</em>. This means that unprotected road travelers correspond to two thirds of all fatal urban traffic accidents, a fact that has to be taken into account in future crash testing of urban vehicles. With all the information gathered a total of four new crash test scenarios for light-weight urban vehicles have been presented:</p>
<em><ul>
<li>Vehicle-to-vehicle side impact at 40km/h with a 1 300kg striking vehicle to evaluate the occupant protection level of the light-weight vehicle.</li>
<li>Vehicle-to-motorcycle side impact at 40km/h with motorcycle rider protection evaluation.</li>
<li>Pedestrian protection assessment at 40km/h over the whole vehicle front and roof area.</li>
<li>Rigid barrier impact at 40km/h corresponding to an urban single vehicle accident with a road side object or a collision with a heavier or similar sized vehicle.</li>
</ul><em>
----------------------------------------------------------------------
In diva2:895396 abstract is:
<p>A lattice physics code is a vital tool, forming a base of reactor coreanalysis. It enables the neutronic properties of the fuel assembly to becalculated and generates a proper set of data to be used by a 3-D full coresimulator. Due to advancement and complexity of modern Boiling WaterReactor assembly designs, a new deterministic lattice physics codeis being developed at Westinghouse Sweden AB, namely PHOENIX5.Each time a new code is written, its methodology of solving the neutrontransport equation, has to be validated to make sure it providesreliable output. In a wake of preparation for PHOENIX5 release andconsecutive validation efforts, a set of reference Monte Carlo calculationswas prepared, using the code Serpent. A depletion calculation with achosen type of branch cases was conducted. Methods implemented inPHOENIX5 are based on the Current Coupling Collision Probabilitymethod used in older versions of the code HELIOS. Therefore, a comparisonbetween reference Monte Carlo simulations and HELIOS 1.8.1is made, in order to discover problems inherent to the said method ofsolving the neutron transport equation. A special care should be givenduring PHOENIX5 validation, to issues highlighted in this work.Discrepancies in results of Serpent and HELIOS are attributed mostlyto disparities in the basic nuclear data used by the codes, as well as arange of approximations and corrections adopted by the deterministiccode.Serpent and HELIOS showed a good agreement in a typical voidrange (up to 90 % void) and ‘less’ challenging branches (coolant void,fuel temperature and spacer grid branches). More significant discrepanciesappeared for extreme cases with a very high void and control rodpresence (k1 differences as high as 1000 pcm) and rather pronouncedconcentrations of the natural boron dissolved in coolant (absolute differencesroughly at a level of 900 pcm). The issues do not seem to stemsolely from discrepancies in the nuclear data libraries used by Serpentand HELIOS.Moreover, a coolant void bias was consistently found in the resultsof branch calculation at changing coolant void. This confirms the analogousphenomenon found in previous studies of the CCCP based deterministiccodes. It most probably stems from the assumptions used bythe method while tackling the neutron transport equation, such as theflat source approximation, the isotropic scattering assumption and thetransport correction. An alternative transport correction approximationis proposed to alleviate this issue.</p>

corrected abstract:
<p>A lattice physics code is a vital tool, forming a base of reactor core analysis. It enables the neutronic properties of the fuel assembly to be calculated and generates a proper set of data to be used by a 3-D full core simulator. Due to advancement and complexity of modern Boiling Water Reactor assembly designs, a new deterministic lattice physics code is being developed at Westinghouse Sweden AB, namely PHOENIX5. Each time a new code is written, its methodology of solving the neutron transport equation, has to be validated to make sure it provides reliable output. In a wake of preparation for PHOENIX5 release and consecutive validation efforts, a set of reference Monte Carlo calculations was prepared, using the code Serpent. A depletion calculation with a chosen type of branch cases was conducted. Methods implemented in PHOENIX5 are based on the Current Coupling Collision Probability method used in older versions of the code HELIOS. Therefore, a comparison between reference Monte Carlo simulations and HELIOS 1.8.1 is made, in order to discover problems inherent to the said method of solving the neutron transport equation. A special care should be given during PHOENIX5 validation, to issues highlighted in this work.</p><p>Discrepancies in results of Serpent and HELIOS are attributed mostly to disparities in the basic nuclear data used by the codes, as well as a range of approximations and corrections adopted by the deterministic code.</p><p>Serpent and HELIOS showed a good agreement in a typical void range (up to 90 % void) and ‘less’ challenging branches (coolant void, fuel temperature and spacer grid branches). More significant discrepancies appeared for extreme cases with a very high void and control rod presence (k<sub>&infin;</sub> differences as high as 1000 pcm) and rather pronounced concentrations of the natural boron dissolved in coolant (absolute differences roughly at a level of 900 pcm). The issues do not seem to stem solely from discrepancies in the nuclear data libraries used by Serpent and HELIOS.</p><p>Moreover, a coolant void bias was consistently found in the results of branch calculation at changing coolant void. This confirms the analogous phenomenon found in previous studies of the CCCP based deterministic codes. It most probably stems from the assumptions used by the method while tackling the neutron transport equation, such as the flat source approximation, the isotropic scattering assumption and the transport correction. An alternative transport correction approximation is proposed to alleviate this issue.</p>
----------------------------------------------------------------------
In diva2:1740195 abstract is:
<p>Carbon fibers submitted to high temperatures (&gt;2000 °C) experience a permanent increasein their thermal conductivity. This change has been attributed to a change in the molecularstructure due to graphitisation. Graphitisation occurs when amorphous carbons are exposed tohigh temperatures (&gt; 1000°C) for a prolonged period of time and describes the process in whichcarbon atoms are rearranged from their amorphous form into structured hexagonal ringed latticesheets. To characterise the extent of this process, one needs to determine certain ring statisticswhich provide information on the bonding structure. In this work, we develop and verify a ringstatistics tool that can be used to analyze the resulting structure of atomistic simulations, and useit in a novel approach to characterise the extent of graphitisation in Molecular Dynamics (MD)simulations of carbon. Different ring definitions, such as Franzblau, Leroux, Hybrid and King arecompared to determine the most appropriate definition for the investigation of carbon structures.A new ring definition, Hybrid, is introduced as an extension of Leroux’s definition, exploiting theefficiency of Leroux’s definition while making the definition more appropriate for carbon systemsby removing shortcuts of length 1. It was found that Franzblau rings most accurately capturecarbon structures, and are most optimal for the investigation of amorphous and graphitisedcarbons. We then apply this tool to two MD simulations of amorphous carbons undergoing anannealing process at 4000K for 300 ps to characterise the extent of graphitisation. We found aprevalence of ∼0.1 hexagonal rings per atom in amorphous carbons prior to annealing, comparedto ∼0.33 hexagonal rings per atom in graphitised carbon after annealing. The likelihood of a ringbeing hexagonal in amorphous carbon was ∼30%, as opposed to ∼75% in graphitised samples.Calculating the ratio in the number of hexagonal rings per atom to the number of hexagonalrings per atom in a fully graphitised system, the extent of graphitisation can be quantified. Sincethis value is normalized by the number of atoms in the simulation this method can be appliedto any domain size. This successful application of the ring statistics tool opens the door toapply it to more realistic and complex systems. The tool has already been expanded to considermulti-component systems and molecule identification. Hence, the tool could already be appliedto more complex cases, such as doped or contaminated systems, investigating the effects on bondstructure. In its current state, the tool could also be used to investigate how the extent andrate of graphitisation changes at different depths in a system. Potentially characterising therate at which graphitisation penetrates a system under various conditions. The tool also hasthe potential to be expanded to consider localisation and identification of defects, bond angles,bond creation and destruction and the structural classification and identification of systems.Combining this tool with MDSuite, a software in development by the Institute for ComputationalPhysics (ICP) at the University of Stuttgart with the collaboration of the von Karman Institutefor Fluid Dynamics (VKI) to analyse MD trajectories, could offer a package that can providedeep system information for minimal cost.</p>

corrected abstract:
<p>Carbon fibers submitted to high temperatures (&gt;2000 °C) experience a permanent increase in their thermal conductivity. This change has been attributed to a change in the molecular structure due to graphitisation. Graphitisation occurs when amorphous carbons are exposed to high temperatures (&gt; 1000°C) for a prolonged period of time and describes the process in which carbon atoms are rearranged from their amorphous form into structured hexagonal ringed lattice sheets. To characterise the extent of this process, one needs to determine certain ring statistics which provide information on the bonding structure. In this work, we develop and verify a ring statistics tool that can be used to analyze the resulting structure of atomistic simulations, and use it in a novel approach to characterise the extent of graphitisation in Molecular Dynamics (MD) simulations of carbon. Different ring definitions, such as Franzblau, Leroux, Hybrid and King are compared to determine the most appropriate definition for the investigation of carbon structures. A new ring definition, Hybrid, is introduced as an extension of Leroux’s definition, exploiting the efficiency of Leroux’s definition while making the definition more appropriate for carbon systems by removing shortcuts of length 1. It was found that Franzblau rings most accurately capture carbon structures, and are most optimal for the investigation of amorphous and graphitised carbons. We then apply this tool to two MD simulations of amorphous carbons undergoing an annealing process at 4000K for 300 ps to characterise the extent of graphitisation. We found a prevalence of ∼0.1 hexagonal rings per atom in amorphous carbons prior to annealing, compared to ∼0.33 hexagonal rings per atom in graphitised carbon after annealing. The likelihood of a ring being hexagonal in amorphous carbon was ∼30%, as opposed to ∼75% in graphitised samples. Calculating the ratio in the number of hexagonal rings per atom to the number of hexagonal rings per atom in a fully graphitised system, the extent of graphitisation can be quantified. Since this value is normalized by the number of atoms in the simulation this method can be applied to any domain size. This successful application of the ring statistics tool opens the door to apply it to more realistic and complex systems. The tool has already been expanded to consider multi-component systems and molecule identification. Hence, the tool could already be applied to more complex cases, such as doped or contaminated systems, investigating the effects on bond structure. In its current state, the tool could also be used to investigate how the extent and rate of graphitisation changes at different depths in a system. Potentially characterising the rate at which graphitisation penetrates a system under various conditions. The tool also has the potential to be expanded to consider localisation and identification of defects, bond angles, bond creation and destruction and the structural classification and identification of systems. Combining this tool with MDSuite, a software in development by the Institute for Computational Physics (ICP) at the University of Stuttgart with the collaboration of the von Karman Institute for Fluid Dynamics (VKI) to analyse MD trajectories, could offer a package that can provide deep system information for minimal cost.</p>
----------------------------------------------------------------------
In diva2:1078086 abstract is:
<p>There is a renewed interest in the wind estimate over and around forest areas dueto the increasing demand of wind-energy resources. Many researches have been donewith simplied forest models. However, the introduction of a simple two-dimensionalclearing adds further parameters, such as the width of the clearing, which furthercomplicates the analysis. The main purpose of the present experimental and nu-merical efforts is, therefore, to characterize the ow over the forest clearing and tosuggest the suitable location for the wind-power generation over the forest clearing.The experiments were performed in the Minimum Turbulence Level (MTL) windtunnel at KTH in Stockholm, and PIV data evaluation and analysis were carried out.The canopy model consists of several wooden at plates, and to each of the plateswooden cylindrical pins were clamped in a staggered layout to mimic a homogeneoushigh-density forest. The total length of the forest model is 40hc, where hc indicatesthe canopy height. Two cases were experimentally investigated, one with a fullforest conguration and the other with the presence of a clearing that starts fromx=hc = 20 and ends at x=hc = 30, where x is a streamwise coordinate that startsat the forest windward edge. Particle Image Velocimetry (PIV) was performed, andplanar velocity snapshots were taken at the downwind edge of the clearing.Large Eddy Simulations (LES) were also conducted to complement the experi-mental information. The present LES code was developed by modifying the DirectNumerical Simulation (DNS) code of the turbulent boundary layer ow by Kametani&amp; Fukagata (2011), by adding the subgrid scale model part and the empirical canopymodel part into the DNS code.Both the experimental and the numerical results indicate that the clearing is as-sociated to a streamwise velocity defect in the mean prole mainly due to the strongturbulent diffusion into the clearing region. The turbulence is redistributed amongstthe various velocity components so that the streamwise velocity variance is reduced,while the vertical velocity variance is enhanced. The streamwise velocity varianceis in fact damped due to the absence of the canopy drag from x=hc = 20, whileenhanced vertical-velocity uctuations can be observed at the end of the clearing.However, the effects are immediately weakened both by a ow re-acceleration andby a new surface layer development right after passing the downwind clearing edge.The clearing effect seems to be dominant in the roughness sublayer at least for theneutral atmospheric conditions. The clearing perturbation seems to be associatedto turbulent mixing at its initial stage near y hc, followed by a rapid distorsionnear the clearing trailing edge. This phenomenon is highlighted by the low valueof the vertical correlation length scale that, after the clearing trailing edge, risesagain towards to homogenous forest condition. The LES results further show thata suitable area for the wind-turbine operation is close to the upwind clearing edgewhere the energy contents is the highest, while the turbulent intensity is lowestbetween the clearing. They also indicate that wind-speed enhancement can be ex-pected downstream of the short forest edge, implying that the ow can be optimizedfor wind-power generation just by changing the forest conguration.</p>

corrected abstract:
<p>There is a renewed interest in the wind estimate over and around forest areas due to the increasing demand of wind-energy resources. Many researches have been done with simplified forest models. However, the introduction of a simple two-dimensional clearing adds further parameters, such as the width of the clearing, which further complicates the analysis. The main purpose of the present experimental and numerical efforts is, therefore, to characterize the flow over the forest clearing and to suggest the suitable location for the wind-power generation over the forest clearing.</p><p>The experiments were performed in the Minimum Turbulence Level (MTL) wind tunnel at KTH in Stockholm, and PIV data evaluation and analysis were carried out. The canopy model consists of several wooden flat plates, and to each of the plates wooden cylindrical pins were clamped in a staggered layout to mimic a homogeneous high-density forest. The total length of the forest model is 40hc, where hc indicates the canopy height. Two cases were experimentally investigated, one with a full forest configuration and the other with the presence of a clearing that starts from <em>x/h<sub>c<sub> = 20</em> and ends at <em>x/h<sub>c</sub> = 30</em>, where <em>x</em> is a streamwise coordinate that starts at the forest windward edge. Particle Image Velocimetry (PIV) was performed, and planar velocity snapshots were taken at the downwind edge of the clearing.</p><p>Large Eddy Simulations (LES) were also conducted to complement the experimental information. The present LES code was developed by modifying the Direct Numerical Simulation (DNS) code of the turbulent boundary layer flow by Kametani &amp; Fukagata (2011), by adding the subgrid scale model part and the empirical canopy model part into the DNS code.</p><p>Both the experimental and the numerical results indicate that the clearing is associated to a streamwise velocity defect in the mean profile mainly due to the strong turbulent diffusion into the clearing region. The turbulence is redistributed amongst the various velocity components so that the streamwise velocity variance is reduced, while the vertical velocity variance is enhanced. The streamwise velocity variance is in fact damped due to the absence of the canopy drag from <em>x/h<sub>c</sub> = 20</em>, while enhanced vertical-velocity fluctuations can be observed at the end of the clearing. However, the effects are immediately weakened both by a flow re-acceleration and by a new surface layer development right after passing the downwind clearing edge. The clearing effect seems to be dominant in the roughness sublayer at least for the neutral atmospheric conditions. The clearing perturbation seems to be associated to turbulent mixing at its initial stage near <em>y ≈ h<sub>c</sub></em>, followed by a rapid distorsion near the clearing trailing edge. This phenomenon is highlighted by the low value of the vertical correlation length scale that, after the clearing trailing edge, rises again towards to homogenous forest condition. The LES results further show that a suitable area for the wind-turbine operation is close to the upwind clearing edge where the energy contents is the highest, while the turbulent intensity is lowest between the clearing. They also indicate that wind-speed enhancement can be expected downstream of the short forest edge, implying that the flow can be optimized for wind-power generation just by changing the forest configuration.</p>
----------------------------------------------------------------------
In diva2:1247161 abstract is:
<p>Nowadays, increasing pressure from legislation and customer demands in the automotive industryare forcing manufacturers to produce greener vehicles with lower emissions and fuel consumption.As a result, electrified and hybrid vehicles are a growing popular alternative to traditional internalcombustion engines (ICE). The noise from an electric vehicle comes mainly from contact betweentyres and road, wind resistance and driveline. The noise emitted from the driveline is for the mostpart related to the gearbox. When developing a driveline, it is a factor of importance to estimatethe noise radiating from the gearbox to achieve an acceptable design.Gears are used extensively in the driveline of electric vehicles. As the gears are in mesh, a mainintrusive concern is known as gear whine noise. Gear whine noise is an undesired vibroacousticphenomenon and is likely to originate through the gear contacts and be transferred through themechanical components to the housing where the vibrations are converted into airborne andstructure-borne noise. The gear whine noise originates primarily from the excitation coming fromtransmission error (TE). Transmission error is defined as the difference between the ideal smoothtransfer of motion of a gear and what is in practice due to lack of smoothness.The main objective of this study is to simulate the vibrations generated by the gear whine noise inan electric powertrain line developed by AVL Vicura. The electric transmission used in this studyprovides only a fixed overall gear ratio, i.e. 9.59, under all operation conditions. It is assumed thatthe system is excited only by the transmission error and the mesh stiffness of the gear contacts. Inorder to perform NVH analysis under different operating conditions, a multibody dynamics modelaccording to the AVL Excite program has been developed. The dynamic simulations are thencompared with previous experimental measurements provided by AVL Vicura.Two validation criteria have been used to analyse the dynamic behaviour of the AVL Excite model:signal processing using the FFT method and comparison with the experimental measurements.The results from the AVL Excite model show that the FFT criterion is quite successful and allexcitation frequencies are properly observed in FFT plots. Nevertheless, when it comes to thesecond criterion, as long as not all dynamic parameters of the system such as damping or stiffnesscoefficients are provided with certainty in the model, it is too difficult to investigate the accuracy ofthe AVL Excite model.Another investigation is a numerical design study to analyses how the damping coefficientsinfluence the response. After reducing the damping parameters, the results show that the housingand bearings have the highest influence on the response. If more acceptable results are desired,future studies must be concentrated on these to obtain more acceptable damping values.</p>


corrected abstract:
<p>Nowadays, increasing pressure from legislation and customer demands in the automotive industry are forcing manufacturers to produce greener vehicles with lower emissions and fuel consumption. As a result, electrified and hybrid vehicles are a growing popular alternative to traditional internal combustion engines (ICE). The noise from an electric vehicle comes mainly from contact between tyres and road, wind resistance and driveline. The noise emitted from the driveline is for the most part related to the gearbox. When developing a driveline, it is a factor of importance to estimate the noise radiating from the gearbox to achieve an acceptable design.</p><p>Gears are used extensively in the driveline of electric vehicles. As the gears are in mesh, a main intrusive concern is known as gear whine noise. Gear whine noise is an undesired vibroacoustic phenomenon and is likely to originate through the gear contacts and be transferred through the mechanical components to the housing where the vibrations are converted into airborne and structure-borne noise. The gear whine noise originates primarily from the excitation coming from transmission error (TE). Transmission error is defined as the difference between the ideal smooth transfer of motion of a gear and what is in practice due to lack of smoothness.</p><p>The main objective of this study is to simulate the vibrations generated by the gear whine noise in an electric powertrain line developed by AVL Vicura. The electric transmission used in this study provides only a fixed overall gear ratio, i.e. 9.59, under all operation conditions. It is assumed that the system is excited only by the transmission error and the mesh stiffness of the gear contacts. In order to perform NVH analysis under different operating conditions, a multibody dynamics model according to the AVL Excite program has been developed. The dynamic simulations are then compared with previous experimental measurements provided by AVL Vicura.</p><p>Two validation criteria have been used to analyse the dynamic behaviour of the AVL Excite model: signal processing using the FFT method and comparison with the experimental measurements. The results from the AVL Excite model show that the FFT criterion is quite successful and all excitation frequencies are properly observed in FFT plots. Nevertheless, when it comes to the second criterion, as long as not all dynamic parameters of the system such as damping or stiffness coefficients are provided with certainty in the model, it is too difficult to investigate the accuracy of the AVL Excite model.</p><p>Another investigation is a numerical design study to analyses how the damping coefficients influence the response. After reducing the damping parameters, the results show that the housing and bearings have the highest influence on the response. If more acceptable results are desired, future studies must be concentrated on these to obtain more acceptable damping values.</p>
----------------------------------------------------------------------
In diva2:1078069 - missing space in title:
"Partially Premixed Combustion (PPC) for low loadconditions in marine engines using computationaland experimental techniques"
==>
"Partially Premixed Combustion (PPC) for low loadconditions in marine engines using computational and experimental techniques"

abstract is:
<p>Diesel Engine has been the most powerful and relevant source of power in the automobile industryfor decades due to their excellent performance, efficiency and power. On the contrary, there arenumerous environmental issues of the diesel engines hampering the environment. It has been agreat challenge for the researchers and scientists to minimize these issues. In the recent years, severalstrategies have been introduced to eradicate the emissions of the diesel engines. Among them,Partially Premixed Combustion (PPC) is one of the most emerging and reliable strategies. PPC is acompression ignited combustion process in which ignition delay is controlled. PPC is intended toendow with better combustion with low soot and NOx emission.The engine used in the present study is a single-cylinder research engine, installed in Aalto UniversityInternal Combustion Engine Laboratory with the bore diameter of 200 mm. The thesis presentsthe validation of the measurement data with the simulated cases followed by the study of the sprayimpingement and fuel vapor mixing in PPC mode for different injection timing. A detailed study ofthe correlation of early injection with the fuel vapor distribution and wall impingement has beenmade.The simulations are carried out with the commercial CFD software STAR CD. Different injectionparameters have been considered and taken into an account to lower the wall impingement and toproduce better air-fuel mixing with the purpose of good combustion and reduction of the emissions.The result of the penetration length of the spray and the fuel vapor distribution for different earlyinjection cases have been illustrated in the study. Comparisons of different thermodynamic propertiesand spray analysis for different injection timing have been very clearly illustrated to get insightof effect of early injection. The parameters like injection timing, injection period, injection pressure,inclusion angle of the spray have an influence the combustion process in PPC mode. Extensivestudy has been made for each of these parameters to better understand their effects in the combustionprocess. Different split injection profiles have been implemented for the study of better fuelvapor distribution in the combustion chamber.The final part of the thesis includes the study of the combustion and implementation of EGR tocontrol the temperature so as to get more prolonged ignition delay to accompany the PPC strategyfor standard piston top and deep bowl piston top. With the injection optimization and implementationof EGR, NOx has been reduced by around 44%, CO by 60% and Soot by 66% in the standardpiston top. The piston optimization resulted in more promising result with 58% reduction in NOx,55% reduction in CO and 67% reduction in Soot. In both cases the percentage of fuel burnt wasincreased by around 8%.</p>



corrected abstract:
<p>Diesel Engine has been the most powerful and relevant source of power in the automobile industry for decades due to their excellent performance, efficiency and power. On the contrary, there are numerous environmental issues of the diesel engines hampering the environment. It has been a great challenge for the researchers and scientists to minimize these issues. In the recent years, several strategies have been introduced to eradicate the emissions of the diesel engines. Among them, Partially Premixed Combustion (PPC) is one of the most emerging and reliable strategies. PPC is a compression ignited combustion process in which ignition delay is controlled. PPC is intended to endow with better combustion with low soot and NOx emission.</p><p>The engine used in the present study is a single-cylinder research engine, installed in Aalto University Internal Combustion Engine Laboratory with the bore diameter of 200 mm. The thesis presents the validation of the measurement data with the simulated cases followed by the study of the spray impingement and fuel vapor mixing in PPC mode for different injection timing. A detailed study of the correlation of early injection with the fuel vapor distribution and wall impingement has been made.</p><p>The simulations are carried out with the commercial CFD software STAR CD. Different injection parameters have been considered and taken into an account to lower the wall impingement and to produce better air-fuel mixing with the purpose of good combustion and reduction of the emissions. The result of the penetration length of the spray and the fuel vapor distribution for different early injection cases have been illustrated in the study. Comparisons of different thermodynamic properties and spray analysis for different injection timing have been very clearly illustrated to get insight of effect of early injection. The parameters like injection timing, injection period, injection pressure, inclusion angle of the spray have an influence the combustion process in PPC mode. Extensive study has been made for each of these parameters to better understand their effects in the combustion process. Different split injection profiles have been implemented for the study of better fuel vapor distribution in the combustion chamber.</p><p>The final part of the thesis includes the study of the combustion and implementation of EGR to control the temperature so as to get more prolonged ignition delay to accompany the PPC strategy for standard piston top and deep bowl piston top. With the injection optimization and implementation of EGR, NOx has been reduced by around 44%, CO by 60% and Soot by 66% in the standard piston top. The piston optimization resulted in more promising result with 58% reduction in NOx, 55% reduction in CO and 67% reduction in Soot. In both cases the percentage of fuel burnt was increased by around 8%.</p>
----------------------------------------------------------------------
In diva2:1816751 abstract is:
<p>This thesis investigates reasons for signiﬁcant uncertainties in added wave resistance predictionsand how wave conditions can potentially aﬀect the design of RoPax ferries. The objectiveis to ﬁnd a suitable prediction method of added wave resistance for the RoPax ferry designapplication. Furthermore, the wave environment on the route strongly inﬂuences this delicateand complex phenomenon. Thus, the emphasis is to understand the added wave resistancethrough a case study with a probabilistic wave environment.The fast transition into decarbonization and regulations regarding energy-eﬃcient ship designhave ramped up the awareness of the inﬂuence of seaways. For lower speeds, the addedresistance becomes a more signiﬁcant part of total resistance, with concerns regarding minimumpropulsion power and safe maneuvering in adverse sea conditions. Consequently, the demandhas rocketed for profound insight and accurate prediction methods of added wave resistance. Inaddition, with new larger ships, added wave resistance domain for short waves becomes essentialand an additional challenge regarding predictions. Nevertheless, added wave resistancepredictions are complex and contain many pitfalls, so accurate estimations of the ship’s addedwave resistance response and wave environment are crucial. In addition, added wave resistanceis very ship-speciﬁc, and published research for RoPax ferries is rare.Due to signiﬁcant uncertainties for general numerical methods, the study investigates a new(modiﬁed NTUA) semi-empirical method reﬁned for ships with a large beam-to-draft ratio.In addition, a realistic wave environment is included by selecting relevant wave spectra forconditions on the route.The study shows that signiﬁcant variances of added wave resistance predictions arise fromselecting prediction methods beyond the range of applicability and rough assumptions of waveconditions and spectra. The case study discovered that errors might also be introduced bythe classiﬁcation society deﬁnitions, which gives reasons to rethink the applied deﬁnition of"average BF 8" wave conditions for Safe Return to Port (SRtP) assessments. This can causea false illusion of the ship’s performance and safety in waves. Only the misjudgment ofthe most critical peak period resulted in a rough underestimation (&gt; 40%) of mean addedwave resistance. The error corresponded to 215% of the still water resistance for the SRtPassessment. In addition, the nature of added wave resistance is very ship speciﬁc. Therefore, theauthor emphasizes caution when selecting the prediction method, especially for semi-empiricalmethods. Despite the ﬁrst promising glance of the applied semi-empirical method, it appearsthat the ship database correlates poorly for RoPax ferries. Reliability for the method is weak forshort waves, with a tendency to large overestimations. The lack of references of RoPax vesselsfor validations, accident statics in adverse sea conditions and recent insight into nonlinear eﬀectsrequest further research on added wave resistance for modern hull shapes.</p>

corrected abstract:
<p>This thesis investigates reasons for significant uncertainties in added wave resistance predictions and how wave conditions can potentially affect the design of RoPax ferries. The objective is to find a suitable prediction method of added wave resistance for the RoPax ferry design application. Furthermore, the wave environment on the route strongly influences this delicate and complex phenomenon. Thus, the emphasis is to understand the added wave resistance through a case study with a probabilistic wave environment.</p><p>The fast transition into decarbonization and regulations regarding energy-efficient ship design have ramped up the awareness of the influence of seaways. For lower speeds, the added resistance becomes a more significant part of total resistance, with concerns regarding minimum propulsion power and safe maneuvering in adverse sea conditions. Consequently, the demand has rocketed for profound insight and accurate prediction methods of added wave resistance. In addition, with new larger ships, added wave resistance domain for short waves becomes essential and an additional challenge regarding predictions. Nevertheless, added wave resistance predictions are complex and contain many pitfalls, so accurate estimations of the ship’s added wave resistance response and wave environment are crucial. In addition, added wave resistance is very ship-specific, and published research for RoPax ferries is rare.</p><p>Due to significant uncertainties for general numerical methods, the study investigates a new (modified NTUA) semi-empirical method refined for ships with a large beam-to-draft ratio. In addition, a realistic wave environment is included by selecting relevant wave spectra for conditions on the route.</p><p>The study shows that significant variances of added wave resistance predictions arise from selecting prediction methods beyond the range of applicability and rough assumptions of wave conditions and spectra. The case study discovered that errors might also be introduced by the classification society definitions, which gives reasons to rethink the applied definition of "average BF 8" wave conditions for Safe Return to Port (SRtP) assessments. This can cause a false illusion of the ship’s performance and safety in waves. Only the misjudgment of the most critical peak period resulted in a rough underestimation (&gt; 40%) of mean added wave resistance. The error corresponded to 215% of the still water resistance for the SRtP assessment. In addition, the nature of added wave resistance is very ship specific. Therefore, the author emphasizes caution when selecting the prediction method, especially for semi-empirical methods. Despite the first promising glance of the applied semi-empirical method, it appears that the ship database correlates poorly for RoPax ferries. Reliability for the method is weak for short waves, with a tendency to large overestimations. The lack of references of RoPax vessels for validations, accident statics in adverse sea conditions and recent insight into nonlinear effects request further research on added wave resistance for modern hull shapes.</p>
----------------------------------------------------------------------
In diva2:1465518 - missing space in title:
"Accelerated test for vehicle body durability basedon vehicle dynamics simulations using pseudo damage"
==>
"Accelerated test for vehicle body durability based on vehicle dynamics simulations using pseudo damage"

abstract is: <p>Vehicle body durability evaluation strongly relies on the possibility of testing areal prototype on different testing surfaces, such as proving grounds, test rigsand real roads. Although many efforts have been made to reduce time requiredfor testing, this still remains one of the main resource-demanding phases in avehicle development. As direct consequence, more and more companies aim tooptimise and to improve vehicle development by means of CAE tools.This master thesis is a step towards virtual testing of a vehicle body, aimingto investigate and to select the most important measurements for a bodydurability evaluation and to reproduce an invariant excitation that could beapplied to other vehicle models for new durability assessments without theneed of real measurements from other models. Moreover, a comparison betweenfrequency-based methods and time-based methods and their differences werehighlighted and the validity of ISO8608:2016 discussed.The method relied on small sets of simple and easy-to-get internal measurements,called desired signal, that allowed back-calculation of wheel hubdisplacements and other excitations by means of the product of the model’stransfer function and desired signal. Then, the iteration procedure allowedto drastically reduce the error between the desired signal and the computedone and it proved to be essential in the process. Thanks to this procedure,damage information of also the not-measured signals could be computed andtheir damage assessed and thus available for durability purposes. Moreover, thechance of applying the same measurements taken from a real vehicle to a modelof a newer generation was investigated. This would avoid the need of buildinga running prototype, allowing a more accurate durability assessment alreadyavailable in the pre-design phase.As a result, using a specific set of 8 measurements, other 22 other forces,displacements and velocities of several components were precisely reproduced,showing that not all measurement are equally valuable in a durability evaluation.A method for the measurement selection, called Observability Method, wasthen developed and compared with a set of measurements selected by meansof experience, showing a better convergence of the pseudo damage and similarpseudo damage values. Eventually a small set of measures from an older carwas applied to a newer version. It was proved that a detailed knowledge ofthe car model is needed, in order to successfully back-calculate the relevantmeasurements.</p>



corrected abstract:
<p>Vehicle body durability evaluation strongly relies on the possibility of testing a real prototype on different testing surfaces, such as proving grounds, test rigs and real roads. Although many efforts have been made to reduce time required for testing, this still remains one of the main resource-demanding phases in a vehicle development. As direct consequence, more and more companies aim to optimise and to improve vehicle development by means of CAE tools.</p><p>This master thesis is a step towards virtual testing of a vehicle body, aiming to investigate and to select the most important measurements for a body durability evaluation and to reproduce an invariant excitation that could be applied to other vehicle models for new durability assessments without the need of real measurements from other models. Moreover, a comparison between frequency-based methods and time-based methods and their differences were highlighted and the validity of ISO8608:2016 discussed.</p><p>The method relied on small sets of simple and easy-to-get internal measurements, called desired signal, that allowed back-calculation of wheel hub displacements and other excitations by means of the product of the model’s transfer function and desired signal. Then, the iteration procedure allowed to drastically reduce the error between the desired signal and the computed one and it proved to be essential in the process. Thanks to this procedure, damage information of also the not-measured signals could be computed and their damage assessed and thus available for durability purposes. Moreover, the chance of applying the same measurements taken from a real vehicle to a model of a newer generation was investigated. This would avoid the need of building a running prototype, allowing a more accurate durability assessment already available in the pre-design phase.</p><p>As a result, using a specific set of 8 measurements, other 22 other forces, displacements and velocities of several components were precisely reproduced, showing that not all measurement are equally valuable in a durability evaluation. A method for the measurement selection, called Observability Method, was then developed and compared with a set of measurements selected by means of experience, showing a better convergence of the pseudo damage and similar pseudo damage values. Eventually a small set of measures from an older car was applied to a newer version. It was proved that a detailed knowledge of the car model is needed, in order to successfully back-calculate the relevant measurements.</p>
----------------------------------------------------------------------
In diva2:915628 abstract is:
<p>Resistance spot welding (RSW) is the dominant joining technology in the automotive industry. This is due to its low costs and high efficiency. Other advantages with RSW is the high ability for automation,low consumption of energy, lack of need for added materials and low degree of pollution,no expensive equipment or education of personal compared to arc welding and laser welding. A modern automobile contains approximately 4000 resistance spot welds,which is why it is of great interest to be ableto predict the properties of the resistance spotwelds. The most important measurement used to ensure the quality of the weld is thenugget size, as it correlates to the weldsmechanical strength. This is usually measured by destructive testing, and the most common method is the coach peel test. Thistest is performed by manually peeling the specimen and then measuring the largest and smallest nugget diameter. It is also possibleto perform non-destructive testing on resistance spot welds with both ultrasonic and x-ray tests, however none of these methods have the same accuracy as the destructive methods and they are cumbersome to use in large-scale. Toimprove the efficiency and lower the cost forthe optimization of the welding parameters,simulation tools have been developed. Thereare both 2D- and 3D-simulation software tomodel the RSW process. When the spotwelds are simulated with 2D or 2D axis symmetry,the number of elements is lowercompared to a full 3D model, which reducesthe computation times. The disadvantageswith the 2D model are the inabilities tomodel misalignments or other asymmetricalgeometries. In contrast, the 3D-simulationsare not limited by these factors, and they arealso capable of modeling the shunt effectsoccurring when a weld is placed close to aprevious weld.The aims of this thesis was to evaluate such a3D-simulation software, Sorpas 3D, and itspotential to be used in industrial processplanning, and to assess the software’s usefulness for both simple and more complexcases.The results from this work show a good correspondence between the simulations andthe physical tests. However, in order to achieve these results a number of modifications in the material properties were  required. Another critical limitation in the software is that no expulsion criterion isimplemented. Considering the possibility that the problems can be solved with a number ofupdates in the software, Sorpas 3D can be auseful tool in the process planning industry inorder to decrease process times and materialcosts and improve the weld quality in thefuture.</p>

corrected abstract:
<p>Resistance spot welding (RSW) is the dominant joining technology in the automotive industry. This is due to its low costs and high efficiency. Other advantages with RSW is the high ability for automation, low consumption of energy, lack of need for added materials and low degree of pollution, no expensive equipment or education of personal compared to arc welding and laser welding. A modern automobile contains approximately 4000 resistance spot welds, which is why it is of great interest to be able to predict the properties of the resistance spot welds. The most important measurement used to ensure the quality of the weld is the nugget size, as it correlates to the welds mechanical strength. This is usually measured by destructive testing, and the most common method is the coach peel test. This test is performed by manually peeling the specimen and then measuring the largest and smallest nugget diameter. It is also possible to perform non-destructive testing on resistance spot welds with both ultrasonic and x-ray tests, however none of these methods have the same accuracy as the destructive methods and they are cumbersome to use in large-scale. To improve the efficiency and lower the cost for the optimization of the welding parameters, simulation tools have been developed. There are both 2D- and 3D-simulation software to model the RSW process. When the spot welds are simulated with 2D or 2D axis-symmetry, the number of elements is lower compared to a full 3D model, which reduces the computation times. The disadvantages with the 2D model are the inabilities to model misalignments or other asymmetrical geometries. In contrast, the 3D-simulations are not limited by these factors, and they are also capable of modeling the shunt effects occurring when a weld is placed close to a previous weld.</p><p>The aims of this thesis was to evaluate such a 3D-simulation software, Sorpas 3D, and its potential to be used in industrial process planning, and to assess the software’s usefulness for both simple and more complex cases.</p><p>The results from this work show a good correspondence between the simulations and the physical tests. However, in order to achieve these results a number of modifications in the material properties were required. Another critical limitation in the software is that no expulsion criterion is implemented. Considering the possibility that the problems can be solved with a number of updates in the software, Sorpas 3D can be a useful tool in the process planning industry in order to decrease process times and material costs and improve the weld quality in the future.</p>
----------------------------------------------------------------------
In diva2:1880451 abstract is:
<p>Positron emission tomography (PET) is a nuclear medicine imaging techniquethat uses radiotracers to visualize processes like metabolism and perfusion. Theradiotracer emits positrons, which collide with shell electrons of the atomsthat make up the surrounding tissue. Such a collision produces two gammarayphotons, emitted roughly 180 degrees apart [1]. PET captures thesephotons using a cylindrical arrangement of detectors. When two photons aredetected simultaneously by different detectors, it registers as a line of response(LOR). These LORs are then pre-processed into a sinogram. A mathematicalreconstruction method is used to computationally recover the 3D distribution ofthe radiotracer (activity map) from the sinogram. However, genuine LORs can becorrupted by false LORs that come from scattering, random events, and spuriousevents. Mitigating these in reconstruction algorithms is essential for improvingPET imaging accuracy and reliability.This paper explores the theoretical foundation of the Time of Flight (TOF) SingleScatter Simulation (SSS) model by Watson (2007) [2]. It also includes a Pythonimplementation of the MATLAB code associated with [2]. The model modelsCompton scattering to accurately estimate scattered photons in PET.Incorporating TOF data into the SSS model improves estimation accuracy, albeitat the cost of increased computational time. To expedite computations, thealgorithm was simplified by restricting operations to a subset of rings anddetectors and by pre-processing images through cropping and downscaling.Interpolation fills in missing data, ensuring complete estimation.The outcome of this project is a Python implementation that exhibited a strongcorrelation with the estimates obtained using the MATLAB implementation. Anotable issue arose during the comparison between the main components ofthe SSS algorithm in Python and MATLAB. The Euclidean norm between theresults from these two implementations was significant, indicating that they wereon different scales. Nevertheless, both implementations accurately predictedthe scatter in the same locations and relative magnitudes, despite the scalediscrepancy. Investigation into the discrepancy’s cause is ongoing, but theproject demonstrates the feasibility of implementing the TOF SSS algorithm inPython.</p>


corrected abstract:
<p>Positron emission tomography (PET) is a nuclear medicine imaging technique that uses radiotracers to visualize processes like metabolism and perfusion. The radiotracer emits positrons, which collide with shell electrons of the atoms that make up the surrounding tissue. Such a collision produces two gamma-ray photons, emitted roughly 180 degrees apart [1]. PET captures these photons using a cylindrical arrangement of detectors. When two photons are detected simultaneously by different detectors, it registers as a line of response (LOR). These LORs are then pre-processed into a sinogram. A mathematical reconstruction method is used to computationally recover the 3D distribution of the radiotracer (activity map) from the sinogram. However, genuine LORs can be corrupted by false LORs that come from scattering, random events, and spurious events. Mitigating these in reconstruction algorithms is essential for improving PET imaging accuracy and reliability.</p><p>This paper explores the theoretical foundation of the Time of Flight (TOF) Single Scatter Simulation (SSS) model by Watson (2007) [2]. It also includes a Python implementation of the MATLAB code associated with [2]. The model models Compton scattering to accurately estimate scattered photons in PET.</p><p>Incorporating TOF data into the SSS model improves estimation accuracy, albeit at the cost of increased computational time. To expedite computations, the algorithm was simplified by restricting operations to a subset of rings and detectors and by pre-processing images through cropping and downscaling. Interpolation fills in missing data, ensuring complete estimation.</p><p>The outcome of this project is a Python implementation that exhibited a strong correlation with the estimates obtained using the MATLAB implementation. A notable issue arose during the comparison between the main components of the SSS algorithm in Python and MATLAB. The Euclidean norm between the results from these two implementations was significant, indicating that they were on different scales. Nevertheless, both implementations accurately predicted the scatter in the same locations and relative magnitudes, despite the scale discrepancy. Investigation into the discrepancy’s cause is ongoing, but the project demonstrates the feasibility of implementing the TOF SSS algorithm in Python.</p>
----------------------------------------------------------------------
In diva2:1110752 - error in title:
"Identification and modelling of noise sources on a realisticnose landing gear using phased array methods applied tocomputational data"
==>
"Identification and modelling of noise sources on a realistic nose landing gear using phased array methods applied to computational data"


abstract is:
<p>Due to the recent development of quieter turbofan engines, airframe noise has started to emerge asthe most important noise source. This is particularly true during the approach/landing phase, whenthe engines are operated at low-thrust levels. In order to meet future noise level regulations, thecharacterization and subsequent reduction of landing gear induced noise is necessary. Wind-tunnelaeroacoustic tests have always been the favoured method for assessing and studying the noise generatedby landing gears, but their prohibitive cost has steered the attention towards numerical methods.Since direct flow noise simulations are still too demanding in computer resources, there is astrong interest in developing coupled CFD-CAA simulations as a tool to model and identify flownoise sources. More recently, they have been coupled with phased array methods in order to conductaeroacoustic studies on scaled-down, or simplified, aircraft components. This project investigates theaerodynamic sound sources on a realistic nose landing gear using numerical phased array methods,based on array data extracted from compressible Detached Eddy Simulations of the flow. Assumingmonopole and dipole modes of propagation, the sound sources are identified in the source regionthrough beamforming approaches: conventional beamforming, dual linear programming (dual-LP)deconvolution, orthogonal beamforming and CLEAN-SC. To assess the accuracy of the employedmethods, beamforming maps from flyover, sideline and forward point of views are obtained andcompared to experimental ones originating from wind-tunnel experiments performed on the samenose landing gear configuration by industrial and academic partners of the ALLEGRA project. Anarray design metric is defined to quantitatively assess the fitness of the employed arrays with respectto the different frequencies and distances separating the beamforming and array planes. A geneticalgorithm based on the Differential Evolution method is used to generate optimized arrays for selectedfrequencies in order to reduce the computational size of the problems solved. The modelledsources are used to generate far-field spectra which are subsequently compared to the ones obtainedwith the FfowcsWilliams and Hawkings acoustic analogy. The results show a good concordance betweenthe numerical phased array beamforming maps and the experimental ones, and a good matchbetween the far-field spectra up to a certain frequency threshold corresponding to the quality of themesh used. The presence of specific noise sources has been validated and their contribution to theoverall generated noise has been quantified. The results obtained demonstrate the potential of numericalphased array methods as a legitimate tool for aeroacoustic simulations in general and as atool to gain insight into the noise generation mechanisms of landing gear components in particular.</p>



corrected abstract:
<p>Due to the recent development of quieter turbofan engines, airframe noise has started to emerge as the most important noise source. This is particularly true during the approach/landing phase, when the engines are operated at low-thrust levels. In order to meet future noise level regulations, the characterization and subsequent reduction of landing gear induced noise is necessary. Wind-tunnel aeroacoustic tests have always been the favoured method for assessing and studying the noise generated by landing gears, but their prohibitive cost has steered the attention towards numerical methods. Since direct flow noise simulations are still too demanding in computer resources, there is a strong interest in developing coupled CFD-CAA simulations as a tool to model and identify flow noise sources. More recently, they have been coupled with phased array methods in order to conduct aeroacoustic studies on scaled-down, or simplified, aircraft components. This project investigates the aerodynamic sound sources on a realistic nose landing gear using numerical phased array methods, based on array data extracted from compressible Detached Eddy Simulations of the flow. Assuming monopole and dipole modes of propagation, the sound sources are identified in the source region through beamforming approaches: conventional beamforming, dual linear programming (dual-LP) deconvolution, orthogonal beamforming and CLEAN-SC. To assess the accuracy of the employed methods, beamforming maps from flyover, sideline and forward point of views are obtained and compared to experimental ones originating from wind-tunnel experiments performed on the same nose landing gear configuration by industrial and academic partners of the ALLEGRA project. An array design metric is defined to quantitatively assess the fitness of the employed arrays with respect to the different frequencies and distances separating the beamforming and array planes. A genetic algorithm based on the Differential Evolution method is used to generate optimized arrays for selected frequencies in order to reduce the computational size of the problems solved. The modelled sources are used to generate far-field spectra which are subsequently compared to the ones obtained with the Ffowcs Williams and Hawkings acoustic analogy. The results show a good concordance between the numerical phased array beamforming maps and the experimental ones, and a good match between the far-field spectra up to a certain frequency threshold corresponding to the quality of the mesh used. The presence of specific noise sources has been validated and their contribution to the overall generated noise has been quantified. The results obtained demonstrate the potential of numerical phased array methods as a legitimate tool for aeroacoustic simulations in general and as a tool to gain insight into the noise generation mechanisms of landing gear components in particular.</p>
----------------------------------------------------------------------
In diva2:618564 merged words and missing ligatures - note the corrected abstract needs to be checked agains the original abstract in the thesis - but there is no full text in DiVA.

abstract is:
<p>Due to progress in CFD (Computational Fluid Dynamics), it is now possible to compute and analyzesteady ow and phenomena for turbomachine design. Unsteady instability predictions are important tocertify that a turbomachine will not encounter high vibration levels in operation. Flutter is one of the mostcommon fan instabilities. Thus, the fan design is bound to respect a given Flutter Margin. It guaranteesa certain operation envelope for the engine and its fan, refered as operability. The operation envelope ofan engine is dened in the fan map mass ow rate - pressure ratio as the space in which the engine can berun in during operation. The fan map is made of isovelocities. An isovelocity is a line described by varyingmass ow rate and keeping constant the rotational speed. This domain is bounded by phenomena such asrotating stall, surge, utter, etc which are hazardous for engine mechanical integrity. Fig. 1 highlights howan operating envelope is bounded. When operation envelope is too small, the fan blade geometry needsto be modied to improve its utter behavior and therefore increase the size of the envelope. Eciency,pressure ratio and operability can be strongly impacted by the changes made. Therefore design parametersinuencing utter must be precisely spotted. This can be done if mechanisms which trigger utter are wellunderstood. Thus the blade can be reshaped to lower the contribution of a given phenomenon. However,when a phenomenon is identied, one should quantify its contribution to the globally stable or unstablebehavior. In fact, to reduce the geometrical changes and therefore consequences on operability, one shouldact on the most critical phenomenon.The study has been performed on a single fan blade with dierent congurations of back pressure androtational speed. Consequently, two kinds of utter are investigated : stall utter and transonic utter. Therst one occurs at low rotational speed. It corresponds to zone 1 in Fig. 1. It is commonly driven by owseparation. As described in,1 separation on the suction side of a blade can be responsible for utter. Article1shows that unsteady pressure and blade motion are out of phase in the separated zone. Furthermore, studiesin2 reveal that traveling Mach waves on the blade surface can destabilize it. It depends on the phase betweenthe waves and the blade motion. Acoustic interference is also studied in.1 Transonic utter occurs at higherrotational speed when the blade is shocked. It corresponds to zone 2 in Fig. 1. There are two main sourcesthat destabilize the blade: interaction between the shock waves and the boundary layer and the shock waveoscillation as presented in.3 That study divides the prole into four parts. Each part corresponds to a givenmechanism: supersonic part (stabilizing), shocked part (both stabilizing and destabilizing if the shock wavesoscillates), downstream the shock (destabilizing due to separation) on the suction side and the pressure side(stabilizing).The objective of this paper is not only to identify mechanisms responsible for utter at low and high rotationalspeed but also to follow their evolution along an isovelocity. A global approach from the mechanismsidentication to the quantication of the phenomenon is then described.The critical mechanisms responsiblefor fan blade utter for a given conguration can be pointed out.</p>

corrected abstract:
<p>Due to progress in CFD (Computational Fluid Dynamics), it is now possible to compute and analyze steady flow and phenomena for turbomachine design. Unsteady instability predictions are important to certify that a turbomachine will not encounter high vibration levels in operation. Flutter is one of the most common fan instabilities. Thus, the fan design is bound to respect a given Flutter Margin. It guarantees a certain operation envelope for the engine and its fan, refered as operability. The operation envelope of an engine is defined in the fan map mass flow rate - pressure ratio as the space in which the engine can be run in during operation. The fan map is made of isovelocities. An isovelocity is a line described by varying mass flow rate and keeping constant the rotational speed. This domain is bounded by phenomena such as rotating stall, surge, flutter, etc which are hazardous for engine mechanical integrity. Fig. 1 highlights how an operating envelope is bounded. When operation envelope is too small, the fan blade geometry needs to be modified to improve its flutter behavior and therefore increase the size of the envelope. Efficiency, pressure ratio and operability can be strongly impacted by the changes made. Therefore design parameters influencing flutter must be precisely spotted. This can be done if mechanisms which trigger flutter are well understood. Thus the blade can be reshaped to lower the contribution of a given phenomenon. However, when a phenomenon is identied, one should quantify its contribution to the globally stable or unstable behavior. In fact, to reduce the geometrical changes and therefore consequences on operability, one should act on the most critical phenomenon. The study has been performed on a single fan blade with different configurations of back pressure and rotational speed. Consequently, two kinds of flutter are investigated : stall flutter and transonic flutter. The first one occurs at low rotational speed. It corresponds to zone 1 in Fig. 1. It is commonly driven by flow separation. As described in, 1 separation on the suction side of a blade can be responsible for flutter. Article 1 shows that unsteady pressure and blade motion are out of phase in the separated zone. Furthermore, studies in 2 reveal that traveling Mach waves on the blade surface can destabilize it. It depends on the phase between the waves and the blade motion. Acoustic interference is also studied in .1 Transonic flutter occurs at higher rotational speed when the blade is shocked. It corresponds to zone 2 in Fig. 1. There are two main sources that destabilize the blade: interaction between the shock waves and the boundary layer and the shock wave oscillation as presented in.3 That study divides the prole into four parts. Each part corresponds to a given mechanism: supersonic part (stabilizing), shocked part (both stabilizing and destabilizing if the shock waves oscillates), downstream the shock (destabilizing due to separation) on the suction side and the pressure side(stabilizing). The objective of this paper is not only to identify mechanisms responsible for flutter at low and high rotational speed but also to follow their evolution along an isovelocity. A global approach from the mechanisms identication to the quantication of the phenomenon is then described. The critical mechanisms responsible for fan blade flutter for a given configuration can be pointed out.</p>
----------------------------------------------------------------------
In diva2:1528140 abstract is:
<p>Due to the accelerating need for decarbonization in the shipping sector, wind-assisted cargo shipsare able to play a key role in achieving the IMO 2050 targets on reducing the total annual GHGemissions from international shipping by at least 50%. The aim of this Master’s Thesis project is todevelop a Performance Prediction Program for wind-assisted cargo ships to contribute knowledgeon the performance of this technology. The three key characteristics of this model are its genericstructure, the small number of input data needed and its ability to predict the performance of threepossible Wind-Assisted Propulsion Systems (WAPS): Rotor Sails, Rigid Wing Sails and DynaRigs.It is a fast and easy tool able to predict, to a good level of accuracy and really low computationaltime, the performance of any commercial ship with these three WAPS options installed with onlythe main particulars and general dimensions as input data.The hull and WAPS models predict the forces and moments, which the program balances in 6degrees of freedom to predict the theoretical sailing performance of the wind-assisted cargo shipwith the specified characteristics for various wind conditions. The model is able to play with differentoptimization objectives. This includes maximizing sailing speed if a VPP is run or maximizingtotal power savings if it is a PPP. The program is based on semi-empirical methods and a WAPSaerodynamic database created from published data on lift and drag coefficients. All WAPS datacan be interpolated with the aim to scale to different sizes and configurations such as number ofunits and different aspect ratios.A model validation is carried out to evaluate its reliability. The model results are compared withthe real sailing data of the Long Range 2 (LR2) class tanker vessel, the Maersk Pelican, whichwas recently fitted with two 30 meter high Rotor Sails; and results from another performanceprediction program. In general, the two performance prediction programs and some of the realsailing measurements show good agreement. However, for some downwind sailing conditions, theperformance predictions are more conservative than the measured values.Results showing and comparing power savings, thrust and side force coefficients for the differentWAPS are also presented and discussed. The results of this Master’s Thesis project show howWind-Assisted Propulsion Systems have high potential in playing a key role in the decarbonizationof the shipping sector. WAPS can prove substantial power, fuel, cost, and emissions savings.Tankers and bulk-carriers are specially suitable for wind propulsion thanks to their available deckspace and relatively low design speeds.The Performance Prediction Program for wind-assisted cargo ships developed in this Master’sThesis shows promising results with a good level of accuracy despite its generic and small numberof input data. It can be a useful tool in early project stages to quickly and accurately assess thepotential and performance of WAPS systems.</p>


corrected abstract:
<p>Due to the accelerating need for decarbonization in the shipping sector, wind-assisted cargo ships are able to play a key role in achieving the IMO 2050 targets on reducing the total annual GHG emissions from international shipping by at least 50%. The aim of this Master’s Thesis project is to develop a Performance Prediction Program for wind-assisted cargo ships to contribute knowledge on the performance of this technology. The three key characteristics of this model are its generic structure, the small number of input data needed and its ability to predict the performance of three possible Wind-Assisted Propulsion Systems (WAPS): Rotor Sails, Rigid Wing Sails and DynaRigs. It is a fast and easy tool able to predict, to a good level of accuracy and really low computational time, the performance of any commercial ship with these three WAPS options installed with only the main particulars and general dimensions as input data.</p><p>The hull and WAPS models predict the forces and moments, which the program balances in 6 degrees of freedom to predict the theoretical sailing performance of the wind-assisted cargo ship with the specified characteristics for various wind conditions. The model is able to play with different optimization objectives. This includes maximizing sailing speed if a VPP is run or maximizing total power savings if it is a PPP. The program is based on semi-empirical methods and a WAPS aerodynamic database created from published data on lift and drag coefficients. All WAPS data can be interpolated with the aim to scale to different sizes and configurations such as number of units and different aspect ratios.</p><p>A model validation is carried out to evaluate its reliability. The model results are compared with the real sailing data of the Long Range 2 (LR2) class tanker vessel, the Maersk Pelican, which was recently fitted with two 30 meter high Rotor Sails; and results from another performance prediction program. In general, the two performance prediction programs and some of the real sailing measurements show good agreement. However, for some downwind sailing conditions, the performance predictions are more conservative than the measured values.</p><p>Results showing and comparing power savings, thrust and side force coefficients for the different WAPS are also presented and discussed. The results of this Master’s Thesis project show how Wind-Assisted Propulsion Systems have high potential in playing a key role in the decarbonization of the shipping sector. WAPS can prove substantial power, fuel, cost, and emissions savings. Tankers and bulk-carriers are specially suitable for wind propulsion thanks to their available deck space and relatively low design speeds.</p><p>The Performance Prediction Program for wind-assisted cargo ships developed in this Master’s Thesis shows promising results with a good level of accuracy despite its generic and small number of input data. It can be a useful tool in early project stages to quickly and accurately assess the potential and performance of WAPS systems.</p>
----------------------------------------------------------------------
In diva2:1083484 abstract is:
<p>Cable-stayed bridges have become very popular over the last ve decades due totheir aesthetic appeal, structural eciency, the limited amount of material usageand nancial benets. The rapid increase of new techniques creating longer spans,slender decks and more spectacular design has given rise to a major concern ofthe dynamic behavior of cable-stayed bridges. This has resulted in a more carefulmodelling procedure that will represent the reality in the most particular way. Amodel is simply an approximation of the reality, thus it is important to establishwhat simplications and approximations that are reasonable to make in order forthe model to be as accurate as possible.The Millau Viaduct is a cable-stayed bridge unique of its kind. At the time thatit was built it was breaking many records: span length, height of deck above thefoundations and the short construction time in just three years. Due to the slendernessof the structure, the extreme height and the location in a deep valley, theviaduct is naturally subjected to external loads. This thesis attempts to describea performed dynamic nonlinear analysis of two models of the Millau Viaduct usingthe FEA packages SAP2000 and BRIGADE/Plus. The models have been renedin order to be compared between the programs and to the reality i.e. the measuredmode shapes and frequencies obtained from reports.The viaduct required many specically designed solutions in order to obtain theelegance and the aesthetic appeal. Approximations in geometry has been essentialdue to the many details that the viaduct consists of, but the details are nonethelessimportant to capture to get the structural mechanics correct. The support conditionshas been considered as important as these were designed to allow for movementthat were caused by a combination of the external loads and the slenderness of thestructure. The most critical support conditions were the deck-pier connection inwhich the piers are split into two columns equipped with spherical bearings allowingfor angular rotation. The two shafts were modelled by one single column and thespherical bearings were simulated by creating two alternative models; one assignedwith a pinned constraint to allow for the angular rotation and the second, since thissupport condition is in fact rigid has been assigned as xed.The SAP and BRIGADE models showed to be consistent with each other, thoughthe beam theories, Euler-Bernoulli were applied to the SAP model and Timoshenkoin BRIGADE. The alternative models with the dierent constraints generated fairresults yet diers signicantly from each other. Alternative approaches towards themodelling have been addressed in the conclusions.</p>


corrected abstract:
<p>Cable-stayed bridges have become very popular over the last five decades due to their aesthetic appeal, structural efficiency, the limited amount of material usage and financial benefits. The rapid increase of new techniques creating longer spans, slender decks and more spectacular design has given rise to a major concern of the dynamic behavior of cable-stayed bridges. This has resulted in a more careful modelling procedure that will represent the reality in the most particular way. A model is simply an approximation of the reality, thus it is important to establish what simplifications and approximations that are reasonable to make in order for the model to be as accurate as possible.</p><p>The Millau Viaduct is a cable-stayed bridge unique of its kind. At the time that it was built it was breaking many records: span length, height of deck above the foundations and the short construction time in just three years. Due to the slenderness of the structure, the extreme height and the location in a deep valley, the viaduct is naturally subjected to external loads. This thesis attempts to describe a performed dynamic nonlinear analysis of two models of the Millau Viaduct using the FEA packages SAP2000 and BRIGADE/Plus. The models have been refined in order to be compared between the programs and to the reality i.e. the measured mode shapes and frequencies obtained from reports.</p><p>The viaduct required many specifically designed solutions in order to obtain the elegance and the aesthetic appeal. Approximations in geometry has been essential due to the many details that the viaduct consists of, but the details are nonetheless important to capture to get the structural mechanics correct. The support conditions has been considered as important as these were designed to allow for movement that were caused by a combination of the external loads and the slenderness of the structure. The most critical support conditions were the deck-pier connection in which the piers are split into two columns equipped with spherical bearings allowing for angular rotation. The two shafts were modelled by one single column and the spherical bearings were simulated by creating two alternative models; one assigned with a pinned constraint to allow for the angular rotation and the second, since this support condition is in fact rigid has been assigned as fixed.</p><p>The SAP and BRIGADE models showed to be consistent with each other, though the beam theories, Euler-Bernoulli were applied to the SAP model and Timoshenko in BRIGADE. The alternative models with the different constraints generated fair results yet differs significantly from each other. Alternative approaches towards the modelling have been addressed in the conclusions.</p>
----------------------------------------------------------------------
In diva2:876188 abstract is:
<p>Every computational fluid dynamics engineer deals with a never ending story – limitedcomputer resources. In computational fluid dynamics there is practically never enoughcomputer power. Limited computer resources lead to long calculation times which result inhigh costs and one of the main reasons is that large quantity of elements are needed in acomputational mesh in order to obtain accurate and reliable results.Although there exist established meshing approaches for the Siemens 4th generation DLEburner, mesh dependency has not been fully evaluated yet. The main goal of this work istherefore to better optimize accuracy versus cell count for this particular burner intended forsimulation of air/gas mixing where eddy-viscosity based turbulence models are employed.Ansys Fluent solver was used for all simulations in this work. For time effectivisationpurposes a 30° sector model of the burner was created and validated for the meshconvergence study. No steady state solutions were found for this case therefore timedependent simulations with time statistics sampling were employed. The mesh convergencestudy has shown that a coarse computational mesh in air casing of the burner does not affectflow conditions downstream where air/gas mixing process is taking place and that a majorpart of the combustion chamber is highly mesh independent. A large reduction of cell count inthose two parts is therefore allowed. On the other hand the RPL (Rich Pilot Lean) and thepilot burner turned out to be highly mesh density dependent. The RPL and the Pilot burnerneed to have significantly more refined mesh as it has been used so far with the establishedmeshing approaches. The mesh optimization has finally shown that at least as accurate resultsof air/gas mixing results may be obtained with 3x smaller cell count. Furthermore it has beenshown that significantly more accurate results may be obtained with 60% smaller cell count aswith the established meshing approaches.A short mesh study of the Siemens 3rd generation DLE burner in ignition stage of operationwas also performed in this work. This brief study has shown that the established meshingapproach for air/gas mixing purposes is sufficient for use with Ansys Fluent solver whilecertain differences were discovered when comparing the results obtained with Ansys Fluentagainst those obtained with Ansys CFX solver. Differences between Fluent and CFX solverwere briefly discussed in this work as identical simulation set up in both solvers producedslightly different results. Furthermore the obtained results suggest that Fluent solver is lessmesh dependent as CFX solver for this particular case.</p>


corrected abstract:
<p>Every computational fluid dynamics engineer deals with a never ending story – limited computer resources. In computational fluid dynamics there is practically never enough computer power. Limited computer resources lead to long calculation times which result in high costs and one of the main reasons is that large quantity of elements are needed in a computational mesh in order to obtain accurate and reliable results.</p><p>Although there exist established meshing approaches for the Siemens 4<sup>th</sup> generation DLE burner, mesh dependency has not been fully evaluated yet. The main goal of this work is therefore to better optimize accuracy versus cell count for this particular burner intended for simulation of air/gas mixing where eddy-viscosity based turbulence models are employed. Ansys Fluent solver was used for all simulations in this work. For time effectivisation purposes a 30° sector model of the burner was created and validated for the mesh convergence study. No steady state solutions were found for this case therefore time dependent simulations with time statistics sampling were employed. The mesh convergence study has shown that a coarse computational mesh in air casing of the burner does not affect flow conditions downstream where air/gas mixing process is taking place and that a major part of the combustion chamber is highly mesh independent. A large reduction of cell count in those two parts is therefore allowed. On the other hand the RPL (Rich Pilot Lean) and the pilot burner turned out to be highly mesh density dependent. The RPL and the Pilot burner need to have significantly more refined mesh as it has been used so far with the established meshing approaches. The mesh optimization has finally shown that at least as accurate results of air/gas mixing results may be obtained with 3x smaller cell count. Furthermore it has been shown that significantly more accurate results may be obtained with 60% smaller cell count as with the established meshing approaches.</p><p>A short mesh study of the Siemens 3<sup>rd</sup> generation DLE burner in ignition stage of operation was also performed in this work. This brief study has shown that the established meshing approach for air/gas mixing purposes is sufficient for use with Ansys Fluent solver while certain differences were discovered when comparing the results obtained with Ansys Fluent against those obtained with Ansys CFX solver. Differences between Fluent and CFX solver were briefly discussed in this work as identical simulation set up in both solvers produced slightly different results. Furthermore the obtained results suggest that Fluent solver is less mesh dependent as CFX solver for this particular case.</p>
----------------------------------------------------------------------
In diva2:854573 abstract is:
<p>Populations in big cities keep a constant inflation. It is estimated that 60% of thepopulation will move into a big city in the next 20 years, regarding this reason there arehigh demands for new solutions to the modern transportation system. A means ofmeasure that a lot of industrialized countries have implemented are high speed railwaytrains. The railway train covers the transportation needs. What would happen if amaglev train would be implemented instead?</p><p>The purpose with this report is to get an estimate if maglev trains can be a superiorsolution than the conventional railway train. In order to proceed this task, an analyszeswith respect to the Shinkansen N700A and ICE3 within railway as well as TransrapidTR09 and SCMaglev MLX01 within Maglev train was carried out. Aspects such asSafety, energy consumption, environmental impact and cost were the four models thatwere investigated.</p><p>Concerning safety, you could establish that both systems keep a high security standard,both systems have been involved in accidents, which is in a way a positive trait, keepsthe companies developing and improving the security measures for their trains, keepingthem safer. There is no guarantee that collisions or derailing occurs. Regarding energyconsumption, the results were that TR09 and ICE3 consume the same amount of energyat maximum allowed speed as well as Shinkansen N700A consumes considerably lessenergy than MLX01. Considering that energy consumption is proportional with thecarbon dioxide emissions results that N700A contributes to less carbon dioxideemissions than MLX01 as well as the TR09 and ICE3 contributed equally.</p><p>None of this aspects were decisive to demonstration if one system was moreadvantageous than the other, but how does the cost portion differentiate with thedifferent train systems? The result shows that the infrastructure cost for the maglev trainwere extremely high compared to the railway train. Germany (pioneers within themaglev technology) have shut down there maglev projects. Difference regardingoperation cost were not significant for the Transrapid and the Intercity-Express trains.Observing the discovered information on vehicle cost, a complete vehicle cost analyzescould not be established due to the lack of information on the vehicle cost on SCMaglevML0X1. With respect to the other three high speed trains, the Shinkasen N700A was themost expensive with a vehicle cost on $44 millions per unit, followed by the Transrapidwith a vehicle cost on $12.9 millions and cheapest train is the ICE3, costing $3.7millions per vehicle.</p><p>Keeping in thought all gathered information and data, the conclusion drawn is that it isnot profitable to construct maglev train with modern technology. If technical aspectsimproves further so that specifically the infrastructure cost decreases considerably thena maglev system could be a worthy solution for the foreseeable future.</p>

corrected abstract:
<p>Populations in big cities keep a constant inflation. It is estimated that 60% of the population will move into a big city in the next 20 years, regarding this reason there are high demands for new solutions to the modern transportation system. A means of measure that a lot of industrialized countries have implemented are high speed railway trains. The railway train covers the transportation needs. What would happen if a maglev train would be implemented instead?</p><p>The purpose with this report is to get an estimate if maglev trains can be a superior solution than the conventional railway train. In order to proceed this task, an analyszes with respect to the Shinkansen N700A and ICE3 within railway as well as Transrapid TR09 and SCMaglev MLX01 within Maglev train was carried out. Aspects such as Safety, energy consumption, environmental impact and cost were the four models that were investigated.</p><p>Concerning safety, you could establish that both systems keep a high security standard, both systems have been involved in accidents, which is in a way a positive trait, keeps the companies developing and improving the security measures for their trains, keeping them safer. There is no guarantee that collisions or derailing occurs. Regarding energy consumption, the results were that TR09 and ICE3 consume the same amount of energy at maximum allowed speed as well as Shinkansen N700A consumes considerably less energy than MLX01. Considering that energy consumption is proportional with the carbon dioxide emissions results that N700A contributes to less carbon dioxide emissions than MLX01 as well as the TR09 and ICE3 contributed equally.</p><p>None of this aspects were decisive to demonstration if one system was more advantageous than the other, but how does the cost portion differentiate with the different train systems? The result shows that the infrastructure cost for the maglev train were extremely high compared to the railway train. Germany (pioneers within the maglev technology) have shut down there maglev projects. Difference regarding operation cost were not significant for the Transrapid and the Intercity-Express trains. Observing the discovered information on vehicle cost, a complete vehicle cost analyzes could not be established due to the lack of information on the vehicle cost on SCMaglev ML0X1. With respect to the other three high speed trains, the Shinkasen N700A was the most expensive with a vehicle cost on $44 millions per unit, followed by the Transrapid with a vehicle cost on $12.9 millions and cheapest train is the ICE3, costing $3.7 millions per vehicle.</p><p>Keeping in thought all gathered information and data, the conclusion drawn is that it is not profitable to construct maglev train with modern technology. If technical aspects improves further so that specifically the infrastructure cost decreases considerably then a maglev system could be a worthy solution for the foreseeable future.</p>
----------------------------------------------------------------------
In diva2:1741184 abstract is:
<p>Accurate estimations of the reverberation time of furnished office spacesis essential as an acoustic consultant. A time efficient way to predict thefurniture’s effect is to investigate it by software modelling. The room acousticsoftware Odeon is suitable for doing this. In today’s geometrical room acousticmodellers (such as Odeon) a parameter called the scattering factor was usedwhich was of large importance in this thesis. This thesis set out to investigatehow well Odeon predicts the reverberation time in smaller and larger officespaces given that the correct scattering factor for each type of furniturecould be established. The method for the investigation was to perform fieldmeasurements in two office spaces of different sizes and geometry. Thenuse different setups of furniture to examine the furniture’s effect on thereverberation time connected to their sound scattering properties. The modelwas designed with 2D objects in SketchUp and exported to Odeon. A referencevalue for a pair of measurement setups was obtained by using the ratio of thetotal reverberation time (octave bands 125 Hz to 4 kHz) of the rooms. Thisratio was used as a target in Odeon for the same simulated pair of room setups.The scattering factor was adjusted in increments of the specific furniture usedin the setup until an optimized fit was reached. These steps were carried outfor each combination of setups. Afterwards the simulations were comparedto the measured and calculated reverberation times using Sabine’s formulaand Arau-Puchades formula. It was possible to establish specific scatteringfactors for the furniture types within Odeon although their accuracy was hardto determine. The resulting reverberation times from the Odeon simulationsof the large and small spaces were not closer to the measured reverberationtime than the calculated ones to any distinct degree. It’s worth noting that allof these calculations are dependant on ideally diffuse circumstances which theactual rooms rarely are. This is why they tend to underestimate the room’sreverberation time. The main goal of this thesis is to an extent fulfilled,although maybe not with as great future utility as hoped. The scattering factorscorrespond to each other independently of what room was being modelled. Inan isolated framework of Odeon, the attained scattering factors may be of use,especially if the measured room can be assumed ideally diffuse.</p>

corrected abstract:
o<p>Accurate estimations of the reverberation time of furnished office spaces is essential as an acoustic consultant. A time efficient way to predict the furniture’s effect is to investigate it by software modelling. The room acoustic software Odeon is suitable for doing this. In today’s geometrical room acoustic modellers (such as Odeon) a parameter called the scattering factor was used which was of large importance in this thesis. This thesis set out to investigate how well Odeon predicts the reverberation time in smaller and larger office spaces given that the correct scattering factor for each type of furniture could be established. The method for the investigation was to perform field measurements in two office spaces of different sizes and geometry. Then use different setups of furniture to examine the furniture’s effect on the reverberation time connected to their sound scattering properties. The model was designed with 2D objects in SketchUp and exported to Odeon. A reference value for a pair of measurement setups was obtained by using the ratio of the total reverberation time (octave bands 125 Hz to 4 kHz) of the rooms. This ratio was used as a target in Odeon for the same simulated pair of room setups. The scattering factor was adjusted in increments of the specific furniture used in the setup until an optimized fit was reached. These steps were carried out for each combination of setups. Afterwards the simulations were compared to the measured and calculated reverberation times using Sabine’s formula and Arau-Puchades formula. It was possible to establish specific scattering factors for the furniture types within Odeon although their accuracy was hard to determine. The resulting reverberation times from the Odeon simulations of the large and small spaces were not closer to the measured reverberation time than the calculated ones to any distinct degree. It’s worth noting that all of these calculations are dependant on ideally diffuse circumstances which the actual rooms rarely are. This is why they tend to underestimate the room’s reverberation time. The main goal of this thesis is to an extent fulfilled, although maybe not with as great future utility as hoped. The scattering factors correspond to each other independently of what room was being modelled. In an isolated framework of Odeon, the attained scattering factors may be of use, especially if the measured room can be assumed ideally diffuse.</p>
----------------------------------------------------------------------
In diva2:1644922 abstract is:
<p>This thesis studies the feasibility of integrating the novelStructural Battery (SB)[1] into the airframe of a UnmannedAerial Vehicle (UAV). The potential advantages in terms ofmass, range and endurance are studied.The aircraft performance is analysed using conventionalflight mechanics, modelled in Matlab and Xfoil. The structureis designed and analysed using composite laminate theoryand beam theory in conjunction with verification in AnsysMechanical. An iterative procedure was used to arriveat a design that satisfied the set structural- and flight requirements.The currently demonstrated structural battery has a specificenergy density of 23.8Wh/kg, an elastic modulus of25GPa and tensile strength of at least 300MPa.[1]The laminae properties used in this master thesis were estimatedusing the Reuss and Voigt model combined with theRule of Mixtures (RoM). A quasi isotropic SB laminate wasmodelled according to the previous structural requirementsand assumed material properties. It yielded an elastic modulusof 54GPa. In order to simplify the analysis the energyand stiffness were decoupled. The SB was assigned a specificenergy of 23.8Wh/kg and 60.6Wh/kg according to thevalues measured and estimated previously[1].A SB with a tensile modulus of 54GPa and specific energyof 24Wh/kg was shown not to be beneficial to integrate intothe primary aircraft structure. The designed SB yieldeda reduction in flight range of 5.8%. This was shown bycomparing the designed SB with a reference aircraft configuration.The reference configuration uses a conventionalbattery that has a specific energy density of 160Wh/kg andconventional Carbon Fibre Composite (CFC) with an elasticmodulus of 71GPa.It was shown that the integration of the SB modelled wouldbecome beneficial compared to the reference aircraft configurationwhen the SB specific energy exceeds 33Wh/kg.The integration of a structural battery with a specific energyof 60.6Wh/kg yielded a flight range improvement of16.9% compared to the reference aircraft.</p>

corrected abstract:
<p>This thesis studies the feasibility of integrating the novel Structural Battery (SB)[1] into the airframe of a Unmanned Aerial Vehicle (UAV). The potential advantages in terms of mass, range and endurance are studied. The aircraft performance is analysed using conventional flight mechanics, modelled in Matlab and Xfoil. The structure is designed and analysed using composite laminate theory and beam theory in conjunction with verification in Ansys Mechanical. An iterative procedure was used to arrive at a design that satisfied the set structural- and flight requirements.</p><p>The currently demonstrated structural battery has a specific energy density of 23.8 Wh/kg, an elastic modulus of 25GPa and tensile strength of at least 300 MPa.[1]</p><p>The laminae properties used in this master thesis were estimated using the Reuss and Voigt model combined with the Rule of Mixtures (RoM). A quasi isotropic SB laminate was modelled according to the previous structural requirements and assumed material properties. It yielded an elastic modulus of 54 GPa. In order to simplify the analysis the energy and stiffness were decoupled. The SB was assigned a specific energy of 23.8 Wh/kg and 60.6 Wh/kg according to the values measured and estimated previously[1].</p><p>A SB with a tensile modulus of 54 GPa and specific energy of 24 Wh/kg was shown not to be beneficial to integrate into the primary aircraft structure. The designed SB yielded a reduction in flight range of 5.8%. This was shown by comparing the designed SB with a reference aircraft configuration. The reference configuration uses a conventional battery that has a specific energy density of 160 Wh/kg and conventional Carbon Fibre Composite (CFC) with an elastic modulus of 71 GPa.i</p><p>It was shown that the integration of the SB modelled would become beneficial compared to the reference aircraft configuration when the SB specific energy exceeds 33 Wh/kg. The integration of a structural battery with a specific energy of 60.6 Wh/kg yielded a flight range improvement of 16.9% compared to the reference aircraft.</p>
----------------------------------------------------------------------
In diva2:1541213 abstract is:
<p>The objectives of the present project were to set up, optimise and characterise a digitalholographic microscopy (DHM) laboratory set-up designed for the study of eyetissue and to implement and optimise digital data processing and noise reductionroutines. This work is part of a collaborative project aiming to provide quantitativemethods for the in vitro and in vivo characterisation of human corneal transparency.The laboratory set-up is based on a commercial laboratory microscope with zoomfunction (a “macroscope”). In continuation of previous work, we completed and optimised,and extended a software for holographic signal processing and numericalpropagation of the wavefront.To characterise the set-up and quantify its performances for standard operationand in its DHM configuration, we compare the magnification and resolution to theoreticalvalues for a given set of parameters. We determined the magnification factorand the rotation angle between the object and camera planes. With a laser wavelengthof 532 nm, a x1 objective and a zoom setting of x2.9 (which corresponds to aplane sample wavefront), we measured a magnification of 1.68. With the same parameters,we measure a holographic resolution of about 11 m. The wavefront phasecould be determined with a precision of a fraction of the wavelength.We subsequently performed analysis of the relative contribution of coherent noiseand implemented and evaluated several noise reduction routines. While the impactof coherent noise remained visible in the amplitude image, interferometric precisionwas obtained for the phase of the wavefront and the set-up was considered qualifiedfor its intended use for corneal characterisation.A first test measurement was performed on primate cornea.Subsequent work will address the further quantitative characterisation of the setupfor the full set of parameters (objectives, zoom positions, wavelengths), test measurementson samples with known transmission and light scattering properties (e.g.solutions of PMMA beads) and the comparison of the results with the predictions ofa theoretical model, and measurements on animal and human tissue.</p>


corrected abstract:
<p>The objectives of the present project were to set up, optimise and characterise a digital holographic microscopy (DHM) laboratory set-up designed for the study of eye tissue and to implement and optimise digital data processing and noise reduction routines. This work is part of a collaborative project aiming to provide quantitative methods for the in vitro and in vivo characterisation of human corneal transparency.</p><p>The laboratory set-up is based on a commercial laboratory microscope with zoom function (a “macroscope”). In continuation of previous work, we completed and optimised, and extended a software for holographic signal processing and numerical propagation of the wavefront.</p><p>To characterise the set-up and quantify its performances for standard operation and in its DHM configuration, we compare the magnification and resolution to theoretical values for a given set of parameters. We determined the magnification factor and the rotation angle between the object and camera planes. With a laser wavelength of 532 nm, a x1 objective and a zoom setting of x2.9 (which corresponds to a plane sample wavefront), we measured a magnification of 1.68. With the same parameters, we measure a holographic resolution of about 11 µm. The wavefront phase could be determined with a precision of a fraction of the wavelength.</p><p>We subsequently performed analysis of the relative contribution of coherent noise and implemented and evaluated several noise reduction routines. While the impact of coherent noise remained visible in the amplitude image, interferometric precision was obtained for the phase of the wavefront and the set-up was considered qualified for its intended use for corneal characterisation.</p><p>A first test measurement was performed on primate cornea.</p><p>Subsequent work will address the further quantitative characterisation of the setup for the full set of parameters (objectives, zoom positions, wavelengths), test measurements on samples with known transmission and light scattering properties (e.g. solutions of PMMA beads) and the comparison of the results with the predictions of a theoretical model, and measurements on animal and human tissue.</p>
----------------------------------------------------------------------
In diva2:1334020 abstract is:
<p>In order to keep up with the increasing demand of fuel-efficiency in the transportationindustry, the interest of making the vehicles as lightweight as possible is steadilyincreasing. One of the ways of reducing the weight is to introduce an anisotropicmaterial as Short Fibre Reinforced Polymers (SFRP) as a replacement for structuralparts made out of metals. To meet the modern vehicle design process which strivestowards a more simulation driven workflow, the need for accurate simulations offibre reinforced composites is of importance.This thesis aims to evaluate and find a working process for fatigue analysis of injectionmoulded SFRP components. To evaluate the fatigue analysis procedure anexisting SFRP component has been studied. The component is the front bracket thatmounts the roof air deflector to the roof on Scania trucks. To correlate the fatigue lifeestimation from the fatigue analysis, experiments were performed at ÅF Test Centerin Borlänge.The anisotropic behaviour is modelled using the commercial software Digimat togetherwith an injection simulation provided by Scania, to estimate the fibre orientationand thereby the material behaviour of the SFRP component. The fatigue analysiswas conducted by performing a coupled structural analysis between Digimat-Abaqus and then import the resulting stress- and strain-fields into the fatigue postprocessornCode DesignLife. The stress is then cyclic tested towards experimentallydetermined S-N curves determined in Digimat.Due to restriction of available fatigue data for the plastic in the front bracket, a fatiguematerial model for a plastic containing the same fibres and matrix but witha different fibre amount was implemented. The fatigue data were scaled using theUTS method to get a good characterisation of the real-life material behaviour of theplastic of the front bracket component.From the correlation between the fatigue analysis and performed experiments, itwas shown that the simulated fatigue life was conservative compared to the fatiguelife determined from the experiments. However, the correlation between the fatigueanalysis and experiments is not fully captured but gives a better estimation of thefatigue life compared to performing the fatigue analysis using an isotropic materialmodel.</p>

corrected abstract:
<p>In order to keep up with the increasing demand of fuel-efficiency in the transportation industry, the interest of making the vehicles as lightweight as possible is steadily increasing. One of the ways of reducing the weight is to introduce an anisotropic material as Short Fibre Reinforced Polymers (SFRP) as a replacement for structural parts made out of metals. To meet the modern vehicle design process which strives towards a more simulation driven workflow, the need for accurate simulations of fibre reinforced composites is of importance.</p><p>This thesis aims to evaluate and find a working process for fatigue analysis of injection moulded SFRP components. To evaluate the fatigue analysis procedure an existing SFRP component has been studied. The component is the front bracket that mounts the roof air deflector to the roof on Scania trucks. To correlate the fatigue life estimation from the fatigue analysis, experiments were performed at ÅF Test Center in Borlänge.</p><p>The anisotropic behaviour is modelled using the commercial software Digimat together with an injection simulation provided by Scania, to estimate the fibre orientation and thereby the material behaviour of the SFRP component. The fatigue analysis was conducted by performing a coupled structural analysis between Digimat-Abaqus and then import the resulting stress- and strain-fields into the fatigue post-processor nCode DesignLife. The stress is then cyclic tested towards experimentally determined S-N curves determined in Digimat.</p><p>Due to restriction of available fatigue data for the plastic in the front bracket, a fatigue material model for a plastic containing the same fibres and matrix but with a different fibre amount was implemented. The fatigue data were scaled using the UTS method to get a good characterisation of the real-life material behaviour of the plastic of the front bracket component.</p><p>From the correlation between the fatigue analysis and performed experiments, it was shown that the simulated fatigue life was conservative compared to the fatigue life determined from the experiments. However, the correlation between the fatigue analysis and experiments is not fully captured but gives a better estimation of the fatigue life compared to performing the fatigue analysis using an isotropic material model.</p>
----------------------------------------------------------------------
In diva2:1110758 abstract is:
<p>In the frame of the improvement of the performances for Ariane 5, an analysis iscarried out to explain the pressure drop observed in the ascent phase of some flights inthe liquid hydrogen (LH2) tank of the upper stage. This stage is mainly idle until therocket is out of the atmosphere but is submitted to important excitation throughoutthe ascent phase in the atmosphere. Due to excitation, the liquid contained in thetank moves and breaks the thermodynamic equilibrium. This movement, sloshing isidentified as the most likely cause of the pressure drop observed. It is investigated inthis thesis to understand how exactly it impacts the thermodynamic equilibrium inthe tank.The pressure drop called creux PGRH can be explained by the mixing of the topof the liquid with the liquid bulk, colder than the top, when the liquid is sloshing.This movement changes the saturation conditions and yields pressure and temperatureevolutions in the ullage volume of the tank. Observations on Ariane 5 flightsshowed that the first asymmetric mode was mainly excited during this first phase ofascension. Simple models such as the pendulum model are used to simulate the dynamicbehaviour of this mode. Its stability is also investigated through lateral andlongitudinal excitations.The thermodynamics of the system in the tank can be modelled by a one-dimensionalmodel. Based on an experiment with liquid nitrogen in a cylindrical tank, the heatfluxes are calculated and plugged in the model. The pressurisation phase is first simulatedthrough self and active pressurisation to estimate the importance of the thicknessof the thermal boundary layer. Sloshing is included in the model thermodynamicallyby considering a more important conductive coefficient in the sloshing layer. The amplitudeof sloshing can be linked to the new conduction term thanks to a literaturerelation but it underestimates the actual magnitude of the pressure drop. The modelis extended to a two-dimensional model to take into account the sloshing mechanically,knowing the velocities from the pendulum model. It is found to be not accurate mostlydue to the turbulence of the sloshing layer not considered in the model.The models give in any case important results regarding the influence of some tankparameters such as the ullage volume, the importance of the pressurisation phase anda necessary distinction between chaotic and stable sloshing. From this, the data ofAriane 5 flights is analysed. The flights are divided in three families according to themagnitude of the pressure drop measured. A fault tree analysis is performed to ruleout possible influences and put forward a theory on the creux eventually.</p>

corrected abstract:
<p>In the frame of the improvement of the performances for Ariane 5, an analysis is carried out to explain the pressure drop observed in the ascent phase of some flights in the liquid hydrogen (LH<sub>2</sub>) tank of the upper stage. This stage is mainly idle until the rocket is out of the atmosphere but is submitted to important excitation throughout the ascent phase in the atmosphere. Due to excitation, the liquid contained in the tank moves and breaks the thermodynamic equilibrium. This movement, sloshing is identified as the most likely cause of the pressure drop observed. It is investigated in this thesis to understand how exactly it impacts the thermodynamic equilibrium in the tank.</p><p>The pressure drop called <em>creux PGRH</em> can be explained by the mixing of the top of the liquid with the liquid bulk, colder than the top, when the liquid is sloshing. This movement changes the saturation conditions and yields pressure and temperature evolutions in the ullage volume of the tank. Observations on Ariane 5 flights showed that the first asymmetric mode was mainly excited during this first phase of ascension. Simple models such as the pendulum model are used to simulate the dynamic behaviour of this mode. Its stability is also investigated through lateral and longitudinal excitations.</p><p>The thermodynamics of the system in the tank can be modelled by a one-dimensional model. Based on an experiment with liquid nitrogen in a cylindrical tank, the heat fluxes are calculated and plugged in the model. The pressurisation phase is first simulated through self and active pressurisation to estimate the importance of the thickness of the thermal boundary layer. Sloshing is included in the model thermodynamically by considering a more important conductive coefficient in the sloshing layer. The amplitude of sloshing can be linked to the new conduction term thanks to a literature relation but it underestimates the actual magnitude of the pressure drop. The model is extended to a two-dimensional model to take into account the sloshing mechanically, knowing the velocities from the pendulum model. It is found to be not accurate mostly due to the turbulence of the sloshing layer not considered in the model.</p><p>The models give in any case important results regarding the influence of some tank parameters such as the ullage volume, the importance of the pressurisation phase and a necessary distinction between chaotic and stable sloshing. From this, the data of Ariane 5 flights is analysed. The flights are divided in three families according to the magnitude of the pressure drop measured. A fault tree analysis is performed to rule out possible influences and put forward a theory on the creux eventually.</p>
----------------------------------------------------------------------
In diva2:783982 - spaces missing in title:
"Evaluating the effectiveness of collisionavoidance functions using state-of-the-artsimulation tools for vehicle dynamics"
==>
"Evaluating the effectiveness of collision avoidance functions using state-of-the-art simulation tools for vehicle dynamics"

abstract is:
<p>The main goal of this work is to gain knowledge of how and to what extent state-of-the-artsimulation tools can be used in a conceptual development phase for vehicle dynamics control atVolvo Car Corporation (VCC).The first part of the thesis deals with an evaluation of vehicle dynamics simulation tools and theiruses. The three simulation tools selected for the study, namely Mechanical Simulation CarSim 8.2.1,IPG CarMaker 4.0.5, and VI-Grade CarRealTime V14, are briefly described and discussed. In order toevaluate and compare these tools with respect to application for vehicle dynamics control, a criterialist is developed covering aspects such as tool requirements and intended usage. Based on thecriteria list and certain identified drawbacks, a ranking of the tools is made possible. Furthermore,the process of developing vehicle models for the different tools is discussed in detail, along with theprocedure of validating the vehicle models.In the second part, the concept of Collision Avoidance Driver Assistance (CADA) function isintroduced and possible approaches for developing CADA functions are discussed in brief. It isimportant to note that the CADA functions in this work are based on cornering the vehicle i.e.maneuvering around the threat, rather than solely reducing vehicle speed. A number ofimplementations of the functions are developed in Simulink. A frequency analysis of a simplifiedlinear vehicle model is performed to investigate the influence of steering, differential braking, andtheir combination on the resultant lateral displacement of the vehicle during an evasive maneuver.The developed CADA functions are then simulated using the vehicle simulation tools. Two specificmetrics - Lateral Displacement gain and DeltaX - are formulated to evaluate the effectiveness of theCADA functions. Based on these metrics, the assistance obtained due to the functions for a specificevasive maneuver is compared.From the evaluation process of the three tools, two were considered suitable for the purpose ofsimulating collision avoidance functions. The evaluation of the CADA functions demonstrates thatcombined assistive steering with differential braking provides considerable assistance in order toavoid collisions. The simulation results also present interesting trends which provide a usefuldirection regarding the conditions for intervention by such collision avoidance functions during anevasive maneuver. The use of simulation tools makes it possible to observe these trends and utilizethem in the development process of the functions.</p>

corrected abstract:
<p>The main goal of this work is to gain knowledge of how and to what extent state-of-the-art simulation tools can be used in a conceptual development phase for vehicle dynamics control at Volvo Car Corporation (VCC).</p><p>The first part of the thesis deals with an evaluation of vehicle dynamics simulation tools and their uses. The three simulation tools selected for the study, namely Mechanical Simulation CarSim 8.2.1, IPG CarMaker 4.0.5, and VI-Grade CarRealTime V14, are briefly described and discussed. In order to evaluate and compare these tools with respect to application for vehicle dynamics control, a criteria list is developed covering aspects such as tool requirements and intended usage. Based on the criteria list and certain identified drawbacks, a ranking of the tools is made possible. Furthermore, the process of developing vehicle models for the different tools is discussed in detail, along with the procedure of validating the vehicle models.</p><p>In the second part, the concept of Collision Avoidance Driver Assistance (CADA) function is introduced and possible approaches for developing CADA functions are discussed in brief. It is important to note that the CADA functions in this work are based on cornering the vehicle i.e. maneuvering around the threat, rather than solely reducing vehicle speed. A number of implementations of the functions are developed in Simulink. A frequency analysis of a simplified linear vehicle model is performed to investigate the influence of steering, differential braking, and their combination on the resultant lateral displacement of the vehicle during an evasive maneuver. The developed CADA functions are then simulated using the vehicle simulation tools. Two specific metrics - Lateral Displacement gain and DeltaX - are formulated to evaluate the effectiveness of the CADA functions. Based on these metrics, the assistance obtained due to the functions for a specific evasive maneuver is compared.</p><p>From the evaluation process of the three tools, two were considered suitable for the purpose of simulating collision avoidance functions. The evaluation of the CADA functions demonstrates that combined assistive steering with differential braking provides considerable assistance in order to avoid collisions. The simulation results also present interesting trends which provide a useful direction regarding the conditions for intervention by such collision avoidance functions during an evasive maneuver. The use of simulation tools makes it possible to observe these trends and utilize them in the development process of the functions.</p>
----------------------------------------------------------------------
In diva2:1527803 abstract is:
<p>Multi-body simulations are given more emphasis over physical tests owing toenvironmental, financial, and time requirements in the competitive automotive industry. Thus,it is imperative to develop models to accurately predict and analyse the system's behaviour.This thesis focuses on developing an air suspension model with Electronic Level Control thathas the ability to communicate with other air springs in a pneumatic circuit thus replicating thepneumatic connection in actual truck and regulate the ride height of the vehicle.To accomplish this, a comprehensive literature study is performed to identify an effectivecontrol variable to manipulate the air springs. This is done by understanding the working andthermodynamic principles of air suspension, understanding various Scania pneumaticconfigurations, and decrypting the working of the Electronic Level Control.Different methods for implementing the model through the identified control variable arediscussed. A brief explanation of the necessary physical tests performed to validate the modelis given. An extensive description of implementation of the static and dynamic model inADAMS through command batch script coding is provided.The developed static model is validated by comparing the results from simulations and the testdata. The axle weights have an error of maximum 6% and the pressure in the air springs havean error of maximum 9% which can be owed to neglection of hysteresis in the air springcharacteristics and using mean values to compare the data. The dynamic model is validated byaltering the ride height level and observing the response of the model. The results obtainedindicate the developed Electronic Level Control is able to regulate the ride height at the desiredlevel.The robustness of the model is validated by modifying the developed model for longitudinalpneumatic connection and for a truck with trailer model. The results indicate the developedmodel is capable to perform satisfactorily and conform to the Scania tolerance limits.Thus, an appropriate control variable for the air springs model is identified. Static and dynamicmodel to identify the suitable pressure in the air springs and thus, the force in the air springs isdeveloped which helped in drastically reducing the manual iterative work that was required.</p>

corrected abstract:
<p>Multi-body simulations are given more emphasis over physical tests owing to environmental, financial, and time requirements in the competitive automotive industry. Thus, it is imperative to develop models to accurately predict and analyse the system's behaviour.</p><p>This thesis focuses on developing an air suspension model with Electronic Level Control that has the ability to communicate with other air springs in a pneumatic circuit thus replicating the pneumatic connection in actual truck and regulate the ride height of the vehicle.</p><p>To accomplish this, a comprehensive literature study is performed to identify an effective control variable to manipulate the air springs. This is done by understanding the working and thermodynamic principles of air suspension, understanding various Scania pneumatic configurations, and decrypting the working of the Electronic Level Control.</p><p>Different methods for implementing the model through the identified control variable are discussed. A brief explanation of the necessary physical tests performed to validate the model is given. An extensive description of implementation of the static and dynamic model in ADAMS through command batch script coding is provided.</p><p>The developed static model is validated by comparing the results from simulations and the test data. The axle weights have an error of maximum 6% and the pressure in the air springs have an error of maximum 9% which can be owed to neglection of hysteresis in the air spring characteristics and using mean values to compare the data. The dynamic model is validated by altering the ride height level and observing the response of the model. The results obtained indicate the developed Electronic Level Control is able to regulate the ride height at the desired level.</p><p>The robustness of the model is validated by modifying the developed model for longitudinal pneumatic connection and for a truck with trailer model. The results indicate the developed model is capable to perform satisfactorily and conform to the Scania tolerance limits.</p><p>Thus, an appropriate control variable for the air springs model is identified. Static and dynamic model to identify the suitable pressure in the air springs and thus, the force in the air springs is developed which helped in drastically reducing the manual iterative work that was required.</p>
----------------------------------------------------------------------
In diva2:1465517 - missing space in the tttle:
"Multi-Objective Optimization of Torque Distribution inHybrid Vehicles"
==>
"Multi-Objective Optimization of Torque Distribution in Hybrid Vehicles"

abstract is:
<p>Electrification is one of the mega-trends in the transportation and automotive industry today. Boththe alarming environmental conditions and the ever decreasing fuel reserves are driving the shifttowards hybrid, all electric and alternative fuel source vehicles. This thesis work is another smallstep towards studying, addressing and handling this issue while also laying the groundwork for developingand moving towards more efficient and commercially viable vehicles.This thesis work aims at investigating the trade-off offered by optimal control techniques betweenenergy consumption and reference tracking for torque allocation to the various actuators available topropel a hybrid electric vehicle. The particular vehicle under consideration has two electric motorsat the rear wheels and an internal combustion engine along with an integrator starter generatordriving the front wheels. The torque allocation problem is originally solved by proposing a one stageoptimization strategy (OSOS) that takes into account actuator limits, losses, and objectives throughconstraints. The performance of this formulation is presented over two simulated test tracks on apareto front where the advantage on relaxing complete reference tracking becomes visible. Next,two new formulations each as a two stage optimization strategies (TSOS) are proposed, the mainobjective being to split the original formulation into two parts. One addressing energy optimalityand the other addressing reference tracking of total wheel torque and yaw moment request fulfilment.These formulations are then similarly investigated and presented in comparison with the originalformulation. In developing the formulations, an assumption about the loss models is made andthe problem size of the second stage quadratic program is significantly reduced. The problems areappropriately scaled and made mathematically robust to handle the constraints and inputs in theoperating range. As reference tracking for the vehicle is split into lateral and longitudinal torquerequests from the vehicle, this becomes a multi-objective optimization problem. To further studythe behaviour of these formulations, they are given constant inputs and simulated over a single timestep. The effect of changing hybridization level, i.e, the amount of electrical energy used comparedto fuel energy on the behaviour of these formulations is also explored. One of the effects of the twostage formulations was the confinement of solutions within a reasonable error for the majority ofchosen weights due to the energy considerations in the first stage. The proposed formulations wereable to generate results close but not equal to the original formulation on the pareto front. Anotherfinding was that due to the implementation of two actuators at the rear of the vehicle, a desired yawrate could be achieved at no additional energy cost because of regenerative and propulsive torquesgenerated respectively on either side of rear axle for torque vectoring. Furthermore with a dedicatedsolver, the TSOS could present an interesting alternative to enhance independent development invehicle dynamics control and energy management of the vehicle.</p>


corrected abstract:
<p>Electrification is one of the mega-trends in the transportation and automotive industry today. Both the alarming environmental conditions and the ever decreasing fuel reserves are driving the shift towards hybrid, all electric and alternative fuel source vehicles. This thesis work is another small step towards studying, addressing and handling this issue while also laying the groundwork for developing and moving towards more efficient and commercially viable vehicles.</p><p>This thesis work aims at investigating the trade-off offered by optimal control techniques between energy consumption and reference tracking for torque allocation to the various actuators available to propel a hybrid electric vehicle. The particular vehicle under consideration has two electric motors at the rear wheels and an internal combustion engine along with an integrator starter generator driving the front wheels. The torque allocation problem is originally solved by proposing a one stage optimization strategy (OSOS) that takes into account actuator limits, losses, and objectives through constraints. The performance of this formulation is presented over two simulated test tracks on a pareto front where the advantage on relaxing complete reference tracking becomes visible. Next, two new formulations each as a two stage optimization strategies (TSOS) are proposed, the main objective being to split the original formulation into two parts. One addressing energy optimality and the other addressing reference tracking of total wheel torque and yaw moment request fulfilment.</p><p>These formulations are then similarly investigated and presented in comparison with the original formulation. In developing the formulations, an assumption about the loss models is made and the problem size of the second stage quadratic program is significantly reduced. The problems are appropriately scaled and made mathematically robust to handle the constraints and inputs in the operating range. As reference tracking for the vehicle is split into lateral and longitudinal torque requests from the vehicle, this becomes a multi-objective optimization problem. To further study the behaviour of these formulations, they are given constant inputs and simulated over a single time step. The effect of changing hybridization level, i.e, the amount of electrical energy used compared to fuel energy on the behaviour of these formulations is also explored. One of the effects of the two stage formulations was the confinement of solutions within a reasonable error for the majority of chosen weights due to the energy considerations in the first stage. The proposed formulations were able to generate results close but not equal to the original formulation on the pareto front. Another finding was that due to the implementation of two actuators at the rear of the vehicle, a desired yaw rate could be achieved at no additional energy cost because of regenerative and propulsive torques generated respectively on either side of rear axle for torque vectoring. Furthermore with a dedicated solver, the TSOS could present an interesting alternative to enhance independent development in vehicle dynamics control and energy management of the vehicle.</p>
----------------------------------------------------------------------
In diva2:1327792 - note: no full text in DiVA

abstract is:
<p>Capacitive deionization is an emerging environmentally friendly technique for waterdesalination that has been getting increasing attention in recent years. In thistechnique, water passes through a cell with nanostructured porous carbon electrodeswhich have a high surface area. When a potential is applied to these electrodes, theelectrodes adsorb the salt ion in the water stream, which results in the production offresh water. While the technique is promising, it still needs to be developed further tosee more widespread use, and modeling can be an essential tool during investigationsto expedite these developments. To make modeling more accessible, it is crucial thatmodels are developed that can predict the process performance transparently andstraightforwardly. This master thesis encompasses three submitted papers. A model,termed the Dynamic Langmuir model, is developed based on a few fundamentalmacroscopic principles. The model is more straightforward and more transparent thanprevious models yet could accurately describe key concepts including ion adsorptionand charge efficiency, in both equilibrium and dynamic settings and for variouselectrode materials and cell structures. The model is shown to accurately predictperformance over crucial parameters including the applied voltage, flow rate of thewater through the cell, inlet concentration, mixtures of ions in the water, varyingelectrode asymmetry and electrode pre-charging. The usefulness of the model isfurther demonstrated by using it to optimize the time spent absorbing salt versuscleaning the cell. This improved ion-removal efficiency by 31 % compared to doing fullsaturation/regeneration. To further aid the goal of making modeling more accessible,a software program has been developed and provided as open source that can bedirectly used to implement the model, without requiring extensive knowledge aboutthe theory, or lots of experiments to set up. Also, an automated experimental setup forcontinuous and stable CDI operation was developed that could provide data for futuremodeling. Finally, this thesis additionally includes an extensive theory section to givea comprehensive introduction to the capacitive-deionization field. In conclusion, asimple and transparent model has been developed, able to accurately describe howcritical concepts in capacitive deionization vary over a wide range of operationalparameters. It is hoped that this work can make the modeling of capacitivedeionization more accessible.</p>

corrected abstract:
<p>Capacitive deionization is an emerging environmentally friendly technique for water desalination that has been getting increasing attention in recent years. In this technique, water passes through a cell with nanostructured porous carbon electrodes which have a high surface area. When a potential is applied to these electrodes, the electrodes adsorb the salt ion in the water stream, which results in the production of fresh water. While the technique is promising, it still needs to be developed further to see more widespread use, and modeling can be an essential tool during investigations to expedite these developments. To make modeling more accessible, it is crucial that models are developed that can predict the process performance transparently and straightforwardly. This master thesis encompasses three submitted papers. A model, termed the Dynamic Langmuir model, is developed based on a few fundamental macroscopic principles. The model is more straightforward and more transparent than previous models yet could accurately describe key concepts including ion adsorption and charge efficiency, in both equilibrium and dynamic settings and for various electrode materials and cell structures. The model is shown to accurately predict performance over crucial parameters including the applied voltage, flow rate of the water through the cell, inlet concentration, mixtures of ions in the water, varying electrode asymmetry and electrode pre-charging. The usefulness of the model is further demonstrated by using it to optimize the time spent absorbing salt versus cleaning the cell. This improved ion-removal efficiency by 31 % compared to doing fullsaturation/regeneration. To further aid the goal of making modeling more accessible, a software program has been developed and provided as open source that can be directly used to implement the model, without requiring extensive knowledge about the theory, or lots of experiments to set up. Also, an automated experimental setup for continuous and stable CDI operation was developed that could provide data for future modeling. Finally, this thesis additionally includes an extensive theory section to give a comprehensive introduction to the capacitive-deionization field. In conclusion, a simple and transparent model has been developed, able to accurately describe how critical concepts in capacitive deionization vary over a wide range of operational parameters. It is hoped that this work can make the modeling of capacitive deionization more accessible.</p>
----------------------------------------------------------------------
In diva2:412700 abstract is:
<p>This diploma thesis captures the three-dimensional implementation of noise-reducing high-liftsystems. A parametric CAD model is developed for the FNG aircraft and different high-lift configurationsare built up. In the course of research, these configurations are designed based onformerly obtained two-dimensional results of DLR’s LEISA project featuring the design of a verylong chord slat (VLCS), whose slat shape resulted in a favourable aeroacoustic behaviour at noiserelevantapproach conditions. The high-lift systems derived in this thesis differ in the spanwisevariation of the slat geometry planform as well as in the applied high-lift settings described bygap, overlap and deflection angle.The aerodynamic performance is computed via CFD RANS simulations and the results arecompared to a reference high-lift system of the FNG aircraft, which has been designed in previousstudies. The observed CFD results are further evaluated in the reference wing section of the FNGaircraft in order to display the agreement between the implemented 3D high-lift configurationsand the 2D LEISA reference data. Besides the aerodynamic performance, aeroacoustic aspects arealso considered in this diploma thesis. By means of the obtained CFD results, indirect statementsabout the success of the 3D low-noise implementation approach are made.The geometrical concordance of the derived reference wing section of the 3D CAD model isfound in general to be very high in comparison to the 2D-optimized LEISA design wing section.With regard to the observed pressure distributions of the initial four designed high-lift systemshowever, small geometry deviations are noticed to affect the obtained pressure distributions in asignificantly unintended way. The requirements of a low-noise high-lift system are thus not metfor these high-lift configurations. In the 3D implementation, the 2D-optimized slat settings haveto be modified in order to maintain the favourable aeroacoustic behaviour of the 2D considerations.Based on a reduced slat deflection angle, a further derived 3D VLCS high-lift system isobtained to match the 2D-optimized pressure distributions in the reference wing section more accurately.A significant pressure increase at the VLCS trailing edge is noticed for this configuration,which shows the noise-reducing potential of the derived VLCS device. However, the aerodynamicdegradations obtained for the designed low-noise high-lift system are found to be too high in orderto still provide improved aeroacoustic behaviour during conditions of increased approachspeed. A 3D noise-reducing high-lift system is therefore not achieved, although the 2D-optimizedLEISA pressure distributions are well captured in the reference wing section of the implemented3D high-lift system featuring modified high-lift setting parameters.</p>


corrected abstract:
<p>This diploma thesis captures the three-dimensional implementation of noise-reducing high-lift systems. A parametric CAD model is developed for the FNG aircraft and different high-lift configurations are built up. In the course of research, these configurations are designed based on formerly obtained two-dimensional results of DLR’s LEISA project featuring the design of a very long chord slat (VLCS), whose slat shape resulted in a favourable aeroacoustic behaviour at noise relevant approach conditions. The high-lift systems derived in this thesis differ in the spanwise variation of the slat geometry plan form as well as in the applied high-lift settings described by gap, overlap and deflection angle.</p><p>The aerodynamic performance is computed via CFD RANS simulations and the results are compared to a reference high-lift system of the FNG aircraft, which has been designed in previous studies. The observed CFD results are further evaluated in the reference wing section of the FNG aircraft in order to display the agreement between the implemented 3D high-lift configurations and the 2D LEISA reference data. Besides the aerodynamic performance, aeroacoustic aspects are also considered in this diploma thesis. By means of the obtained CFD results, indirect statements about the success of the 3D low-noise implementation approach are made.</p><p>The geometrical concordance of the derived reference wing section of the 3D CAD model is found in general to be very high in comparison to the 2D-optimized LEISA design wing section. With regard to the observed pressure distributions of the initial four designed high-lift systems however, small geometry deviations are noticed to affect the obtained pressure distributions in a significantly unintended way. The requirements of a low-noise high-lift system are thus not met for these high-lift configurations. In the 3D implementation, the 2D-optimized slat settings have to be modified in order to maintain the favourable aeroacoustic behaviour of the 2D considerations. Based on a reduced slat deflection angle, a further derived 3D VLCS high-lift system is obtained to match the 2D-optimized pressure distributions in the reference wing section more accurately. A significant pressure increase at the VLCS trailing edge is noticed for this configuration, which shows the noise-reducing potential of the derived VLCS device. However, the aerodynamic degradations obtained for the designed low-noise high-lift system are found to be too high in order to still provide improved aeroacoustic behaviour during conditions of increased approach speed. A 3D noise-reducing high-lift system is therefore not achieved, although the 2D-optimized LEISA pressure distributions are well captured in the reference wing section of the implemented 3D high-lift system featuring modified high-lift setting parameters.</p>
----------------------------------------------------------------------
In diva2:408831 abstract is:
<p>This thesis is about teachers who share knowledge through lektion.se. Lektion.se is anInternet site and started in 2003 by three teachers. It gives its members an opportunity toshare ideas for lessons, discuss interesting topics and give advice to other members. Thethesis focuses on how the teachers define and use lection.se for sharing knowledge and howthe site can be improved concerning knowledge sharing. To investigate this, seven interviewshave been conducted with teachers in and around Stockholm and a questionnaire has beenpublished on lektion.se. I have also done an observation on how the members act on the site’sforum and the lesson database.The teachers in the study define the site depending on which functions they use and how theyuse them. The teachers who only use the lessons database or forum to get access to otherteacher’s ideas, often see the site as a collection of tips. The teachers who use the lessondatabase or forum for both getting other teacher’s ideas and sharing their own ideas often seethe site as a community of practice for teachers.The use of the functions on the site varies. It is mainly the lesson database and the forum thatis used. Only 1 % of the members share their knowledge with other members. But almostevery participant in the study has used some other teacher’s advice or lesson plan. A numberof barriers and motives for sharing knowledge on the site have been identified in the study.The barriers are both personal and organizational. Cowardice, fear and a feeling ofinadequate ideas are some of the personal barriers identified in the study. A lack of time andtechnical problems are examples of organizational barriers. The knowledge sharing amongthe teachers are motivated by own needs and altruism. Through knowledge sharing, they getfeedback on their ideas and facilitate other teacher’s work.To maintain and, hopefully, increase the sharing of knowledge, the number of barriers forsharing knowledge must be minimized. It can be done by creating more opportunities forcollaboration and that the functions in the site will encourage sharing knowledge. It is alsorequired that the teachers create a reflecting culture where knowledge sharing are consideredto be something natural.</p>


corrected abstract:
<p>This thesis is about teachers who share knowledge through lektion.se. Lektion.se is an Internet site and started in 2003 by three teachers. It gives its members an opportunity to share ideas for lessons, discuss interesting topics and give advice to other members. The thesis focuses on how the teachers define and use lection.se for sharing knowledge and how the site can be improved concerning knowledge sharing. To investigate this, seven interviews have been conducted with teachers in and around Stockholm and a questionnaire has been published on lektion.se. I have also done an observation on how the members act on the site’s forum and the lesson database.</p><p>The teachers in the study define the site depending on which functions they use and how they use them. The teachers who only use the lessons database or forum to get access to other teacher’s ideas, often see the site as a collection of tips. The teachers who use the lesson database or forum for both getting other teacher’s ideas and sharing their own ideas often see the site as a community of practice for teachers.</p><p>The use of the functions on the site varies. It is mainly the lesson database and the forum that is used. Only 1 % of the members share their knowledge with other members. But almost every participant in the study has used some other teacher’s advice or lesson plan. A number of barriers and motives for sharing knowledge on the site have been identified in the study. The barriers are both personal and organizational. Cowardice, fear and a feeling of inadequate ideas are some of the personal barriers identified in the study. A lack of time and technical problems are examples of organizational barriers. The knowledge sharing among the teachers are motivated by own needs and altruism. Through knowledge sharing, they get feedback on their ideas and facilitate other teacher’s work.</p><p>To maintain and, hopefully, increase the sharing of knowledge, the number of barriers for sharing knowledge must be minimized. It can be done by creating more opportunities for collaboration and that the functions in the site will encourage sharing knowledge. It is also required that the teachers create a reflecting culture where knowledge sharing are considered to be something natural.</p>
----------------------------------------------------------------------
In diva2:1804716 abstract is:
<p>The thesis examines the prospects of using the superconductor NbN as the gatemetal for an InP HEMT. A HEMT or High Electron Mobility Transistor is aheterostructure transistor engineered to reach very high electron mobility. InPHEMTs are used as cryogenic Low Noise Amplifiers (LNAs), which have increasedin demand as quantum computing is scaling up. A superconducting NbN gate isof interest as it has the potential to decrease the amount of noise generated by theHEMT LNAs.A gate width dependence for both the transconductance (gm) and the large-signal HEMT channel resistance (RON ) of the NbN HEMTs at room temperaturehas been observed, and the first goal pf the thesis is to determine the originof the dependence. Moreover, the measured RF characteristics of the NbNdevices tend to deviate from the norm of a standard HEMT, and the secondgoal is to understand why. The third goal is to determine if the NbN gate stayssuperconducting at cryogenic temperatures or if self-heating from the channelduring DC operations will break superconductivity.In the thesis, it was possible to recreate the observed gate width dependence withnew devices, and additionally, a gate width dependence in the threshold voltageis observed. The origin of width dependence is most likely related to the straincreated by the NbN gate. At DC, extremely high peaks in the transconductanceare observed, which is most likely related to impact ionization and a subsequentincrease in hole trapping caused by the introduction of the NbN gate.Using simulations, it was possible to accurately recreate the observed deviantbehaviour, likely associated with the NbN gate’s high capacitance, inductance andresistance at room temperature. The high capacitance is likely partly related tosome NbN gates of the HEMTs being broken. Finally, the HEMT can operatein DC at 2 K with VG = 0.3 V and a maximum VD = 0.1 V before self-heatingfrom the channel will break the NbN superconductivity of the gate. This is oneof the critical conclusions of the work because it shows that a superconductinggate electrode can be implemented and functional in a high-performance HEMTdevice structure and under realistic operating bias conditions. As long as it can bedemonstrated that the superconductivity does not break when operating in RF, aNbN gate is a promising avenue to increase the noise performance of the cryogenicHEMT.</p>


corrected abstract:
<p>The thesis examines the prospects of using the superconductor NbN as the gate metal for an InP HEMT. A HEMT or High Electron Mobility Transistor is a heterostructure transistor engineered to reach very high electron mobility. InP HEMTs are used as cryogenic Low Noise Amplifiers (LNAs), which have increased in demand as quantum computing is scaling up. A superconducting NbN gate is of interest as it has the potential to decrease the amount of noise generated by the HEMT LNAs.</p><p>A gate width dependence for both the transconductance (<em>g<sub>m</sub></em>) and the large-signal HEMT channel resistance (<em>R<sub>ON</sub></em>) of the NbN HEMTs at room temperature has been observed, and the first goal pf the thesis is to determine the origin of the dependence. Moreover, the measured RF characteristics of the NbN devices tend to deviate from the norm of a standard HEMT, and the second goal is to understand why. The third goal is to determine if the NbN gate stays superconducting at cryogenic temperatures or if self-heating from the channel during DC operations will break superconductivity.</p><p>In the thesis, it was possible to recreate the observed gate width dependence with new devices, and additionally, a gate width dependence in the threshold voltage is observed. The origin of width dependence is most likely related to the strain created by the NbN gate. At DC, extremely high peaks in the transconductance are observed, which is most likely related to impact ionization and a subsequent increase in hole trapping caused by the introduction of the NbN gate.</p><p>Using simulations, it was possible to accurately recreate the observed deviant behaviour, likely associated with the NbN gate’s high capacitance, inductance and resistance at room temperature. The high capacitance is likely partly related to some NbN gates of the HEMTs being broken. Finally, the HEMT can operate in DC at 2 K with <em>V<sub>G</sub> = 0.3</em> V and a maximum <em>V<sub>D</sub> = 0.1</em> V before self-heating from the channel will break the NbN superconductivity of the gate. This is one of the critical conclusions of the work because it shows that a superconducting gate electrode can be implemented and functional in a high-performance HEMT device structure and under realistic operating bias conditions. As long as it can be demonstrated that the superconductivity does not break when operating in RF, a NbN gate is a promising avenue to increase the noise performance of the cryogenic HEMT.</p>
----------------------------------------------------------------------
In diva2:1698151 abstract is:
<p>An extensive technological shift is currently taking place to mitigate climate changeand this trend is particularly noticeable in the transport sector. This is interestingfrom an acoustical perspective, since it changes the noise environment in society.For example, a study in Gothenburg has shown that a complete electrification ofthe road traffic would reduce the noise levels by between 2 and 5 dB(A). In Sweden,noise emissions are calculated with a calculation model from 1996, called the Nordiskberäkningsmodell (Nordiska). Given the age of the model it is reasonable to investigate whether Sweden should change completely to the EU-common calculationmodel Common NOise aSSessment methOdS (CNOSSOS), since it is mandatory touse for national noise mapping.</p><p>This master thesis has performed a computation analysis to compare and discussdifferences between CNOSSOS and Nordiska, to contribute to answering the question whether Sweden should change to CNOSSOS (or perhaps another model). Theresults show that CNOSSOS overall computes higher noise levels than Nordiska andthat the differences between them increase linearly with distance. Farthest from thenoise source the differences are up to 5 dB(A) for the road case and 9 dB(A) forthe railway case. In other words, the differences are larger for the railway trafficmodels than they are for the road traffic models, which is thought to be a result ofthe complexity of the CNOSSOS railway model. Another interesting phenomenonis that the differences behind buildings between the models are different for roadand railway traffic, which can be explained by the fact that the screening effects inNordiska’s road and railway models are different.</p><p>My conclusion is that CNOSSOS is unsuitable for domestic calculations of noiseemissions. The model does not align with Swedish legislation and there is uncertaintydue to the fact that the differences between the CNOSSOS and Nordiska road andrailway models are different in size. Moreover, CNOSSOS railway model requires alot of computational power, which can delay and increase the costs of noise mappingor reduce the accuracy of the results. However, additional work is needed in whicheach calculation model is compared with measurements in situ to see which modelbest describes reality. If the conclusion thereafter is that CNOSSOS still is not asuitable option, it could be examined whether it is possible to create an updatedversion of Nord2000 (another Nordic calculation model used e.g. in Denmark) toobtain a calculation model that is more suitable for future traffic conditions.</p>


corrected abstract:
<p>An extensive technological shift is currently taking place to mitigate climate change and this trend is particularly noticeable in the transport sector. This is interesting from an acoustical perspective, since it changes the noise environment in society. For example, a study in Gothenburg has shown that a complete electrification of the road traffic would reduce the noise levels by between 2 and 5 dB(A). In Sweden, noise emissions are calculated with a calculation model from 1996, called the <em lang="sv">Nordisk beräkningsmodell</em> (Nordiska). Given the age of the model it is reasonable to investigate whether Sweden should change completely to the EU-common calculation model <em>Common NOise aSSessment methOdS</em> (CNOSSOS), since it is mandatory to use for national noise mapping.</p><p>This master thesis has performed a computation analysis to compare and discuss differences between CNOSSOS and Nordiska, to contribute to answering the question whether Sweden should change to CNOSSOS (or perhaps another model). The results show that CNOSSOS overall computes higher noise levels than Nordiska and that the differences between them increase linearly with distance. Farthest from the noise source the differences are up to 5 dB(A) for the road case and 9 dB(A) for the railway case. In other words, the differences are larger for the railway traffic models than they are for the road traffic models, which is thought to be a result of the complexity of the CNOSSOS railway model. Another interesting phenomenon is that the differences behind buildings between the models are different for road and railway traffic, which can be explained by the fact that the screening effects in Nordiska’s road and railway models are different.</p><p>My conclusion is that CNOSSOS is unsuitable for domestic calculations of noise emissions. The model does not align with Swedish legislation and there is uncertainty due to the fact that the differences between the CNOSSOS and Nordiska road and railway models are different in size. Moreover, CNOSSOS railway model requires a lot of computational power, which can delay and increase the costs of noise mapping or reduce the accuracy of the results. However, additional work is needed in which each calculation model is compared with measurements in situ to see which model best describes reality. If the conclusion thereafter is that CNOSSOS still is not a suitable option, it could be examined whether it is possible to create an updated version of Nord2000 (another Nordic calculation model used e.g. in Denmark) to obtain a calculation model that is more suitable for future traffic conditions.</p>
----------------------------------------------------------------------
In diva2:1465539 abstract is:
<p>Throughout the history of motor vehicles, the tyres have always been consideredas one of the most important components of the vehicle due to their interactionwith the road. One important aspect is the wheel alignment, with the purposeto adjust the static wheel angles that are essential for many reasons, such assafety and fuel consumption for instance. Despite the numerous methods forwheel angle measurements, there seems to be no existing technical solutionbased on computer vision, that is suitable for residential use, regarding bothcost and size of the equipment. The study aims to investigate the feasibility ofsuch a system.The proposed system is based on planar fiducial markers called ArUco.From images or video frames of the marker, the pose of the marker can beestimated. Thus, by placing such markers on the ground, on the wheel andon the vehicle, the estimated pose of the markers can be used to measure andcalculate the wheel alignment parameters. Only toe and camber angles aremeasured within the scope of this thesis, even if the system has the potential tomeasure other wheel alignment parameters as well.After camera calibration, simplified ArUco marker tests were done by measuringthe known displacement and inclination of a marker with respect to areference marker. The mean absolute error was 030400 and 0:024mm for theinclination angle and displacement, respectively. Furthermore, the toe and camberangles of a vehicle were measured and compared to reference measurementsperformed with a commercial wheel alignment system, giving mean absoluteerrors of 0520 and 0280 for the camber and toe angles, respectively. Despitethe relatively large errors for the toe and camber angle measurements, theresults from the initial inclination and displacement tests show the potential ofthe system. In addition, several error sources and suggestions for improvementcan be identified.As a conclusion, the proposed system can be considered a working firstprototype, which after improvement and optimisation has the potential tobecome a feasible alternative, especially for residential use and for mobileworkshops due to the low cost, size and usability of the system.</p>


corrected abstract:
<p>Throughout the history of motor vehicles, the tyres have always been considered as one of the most important components of the vehicle due to their interaction with the road. One important aspect is the wheel alignment, with the purpose to adjust the static wheel angles that are essential for many reasons, such as safety and fuel consumption for instance. Despite the numerous methods for wheel angle measurements, there seems to be no existing technical solution based on computer vision, that is suitable for residential use, regarding both cost and size of the equipment. The study aims to investigate the feasibility of such a system.</p><p>The proposed system is based on planar fiducial markers called ArUco. From images or video frames of the marker, the pose of the marker can be estimated. Thus, by placing such markers on the ground, on the wheel and on the vehicle, the estimated pose of the markers can be used to measure and calculate the wheel alignment parameters. Only toe and camber angles are measured within the scope of this thesis, even if the system has the potential to measure other wheel alignment parameters as well.</p><p>After camera calibration, simplified ArUco marker tests were done by measuring the known displacement and inclination of a marker with respect to a reference marker. The mean absolute error was 0&deg;3&prime;4&Prime; and 0.024 mm for the inclination angle and displacement, respectively. Furthermore, the toe and camber angles of a vehicle were measured and compared to reference measurements performed with a commercial wheel alignment system, giving mean absolute errors of 0&deg;52&prime; and 0&deg;28&prime; for the camber and toe angles, respectively. Despite the relatively large errors for the toe and camber angle measurements, the results from the initial inclination and displacement tests show the potential of the system. In addition, several error sources and suggestions for improvement can be identified.</p><p>As a conclusion, the proposed system can be considered a working first prototype, which after improvement and optimisation has the potential to become a feasible alternative, especially for residential use and for mobile workshops due to the low cost, size and usability of the system.</p>
----------------------------------------------------------------------
In diva2:1189528 abstract is:
<p>Road accidents have been a persistent cause of death worldwide, and claim millions of lives everyyear. Recent developments in the active safety systems like Electronic Stability Control (ESC) havehelped in reducing these numbers quite signicantly over the years. However, a major challenge forthese systems is to know the friction coecient between the tire and the road, as this value limits theamount of force the tires can generate. Knowledge of the coecient of friction can be used to adaptthe driving style, thereby avoiding interventions by stability control at the limit, making vehiclessafer. However, it is a major challenge within the automotive industry to estimate the coecientof friction accurately, and with sucient availability, as that requires high levels of tire utilization,such that the tire is forced to reach the non-linear range of operation. Such events are very rarein everyday driving, and requires a system induced active excitation of the tires. One such methodthat has been proposed earlier, to carry out an active tire excitation, is by using a simultaneouspropulsive and brake force on front and the rear the axles. However, applying an equal magnitudeof propulsive and brake force results in a force neutral situation at the vehicle level, which forcesthe velocity to be constant, overriding driver acceleration requests. Thus, an active tire excitationmethod was proposed by Volvo Cars, which is able to apply an unequal propulsive and brake forceto the front and the rear axle, such that the driver's acceleration demand can be met, during frictionestimation. However, such an excitation can be dangerous to carry out, if it leads to instability ofthe vehicle.Several methods have been developed to analyze and quantify stability of a vehicle, but detailedanalysis about the stability under forced excitation, for friction estimation, is very rare. This thesiswork investigates the lateral stability of a vehicle undergoing an active tire excitation for frictionestimation. The objective is to understand which vehicle and tire models can be used to quantifythe lateral stability of a vehicle under forced excitation, and how phase portrait methods can beused to develop a stability monitor that is able to indicate the lateral stability of the vehicle undera forced excitation.The results of using a stability monitor during active tire excitation clearly show that it is able toindicate when the vehicle becomes unstable and looses control. It also shows that for slow speedsteady-state maneuvers and straight line maneuvers, the stability monitor does not indicate instability.A comparison between phase portrait based and conventional side-slip based stability monitorsshow the eectiveness and generality of the phase portrait based monitor, which is able to detectinstability earlier than the conventional side-slip based method.</p>

corrected abstract:
<p>Road accidents have been a persistent cause of death worldwide, and claim millions of lives every year. Recent developments in the active safety systems like Electronic Stability Control (ESC) have helped in reducing these numbers quite significantly over the years. However, a major challenge for these systems is to know the friction coefficient between the tire and the road, as this value limits the amount of force the tires can generate. Knowledge of the coefficient of friction can be used to adapt the driving style, thereby avoiding interventions by stability control at the limit, making vehicles safer. However, it is a major challenge within the automotive industry to estimate the coefficient of friction accurately, and with sufficient availability, as that requires high levels of tire utilization, such that the tire is forced to reach the non-linear range of operation. Such events are very rare in everyday driving, and requires a system induced active excitation of the tires. One such method that has been proposed earlier, to carry out an active tire excitation, is by using a simultaneous propulsive and brake force on front and the rear the axles. However, applying an equal magnitude of propulsive and brake force results in a force neutral situation at the vehicle level, which forces the velocity to be constant, overriding driver acceleration requests. Thus, an active tire excitation method was proposed by Volvo Cars, which is able to apply an unequal propulsive and brake force to the front and the rear axle, such that the driver's acceleration demand can be met, during friction estimation. However, such an excitation can be dangerous to carry out, if it leads to instability of the vehicle.</p><p>Several methods have been developed to analyze and quantify stability of a vehicle, but detailed analysis about the stability under forced excitation, for friction estimation, is very rare. This thesis work investigates the lateral stability of a vehicle undergoing an active tire excitation for friction estimation. The objective is to understand which vehicle and tire models can be used to quantify the lateral stability of a vehicle under forced excitation, and how phase portrait methods can be used to develop a stability monitor that is able to indicate the lateral stability of the vehicle under a forced excitation.</p><p>The results of using a stability monitor during active tire excitation clearly show that it is able to indicate when the vehicle becomes unstable and looses control. It also shows that for slow speed steady-state maneuvers and straight line maneuvers, the stability monitor does not indicate instability. A comparison between phase portrait based and conventional side-slip based stability monitors show the effectiveness and generality of the phase portrait based monitor, which is able to detect instability earlier than the conventional side-slip based method.</p>
----------------------------------------------------------------------
In diva2:1087251 - space missing in title:
"Numerical study on hydraulic verticallift gate during shutdown process"
==>
"Numerical study on hydraulic vertical lift gate during shutdown process"

abstract is:
<p>China is undergoing a rapid increase in their development of hydropower.Due to this rapid increase, China has become one of theleading countries in technological solutions regarding the constructionof the hydropower plant. The hydro resources in China are extensivebut building a new power plant is laborious and costly. Upgrading anexisting power plant is therefore of interest. Increasing the volume flowis one way, but this can bring problems to the hydraulic structures.The design of hydraulic gates is crucial for operating a hydropowerplant safely. An emergency gate is especially important as it protectsthe turbine situated downstream of the gate. In this study, a numericalsimulation of the shutdown process of a hydraulic vertical lift gatewas conducted. The simulation was done in two dimensions using theReynolds Navier Stokes Equations (RANS), together with the RNGk ≠ ‘ turbulence model and the Volume of Fluid method (VOF). Thegoal was to extract the pressure distribution around the gate, subsequently,attaining the hydrodynamic forces and also to observe andanalyze the flow surrounding the gate. The simulation was comparedwith existing experimental data, from a 1/18 scale model, for validation.Once the model was validated, eight different cases were tested toimprove the operating conditions. The closing speed of the gate andthe gate bottom angle was altered in order to reduce the down-pullforce and undesirable flow phenomena. It was found that lowering thegate speed to 8.1 m/min would have positive effect. As the gate closesrelatively fast with reduced forces compared to a faster speed, and withless induced vibrations than with a slower speed. Changing the gatebottom angle from 9¶ to 30¶, would also have a considerable positiveinfluence of the lowered gate vibrations. However changing the bottomangle needs to be more thoroughly studied concerning structuraleffects.</p>

corrected abstract:
<p>China is undergoing a rapid increase in their development of hydropower. Due to this rapid increase, China has become one of the leading countries in technological solutions regarding the construction of the hydropower plant. The hydro resources in China are extensive but building a new power plant is laborious and costly. Upgrading an existing power plant is therefore of interest. Increasing the volume flow is one way, but this can bring problems to the hydraulic structures. The design of hydraulic gates is crucial for operating a hydropower plant safely. An emergency gate is especially important as it protects the turbine situated downstream of the gate. In this study, a numerical simulation of the shutdown process of a hydraulic vertical lift gate was conducted. The simulation was done in two dimensions using the Reynolds Navier Stokes Equations (RANS), together with the RNG <em>k - &epsilon;</em> turbulence model and the Volume of Fluid method (VOF). The goal was to extract the pressure distribution around the gate, subsequently, attaining the hydrodynamic forces and also to observe and analyze the flow surrounding the gate. The simulation was compared with existing experimental data, from a 1/18 scale model, for validation. Once the model was validated, eight different cases were tested to improve the operating conditions. The closing speed of the gate and the gate bottom angle was altered in order to reduce the down-pull force and undesirable flow phenomena. It was found that lowering the gate speed to 8.1 m/min would have positive effect. As the gate closes relatively fast with reduced forces compared to a faster speed, and with less induced vibrations than with a slower speed. Changing the gate bottom angle from 9&deg; to 30&deg;, would also have a considerable positive influence of the lowered gate vibrations. However changing the bottom angle needs to be more thoroughly studied concerning structural effects.</p>
----------------------------------------------------------------------
In diva2:1078078 - missing space and ligature in title:
"An Experimental Study on Global TurbineArray Eects in Large Wind Turbine Clusters"
==>
"An Experimental Study on Global Turbine Array Effects in Large Wind Turbine Clusters"

abstract is:
<p>It is well known that the layout of a large wind turbine cluster aects the energyoutput of the wind farm. The individual placement and distances betweenturbines will in uence the wake spreading and the wind velocity decit. Manyanalytical models and simulations have been made trying to calculate this, butstill there is a lack of experimental data to conrm the models. This thesis isdescribing the preparations and the execution of an experiment that has beenconducted using about 250 small rotating turbine models in a wind tunnel. Theturbine models were developed before the experiment and the characteristicswere investigated. The main focus was laid on special eects occurring in largewind turbine clusters, which were named Global Turbine Array Eects.It was shown that the upstream wind was little aected by a large windfarm downstream, even though there existed a small dierence in wind speedbetween the undisturbed free stream and the wind that arrived to the rstturbines in the wind farm. The dierence in wind speed was shown to beunder 1% of the undisturbed free stream. It was also shown that the densityof the wind farm was related to the reduced wind velocity, with a more densefarm the reduction could get up to 2.5% of the undisturbed free stream at theupstream center turbine. Less velocity decit was observed at the upstreamcorner turbines in the wind farm.When using small rotating turbine models some scaling requirements hadto be considered to make the experiment adaptable to reality. It was concludedthat the thrust coecient of the turbine models was the most important parameterwhen analysing the eects. One problem discussed was the low Reynoldsnumber, an eect always present in wind tunnel studies on small wind turbinemodels.A preliminary investigation of a photo measuring technique was also performed,but the technique was not fully developed. The idea was to take oneor a few photos instantaneously and then calculate the individual rotationalspeed of all the turbine models. It was dicult to apply the technique becauseof uctuations in rotational speed during the experiment, therefore thecalculated values could not represent the mean value over a longer time period.</p>

corrected abstract:
<p>It is well known that the layout of a large wind turbine cluster affects the energy output of the wind farm. The individual placement and distances between turbines will influence the wake spreading and the wind velocity deficit. Many analytical models and simulations have been made trying to calculate this, but still there is a lack of experimental data to confirm the models. This thesis is describing the preparations and the execution of an experiment that has been conducted using about 250 small rotating turbine models in a wind tunnel. The turbine models were developed before the experiment and the characteristics were investigated. The main focus was laid on special effects occurring in large wind turbine clusters, which were named Global Turbine Array Effects.</p><p>It was shown that the upstream wind was little affected by a large wind farm downstream, even though there existed a small difference in wind speed between the undisturbed free stream and the wind that arrived to the first turbines in the wind farm. The difference in wind speed was shown to be under 1% of the undisturbed free stream. It was also shown that the density of the wind farm was related to the reduced wind velocity, with a more dense farm the reduction could get up to 2.5% of the undisturbed free stream at the upstream center turbine. Less velocity deficit was observed at the upstream corner turbines in the wind farm.</p><p>When using small rotating turbine models some scaling requirements had to be considered to make the experiment adaptable to reality. It was concluded that the thrust coefficient of the turbine models was the most important parameter when analysing the effects. One problem discussed was the low Reynolds number, an effect always present in wind tunnel studies on small wind turbine models.</p><p>A preliminary investigation of a photo measuring technique was also performed, but the technique was not fully developed. The idea was to take one or a few photos instantaneously and then calculate the individual rotational speed of all the turbine models. It was difficult to apply the technique because of fluctuations in rotational speed during the experiment, therefore the calculated values could not represent the mean value over a longer time period.</p>
----------------------------------------------------------------------
In diva2:783994 abstract is:
<p>The performance and sound emission of a fan is strongly influenced by the installationeffect, which can be defined as the difference between the performance of a fan in ainstallation and the ideal configuration of the same fan. The factors that one should keepin mind while designing a fan system are many, but if some ground rules are followed thenoise can be drastically reduced. The choice of location for the equipment in the buildingis a critical decision and a less ideal location can result in expensive reconstructions and,or that spaces around the fan room can not be used for its initial purpose. A large fan withlower rotation speed will have lower sound emissions then a smaller fan with a higherrotation speed, for the same air flow. The sound and vibration emissions, as well as theenergy consumption of the fan will be at its lowest values when it is at its point ofmaximum efficiency. The outlet configuration of the duct from the fan should be straightand without dampers or ducts silencers that can create turbulence or a higher staticpressure close to the fan, which will decrease the fans performance drastically.The vibration isolation of the fan should be created and specified for the specificinstallation and not solely the fan characteristics. Proposals to predict and measure thestructure-borne sound pressure and transmissions in buildings have recently beenreleased. With a standard over the structure borne sound, the manufactures can declarethe source data for the fans under different operations. This brings that more accuratepredictions and calculations of the structure borne sound from installations can be done.Earlier calculation methods show big deviances between measured and calculated soundpressure in several cases. Above all the spread of the results is large, which makes themethod somewhat unreliable when sound rating spaces, regarding fan room noise.Calculations and predictions of the sound pressure in a fan room can, after proposals ofchange, be done with a deviation of 10 dBfor all frequencies between 63 and 4000 Hz.The method shows a tendency to overrate the sound pressure with a relatively smallspread of the results. It also shows signs to be able to predict the sound pressure in fanrooms with smaller fans then big fan units.Calculations of the increase of sound pressure that occur in the cavity between the floorstructure and the fan unit show big deviations it if is done for specific frequencies.However results show that calculations of the total sound pressure can be done with abetter accuracy.</p>


corrected abstract:
<p>The performance and sound emission of a fan is strongly influenced by the installation effect, which can be defined as the difference between the performance of a fan in a installation and the ideal configuration of the same fan. The factors that one should keep in mind while designing a fan system are many, but if some ground rules are followed the noise can be drastically reduced. The choice of location for the equipment in the building is a critical decision and a less ideal location can result in expensive reconstructions and, or that spaces around the fan room can not be used for its initial purpose. A large fan with lower rotation speed will have lower sound emissions then a smaller fan with a higher rotation speed, for the same air flow. The sound and vibration emissions, as well as the energy consumption of the fan will be at its lowest values when it is at its point of maximum efficiency. The outlet configuration of the duct from the fan should be straight and without dampers or ducts silencers that can create turbulence or a higher static pressure close to the fan, which will decrease the fans performance drastically.</p><p>The vibration isolation of the fan should be created and specified for the specific installation and not solely the fan characteristics. Proposals to predict and measure the structure-borne sound pressure and transmissions in buildings have recently been released. With a standard over the structure borne sound, the manufactures can declare the source data for the fans under different operations. This brings that more accurate predictions and calculations of the structure borne sound from installations can be done.</p><p>Earlier calculation methods show big deviances between measured and calculated sound pressure in several cases. Above all the spread of the results is large, which makes the method somewhat unreliable when sound rating spaces, regarding fan room noise.</p><p>Calculations and predictions of the sound pressure in a fan room can, after proposals of change, be done with a deviation of ±10 dB for all frequencies between 63 and 4000 Hz. The method shows a tendency to overrate the sound pressure with a relatively small spread of the results. It also shows signs to be able to predict the sound pressure in fan rooms with smaller fans then big fan units. Calculations of the increase of sound pressure that occur in the cavity between the floor structure and the fan unit show big deviations it if is done for specific frequencies. However results show that calculations of the total sound pressure can be done with abetter accuracy.</p>
----------------------------------------------------------------------
In diva2:618588 error in title
"Modeling And Analysis Of Fault Conditions In Avehicle With Four In-Wheel Motors"
==>
"Modeling And Analysis Of Fault Conditions In A Vehicle With Four In-Wheel Motors"

abstract is:
<p>A vast expansion is found in the field of automotive electronic systems. The expansion iscoupled with a related increase in the demands of power and design. Now, this is goodarena of engineering opportunities and challenges. One of the challenges faced, isdeveloping fault tolerant systems, which increases the overall automotive and passengersafety. The development in the field of automotive electronics has led to the innovationof some very sophisticated technology. However, with increasing sophistication intechnology also rises the requirement to develop fault tolerant solutions.As one of many steps towards developing a fault tolerant system, this thesis presents anexhaustive fault analysis. The modeling and fault analysis is carried out for a vehicle withfour in-wheel motors. The primary goal is to collect as many of the possible failuremodes that could occur in a vehicle. A database of possible failure modes is retrievedfrom the Vehicle Dynamics research group at KTH. Now with further inputs to thisdatabase the individual faults are factored with respect to change in parameters of vehicleperformance. The factored faults are grouped with respect to similar outputcharacterization.The fault groups are modeled and integrated into a vehicle model developed earlier inMatlab/Simulink. All the fault groups are simulated under specific conditions and theresults are obtained. The dynamic behavior of the vehicle under such fault conditions isanalyzed. Further, in particular the behavior of the vehicle with electronic stabilitycontrol (ESC) under the fault conditions is tested. The deviation in the vital vehicleperformance parameters from nominal is computed.Finally based on the results obtained, a ranking system termed Severity Ranking System(SeRS) is presented. The severity ranking is presented based on three essential vehicleperformance parameters, such as longitudinal acceleration ( ), lateral acceleration ( )and yaw rate ( ̇ ). The ranking of the faults are classified as low severity S1, mediumseverity S2, high severity S3 and very high severity S4. A fault tolerant system must beable to successfully detect the fault condition, isolate the fault and provide correctiveaction. Hence, this database would serve as an effective input in developing fault tolerantsystems.</p>

corrected abstract:
<p>A vast expansion is found in the field of automotive electronic systems. The expansion is coupled with a related increase in the demands of power and design. Now, this is good arena of engineering opportunities and challenges. One of the challenges faced, is developing fault tolerant systems, which increases the overall automotive and passenger safety. The development in the field of automotive electronics has led to the innovation of some very sophisticated technology. However, with increasing sophistication in technology also rises the requirement to develop fault tolerant solutions.</p><p>As one of many steps towards developing a fault tolerant system, this thesis presents an exhaustive fault analysis. The modeling and fault analysis is carried out for a vehicle with four in-wheel motors. The primary goal is to collect as many of the possible failure modes that could occur in a vehicle. A database of possible failure modes is retrieved from the Vehicle Dynamics research group at KTH. Now with further inputs to this database the individual faults are factored with respect to change in parameters of vehicle performance. The factored faults are grouped with respect to similar output characterization.</p><p>The fault groups are modeled and integrated into a vehicle model developed earlier in Matlab/Simulink. All the fault groups are simulated under specific conditions and the results are obtained. The dynamic behavior of the vehicle under such fault conditions is analyzed. Further, in particular the behavior of the vehicle with electronic stability control (ESC) under the fault conditions is tested. The deviation in the vital vehicle performance parameters from nominal is computed.</p><p>Finally based on the results obtained, a ranking system termed Severity Ranking System (SeRS) is presented. The severity ranking is presented based on three essential vehicle performance parameters, such as longitudinal acceleration (<strong>a<sub>x</sub></strong>), lateral acceleration (<strong>a<sub>y</sub></strong>) and yaw rate (<strong>&psi;&#x307;</strong>). The ranking of the faults are classified as low severity S1, medium severity S2, high severity S3 and very high severity S4. A fault tolerant system must be able to successfully detect the fault condition, isolate the fault and provide corrective action. Hence, this database would serve as an effective input in developing fault tolerant systems.</p>
----------------------------------------------------------------------
In diva2:1670946 - error in title:
"Mechanical Design,Analysis, andManufacturing of Wind Tunnel Modeland support structure"
==>
"Mechanical Design, Analysis, and Manufacturing of Wind Tunnel Model and Support Structure"

abstract is:
<p>This volume covers the phases from design to manufacturing of a wind tunnel testsupport structure for a conceptual blended wingbodyUAV designed by KTH GreenRaven Project students. The innovative aircraft design demonstrates sustainabilitywithin aviation by utilizing a hybrid electricfuelcell propulsion system. The windtunnel test to be conducted at Bristol University will produce data to evaluate theaerodynamic properties of the model for design verification. The wind tunnel modelis a smallscaled1.5mspanmodel supported by struts that change the pitch andyaw angles during testing. An external force balance provided by Bristol Universitymeasures the loads and moments experienced by the model. The main requirementsfor the structure are to withstand the aerodynamic loads imposed by the model andto change the model’s orientation while maintaining wind speed during the test. Themaximum aerodynamic loads were provided in a matrix, the largest of which was usedas the load condition for the support equating to a 512N lift at 14◦ AOA. Trade studieswere conducted to determine the mechanisms to satisfy the requirements while stayingwithin budget. The chosen design for the support structure includes a circular baseplate constrained by a locking ring with positioning pins to change the yaw angle. Themain strut is mounted at the the center of the circular base plate. A hinge bracketat the top of the strut interfaces with another hinge bracket within the model viaa clevis pin. An electric linear actuator mounted downstream of the main strut isused to vary the pitch angle, with the center of rotation at the clevis pin. Once thedesign was finalized, finite element analysis was done to verify the structural stabilityof the design. The FEA results were compared to EulerBernoulliapproximations fordeflection. Manufacturing of the components was outsourcedwhile assembly andprogramming of the actuator was done inhouse.</p>


corrected abstract:
<p>This volume covers the phases from design to manufacturing of a wind tunnel test support structure for a conceptual blended wing-body UAV designed by KTH Green Raven Project students. The innovative aircraft design demonstrates sustainability within aviation by utilizing a hybrid electric-fuel cell propulsion system. The wind tunnel test to be conducted at Bristol University will produce data to evaluate the aerodynamic properties of the model for design verification. The wind tunnel model is a small-scaled 1.5m-span model supported by struts that change the pitch and yaw angles during testing. An external force balance provided by Bristol University measures the loads and moments experienced by the model. The main requirements for the structure are to withstand the aerodynamic loads imposed by the model and to change the model’s orientation while maintaining wind speed during the test. The maximum aerodynamic loads were provided in a matrix, the largest of which was used as the load condition for the support equating to a 512N lift at 14&deg; AOA. Trade studies were conducted to determine the mechanisms to satisfy the requirements while staying within budget. The chosen design for the support structure includes a circular base plate constrained by a locking ring with positioning pins to change the yaw angle. The main strut is mounted at the the center of the circular base plate. A hinge bracket at the top of the strut interfaces with another hinge bracket within the model via a clevis pin. An electric linear actuator mounted downstream of the main strut is used to vary the pitch angle, with the center of rotation at the clevis pin. Once the design was finalized, finite element analysis was done to verify the structural stability of the design. The FEA results were compared to Euler-Bernoulli approximations for deflection. Manufacturing of the components was out-sourced while assembly and programming of the actuator was done in-house.</p>
----------------------------------------------------------------------
In diva2:1547559 abstract is:
<p>A hydraulic damper plays an important role in tuning the handling and comfort characteristicsof a vehicle. Tuning and selecting a damper based on subjective evaluation, by considering theopinions of various users, would be an inefficient method since the comfort requirements of usersvary a lot. Instead, mathematical models of damper and simulation of these models in variousoperating conditions are preferred to standardize the tuning procedure, quantify the comfortlevels and reduce cost of testing. This would require a model, which is good enough to capture thebehaviour of damper in various operating and extreme conditions.The Force-Velocity (FV) curve is one of the most widely used model of a damper. This curve isimplemented either as an equation or as a look-up table. It is a plot between the maximum forceat each peak velocity point. There are certain dynamic phenomena like hysteresis and dependencyon the displacement of damper, which cannot be captured with a FV curve model, but are requiredfor better understanding of the vehicle behaviour.This thesis was conducted in cooperation with Volvo Cars with an aim to improve the existingdamper model which is a Force-Velocity curve. This work focuses on developing a damper model,which is complex enough to capture the phenomena discussed above and simple enough to beimplemented in real time simulations. Also, the thesis aims to establish a standard method toparameterise the damper model and generate the Force-Velocity curve from the tests performedon the damper test rig. A test matrix which includes the standard tests for parameterising andthe extreme test cases for the validation of the developed model will be developed. The final focusis to implement the damper model in a multi body simulation (MBS) software.The master thesis starts with an introduction, where the background for the project is described and then the thesis goals are set. It is followed by a literature review in which fewadvanced damper models are discussed in brief. Then, a step-by-step process of developing thedamper model is discussed along with few more possible options. Later, the construction of a testmatrix is discussed in detail followed by the parameter identification process. Next, the validationof the developed damper model is discussed using the test data from Volvo Hällered ProvingGround (HPG). After validation, implementation of the model in VI CarRealTime and Adams Caralong with the results are presented. Finally the thesis is concluded and the recommendations forfuture work are made on further improving the model.</p>


corrected abstract:
<p>A hydraulic damper plays an important role in tuning the handling and comfort characteristics of a vehicle. Tuning and selecting a damper based on subjective evaluation, by considering the opinions of various users, would be an inefficient method since the comfort requirements of users vary a lot. Instead, mathematical models of damper and simulation of these models in various operating conditions are preferred to standardize the tuning procedure, quantify the comfort levels and reduce cost of testing. This would require a model, which is good enough to capture the behaviour of damper in various operating and extreme conditions.</p><p>The Force-Velocity (FV) curve is one of the most widely used model of a damper. This curve is implemented either as an equation or as a look-up table. It is a plot between the maximum force at each peak velocity point. There are certain dynamic phenomena like hysteresis and dependency on the displacement of damper, which cannot be captured with a FV curve model, but are required for better understanding of the vehicle behaviour.</p><p>This thesis was conducted in cooperation with Volvo Cars with an aim to improve the existing damper model which is a Force-Velocity curve. This work focuses on developing a damper model, which is complex enough to capture the phenomena discussed above and simple enough to be implemented in real time simulations. Also, the thesis aims to establish a standard method to parameterise the damper model and generate the Force-Velocity curve from the tests performed on the damper test rig. A test matrix which includes the standard tests for parameterising and the extreme test cases for the validation of the developed model will be developed. The final focus is to implement the damper model in a multi body simulation (MBS) software.</p><p>The master thesis starts with an introduction, where the background for the project is described and then the thesis goals are set. It is followed by a literature review in which few advanced damper models are discussed in brief. Then, a step-by-step process of developing the damper model is discussed along with few more possible options. Later, the construction of a test matrix is discussed in detail followed by the parameter identification process. Next, the validation of the developed damper model is discussed using the test data from Volvo Hällered Proving Ground (HPG). After validation, implementation of the model in VI CarRealTime and Adams Car along with the results are presented. Finally the thesis is concluded and the recommendations for future work are made on further improving the model.</p>
----------------------------------------------------------------------
In diva2:1465540 abstract is:
<p>There are many applications where 3D models of landscapes can be used, suchas determining volume of objects, inspecting buildings and planning of infrastructure.One common way of creating 3D models of a geographical area is totake overlapping geotagged photos with a drone and then perform an aerotriangulation(AT). The aerotriangulating software find common key points in theimages and create a 3D surface model of the area. This process requires the use ofground control points (GCPs), which are used to map the 3D model onto a globalcoordinate system. These GCPs consume a lot of time to place at the location,measure accurately with total station and manually pinpoint in several photos.The purpose of this study is to compare models created by images taken with differentGNSS-based drone positioning systems and investigate for example howmany GCPs are needed, how the GCPs should be placed, and how the accuracyof models vary between the drone positioning systems considering both relativeand absolute accuracy.Two data acquisition sessions were done where images from two differentlocations were collected. In the first session the use of regular GPS and the use ofa local reference station are used as positioning system for the drone, and in thesecond session network real time kinematics (RTK) is also used as a third kind ofpositioning system.From the produced 3D models there is no significant difference between modelsusing a local reference station and network RTK, but when only using GPS thevertical accuracy drops significantly which means that more GCPs are required inorder for the model to be accurate. The standard deviation for points in createdmodels is calculated in easting, northing and vertical for the coordinate differencesfor the three positioning methods. When no GCPs are used the absoluteaccuracy is drastically lowered to meter level accuracy. In conclusion, by usingnetwork RTK or a local reference station the same accuracy for the 3D model canbe acquired with much fewer GCPs than if stand-alone GPS is used. None ofthe positioning systems can fully replace GCPs when a high absolute accuracy isneeded. With a relative accuracy requirement of 10 cm or more, both networkRTK and use of a local reference station has the potential to provide such qualitythat a 3D model would not need GCPs.</p>


corrected abstract:
<p>There are many applications where 3D models of landscapes can be used, such as determining volume of objects, inspecting buildings and planning of infrastructure. One common way of creating 3D models of a geographical area is to take overlapping geotagged photos with a drone and then perform an aerotriangulation (AT). The aerotriangulating software find common key points in the images and create a 3D surface model of the area. This process requires the use of ground control points (GCPs), which are used to map the 3D model onto a global coordinate system. These GCPs consume a lot of time to place at the location, measure accurately with total station and manually pinpoint in several photos. The purpose of this study is to compare models created by images taken with different GNSS-based drone positioning systems and investigate for example how many GCPs are needed, how the GCPs should be placed, and how the accuracy of models vary between the drone positioning systems considering both relative and absolute accuracy.</p><p>Two data acquisition sessions were done where images from two different locations were collected. In the first session the use of regular GPS and the use of a local reference station are used as positioning system for the drone, and in the second session network real time kinematics (RTK) is also used as a third kind of positioning system.</p><p>From the produced 3D models there is no significant difference between models using a local reference station and network RTK, but when only using GPS the vertical accuracy drops significantly which means that more GCPs are required in order for the model to be accurate. The standard deviation for points in created models is calculated in easting, northing and vertical for the coordinate differences for the three positioning methods. When no GCPs are used the absolute accuracy is drastically lowered to meter level accuracy. In conclusion, by using network RTK or a local reference station the same accuracy for the 3D model can be acquired with much fewer GCPs than if stand-alone GPS is used. None of the positioning systems can fully replace GCPs when a high absolute accuracy is needed. With a relative accuracy requirement of 10 cm or more, both network RTK and use of a local reference station has the potential to provide such quality that a 3D model would not need GCPs.</p>
----------------------------------------------------------------------
In diva2:1142969 abstract is:
<p>Standardized information and mathematicalmodels, which model the characteristics of the power generationand power transmission systems, are requirements for futuredevelopment and maintenance of different applications tooperate the electrical grid. Available databases such as Nordpoolprovides large amounts of data for power supply and demand [1].The typical misconception with open availability of data is thatexisting power system software tools can interact and process thisdata. Difficulties occur mainly because of two reasons. The firston is the amount of data produced. When the topology of theelectrical grid changes e.g. when a switch opens or closes, the flowof electrical power changes. This event produce changes ingeneration, transmission and distribution of the energy anddifferent data sets are produced. The second problem is therepresentation of information [2]. There are a limited number ofsoftware tools that can analyze this data, but each software toolrequires a specific data format structure to run. Dealing withthese difficulties requires an effective way to transform theprovided data representation into new data structures that canbe used in different execution platforms. This project aims tocreate a generic Model-to-Text (M2T) transformation capable oftransforming standardized power system information modelsinto input files executable by the Power System Analysis Tool(PSAT). During this project, a working M2T transformation wasnever achieved. However, missing functionality in someprograms connected to sub processes resulted in unexpectedproblems. This led to a new task of updating the informationmodel interpreter PyCIM. This task is partially completed andcan load basic power system information models.</p>

corrected abstract:
<p>Standardized information and mathematical models, which model the characteristics of the power generation and power transmission systems, are requirements for future development and maintenance of different applications to operate the electrical grid. Available databases such as Nordpool provides large amounts of data for power supply and demand [1]. The typical misconception with open availability of data is that existing power system software tools can interact and process this data. Difficulties occur mainly because of two reasons. The first on is the amount of data produced. When the topology of the electrical grid changes e.g. when a switch opens or closes, the flow of electrical power changes. This event produce changes in generation, transmission and distribution of the energy and different data sets are produced. The second problem is the representation of information [2]. There are a limited number of software tools that can analyze this data, but each software tool requires a specific data format structure to run. Dealing with these difficulties requires an effective way to transform the provided data representation into new data structures that can be used in different execution platforms. This project aims to create a generic Model-to-Text (M2T) transformation capable of transforming standardized power system information models into input files executable by the Power System Analysis Tool (PSAT). During this project, a working M2T transformation was never achieved. However, missing functionality in some programs connected to sub processes resulted in unexpected problems. This led to a new task of updating the information model interpreter PyCIM. This task is partially completed and can load basic power system information models.</p>
----------------------------------------------------------------------
In diva2:405988 abstract is:
<p>Recently, Volvo Construction Equipment AB has developed a weld class system forimperfections in welded joints, which contains demands for the toe radii, cold laps, undercutsetc. and where root defects are treated as requirements on the drawing. In this master thesis, thetoe radius has been studied more carefully along with the selection of reliable measurementsystems which are able to measure the toe radius along the weld. A computerized vision systemhas been evaluated by performing a measurement system analysis. FE-simulations anddestructive fatigue testing has also been carried out to determine which radial geometry beingcritical to the fatigue life.The results show that the currently used methods and gauges do not provide the requiredaccuracy when measuring the toe radius. The gauges are handled differently by differentoperators – even when using the vision system – which makes the methods subjective andtherefore unreliable. There are measuring systems that can gather surface data along the weldwith high accuracy, but there is no reliable method to assess the data. Therefore, the authors havedeveloped an algorithm – named STELIN – that assess the gathered surface data andautomatically identifies and calculates the toe radius and the toe angle along the weld. Using thatinformation an opportunity to improve the process control when welding is possible.The performed FE-calculations show that the surface roughness in the weld toe probably has aninfluence at the fatigue life of the joint. A more precise separate study should be made todetermine the impact of the surface roughness on the fatigue life. Those results should serve as abase when reviewing the theory used when predicting the fatigue life. Currently, stress averagingapproach is used in the notches of the root and the weld toe. In the future though, there might beanother stress condition to be taken into account, if the goal of reducing weight of the finishedproduct shall be achieved. Regarding measuring the surface roughness in the weld toe, theevaluated vision system has enough accuracy to deliver reliable data.More work remains with the STELIN-algorithm. The method used when assessing the calculatedtoe radii should be based on the conclusions from the performed FE-calculations. Integrating theSTELIN-algorithm in a fast feedback measurement system – for instance, on a laser – willprobably provide good opportunities for a better process control in order to achieve higherfatigue life of the welded joint.</p>

corrected abstract:
<p>Recently, Volvo Construction Equipment AB has developed a weld class system for imperfections in welded joints, which contains demands for the toe radii, cold laps, undercuts etc. and where root defects are treated as requirements on the drawing. In this master thesis, the toe radius has been studied more carefully along with the selection of reliable measurement systems which are able to measure the toe radius along the weld. A computerized vision system has been evaluated by performing a measurement system analysis. FE-simulations and destructive fatigue testing has also been carried out to determine which radial geometry being critical to the fatigue life.</p><p>The results show that the currently used methods and gauges do not provide the required accuracy when measuring the toe radius. The gauges are handled differently by different operators – even when using the vision system – which makes the methods subjective and therefore unreliable. There are measuring systems that can gather surface data along the weld with high accuracy, but there is no reliable method to assess the data. Therefore, the authors have developed an algorithm – named STELIN – that assess the gathered surface data and automatically identifies and calculates the toe radius and the toe angle along the weld. Using that information an opportunity to improve the process control when welding is possible.</p><p>The performed FE-calculations show that the surface roughness in the weld toe probably has an influence at the fatigue life of the joint. A more precise separate study should be made to determine the impact of the surface roughness on the fatigue life. Those results should serve as a base when reviewing the theory used when predicting the fatigue life. Currently, stress averaging approach is used in the notches of the root and the weld toe. In the future though, there might be another stress condition to be taken into account, if the goal of reducing weight of the finished product shall be achieved. Regarding measuring the surface roughness in the weld toe, the evaluated vision system has enough accuracy to deliver reliable data.</p><p>More work remains with the STELIN-algorithm. The method used when assessing the calculated toe radii should be based on the conclusions from the performed FE-calculations. Integrating the STELIN-algorithm in a fast feedback measurement system – for instance, on a laser – will probably provide good opportunities for a better process control in order to achieve higher fatigue life of the welded joint.</p>
----------------------------------------------------------------------
In diva2:1800339 abstract is:
<p>Coil supports are integral load-bearing components employed in generators andmotors. They serve the purpose of preventing excessive deformation and maintaininga stable position of the coils responsible for generating power and magnetic fieldswhen rotating. However, a problem with these coil supports is that they block theairflow aimed to cool the coils. Thus, this master thesis aimed to conduct a topologyoptimization to develop a cooling-air permeable coil support and select a suitablematerial. The new design was required to withstand 30,000 operational cycles andan overspeed test running at 120% speed without plastic deformation or failure.</p><p>The material selection process was initiated and based on mechanical and physicalproperties requirements. One of these was that the material should be non-magnetic.Utilizing Ansys Granta EduPack, two materials were suggested, the reference materialcurrently used for the coil support, and a titanium alloy, Ti-6Al-4V. The subsequentstep was to create a CAD model of the original design based on technical drawingsprovided by ABB. With the generated design, finite element analysis (FEA) simulationand the topology optimization could be performed. The generated topology optimizedmodel was modified and two new models were created, one with smaller central cutoutsand one with larger central cutouts and a top surface cutout. Furthermore, a thirdmodel was created based on the fundamentals of fluid mechanics, the Rounded originalmodel. Computational Fluid Dynamics (CFD) simulations of the four models wereexecuted.</p><p>The findings indicate that the design with larger central cutouts exhibited the mostsubstantial increase in airflow through and in between the coil supports, achieving a122 % improvement compared to the original design. The model satisfied the fatiguerequirement and successfully passed the overspeed test. Both the current referencematerial and the Ti-6Al4V alloy are suitable to use for coil support. However, theutilization of a titanium alloy might be deemed excessive in terms of its mechanicalproperties and cost.</p>

corrected abstract:
<p>Coil supports are integral load-bearing components employed in generators and motors. They serve the purpose of preventing excessive deformation and maintaining a stable position of the coils responsible for generating power and magnetic fields when rotating. However, a problem with these coil supports is that they block the airflow aimed to cool the coils. Thus, this master thesis aimed to conduct a topology optimization to develop a cooling-air permeable coil support and select a suitable material. The new design was required to withstand 30,000 operational cycles and an overspeed test running at 120% speed without plastic deformation or failure.</p><p>The material selection process was initiated and based on mechanical and physical properties requirements. One of these was that the material should be non-magnetic. Utilizing Ansys Granta EduPack, two materials were suggested, the reference material currently used for the coil support, and a titanium alloy, Ti-6Al-<sub>4</sub>V. The subsequent step was to create a CAD model of the original design based on technical drawings provided by ABB. With the generated design, finite element analysis (FEA) simulation and the topology optimization could be performed. The generated topology optimized model was modified and two new models were created, one with smaller central cutouts and one with larger central cutouts and a top surface cutout. Furthermore, a third model was created based on the fundamentals of fluid mechanics, the Rounded original model. Computational Fluid Dynamics (CFD) simulations of the four models were executed.</p><p>The findings indicate that the design with larger central cutouts exhibited the most substantial increase in airflow through and in between the coil supports, achieving a 122 % improvement compared to the original design. The model satisfied the fatigue requirement and successfully passed the overspeed test. Both the current reference material and the Ti-6Al<sub>4</sub>V alloy are suitable to use for coil support. However, the utilization of a titanium alloy might be deemed excessive in terms of its mechanical properties and cost.</p>
----------------------------------------------------------------------
In diva2:1701306 abstract is:
<p>A big focus of the Automotive Industry’s work is now on the development ofAutonomous Vehicles (AVs). In order to be able to release them on the market,they need to be tested and validated in a safe and efficient way. That is whycompanies working on the development of AVs use simulation to test the workthey are completing. Before putting an Autonomous Vehicle on any road, itwould be ideal to make sure, it will be able to navigate through the given roadsafely and react to everything that has ever happened on this road. What ismore, for the Autonomous Vehicle to be safely on the roads, it should also beable to react to uncommon situations, not seen exactly in the data it was trainedon before. In this thesis, the focus is on trajectories and their variations. Theaim of this work is to develop a framework, which would allow, having discretedata of traffic participants from chosen locations, to model the trajectories ofthose vehicles and the variations of those trajectories. This is to help withthe testing of Autonomous Vehicles in a simulation environment. The data,which is used to develop this method are from an intersection in Denmark,however, it is believed the method can be applied to data from anywhere,as long as it contains information about x and y coordinates of the vehiclesand the corresponding times of the vehicles being at those positions. In thiswork, only trajectories of cars are considered, but again other vehicles can betaken into account in the future. First, vehicle trajectories from given data aremodelled with the use of B-splines. The routine is set up as a constrainedoptimization problem with seven different constraints developed for a car.The constraints are highly nonlinear and therefore a constrained nonlinearoptimization problem is solved. The chosen method for this is the interior-pointmethod. After obtaining the approximation of the trajectory in the Bsplineform, a variation of it is achieved through the change of the speed of thevehicle and its initial position. A projection of the required velocity change onthe derivative of B-spline basis space is calculated and then a new variation ofthe original approximated trajectory in B-spline form is obtained. The methodwas implemented in Matlab and successfully used to approximate and varytrajectories from a dataset from an intersection in Denmark, Aalborg.</p>

corrected abstract:
<p>A big focus of the Automotive Industry’s work is now on the development of Autonomous Vehicles (AVs). In order to be able to release them on the market, they need to be tested and validated in a safe and efficient way. That is why companies working on the development of AVs use simulation to test the work they are completing. Before putting an Autonomous Vehicle on any road, it would be ideal to make sure, it will be able to navigate through the given road safely and react to everything that has ever happened on this road. What is more, for the Autonomous Vehicle to be safely on the roads, it should also be able to react to uncommon situations, not seen exactly in the data it was trained on before. In this thesis, the focus is on trajectories and their variations. The aim of this work is to develop a framework, which would allow, having discrete data of traffic participants from chosen locations, to model the trajectories of those vehicles and the variations of those trajectories. This is to help with the testing of Autonomous Vehicles in a simulation environment. The data, which is used to develop this method are from an intersection in Denmark, however, it is believed the method can be applied to data from anywhere, as long as it contains information about x and y coordinates of the vehicles and the corresponding times of the vehicles being at those positions. In this work, only trajectories of cars are considered, but again other vehicles can be taken into account in the future. First, vehicle trajectories from given data are modelled with the use of B-splines. The routine is set up as a constrained optimization problem with seven different constraints developed for a car. The constraints are highly nonlinear and therefore a constrained nonlinear optimization problem is solved. The chosen method for this is the interior-point method. After obtaining the approximation of the trajectory in the B-spline form, a variation of it is achieved through the change of the speed of the vehicle and its initial position. A projection of the required velocity change on the derivative of B-spline basis space is calculated and then a new variation of the original approximated trajectory in B-spline form is obtained. The method was implemented in Matlab and successfully used to approximate and vary trajectories from a dataset from an intersection in Denmark, Aalborg.</p>
----------------------------------------------------------------------
In diva2:1350191 abstract is:
<p>Bending stiness is one of the most important mechanical properties in paperboard making,giving rigidity to panels and boxes. This property is currently only possible to measure bydestructive measure o the production line. The current quality control method is decient byassuming a non-realistic consistency of the paperboard properties along the machine direction.The objective of this thesis is to predict the thickness and bending stiness of the nal boardsfrom process data.Two modelling approaches are used: the rst model calculates the bending stiness from acalculated thickness, while the other one uses the measured baseboard thickness. Both modelsuse common inputs such as material properties and grammage measurement. The grammage istaken from the online baseboard measurement. The material properties come from laboratorymeasurements and assumptions. It is assumed that the density ratio between the outer andmiddle plies is constant for all product lines, at all times. The TSI of each ply is dened fromtensile testing experiments and nominal bending stiness. It is also assumed that the coatingdoes not contribute to bending stiness. The two models use equations based on laminatetheory assuming orthotropic layers and neglecting the interlaminar shear forces. The modelsuse data of two dierent natures: i.e. laboratory data and online data. Laboratory data is usedas a comparative to evaluate the models' performance of calculated values from online data.The results show various levels of prediction accuracy for dierent paperboard grades. Theaverage thickness predictions are all underestimations within a 5% error while the bendingstiness estimations vary much more from product to product; varying from 9% underestimationto 32% overestimation. The bending stiness prediction for CD is consistently higher thanfor MD for both models. Most product lines have better results with the calculated thickness,approach 1. The calculated thickness is always underestimated and bending stiness is overestimated,hence the better results with the rst approach.The most important conclusion from the models' results is the spread of laboratory measurements,when compared to the predicted values. The large variation most likely comes fromproduction, implying inconsistencies in the manufacturing process that are not accounted forby the models. These modelling approaches have failed to capture the production variationsbecause of the lack of input parameters.</p>

corrected abstract:
<p>Bending stiffness is one of the most important mechanical properties in paperboard making, giving rigidity to panels and boxes. This property is currently only possible to measure by destructive measure off the production line. The current quality control method is deficient by assuming a non-realistic consistency of the paperboard properties along the machine direction. The objective of this thesis is to predict the thickness and bending stiffness of the final boards from process data.</p><p>Two modelling approaches are used: the first model calculates the bending stiffness from a calculated thickness, while the other one uses the measured baseboard thickness. Both models use common inputs such as material properties and grammage measurement. The grammage is taken from the online baseboard measurement. The material properties come from laboratory measurements and assumptions. It is assumed that the density ratio between the outer and middle plies is constant for all product lines, at all times. The TSI of each ply is defined from tensile testing experiments and nominal bending stiffness. It is also assumed that the coating does not contribute to bending stiffness. The two models use equations based on laminate theory assuming orthotropic layers and neglecting the interlaminar shear forces. The models use data of two different natures: i.e. laboratory data and online data. Laboratory data is used as a comparative to evaluate the models' performance of calculated values from online data.</p><p>The results show various levels of prediction accuracy for different paperboard grades. The average thickness predictions are all underestimations within a 5% error while the bending stiffness estimations vary much more from product to product; varying from 9% underestimation to 32% overestimation. The bending stiffness prediction for CD is consistently higher than for MD for both models. Most product lines have better results with the calculated thickness, approach 1. The calculated thickness is always underestimated and bending stiffness is overestimated, hence the better results with the first approach.</p><p>The most important conclusion from the models' results is the spread of laboratory measurements, when compared to the predicted values. The large variation most likely comes from production, implying inconsistencies in the manufacturing process that are not accounted for by the models. These modelling approaches have failed to capture the production variations because of the lack of input parameters.</p>
----------------------------------------------------------------------
In diva2:1183391 abstract is:
<p>Transportation underlines the vehicle industry's critical role in a country's economic future.The amount of goods moved, specically by trucks, is only expected to increase inthe near future. This work attempts to tackle the problem of optimizing fuel consumptionin Volvo trucks, when there are hard constraints on the delivery time and speed limits.Knowledge of the truck such as position, state, conguration etc., along with the completeroute information of the transport mission is used for fuel optimization.Advancements in computation, storage, and communication on cloud based systems, hasmade it possible to easily incorporate such systems in assisting modern eet. In this work,an algorithm is developed in a cloud based system to compute a speed plan for the completemission for achieving fuel minimization. This computation is decoupled from thelocal control operations on the truck such as prediction control, safety, cruise control, etc.;and serves as a guide to the truck driver to reach the destination on time by consumingminimum fuel.To achieve fuel minimization under hard constraints on delivery (or arrival) time andspeed limits, a non-linear optimization problem is formulated for the high delity modelestimated from real-time drive cycles. This optimization problem is solved using a Nonlinearprogramming solver in Matlab.The optimal policy was tested on two drive cycles provided by Volvo. The policy wascompared with two dierent scenarios, where the mission demands hard constraints ontravel time and the speed limits in addition to no trac uncertainties (deterministic). with a cruise controller running at a constant set speed throughout the mission. Itis observed that there is no signicant fuel savings. with maximum possible fuel consumption; achieved without the help of optimalspeed plan (worst case). It is seen that there is a notable improvement in fuelsaving.In a real world scenario, a transport mission is interrupted by uncertainties such as trac ow, road blocks, re-routing, etc. To this end, a stochastic optimization algorithm is proposedto deal with the uncertainties modeled using historical trac ow data. Possiblesolution methodologies are suggested to tackle this stochastic optimization problem.</p>


corrected abstract:
<p>Transportation underlines the vehicle industry's critical role in a country's economic future. The amount of goods moved, specifically by trucks, is only expected to increase in the near future. This work attempts to tackle the problem of optimizing fuel consumption in Volvo trucks, when there are hard constraints on the delivery time and speed limits. Knowledge of the truck such as position, state, configuration etc., along with the complete route information of the transport mission is used for fuel optimization.</p><p>Advancements in computation, storage, and communication on cloud based systems, has made it possible to easily incorporate such systems in assisting modern fleet. In this work, an algorithm is developed in a cloud based system to compute a speed plan for the complete mission for achieving fuel minimization. This computation is decoupled from the local control operations on the truck such as prediction control, safety, cruise control, etc.; and serves as a guide to the truck driver to reach the destination on time by consuming minimum fuel.</p><p>To achieve fuel minimization under hard constraints on delivery (or arrival) time and speed limits, a non-linear optimization problem is formulated for the high fidelity model estimated from real-time drive cycles. This optimization problem is solved using a Nonlinear programming solver in Matlab.</p><p>The optimal policy was tested on two drive cycles provided by Volvo. The policy was compared with two different scenarios, where the mission demands hard constraints on travel time and the speed limits in addition to no traffic uncertainties (deterministic).</p><ul><li>with a cruise controller running at a constant set speed throughout the mission. It is observed that there is no significant fuel savings.</li><li>with maximum possible fuel consumption; achieved without the help of optimal speed plan (worst case). It is seen that there is a notable improvement in fuel saving.</li></ul><p>In a real world scenario, a transport mission is interrupted by uncertainties such as traffic flow, road blocks, re-routing, etc. To this end, a stochastic optimization algorithm is proposed to deal with the uncertainties modeled using historical traffic flow data. Possible solution methodologies are suggested to tackle this stochastic optimization problem.</p>
----------------------------------------------------------------------
In diva2:1142952 abstract is:
<p>Sending bacteria to space is a further step withinthe framework of transporting humans to distant locations inspace. This can build a knowledge platform of how the bacteriabehaves in the space environments, in order to be able to functionin the long term as a LLS (long term life support system), i.ea mini ecology for the space station that handles waste (gas,liquid and solid) and transforms it into food, water and oxygen.By constructing a bacterial experiment (MoreBac) in a smallsatellite and thermally simulating it in space environment, itcan aid future projects performed in similar but larger scales.To visualize the experiment in presentations, a CAD-model ofthe experiment will be designed and constructed in SIEMENSSolid Edge. The thermal analysis is made in Airbus SYSTEMAThermica and will help show on the critical problem, which isto maintain suitable temperature conditions on the microfluidicchip inside the experiment. By performing the simulations, onecan assure that the design is suitable and that the heat gradientis in required intervals for different components. The CADmodelwas designed in a sandwich layout and consist of twoprinted circuit boards, one microfluidic chip and one reservoir.Not specified components of the experiment was not used in theCAD- model since they where still in early development. Thethermal analysis of the experiment was studied in a steady stateenvironment, with boundary conditions of 5˝C in the cold caseand 30˝C in the hot case, which means that the time variablewas not considered. Three configurations of heat dissipation weremade; 16 nodes at the illumination board with 0,05 W each, 16nodes at the detection board with 0,05 W each and finally 36nodes on both PCBs together with 0,025 W each. In the hot case,the microfluidic chip reaches temperatures between 34, 16˝C and42, 15˝C when 0,8 W is equally divided to both PCBs. In thecold case, the microfluidic chip reaches temperatures between13, 82˝C and 22, 32˝C with the same heat distribution as thehot case.</p>

corrected abstract:
<p>Sending bacteria to space is a further step within the framework of transporting humans to distant locations in space. This can build a knowledge platform of how the bacteria behaves in the space environments, in order to be able to function in the long term as a LLS (long term life support system), i.e a mini ecology for the space station that handles waste (gas, liquid and solid) and transforms it into food, water and oxygen. By constructing a bacterial experiment (MoreBac) in a small satellite and thermally simulating it in space environment, it can aid future projects performed in similar but larger scales. To visualize the experiment in presentations, a CAD-model of the experiment will be designed and constructed in SIEMENS Solid Edge. The thermal analysis is made in Airbus SYSTEMA Thermica and will help show on the critical problem, which is to maintain suitable temperature conditions on the microfluidic chip inside the experiment. By performing the simulations, one can assure that the design is suitable and that the heat gradient is in required intervals for different components. The CAD-model was designed in a sandwich layout and consist of two printed circuit boards, one microfluidic chip and one reservoir. Not specified components of the experiment was not used in the CAD- model since they where still in early development. The thermal analysis of the experiment was studied in a steady state environment, with boundary conditions of 5˝C in the cold case and 30˝C in the hot case, which means that the time variable was not considered. Three configurations of heat dissipation were made; 16 nodes at the illumination board with 0,05 W each, 16 nodes at the detection board with 0,05 W each and finally 36 nodes on both PCBs together with 0,025 W each. In the hot case, the microfluidic chip reaches temperatures between 34,16˝C and 42,15˝C when 0,8 W is equally divided to both PCBs. In the cold case, the microfluidic chip reaches temperatures between 13,82˝C and 22,32˝C with the same heat distribution as the hot case.</p>
----------------------------------------------------------------------
In diva2:1083778 error in title:
"Pressure measurements in pulsatingflows"
==>
"Pressure measurements in pulsating flows"

abstract is:
<p>Due to confidentiality several axis in the figures and large parts of the specifics of the resultsand of the experimental setups have been replaced by symbols. Also one section of the report,concerning a prototype sensor, has been removed completely due to the sensitive nature of theresults.Measuring the exhaust gas pressure and the boost pressure at the air intake manifold isconsidered a standard procedure in modern cars and trucks. Although how to measure thepressure accurately for steady flows is well known, the pressure measurements in pulsatingflows is not a trivial task. This theses shows, experimentally, how well the characteristics of apressure measurement systems, using different dimensions of straight pneumatic tubing, canbe predicted using the Helmholtz resonator model. Also how much this resonance influencethe pressure measurements for different pressure transducers used in trucks today. This thesisalso demonstrates the effects that the sampling frequency and the averaging time has on theaccuracy of measuring an average pressure in pulsating gas flows and how clogging of thepneumatic tubes influence the measurements. This was done using two types of experiments;a step response experiment to properly show the characteristics of the measuring system and apulse rig experiment that shows the impact, of the tubing, on the measurements for typicalfrequencies found in medium sized trucks. The experiments shows that the response time andresonance frequency of a measurement system can be predicted with an accuracy of 𝜇! % fortubes longer than 725 mm. It also that the average absolute pressure measurement keeps anaccuracy of 𝜇! % for all tube dimensions, including clogging of the tube with a decrease ofdiameter up to 𝜇! %. It does however show that if the sensor has some internal resonance that4matches the Helmholtz resonance the measurement can be overestimated by over 𝜇! %. Testsof the sampling frequency shows that if the sampling frequency is chosen as a divisor or amultiple of the pulse frequency the error due to averaging is increased by one order ofmagnitude. Using the information given in this thesis it is possible to avoid unnecessary errorswhen performing pressure measurements in a pulsating flow.</p>


corrected abstract:
<p>Due to confidentiality several axis in the figures and large parts of the specifics of the results and of the experimental setups have been replaced by symbols. Also one section of the report, concerning a prototype sensor, has been removed completely due to the sensitive nature of the results.</p><p>Measuring the exhaust gas pressure and the boost pressure at the air intake manifold is considered a standard procedure in modern cars and trucks. Although how to measure the pressure accurately for steady flows is well known, the pressure measurements in pulsating flows is not a trivial task. This theses shows, experimentally, how well the characteristics of a pressure measurement systems, using different dimensions of straight pneumatic tubing, can be predicted using the Helmholtz resonator model. Also how much this resonance influence the pressure measurements for different pressure transducers used in trucks today. This thesis also demonstrates the effects that the sampling frequency and the averaging time has on the accuracy of measuring an average pressure in pulsating gas flows and how clogging of the pneumatic tubes influence the measurements. This was done using two types of experiments; a step response experiment to properly show the characteristics of the measuring system and a pulse rig experiment that shows the impact, of the tubing, on the measurements for typical frequencies found in medium sized trucks. The experiments shows that the response time and oresonance frequency of a measurement system can be predicted with an accuracy of µ<sub>1</sub> % for tubes longer than 725 mm. It also that the average absolute pressure measurement keeps an accuracy of µ<sub>2</sub> % for all tube dimensions, including clogging of the tube with a decrease of diameter up to µ<sub>3</sub> %. It does however show that if the sensor has some internal resonance that matches the Helmholtz resonance the measurement can be overestimated by over µ<sub>4</sub> %. Tests of the sampling frequency shows that if the sampling frequency is chosen as a divisor or a multiple of the pulse frequency the error due to averaging is increased by one order of magnitude. Using the information given in this thesis it is possible to avoid unnecessary errors when performing pressure measurements in a pulsating flow.</p>
----------------------------------------------------------------------
In diva2:860546 abstract is:
<p>Ankylosing Spondylitis (AS), or Bechterew’s disease, is an inflammatory rheumaticdisease that through the formation of additional bone tissue in the spine eventuallyleads to the complete fusion of the vertebrae, in effect turning the spine into one longbone. Due to the reduced flexibility of the spine with the long lever arms, spinalfractures in AS-patients are relatively common even after minor trauma.</p><p>The aim of this thesis was to use an existing finite element model of a healthy spineand adapt it to the conditions of AS, thus gaining some insight into the effects ofsurgical stabilization of cervical fractures, using posterior screws and rods. Althoughthis type of surgery is often performed, it has not been previously investigated in abiomechanical model. This thesis should be considered as a starting point for how afinite element model of the spine could be used to investigate the effect of spinalimplants in the case of a fracture in the ankylosed spine.</p><p>An existing FE-model was modified to some of the conditions of AS: The vertebraewere fused by adding ossifications at the intervertebral discs (with the Head-C1 andC1-C2 joints left mobile). A fracture was simulated at the C6C7 disc level. Fourdifferent implant configurations were tested: Short instrumentation C6C7, mediuminstrumentation C5toT1, long instrumentation C3toT3, and a long instrumentationC3C6C7T3 with skipped intermediate levels. Three loads (1.5g, 3.0g, 4.5g) wereapplied according to a specific load curve. Kinematic data such as the gap distance inthe fracture site were obtained. Furthermore the stresses in the ossified parts of thediscs were evaluated.</p><p>It was shown that the chosen methods of adapting the model to the AS conditions, andmodeling the fracture and implant, changed the kinematics so that less movementoccurred between the vertebra, which is typical for AS. Measured as fracture gap, alltested implant configurations were equally good at stabilizing the fracture, althoughthey all allowed more movement than the non-fractured AS-model did. All implantconfigurations were also able to stabilize the fracture in terms of the horizontal translation in the fracture. The disc ossifications were somewhat shielded from stress for those ossifications that were within the range of the implant. This was so for all implant configurations. No increased stress was observed in the ossifications immediately outside the range for the implants, relative the non-fractured AS-model.</p><p>For the C6C7 and C5toT1 implant configurations as well as the non-fractured ASmodel,the stresses were highest at the T1T2 level. Stresses in the ossifications in the thoracic spine were generally low, apart from the T1T2 level. The results show that the chosen AS-adaptations and the modeled implant seem reasonable for testing some of the considerations of cervical fractures in the ankylosed spine as well as for some implant configurations. The results also make it possible to speculate about the optimal type of implant. The effects of screw placement and anchoring, osteoporosis, muscle activation and possible spinal deformity on the implant stability were not investigated, and should be a matter for further studies.</p>

corrected abstract:
<p>Ankylosing Spondylitis (AS), or Bechterew’s disease, is an inflammatory rheumatic disease that through the formation of additional bone tissue in the spine eventually leads to the complete fusion of the vertebrae, in effect turning the spine into one long bone. Due to the reduced flexibility of the spine with the long lever arms, spinal fractures in AS-patients are relatively common even after minor trauma.</p><p>The aim of this thesis was to use an existing finite element model of a healthy spine and adapt it to the conditions of AS, thus gaining some insight into the effects of surgical stabilization of cervical fractures, using posterior screws and rods. Although this type of surgery is often performed, it has not been previously investigated in a biomechanical model. This thesis should be considered as a starting point for how a finite element model of the spine could be used to investigate the effect of spinal implants in the case of a fracture in the ankylosed spine.</p><p>An existing FE-model was modified to some of the conditions of AS: The vertebrae were fused by adding ossifications at the intervertebral discs (with the Head-C1 and C1-C2 joints left mobile). A fracture was simulated at the C6C7 disc level. Four different implant configurations were tested: Short instrumentation C6C7, medium instrumentation C5toT1, long instrumentation C3 to T3, and a long instrumentation C3C6C7T3 with skipped intermediate levels. Three loads (1.5g, 3.0g, 4.5g) were applied according to a specific load curve. Kinematic data such as the gap distance in the fracture site were obtained. Furthermore the stresses in the ossified parts of the discs were evaluated.</p><p>It was shown that the chosen methods of adapting the model to the AS conditions, and modeling the fracture and implant, changed the kinematics so that less movement occurred between the vertebra, which is typical for AS. Measured as fracture gap, all tested implant configurations were equally good at stabilizing the fracture, although they all allowed more movement than the non-fractured AS-model did. All implant configurations were also able to stabilize the fracture in terms of the horizontal translation in the fracture. The disc ossifications were somewhat shielded from stress for those ossifications that were within the range of the implant. This was so for all implant configurations. No increased stress was observed in the ossifications immediately outside the range for the implants, relative the non-fractured AS-model.</p><p>For the C6C7 and C5toT1 implant configurations as well as the non-fractured AS-model, the stresses were highest at the T1T2 level. Stresses in the ossifications in the thoracic spine were generally low, apart from the T1T2 level.</p><p>The results show that the chosen AS-adaptations and the modeled implant seem reasonable for testing some of the considerations of cervical fractures in the ankylosed spine as well as for some implant configurations. The results also make it possible to speculate about the optimal type of implant. The effects of screw placement and anchoring, osteoporosis, muscle activation and possible spinal deformity on the implant stability were not investigated, and should be a matter for further studies.</p>
----------------------------------------------------------------------
In diva2:643818 abstract is:
<p>This master thesis, which has been carried out in collaboration with Fairtrade,investigates how sustainable development can be integrated in the mathematicaldiscipline in the Swedish upper secondary school. The study includedfindings on design and disposition, with sustainability themes relatedto Fairtrade. This since Fairtrade is going to use the study as a basis forthe development of an educational material. The results of the study areof interest to teachers and organizations that want to develop educationalmaterial that integrate sustainable development in mathematics education.The main focus of the study has been the teachers’ preferences on dispositionand design.Semi-structured interviews were conducted with seven mathematics teachersin the Stockholm area. A workshop was also conducted with three mathematicsstudent teachers at the end of their training, and two people workingat Fairtrade. The result of the study shows that it is important that theeducational material corresponds to the achievement goals of the mathematicscourses. It also shows the importance of making the material differsfrom ordinary learning milieu, as this would inspire teachers to vary theirteaching. The results also indicate that mathematics education in the exerciseparadigm is not suitable for integrating sustainable development inmathematics education. Instead an investigative approach and working withtasks referring to real-life situations where the students are encouraged totake a position on questions of sustainable development is preferred. For instance,child labor and poverty can be investigated using mathematics andby linking those problems to consumptions the students can question theirown role as consumers.</p>

corrected abstract:
<p>This master thesis, which has been carried out in collaboration with Fairtrade, investigates how sustainable development can be integrated in the mathematical discipline in the Swedish upper secondary school. The study included findings on design and disposition, with sustainability themes related to Fairtrade. This since Fairtrade is going to use the study as a basis for the development of an educational material. The results of the study are of interest to teachers and organizations that want to develop educational material that integrate sustainable development in mathematics education. The main focus of the study has been the teachers’ preferences on disposition and design.</p><p>Semi-structured interviews were conducted with seven mathematics teachers in the Stockholm area. A workshop was also conducted with three mathematics student teachers at the end of their training, and two people working at Fairtrade. The result of the study shows that it is important that the educational material corresponds to the achievement goals of the mathematics courses. It also shows the importance of making the material differs from ordinary learning milieu, as this would inspire teachers to vary their teaching. The results also indicate that mathematics education in the exercise paradigm is not suitable for integrating sustainable development in mathematics education. Instead an investigative approach and working with tasks referring to real-life situations where the students are encouraged to take a position on questions of sustainable development is preferred. For instance, child labor and poverty can be investigated using mathematics and by linking those problems to consumptions the students can question their own role as consumers.</p>
----------------------------------------------------------------------
In diva2:405926 - missing space in title:
"Development of a dynamic calculation tool forsimulation of ditching"
==>
"Development of a dynamic calculation tool for simulation of ditching"

abstract is:
<p>The present document is the final master thesis report written by Marc PILORGET,student at SUPAERO (home institution) and KTH (Royal Institute of Technology,Exchange University). This six months internship was done at DASSAULT AVIATION(Airframe engineering department) based in Saint-Cloud, France. It spanned from the 5thof July to the 23rd of December. The thesis work aims at developing an SPH (SmoothParticle Hydrodynamics) calculation method for ditching and implementing it in the finiteelement software ELFINI® developed by DASSAULT. Ditching corresponds to a phasewhen the aeroplane is touching the water. The problematic of ditching has always beenan area of interest for DASSAULT and the whole aeronautical industry. So far, only testsand simple analytical calculations have been performed. Most of the work was carried bythe NACA (National Advisory Committee for Aeronautics) in the late 70's. However in thepast decade, a new method for fluid-structure coupling problems has been developed. Itis called SPH. The basic principle is the following: the domain is represented by means ofparticles and each particle of fluid is treated separately and submitted to the Navier-Stokes equations. The particle is influenced by the neighbouring particles with a weightfunction depending on the distance between the two particles. Particles are also placed atthe interface solid-fluid: they are called limit particles. The final purpose of this SPHmethod is to access to the structural response of an aircraft when ditching. The crucialinterest of such a method compared to methods used so far is the absence of mesh. Theanalysis of large deformation problems by the finite element method may require thecontinuous remeshing of the domain to avoid the breakdown of the calculation due toexcessive mesh distortion. When considering ditching or other large deformationsproblems, the mesh generation is a far more time-consuming task than the constructionand solution of a discrete set of equations. For DASSAULT-AVIATION, the long termobjective is to get a numerical tool able to model ditching. The SPH method is used tosolve the equations for the fluid and is coupled with a finite element method for thestructure. So far, the compressible solver for 2D geometries has been implemented.Tests are going to be performed to ensure the program’s robustness. Then theincompressible solver for 2D geometries will be studied both theoretically andnumerically.</p>

corrected abstract:
<p>The present document is the final master thesis report written by Marc PILORGET, student at SUPAERO (home institution) and KTH (Royal Institute of Technology, Exchange University). This six months internship was done at DASSAULT AVIATION (Airframe engineering department) based in Saint-Cloud, France. It spanned from the 5<sup>th</sup> of July to the 23<sup>rd</sup> of December. The thesis work aims at developing an SPH (Smooth Particle Hydrodynamics) calculation method for ditching and implementing it in the finite element software ELFINI® developed by DASSAULT. Ditching corresponds to a phase when the aeroplane is touching the water. The problematic of ditching has always been an area of interest for DASSAULT and the whole aeronautical industry. So far, only tests and simple analytical calculations have been performed. Most of the work was carried by the NACA (National Advisory Committee for Aeronautics) in the late 70's. However in the past decade, a new method for fluid-structure coupling problems has been developed. It is called SPH. The basic principle is the following: the domain is represented by means of particles and each particle of fluid is treated separately and submitted to the Navier-Stokes equations. The particle is influenced by the neighbouring particles with a weight function depending on the distance between the two particles. Particles are also placed at the interface solid-fluid: they are called limit particles. The final purpose of this SPH method is to access to the structural response of an aircraft when ditching. The crucial interest of such a method compared to methods used so far is the absence of mesh. The analysis of large deformation problems by the finite element method may require the continuous remeshing of the domain to avoid the breakdown of the calculation due to excessive mesh distortion. When considering ditching or other large deformations problems, the mesh generation is a far more time-consuming task than the construction and solution of a discrete set of equations. For DASSAULT-AVIATION, the long term objective is to get a numerical tool able to model ditching. The SPH method is used to solve the equations for the fluid and is coupled with a finite element method for the structure. So far, the compressible solver for 2D geometries has been implemented. Tests are going to be performed to ensure the program’s robustness. Then the incompressible solver for 2D geometries will be studied both theoretically and numerically.
----------------------------------------------------------------------
In diva2:1707348 abstract is:
<p>This thesis studies the implementation of an Explicit Algebraic Reynolds-Stress Model(EARSM) for Atmospheric Boundary Layer (ABL) in an open source ComputationalFluid Dynamics (CFD) software, OpenFOAM, following the guidance provided by thewind company ENERCON that aims to make use of this novel model to improvesites’ wind-field predictions. After carefully implementing the model in OpenFOAM,the EARSM implementation is verified and validated by testing it with a stratifiedCouette flow case. The former was done by feeding mean flow properties, takenfrom OpenFOAM, in a python tool containing the full EARSM system of equationsand constants, and comparing the resulting flux profiles with the ones extracted bythe OpenFOAM simulations. Subsequently, the latter was done by comparing theprofiles of the two universal functions used by Monin-Obukhov Similarity Theory(MOST) for mean velocity and temperature to the results obtained by Želi et al. intheir study of the EARSM applied to a single column ABL, in “Modelling of stably-stratified, convective and transitional atmospheric boundary layers using the explicitalgebraic Reynolds-stress model” (2021). The verification of the model showed minordifferences between the flux profiles from the python tool and OpenFOAM thus, themodel’s implementation was deemed verified, while the validation step showed nodifference in the unstable and neutral stratification cases, but a significant discrepancyfor stably stratified flow. Nonetheless, the reason behind the inconsistency is believedto be related to the choice of boundary conditions thus, the model’s implementationitself is considered validated.</p><p>Finally, the comparison between the EARSM and the k − ε model showed thatthe former is able to capture the physics of the flow properties where the latter failsto. In particular, the diagonal momentum fluxes resulting from the EARSM reflectthe observed behaviour of being different from each other, becoming isotropic withaltitude in the case of unstable stratification, and having magnitude u′u′ &gt; v′v′ &gt; w′w′ for stably stratified flows. On the other hand, the eddy viscosity assumption used bythe k − ε model computes the diagonal momentum fluxes as being equal to each other.Moreover, the EARSM captures more than one non-zero heat flux component in theCouette flow case, which has been observed to be the case in literature, while the eddydiffusivity assumption used by the k − ε model only accounts for one non-zero heat fluxcomponent.</p><p> </p>

corrected abstract:
<p>This thesis studies the implementation of an Explicit Algebraic Reynolds-Stress Model (EARSM) for Atmospheric Boundary Layer (ABL) in an open source Computational Fluid Dynamics (CFD) software, OpenFOAM, following the guidance provided by the wind company ENERCON that aims to make use of this novel model to improve sites’ wind-field predictions. After carefully implementing the model in OpenFOAM, the EARSM implementation is verified and validated by testing it with a stratified Couette flow case. The former was done by feeding mean flow properties, taken from OpenFOAM, in a python tool containing the full EARSM system of equations and constants, and comparing the resulting flux profiles with the ones extracted by the OpenFOAM simulations. Subsequently, the latter was done by comparing the profiles of the two universal functions used by Monin-Obukhov Similarity Theory (MOST) for mean velocity and temperature to the results obtained by Želi et al. in their study of the EARSM applied to a single column ABL, in “Modelling of stably-stratified, convective and transitional atmospheric boundary layers using the explicit algebraic Reynolds-stress model” (2021). The verification of the model showed minor differences between the flux profiles from the python tool and OpenFOAM thus, the model’s implementation was deemed verified, while the validation step showed no difference in the unstable and neutral stratification cases, but a significant discrepancy for stably stratified flow. Nonetheless, the reason behind the inconsistency is believed to be related to the choice of boundary conditions thus, the model’s implementation itself is considered validated.</p><p>Finally, the comparison between the EARSM and the <em>k − ϵ</em> model showed that the former is able to capture the physics of the flow properties where the latter fails to. In particular, the diagonal momentum fluxes resulting from the EARSM reflect the observed behaviour of being different from each other, becoming isotropic with altitude in the case of unstable stratification, and having magnitude <em><span style="border-top: 1px solid black; padding: 0.2rem;">u&prime;u&prime;</span></em> &gt; <em><span style="border-top: 1px solid black; padding: 0.2rem;">v&prime;v&prime;</span></em> &gt; <em><span style="border-top: 1px solid black; padding: 0.2rem;">w&prime;w&prime;</span></em> for stably stratified flows. On the other hand, the eddy viscosity assumption used by the <em>k − ϵ</em> model computes the diagonal momentum fluxes as being equal to each other. Moreover, the EARSM captures more than one non-zero heat flux component in the Couette flow case, which has been observed to be the case in literature, while the eddy diffusivity assumption used by the <em>k − ϵ</em> model only accounts for one non-zero heat flux component.</p>


Note: In the corrected abstract I could not use an overline, as the primes punctured the overline; therefore, I used the border-top echanism and raised by adding more padding.
----------------------------------------------------------------------
In diva2:1228966 abstract is:
<p>Much research is done today on how to make vehicles autonomous. But the main focuslies in how to make the techniques and safety sufficient. This means that the comfort forthe passengers has fallen behind. Studies show that approximately 25 % of the users ofautonomous vehicles would experience motion sickness.The purpose of this thesis is to use existing hypotheses about what causes motion sicknessto analyse different technical solutions that could decrease motion sickness inautonomous vehicles.To investigate this a literature study is done. Only existing research and experiments areused.The report is based on the theories about the sensor conflict and postural instability. Thetheory about the sensor conflict means that a person gets symptoms of motion sicknesswhen the visual impression doesn’t match with the ones from the balance organs. Thetheory about postural instability says that the motion sickness is caused when the braindoesn’t have control over the posture of the body.The different solutions analysed were active suspension, installing a screen on which thepassengers do all the activities and a Virtual Reality headset where the passengers gets apreview of the road to make their body prepare for the movement of the cars so that theycan make a countermovement. The conclusions are that the VR-headset can’t be usedbecause it limits the user possibilities to do other activities while traveling. The screen inthe middle of the passenger’s view can work for the activities that can be done using ascreen but doesn’t work for other activities. The solution with active suspension is apromising solution but perhaps a bit expensive.Finally a new solution to the problem is presented by the authors. The new solution isactive suspension of the car seats and is a mix of the two analysed solutions VirtualRealityheadset and active suspension.</p>


corrected abstract:
<p>Much research is done today on how to make vehicles autonomous. But the main focus lies in how to make the techniques and safety sufficient. This means that the comfort for the passengers has fallen behind. Studies show that approximately 25 % of the users of autonomous vehicles would experience motion sickness.</p><p>The purpose of this thesis is to use existing hypotheses about what causes motion sickness to analyse different technical solutions that could decrease motion sickness in autonomous vehicles.</p><p>To investigate this a literature study is done. Only existing research and experiments are used.</p><p>The report is based on the theories about the sensor conflict and postural instability. The theory about the sensor conflict means that a person gets symptoms of motion sickness when the visual impression doesn’t match with the ones from the balance organs. The theory about postural instability says that the motion sickness is caused when the brain doesn’t have control over the posture of the body.</p><p>The different solutions analysed were active suspension, installing a screen on which the passengers do all the activities and a Virtual Reality headset where the passengers gets a preview of the road to make their body prepare for the movement of the cars so that they can make a countermovement. The conclusions are that the VR-headset can’t be used because it limits the user possibilities to do other activities while traveling. The screen in the middle of the passenger’s view can work for the activities that can be done using a screen but doesn’t work for other activities. The solution with active suspension is a promising solution but perhaps a bit expensive.</p><p>Finally a new solution to the problem is presented by the authors. The new solution is active suspension of the car seats and is a mix of the two analysed solutions Virtual Reality headset and active suspension.</p>
----------------------------------------------------------------------
In diva2:408836 abstract is:
<p>This study illuminates how the science center as a concept can be developed and how asociocultural perspective on learning influences the design of an interactive exhibit. The aimof the study is to propose ideas on how a science center can be designed and developed withthe purpose of creating good conditions for learning.The work was divided in three parts. In the first part literature was studied with theaim of highlighting aspects important for learning from a sociocultural perspective. In thesecond part an educational model was formulated based on the result from the literature study,interviews and study visits. The educational model was then used to guide the design of aninteractive exhibit on hydro power. The third part consists of an evaluation of the exhibitbased on observation of the visitors’ interaction with the exhibit.In this thesis the work and the result of the three parts are presented leading to a finaldiscussion about the key question of the study: How can learning possibilities be createdthrough the design of an interactive exhibition?Creating possibilities for learning is about creating possibilities for activities thatmakes learning possible. Through the design and in the choice of content of an exhibition it ispossible to create more or less good conditions for learning. Therefore it is important to havea clear picture of what type of activities it is desirable that an exhibition invite the visitor to,for example cooperation and conversation and the visitors’ possibility to influence the resultof the activity.The traditional science center is often criticized for presenting science and technologyas something static and finished. To create interest and engagement for the subject area it isinstead needed to be presented from a wide range of perspectives. Therefore throughout thework of developing an interactive exhibition or a science center it is important to discuss howthe subject area can be presented to fulfil this aim.Creating and developing an exhibition is a work that involves a number of people withdifferent backgrounds, knowledge and ideas. Just as in any other project that involves manypeople a clear ambition with clear goals is needed and makes a shared vision possible that canlead and steer all parts of the work.</p>

corrected abstract:
<p>This study illuminates how the science center as a concept can be developed and how a sociocultural perspective on learning influences the design of an interactive exhibit. The aim of the study is to propose ideas on how a science center can be designed and developed with the purpose of creating good conditions for learning.</p><p>The work was divided in three parts. In the first part literature was studied with the aim of highlighting aspects important for learning from a sociocultural perspective. In the second part an educational model was formulated based on the result from the literature study, interviews and study visits. The educational model was then used to guide the design of an interactive exhibit on hydro power. The third part consists of an evaluation of the exhibit based on observation of the visitors’ interaction with the exhibit.</p><p>In this thesis the work and the result of the three parts are presented leading to a final discussion about the key question of the study: How can learning possibilities be created through the design of an interactive exhibition?</p><p>Creating possibilities for learning is about creating possibilities for activities that makes learning possible. Through the design and in the choice of content of an exhibition it is possible to create more or less good conditions for learning. Therefore it is important to have a clear picture of what type of activities it is desirable that an exhibition invite the visitor to, for example cooperation and conversation and the visitors’ possibility to influence the result of the activity.</p><p>The traditional science center is often criticized for presenting science and technology as something static and finished. To create interest and engagement for the subject area it is instead needed to be presented from a wide range of perspectives. Therefore throughout the work of developing an interactive exhibition or a science center it is important to discuss how the subject area can be presented to fulfil this aim.</p><p>Creating and developing an exhibition is a work that involves a number of people with different backgrounds, knowledge and ideas. Just as in any other project that involves many people a clear ambition with clear goals is needed and makes a shared vision possible that can lead and steer all parts of the work.</p>
----------------------------------------------------------------------
In diva2:401129 abstract is:
<p>CFD use has increased signi cantly in airplane conception, and the industry demands more andmore precise and reliable tools. This was the goal of the SimSAC project. The result is CEASIOM,a computerized environment made of several modules for the design and prediction of the aircraft'scharacteristics. It constructs aerodynamic tables used in the prediction of the characteristics of anaircraft. In simple ight conditions, simple computation methods are used, whereas in complex ightconditions,involving turbulences, more advanced methods are used. This reduces the computationalcost, but the tables resulting from di erent delity sources must be fused to obtain a coherent tablecovering the whole ight envelope.The goal of this project was to realize the fusion. Additionally, a lter and a custom-made mapping toenhance the accuracy of the results from the fusion were required. The addition of helpful visualizationtools was suggested. The whole should be integrated in the CEASIOM interface as a Fusion module.For this, 6 functions were coded. The rst one loads the data sets. The second, myplot, allows theengineer by plotting the data in a coherent way, to spot any big mistakes or incompatibility in thedata sets. The third, myvisual, displays the elements spotted as outliers or potentially out of pattern.This is used by the next function, my ltermap, to lter out the erroneous data. This function alsorealizes the custom-made mapping.The fth function, myfusion, fuses the data and saves it in a .xmlCEASIOM formatted structure to be used by the next CEASIOM module. The sixth function ltersout, in the same way as my ltermap, the outliers from the fused data, and saves the ltered fused dataset in a .xml CEASIOM formatted structure. Finally, a Matlab GUI was implemented and integratedinto the main CEASIOM interface.The module works perfectly, except for the mapping part, that needs a few readjustments.</p>


corrected abstract:
<p>CFD use has increased significantly in airplane conception, and the industry demands more and more precise and reliable tools. This was the goal of the SimSAC project. The result is CEASIOM, a computerized environment made of several modules for the design and prediction of the aircraft's characteristics. It constructs aerodynamic tables used in the prediction of the characteristics of an aircraft. In simple flight conditions, simple computation methods are used, whereas in complex flight conditions, involving turbulences, more advanced methods are used. This reduces the computational cost, but the tables resulting from different fidelity sources must be fused to obtain a coherent table covering the whole flight envelope.</p><p>The goal of this project was to realize the fusion. Additionally, a filter and a custom-made mapping to enhance the accuracy of the results from the fusion were required. The addition of helpful visualization tools was suggested. The whole should be integrated in the CEASIOM interface as a Fusion module. For this, 6 functions were coded. The first one loads the data sets. The second, myplot, allows the engineer by plotting the data in a coherent way, to spot any big mistakes or incompatibility in the data sets. The third, myvisual, displays the elements spotted as outliers or potentially out of pattern. This is used by the next function, myfiltermap, to filter out the erroneous data. This function also realizes the custom-made mapping. The fifth function, myfusion, fuses the data and saves it in a .xml CEASIOM formatted structure to be used by the next CEASIOM module. The sixth function filters out, in the same way as myfiltermap, the outliers from the fused data, and saves the filtered fused data set in a .xml CEASIOM formatted structure. Finally, a Matlab GUI was implemented and integrated into the main CEASIOM interface.</p><p>The module works perfectly, except for the mapping part, that needs a few readjustments.</p>
----------------------------------------------------------------------
In diva2:1818051 abstract is:
<p>The oceans are a key element in our society, economy and environmental system.They cover over 70% of the worlds surface and contribute substantially to ecosystemservices such as climate management as well as to economic sectors such as foodproduction and tourism. While the importance of the oceans for climate changeand the society is generally acknowledged in science and literature, it is often notreflected in policy. Integrated Assessment Models (IAMs) which are used to advicepolicy on carbon prices often systematically omit process and damages related tothe ocean such as ocean acidification, loss of biodiversity and changes in oceancurrents.The aim of this study is to give a more detailed perspective on ocean related processesand their role and importance for the economy under climate change and to testassumptions made in the development of IAMs - and more precisely the DynamicIntegrated Climate-Economy model also referred to as the DICE model. The initialresults of the DICE model resulted in a optimal temperature trajectory with amaximum of 4 ◦C contradicting the goals set with the Paris Agreement.This thesis is the first of its kind attempt in reviewing the most recentbiophysical evidence on climate change impacts with a focus on marine systemsand incorporating these damages to market and non-market sectors into the DICEmodel. The impacts from climate change are implemented into the DICE modelthrough economic valuation of the damages and an update of the damage function.The analysis is based on the damage function used in the original DICE2016R2model as well as the suggested update presented by Hänsel et al. (2020)The results show, that incorporating marine damages into the model yields in amajor increase in economic damages particularly in the temperature range up to 2◦C.These increased damages influence the results of the optimal temperature trajectoryand give a clear indication for a more stringent climate policy, drastically limitingthe maximum temperature increase compared to the original DICE model.</p>


corrected abstract:
<p>The oceans are a key element in our society, economy and environmental system. They cover over 70% of the worlds surface and contribute substantially to ecosystem services such as climate management as well as to economic sectors such as food production and tourism. While the importance of the oceans for climate change and the society is generally acknowledged in science and literature, it is often not reflected in policy. Integrated Assessment Models (IAMs) which are used to advice policy on carbon prices often systematically omit process and damages related to the ocean such as ocean acidification, loss of biodiversity and changes in ocean currents.</p><p>The aim of this study is to give a more detailed perspective on ocean related processes and their role and importance for the economy under climate change and to test assumptions made in the development of IAMs - and more precisely the Dynamic Integrated Climate-Economy model also referred to as the DICE model. The initial results of the DICE model resulted in a optimal temperature trajectory with a maximum of 4 ˚C contradicting the goals set with the Paris Agreement.</p><p>This thesis is the first of its kind attempt in reviewing the most recent biophysical evidence on climate change impacts with a focus on marine systems and incorporating these damages to market and non-market sectors into the DICE model. The impacts from climate change are implemented into the DICE model through economic valuation of the damages and an update of the damage function. The analysis is based on the damage function used in the original DICE2016R2 model as well as the suggested update presented by Hänsel et al. (2020)</p><p>The results show, that incorporating marine damages into the model yields in a major increase in economic damages particularly in the temperature range up to 2˚C. These increased damages influence the results of the optimal temperature trajectory and give a clear indication for a more stringent climate policy, drastically limiting the maximum temperature increase compared to the original DICE model.</p>
----------------------------------------------------------------------
In diva2:1800176 abstract is:
<p>Bolted joints are important due to their energy dissipation property in structures,but the damping mechanism is also highly nonlinear and localized. The goal ofthis thesis is to develop an accurate method for modeling bolted joint dampingin large structures using finite element (FE) software. To model bolted jointdamping, the first step is to study the mechanism and define the terms like slip,micro-slip, and macro-slip. An extensive literature review identified the necessarymethods: detailed contact model, thin-layer elements, and connector elements.These methods are compared based on parameters such as computation time,modeling time, etc. The thin-layer method was used for modeling bolted jointdamping in large structures.</p><p>To evaluate parameters for thin-layer element modeling, a local joint model wasbuilt using contact formulation of an engine housing and ladder frame assembly.The computed parameters include normal stiffness, tangential stiffness, and lossfactor. Analysis reveals that the loss factor depends on pre-load and amplitudeload. The micro-slip is the region of interest where the loss factor was computed. Using curve-fitting, a range of amplitude-dependent loss factors was calculated.</p><p>Finally, the thin-layer elements are used in the engine housing and ladder frameassembly to model bolted joint damping. The parameters estimated using the localjoint model are used to define the properties of the thin-layer elements such thatthe elements are a phenomenological representation of bolted joint. A mode-basedsteady-state analysis has been performed to estimate the loss factor on a systemlevel. The frequency response of such an analysis accurately captures the frequencyresponse curves of structures with bolted joints. The two important behaviors thathave been captured are the shifting of the resonance peak to a lower value and thewidening of the frequency response curve as the applied load increases. However,the resonance frequency shifting to a lower frequency (softening) has not beencaptured due to modeling limitations in the FE software. A substructure couplingmodel using the Craig-Bampton formulation of the engine housing and ladderframe assembly has been analyzed using a constant loss factor. The frequencyresponse of such a system appears to give an approximate behavior of a structurewith bolted joint damping.</p>

corrected abstract:
<p>Bolted joints are important due to their energy dissipation property in structures, but the damping mechanism is also highly nonlinear and localized. The goal of this thesis is to develop an accurate method for modeling bolted joint damping in large structures using finite element (FE) software. To model bolted joint damping, the first step is to study the mechanism and define the terms like slip, micro-slip, and macro-slip. An extensive literature review identified the necessary methods: detailed contact model, thin-layer elements, and connector elements. These methods are compared based on parameters such as computation time, modeling time, etc. The thin-layer method was used for modeling bolted joint damping in large structures.</p><p>To evaluate parameters for thin-layer element modeling, a local joint model was built using contact formulation of an engine housing and ladder frame assembly. The computed parameters include normal stiffness, tangential stiffness, and loss factor. Analysis reveals that the loss factor depends on pre-load and amplitude load. The micro-slip is the region of interest where the loss factor was computed. Using curve-fitting, a range of amplitude-dependent loss factors was calculated.</p><p>Finally, the thin-layer elements are used in the engine housing and ladder frame assembly to model bolted joint damping. The parameters estimated using the local joint model are used to define the properties of the thin-layer elements such that the elements are a phenomenological representation of bolted joint. A mode-based steady-state analysis has been performed to estimate the loss factor on a system level. The frequency response of such an analysis accurately captures the frequency response curves of structures with bolted joints. The two important behaviors that have been captured are the shifting of the resonance peak to a lower value and the widening of the frequency response curve as the applied load increases. However, the resonance frequency shifting to a lower frequency (softening) has not been captured due to modeling limitations in the FE software. A substructure coupling model using the Craig-Bampton formulation of the engine housing and ladder frame assembly has been analyzed using a constant loss factor. The frequency response of such a system appears to give an approximate behavior of a structure with bolted joint damping.</p>
----------------------------------------------------------------------
In diva2:1698414 abstract is:
<p>The impact of air travel on the climate, along with its increasing share in CO2 emissions haveraised the demand for sustainable air travel solutions. The current aircraft technologies haveseen significant improvement throughout the years. Although, the rate at which new aircrafttechnologies are developed can not keep up with the increased demand for air travel. Hence, adifferent approach to reduce the aviation’s impact on climate can be achieved by optimizing thevertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming.Upon departure, an optimization of the vertical flight path is initiated and an optimal flight planis suggested to the flight crew.</p><p>The fuel saving produced by the optimal flight plan is a potential saving that can only be fullyachieved if the flight crew chose to fly according to the optimized flight path. However, restrictionsfrom the Air Traffic Control, as well as the flight crew’s willingness to follow theoptimized flight path can affect the achieved saving. Hence, a tool is developed in order tocompute trip fuel consumption from post-flight data obtained from the Automatic DependentSurveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start andend positions of cruise segments is successfully implemented. Two methods of calculating thefuel are implemented and compared. The first method is based on simulating the actual flight,which uses the same performance model as for the simulation of the operational flight plantrip and optimized trip. The second method is based on utilizing the ADS-B data to obtain theaircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft,hence the trip is not simulated. The results reveals that the simulation method produces flighttrajectories that are comparable to the operational and optimized flight plans since they use thesame model structure. However, using ADS-B data to obtain fuel consumption represents theactual flight trajectory more accurately.</p><p>Furthermore, an optimization algorithm based on the on-board Flight Management Computeris implemented. According to the results, the FMC optimization offers a sufficient optimizationof the cruise phase, when compared to the OFP trip, however performs worse than the dynamicprogramming, which provides a global optimal solution</p>


corrected abstract:
<p>The impact of air travel on the climate, along with its increasing share in CO2 emissions have raised the demand for sustainable air travel solutions. The current aircraft technologies have seen significant improvement throughout the years. Although, the rate at which new aircraft technologies are developed can not keep up with the increased demand for air travel. Hence, a different approach to reduce the aviation’s impact on climate can be achieved by optimizing the vertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming. Upon departure, an optimization of the vertical flight path is initiated and an optimal flight plan is suggested to the flight crew.</p><p>The fuel saving produced by the optimal flight plan is a potential saving that can only be fully achieved if the flight crew chose to fly according to the optimized flight path. However, restrictions from the Air Traffic Control, as well as the flight crew’s willingness to follow the optimized flight path can affect the achieved saving. Hence, a tool is developed in order to compute trip fuel consumption from post-flight data obtained from the Automatic Dependent Surveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start and end positions of cruise segments is successfully implemented. Two methods of calculating the fuel are implemented and compared. The first method is based on simulating the actual flight, which uses the same performance model as for the simulation of the operational flight plan trip and optimized trip. The second method is based on utilizing the ADS-B data to obtain the aircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft, hence the trip is not simulated. The results reveals that the simulation method produces flight trajectories that are comparable to the operational and optimized flight plans since they use the same model structure. However, using ADS-B data to obtain fuel consumption represents the actual flight trajectory more accurately.</p><p>Furthermore, an optimization algorithm based on the on-board Flight Management Computer is implemented. According to the results, the FMC optimization offers a sufficient optimization of the cruise phase, when compared to the OFP trip, however performs worse than the dynamic programming, which provides a global optimal solution</p>
----------------------------------------------------------------------
In diva2:1216784 missing space in title:
"Mobile Network trafficprediction: Based on machine learning"
==>
"Mobile Network traffic prediction: Based on machine learning"

The following abstract is in DiVA - but it belongs to another thesis. Compare to the Swedish abstract!

abstract is:
<p>The investing market can be a cold ruthless placefor the layman. In order to get the chance of making money inthis business one must place countless hours on research, withmany different parameters to handle in order to reach success.To reduce the risk, one must look to many different companiesoperating in multiple fields and industries. In other words, it canbe a hard task to manage this feat.With modern technology, there is now lots of potential tohandle this tedious analysis autonomously using machine learningand clever algorithms. With this approach, the amount ofanalyzes is only limited by the capacity of the computer. Resultingin a number far greater than if done by hand.This study aims at exploring the possibilities to modify andimplement efficient algorithms in the field of finance. The studyutilizes the power of kernel methods in order to algorithmicallyanalyze the patterns found in financial data efficiently. Bycombining the powerful tools of change point detection andnonlinear regression the computer can classify the differenttrends and moods in the market.The study culminates to a tool for analyzing data from thestock market in a way that minimizes the influence from shortspikes and drops, and instead is influenced by the underlying pattern.But also, an additional tool for predicting future movementsin the price.</p>

corrected abstract:
<p>The amount of data traffic sent through mobile networks varies throughout the day and week. Thus, the network experiences varying demand and therefore, the load on all the back end systems in the core network is far from constant. By being able to predict the load, the back end system capacity can be optimized during the day, reducing maintenance costs and energy consumption, affecting the environment positively. The predictions may also be used for network planning.</p><p>The aim of this project was to predict the mobile network data traffic based on two weeks of data aggregated into five minute intervals. The data was treated as a time series and time series forecasting methods were used, the ARIMA model using external regressors based on a polynomial model and a Fourier series as well as the TBATS model. Also, a recurrent neural network based on a method called Long Short Term Memory was used.</p><p>The results show that the seasonal components of the time series are modelled well using simple methods such as a polynomial model or Fourier series. However, modelling the dynamics of the stationary time series is very difficult and the ARIMA model did not perform well in this situation due to the long time predictions made. Neither did the neural network or TBATS model manage to model the stationary dynamics and were only able to capture the seasonal components.</p>
----------------------------------------------------------------------
In diva2:1799891 abstract is:
<p>This thesis examines water mixing and exchange in a drinking water reservoir operated by themunicipal association Norrvatten. Recent water samples from the reservoir’s outgoing waterhave shown an increase in culturable bacteria during late summer and fall. This thesis utilizesComputational Fluid Dynamics (CFD) modeling and analysis in OpenFOAM to simulatereservoir inflow and outflow, analyzing mixing processes and their relationship to operationalstrategies. The objective is to understand the correlation between the residence time of waterand microbial growth and propose operational improvements to increase the exchange of waterin order to achieve improved water quality. A trace element was implemented in the CFDmodel to simulate the residence time of water. Initial simulations were based on the reservoir’shistorical operational data, utilizing temperature and water level measurements providedby Norrvatten. After the initial simulations, four alternative simulations were performed,comparing different operational strategies by modifying inflow parameters. Inflow parametersthat were changed were the volumetric inflow rate, water level variation, and the temperatureof the inflowing water. The post­processing in ParaView focused on the thermal stratificationand residence time distribution near the outlet during each mixing process. The study revealeda complex relationship between flow conditions and microbial growth, making it challengingto identify a clear pattern. However, based on the simulations with the alternative operationalstrategies it was concluded that the set of operational strategies called ”Strategy 1” generated themost optimal flow conditions. This strategy involves a three times larger volumetric inflow rate(an increase from 0.05 to 0.15 m^3/s) and a water level that is kept at the same values comparedto the original simulation. Strategy 1 resulted in a 3.6 % higher water exchange compared to theoriginal simulation. In comparison to the other simulated strategies, Strategy 1 generates thehighest water exchange, with a 63.6 % increase compared to the worst­-case scenario involvingcolder inflow. The conclusion that could be drawn is that the most favorable operationalstrategies involve higher volumetric inflow rates, lower water levels, and an incoming watertemperature that is higher than the initial reservoir temperature.</p><p> </p>

corrected abstract:
<p>This thesis examines water mixing and exchange in a drinking water reservoir operated by the municipal association Norrvatten. Recent water samples from the reservoir’s outgoing water have shown an increase in culturable bacteria during late summer and fall. This thesis utilizes Computational Fluid Dynamics (CFD) modeling and analysis in OpenFOAM to simulate reservoir inflow and outflow, analyzing mixing processes and their relationship to operational strategies. The objective is to understand the correlation between the residence time of water and microbial growth and propose operational improvements to increase the exchange of water in order to achieve improved water quality. A trace element was implemented in the CFD model to simulate the residence time of water. Initial simulations were based on the reservoir’s historical operational data, utilizing temperature and water level measurements provided by Norrvatten. After the initial simulations, four alternative simulations were performed, comparing different operational strategies by modifying inflow parameters. Inflow parameters that were changed were the volumetric inflow rate, water level variation, and the temperature of the inflowing water. The post­processing in ParaView focused on the thermal stratification and residence time distribution near the outlet during each mixing process. The study revealed a complex relationship between flow conditions and microbial growth, making it challenging to identify a clear pattern. However, based on the simulations with the alternative operational strategies it was concluded that the set of operational strategies called ”Strategy 1” generated the most optimal flow conditions. This strategy involves a three times larger volumetric inflow rate (an increase from 0.05 to 0.15 m<sup>3</sup>/s) and a water level that is kept at the same values compared to the original simulation. Strategy 1 resulted in a 3.6 % higher water exchange compared to the original simulation. In comparison to the other simulated strategies, Strategy 1 generates the highest water exchange, with a 63.6 % increase compared to the worst-case scenario involving colder inflow. The conclusion that could be drawn is that the most favorable operational strategies involve higher volumetric inflow rates, lower water levels, and an incoming water temperature which is higher than the initial reservoir temperature.</p>
----------------------------------------------------------------------
In diva2:1740181 abstract is:
<p>This project is about the design process of a resonancemitigating algorithm for a large solar sail. The solar sail isa structure made up by four long booms in a cross patternwith suspended sails between the edges of the booms. Thespacecraft in this report is controlled using smaller rotatingsails at each tip of the booms. Since the booms are longthey will experience significant bending moment even thoughthe actual force from the control sails will be small. Thisbending has a high risk of exciting the spacecraft’s resonancemodes, which will in turn complicate the control of theentire spacecraft. Because of this it is necessary to designan algorithm to mitigate resonance excitation. To design thisalgorithm three main steps were taken, an approximation ofspacecraft resonance modes, a valid mathematical model ofthe system and robustness building to handle model error.The resonance modes were approximated through analyticalformulas. The boom model was determined by combiningdifferent approaches from previous similar works. Finally,the controller chosen was a simple PID controller with abuilt-in saturation limiter to make sure the controller stayswithin the spacecraft’s operating bounds. To ensure robustness,multiple test simulations were made on systems with differentresonance modes compared to the true system. The chosencontroller passed these tests. Ultimately the chosen controllerreduced the settling time of the resonance oscillations by 75%.</p>

corrected abstract:
<p>This project is about the design process of a resonance mitigating algorithm for a large solar sail. The solar sail is a structure made up by four long booms in a cross pattern with suspended sails between the edges of the booms. The spacecraft in this report is controlled using smaller rotating sails at each tip of the booms. Since the booms are long they will experience significant bending moment even though the actual force from the control sails will be small. This bending has a high risk of exciting the spacecraft’s resonance modes, which will in turn complicate the control of the entire spacecraft. Because of this it is necessary to design an algorithm to mitigate resonance excitation. To design this algorithm three main steps were taken, an approximation of spacecraft resonance modes, a valid mathematical model of the system and robustness building to handle model error. The resonance modes were approximated through analytical formulas. The boom model was determined by combining different approaches from previous similar works. Finally, the controller chosen was a simple PID controller with a built-in saturation limiter to make sure the controller stays within the spacecraft’s operating bounds. To ensure robustness, multiple test simulations were made on systems with different resonance modes compared to the true system. The chosen controller passed these tests. Ultimately the chosen controller reduced the settling time of the resonance oscillations by 75%.</p>
----------------------------------------------------------------------
In diva2:1571119 abstract is:
<p>Shaped charges (SC) have been used as a means of explosives in military andcivilian use for decades. Thus, there is a substantial amount of research behindthis area. However, as this is a sensitive subject much of this research is notpublicly available.</p><p>This thesis will look at how one can use asymmetries in SC’s to velocity compensatethe jet formation. Velocity compensation is required when the SC is perpendicularto the projectile direction, hence, leading to an angled jet which decreases thepenetration potential.</p><p>The asymmetries that were investigated are• off-­center detonation• angled liner• displaced wave shaper• displaced wave shaper &amp; angled liner.</p><p>The 3D explosive simulation was conducted in IMPETUS AFEA solver and tocompare the performance of these asymmetries the position and velocity of thejet were measured. To create a baseline a simulation without any asymmetrieswas used.</p><p>The off­-center detonation showed some velocity compensating characteristicsat the tip of the jet. However, as the jet progressed it converged towards thereference.</p><p>Angled liner simulations were conducted with an angle of 0.5 degrees and 1 degreeand these asymmetries behaved vastly differently. Angled Liner 0.5 degrees hada greater jet angle but a greater quantity of the jet particles were concentratedaround one point increasing the penetration potential. A general characteristicthat angled liner displaced was the fact that it had desirable velocity compensatingtraits all through the jet.</p><p>Displaced Wave Shaper, like off­-center detonation, showed promising velocitycompensating attributes at the tip of the jet, however, it too converged towardsthe reference on the later part of the jet.</p><p>When combining the displaced wave shaper and angled liner asymmetries thedesire was to also combine their velocity compensating traits, i.e achievingthe displaced wave shaper’s tip compensation with the angled liner’s totalcompensation. Unfortunately, this was not achieved. The tip, again, showedpromising velocity compensating attributes but the rest of the jet convergedtowards the reference.</p><p>Conclusively, angled liner shows the highest potential for compensating thevelocity and allowed the most amount of jet particles to be concentrated aroundone point increasing the penetration potential.</p><p> </p>

corrected abstract:
<p>Shaped charges (SC) have been used as a means of explosives in military and civilian use for decades. Thus, there is a substantial amount of research behind this area. However, as this is a sensitive subject much of this research is not publicly available.</p><p>This thesis will look at how one can use asymmetries in SC’s to velocity compensate the jet formation. Velocity compensation is required when the SC is perpendicular to the projectile direction, hence, leading to an angled jet which decreases the penetration potential.</p><p>The asymmetries that were investigated are<ul><li>off-center detonation</li><li>angled liner</li><li>displaced wave shaper</li><li>displaced wave shaper &amp; angled liner.</li></ul></p><p>The 3D explosive simulation was conducted in IMPETUS AFEA solver and to compare the performance of these asymmetries the position and velocity of the jet were measured. To create a baseline a simulation without any asymmetries was used.</p><p>The off-center detonation showed some velocity compensating characteristics at the tip of the jet. However, as the jet progressed it converged towards the reference.</p><p>Angled liner simulations were conducted with an angle of 0.5 degrees and 1 degree and these asymmetries behaved vastly differently. Angled Liner 0.5 degrees had a greater jet angle but a greater quantity of the jet particles were concentrated around one point increasing the penetration potential. A general characteristic that angled liner displaced was the fact that it had desirable velocity compensating traits all through the jet.</p><p>Displaced Wave Shaper, like off-center detonation, showed promising velocity compensating attributes at the tip of the jet, however, it too converged towards the reference on the later part of the jet.</p><p>When combining the displaced wave shaper and angled liner asymmetries the desire was to also combine their velocity compensating traits, i.e achieving the displaced wave shaper’s tip compensation with the angled liner’s total compensation. Unfortunately, this was not achieved. The tip, again, showed promising velocity compensating attributes but the rest of the jet converged towards the reference.</p><p>Conclusively, angled liner shows the highest potential for compensating the velocity and allowed the most amount of jet particles to be concentrated around one point increasing the penetration potential.</p>
----------------------------------------------------------------------
In diva2:1527916 - missing space in title:
"Connected Tyres: Real-time Tyre Monitoring System for Fleet& Autonomous Vehicles with Tyre WearEstimation through Sensor Fusion"
==>
"Connected Tyres: Real-time Tyre Monitoring System for Fleet & Autonomous Vehicles with Tyre Wear Estimation through Sensor Fusion"

abstract is:
<p>Tyres are one crucial part for vehicles, as they are the only contact pointbetween the vehicle and the road. Intelligent tyres are a trending new subjectin the tyre industry. They are designed to monitor various tyre states and sendthis information to both drivers and remote servers. The master thesis focuseson the proposal of a real-time tyre monitoring system for fleet and autonomousvehicles. It includes developing a tyre wear model and analysis of the currenttyre pressure monitoring functionality by leveraging the connectivity of fleetvehicles equipped with a Volvo web cloud service. The tyre wear model indirectlymonitors the tread depth of the vehicles all four tyres by identifyingcharacteristics between worn and fresh tyres. The two characteristics are identifiedby monitoring and analyzing vehicle speed and braking signals. The twocharacteristics is input to a voting scheme which decides when a worn tyre isdetected. The test vehicle was a Volvo XC40 with three types of tyres: wintertyres, summer tyres and worn summer tyres. The wear model gives 90 %accuracy to 10 set of test data, randomly selected from all dataset at HälleredProving Ground (Sweden). The connectivity realizes the data transmissionfrom the raw data of onCAN and FlexRay signals stored in a Volvo web cloudservice to the tyre monitoring fleet system. The signals are filtered and resampled,leaving the required signals of the tyre pressure monitor system andthe tyre wear model. Two signals, Calibration Status and iTPMS Status, areused to perform a statistical analysis on tyre pressure by categorizing the calibrationstatus and the tyre pressure conditions.The project outcome is an interfacebuilt on MATLAB GUI for demonstration of vehicle identification andtyre health conditions, with the embedded tyre wear model and connectivity.</p>

corrected abstract:
<p>Tyres are one crucial part for vehicles, as they are the only contact point between the vehicle and the road. Intelligent tyres are a trending new subject in the tyre industry. They are designed to monitor various tyre states and send this information to both drivers and remote servers. The master thesis focuses on the proposal of a real-time tyre monitoring system for fleet and autonomous vehicles. It includes developing a tyre wear model and analysis of the current tyre pressure monitoring functionality by leveraging the connectivity of fleet vehicles equipped with a Volvo web cloud service. The tyre wear model indirectly monitors the tread depth of the vehicles all four tyres by identifying characteristics between worn and fresh tyres. The two characteristics are identified by monitoring and analyzing vehicle speed and braking signals. The two characteristics is input to a voting scheme which decides when a worn tyre is detected. The test vehicle was a Volvo XC40 with three types of tyres: winter tyres, summer tyres and worn summer tyres. The wear model gives 90 % accuracy to 10 set of test data, randomly selected from all dataset at Hällered Proving Ground (Sweden). The connectivity realizes the data transmission from the raw data of on CAN and FlexRay signals stored in a Volvo web cloud service to the tyre monitoring fleet system. The signals are filtered and resampled, leaving the required signals of the tyre pressure monitor system and the tyre wear model. Two signals, Calibration Status and iTPMS Status, are used to perform a statistical analysis on tyre pressure by categorizing the calibration status and the tyre pressure conditions. The project outcome is an interface built on MATLAB GUI for demonstration of vehicle identification and tyre health conditions, with the embedded tyre wear model and connectivity.</p>
----------------------------------------------------------------------
In diva2:1142785 - possible duplicate 'diva2:1120402

abstract is:
<p>Due to demographic changes, the transportationdemand is predicted to increase significantly in the next decades.Considering the transport sector’s impact on society and theenvironment, the development of a sustainable transport systemis of great importance. Two possible building blocks in such asystem are connectivity and automation, and this project aims tostudy a way of combining these two.The purpose of this project is to investigate how the introductionof autonomous minibuses to a pre-existing bus systemwould affect its operational cost and environmental impact. Thisis done using a linear programming model that finds the optimalcombination of conventional buses and autonomous minibuseswith respect to cost. The model is implemented in the modellingsystem GAMS for bus lines 1–4 in Stockholm using data ontravel demand. Two scenarios are analysed; the first allowingan arbitrary number of minibuses, and the second being morerealistic and restricting the number of minibuses. The solutionsare then compared to the corresponding solutions using onlyconventional buses.In both cases, the results indicate that considerable savingscan be obtained while maintaining or even improving availability.From this, we draw the conclusion that when such technologyis truly available, it would be advisable to investigate if thesesavings can weigh up the costs related to necessary investments.</p>

corrected abstract:
<p>Due to demographic changes, the transportation demand is predicted to increase significantly in the next decades. Considering the transport sector’s impact on society and the environment, the development of a sustainable transport system is of great importance. Two possible building blocks in such a system are connectivity and automation, and this project aims to study a way of combining these two.</p><p>The purpose of this project is to investigate how the introduction of autonomous minibuses to a pre-existing bus system would affect its operational cost and environmental impact. This is done using a linear programming model that finds the optimal combination of conventional buses and autonomous minibuses with respect to cost. The model is implemented in the modelling system GAMS for bus lines 1–4 in Stockholm using data on travel demand. Two scenarios are analysed; the first allowing an arbitrary number of minibuses, and the second being more realistic and restricting the number of minibuses. The solutions are then compared to the corresponding solutions using only conventional buses.</p><p>In both cases, the results indicate that considerable savings can be obtained while maintaining or even improving availability. From this, we draw the conclusion that when such technology is truly available, it would be advisable to investigate if these savings can weigh up the costs related to necessary investments.</p>
----------------------------------------------------------------------
In diva2:1110767 abstract is:
<p>The present work reports the first systematic results obtained in wind tunnel and at full scale with theexperimental apparatus developed within the joint project among Politecnico di Milano, North Sails andCSEM. The steady state upwind aerodynamics of sailing yachts are investigated through the contemporarymeasurement of global forces, distributed pressures and sail flying shapes, and with numerical simulationsbased on Potential Theory and Navier-Stokes equations.The wind tunnel of Politecnico di Milano and the Sailing Yacht Lab (SYL), used for full scale investigations,are described together with the measurement systems adopted for recording forces, pressures and sailshapes. The aerodynamic loads are obtained through dedicated arrangements of load cells; the pressuredistributions on sail sections are evaluated with integrated systems of customized local measurementsolutions and MEMS sensors; the sail flying shapes are detected thanks to two laser scanners based of theTime of Flight technology. The experimental procedures adopted during tests are presented and discussedin relation with the aim of the work.Numerical simulations of selected wind tunnel cases are performed in order to assess the capabilities of theempirical techniques to validate Computational Fluid Dynamics (CFD) codes and to propose solid numericalset-ups for investigating sailing yacht aerodynamics. A Vortex Lattice Method (VLM) code, written inMatlab, is used for quick preliminary analyses of the global forces developed by the sail plan, whereas theopen source environment OpenFOAM is adopted to perform 3D simulations for investigating in details thelocal flow patterns.The experimental apparatus, both at model and full scale, proved to be extremely well suited for thepurposes of the study, giving remarkable results regarding, in particular, pressures and sail shapes. Theexpected distributions are obtained during wind tunnel tests and interesting considerations arise from thecomparison with the full scale outcomes. Numerically, the results of simulations meet the preliminaryintuitions with the VLM code capable of accurately predicting global forces up to certain wind angles andthe RANS-based computations providing notable agreement with the measured local pressures.</p>

corrected abstract:
<p>The present work reports the first systematic results obtained in wind tunnel and at full scale with the experimental apparatus developed within the joint project among Politecnico di Milano, North Sails and CSEM. The steady state upwind aerodynamics of sailing yachts are investigated through the contemporary measurement of global forces, distributed pressures and sail flying shapes, and with numerical simulations based on Potential Theory and Navier-Stokes equations.</p><p>The wind tunnel of Politecnico di Milano and the Sailing Yacht Lab (SYL), used for full scale investigations, are described together with the measurement systems adopted for recording forces, pressures and sail shapes. The aerodynamic loads are obtained through dedicated arrangements of load cells; the pressure distributions on sail sections are evaluated with integrated systems of customized local measurement solutions and MEMS sensors; the sail flying shapes are detected thanks to two laser scanners based of the Time of Flight technology. The experimental procedures adopted during tests are presented and discussed in relation with the aim of the work.</p><p>Numerical simulations of selected wind tunnel cases are performed in order to assess the capabilities of the empirical techniques to validate Computational Fluid Dynamics (CFD) codes and to propose solid numerical set-ups for investigating sailing yacht aerodynamics. A Vortex Lattice Method (VLM) code, written in Matlab, is used for quick preliminary analyses of the global forces developed by the sail plan, whereas the open source environment OpenFOAM is adopted to perform 3D simulations for investigating in details the local flow patterns.</p><p>The experimental apparatus, both at model and full scale, proved to be extremely well suited for the purposes of the study, giving remarkable results regarding, in particular, pressures and sail shapes. The expected distributions are obtained during wind tunnel tests and interesting considerations arise from the comparison with the full scale outcomes. Numerically, the results of simulations meet the preliminary intuitions with the VLM code capable of accurately predicting global forces up to certain wind angles and the RANS-based computations providing notable agreement with the measured local pressures.</p>
----------------------------------------------------------------------
In diva2:1083779 abstract is:
<p>Carbonated sparkling water has been widely used from ancient age [1]. The original ideacame from natural sparkling water and people believed that taking baths at carbonatedhot springs was good for health and healed their sicknesses. This fact led people to startthinking that sparkling water could have more effective uses. Joseph Priestley success-fully produced artificial carbonated water in 1767 and sparkling water quickly becamewidely spread because it gives people refreshing feeling. The bottled and canned beverageindustry has grown from the 19th century and has become one of the biggest markets inthe world. According to Bloomberg Intelligence and Euromonitor, the global market ofthe carbonated beverages is around 350 billion dollar. One main drawback was that itwas not possible to re-cork the bottle to save the carbonation so that once it was opened,fizz was kept only for a short time. In 1813, the method to dispense a portion of carbon-ated water was invented by Charles Plinth[2]. This was the origin of the Soda Syphon.As the demand of sparkling water increased, the machine with which people could makesparkling water by themselves was introduced. Recently, it has become a very popularhome appliance, especially in Europe and North America. The most common way tocarbonate water is by injecting high-pressure CO2 into a water bottle. However, currentsystems waste a lot of CO2 during this carbonating process. In this thesis, the flow insidethe bottle during the injection of CO2 into water was studied in order to determine the pa-rameters that had most influence on the carbonation process. CFD (Computational FluidDynamics) simulations were performed in STAR-CCM+ of an axisymmetric 2D modeland a 3D model that was a 30 degree wedge of the real bottle shape. The Volume of Fluidmethod was used to solve the multiphase flow of gas and liquid. The RANS approachwas used with k 􀀀ϵ model and implicit time marching. To validate the simulations, axialpropagation of the volume fraction of CO2 was compared with the experimental visual-ization of the CO2 and H2O distribution. At the beginning of the phenomena, the gaspropagation was reasonably predicted and the results capture the features of the bubbleshape. However the results did not perfectly match with the experimental visualization.To seek the reason for the unrealistic results, the grid sensitivity study was performedand to consider the 3D effect the results with the 2D and the 3D model were compared.In addition, the bubble breakup process was deeply investigated.</p>

corrected abstract:
<p>Carbonated sparkling water has been widely used from ancient age [1]. The original idea came from natural sparkling water and people believed that taking baths at carbonated hot springs was good for health and healed their sicknesses. This fact led people to start thinking that sparkling water could have more effective uses. Joseph Priestley successfully produced artificial carbonated water in 1767 and sparkling water quickly became widely spread because it gives people refreshing feeling. The bottled and canned beverage industry has grown from the 19th century and has become one of the biggest markets in the world. According to Bloomberg Intelligence and Euromonitor, the global market of the carbonated beverages is around 350 billion dollar. One main drawback was that it was not possible to re-cork the bottle to save the carbonation so that once it was opened, fizz was kept only for a short time. In 1813, the method to dispense a portion of carbonated water was invented by Charles Plinth[2]. This was the origin of the Soda Syphon. As the demand of sparkling water increased, the machine with which people could make sparkling water by themselves was introduced. Recently, it has become a very popular home appliance, especially in Europe and North America. The most common way to carbonate water is by injecting high-pressure CO<sub>2</sub> into a water bottle. However, current systems waste a lot of CO<sub>2</sub> during this carbonating process. In this thesis, the flow inside the bottle during the injection of CO<sub>2</sub> into water was studied in order to determine the parameters that had most influence on the carbonation process. CFD (Computational Fluid Dynamics) simulations were performed in STAR-CCM+ of an axisymmetric 2D model and a 3D model that was a 30 degree wedge of the real bottle shape. The Volume of Fluid method was used to solve the multiphase flow of gas and liquid. The RANS approach was used with <em>k - ϵ</em> model and implicit time marching. To validate the simulations, axial propagation of the volume fraction of CO<sub>2</sub> was compared with the experimental visualization of the CO<sub>2</sub> and H<sub>2</sub>O distribution. At the beginning of the phenomena, the gas propagation was reasonably predicted and the results capture the features of the bubble shape. However the results did not perfectly match with the experimental visualization. To seek the reason for the unrealistic results, the grid sensitivity study was performed and to consider the 3D effect the results with the 2D and the 3D model were compared. In addition, the bubble breakup process was deeply investigated.</p>
----------------------------------------------------------------------
In diva2:1081137 - missing space in title:
"Revisiting Hot-Wire AnemometryClose to Solid Walls"
==>
"Revisiting Hot-Wire Anemometry Close to Solid Walls"

abstract is:
<p>A well-known problem of hot-wire anemometry (HWA), is the “wall effect”, namely theoverestimation of the measured velocity near a wall. The overestimation occurs due toadditional heat loss from the heated wire-sensor to the wall. The extra heat loss dependson parameters such as the heat conductivity of the wall material, the overheat ratio ofthe wire, and the sensor geometry. This problem has been studied for quite some timeand there are several suggestions with regard to the effect of these parameters for meanflow corrections, however the effect on measurements of turbulent fluctuation has notbeen investigated. The present work aims at providing further insight on this topic, byelucidating how these parameters affect measurements of both the mean and fluctuatingvelocity. Furthermore, the present study proposes a theoretical model on the total heattransfer from hot-wire sensor to explain the phenomenon.In the experimental part of the study, the measurements under both no flow and flowconditions are carried out to consider natural convection and forced convection separately.The results showed that the effect of the parameters is consistent with what is agreedwidely: higher wall conductivity, higher overheat ratio, and larger wire exposed area leadto higher output from an anemometer. On the other hand, it is observed that the conduc-tion under natural convection can be scaled with the overheat ratio. Velocity fluctuationsare found to decrease by employing higher overheat ratio and for walls with higher heatconductivity.In the numerical part of the study, a two-dimensional steady calculation using Open-FOAM is performed and the parameter dependency with respect to the overheat ratio andwall heat conductivity is investigated. The results qualitatively agree with the experi-mental results. Moreover, the inner scaling commonly employed in wall-turbulence isfound to be inadequate to resolve the wall effect of HWA when various sensor heights areconcerned.Lastly, a theoretical model on the total heat transfer from the wire close to solid wallsis established based on a superposition of the convection and the conduction contributions.The proposed model with the empirically determined coefficients is found to be capableof capturing the qualitative behaviours found in the experiment and numerical analysis, howewer for more practical use it leaves several issues to be further analysed.</p>

corrected abstract:
<p>A well-known problem of hot-wire anemometry (HWA), is the “wall effect”, namely the overestimation of the measured velocity near a wall. The overestimation occurs due to additional heat loss from the heated wire-sensor to the wall. The extra heat loss depends on parameters such as the heat conductivity of the wall material, the overheat ratio of the wire, and the sensor geometry. This problem has been studied for quite some time and there are several suggestions with regard to the effect of these parameters for mean flow corrections, however the effect on measurements of turbulent fluctuation has not been investigated. The present work aims at providing further insight on this topic, by elucidating how these parameters affect measurements of both the mean and fluctuating velocity. Furthermore, the present study proposes a theoretical model on the total heat transfer from hot-wire sensor to explain the phenomenon.</p><p>In the experimental part of the study, the measurements under both no flow and flow conditions are carried out to consider natural convection and forced convection separately. The results showed that the effect of the parameters is consistent with what is agreed widely: higher wall conductivity, higher overheat ratio, and larger wire exposed area lead to higher output from an anemometer. On the other hand, it is observed that the conduction under natural convection can be scaled with the overheat ratio. Velocity fluctuations are found to decrease by employing higher overheat ratio and for walls with higher heat conductivity.</p><p>In the numerical part of the study, a two-dimensional steady calculation using OpenFOAM is performed and the parameter dependency with respect to the overheat ratio and wall heat conductivity is investigated. The results qualitatively agree with the experimental results. Moreover, the inner scaling commonly employed in wall-turbulence is found to be inadequate to resolve the wall effect of HWA when various sensor heights are concerned.</p><p>Lastly, a theoretical model on the total heat transfer from the wire close to solid walls is established based on a superposition of the convection and the conduction contributions. The proposed model with the empirically determined coefficients is found to be capable of capturing the qualitative behaviours found in the experiment and numerical analysis, however for more practical use it leaves several issues to be further analysed.</p>
----------------------------------------------------------------------
In diva2:1894689 abstract is:
<p>This thesis evaluates the performance of four equity funds managed by SEBInvestment Management using the Fama-French three-factor model whichconsiders market risk, size (SMB), and value (HML) factors in addition to thegeneral market movement. The study aims to understand how these factorscontribute to the fund’s returns and to examine potential investment biasesthat could influence the funds performance.The analysis revealed varying degrees of sensitivity to these factors amongthe funds, overall reflecting their distinct investment strategies. For instance,the small cap fund displayed a significant positive relationship with SMB,indicating a tendency to benefit from investments in smaller companies.Conversely, the value fund showed a positive HML coeﬀicient, suggestinga preference for value stocks. However, several of the funds saw correlationwith development of small cap stocks.Results indicate that while market risk remains a dominant factor, size andvalue significantly contribute to fund performance. This is offering insightsbeyond the traditional CAPM model. The study’s findings are significant,suggesting that actively managed funds can exhibit distinct behavioral patternswhich could be systematically explored to enhance investment strategies anddecision making.The thesis concludes with recommendations for extending this research toinclude additional factors like momentum and liquidity which could providea deeper understanding of the influences affecting fund performance. Theadoption of multi factor models may offer a more comprehensive frameworkfor predicting stock returns and assisting portfolio management.</p>

corrected abstract:
<p>This thesis evaluates the performance of four equity funds managed by SEB Investment Management using the Fama-French three-factor model which considers market risk, size (SMB), and value (HML) factors in addition to the general market movement. The study aims to understand how these factors contribute to the fund’s returns and to examine potential investment biases that could influence the funds performance.</p><p>The analysis revealed varying degrees of sensitivity to these factors among the funds, overall reflecting their distinct investment strategies. For instance, the small cap fund displayed a significant positive relationship with SMB, indicating a tendency to benefit from investments in smaller companies. Conversely, the value fund showed a positive HML coefficient, suggesting a preference for value stocks. However, several of the funds saw correlation with development of small cap stocks.</p><p>Results indicate that while market risk remains a dominant factor, size and value significantly contribute to fund performance. This is offering insights beyond the traditional CAPM model. The study’s findings are significant, suggesting that actively managed funds can exhibit distinct behavioral patterns which could be systematically explored to enhance investment strategies and decision making.</p><p>The thesis concludes with recommendations for extending this research to include additional factors like momentum and liquidity which could provide a deeper understanding of the influences affecting fund performance. The adoption of multi factor models may offer a more comprehensive framework for predicting stock returns and assisting portfolio management.</p>
----------------------------------------------------------------------
In diva2:1881360 abstract is:
<p>ArtEmis is an EU project that today consists of 14 differentinstitutions collectively working towards the final goal of buildinga system that can make trustworthy earthquake predictions withthe help of radon gas. The purpose of this report is to analyse theconcentration of radon measured by four out of six prototype sensorsinstalled by the ArtEmis project.</p><p>In the occurrence of an earthquake, tectonic plates slide together,causing stress levels to rise within the Earth’s crust and microcracksbegin to form. When these microcracks form, specific elements suchas radon gas can ascend toward the surface and reach groundwater.Once in groundwater, the concentration of radon can be measuredby analysing the amount of γ rays at certain energies using γ-rayspectroscopy.</p><p>With energy spectra measured by these sensors, an energy intervalcorresponding to the presence of the isotope 222Rn could be identified,namely the interval around 609 keV. This stems from a daughterisotope of 222Rn. Further analysis on the activity over time, andcomparisons to instances when earthquakes occurred, could thenbe done. These measurements were also tested against a statisticalmodel based on Gaussian distribution, showing correlation in severalcases.</p><p>One sensor location had an extra interesting find. Here it wasable to see, on two different occasions, a distinct increase in 222Rnconcentration roughly 10 days prior to an earthquake.Considering that these sensors are active for the very first timeduring the time span of this report, unintended behaviour occurredon several occasions. A large focus of the project currently lies onfixing these issues. This leads to limited conclusions being able tobe drawn from such a short time span, but could give indication ofpositive results moving forward.</p>

corrected abstract:
<p>ArtEmis is an EU project that today consists of 14 different institutions collectively working towards the final goal of building a system that can make trustworthy earthquake predictions with the help of radon gas. The purpose of this report is to analyse the concentration of radon measured by four out of six prototype sensors installed by the ArtEmis project.</p><p>In the occurrence of an earthquake, tectonic plates slide together, causing stress levels to rise within the Earth’s crust and microcracks begin to form. When these microcracks form, specific elements such as radon gas can ascend toward the surface and reach groundwater. Once in groundwater, the concentration of radon can be measured by analysing the amount of γ rays at certain energies using γ-ray spectroscopy.</p><p>With energy spectra measured by these sensors, an energy interval corresponding to the presence of the isotope <sup>222</sup><em>Rn</em> could be identified, namely the interval around 609 keV. This stems from a daughter isotope of <sup>222</sup><em>Rn</em>. Further analysis on the activity over time, and comparisons to instances when earthquakes occurred, could then be done. These measurements were also tested against a statistical model based on Gaussian distribution, showing correlation in several cases.</p><p>One sensor location had an extra interesting find. Here it was able to see, on two different occasions, a distinct increase in <sup>222</sup><em>Rn</em> concentration roughly 10 days prior to an earthquake.</p><p>Considering that these sensors are active for the very first time during the time span of this report, unintended behaviour occurred on several occasions. A large focus of the project currently lies on fixing these issues. This leads to limited conclusions being able to be drawn from such a short time span, but could give indication of positive results moving forward.</p>
----------------------------------------------------------------------
In diva2:1878576 abstract is:
<p>Hematology analyzers can be used for screening patients for blood abnor-malities. The techniques used in a hematology analyzer include impedanceanalysis, flow cytometry and spectroscopy, which allow for measuring of forexample absolute count, sizes and concentration of different cells in a patient’sblood sample. Hyperlipidemia, which refers to elevated blood lipid levels, isthe primary cause of heart-related illness and fatalities in today’s developedor developing countries. Currently, blood lipid levels are not measured as aparameter with hematology analyzers. Since hematology analyzers allow for arapid general screening of blood parameters, an area of interest is therefore tobe able to measure blood lipids with a hematology analyzer. Thus, this studyaims to investigate the possibility of detecting and measuring blood lipids witha hematology analyzer, using flow cytometry and/or spectrophotometry.</p><p>In order to investigate this possibility, two simulating methods were conductedwhere in the first method Intralipid 20% was mixed with saline into sampleswith different lipid concentrations. In the second method, diluent wasused instead of saline. Lastly a Correlation study was performed whereIntralipid 20% was mixed with donor blood to prepare samples with differentlipid concentrations. All samples were then analyzed in a hematologyanalyser and scatter plots from flow cytometry and light absorption datafrom spectrophotometry measurements were obtained. The methods showedthat there is a strong correlation between number of detected pulse countsfrom the scatter plots and lipid concentration. Same applies to lightabsorption compared to the lipid concentration of the samples, measured withspectrophotometry.</p><p>The results from this study show that it is in fact possible to detect andmeasure blood lipid levels with a hematology analyser using flow cytometryand spectrophotometry. Further development within this area could thereforeenable simple screening of this additional parameter and early detection ofindications of hyperlipidemia.</p>


corrected abstract:
<p>Hematology analyzers can be used for screening patients for blood abnormalities. The techniques used in a hematology analyzer include impedance analysis, flow cytometry and spectroscopy, which allow for measuring of for example absolute count, sizes and concentration of different cells in a patient’s blood sample. Hyperlipidemia, which refers to elevated blood lipid levels, is the primary cause of heart-related illness and fatalities in today’s developed or developing countries. Currently, blood lipid levels are not measured as a parameter with hematology analyzers. Since hematology analyzers allow for a rapid general screening of blood parameters, an area of interest is therefore to be able to measure blood lipids with a hematology analyzer. Thus, this study aims to investigate the possibility of detecting and measuring blood lipids with a hematology analyzer, using flow cytometry and/or spectrophotometry.</p><p>In order to investigate this possibility, two simulating methods were conducted where in the first method Intralipid 20% was mixed with saline into samples with different lipid concentrations. In the second method, diluent was used instead of saline. Lastly a Correlation study was performed where Intralipid 20% was mixed with donor blood to prepare samples with different lipid concentrations. All samples were then analyzed in a hematology analyser and scatter plots from flow cytometry and light absorption data from spectrophotometry measurements were obtained. The methods showed that there is a strong correlation between number of detected pulse counts from the scatter plots and lipid concentration. Same applies to light absorption compared to the lipid concentration of the samples, measured with spectrophotometry.</p><p>The results from this study show that it is in fact possible to detect and measure blood lipid levels with a hematology analyser using flow cytometry and spectrophotometry. Further development within this area could therefore enable simple screening of this additional parameter and early detection of indications of hyperlipidemia.</p>
----------------------------------------------------------------------
In diva2:1817475 abstract is:
<p>Both commuting to work and long-distance travelling with one's own bicycle have been intrend for years and a large market has emerged. While the reasons on the customer side aremainly sustainability and sporting activity, the companies offer customised products for manydifferent use-cases.In this master's thesis, a transport box for bicycles is being developed that can be taken on aplane, for example, and converted into a cargo trailer at the destination. A market researchshows that such a product is already available for certain folding bikes, whereas the goal is auniversal solution for utility bikes as well as mountain bikes.The methodological development follows a standard that divides the process into four phases.After the market research, main functions of the product are identified, which are "Transport abicycle as luggage" and "Carry goods during the bicycle ride". These are then divided intosub-functions in order to find different design variants for each function, combine them intodifferent drafts with the help of a morphological box and finally evaluate according totechnical and economic criteria, so that a final draft is determined at the end of this phase. It isa hard case providing enough space for different types of bicycles, that can be converted intoa trailer by mounting two wheels on the sides and a tow bar.In phase 3, the wheel size is set at 12 inches due to small space requirements and weight, andthe material is set to ABS-plastic for reasons of sustainability and mechanical properties.The final phase involves 3D design using Fusion360 with drawing derivation and validationof the model. The model shows that the requirements regarding geometry and weight havebeen implemented and that a practical transport option for bicycles has been found. In order topotentially launch the product on the market, further investigations such as FEM analysis ordynamic simulations are necessary.</p>

corrected abstract:
<p>Both commuting to work and long-distance travelling with one's own bicycle have been in trend for years and a large market has emerged. While the reasons on the customer side are mainly sustainability and sporting activity, the companies offer customised products for many different use-cases.</p><p>In this master's thesis, a transport box for bicycles is being developed that can be taken on a plane, for example, and converted into a cargo trailer at the destination. A market research shows that such a product is already available for certain folding bikes, whereas the goal is a universal solution for utility bikes as well as mountain bikes.</p><p>The methodological development follows a standard that divides the process into four phases. After the market research, main functions of the product are identified, which are "Transport a bicycle as luggage" and "Carry goods during the bicycle ride". These are then divided into sub-functions in order to find different design variants for each function, combine them into different drafts with the help of a morphological box and finally evaluate according to technical and economic criteria, so that a final draft is determined at the end of this phase. It is a hard case providing enough space for different types of bicycles, that can be converted into a trailer by mounting two wheels on the sides and a tow bar.</p><p>In phase 3, the wheel size is set at 12 inches due to small space requirements and weight, and the material is set to ABS-plastic for reasons of sustainability and mechanical properties. The final phase involves 3D design using Fusion360 with drawing derivation and validation of the model. The model shows that the requirements regarding geometry and weight have been implemented and that a practical transport option for bicycles has been found. In order to potentially launch the product on the market, further investigations such as FEM analysis or dynamic simulations are necessary.</p>
----------------------------------------------------------------------
In diva2:1345189 abstract is:
<p>-aminobutyric acid receptors of type A (GABAARs) are the majorinhibitory neurotransmitter receptors in the human brain, andare modulated by a vast range of exogenous molecules, such assedatives and anesthetics. In the last year, the first cryo-electronmicroscopy (cryo-EM) images of the closed and desensitized statesof the GABAAR were released, enabling fruitful research throughsimulations of these complex proteins. This report investigatesthe characteristics of the two structures. Specifically, pore hydration,radius, and hydrophobicity is compared, and a major focuslies in the general anesthetic (GA) binding pockets in the transmembranedomain, as well as the ligands propofol, etomidate,and pentobarbital. Furthermore, different models for the missingstructure of the intracellular domain (ICD) are compared. Thestructures were simulated for 1 μs using GROMACS. Using multiplesequence alignment as the basis of different models with theheptapeptid SQPARAA in the place of the ICD, resulted in stablestructures with a backbone RMSD close to 2 Å after 1 μs. Thepores are shown to exhibit significant differences between the twostates, with heavier constriction at the 9’ site of the closed state,but also suspected faulty expansion of the pore near the top in thedesensitized state after equilibration. Two of the pockets in thedesensitized state further deviates from expectation, by being tooconstricted. The other pockets were large enough to bind ligandsin the desensitized state, but not in the closed state, as expected.The binding analysis of the GAs suggests that etomidate bindswith the phenyl ring pointing towards the ICD, and that pentobarbitalbinds with the head group pointing towards the pore. Italso suggests that the GAs can bind to every GA pocket, but thatmodulatory activity is dependent on consistently low binding energies,which varies between the ligands for the different pockets.</p>

corrected abstract:
<p>γ-aminobutyric acid receptors of type A (GABA<sub>A</sub>Rs) are the major inhibitory neurotransmitter receptors in the human brain, and are modulated by a vast range of exogenous molecules, such as sedatives and anesthetics. In the last year, the first cryo-electron microscopy (cryo-EM) images of the closed and desensitized states of the GABA<sub>A</sub>R were released, enabling fruitful research through simulations of these complex proteins. This report investigates the characteristics of the two structures. Specifically, pore hydration, radius, and hydrophobicity is compared, and a major focus lies in the general anesthetic (GA) binding pockets in the transmembrane domain, as well as the ligands propofol, etomidate, and pentobarbital. Furthermore, different models for the missing structure of the intracellular domain (ICD) are compared. The structures were simulated for 1 µs using GROMACS. Using multiple sequence alignment as the basis of different models with the heptapeptid SQPARAA in the place of the ICD, resulted in stable structures with a backbone RMSD close to 2 Å after 1 µs. The pores are shown to exhibit significant differences between the two states, with heavier constriction at the 9’ site of the closed state, but also suspected faulty expansion of the pore near the top in the desensitized state after equilibration. Two of the pockets in the desensitized state further deviates from expectation, by being too constricted. The other pockets were large enough to bind ligands in the desensitized state, but not in the closed state, as expected. The binding analysis of the GAs suggests that etomidate binds with the phenyl ring pointing towards the ICD, and that pentobarbital binds with the head group pointing towards the pore. It also suggests that the GAs can bind to every GA pocket, but that modulatory activity is dependent on consistently low binding energies, which varies between the ligands for the different pockets.</p>
----------------------------------------------------------------------
In diva2:1307667 abstract is:
<p>The industrial robot is a flexible and cheap standard component that can becombined with a milling head to complete low accuracy milling tasks. Thefuture goal for researchers and industry is to increase the milling accuracy, suchthat it can be introduced to more high value added operations.The serial build up of an industrial robot bring non-linear compliance andchallenges in vibration mitigation due to the member and reducer design. WithAdditive Manufacturing (AM), the traditional cast aluminum structure couldbe revised and, therefore, milling accuracy gain could be made possible due tostructural changes.This thesis proposes the structural changes that would improve the millingaccuracy for a specific trajectory. To quantify the improvement, first the robothad to be reverse engineered and a kinematic simulation model be built. Nextthe kinematic simulation process was automated such that multiple input parameterscould be varied and a screening conducted that proposed the mostprofitable change.It was found that a mass decrease in any member did not affect the millingaccuracy and a stiffness increase in the member of the second axis would increasethe milling accuracy the most, without changing the design concept. To changethe reducer in axis 1 would reduce the mean position error by 7.5 % and themean rotation error by 4.5 % approximately, but also reduces the maximumspeed of the robot. The best structural change would be to introduce twosupport bearings for axis two and three, which decreased the mean positioningerror and rotation error by approximately 8 % and 13 % respectively.</p>

corrected abstract:
<p>The industrial robot is a flexible and cheap standard component that can be combined with a milling head to complete low accuracy milling tasks. The future goal for researchers and industry is to increase the milling accuracy, such that it can be introduced to more high value added operations.</p><p>The serial build up of an industrial robot bring non-linear compliance and challenges in vibration mitigation due to the member and reducer design. With Additive Manufacturing (AM), the traditional cast aluminum structure could be revised and, therefore, milling accuracy gain could be made possible due to structural changes.</p><p>This thesis proposes the structural changes that would improve the milling accuracy for a specific trajectory. To quantify the improvement, first the robot had to be reverse engineered and a kinematic simulation model be built. Next the kinematic simulation process was automated such that multiple input parameters could be varied and a screening conducted that proposed the most profitable change.</p><p>It was found that a mass decrease in any member did not affect the milling accuracy and a stiffness increase in the member of the second axis would increase the milling accuracy the most, without changing the design concept. To change the reducer in axis 1 would reduce the mean position error by 7.5 % and the mean rotation error by 4.5 % approximately, but also reduces the maximum speed of the robot. The best structural change would be to introduce two support bearings for axis two and three, which decreased the mean positioning error and rotation error by approximately 8 % and 13 % respectively.</p>
----------------------------------------------------------------------
In diva2:1142911 abstract is:
<p>Recommender systems can be seen everywhere today,having endless possibilities of implementation. However,operating in the background, they can easily be passed withoutnotice. Essentially, recommender systems are algorithms thatgenerate predictions by operating on a certain data set. Eachcase of recommendation is environment sensitive and dependenton the condition of the data at hand. Consequently, it is difficultto foresee which method, or combination of methods, to apply in aparticular situation for obtaining desired results. The area of recommendersystems that this thesis is delimited to is Collaborativefiltering (CF) and can be split up into three different categories,namely memory based, model based and hybrid algorithms. Thisthesis implements a CF algorithm for each of these categoriesand sets focus on comparing their prediction accuracy and theirdependency on the amount of available training data (i.e. asa function of sparsity). The results show that the model basedalgorithm clearly performs better than the memory based, bothin terms of overall accuracy and sparsity dependency. With anincreasing sparsity level, the problem of having users without anyratings is encountered, which greatly impacts the accuracy forthe memory based algorithm. A hybrid between these algorithmsresulted in a better accuracy than the model based algorithmitself but with an insignificant improvement.</p>

corrected abstract:
<p>Recommender systems can be seen everywhere today, having endless possibilities of implementation. However, operating in the background, they can easily be passed without notice. Essentially, recommender systems are algorithms that generate predictions by operating on a certain data set. Each case of recommendation is environment sensitive and dependent on the condition of the data at hand. Consequently, it is difficult to foresee which method, or combination of methods, to apply in a particular situation for obtaining desired results. The area of recommender systems that this thesis is delimited to is Collaborative filtering (CF) and can be split up into three different categories, namely memory based, model based and hybrid algorithms. This thesis implements a CF algorithm for each of these categories and sets focus on comparing their prediction accuracy and their dependency on the amount of available training data (i.e. as a function of sparsity). The results show that the model based algorithm clearly performs better than the memory based, both in terms of overall accuracy and sparsity dependency. With an increasing sparsity level, the problem of having users without any ratings is encountered, which greatly impacts the accuracy for the memory based algorithm. A hybrid between these algorithms resulted in a better accuracy than the model based algorithm itself but with an insignificant improvement.</p>
----------------------------------------------------------------------
In diva2:1078073 spaces missing in title:
"An Experimental Study of Fibre SuspensionFlows in Pipes using Nuclear MagneticResonance Imaging"
==>
"An Experimental Study of Fibre Suspension Flows in Pipes using Nuclear Magnetic Resonance Imaging"

abstract is:
<p>This study deals with fibre suspension flows through cylindrical pipes. Thepresent work aims at measurements of opaque flows, which are common inindustries. Nuclear magnetic resonance imaging (NMRI) and ultrasound velocimetryprofiling (UVP) were employed as non-invasive and optic-independenttools to measure the velocity profiles. As a first experiment, a paper-pulp suspensionflow through a sudden contraction and expansion was investigated.The results show the NMRI technique can be used to measure the stronglyunsteady flow such as separated regions though the MR signal is attenuateddue to the turbulence in the flow. The flow loop had however an insufficientinlet length which caused asymmetric profiles at the test section. As a secondexperiment, a flow loop which provided fully developed flows at the test sectionwas designed. After that, the velocity profiles of rayon-fibre and micro-spheresuspension flows were measured by the NMRI and the UVP independently.In principle, these two techniques measure the different velocities of the fibresuspensionflows, i.e. the velocity of the water and the fibre. In dilute suspensionflows, where the velocities of the two phases were assumed to be thesame, the velocity profiles were in good agreement. This shows the validityof the two measurement techniques. However, it should be pointed out thatthere is a limitation of the current UVP method for highly concentrated flows.The velocity profiles obtained by the UVP at high concentrations seems notto represent physics while the NMRI is not affected by the concentrations. Itis argued that the advances of the NMRI for the measurement of the highlyconcentrated flows.</p>

corrected abstract:
<p>This study deals with fibre suspension flows through cylindrical pipes. The present work aims at measurements of opaque flows, which are common in industries. Nuclear magnetic resonance imaging (NMRI) and ultrasound velocimetry profiling (UVP) were employed as non-invasive and optic-independent tools to measure the velocity profiles. As a first experiment, a paper-pulp suspension flow through a sudden contraction and expansion was investigated. The results show the NMRI technique can be used to measure the strongly unsteady flow such as separated regions though the MR signal is attenuated due to the turbulence in the flow. The flow loop had however an insufficient inlet length which caused asymmetric profiles at the test section. As a second experiment, a flow loop which provided fully developed flows at the test section was designed. After that, the velocity profiles of rayon-fibre and micro-sphere suspension flows were measured by the NMRI and the UVP independently. In principle, these two techniques measure the different velocities of the fibresuspension flows, i.e. the velocity of the water and the fibre. In dilute suspension flows, where the velocities of the two phases were assumed to be the same, the velocity profiles were in good agreement. This shows the validity of the two measurement techniques. However, it should be pointed out that there is a limitation of the current UVP method for highly concentrated flows. The velocity profiles obtained by the UVP at high concentrations seems not to represent physics while the NMRI is not affected by the concentrations. It is argued that the advances of the NMRI for the measurement of the highly concentrated flows.</p>
----------------------------------------------------------------------
 diva2:854657 error in title:
 "Analysis oft yre wear using the expanded brush tyre model"
 ==>
 ""Analysis of tyre wear using the expanded brush tyre model"
 
abstract is:
<p>Approximately 60 000 tonnes of tyres are produced annually in Sweden to meet thedemand in the market. It is believed that 10 000 tonnes of rubber particles contaminatesthe Swedish roads every year. Some of the elements in the emitted particles cannegatively impact the environment. These elements can lead to leaching in water thatcan cause serious problems to aquatic organisms. Furthermore worn out tyres negativelyinfluence the driving dynamics. It increases the risk of aquaplaning which can have fatalconsequences. Innovative ways of recycling tyres are constantly being developed butstill faces major challenges. It is therefore important to understand tyre wear, whatinfluences it and how to reduce it.</p><p>The aim of the project is to acquire knowledge related to tyre wear, its environmentalimpacts, use a mathematical model to simulate tyre wear and study how the differentparameters influences wear.</p><p>First a literature survey was performed to acquire knowledge related to tyre wear. It wasfound that tyre wear is mainly due to adhesive and hysteresis wear. Several factors werefound to affect tyre wear for example velocity, slip angle and the type of road surface.The environment impact was also studied and the results show the composition of theparticles emitted to the environment. Some of the emitted particles negatively affect theaquatic organism and human beings.</p><p>In the second part of the project a mathematical model based on the well-known brushtyre model was used to simulate how wear changes with different parameters. Themodel created at KTH Vehicle Dynamics is named the expanded brush tyre model(EBM). The wear model chosen for this evaluation was the Archards wear law. Thismodel was used to be able to quantify wear and study how it is influenced by differentfactors.</p><p>The result of the mathematical models shows clearly an exponential increase in thevolume of wear with increases in velocity, slip angle and vertical load. The analysis wasdone using zero camber angle.</p><p>For future work it is recommended to investigate camber angle as it is also one of themajor factors that affects wear. Temperature is also another factor that was not taken into account in the study. It can also be studied in future work.</p>

corrected abstract:
<p>Approximately 60 000 tonnes of tyres are produced annually in Sweden to meet the demand in the market. It is believed that 10 000 tonnes of rubber particles contaminates the Swedish roads every year. Some of the elements in the emitted particles can negatively impact the environment. These elements can lead to leaching in water that can cause serious problems to aquatic organisms. Furthermore worn out tyres negatively influence the driving dynamics. It increases the risk of aquaplaning which can have fatal consequences. Innovative ways of recycling tyres are constantly being developed but still faces major challenges. It is therefore important to understand tyre wear, what influences it and how to reduce it.</p><p>The aim of the project is to acquire knowledge related to tyre wear, its environmental impacts, use a mathematical model to simulate tyre wear and study how the different parameters influences wear.</p><p>First a literature survey was performed to acquire knowledge related to tyre wear. It was found that tyre wear is mainly due to adhesive and hysteresis wear. Several factors were found to affect tyre wear for example velocity, slip angle and the type of road surface. The environment impact was also studied and the results show the composition of the particles emitted to the environment. Some of the emitted particles negatively affect the aquatic organism and human beings.</p><p>In the second part of the project a mathematical model based on the well-known brushf tyre model was used to simulate how wear changes with different parameters. The model created at KTH Vehicle Dynamics is named the expanded brush tyre model (EBM). The wear model chosen for this evaluation was the Archards wear law. This model was used to be able to quantify wear and study how it is influenced by different factors.</p><p>The result of the mathematical models shows clearly an exponential increase in the volume of wear with increases in velocity, slip angle and vertical load. The analysis was done using zero camber angle.</p><p>For future work it is recommended to investigate camber angle as it is also one of the major factors that affects wear. Temperature is also another factor that was not taken in to account in the study. It can also be studied in future work.</p>
----------------------------------------------------------------------
In diva2:1890520 abstract is:
<p>The observed cases of increased radon emissions from the earth precedingheightened seismic activity have given rise to numerous papers exploringthe viability of using radon concentration levels in soil gas and groundwateras a precursor in earthquake forecasting. In this paper, these methods areexplored further through a statistical analysis of the initial data collected by thegamma detectors installed within the scope of ArtEmis, a project funded bythe European Union with the goal of producing a reliable model for earthquakeforecasting using measured radioactivity as a precursor. A Gaussian RandomWalk model is implemented using Integrated Nested Laplace Approximationto infer a set of points defining the hidden distribution from which the observeddata are drawn. The model is implemented and trained on data sets recordedby five gamma detectors. The inferences made by the model imply thatthe model is applicable to the data collected by four of the detectors. Theinferred distributions are compared to seismic data collected by 142 seismicobservatories in Greece. No significant correlations between the inferredchanges in radon levels and seismic activity were found. The impact ofchanging the sampling frequency of radon data is investigated, with theconclusion that the created model infers a distribution corresponding to the rawdata obtained with a lower sampling frequency. It is concluded that the modelshould be improved further for more advanced analyses, with some of the mostimportant developments suggested being the inclusion of meteorological dataand upgrading from a Gaussian Random Walk of order 1 to order 2. Moreseismic and radiation data is also deemed necessary for meaningful analysesof the correlation between the two. More advanced analyses of the availableseismic data are required for improved classification of the events expected togive rise to precursory phenomena observable in the radiation data collectedby the various detectors.</p>


corrected abstract:
<p>The observed cases of increased radon emissions from the earth preceding heightened seismic activity have given rise to numerous papers exploring the viability of using radon concentration levels in soil gas and ground water as a precursor in earthquake forecasting. In this paper, these methods are explored further through a statistical analysis of the initial data collected by the gamma detectors installed within the scope of ArtEmis, a project funded by the European Union with the goal of producing a reliable model for earthquake forecasting using measured radioactivity as a precursor. A Gaussian Random Walk model is implemented using Integrated Nested Laplace Approximation to infer a set of points defining the hidden distribution from which the observed data are drawn. The model is implemented and trained on data sets recorded by five gamma detectors. The inferences made by the model imply that the model is applicable to the data collected by four of the detectors. The inferred distributions are compared to seismic data collected by 142 seismic observatories in Greece. No significant correlations between the inferred changes in radon levels and seismic activity were found. The impact of changing the sampling frequency of radon data is investigated, with the conclusion that the created model infers a distribution corresponding to the raw data obtained with a lower sampling frequency. It is concluded that the model should be improved further for more advanced analyses, with some of the most important developments suggested being the inclusion of meteorological data and upgrading from a Gaussian Random Walk of order 1 to order 2. More seismic and radiation data is also deemed necessary for meaningful analyses of the correlation between the two. More advanced analyses of the available seismic data are required for improved classification of the events expected to give rise to precursory phenomena observable in the radiation data collected by the various detectors.</p>
----------------------------------------------------------------------
In diva2:1881327
abstract is:
<p>Formula Student is Europe’s most established engineering competition, with teamsall over the world. Practical problem solving in combination with applyingacademic knowledge, give students the opportunity to explore their field of study inan exciting and meaningful way.</p><p>Aerodynamic development of race cars have seen significant results in competitionsince its introduction in the 1960s. Initial designs were adaptations of aerospaceconcepts for ground vehicles. Development relied solely on track- and wind tunneltesting but despite their rudimentary designs, significant performance increaseswere made. The purpose of aerodynamic development of race cars is to balance thecar, getting it to behave as desired. As a consequence of the forces generated, thevehicle corners faster at the cost of acceleration and top speed. With more powerfulcomputers, earlier unsolvable equations started to get numerically solved andcomputational fluid dynamics was born. CFD introduced the possibility for rapiditeration and exploration of more intricate designs. This report will solely utilizeCFD as a simulation tool, recognising its limitations in accuracy and real worldcorrelation.</p><p>The aim of this study is to increase downforce on the front wing, whilst beingcautious of downstream impact. The goal set by the team is an adjustable frontwing that generates as much downforce as possible, whilst allowing for adjustmentsto shift the center of pressure by promoting more air to the side-structure. Toachieve this, an iterative design process based on literature is the chosen method.Continuous cross evaluations with other parts of the design team is of the highestimportance to avoid poor interaction between aerodynamic devices.</p><p>The (negative) lift coefficient was increased from 4.7 to 5.7 for the entire vehicle, byonly improving the front wing. This was very satisfactory as increases upstreamoften lead do degraded performance downstream. An increased lift coefficient ofover 20%, with improvements to front wheel drag and similar side-structureperformance, demonstrate the quality and effectiveness of the design.</p>

corrected abstract:
<p>Formula Student is Europe’s most established engineering competition, with teams all over the world. Practical problem solving in combination with applying academic knowledge, give students the opportunity to explore their field of study in an exciting and meaningful way.</p><p>Aerodynamic development of race cars have seen significant results in competition since its introduction in the 1960s. Initial designs were adaptations of aerospace concepts for ground vehicles. Development relied solely on track- and wind tunnel testing but despite their rudimentary designs, significant performance increases were made. The purpose of aerodynamic development of race cars is to balance the car, getting it to behave as desired. As a consequence of the forces generated, the vehicle corners faster at the cost of acceleration and top speed. With more powerful computers, earlier unsolvable equations started to get numerically solved and computational fluid dynamics was born. CFD introduced the possibility for rapid iteration and exploration of more intricate designs. This report will solely utilize CFD as a simulation tool, recognising its limitations in accuracy and real world correlation.</p><p>The aim of this study is to increase downforce on the front wing, whilst being cautious of downstream impact. The goal set by the team is an adjustable front wing that generates as much downforce as possible, whilst allowing for adjustments to shift the center of pressure by promoting more air to the side-structure. To achieve this, an iterative design process based on literature is the chosen method. Continuous cross evaluations with other parts of the design team is of the highest importance to avoid poor interaction between aerodynamic devices.</p><p>The (negative) lift coefficient was inc
reased from 4.7 to 5.7 for the entire vehicle, by only improving the front wing. This was very satisfactory as increases upstream often lead do degraded performance downstream. An increased lift coefficient of over 20%, with improvements to front wheel drag and similar side-structure performance, demonstrate the quality and effectiveness of the design.</p>
----------------------------------------------------------------------
In diva2:1880362 

abstract is:

<p>Measurements of radon in groundwater before, during and after the 1995 Kobe earth-quake in Japan indicated that there might be a correlation between levels of 222Rn ingroundwater and seismological activity. The artEmis project investigates this possibleconnection with the goal of building a network of detectors in seismically active parts ofEurope. The detectors will be placed in groundwater and measure many factors, one ofthem being the radon level by measuring gamma radiation. The original vision for thedetectors also included alpha detection. The obtained data is analyzed with artificialintelligence.</p><p>This thesis investigates a possible method for alpha detection under water. Specif-ically by seeing if it is possible for radon dissolved in water to diffuse from the water,through silicone tubes and into the air inside of the silicone tubes. There is a possibilityfor alpha detection of the radon decay if the radon gas could get into the air. This wasinvestigated by submerging an air-filled silicone construction in water with high levelsof radon. The level of radon in the water was increased by placing pieces of lightweightconcrete in the water. The construction was removed after a period of time and itsgamma-ray spectrum was measured. A statistically significant increase in radon levelscompared to the background radiation would indicate that diffusion happened.</p><p>Measurements of the silicone construction with a germanium detector resulted ingamma spectra that were analyzed with a Python program to determine the activity of222Rn over time. Short measurements, around 1 hour long, showed a significant increaseof radon compared to the background. For longer measurements however, around one ortwo days, this effect was no longer apparent. The conclusion is that radon diffused intothe silicone construction, either into the silicone material itself or into the air inside theconstruction, but it comes out again quickly. If the radon diffused into the air inside ofthe silicone, the use of alpha detection to measure radon levels in groundwater is muchless far-fetched than before. Therefore, the artEmis project might be one step closer tousing alpha detection in their detector network.</p>


corrected abstract:
<p>Measurements of radon in groundwater before, during and after the 1995 Kobe earth-quake in Japan indicated that there might be a correlation between levels of <sup>222</sup>Rn in groundwater and seismological activity. The artEmis project investigates this possible connection with the goal of building a network of detectors in seismically active parts of Europe. The detectors will be placed in groundwater and measure many factors, one of them being the radon level by measuring gamma radiation. The original vision for the detectors also included alpha detection. The obtained data is analyzed with artificial intelligence.</p><p>This thesis investigates a possible method for alpha detection under water. Specifically by seeing if it is possible for radon dissolved in water to diffuse from the water, through silicone tubes and into the air inside of the silicone tubes. There is a possibility for alpha detection of the radon decay if the radon gas could get into the air. This was investigated by submerging an air-filled silicone construction in water with high levels of radon. The level of radon in the water was increased by placing pieces of lightweight concrete in the water. The construction was removed after a period of time and its gamma-ray spectrum was measured. A statistically significant increase in radon levels compared to the background radiation would indicate that diffusion happened.</p><p>Measurements of the silicone construction with a germanium detector resulted in gamma spectra that were analyzed with a Python program to determine the activity of <sup>222</supZRn over time. Short measurements, around 1 hour long, showed a significant increase of radon compared to the background. For longer measurements however, around one or two days, this effect was no longer apparent. The conclusion is that radon diffused into the silicone construction, either into the silicone material itself or into the air inside the construction, but it comes out again quickly. If the radon diffused into the air inside of the silicone, the use of alpha detection to measure radon levels in groundwater is much less far-fetched than before. Therefore, the artEmis project might be one step closer to using alpha detection in their detector network.</p>
----------------------------------------------------------------------
In diva2:1779369 -error in title:
"Computational Fluid Dynamics of the flow in a diffuser: - like geometry"
==>
"Computational Fluid Dynamics of the flow in a diffuser - like geometry"

If you want to include the subtitle, it is:
"Computational Fluid Dynamics of the flow in a diffuser - like geometry:
A study of flow separation using Computational Fluid Dynamics"

abstract is:
<p>Simulations were performed to investigate flow separation of an asymmetricdiffuser - like geometry. The geometry used for the simulations was modeledafter an experimental setup with recorded flow data, which was compared tothe simulated data. For all simulations, steady state flow at the inlet was usedwith the assumption of a 2D flow.A grid convergence study consisting of three different grids was performed.From this study no apparent change in simulation results were observed forfiner grids. This is caused by the fact that the coarse grid had a high enoughresolution to fully capture the flow, meaning that the higher resolution gridsyielded small improvements.Additionally, two different turbulence models RN G k − ε and SST k − ωwere used for evaluating which model was best suited to model flow separation.The simulations showed that the RN G k − ε model could not capture the flowseparation and had a poor accuracy when predicting the turbulent kinetic energy(TKE). Simulation results from SST k − ω gave good results in capturing flowseparation and predicting both the velocity and TKE when compared to theexperimental data.Finally, a turbulence intensity study was made for the mid grid with theSST k − ω model. The turbulent intensity was set to 5%, 10%, 15% and 20%at the inlet. This resulted in the point of separation moving further down thegeometry to x/H ≈ [17.68, 18.71, 19.58, 20.72] for respective intensity. The pointof reattachment also moves to x/H ≈ [44.85, 43.60, 42.67, 41.67] for respectiveintensity.In summary for simulating flow separation in turbulent flows the SST k − ωmodel is optimal and an increase in turbulent intensity reduces the recirculationzone.</p>


corrected abstract:
<p>Simulations were performed to investigate flow separation of an asymmetric diffuser - like geometry. The geometry used for the simulations was modeled after an experimental setup with recorded flow data, which was compared to the simulated data. For all simulations, steady state flow at the inlet was used with the assumption of a 2D flow.</p><p>A grid convergence study consisting of three different grids was performed. From this study no apparent change in simulation results were observed for finer grids. This is caused by the fact that the coarse grid had a high enough resolution to fully capture the flow, meaning that the higher resolution grids yielded small improvements.</p><p>Additionally, two different turbulence models <em>RNG k − ε</em> and <em>SST k − ω</em>  were used for evaluating which model was best suited to model flow separation. The simulations showed that the <em>RNG k − ε</em> model could not capture the flow separation and had a poor accuracy when predicting the turbulent kinetic energy (TKE). Simulation results from <em>SST k − ω</em> gave good results in capturing flow separation and predicting both the velocity and TKE when compared to the experimental data.</p><p>Finally, a turbulence intensity study was made for the mid grid with the <em>SST k − ω</em> model. The turbulent intensity was set to 5%, 10%, 15% and 20% at the inlet. This resulted in the point of separation moving further down the geometry to <em>x/H</em> ≈ [17.68, 18.71, 19.58, 20.72] for respective intensity. The point of reattachment also moves to <em>x/H</em> ≈ [44.85, 43.60, 42.67, 41.67] for respective intensity.</p><p>In summary for simulating flow separation in turbulent flows the <em>SST k − ω</em> model is optimal and an increase in turbulent intensity reduces the recirculation zone.</p>
----------------------------------------------------------------------
In diva2:1576894 correct the title:
"Tensile strength reduction for insufficient thread engagement A FEM study of a wall-shoe assembly"
==>
"Tensile strength reduction for insufficient thread engagement: A FEM study of a wall-shoe assembly"

abstract is:
<p>The purpose of this master thesis is to determine whether or not it exist, a model that can describethe reduction in strength, due to missing threads in a bond between a bolt and nut. And how thereduction in strength might effect a wall-shoe assembly, used to connect a wall to another wall or aconcrete base plate. This is done by firstly considering the strength of the entire wall-shoe assembly andthe strength of the bond between the nut and the bolt is then considered.</p><p>The strength of the entire assembly is calculated using some simple analytical models. The strengthof the bolt is the limiting factor for the assembly given the analytical models. Four FEM-models arethen created, three to evaluate the strength of the bolt and nut assembly, for bolt sized from M6to M60 with thread engagement from one thread to seven threads. </p><p>An elastic model without a defined tensile stress limit is proposed. The maximum stress at the sharpgeometry changes (stress concentrations) are used to dimension the maximum allowed load accordingto the von Mises yield criterion. </p><p>The yield limit is implemented by introducing a elastic-plastic (without hardening) for three different materials. Where the maximum yield force is determined as the maximum reactionforce on the frictionless support boundary condition, when a displacement is applied.</p><p>Material hardening is applied according to a bi-linear material model (with hardening). Thereaction force is evaluated and the maximum force and displacement can be determined using thedefinition of property class 8.8 that propose a yield limit of 80 percent of maximum load.</p><p>The behavior of the anchor bolt when it is pulled out of the concrete was also modeled. To obtain anunderstanding of the pull-out behavior.</p><p>The FEM-models makes it possible to formulate a simple reduction model, where the bolt failure loadis reduced by the calculated reduction factors. The reduction factors are dependent on the amountof missing thread due to damages or insufficient bolt height. The reduction factors are also highlydependent on the ratio between the rise and bolt diameter. The reduction factors are significantlysmaller when a perfect plastic material model is applied.</p><p>The failure mode is dependent on the material model, when hardening is applied fewer threads areneeded to achieve bolt failure rather then thread failure, compared to when a ideal plastic materialmodel is applied.</p><p>The reduction factors are not affected by the material yield limit, the maximal load is however highlydependent on the yield limit.</p><p>The placement of the missing thread does not effect the reduction factors. They can therefor beused regardless of if threads are missing due to damage or due to partial thread engagement.</p><p>A test of an M8 bolt was performed to attempt to validate the FEM-model. Due to some inherent flawsin the test procedure no clear conclusion, about the validity of the model can be made. It is howeverclear that missing threads induces risk of failure when tightening the assembly, since the bolt or nutcan be damaged without any clear signs that the assembly is compromised, leading to catastrophicfailure. Reducing the load with the reduction factor model should be done with caution.</p>


corrected abstract:
<p>The purpose of this master thesis is to determine whether or not it exist, a model that can describe the reduction in strength, due to missing threads in a bond between a bolt and nut. And how the reduction in strength might effect a wall-shoe assembly, used to connect a wall to another wall or a concrete base plate. This is done by firstly considering the strength of the entire wall-shoe assembly and the strength of the bond between the nut and the bolt is then considered.</p><p>The strength of the entire assembly is calculated using some simple analytical models. The strength of the bolt is the limiting factor for the assembly given the analytical models. Four FEM-models are then created, three to evaluate the strength of the bolt and nut assembly, for bolt sized from M6 to M60 with thread engagement from one thread to seven threads.</p><ul><li>An elastic model without a defined tensile stress limit is proposed. The maximum stress at the sharp geometry changes (stress concentrations) are used to dimension the maximum allowed load according to the von Mises yield criterion.</li><li>The yield limit is implemented by introducing a elastic-plastic (without hardening) for three different materials. Where the maximum yield force is determined as the maximum reaction force on the frictionless support boundary condition, when a displacement is applied.</li><li><p>Material hardening is applied according to a bi-linear material model (with hardening). The reaction force is evaluated and the maximum force and displacement can be determined using the definition of property class 8.8 that propose a yield limit of 80 percent of maximum load.</p><p>The behavior of the anchor bolt when it is pulled out of the concrete was also modeled. To obtain an understanding of the pull-out behavior.</p></li></ul><p>The FEM-models makes it possible to formulate a simple reduction model, where the bolt failure load is reduced by the calculated reduction factors. The reduction factors are dependent on the amount of missing thread due to damages or insufficient bolt height. The reduction factors are also highly dependent on the ratio between the rise and bolt diameter. The reduction factors are significantly smaller when a perfect plastic material model is applied.</p><p>The failure mode is dependent on the material model, when hardening is applied fewer threads are needed to achieve bolt failure rather then thread failure, compared to when a ideal plastic material model is applied.</p><p>The reduction factors are not affected by the material yield limit, the maximal load is however highly dependent on the yield limit.</p><p>The placement of the missing thread does not effect the reduction factors. They can therefor be used regardless of if threads are missing due to damage or due to partial thread engagement.</p><p>A test of an M8 bolt was performed to attempt to validate the FEM-model. Due to some inherent flaws in the test procedure no clear conclusion, about the validity of the model can be made. It is however clear that missing threads induces risk of failure when tightening the assembly, since the bolt or nut can be damaged without any clear signs that the assembly is compromised, leading to catastrophic failure. Reducing the load with the reduction factor model should be done with caution.</p>
----------------------------------------------------------------------
In diva2:1527832 abstract is:
<p>This master’s thesis covers the structuring and implementation of a digital testbench for the air brake system of freight trains. The test bench will serveto further improve the existing brake models at Transrail Sweden AB. Theseare used for the optimised calculation of train speed profiles by the DriverAdvisory System CATO. This work is based on the research of the technicalbackground, as well as the methodical approach to physical modelling anda modular implementation of the test bench. It gives full flexibility for thesimulation of customised train configurations using the European UIC brakesystem. Train length and vehicle arrangement can be adapted to the user’sspecific needs. For example, the test bench could be used for the simulation ofa train with distributed power. The system parameters are stored in a vehiclelibrary for the convenient generation of train configurations. This vehiclelibrary is freely expandable.The simulation is based on an equivalent electric circuit model which iscompleted with nozzle flow modelling. This model involves monitoring themain pipe, brake cylinder and reservoir pressure. Linear approximation is usedto obtain braking forces for the individual wagons and for the whole train. Thedepiction of the brake system behaviour is mostly accurate in the operationalscenarios, which is validated with measurement data. Additional calibrationis required for further reduction of the simulation errors and an extension ofthe model’s domain of validity. The test bench is developed by incrementaland iterative modelling and prepared for further improvements and variations,for example the adaption to the American AAR system variant.The presented work can also be used as a basis for similar implementationssuch as driving simulators. The methods are transferable to other applicationsof modular simulation.</p>

corrected abstract:
<p>This master’s thesis covers the structuring and implementation of a digital test bench for the air brake system of freight trains. The test bench will serve to further improve the existing brake models at Transrail Sweden AB. These are used for the optimised calculation of train speed profiles by the Driver Advisory System CATO. This work is based on the research of the technical background, as well as the methodical approach to physical modelling and a modular implementation of the test bench. It gives full flexibility for the simulation of customised train configurations using the European UIC brake system. Train length and vehicle arrangement can be adapted to the user’s specific needs. For example, the test bench could be used for the simulation of a train with distributed power. The system parameters are stored in a vehicle library for the convenient generation of train configurations. This vehicle library is freely expandable.</p><p>The simulation is based on an equivalent electric circuit model which is completed with nozzle flow modelling. This model involves monitoring the main pipe, brake cylinder and reservoir pressure. Linear approximation is used to obtain braking forces for the individual wagons and for the whole train. The depiction of the brake system behaviour is mostly accurate in the operational scenarios, which is validated with measurement data. Additional calibration is required for further reduction of the simulation errors and an extension of the model’s domain of validity. The test bench is developed by incremental and iterative modelling and prepared for further improvements and variations, for example the adaption to the American AAR system variant.</p><p>The presented work can also be used as a basis for similar implementations such as driving simulators. The methods are transferable to other applications of modular simulation.</p>
----------------------------------------------------------------------
In diva2:1500046 abstract is:
<p>The reliability of a mechanical system containing electronic packages is highly affectedby the environment the system is stationed in. The difference and fluctuationsbetween the ambient temperature and the operating temperature of the electronicpackage cause accumulation of inelastic strains in the package components thusdecreasing the service life. The most common failure modes of an electronic packagehas been identified from inspection of malfunctioning machines as cracks in the solderjoint and delamination between the glue and the die. Knowledge regarding therelationships between parameters affecting these failure modes, which are importantand which are not, is of high interest when developing new and existing products. SAAB AB would like to develop a methodology using design exploration to allow forevaluation of electronic packages using nonlinear finite element methods.</p><p>A surrogate model was created and parameterized with HyperMorph to be used forthree linear static variations of design of experiments, where both the performance ofthe methods themselves and the relative importance of the parameters were ofinterest. A connectivity condition was also implemented to allow for relativemovement between components while keeping the mesh intact. The designexploration was executed using a Taguchi design, a Modified extensive latticesequence design and a fractional factorial design where the three methods werecompared as well as the parameter significance analysed. An optimization was thenperformed to find the optimal parameter settings within the allowed bounds to beused where a nominal model and an optimized model are evaluated with animplemented creep law. The fatigue life of the two models were then estimated.</p>

corrected abstract:
<p>The reliability of a mechanical system containing electronic packages is highly affected by the environment the system is stationed in. The difference and fluctuations between the ambient temperature and the operating temperature of the electronic package cause accumulation of inelastic strains in the package components thus decreasing the service life. The most common failure modes of an electronic package has been identified from inspection of malfunctioning machines as cracks in the solder joint and delamination between the glue and the die. Knowledge regarding the relationships between parameters affecting these failure modes, which are important and which are not, is of high interest when developing new and existing products. SAAB AB would like to develop a methodology using design exploration to allow for evaluation of electronic packages using nonlinear finite element methods.</p><p>A surrogate model was created and parameterized with HyperMorph to be used for three linear static variations of design of experiments, where both the performance of the methods themselves and the relative importance of the parameters were of interest. A connectivity condition was also implemented to allow for relative movement between components while keeping the mesh intact. The design exploration was executed using a Taguchi design, a Modified extensive lattice sequence design and a fractional factorial design where the three methods were compared as well as the parameter significance analysed. An optimization was then performed to find the optimal parameter settings within the allowed bounds to be used where a nominal model and an optimized model are evaluated with an implemented creep law. The fatigue life of the two models were then estimated.</p>
----------------------------------------------------------------------
In diva2:1229161 missing hyphen in title:
"Direct optimization of dose-volume histogram metrics in intensity modulated radiation therapy treatment planning"
==>
"Direct optimization of dose-volume histogram metrics in intensity-modulated radiation therapy treatment planning"

abstract is:
<p>In optimization of intensity-modulated radiation therapy treatment plans, dose-volumehistogram (DVH) functions are often used as objective functions to minimize the violationof dose-volume criteria. Neither DVH functions nor dose-volume criteria, however,are ideal for gradient-based optimization as the former are not continuously differentiableand the latter are discontinuous functions of dose, apart from both beingnonconvex. In particular, DVH functions often work poorly when used in constraintsdue to their being identically zero when feasible and having vanishing gradients on theboundary of feasibility.In this work, we present a general mathematical framework allowing for direct optimizationon all DVH-based metrics. By regarding voxel doses as sample realizations ofan auxiliary random variable and using kernel density estimation to obtain explicit formulas,one arrives at formulations of volume-at-dose and dose-at-volume which are infinitelydifferentiable functions of dose. This is extended to DVH functions and so calledvolume-based DVH functions, as well as to min/max-dose functions and mean-tail-dosefunctions. Explicit expressions for evaluation of function values and corresponding gradientsare presented. The proposed framework has the advantages of depending on onlyone smoothness parameter, of approximation errors to conventional counterparts beingnegligible for practical purposes, and of a general consistency between derived functions.Numerical tests, which were performed for illustrative purposes, show that smoothdose-at-volume works better than quadratic penalties when used in constraints and thatsmooth DVH functions in certain cases have significant advantage over conventionalsuch. The results of this work have been successfully applied to lexicographic optimizationin a fluence map optimization setting.</p>

corrected abstract:
<p>In optimization of intensity-modulated radiation therapy treatment plans, dose-volume histogram (DVH) functions are often used as objective functions to minimize the violation of dose-volume criteria. Neither DVH functions nor dose-volume criteria, however, are ideal for gradient-based optimization as the former are not continuously differentiable and the latter are discontinuous functions of dose, apart from both being nonconvex. In particular, DVH functions often work poorly when used in constraints due to their being identically zero when feasible and having vanishing gradients on the boundary of feasibility.</p><p>In this work, we present a general mathematical framework allowing for direct optimization on all DVH-based metrics. By regarding voxel doses as sample realizations of an auxiliary random variable and using kernel density estimation to obtain explicit formulas, one arrives at formulations of volume-at-dose and dose-at-volume which are infinitely differentiable functions of dose. This is extended to DVH functions and so called volume-based DVH functions, as well as to min/max-dose functions and mean-tail-dose functions. Explicit expressions for evaluation of function values and corresponding gradients are presented. The proposed framework has the advantages of depending on only one smoothness parameter, of approximation errors to conventional counterparts being negligible for practical purposes, and of a general consistency between derived functions.</p><p>Numerical tests, which were performed for illustrative purposes, show that smooth dose-at-volume works better than quadratic penalties when used in constraints and that smooth DVH functions in certain cases have significant advantage over conventional such. The results of this work have been successfully applied to lexicographic optimization in a fluence map optimization setting.</p>
----------------------------------------------------------------------
In diva2:1120314 abstract is:
<p>A better understanding of turbine performance and its sensitivity to variations in the inletboundary conditions is crucial in the quest of further improving the efficiency of aero engines.Within the research efforts to reach this goal, a high-pressure turbine test rig has been designedby Rolls-Royce Deutschland in cooperation with the Deutsches Zentrum für Luft- und Raumfahrt(DLR), the German Aerospace Center. The scope of the test rig is high-precision measurement ofaerodynamic efficiency including the effects of film cooling and secondary air flows as well as theimprovement of numerical prediction tools, especially 3D Computational Fluid Dynamics (CFD).A sensitivity analysis of the test rig based on detailed 3D CFD computations was carried outwith the aim to quantify the influence of inlet boundary condition variations occurring in the testrig on the outlet capacity of the first stage nozzle guide vane (NGV) and the turbine efficiency.The analysis considered variations of the cooling and rimseal leakage mass flow rates as well asfluctuations in the inlet distributions of total temperature and pressure. The influence of anincreased rotor tip clearance was also studied.This thesis covers the creation, calibration and validation of the steady state 3D CFD modelof the full turbine domain. All relevant geometrical details of the blades, walls and the rimsealcavities are included with the exception of the film cooling holes that are replaced by a volumesource term based cooling strip model to reduce the computational cost of the analysis. Thehigh-fidelity CFD computation is run only on a sample of parameter combinations spread overthe entire input parameter space determined using the optimal latin hypercube technique. Thesubsequent sensitivity analysis is based on a Kriging response surface model fit to the sampledata. The results are discussed with regard to the planned experimental campaign on the test rigand general conclusions concerning the impacts of the studied parameters on turbine performanceare deduced.</p>


corrected abstract:
<p>A better understanding of turbine performance and its sensitivity to variations in the inlet boundary conditions is crucial in the quest of further improving the efficiency of aero engines. Within the research efforts to reach this goal, a high-pressure turbine test rig has been designed by Rolls-Royce Deutschland in cooperation with the Deutsches Zentrum für Luft- und Raumfahrt (DLR), the German Aerospace Center. The scope of the test rig is high-precision measurement of aerodynamic efficiency including the effects of film cooling and secondary air flows as well as the improvement of numerical prediction tools, especially 3D Computational Fluid Dynamics (CFD).</p><p>A sensitivity analysis of the test rig based on detailed 3D CFD computations was carried out with the aim to quantify the influence of inlet boundary condition variations occurring in the test rig on the outlet capacity of the first stage nozzle guide vane (NGV) and the turbine efficiency. The analysis considered variations of the cooling and rimseal leakage mass flow rates as well as fluctuations in the inlet distributions of total temperature and pressure. The influence of an increased rotor tip clearance was also studied.</p><p>This thesis covers the creation, calibration and validation of the steady state 3D CFD model of the full turbine domain. All relevant geometrical details of the blades, walls and the rimseal cavities are included with the exception of the film cooling holes that are replaced by a volume source term based cooling strip model to reduce the computational cost of the analysis. The high-fidelity CFD computation is run only on a sample of parameter combinations spread over the entire input parameter space determined using the optimal latin hypercube technique. The subsequent sensitivity analysis is based on a Kriging response surface model fit to the sample data. The results are discussed with regard to the planned experimental campaign on the test rig and general conclusions concerning the impacts of the studied parameters on turbine performance are deduced.</p>
----------------------------------------------------------------------
In diva2:618595 missing space in title:
"Future fuel for worldwide tankershipping in spot market"
==>
"Future fuel for worldwide tanker shipping in spot market"

abstract is:
<p>Ship exhausts contain high levels of sulphur oxides, nitrogen oxides, carbon dioxide and particles dueto the heavy fuel oil, HFO, used for combustion and the combustion characteristics of the engine.As a result of upcoming stricter regulations for shipping pollution, as well as growing attentionto greenhouse gas emissions, air pollution and uncertainty of future petroleum oil supply, a shifttowards a cleaner burning fuel is needed.This work explores potential alternative fuels, both conventional and unconventional, and abatementtechnologies, to be used by tankers in the worldwide spot market to comply with upcomingenvironmental regulations in the near and coming future. As a reference the product tanker M/TGotland Marieann is used and recommendations for which fuel that shall be used by the referenceship in 2015 and 2020 are presented.The environmental assessment and evaluation of the fuels are done from a life cycle perspective usingresults from Life Cycle Assessment, LCA, studies.This study illustrates that, of the various alternatives, methanol appears to be the best candidatefor long-term, widespread replacement of petroleum-based fuels within tanker shipping. It does notemit any sulphur oxides nor particles and the nitrogen oxides are shown to be lower than those ofmarine gas oil, MGO. The global warming potential of the natural gas produced methanol is notlower than that of MGO, but when gradually switching to bio-methanol the greenhouse gas emissionsare decreasing and with methanol the vision of a carbon free society can be reached.For 2015 a switch towards methanol is not seen as realistic. Further research and establishment ofregulations and distribution systems are needed, however there are indications that a shift will bepossible sometime between 2015 and 2020. For 2015 a shift towards MGO is suggested as it involveslow investment costs and there is no need for infrastructure changes. As MGO is more expensivethan methanol, a shift is preferable as soon as the market, technology and infrastructure are ready.</p>

corrected abstract:
<p>Ship exhausts contain high levels of sulphur oxides, nitrogen oxides, carbon dioxide and particles due to the heavy fuel oil, HFO, used for combustion and the combustion characteristics of the engine. As a result of upcoming stricter regulations for shipping pollution, as well as growing attention to greenhouse gas emissions, air pollution and uncertainty of future petroleum oil supply, a shift towards a cleaner burning fuel is needed.</p><p>This work explores potential alternative fuels, both conventional and unconventional, and abatement technologies, to be used by tankers in the worldwide spot market to comply with upcoming environmental regulations in the near and coming future. As a reference the product tanker M/T Gotland Marieann is used and recommendations for which fuel that shall be used by the reference ship in 2015 and 2020 are presented.</p><p>The environmental assessment and evaluation of the fuels are done from a life cycle perspective using results from Life Cycle Assessment, LCA, studies.</p><p>This study illustrates that, of the various alternatives, methanol appears to be the best candidate for long-term, widespread replacement of petroleum-based fuels within tanker shipping. It does not emit any sulphur oxides nor particles and the nitrogen oxides are shown to be lower than those of marine gas oil, MGO. The global warming potential of the natural gas produced methanol is not lower than that of MGO, but when gradually switching to bio-methanol the greenhouse gas emissions are decreasing and with methanol the vision of a carbon free society can be reached.</p><p>For 2015 a switch towards methanol is not seen as realistic. Further research and establishment of regulations and distribution systems are needed, however there are indications that a shift will be possible sometime between 2015 and 2020. For 2015 a shift towards MGO is suggested as it involves low investment costs and there is no need for infrastructure changes. As MGO is more expensive than methanol, a shift is preferable as soon as the market, technology and infrastructure are ready.</p>
----------------------------------------------------------------------
In diva2:401149 abstract is:
<p> </p>
<p> </p>
<p>During the 2000s, the ship owners have become more and more concerned thattheir ships save fuel. Several projects have been undertaken to exploit the resourcesavailable on board today’s vessels to reduce fuel consumption. As a stepin this the Swedish Meteorological and Hydrological Institute (SMHI) today offera Weather Routing service to ships. By planning your route more effectivelymuch fuel can be saved.This thesis has been about developing a fuel prediction program (FPP) forhow much fuel a ship consumes in different sea conditions. The model takes intoaccount the ship’s loading condition, winds, wind waves and swell. Any othereffects are pooled in one term. This makes it possible to also consider how muchfuel the ship consumes on the various route options in the planning process.The model will also be a useful tool to retrospectively evaluate how a ship hasperformed in relation to the contract.On the ships in this report the prediction program was able to calculate thefuel consumption with an error of only 1% of the reported fuel consumption.This requires that the data about the vessel is accurate and up to date. If not,the model can still, with thoughtful assumptions, reach an error of less than10% of the reported consumption, which is better than the strategy that SMHIuses today.</p>



corrected abstract:
<p>During the 2000s, the ship owners have become more and more concerned that their ships save fuel. Several projects have been undertaken to exploit the resources available on board today’s vessels to reduce fuel consumption. As a step in this the Swedish Meteorological and Hydrological Institute (SMHI) today offer a Weather Routing service to ships. By planning your route more effectively much fuel can be saved.</p><p>This thesis has been about developing a fuel prediction program (FPP) for how much fuel a ship consumes in different sea conditions. The model takes into account the ship’s loading condition, winds, wind waves and swell. Any other effects are pooled in one term. This makes it possible to also consider how much fuel the ship consumes on the various route options in the planning process. The model will also be a useful tool to retrospectively evaluate how a ship has performed in relation to the contract.</p><p>On the ships in this report the prediction program was able to calculate the fuel consumption with an error of only 1% of the reported fuel consumption. This requires that the data about the vessel is accurate and up to date. If not, the model can still, with thoughtful assumptions, reach an error of less than 10% of the reported consumption, which is better than the strategy that SMHI uses today.</p>
----------------------------------------------------------------------
In diva2:1877617 - Note: no full text in DiVA

abstract is:
<p>This thesis investigates the optimization of GaAs-based single-line defect photoniccrystal waveguides (PCWs) for mid-infrared (MIR) gas sensing applications. Photoniccrystals (PhCs) are materials with a periodic dielectric constant variation, structuredto create photonic bandgaps for precise light propagation control. This work focuseson the design of PCWs in GaAs membranes and development of processes to fabricateair-holes with smooth and straight sidewalls, which are crucial for optical PhC deviceperformance.The project is structured into two phases. In the first phase, AnsysLumerical is used for the simulation of PCWs, which includes designing a 2D hexagonallattice structure and optimizing parameters such as radius and lattice constant toachieve desired bandstructure, light confinement, and single-mode propagation.Simulation results show that the optimal PCW configuration effectively confines light,provides evanescent fields in both lateral and vertical directions, and supports singlemodepropagation at the target wavelength of 4.26 μm, at which CO2 has a majorabsorption peak. The second phase focuses on the fabrication process, utilizingcharged colloidal lithography and Inductively Coupled Plasma Reactive Ion Etching(ICP-RIE) with Ar/Cl2 chemistry to create air holes in GaAs substrates. The aim is toexplore the effects of varying hole diameters and process conditions to achieve goodhole profiles at a depth of 600 nm — the designed thickness of the GaAs membrane foreffective vertical light confinement. Impact of various parameters on the etch rate andhole profile is studied. A feature-size dependent etching phenomenon, the lag effect,is observed, with a significant variation in etch depths for different hole diameters.The findings provide clear guidelines for optimizing conditions to achieve suitable holedepths and profiles for fabricating MIR PhC devices in GaAs membranes.</p>

corrected abstract:
<p>This thesis investigates the optimization of GaAs-based single-line defect photoniccrystal waveguides (PCWs) for mid-infrared (MIR) gas sensing applications. Photonic crystals (PhCs) are materials with a periodic dielectric constant variation, structured to create photonic bandgaps for precise light propagation control. This work focuses on the design of PCWs in GaAs membranes and development of processes to fabricate air-holes with smooth and straight sidewalls, which are crucial for optical PhC device performance. The project is structured into two phases. In the first phase, Ansys Lumerical is used for the simulation of PCWs, which includes designing a 2D hexagonal lattice structure and optimizing parameters such as radius and lattice constant to achieve desired band structure, light confinement, and single-mode propagation. Simulation results show that the optimal PCW configuration effectively confines light, provides evanescent fields in both lateral and vertical directions, and supports single mode propagation at the target wavelength of 4.26 μm, at which CO<sub>2</sub> has a major absorption peak. The second phase focuses on the fabrication process, utilizing charged colloidal lithography and Inductively Coupled Plasma Reactive Ion Etching(ICP-RIE) with Ar/Cl<<sub>2</sub> chemistry to create air holes in GaAs substrates. The aim is to explore the effects of varying hole diameters and process conditions to achieve good hole profiles at a depth of 600 nm — the designed thickness of the GaAs membrane for effective vertical light confinement. Impact of various parameters on the etch rate and hole profile is studied. A feature-size dependent etching phenomenon, the lag effect, is observed, with a significant variation in etch depths for different hole diameters. The findings provide clear guidelines for optimizing conditions to achieve suitable hole depths and profiles for fabricating MIR PhC devices in GaAs membranes.</p>
----------------------------------------------------------------------
In diva2:1876088 abstract is:
<p>A key component of biological research is cell culture technology, which allows researchersto examine the behavior and functionality of cells in controlled environments. Conventionalcell culture monitoring frequently necessitates taking the cultures out of their incubators tomake observations under a microscope. This exposes them to pollutants and changes in thesurrounding environment and may jeopardize the integrity of the experiment.This thesis presents the development of a cost-effective, miniaturized microscope designedfor imaging of cell cultures directly within incubators. By integrating simple, inexpensiveglass lenses and 3D-printed components and focusing on the ESP32-CAM module for digitalimaging, the project explores various optical setups to optimize image quality while minimizingdisruption to cell environments.Central to the research was the identification and testing of diverse optical configurations todetermine the most effective arrangement for both brightfield and fluorescence microscopy.The design features a baseplate for stability, a filter plate for fluorescence imaging, and afocus adjustment mechanism using magnets. Iterative enhancements led to a side illuminationtechnique using an economical LED, removing the need for a beamsplitter and simplifying theoptical path.The final microscope demonstrated successful brightfield imaging and weak fluorescenceimaging of Madin-Darby Canine Kidney (MDCK) II cell cultures marked with Green FluorescentProtein (GFP), using a magnification ratio of 2.5:1 through an infinity-corrected optical system.The findings illustrate the potential of developing an economical, functional microscope thatcan be readily replicated and scaled for use in cell culture technology.</p><p> </p>

corrected abstract:
<p>A key component of biological research is cell culture technology, which allows researchers to examine the behavior and functionality of cells in controlled environments. Conventional cell culture monitoring frequently necessitates taking the cultures out of their incubators to make observations under a microscope. This exposes them to pollutants and changes in the surrounding environment and may jeopardize the integrity of the experiment.</p><p>This thesis presents the development of a cost-effective, miniaturized microscope designed for imaging of cell cultures directly within incubators. By integrating simple, inexpensive glass lenses and 3D-printed components and focusing on the ESP32-CAM module for digital imaging, the project explores various optical setups to optimize image quality while minimizing disruption to cell environments.</p><p>Central to the research was the identification and testing of diverse optical configurations to determine the most effective arrangement for both brightfield and fluorescence microscopy. The design features a baseplate for stability, a filter plate for fluorescence imaging, and a focus adjustment mechanism using magnets. Iterative enhancements led to a side illumination technique using an economical LED, removing the need for a beamsplitter and simplifying the optical path.</p><p>The final microscope demonstrated successful brightfield imaging and weak fluorescence imaging of Madin-Darby Canine Kidney (MDCK) II cell cultures marked with Green Fluorescent Protein (GFP), using a magnification ratio of 2.5:1 through an infinity-corrected optical system. The findings illustrate the potential of developing an economical, functional microscope that can be readily replicated and scaled for use in cell culture technology.</p>
----------------------------------------------------------------------
In diva2:1787386 abstract is:
<p>In this thesis, an innovative coarse grid CFD approach is developed that aims toexploit the capabilities of sub-channel codes and CFD methods while overcoming theirlimitations. In the approach, a very coarse mesh is implemented in the CFD softwareOpenFOAM and a new wall treatment, based on the traditional concept of the wallfunction, is applied to the wall boundary conditions of the domain to take into accountthe low resolution of the grid which does not allow to effectively capture the effect of thesolid walls on the thermo-hydraulics of the flow. To investigate the performance of thenew approach, the method is implemented first in three simple test cases for whichthe sub-channel codes are the state-of-the-art thermo-hydraulic analysis since theyare single-phase flow problems in which there are no prevailing 3D flow conditions.An additional test case representing a 2x2 fuel bundle with three full-length rods andone half-length rod is investigated to verify the behavior of the new approach in caseswhere secondary flows are present. The results for the pressure fields are comparedwith the analytical pressure profiles for the four test cases that well represent the onesthat would be obtained with sub-channel code analysis, while the results for the wallshear stresses obtained in the four test cases are compared with the ones obtained witha more refined mesh in which the traditional wall function approach is implementedsince they should be the best estimation of the actual wall shear stresses at the walldomain. For the first two cases, the developed approach produces reasonable resultswith a good agreement to the analytical pressure profiles while the other two testcases show that the methodology has a limited applicability and, before proceedingwith the extension of the new approach to single-phase problems with 3D prevailingphenomena and two-phase problems, it is necessary to solve the issues that emerge forsome types of cases.</p>

corrected abstract:
<p>In this thesis, an innovative coarse grid CFD approach is developed that aims to exploit the capabilities of sub-channel codes and CFD methods while overcoming their limitations. In the approach, a very coarse mesh is implemented in the CFD software OpenFOAM and a new wall treatment, based on the traditional concept of the wall function, is applied to the wall boundary conditions of the domain to take into account the low resolution of the grid which does not allow to effectively capture the effect of the solid walls on the thermo-hydraulics of the flow. To investigate the performance of the new approach, the method is implemented first in three simple test cases for which the sub-channel codes are the state-of-the-art thermo-hydraulic analysis since they are single-phase flow problems in which there are no prevailing 3D flow conditions. An additional test case representing a 2x2 fuel bundle with three full-length rods and one half-length rod is investigated to verify the behavior of the new approach in cases where secondary flows are present. The results for the pressure fields are compared with the analytical pressure profiles for the four test cases that well represent the ones that would be obtained with sub-channel code analysis, while the results for the wall shear stresses obtained in the four test cases are compared with the ones obtained with a more refined mesh in which the traditional wall function approach is implemented since they should be the best estimation of the actual wall shear stresses at the wall domain. For the first two cases, the developed approach produces reasonable results with a good agreement to the analytical pressure profiles while the other two test cases show that the methodology has a limited applicability and, before proceeding with the extension of the new approach to single-phase problems with 3D prevailing phenomena and two-phase problems, it is necessary to solve the issues that emerge for some types of cases.</p>
----------------------------------------------------------------------
In diva2:1782728 abstract is:
<p>Composite monocoque frames are becoming increasingly more popular inperformance cars. Compared to their steel and aluminum counterparts theyprovide additional torsional stiffness at the cost of less weight. This thesiscovers the complex optimization process of a monocoque applied within theregulations of a Formula Student competition. It aims to give the reader a goodunderstanding of the rules and how they affect the optimization process whilegenerating an optimized design used in the competition of Formula StudentGermany -21 by KTH Formula Student.</p><p>The rules of Formula Student dictate the structural requirements on themonocoque based on a steel space frame. All materials except low carbon steelused in the structure require proof of equivalence through regulated testingmethods. However, this thesis shows that the regulated setup can severelyaffect results through a deep analysis of the testing methodology.The torsional stiffness of the monocoque is analyzed and optimized accordingto the results of a free-size optimization. Both through slight adjustmentsin chassis geometry and the laminate, resulting in a theoretical torsionalstiffness of 9.9 kNm/deg, more than five times as much as the old space frame.Weighing in at 20 kg, a significant weight reduction of about 10 kg, eventhough it was larger, with a surface area of about 4.2 m2.</p><p>This design will be the first monocoque manufactured within KTH FormulaStudent since 2010. Therefore, a lot of focus was put on analyzing the rulesand lay the ground for future development by conducting tests on optimizedpanels. These results have the potential to further reduce the weight of a futuremonocoque with a different geometry.</p>

corrected abstract:
<p>Composite monocoque frames are becoming increasingly more popular in performance cars. Compared to their steel and aluminum counterparts they provide additional torsional stiffness at the cost of less weight. This thesis covers the complex optimization process of a monocoque applied within the regulations of a Formula Student competition. It aims to give the reader a good understanding of the rules and how they affect the optimization process while generating an optimized design used in the competition of Formula Student Germany -21 by KTH Formula Student.</p><p>The rules of Formula Student dictate the structural requirements on the monocoque based on a steel space frame. All materials except low carbon steel used in the structure require proof of equivalence through regulated testing methods. However, this thesis shows that the regulated setup can severely affect results through a deep analysis of the testing methodology. The torsional stiffness of the monocoque is analyzed and optimized according to the results of a free-size optimization. Both through slight adjustments in chassis geometry and the laminate, resulting in a theoretical torsional stiffness of 9.9 kNm/deg, more than five times as much as the old space frame. Weighing in at 20 kg, a significant weight reduction of about 10 kg, even though it was larger, with a surface area of about 4.2 m<sup>2</sup>.</p><p>This design will be the first monocoque manufactured within KTH Formula Student since 2010. Therefore a lot of focus was put on analyzing the rules and lay the ground for future development by conducting tests on optimized panels. These results have the potential to further reduce the weight of a future monocoque with a different geometry.</p>
----------------------------------------------------------------------
In diva2:1780558 abstract is:
<p>To mitigate climate change a proposed space-based geoengineering solutionis to screen off solar irradiance by placing a membrane in between the Earthand the Sun. The feasibility of such a project largely depends on minimizingthe mass of the shading screen and as an extension to the Sunshade projectthis thesis investigated how such a low-mass membrane could be designed.Because of the acting forces at location in space, minimizing the mass impliesthat the material ought to have a low reflection coeﬀicient and surface densityand therefore the highly transparent material of artificial spider silk was chosenas the proposed material. The only possibility to block light is then byrefraction or diffraction and, since the presence of apertures might lower thesurface density, the structure of the suggested membrane is a grid patternof wires. Such a diffraction grating was investigated while applying twomethods. Method 1 optimized the dimensions of the structure to lower thetotal transmission on Earth when placed on the direct transmission axis ofthe membrane and method 2 tilted the membrane in order to place Earth ata diffraction minimum. This resulted in three suggested designs A, B, andC with surface densities varying from that of 0.00867 to 0.228 gm−2. Theresults were compared with two previous design proposals where the lowestareal density was 0.34g/m2, which is 3/2 to 40 times larger than the densitiesproposed in this paper. The reflectivities for A and B were 12.5 and 3.75 timeslarger than that of the smallest previously achieved reflectivity. The reflectivityof C could not be determined exactly but ought to have a reflectivity at leastas low as B at 3%, making it the most promising candidate for a membranedesign of the three.</p>

corrected abstract:
<p>To mitigate climate change a proposed space-based geoengineering solution is to screen off solar irradiance by placing a membrane in between the Earth and the Sun. The feasibility of such a project largely depends on minimizing the mass of the shading screen and as an extension to the Sunshade project this thesis investigated how such a low-mass membrane could be designed. Because of the acting forces at location in space, minimizing the mass implies that the material ought to have a low reflection coefficient and surface density and therefore the highly transparent material of artificial spider silk was chosen as the proposed material. The only possibility to block light is then by refraction or diffraction and, since the presence of apertures might lower the surface density, the structure of the suggested membrane is a grid pattern of wires. Such a diffraction grating was investigated while applying two methods. Method 1 optimized the dimensions of the structure to lower the total transmission on Earth when placed on the direct transmission axis of the membrane and method 2 tilted the membrane in order to place Earth at a diffraction minimum. This resulted in three suggested designs A, B, and C with surface densities varying from that of 0.00867 to 0.228 gm<sup>−2</sup>. The results were compared with two previous design proposals where the lowest areal density was 0.34gm<sup>-2</sup>, which is 3/2 to 40 times larger than the densities proposed in this paper. The reflectivities for A and B were 12.5 and 3.75 times larger than that of the smallest previously achieved reflectivity. The reflectivity of C could not be determined exactly but ought to have a reflectivity at least as low as B at 3%, making it the most promising candidate for a membrane design of the three.</p>
----------------------------------------------------------------------
In diva2:1761916 abstract is:
<p>Since its introduction by Max Otto Lorenz, the Lorenz curve has been utilizedin several financial contexts. By using regression analysis to approximate theclaim cost of policyholders, a vector consisting of policyholder characteristics canbe obtained. The ordered Lorenz curve can subsequently be used to understandwhat commonalities are shared between profitable policyholders. This allows forbetter management of the insurance portfolio and thus better customer relationstowards both the policyholders and the insurer, which is important for an insuranceconsultancy agency. The aim of this thesis was to investigate which attributesapproximate the policyholder claim costs and consequently obtain insight into whatattributes are shared among profitable portfolio clients. The results presented inthis thesis show that a multi-linear regression model, transformed using the Box-Cox method is insuﬀicient to approximate the claim costs in a convincing manner.The model obtained in the thesis was capable of identifying significant regressorsbut the overall result displayed uncertainties in regards to overall goodness of fit.This means that the variability explained by the regression model only represents4.95% of the variability in the claim cost data. Thus, the relativity measureintroduced in section 2.1.1 was deemed uninterruptible in a meaningful way.Consequently, the empirical distribution functions presented in section 1.1 wouldbe based on a faulty order statistic, and in turn the visualization of an orderedLorenz curve with such a relativity measure is unnecessary.</p>

corrected abstract:
<p>Since its introduction by Max Otto Lorenz, the Lorenz curve has been utilized in several financial contexts. By using regression analysis to approximate the claim cost of policyholders, a vector consisting of policyholder characteristics can be obtained. The ordered Lorenz curve can subsequently be used to understand what commonalities are shared between profitable policyholders. This allows for better management of the insurance portfolio and thus better customer relations towards both the policyholders and the insurer, which is important for an insurance consultancy agency. The aim of this thesis was to investigate which attributes approximate the policyholder claim costs and consequently obtain insight into what attributes are shared among profitable portfolio clients. The results presented inthis thesis show that a multi-linear regression model, transformed using the Box-Cox method is insufficient to approximate the claim costs in a convincing manner. The model obtained in the thesis was capable of identifying significant regressors but the overall result displayed uncertainties in regards to overall goodness of fit. This means that the variability explained by the regression model only represents 4.95% of the variability in the claim cost data. Thus, the relativity measure introduced in section 2.1.1 was deemed uninterruptible in a meaningful way. Consequently, the empirical distribution functions presented in section 1.1 would be based on a faulty order statistic, and in turn the visualization of an ordered Lorenz curve with such a relativity measure is unnecessary.</p>
----------------------------------------------------------------------
In diva2:1583521 abstract is:
<p>With the increasing number of satellites operating in orbit and the development of nanosatelliteconstellations, it has become more and more arduous for operators to keep track of every satellitestate, and perform corrective or avoidance manoeuvres. That is why CNES, the French space agency,is developing new algorithms, which aimed at making satellites more self-su cient. More especially,these algorithms are in charge of autonomous orbit control, collision risk calculations and satellitestatus monitoring. In this thesis, we present the architecture of these three algorithms and how theyinteract between them to deal with the autonomous control of a satellite. In addition, this paper studiestheir integration within the OPS-SAT nanosatellite, which is an in-orbit demonstrator developed bythe European Space Agency (ESA) and opened to worldwide experimenters. By analysing the dataused by the numerical propagators, the size of the input configuration files sent to the nanosatellitewas optimised. Thanks to this optimisation, the size of telecommands sent during each OPS-SATflyby above the ESOC ground station meets the requirements.</p><p>Due to some issues encountered with the nanosatellite’s GPS, a solution was found to update thecurrent orbit on-board, and thus allow the proper algorithms’ operation. This thesis also introduceshow the tests were carried out in order to validate these algorithms, both on flat-sat and on the realsatellite. The results demonstrate that their integration on the OPS-SAT numerical environment issuccessful, meaning that the algorithms and their dependences are correctly packaged, sent and uploaded,and that they work as expected. Their execution time are of course longer due to the limitedcalculation capacity of the on-board computer, but are still compatible with real operations, except forthe collision risk computation, which can exceed the orbital period depending on the initial conditions.Finally, the thesis presents the process of real operations for one of the three algorithms developed byCNES, the di culties encountered and the solutions considered.</p>

corrected abstract:
<p>With the increasing number of satellites operating in orbit and the development of nanosatellite constellations, it has become more and more arduous for operators to keep track of every satellite state, and perform corrective or avoidance manoeuvres. That is why CNES, the French space agency, is developing new algorithms, which aimed at making satellites more self-sufficient. More especially, these algorithms are in charge of autonomous orbit control, collision risk calculations and satellite status monitoring. In this thesis, we present the architecture of these three algorithms and how they interact between them to deal with the autonomous control of a satellite. In addition, this paper studies their integration within the OPS-SAT nanosatellite, which is an in-orbit demonstrator developed by the European Space Agency (ESA) and opened to worldwide experimenters. By analysing the data used by the numerical propagators, the size of the input configuration files sent to the nanosatellite was optimised. Thanks to this optimisation, the size of telecommands sent during each OPS-SAT flyby above the ESOC ground station meets the requirements.</p><p>Due to some issues encountered with the nanosatellite’s GPS, a solution was found to update the current orbit on-board, and thus allow the proper algorithms’ operation. This thesis also introduces how the tests were carried out in order to validate these algorithms, both on flat-sat and on the real satellite. The results demonstrate that their integration on the OPS-SAT numerical environment is successful, meaning that the algorithms and their dependences are correctly packaged, sent and uploaded, and that they work as expected. Their execution time are of course longer due to the limited calculation capacity of the on-board computer, but are still compatible with real operations, except for the collision risk computation, which can exceed the orbital period depending on the initial conditions. Finally, the thesis presents the process of real operations for one of the three algorithms developed by CNES, the difficulties encountered and the solutions considered.</p>
----------------------------------------------------------------------
In diva2:1465506 abstract is:
<p>Silencers are used in vehicles to reduce the noise in the engine system. However,silencers themselves may produce break-out noise due to the interactionwith the exhaust gas ow and structure. In this Master thesis project, thenumerical simulation of vibrational behavior of housing plates of silencers isdeveloped.The housing plate is composed of two steel plates and a brous materiallayer. Measurement results show that the brous material has good dampingeect to decrease the vibration and radiated sound of steel plates. Steel platesare connected by spot welding. Modeling of spot welds can improve themodal assurance criterion between simulation and measurements. Interfacedamping is introduced into the simulation models to simulate the contacteect between two steel plates so that the simulated amplitude can have agood agreement with measurement result.Several numerical models of brous material are investigated. The Mikimodel is not chosen for the nal result due to the limit of range of frequencies.The rigid frame model can simulate the sound absorption but is unfeasible forthe simulation of vibration. The limp frame model can simulate the vibrationof light glass wool but cannot simulate the vibration of heavy glass wool.Finally, the Biot-Allard model which is a poro-elastic model is investigatedfor the nal result. The simulation results show good agreement with themeasurement result.</p>


corrected abstract:
<p>Silencers are used in vehicles to reduce the noise in the engine system. However, silencers themselves may produce break-out noise due to the interaction with the exhaust gas flow and structure. In this Master thesis project, the numerical simulation of vibrational behavior of housing plates of silencers is developed.</p><p>The housing plate is composed of two steel plates and a fibrous material layer. Measurement results show that the fibrous material has good damping effect to decrease the vibration and radiated sound of steel plates. Steel plates are connected by spot welding. Modeling of spot welds can improve the modal assurance criterion between simulation and measurements. Interface damping is introduced into the simulation models to simulate the contact effect between two steel plates so that the simulated amplitude can have a good agreement with measurement result.</p><p>Several numerical models of fibrous material are investigated. The Miki model is not chosen for the final result due to the limit of range of frequencies. The rigid frame model can simulate the sound absorption but is unfeasible for the simulation of vibration. The limp frame model can simulate the vibration of light glass wool but cannot simulate the vibration of heavy glass wool. Finally, the Biot-Allard model which is a poro-elastic model is investigated for the final result. The simulation results show good agreement with the measurement result.</p>
----------------------------------------------------------------------
In diva2:1244326 missing space in title:
"Transonic Flutter for aGeneric Fighter Configuration"
==>
"Transonic Flutter for a Generic Fighter Configuration"

abstract is:
<p>A hazardous and not fully understood aeroelastic phenomenon is the transonic dip,the decrease in flutter dynamic pressure that occurs for most aircraft configurationsin transonic flows. The difficulty of predicting this phenomenon forces aircraft manufacturersto run long and costly flight test campaigns to demonstrate flutter-free behaviourof their aircraft at transonic Mach numbers.In this project, subsonic and transonic flutter calculations for the KTH-NASA genericfighter research model have been performed and compared to existing experimentalflutter data from wind tunnel tests performed at NASA Langley in 2016. For the fluttercalculations, industry-standard linear panel methods have been used together with afinite element model from NASTRAN.Further, an alternative approach for more accurate transonic flutter predictions usingthe full-potential solver Phi has been investigated. To predict flutter using this newmethodology a simplified structural model has been used together with aerodynamicmeshes of the main wing. The purpose of the approach was to see if it was possibleto find a method that was more accurate than panel methods in the transonic regimewhilst still being suitable for use during iterative design processes.The results of this project demonstrated that industry-standard linear panel methodssignificantly over-predict the flutter boundary in the transonic regime. It was alsoseen that the flutter predictions using Phi showed potential, being close to the linearresults for the same configuration as tested in Phi. For improved transonic accuracy inPhi, an improved transonic flow finite element formulation could possibly help .Another challenge with Phi is the requirement of an explicit wake from all liftingsurfaces in the aerodynamic mesh. Therefore, a method for meshing external storeswith blunt trailing edges needs to be developed. One concept suggested in this projectis to model external stores in "2.5D", representing external stores using airfoils withsharp trailing edges.</p>

corrected abstract:
<p>A hazardous and not fully understood aeroelastic phenomenon is the transonic dip, the decrease in flutter dynamic pressure that occurs for most aircraft configurations in transonic flows. The difficulty of predicting this phenomenon forces aircraft manufacturers to run long and costly flight test campaigns to demonstrate flutter-free behaviour of their aircraft at transonic Mach numbers.</p><p>In this project, subsonic and transonic flutter calculations for the KTH-NASA generic fighter research model have been performed and compared to existing experimental flutter data from wind tunnel tests performed at NASA Langley in 2016. For the flutter calculations, industry-standard linear panel methods have been used together with a finite element model from NASTRAN.</p><p>Further, an alternative approach for more accurate transonic flutter predictions using the full-potential solver Phi has been investigated. To predict flutter using this new methodology a simplified structural model has been used together with aerodynamic meshes of the main wing. The purpose of the approach was to see if it was possible to find a method that was more accurate than panel methods in the transonic regime whilst still being suitable for use during iterative design processes.</p><p>The results of this project demonstrated that industry-standard linear panel methods significantly over-predict the flutter boundary in the transonic regime. It was also seen that the flutter predictions using Phi showed potential, being close to the linear results for the same configuration as tested in Phi. For improved transonic accuracy in Phi, an improved transonic flow finite element formulation could possibly help.</p><p>Another challenge with Phi is the requirement of an explicit wake from all lifting surfaces in the aerodynamic mesh. Therefore, a method for meshing external stores with blunt trailing edges needs to be developed. One concept suggested in this project is to model external stores in "2.5D", representing external stores using airfoils with sharp trailing edges.</p>
----------------------------------------------------------------------
In diva2:1083220 abstract is:
<p>The current pollution policies in all European and American countries are forcing the industry to movetowards a more efficient and environmentally friendly engines. On the other hand, customers requiremaintaining the power and fuel consumption. Lowering mainly nitrous oxides (NOx) and carbon particles(Soot) is therefore a challenging task with a very strong impact on mainly the automotive andaeronautical market.The purpose of the current work is to research the pollution production of automotive diesel enginesand optimize the fuel injection and piston geometry to lower the emissions. The interaction betweenfuel and air as well as the combustion are the two main physical and chemical processes governing thepollutants formation. Converged-CFD will be the CFD tool employed during the analysis of the previousproblems.The fuel-air interaction is related to jet break up, vaporization and turbulence. The strong dependenceon the surrounding flow field of the previous processes require the equations to be solved numericallywithin a CFD code. The fuel is to be placed in a combustion chamber (piston) where the spray will affectthe surrounding flow field and ultimately the combustion process.In order to accurately represent the nature of the processes, the current work is divided into two mainchapters. Spray modelling and Combustion Modelling. The first will help to accurately model the discretephase (fuel spray) and the vapour entrainment. The second chapter, combustion modelling willretrieve the knowledge gain in the first part to accurately represent the fuel injection in the chamber aswell as the combustion process to ultimately model the pollutants emissions.Finally, a piston bowl optimization will be performed using the previous analysed models and give theindustry a measure of the potential improvement by just adjusting the fuel injection or by modifyingthe piston bowl geometry.</p>


corrected abstract:
<p>The current pollution policies in all European and American countries are forcing the industry to move towards a more efficient and environmentally friendly engines. On the other hand, customers require maintaining the power and fuel consumption. Lowering mainly nitrous oxides (NOx) and carbon particles (Soot) is therefore a challenging task with a very strong impact on mainly the automotive and aeronautical market.</p><p>The purpose of the current work is to research the pollution production of automotive diesel engines and optimize the fuel injection and piston geometry to lower the emissions. The interaction between fuel and air as well as the combustion are the two main physical and chemical processes governing the pollutants formation. Converged-CFD will be the CFD tool employed during the analysis of the previous problems.</p><p>The fuel-air interaction is related to jet break up, vaporization and turbulence. The strong dependence on the surrounding flow field of the previous processes require the equations to be solved numerically within a CFD code. The fuel is to be placed in a combustion chamber (piston) where the spray will affect the surrounding flow field and ultimately the combustion process.</p><p>In order to accurately represent the nature of the processes, the current work is divided into two main chapters. Spray modelling and Combustion Modelling. The first will help to accurately model the discrete phase (fuel spray) and the vapour entrainment. The second chapter, combustion modelling will retrieve the knowledge gain in the first part to accurately represent the fuel injection in the chamber as well as the combustion process to ultimately model the pollutants emissions.</p><p>Finally, a piston bowl optimization will be performed using the previous analysed models and give the industry a measure of the potential improvement by just adjusting the fuel injection or by modifying the piston bowl geometry.</p>
----------------------------------------------------------------------
In diva2:1078083 missing space in title:
"Wind tunnel blockage corrections forwind turbine measurements"
==>
"Wind tunnel blockage corrections for wind turbine measurements"

abstract is:
<p>Wind-tunnel measurements are an important step during the windturbinedesign process. The goal of wind-tunnel tests is to estimate theoperational performance of the wind turbine, for example by measuringthe power and thrust coecients. Depending on the sizes of both thewind turbine and the test section, the eect of blockage can be substantial.Correction schemes for the power and thrust coecients havebeen proposed in the literature, but for high blockage and highly loadedrotors these correction schemes become less accurate.A new method is proposed here to calculate the eect a cylindricalwind-tunnel test section has on the performance of the wind turbine.The wind turbine is modeled with a simplied vortex model. Usingvortices of constant circulation to model the wake vortices, the performancecharacteristics are estimated. The test section is modeled witha panel method, adapted for this specic situation. It uses irrotationalaxisymmetric source panels to enforce the solid-wall boundary condition.Combining both models in an iterative scheme allows for thesimulation of the eect of the presence of the test-section walls on windturbines performace.Based on the proposed wind-tunnel model, a more general empirical correlationscheme is proposed to estimate the performance characteristicsof a wind turbine operating under unconned conditions by correctingthe performance measured in the conned wind-tunnel conguration.The proposed correction scheme performs better than the existing correctionschemes, including cases with high blockage and highly loadedrotors.</p>


corrected abstract:
<p>Wind-tunnel measurements are an important step during the windturbine design process. The goal of wind-tunnel tests is to estimate the operational performance of the wind turbine, for example by measuring the power and thrust coefficients. Depending on the sizes of both the wind turbine and the test section, the effect of blockage can be substantial. Correction schemes for the power and thrust coefficients have been proposed in the literature, but for high blockage and highly loaded rotors these correction schemes become less accurate.</p><p>A new method is proposed here to calculate the effect a cylindrical wind-tunnel test section has on the performance of the wind turbine. The wind turbine is modeled with a simplified vortex model. Using vortices of constant circulation to model the wake vortices, the performance characteristics are estimated. The test section is modeled with a panel method, adapted for this specific situation. It uses irrotational axisymmetric source panels to enforce the solid-wall boundary condition. Combining both models in an iterative scheme allows for the simulation of the effect of the presence of the test-section walls on wind turbines performace.</p><p>Based on the proposed wind-tunnel model, a more general empirical correlation scheme is proposed to estimate the performance characteristics of a wind turbine operating under unconfined conditions by correcting the performance measured in the confined wind-tunnel configuration. The proposed correction scheme performs better than the existing correction schemes, including cases with high blockage and highly loaded rotors.</p>
----------------------------------------------------------------------
In diva2:1817116 abstract is:
<p>Composite structures are commonly joined using adhesive or mechanical joints, withmechanical joints being preferred when components need to be removable for maintenancepurposes. However, the presence of mechanical joints introduces a discontinuity in theload path, which can serve as an initiation point for failure and needs to be taken intoaccount in the design of the joint. Additionally, delaminations may occur around thefastener hole during the manufacturing and assembly processes, further impacting thestrength of the laminate under compressive loading. While some studies have assessedthe residual strength of open-hole specimens, limited information exists regarding theresidual bearing strength in delaminated composite joints. This study aims to assessthe significance of delaminations of varying sizes on the bearing strength of single-bolt,single-lap shear joints under static loading using numerical analysis methods. The effectsof countersinking and bolt size are also examined. Stress and progressive failure analysisare utilized to evaluate different parameters and account for the nonlinear behavior of thematerials. The study reveals that the presence of delamination leads to degradation ofthe bearing strength of approximately five percent when bolt pretension is applied and15 percent in the absence of pretension. Countersinking increases maximum and averagestresses on the cylindrical section of the hole, while a larger bolt size enhances bearingstrength by reducing bolt bending in single-lap shear joints.</p>

corrected abstract:
<p>Composite structures are commonly joined using adhesive or mechanical joints, with mechanical joints being preferred when components need to be removable for maintenance purposes. However, the presence of mechanical joints introduces a discontinuity in the load path, which can serve as an initiation point for failure and needs to be taken into account in the design of the joint. Additionally, delaminations may occur around the fastener hole during the manufacturing and assembly processes, further impacting the strength of the laminate under compressive loading. While some studies have assessed the residual strength of open-hole specimens, limited information exists regarding the residual bearing strength in delaminated composite joints. This study aims to assess the significance of delaminations of varying sizes on the bearing strength of single-bolt, single-lap shear joints under static loading using numerical analysis methods. The effects of countersinking and bolt size are also examined. Stress and progressive failure analysis are utilized to evaluate different parameters and account for the nonlinear behavior of the materials. The study reveals that the presence of delamination leads to degradation of the bearing strength of approximately five percent when bolt pretension is applied and 15 percent in the absence of pretension. Countersinking increases maximum and average stresses on the cylindrical section of the hole, while a larger bolt size enhances bearing strength by reducing bolt bending in single-lap shear joints.</p>
----------------------------------------------------------------------
In diva2:1817018 abstract is:
<p>Space industry has been booming in the recent times, investments have not justbeen made on satellites and launch vehicles but also on space sustainability. Spaceindustry has its users in national agencies, private commercial agencies, academia andexperimental organisations. Smallsats and cubesats are one of the most interestingdomains in space industry today. This has led to pure research and astonishinginnovations in this domain to enable lower costs, lower mass, increased orbital period,better accessibility and global impact. Development of sustainable products requiresthe system to qualify a certain standard set up in the industry. This standard ensuresthat the system safely completes its mission up in space. The problem described byGomSpace Sweden concerns one of their ongoing products, a cold gas propulsionunit which is suitable for a typical 6U cubesat called ESA6DOF. In large, the productconsists of a structure, two propellant tanks, one plenum tank, piping, electronics andsix thrusters. Qualification of this propulsion system module involves the system toundergo a random vibration test according to ECSS standards. This thesis work shallbe focused on setting up the structure needed to perform random vibration simulationsin COMSOL. This step is done to primarily iterate the design to make it robust enoughto sustain loads during flight and also to avoid any physical damages during testingcampaign. Followed by performing the actual random vibration test at a facility usingthe assembled ESA6DOF propulsion module. Finally, this ends with validating thesimulation results with that of testing.</p>

corrected abstract:
<p>Space industry has been booming in the recent times, investments have not just been made on satellites and launch vehicles but also on space sustainability. Space industry has its users in national agencies, private commercial agencies, academia and experimental organisations. Smallsats and cubesats are one of the most interesting domains in space industry today. This has led to pure research and astonishing innovations in this domain to enable lower costs, lower mass, increased orbital period, better accessibility and global impact. Development of sustainable products requires the system to qualify a certain standard set up in the industry. This standard ensures that the system safely completes its mission up in space. The problem described by GomSpace Sweden concerns one of their ongoing products, a cold gas propulsion unit which is suitable for a typical 6U cubesat called ESA6DOF. In large, the product consists of a structure, two propellant tanks, one plenum tank, piping, electronics and six thrusters. Qualification of this propulsion system module involves the system to undergo a random vibration test according to ECSS standards. This thesis work shall be focused on setting up the structure needed to perform random vibration simulations in COMSOL. This step is done to primarily iterate the design to make it robust enough to sustain loads during flight and also to avoid any physical damages during testing campaign. Followed by performing the actual random vibration test at a facility using the assembled ESA6DOF propulsion module. Finally, this ends with validating the simulation results with that of testing.</p>
----------------------------------------------------------------------
In diva2:1745587 abstract is:
<p>This project’s idea revolved around utilizing the most recent techniques in MachineLearning, Neural Networks, and Data processing to construct a model to be used asa tool to determine stability during core design work. This goal will be achieved bycollecting distribution profiles describing the core state from different steady statesin five burn-up cycles in a reactor to serve as the dataset for training the model. Anadditional cycle will be reserved as a blind testing dataset for the trained model topredict. The variables that will be the target for the predictions are the decay ratioand the frequency since they describe the core stability.The distribution profiles extracted from the core simulator POLCA7 were subjectedto many different Data processing techniques to isolate the most relevant variablesto stability. The processed input variables were merged with the decay ratio andfrequency for those cases, as calculated with POLCA-T. Two different MachineLearning models, one for each output parameter, were designed with Pytorch toanalyze those labeled datasets. The goal of the project was to predict the outputvariables with an error lower than 0.1 for decay ratio and 0.05 for frequency. Themodels were able to predict the testing data with an RMSE of 0.0767 for decay ratioand 0.0354 for frequency.Finally, the trained models were saved and tasked with predicting the outputparameters for a completely unknown cycle. The RMSE was even better forthe unknown cycle, with 0.0615 for decay ratio and 0.0257 for frequency,respectively.</p>

corrected abstract:
<p>This project’s idea revolved around utilizing the most recent techniques in Machine Learning, Neural Networks, and Data processing to construct a model to be used as a tool to determine stability during core design work. This goal will be achieved by collecting distribution profiles describing the core state from different steady states in five burn-up cycles in a reactor to serve as the dataset for training the model. An additional cycle will be reserved as a blind testing dataset for the trained model to predict. The variables that will be the target for the predictions are the decay ratio and the frequency since they describe the core stability.</p><p>The distribution profiles extracted from the core simulator POLCA7 were subjected to many different Data processing techniques to isolate the most relevant variables to stability. The processed input variables were merged with the decay ratio and frequency for those cases, as calculated with POLCA-T. Two different Machine Learning models, one for each output parameter, were designed with Pytorch to analyze those labeled datasets. The goal of the project was to predict the output variables with an error lower than 0.1 for decay ratio and 0.05 for frequency. The models were able to predict the testing data with an RMSE of 0.0767 for decay ratio and 0.0354 for frequency.</p><p>Finally, the trained models were saved and tasked with predicting the output parameters for a completely unknown cycle. The RMSE was even better for the unknown cycle, with 0.0615 for decay ratio and 0.0257 for frequency, respectively.</p>
----------------------------------------------------------------------
In diva2:1679005 abstract is:
<p>As an alternative solution to global warming, this thesis explores the possibility of aspace-based geoengineering scheme that may prove worthwhile to implement in parallel toother environmental efforts that help mitigate impact of climate change. One suggestionof a geoengineering solution is deploying a large number of sunshades in the vicinity ofthe first Lagrange point of the Sun-Earth system, and this prospective sunshade projectwould serve to shield Earth from incident solar radiation. This thesis is an extension ofa feasibility study for the implementation of this large-scale mission, and has a focus oncomparing electric thrusters to solar sailing as a means of propulsion. Background onelectric propulsion systems and spaceflight mechanics is provided. The investigation wasperformed by defining the spacecraft configurations, and then computing trajectories toa point of escape from Earth and from there to the final equilibrium point.Our results show that in order to meet the propellant demands of the electric thrusters,the launch mass would need to increase by around 15-25 % compared to the solar sailingimplementation, equating to around 1010 kg. Nevertheless, electric propulsion could stillbe a beneficial choice since it would allow shorter transfer times for each shade whichreduces the radiation exposure and subsequent degradation of the spacecraft’s systems.It was found that the transfer time with electric propulsion would be about one-half orone-fifth that of solar sailing, depending on spacecraft parameters. Additionally, electricpropulsion allows a much lower initial parking orbit, and while this would increase the ra-diation exposure it would also reduce the launch costs due to the higher payload capacityto lower altitudes. However, electric propulsion of this scale require prior advancementsin xenon or other inert propellant extraction methods and possibly a wide-scale construc-tion of air separation plants.</p>


corrected abstract:
<p>As an alternative solution to global warming, this thesis explores the possibility of a space-based geoengineering scheme that may prove worthwhile to implement in parallel to other environmental efforts that help mitigate impact of climate change. One suggestion of a geoengineering solution is deploying a large number of sunshades in the vicinity of the first Lagrange point of the Sun-Earth system, and this prospective sunshade project would serve to shield Earth from incident solar radiation. This thesis is an extension of a feasibility study for the implementation of this large-scale mission, and has a focus on comparing electric thrusters to solar sailing as a means of propulsion. Background on electric propulsion systems and spaceflight mechanics is provided. The investigation was performed by defining the spacecraft configurations, and then computing trajectories to a point of escape from Earth and from there to the final equilibrium point.</p><p>Our results show that in order to meet the propellant demands of the electric thrusters, the launch mass would need to increase by around 15-25 % compared to the solar sailing implementation, equating to around 10<sup>10</sup> kg. Nevertheless, electric propulsion could still be a beneficial choice since it would allow shorter transfer times for each shade which reduces the radiation exposure and subsequent degradation of the spacecraft’s systems. It was found that the transfer time with electric propulsion would be about one-half or one-fifth that of solar sailing, depending on spacecraft parameters. Additionally, electric propulsion allows a much lower initial parking orbit, and while this would increase the radiation exposure it would also reduce the launch costs due to the higher payload capacity to lower altitudes. However, electric propulsion of this scale require prior advancements in xenon or other inert propellant extraction methods and possibly a wide-scale construction of air separation plants.</p>
----------------------------------------------------------------------
In diva2:1583422 abstract is:
<p>Highly automated driving is approaching reality at a high speed. BMW is planningto put its first autonomous driving vehicle on the road already by 2021. The path torealising this new technology is however, full of challenges. Not only the transverseand longitudinal dynamic vehicle motion play an important role in experiencedcomfort but also the requirements and expectations of the occupants regarding thevertical dynamic vibration behaviour. Especially during long trips on the motorwaywhere the so far active driver becomes the chauffeured passenger, who reads, worksor sleeps in his newly gained time. These new use-cases create new requirements forthe future design of driving comfort which are yet to be fully discovered.This work was carried out at the BMW headquarters and had the aim to usedifferent machine learning models to investigate and identify patterns between thesubjective comfort values reported by participants in a study, on a comfort scale of 1-7 and the mechanical vibrations that they experienced, measured inm/s2. The datawas collected in a previous independent study and statistical methods were used toinsure the quality of the data. A comparison of the ISO 2631-1 comfort ratings andthe study’s findings is done to understand the need for a more sophisticated model to predict comfort in autonomous driving. The work continued by investigating different dimensionality reduction methods and their influence on the performance of the models. The process used to build, optimise and validate neural networks and other models is included in the method chapter and the results are presented. The work ends with a discussion of both the prediction results and the modelsre-usability. The machine learning models investigated in this thesis have shown great po-tential for detecting complex pattern that link feelings and thoughts to mechanical variables. The models were able to predict the correct level of comfort with up to50% precision when trying to predict 6 or 7 levels of comfort. When divided into high versus low discomfort, i.e. predicting one of two comfort levels, the models were able to achieve a precision of up to 75.4%.Excluded from this thesis is the study of differences in attentive vs inattentive state when being driven in an autonomous driving vehicle. It became clear shortly before the start of this work, that the experiment that yielded the data used for it failed to find a statistically significant difference between the two states.</p><p> </p>

corrected abstract:
<p>Highly automated driving is approaching reality at a high speed. BMW is planning to put its first autonomous driving vehicle on the road already by 2021. The path to realising this new technology is however, full of challenges. Not only the transverse and longitudinal dynamic vehicle motion play an important role in experienced comfort but also the requirements and expectations of the occupants regarding the vertical dynamic vibration behaviour. Especially during long trips on the motorway where the so far active driver becomes the chauffeured passenger, who reads, works or sleeps in his newly gained time. These new use-cases create new requirements for the future design of driving comfort which are yet to be fully discovered.</p><p>This work was carried out at the BMW headquarters and had the aim to use different machine learning models to investigate and identify patterns between the subjective comfort values reported by participants in a study, on a comfort scale of 1-7 and the mechanical vibrations that they experienced, measured in <em>m/s<sup>2</sup></em>. The data was collected in a previous independent study and statistical methods were used to insure the quality of the data. A comparison of the ISO 2631-1 comfort ratings and the study’s findings is done to understand the need for a more sophisticated model to predict comfort in autonomous driving. The work continued by investigating different dimensionality reduction methods and their influence on the performance of the models. The process used to build, optimise and validate neural networks and other models is included in the method chapter and the results are presented. The work ends with a discussion of both the prediction results and the models re-usability.</p><p>The machine learning models investigated in this thesis have shown great potential for detecting complex pattern that link feelings and thoughts to mechanical variables. The models were able to predict the correct level of comfort with up to 50% precision when trying to predict 6 or 7 levels of comfort. When divided into high versus low discomfort, i.e. predicting one of two comfort levels, the models were able to achieve a precision of up to 75.4%.</p><p>Excluded from this thesis is the study of differences in attentive vs inattentive state when being driven in a autonomous driving vehicle. It became clear shortly before the start of this work, that the experiment that yielded the data used for it failed to find a statistically significant difference between the two states.</p>
----------------------------------------------------------------------
In diva2:1579558 - Note: no full text in DiVA
abstract is:
<p>Ion channels are essential for numerous functions in the human body e.g.they control electrical signaling, absorb salts and regulate the vital osmoticgradient. They are embedded in cell membranes and the interaction thatoccurs between molecules of the membranes and the channel proteins playsa role in the regulation of the protein structure and function. KcsA is aprototypical K+-channel whose inactivation is suppressed when the bindingto anionic lipids is inhibited. However, the biophysical phenomenon that liesbehind this effect on channel inactivation is not yet discovered. Here, wehave trained several random forest classifiers trained with MD simulation datato distinguish between datasets with lipids bound to the protein and datasetwithout lipids bound. When KcsA is in the Open state, there is a significantlipid impact on the intracellular end of the TM2 helix. We show that thereis a potential favouring of the Fully Open state when the KcsA channel isbound to the anionic DOPG lipid. This was seen in all random forest modelset ups. Our results suggest that a potential acceleration of the opening of theKcsA activation gate, which allosterically controls the selectivity filter, mightexplain why the binding to anionic lipids is necessary for efficient inactivation.We believe that using machine learning together with further biophysical,biochemical analysis of MD simulations is a promising method for findingnew interaction patterns between proteins and other molecules or stimuli.</p>


corrected abstract:
<p>Ion channels are essential for numerous functions in the human body e.g. they control electrical signaling, absorb salts and regulate the vital osmotic gradient. They are embedded in cell membranes and the interaction that occurs between molecules of the membranes and the channel proteins plays a role in the regulation of the protein structure and function. KcsA is a prototypical K+-channel whose inactivation is suppressed when the binding to anionic lipids is inhibited. However, the biophysical phenomenon that lies behind this effect on channel inactivation is not yet discovered. Here, we have trained several random forest classifiers trained with MD simulation data to distinguish between datasets with lipids bound to the protein and dataset without lipids bound. When KcsA is in the Open state, there is a significant lipid impact on the intracellular end of the TM2 helix. We show that there is a potential favouring of the Fully Open state when the KcsA channel is bound to the anionic DOPG lipid. This was seen in all random forest model set ups. Our results suggest that a potential acceleration of the opening of the KcsA activation gate, which allosterically controls the selectivity filter, might explain why the binding to anionic lipids is necessary for efficient inactivation. We believe that using machine learning together with further biophysical, biochemical analysis of MD simulations is a promising method for finding new interaction patterns between proteins and other molecules or stimuli.</p>
----------------------------------------------------------------------
In diva2:1334283 abstract is:
<p>The development of chisels is currently mainly based on experiments and empirical researcheswithin the company. Recently it was decided to develop a simulation tool aiming at predictingthe performances and the capabilities of its chisels. The simulation of concrete during thechiselling process is based on a Cohesive Particle Model for concrete developed in partnershipwith external universities and implemented in the open-source software YADE using the DiscreteElement Method.The goal of this thesis is first to continue the development of the simulation tool by improvingthe calibration method of the material parameters in order to better describe the concretebehaviour under static loading. Afterward a validation phase, aiming at evaluating the real capabilitiesof the simulation tool to predict the demolition process, is performed by comparingsimulations and experiments results. The last objective is to define a simulation method in orderto evaluate the clamping phenomenon during chiselling in an acceptable amount of time.The new calibration algorithm has produced significant improvements in the determinationof the material parameters. Moreover, it was discovered that the ratio between the ultimatetensile and compressive strengths as well as the concrete brittleness are key parameters forthe material calibration accuracy. On the other hand, the validation phase was performed byevaluating the influence of seven parameters, such as the impact energy, the chisel length, onthe demolition process for both experiments and simulations. The procedure and the key resultsare presented in the thesis report. Concerning the clamping phenomenon, it was discovered thatthere is a relation between the pull-out force and the contact force during chiselling. This resultoffers a new and quick possibility for the evaluation of chisels sticking behaviour.</p>

corrected abstract:
<p>The development of chisels is currently mainly based on experiments and empirical researches within the company. Recently it was decided to develop a simulation tool aiming at predicting the performances and the capabilities of its chisels. The simulation of concrete during the chiselling process is based on a Cohesive Particle Model for concrete developed in partnership with external universities and implemented in the open-source software YADE using the Discrete Element Method. The goal of this thesis is first to continue the development of the simulation tool by improving the calibration method of the material parameters in order to better describe the concrete behaviour under static loading. Afterward a validation phase, aiming at evaluating the real capabilities of the simulation tool to predict the demolition process, is performed by comparing simulations and experiments results. The last objective is to define a simulation method in orderto evaluate the clamping phenomenon during chiselling in an acceptable amount of time. The new calibration algorithm has produced significant improvements in the determination of the material parameters. Moreover, it was discovered that the ratio between the ultimate tensile and compressive strengths as well as the concrete brittleness are key parameters for the material calibration accuracy. On the other hand, the validation phase was performed by evaluating the influence of seven parameters, such as the impact energy, the chisel length, onthe demolition process for both experiments and simulations. The procedure and the key results are presented in the thesis report. Concerning the clamping phenomenon, it was discovered that there is a relation between the pull-out force and the contact force during chiselling. This result offers a new and quick possibility for the evaluation of chisels sticking behaviour.</p>
----------------------------------------------------------------------
In diva2:1321182 abstract is:
<p>A method to evaluate technical solutions to handle external loads on subsea wellheadshas been developed. The solutions, or concepts, are compared with respect to load relief,cost and operation. As a basis, a Pugh matrix was used. It is well proven and commonlyused amongst engineers to evaluate concepts. However, it has some major cons due toits simplicity.Two more layers were added trying to solve or minimize these cons. This made up a totalof three layers.I. Evaluation - gather concept data, answer questions with valuesII. Transformation - transforms gathered values to a [1-5] scalingIII. Comparison - scaled values are presented in a Pugh matrixIn layer I, questions are to be answered by analyses and expert knowledge, carried outby developers.As for layer II, a value-scaling relationship should be set by developers and decisionmakers. They decide what is a good difference compared with a reference, and what isnot. The values for layer I can then be translated to layer III.Lastly, in layer III the performances of concepts with respect to different criteria are statedin a Pugh matrix. A scaling [1-5] is used for this. The decision maker decides what criteriaare most important by weighting them.Besides that, everything could be made automated. So when the method was carried outon two concepts, a winner could be decided immediately in layer III when the questionsin layer I had been answered.A simple and straightforward method to compare concepts have been done. Visualizingthe concept evaluation process and the connection between developers and decisionmakers. Making it easier for them to understand one another.The method can continuously be improved over time and might have the potential to makethe development process in many companies leaner.</p>

corrected abstract:
<p>A method to evaluate technical solutions to handle external loads on subsea wellheads has been developed. The solutions, or concepts, are compared with respect to load relief, cost and operation. As a basis, a Pugh matrix was used. It is well proven and commonly used amongst engineers to evaluate concepts. However, it has some major cons due to its simplicity.</p><p>Two more layers were added trying to solve or minimize these cons. This made up a total of three layers.</p><ol type="I"><li>Evaluation - gather concept data, answer questions with values</li><li>Transformation - transforms gathered values to a [1-5] scaling</li><li>Comparison - scaled values are presented in a Pugh matrix</li></ol><p>In layer I, questions are to be answered by analyses and expert knowledge, carried out by developers.</p><p>As for layer II, a value-scaling relationship should be set by developers and decision makers. They decide what is a good difference compared with a reference, and what is not. The values for layer I can then be translated to layer III. Lastly, in layer III the performances of concepts with respect to different criteria are stated in a Pugh matrix. A scaling [1-5] is used for this. The decision maker decides what criteria are most important by weighting them.</p><p>Besides that, everything could be made automated. So when the method was carried out on two concepts, a winner could be decided immediately in layer III when the questions in layer I had been answered.</p><p>A simple and straightforward method to compare concepts have been done. Visualizing the concept evaluation process and the connection between developers and decision makers. Making it easier for them to understand one another.</p><p>The method can continuously be improved over time and might have the potential to make the development process in many companies leaner.</p>
----------------------------------------------------------------------
In diva2:1142912 abstract is:
<p>The purpose of this report is to examine the combinationof an Extreme Learning Machine (ELM) with the KernelMethod. Kernels lies at the core of Support Vector Machines successin classifying non-linearly separable datasets. The hypothesisis that by combining ELM with a kernel we will utilize featuresin the ELM-space otherwise unused. The report is intended asa proof of concept for the idea of using kernel methods in anELM setting. This will be done by running the new algorithmagainst five image datasets for a classification accuracy and timecomplexity analysis.Results show that our extended ELM algorithm, which we havenamed Extreme Kernel Machine (EKM), improve classificationaccuracy for some datasets compared to the regularised ELM,in the best scenarios around three percentage points. We foundthat the choice of kernel type and parameter values had greateffect on the classification performance. The implementation ofthe kernel does however add computational complexity, but wherethat is not a concern EKM does have an advantage. This tradeoffmight give EKM a place between other neural networks andregular ELMs.</p>

corrected abstract:
<p>The purpose of this report is to examine the combination of an <em>Extreme Learning Machine</em> (ELM) with the <em>Kernel Method</em>. Kernels lies at the core of <em>Support Vector Machines</em> success in classifying non-linearly separable datasets. The hypothesis is that by combining ELM with a kernel we will utilize features in the ELM-space otherwise unused. The report is intended as a proof of concept for the idea of using kernel methods in an ELM setting. This will be done by running the new algorithm against five image datasets for a classification accuracy and time complexity analysis.</p><p>Results show that our extended ELM algorithm, which we have named <em>Extreme Kernel Machine</e> (EKM), improve classification accuracy for some datasets compared to the regularised ELM, in the best scenarios around three percentage points. We found that the choice of kernel type and parameter values had great effect on the classification performance. The implementation of the kernel does however add computational complexity, but where that is not a concern EKM does have an advantage. This tradeoff might give EKM a place between other neural networks and regular ELMs.</p>
----------------------------------------------------------------------
In diva2:1139499 
Note: There was soe text missing in the DiVA abstract.

abstract is:
<p>The entropy noise in modern engines is mainly originating from two types of mechanisms.First, chemical reactions in the combustion chamber lead to unsteady heat releasewhich is responsible of the direct combustion noise. Second, hot and cold blobsof air coming from the combustion chamber are advected and accelerated throughturbine stages, giving rise to the so-called entropy noise (or indirect combustionnoise). In the present work, numerical characterization of indirect combustion noiseof a Nozzle Guide Vane passage was assessed using three-dimensional Large EddySimulations. The study was conducted on a simplified topology of a real turbinestator passage, for which experimental data were available in transonic operatingconditions. First, a baseline case was reproduced to validate a numerical finite volumesolver against the experimental measurements. Then, the same solver is used toreproduce the effects of incoming entropy waves from the combustion chamber andto characterize the additional generated acoustic power. Periodic temperature fluctuationsare imposed at the inlet, permitting to simulate hot and cold packets of aircoming from the unsteady combustion. The incoming waves are characterized bytheir characteristic wavelength; therefore, a parametric study has been conductedvarying the inlet temperature of the passage, generating entropy waves of greaterwavelengths. The study proves that the generated indirect combustion noise canbe significant. Moreover, the generated indirect combustion noise increases as thewavelength of the incoming disturbances increases. Finally, the present work suggeststhat, in transonic conditions, there might be flow features which enhance theindirect combustion noise generation mechanism.</p>

corrected abstract:
<p>The entropy noise in modern engines is mainly originating from two types of mechanisms. First, chemical reactions in the combustion chamber lead to unsteady heat release which is responsible of the direct combustion noise. Second, hot and cold blobs of air coming from the combustion chamber are advected and accelerated through turbine stages, giving rise to the so-called entropy noise (or indirect combustion noise). In the present work, numerical characterization of indirect combustion noise of a Nozzle Guide Vane passage was assessed using three-dimensional Large Eddy Simulations. The study was conducted on a simplified topology of a real turbine stator passage, for which experimental data were available in transonic operating conditions. First, a baseline case was reproduced to validate a numerical finite volume solver against the experimental measurements. Then, the same solver is used to reproduce the effects of incoming entropy waves from the combustion chamber and to characterize the additional generated acoustic power. Periodic temperature fluctuations are imposed at the inlet, permitting to simulate hot and cold packets of air coming from the unsteady combustion. The incoming waves are characterized by their characteristic wavelength; therefore, a parametric study has been conducted varying the inlet temperature of the passage, generating entropy waves of greater wavelengths. The study proves that the generated indirect combustion noise can be significant for transonic operating conditions. Moreover, the generated indirect combustion noise increases as the wavelength of the incoming disturbances increases. Finally, the present work suggests that, in transonic conditions, there might be flow features which enhance the indirect combustion noise generation mechanism.</p>
----------------------------------------------------------------------
In diva2:623694 abstract is:
<p>Companiesissuing insurance cover, in return for insurance premiums, face the payments ofclaims occurring according to a loss distribution. Hence, capital must be heldby the companies so that they can guarantee the fulfilment of the claims ofeach line of insurance. The increased incidence of insurance insolvencymotivates the birth of new legislations as the European Solvency II Directive.Companies have to determine the required amount of capital and the optimalcapital allocation across the different lines of insurance in order to keep therisk of insolvency at an adequate level. The capital allocation problem may betreated in different ways, starting from the insurance company balance sheet.Here, the running process and efficiency of four methods are evaluated andcompared so as to point out the characteristics of each of the methods. TheValue-at-Risk technique is straightforward and can be easily generated for anyloss distribution. The insolvency put option principle is easily implementableand is sensitive to the degree of default. The capital asset pricing model isone of the oldest reliable methods and still provides very helpful intermediateresults. The Myers and Read marginal capital allocation approach encouragesdiversification and introduces the concept of default value. Applications ofthe four methods to some fictive and real insurance companies are provided. Thethesis further analyses the sensitivity of those methods to changes in the economiccontext and comments how insurance companies can anticipate those changes.</p>


corrected abstract:
<p>Companies issuing insurance cover, in return for insurance premiums, face the payments of claims occurring according to a loss distribution. Hence, capital must be held by the companies so that they can guarantee the fulfilment of the claims of each line of insurance. The increased incidence of insurance insolvency motivates the birth of new legislations as the European Solvency II Directive. Companies have to determine the required amount of capital and the optimal capital allocation across the different lines of insurance in order to keep the risk of insolvency at an adequate level. The capital allocation problem may be treated in different ways, starting from the insurance company balance sheet. Here, the running process and efficiency of four methods are evaluated and compared so as to point out the characteristics of each of the methods. The Value-at-Risk technique is straightforward and can be easily generated for any loss distribution. The insolvency put option principle is easily implementable and is sensitive to the degree of default. The capital asset pricing model is one of the oldest reliable methods and still provides very helpful intermediate results. The Myers and Read marginal capital allocation approach encourages diversification and introduces the concept of default value. Applications of the four methods to some fictive and real insurance companies are provided. The thesis further analyses the sensitivity of those methods to changes in the economic context and comments how insurance companies can anticipate those changes.</p>
----------------------------------------------------------------------
In diva2:618335 - missing spaces in title:
"Structural strength of work boatsand high speed crafts with floatingframes"
==>
"Structural strength of work boats and high speed crafts with floating frames"

abstract is:
<p>This thesis investigates the usage of floating frames in boats. A floating frame is a transverse frame fittedto the longitudinal stiffener flanges without contact with the shell plating, as opposed to the traditionalfixed frame which is welded to the shell plating with the stiffeners most commonly fitted through cut outsin the frame.To study the floating frame structure in a bigger perspective a finite element analysis is performed on amid ship compartment of an existing 60 m high speed catamaran ferry. The analysis is performed on amodel with scantlings as the original craft but with introduced floating frames. Stresses are analysed withrespect to maximum allowable stress as given in the DNV-rules for HAZ.High stresses are found in the bottom of the frames due to the reduced bending stiffness without effectiveflange from the shell plating. A large deformation in the shell plating relative the transverse frames isfound, creating high stresses in the stiffener webs. This deformation is induced by a large verticaldeformation of the frames.It is concluded that the transverse frames requires an increased stiffness to achieve acceptable stress levels.Possible solutions to increase stiffness are discussed, further studies are required to achieve an acceptablestructure.A design criterion for stiffeners in floating frame constructions is evaluated. The criterion considers theinteraction between a concentrated contact force and a bending moment with the purpose of simplifyingthe design process of stiffeners. The criterion is a combination of design methods from DNV HSLC andEurocode 9.The design criterion is found to give conservative results, although not unreasonably conservative. Thecriterion is suitable for the design of smaller work boats where the scantlings traditionally are not veryoptimized.</p>

corrected abstract:
<p>This thesis investigates the usage of floating frames in boats. A floating frame is a transverse frame fitted to the longitudinal stiffener flanges without contact with the shell plating, as opposed to the traditional fixed frame which is welded to the shell plating with the stiffeners most commonly fitted through cut outs in the frame.</p><p>To study the floating frame structure in a bigger perspective a finite element analysis is performed on a mid ship compartment of an existing 60 m high speed catamaran ferry. The analysis is performed on a model with scantlings as the original craft but with introduced floating frames. Stresses are analysed with respect to maximum allowable stress as given in the DNV-rules for HAZ.</p><p>High stresses are found in the bottom of the frames due to the reduced bending stiffness without effective flange from the shell plating. A large deformation in the shell plating relative the transverse frames is found, creating high stresses in the stiffener webs. This deformation is induced by a large vertical deformation of the frames.</p><p>It is concluded that the transverse frames requires an increased stiffness to achieve acceptable stress levels. Possible solutions to increase stiffness are discussed, further studies are required to achieve an acceptable structure.</p><p>A design criterion for stiffeners in floating frame constructions is evaluated. The criterion considers the interaction between a concentrated contact force and a bending moment with the purpose of simplifying the design process of stiffeners. The criterion is a combination of design methods from DNV HSLC and Eurocode 9.</p><p>The design criterion is found to give conservative results, although not unreasonably conservative. The criterion is suitable for the design of smaller work boats where the scantlings traditionally are not very optimized.</p>
----------------------------------------------------------------------
In diva2:618217 - title is missing spaces:
"Driver Assistance Systemswith focus onAutomatic Emergency Brake"
==>
"Driver Assistance Systems with focus on Automatic Emergency Brake"

abstract is:
<p>This thesis work aims at performing a survey of those technologies generally called DriverAssistance Systems (DAS). This thesis work focuses on gathering information in terms ofaccident statistics, sensors and functions and analyzing this information and shall thruaccessible information match functions with accidents, functions with sensors etc.This analysis, based on accidents in United States and Sweden during the period 1998 – 2002and two truck accident studies, shows that of all accidents with fatalities or sever injuriesinvolving a heavy truck almost half are the result of a frontal impact. About one fourth of theaccidents are caused by side impact, whereas single vehicle and rear impact collisions causesaround 14 % each. Of these, about one fourth is collision with unprotected (motorcycles,mopeds, bicycles, and pedestrians) whereas around 60 % are collision with other vehicles.More than 90 % of all accidents are partly the result of driver error and about 75 % aredirectly the result of driver error. Hence there exist a great opportunity to reduce the numberof accidents by introducing DAS.In this work, an analysis of DAS shows that six of the systems discussed today have thepotential to prevent 40 – 50 % of these accidents, whereas 20 – 40 % are estimated to actuallyhaving the chance to be prevented.One of these DAS, automatic emergency brake (AEB), has been analyzed in more detail.Decision models for an emergency brake capable to mitigate rear-end accidents has beendesigned and evaluated. The results show that this model has high capabilities to mitigatecollisions.</p>

corrected abstract:
<p>This thesis work aims at performing a survey of those technologies generally called <em>Driver Assistance Systems (DAS)</em>. This thesis work focuses on gathering information in terms of accident statistics, sensors and functions and analyzing this information and shall thru accessible information match functions with accidents, functions with sensors etc.</p><p>This analysis, based on accidents in United States and Sweden during the period 1998 – 2002 and two truck accident studies, shows that of all accidents with fatalities or sever injuries involving a heavy truck almost half are the result of a frontal impact. About one fourth of the accidents are caused by side impact, whereas single vehicle and rear impact collisions causes around 14 % each. Of these, about one fourth is collision with unprotected (motorcycles, mopeds, bicycles, and pedestrians) whereas around 60 % are collision with other vehicles.</p><p>More than 90 % of all accidents are partly the result of driver error and about 75 % are directly the result of driver error. Hence there exist a great opportunity to reduce the number of accidents by introducing <em>DAS</em>.</p><p>In this work, an analysis of <em>DAS</em> shows that six of the systems discussed today have the potential to prevent 40 – 50 % of these accidents, whereas 20 – 40 % are estimated to actually having the chance to be prevented.</p><p>One of these <em>DAS</em>, <em>automatic emergency brake (AEB)</em>, has been analyzed in more detail. Decision models for an emergency brake capable to mitigate rear-end accidents has been designed and evaluated. The results show that this model has high capabilities to mitigate collisions.</p>
----------------------------------------------------------------------
In diva2:503940 abstract is:
<p>The wheel corner module represents a new technology for controlling the motion of avehicle. It is based on a modular design around the geometric boundaries of a conventionalwheel. The typical WCM consists of a wheel containing an electrical in-wheel propulsion motor, a friction brake, a steering system and a suspension system. Generally, the braking,steering and suspension systems are controlled by means of electrical actuators. The WCMis designed to easily, by means of bolted connections and a power connector, attach toa vehicle platform constructed for the specic purpose. All functions are controlled viaan electrical system, connecting the steering column to the module. A WCM vehicle cancontain two or four wheel corner modules.The purpose of this thesis is to serve as an introduction to wheel corner module technology.The technology itself, as well as advantages and disadvantages related to wheelcorner modules are discussed. An analysis of a variety of wheel corner module concepts iscarried out. In addition, simulations are conducted in order to estimate how an increasedunsprung mass aects the ride comfort and handling performance of a vehicle.Longitudinal translation over two types of road disturbance proles, a curb and a bump,is simulated. A quarter car model as well as a full car model is utilized. The obtainedresults indicate that handling performance is deteriorated in connection to an increase dunsprung mass. The RMS value of the tire force uctuation increases with up to 18%,when 20 kg is added to each of the rear wheels of the full car model. Ride comfort is deteriorated or enhanced in connection to an increased unsprung mass, depending on the disturbance frequency of the road. When subjected to a road disturbance frequency below the eigenfrequency of the unsprung mass, ride comfort deterioration is indicated. The RMS vertical acceleration of the sprung mass increases with up to 6%, in terms of the full car model. When subjected to a road disturbance frequency above the eigenfrequency ofthe unsprung mass, decreased RMS vertical acceleration of up to 25% is noted, indicatinga signicantly enhanced ride comfort. Implementation of wheel corner module technology enables improved handling performance,safety and ride comfort compared to conventional vehicle technology. Further development, e.g. in terms of in-wheel motors and alternative power sources, is however required. In addition, major investments related to manufacturing equipment andtechnology is regarded as a signicant obstacle in terms of serial production.</p>

corrected abstract:
<p>The wheel corner module represents a new technology for controlling the motion of a vehicle. It is based on a modular design around the geometric boundaries of a conventional wheel. The typical WCM consists of a wheel containing an electrical in-wheel propulsion motor, a friction brake, a steering system and a suspension system. Generally, the braking, steering and suspension systems are controlled by means of electrical actuators. The WCM is designed to easily, by means of bolted connections and a power connector, attach to a vehicle platform constructed for the specific purpose. All functions are controlled via an electrical system, connecting the steering column to the module. A WCM vehicle can contain two or four wheel corner modules.</p><p>The purpose of this thesis is to serve as an introduction to wheel corner module technology. The technology itself, as well as advantages and disadvantages related to wheel corner modules are discussed. An analysis of a variety of wheel corner module concepts is carried out. In addition, simulations are conducted in order to estimate how an increased unsprung mass affects the ride comfort and handling performance of a vehicle.</p><p>Longitudinal translation over two types of road disturbance profiles, a curb and a bump, is simulated. A quarter car model as well as a full car model is utilized. The obtained results indicate that handling performance is deteriorated in connection to an increased unsprung mass. The RMS value of the tire force fluctuation increases with up to 18%, when 20 kg is added to each of the rear wheels of the full car model. Ride comfort is deteriorated or enhanced in connection to an increased unsprung mass, depending on the disturbance frequency of the road. When subjected to a road disturbance frequency below the eigenfrequency of the unsprung mass, ride comfort deterioration is indicated. The RMS vertical acceleration of the sprung mass increases with up to 6%, in terms of the full car model. When subjected to a road disturbance frequency above the eigenfrequency of the unsprung mass, decreased RMS vertical acceleration of up to 25% is noted, indicating a significantly enhanced ride comfort.</p><p>Implementation of wheel corner module technology enables improved handling performance, safety and ride comfort compared to conventional vehicle technology. Further development, e.g. in terms of in-wheel motors and alternative power sources, is however required. In addition, major investments related to manufacturing equipment and technology is regarded as a significant obstacle in terms of serial production.</p>
----------------------------------------------------------------------
In diva2:411684 abstract is:
<p>The aim of this master thesis is to characterize the fluid forces applied to a fuel assembly inthe core of a nuclear power plant in case of seism. The forces are studied with a simplifiedtwo-dimensional model constituted of an array of 3 by 3 infinite cylinders oscillating in aclosed box. The axial flow of water, which convects the heat in the core of a nuclear powerplant, is also taken into account. The velocity of the axial flow reaches 4m/s in the middle ofthe assembly and modifies the forces features when the cylinders move laterally.The seism is modeled as a lateral displacement with high amplitude (several cylinderdiameters) and low frequencies (below 20 Hz). In order to study the effects of the amplitudeand of the frequency of the displacement, the displacement taken is a sine function withboth controlled amplitude and frequency. Four degrees of freedom of the system will bestudied: the amplitude of the displacement, its frequency, the axial velocity amplitude andthe confinement (due to the closed box).The fluid forces exerted on the cylinders can be seen as a combination of three terms: anadded mass, related to the acceleration of cylinders, a drift force, related to the damping ofthe fluid and a force due to the interaction of the cylinder with residual vortices. The firsttwo components will be characterized through the Morison expansion, and their evolutionwith the variation of the degree of freedom of the system will be quantified. The effect ofthe interaction with the residual vortices will be observed in the plots of the forces vs. timebut also in the velocity and vorticity map of the fluid.The fluid forces are calculated with the CFD code Code_Saturne, which uses a second orderaccurate finite volume method. Unsteady Reynolds Averaged Navier Stokes simulations arerealized with a k-epsilon turbulence model. The Arbitrary Lagrange Euler model is used todescribe the structure displacement. The domain is meshed with hexahedra with thesoftware gmsh [1] and the flow is visualized with Paraview [2]. The modeling techniquesused for the simulations are described in the first part of this master thesis.</p>

corrected abstract:
<p>The aim of this master thesis is to characterize the fluid forces applied to a fuel assembly in the core of a nuclear power plant in case of seism. The forces are studied with a simplified two-dimensional model constituted of an array of 3 by 3 infinite cylinders oscillating in a closed box. The axial flow of water, which convects the heat in the core of a nuclear power plant, is also taken into account. The velocity of the axial flow reaches 4m/s in the middle of the assembly and modifies the forces features when the cylinders move laterally.</p><p>The seism is modeled as a lateral displacement with high amplitude (several cylinder diameters) and low frequencies (below 20 Hz). In order to study the effects of the amplitude and of the frequency of the displacement, the displacement taken is a sine function with both controlled amplitude and frequency. Four degrees of freedom of the system will be studied: the amplitude of the displacement, its frequency, the axial velocity amplitude and the confinement (due to the closed box).</p><p>The fluid forces exerted on the cylinders can be seen as a combination of three terms: an added mass, related to the acceleration of cylinders, a drift force, related to the damping of the fluid and a force due to the interaction of the cylinder with residual vortices. The first two components will be characterized through the Morison expansion, and their evolution with the variation of the degree of freedom of the system will be quantified. The effect of the interaction with the residual vortices will be observed in the plots of the forces vs. time but also in the velocity and vorticity map of the fluid.</p><p>The fluid forces are calculated with the CFD code Code_Saturne, which uses a second order accurate finite volume method. Unsteady Reynolds Averaged Navier Stokes simulations are realized with a k-epsilon turbulence model. The Arbitrary Lagrange Euler model is used to describe the structure displacement. The domain is meshed with hexahedra with the software gmsh [1] and the flow is visualized with Paraview [2]. The modeling techniques used for the simulations are described in the first part of this master thesis.</p>
----------------------------------------------------------------------
In diva2:1808437 - Note: The French and ENglish abstracts are at the end of the PDF file. The French abstracts is not in DiVA.


abstract is:
<p>The PTR system allows the EPR2 fuel pool to be cooled. The evacuation of the residual power fromthe pool is ensured by several heat exchangers and pumps, which have to be dimensioned in order to meetdifferent requirements.In order to dimension them, the worst-case scenario of the components must first be determined.Sensitivity to external conditions and efficiency studies enable to propose a heat exchanger design tomeet the requirements. A parametric study then allows to study more precisely the influence of thegeometry of the exchanger on the heat transfer. This allows to guide the conception of a CFD study ofthe design on the Comsol software in order to validate it. The proposed design can then be integratedinto the PTR cooling train. The train is modeled with FloMaster, in order to compute the head losses inthe hydraulic system and to propose a pump altimetry preventing cavitation.The dimensioning case of the exchangers corresponds to the operating case of the PTR trains duringunit shutdown, while the scenario that facilitates cavitation corresponds to the boiling of the fuel pool.The temperature of the cold source RRI is a sensitive data for the operation of the exchangers. In addition,the placement of the baffles and the space between the tubes play a determining role in the heat removal.It was difficult to construct the desired exchanger geometry in CFD. A compromise model was thusidentified and studied in CFD. The FloMaster study showed that the pressure drop in the PTR network isabout 15.5 mCE at the considered flow rate. Cavitation in a main train is not a problem if the pumps arelowered by at least 1.8 meters from the pool suction point.The sizing study therefore allowed us to propose a heat exchanger design close to the specifications,but this could not be precisely studied in CFD. The pressure drop study allowed to propose a pumpaltimetry preventing cavitation.</p>

corrected abstract:
<p>The PTR system allows the EPR2 fuel pool to be cooled. The evacuation of the residual power from the pool is ensured by several heat exchangers and pumps, which have to be dimensioned in order to meet different requirements.</p><p>In order to dimension them, the worst-case scenario of the components must first be determined. Sensitivity to external conditions and efficiency studies enable to propose a heat exchanger design to meet the requirements. A parametric study then allows to study more precisely the influence of the geometry of the exchanger on the heat transfer. This allows to guide the conception of a CFD study of the design on the Comsol software in order to validate it. The proposed design can then be integrated into the PTR cooling train. The train is modeled with FloMaster, in order to compute the head losses in the hydraulic system and to propose a pump altimetry preventing cavitation.</p><p>The dimensioning case of the exchangers corresponds to the operating case of the PTR trains during unit shutdown, while the scenario that facilitates cavitation corresponds to the boiling of the fuel pool. The temperature of the cold source RRI is a sensitive data for the operation of the exchangers. In addition, the placement of the baffles and the space between the tubes play a determining role in the heat removal. It was difficult to construct the desired exchanger geometry in CFD. A compromise model was thus identified and studied in CFD. The FloMaster study showed that the pressure drop in the PTR network is about 15.5 mCE at the considered flow rate. Cavitation in a main train is not a problem if the pumps are lowered by at least 1.8 meters from the pool suction point.</p><p>The sizing study therefore allowed us to propose a heat exchanger design close to the specifications, but this could not be precisely studied in CFD. The pressure drop study allowed to propose a pump altimetry preventing cavitation.</p>

French abstract:

<p>Le système PTR permet le refroidissement de la piscine combustible de l’EPR2. L’évacuation de la puissance résiduelle de la piscine est assurée par plusieurs échangeurs de chaleur et pompes, qui doivent être dimensionnés afin de respecter plusieurs critères.</p><p>Pour les dimensionner, les conditions de fonctionnement les plus défavorable des composants doivent d’abord être déterminées. Des études de sensibilité au conditions extérieures, ainsi que d’efficacité permettent de proposer un type d’échangeur pour répondre aux exigences. Une étude paramétrique permet ensuite d’étudier plus précisément l’influence de la géométrie de l’échangeur sur le transfert de chaleur. Celle-ci permet d’orienter la conception d’une étude CFD du design sur le logiciel Comsol permettant de le valider. Le design proposé peut être intégré au train de refroidissement PTR. Le train est modélisé sous FloMaster, afin de calculer les pertes de charges du réseau et proposer une altimétrie de pompe prévenant la cavitation.</p><p>Le cas dimensionnant des échangeurs correspond au cas de fonctionnement des trains PTR en arrêt de tranche, tandis que celui favorisant la cavitation correspond au scenario d’ébullition de la piscine combustible. La température de la source froide RRI est une donnée sensible pour le fonctionnement des échangeurs. De plus, le positionnement de chicanes et l’espace entre les tubes jouent un rôle déterminant dans l’évacuation de la chaleur. Il a été difficile de construire la géométrie d’échangeur souhaitée en CFD. Un modèle compromis a été trouvé et étudié en CFD. L’étude <em>FloMaster</em> a montré que les pertes de charge dans le réseau PTR sont de l’ordre de 15.5 mCE au débit considéré. La cavitation dans un train principal n’est pas un problème en abaissant de 2 mètres minimum les pompes par rapport au point d’aspiration piscine.</p><p>L’étude de dimensionnement a donc permis de proposer un design d’échangeur de chaleur se rapprochant du cahier des charges, mais celui-ci n’a pas pu être précisément étudié en CFD. L’étude de perte de charge a permis de proposer une altimétrie de pompe prévenant la cavitation.</p>
----------------------------------------------------------------------
In diva2:1795177 abstract is:
<p>Since the global financial crisis of 2008, regulatory bodies worldwide have implementedincreasingly stringent requirements for measuring and pricing default risk in financialderivatives. Counterparty Credit Risk (CCR) serves as the measure for default risk infinancial derivatives, and Credit Valuation Adjustment (CVA) is the pricing method used toincorporate this default risk into derivatives prices. To calculate the CVA, one needs the risk-neutral Probability of Default (PD) for the counterparty, which is the centre in this type ofderivative.The traditional method for calculating risk-neutral probabilities of default involves constructingcredit curves, calibrated using the credit derivative Credit Default Swap (CDS). However,liquidity issues in CDS trading present a major challenge, as the majority of counterpartieslack liquid CDS spreads. This poses the difficult question of how to model risk-neutral PDwithout liquid CDS spreads.The current method for generating proxy credit curves, introduced by the Japanese BankNomura in 2013, involves a cross-sectional linear regression model. Although this model issufficient in most cases, it often generates credit curves unsuitable for larger counterpartiesin more volatile times. In this thesis, we introduce two Long Short-Term Memory (LSTM)models trained on similar entities, which use CDS spreads as input. Our introduced modelsshow some improvement in generating proxy credit curves compared to the Nomura model,especially during times of higher volatility. While the result were more in line with the tradedCDS-market, there remains room for improvement in the model structure by using a moreextensive dataset.</p>

corrected abstract:
<p>Since the global financial crisis of 2008, regulatory bodies worldwide have implemented increasingly stringent requirements for measuring and pricing default risk in financial derivatives. Counterparty Credit Risk (CCR) serves as the measure for default risk in financial derivatives, and Credit Valuation Adjustment (CVA) is the pricing method used to incorporate this default risk into derivatives prices. To calculate the CVA, one needs the risk-neutral Probability of Default (PD) for the counterparty, which is the centre in this type of derivative.</p><p>The traditional method for calculating risk-neutral probabilities of default involves constructing credit curves, calibrated using the credit derivative Credit Default Swap (CDS). However, liquidity issues in CDS trading present a major challenge, as the majority of counterparties lack liquid CDS spreads. This poses the difficult question of how to model risk-neutral PD without liquid CDS spreads.</p><p>The current method for generating proxy credit curves, introduced by the Japanese bank Nomura in 2013, involves a cross-sectional linear regression model. Although this model is sufficient in most cases, it often generates credit curves unsuitable for larger counterparties in more volatile times.</p><p>In this thesis, we introduce two Long Short-Term Memory (LSTM) models trained on similar entities, which use CDS spreads as input. Our introduced models show some improvement in generating proxy credit curves compared to the Nomura model, especially during times of higher volatility. While the result were more in line with the traded CDS-market, there remains room for improvement in the model structure by using a more extensive dataset.</p>
----------------------------------------------------------------------
In diva2:1781520 - missing space in title:
"Extraction and optimization for modeling ofdesalination by capacitive deionization"
==>
"Extraction and optimization for modeling of desalination by capacitive deionization"

Note there were many wording differences between the abstract in the thesis and the abstract in DiVA. It seems that someone tried to clean up the grammer.

abstract is:
<p>Water scarcity is set to become a big challenge in the 21st century and more efficient desalinationtechnologies will be needed in the future. In this project, one desalination method called capacitivedeionization (CDI) is explored and we used a model called the ELC model to simulate CDI withComsol. The goal of this project focuses on evaluating the performance of CDI and how changingdifferent operational parameters of the process affects other aspects of desalination. Some examplesare power consumption, desalination rate and water usage. With the gathered information, the process of CDI can be optimized in some way. Even though our project simulates a specific model ofCDI, the hope is to have come to general conclusions regarding CDI so that the results can be usedfor other models. If the correlations between parameters are known, it will be easier to calibrate anysetup of CDI. The gathered data is exported, stored, processed, and plotted using Matlab functionsintegrated with Comsol. The results consist of two sets, the first for constant voltage and the secondfor constant current. Both have results on how desalination rate and energy efficiency are related toparameters such as internal voltage intervals controlling how long the desalination cycle is running,external voltage, and inflow salt concentration in the water. The key conclusions drawn are as thefollowing for constant voltage. High external voltages are effective in increasing both desalinationrate and energy efficiency but will degrade the CDI electrodes. The internal voltage span should bepretty long with high max internal voltage and the minimum internal voltage the same as the external voltage. The energy efficiency increase with lower salt concentrations in the inflow water up toa point. The best setup for the desalination rate is at quite a high maximum internal voltage withvaried low minimum internal voltage. For constant current, low current is generally efficient, whilethe maximum external voltage depends on the current. Avoid a high current with a low externalvoltage. By relating all these parameters, we get more insights into what an energy-efficient and fastadsorbing CDI setup looks like.</p>


corrected abstract:
<p>Water scarcity is set to become a significant challenge in the 21st century and more efficient desalination technologies will be needed in the future. In this project, a desalination method called capacitive deionization (CDI) is explored and we used the ELC model to simulate CDI with Comsol. The goal of this project focuses on evaluating the performance of CDI and how changing different operational parameters of the process affects other aspects of desalination. Some examples are power consumption, desalination rate and water usage. With the gathered information, the process of CDI can be optimized in some way. Even though our project simulates a specific model of CDI, the hope is to have come to general conclusions regarding CDI so that the results can be used for other models. If the correlations between parameters are known, it will be easier to calibrate any setup of CDI. The gathered data is exported, stored, processed, and plotted using Matlab functions integrated with Comsol. The results consist of two sets, the first for constant voltage and the second for constant current. Both show how desalination rate and energy efficiency are related to parameters such as internal voltage intervals controlling how big the desalination cycle is running, external voltage, and salinity of inlet water. The key conclusions drawn are as the following for constant voltage. High external voltages effectively increase both desalination rate and energy efficiency but will degrade the CDI electrodes. The internal voltage span should be as big as possible with high max internal voltage and a minimum internal voltage the same as the external voltage. The energy efficiency increase with lower salt concentrations in the inflow water up to a point. The best setup for the desalination rate is at a high maximum internal voltage with varied low minimum internal voltage. For constant current, low current is generally efficient, while the maximum external voltage depends on the current. Avoid a high current with a low external voltage. By relating all these parameters, we get more insights into what an energy-efficient and fast-adsorbing CDI configuration should be set up.</p>
----------------------------------------------------------------------
In diva2:1781504 abstract is:
<p>Computed x-ray tomography is one of the most common medical imaging modalities andas such ways of improving the images are of high relevance. Applying deep learningmethods to denoise CT images has been of particular interest in recent years. In thisstudy, rather than using traditional denoising metrics such as MSE or PSNR for evaluation, we use a radiomic approach combined with 3D printed phantoms as a "groundtruth" to compare with. Our approach of having a ground truth ensures that we withabsolute certainty can say what a scanned tumor is supposed to look like and compareour results to a true value. This performance metric is better suited for evaluation thanMSE since we want to maintain structures and edges in tumors and MSE-evaluationrewards over-smoothing.</p><p>Here we apply U-Net networks to images of 3D printed tumors. The 4 tumors and alung phantom were printed with PLA filament and 80% fill rate with a gyroidal patternto mimic soft tissue in a CT-scan while maintaining isotropicity. CT images of the 3Dprinted phantom and tumors were taken with a GE revolution DE scanner at KarolinskaUniversity Hospital. The networks were trained on the 2016 NIH-AAPM-Mayo ClinicLow Dose CT Grand Challenge dataset, mapping Low Dose CT images to Normal DoseCT images using three different loss functions, l1, vgg16, and vgg16_l1.</p><p>Evaluating the networks on RadiomicsShape features from SlicerRadiomics® we findcompetitive performance with TrueFidelityTM Deep Learning Image Reconstruction (DLIR)by GE HealthCareTM. With one of our networks (UNet_alt, vgg16_l1 loss function with32 features, and batch size 16 in training.) outperforming TrueFidelity in 63% of caseswhen evaluated by counting if a radiomic feature has a lower relative error comparedto ground truth after our own denoising for four different kind of tumors. The samenetwork outperformed FBP in 84% of cases which in combination with the majority ofour networks performing substantially better against FBP than TrueFidelity shows theviability of DLIR compared to older methods such as FBP.</p><p> </p>

corrected abstract:
<p>Computed x-ray tomography is one of the most common medical imaging modalities and as such ways of improving the images are of high relevance. Applying deep learning methods to denoise CT images has been of particular interest in recent years. In this study, rather than using traditional denoising metrics such as MSE or PSNR for evaluation, we use a radiomic approach combined with 3D printed phantoms as a "ground truth" to compare with. Our approach of having a ground truth ensures that we with absolute certainty can say what a scanned tumor is supposed to look like and compare our results to a true value. This performance metric is better suited for evaluation than MSE since we want to maintain structures and edges in tumors and MSE-evaluation rewards over-smoothing.</p><p>Here we apply U-Net networks to images of 3D printed tumors. The 4 tumors and a lung phantom were printed with PLA filament and 80% fill rate with a gyroidal pattern to mimic soft tissue in a CT-scan while maintaining isotropicity. CT images of the 3D printed phantom and tumors were taken with a GE revolution DE scanner at Karolinska University Hospital. The networks were trained on the 2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge dataset, mapping Low Dose CT images to Normal Dose CT images using three different loss functions, l1, vgg16, and vgg16_l1.</p><p>Evaluating the networks on RadiomicsShape features from SlicerRadiomics® we find competitive performance with TrueFidelity™ Deep Learning Image Reconstruction (DLIR) by GE HealthCare™. With one of our networks (UNet_alt, vgg16_l1 loss function with 32 features, and batch size 16 in training.) outperforming TrueFidelity in 63% of cases when evaluated by counting if a radiomic feature has a lower relative error compared to ground truth after our own denoising for four different kind of tumors. The same network outperformed FBP in 84% of cases which in combination with the majority of our networks performing substantially better against FBP than TrueFidelity shows the viability of DLIR compared to older methods such as FBP.</p>
----------------------------------------------------------------------
In diva2:1698135 abstract is:
<p>Wear condition modelling is a research topic which lately has increased in popularitydue to its significance to the railway industry. Large sums are spent on maintenanceof the network every year, which has generated a demand for understanding and predictingwear mechanisms in a multitude of components within both railway vehiclesand infrastructure. To acquire this knowledge, using a simulation software to predictwear and maintenance needs, is a cheap and dependable option. This thesis sets outto create a wear modelling software for the catenary-pantograph interaction, which intheory could enlighten railway operators of the condition of their infrastructure.</p><p>This research contains an extensive literature review on the subject of wear modelling,analysing different strategies and theories of the wear mechanism and desired modelproperties. Thereafter, an attempt to recreate a model for a representative Swedishcase was conducted; implementing parameters of a typical Swedish rail vehicle to awear rate formula, recreating a passage of a pantograph sliding along the catenary.The output was a wear rate as a function of time, a determination of life expectancyfor the infrastructure and a worn profile of the catenary wire after one year of operation.</p><p>The model, named AWEAR, was deemed functional for determining wear mechanismtendencies and behaviour for a few cases of alternating parameters. However, sinceno validation could be performed due to a lack of resources, the validity of the outputvalues could not be confirmed - thus leaving calibration of the model to future work.In conclusion, AWEAR needs to be calibrated for future research but does contain amultitude of enhancement opportunities proposed in this thesis. While not completelyfunctional, the software was deemed a useful foundation for future projects and mightresult in a product that aids operators maintaining their infrastructure.Keywords: Wear modelling, catenary pantograph interaction, mechanism,</p>

corrected abstract:
<p>Wear condition modelling is a research topic which lately has increased in popularity due to its significance to the railway industry. Large sums are spent on maintenance of the network every year, which has generated a demand for understanding and predicting wear mechanisms in a multitude of components within both railway vehicles and infrastructure. To acquire this knowledge, using a simulation software to predict wear and maintenance needs, is a cheap and dependable option. This thesis sets out to create a wear modelling software for the catenary-pantograph interaction, which in theory could enlighten railway operators of the condition of their infrastructure.</p><p>This research contains an extensive literature review on the subject of wear modelling, analysing different strategies and theories of the wear mechanism and desired model properties. Thereafter, an attempt to recreate a model for a representative Swedish case was conducted; implementing parameters of a typical Swedish rail vehicle to a wear rate formula, recreating a passage of a pantograph sliding along the catenary. The output was a wear rate as a function of time, a determination of life expectancy for the infrastructure and a worn profile of the catenary wire after one year of operation.</p><p>The model, named AWEAR, was deemed functional for determining wear mechanism tendencies and behaviour for a few cases of alternating parameters. However, since no validation could be performed due to a lack of resources, the validity of the output values could not be confirmed - thus leaving calibration of the model to future work.  In conclusion, AWEAR needs to be calibrated for future research but does contain a multitude of enhancement opportunities proposed in this thesis. While not completely functional, the software was deemed a useful foundation for future projects and might result in a product that aids operators maintaining their infrastructure.</p>
----------------------------------------------------------------------
In diva2:1595652 abstract is:
<p>A comparative study is carried out to investigate the most promising route towardsthe lightweight construction of a retractable mast for a sailing cargo vessel.Four design families are developed and compared. The primary criteria forjudgment are the structural mass, strength, and stiffness in relation to a providedbenchmark design. Additional evaluation criteria are the capital costsfor raw materials and manufacturing.The design space includes isotropic materials as well as fiber-reinforced polymer(FRP) solutions and is navigated by employing analytical evaluation methodssupported by finite element analysis (FEA). Restrictions to the designspace are given by a general arrangement of the benchmark design. This includesthe limitation to the ULS loads and the overall mast geometry.A review of relevant Det Norske Veritas (DNV) rules for classification is performedand the guidelines for wind turbine blades and wind-powered units(WPU) are judged most suitable to the design challenge. Relevant design principlesare implemented in the structural analysis.It is concluded that pure metal constructions imply an unreasonably large weightpenalty. Local buckling is found to disqualify FRP single-skin solutions as successfulcandidates. Secondary to that, strength concerns are the major driversfor the structural mass.The report presents two designs that are judged fit for the purpose, one is ahybrid truss structure from high strength low alloy steel (HSLA steel) and carbonfiber-reinforced polymer (CFRP). The second design is a sandwich constructionwith CFRP face sheets, a PVC foam core, and additional stiffeningmembers in steel.</p>

corrected abstract:
<p>A comparative study is carried out to investigate the most promising route towards lightweight construction of a retractable mast for a sailing cargo vessel. Four design families are developed and compared. The primary criteria for judgement are the structural mass, strength and stiffness in relation to a provided benchmark design. Additional evaluation criteria are the capital costs for raw materials and manufacturing.</p><p>The design space includes isotropic materials as well as fiber reinforced polymer (FRP) solutions and is navigated by employing analytical evaluation methods supported by finite element analysis (FEA). Restrictions to the design space are given by a general arrangement of the benchmark design. This includes the limitation to the ULS loads and the overall mast geometry.</p><p>A review of relevant Det Norske Veritas (DNV) rules for classification is performed and the guidelines for wind turbine blades and wind powered units (WPU) are judged most suitable to the design challenge. Relevant design principles are implemented in the structural analysis.</p><p>It is concluded that pure metal constructions imply an unreasonably large weight penalty. Local buckling is found to disqualify FRP single skin solutions as successful candidates. Secondary to that, strength concerns are the major drivers for the structural mass.</p><p>The report presents two designs that are judged fit for the purpose, one is a hybrid truss structure from high strength low alloy steel (HSLA steel) and carbon fiber reinforced polymer (CFRP). The second design is a sandwich construction with CFRP face sheets, a PVC foam core and additional stiffening members in steel.</p>
----------------------------------------------------------------------
In diva2:1527800 abstract is:
<p>With the advent of hybrids and electric vehicles, the need for lightweight and highperformancematerials is growing. Sheet molding compound (SMC) is a compositemade of short and randomized  bers that o ers a substantial weight reduction andgood mechanical properties while meeting the demand for large volume production.This thesis aims to develop a constitutive FE model of the SMC used in the bodyin black of an autonomous vehicle.To extract its properties, several physical tests were performed on specimens madeof the above-mentioned material. Both the tensile and three point bending testsresults show that the material is not homogeneous and that its properties vary fordi erent directions. The damping ratio extracted from the vibration test is muchlower than in conventional structural materials like aluminum and steel.In the FE analysis, the material was modeled both as isotropic and orthotropic.After adjusting the Young's modulus, the isotropic model shows accurate resultsuntil 1200 Hz. On the other hand, without knowing in which directions the propertiesoccur, the orthotropic model is very limited.In conclusion, even though the properties were tailored speci cally for the specimen,the model might not correctly represent the material's behavior, being itsproperties not the same for di erent components. Therefore, it is more reasonableto use average data instead.</p>

corrected abstract:
<p>With the advent of hybrids and electric vehicles, the need for lightweight and high performance materials is growing. Sheet molding compound (SMC) is a composite made of short and randomized fibers that offers a substantial weight reduction and good mechanical properties while meeting the demand for large volume production. This thesis aims to develop a constitutive FE model of the SMC used in the body in black of an autonomous vehicle.</p><p>To extract its properties, several physical tests were performed on specimens made of the above-mentioned material. Both the tensile and three point bending tests results show that the material is not homogeneous and that its properties vary for different directions. The damping ratio extracted from the vibration test is much lower than in conventional structural materials like aluminum and steel.</p><p>In the FE analysis, the material was modeled both as isotropic and orthotropic. After adjusting the Young's modulus, the isotropic model shows accurate results until 1200 Hz. On the other hand, without knowing in which directions the properties occur, the orthotropic model is very limited.</p><p>In conclusion, even though the properties were tailored specifically for the specimen, the model might not correctly represent the material's behavior, being its properties not the same for different components. Therefore, it is more reasonable to use average data instead.</p>
----------------------------------------------------------------------
In diva2:1285510 - missing space and hyphen in title:
"Method for rebuilding gaspoweredtrucks"
==>
"Method for rebuilding gas-powered trucks"

abstract is:
<p>This master thesis investigates the possibility to rebuild heavy-duty trucks poweredby liquefied natural gas (LNG) to other fuel options such as compressednatural gas (CNG), diesel or ethanol. The background is that the second handvalue for a LNG truck is lower than a similar diesel truck due to an undevelopedinfrastructure with few refuelling stations. This results in a small market for secondhand LNG trucks.Both tractor and rigid trucks are evaluated from a technical perspective to determinewhich components that need to be changed when switching from LNG toanother type of fuel. When that is completed each fuel alternative is evaluatedbased on cost and market interest. Also certification and other legislations areinvestigated to determine if they will affect the rebuilding process.The result shows that the rebuild faces different technical complexity dependingon the target fuel alternative. It is concluded that a rebuild to diesel or ethanolis expensive due to many changes needed for the engine and aftertreatment andtherefore these alternatives are not a good choice for a rebuild. A rebuild to CNGis still expensive but can be of interest for rigid trucks, but not for tractor truckssince they usually have a demand for longer range. In order to get the final cost ofthe rebuild to CNG a commercial assessment has to be made and the rebuild willdepend on in which country the rebuild is performed due to different legislationsfor re-registration which may be a an obstacle.</p>

corrected abstract:
<p>This master thesis investigates the possibility to rebuild heavy-duty trucks powered by liquefied natural gas (LNG) to other fuel options such as compressed natural gas (CNG), diesel or ethanol. The background is that the second hand value for a LNG truck is lower than a similar diesel truck due to an undeveloped infrastructure with few refuelling stations. This results in a small market for second hand LNG trucks.</p><p>Both tractor and rigid trucks are evaluated from a technical perspective to determine which components that need to be changed when switching from LNG to another type of fuel. When that is completed each fuel alternative is evaluated based on cost and market interest. Also certification and other legislations are investigated to determine if they will affect the rebuilding process.</p><p>The result shows that the rebuild faces different technical complexity depending on the target fuel alternative. It is concluded that a rebuild to diesel or ethanol is expensive due to many changes needed for the engine and aftertreatment and therefore these alternatives are not a good choice for a rebuild. A rebuild to CNG is still expensive but can be of interest for rigid trucks, but not for tractor trucks since they usually have a demand for longer range. In order to get the final cost of the rebuild to CNG a commercial assessment has to be made and the rebuild will depend on in which country the rebuild is performed due to different legislations for re-registration which may be a an obstacle.</p>
----------------------------------------------------------------------
In diva2:1184068

abstract is:
<p>In this project, the design of a transverse leaf spring for an automotive vehicle isinvestigated. A transverse leaf spring is a concept for implementing the traditionalcoil spring for the vehicle, into a spring operating through beam bending. There aredifferent constructions and layouts of said leaf spring developed previously. Onesolution is where the spring is spanning from one side to the other of the vehicle,making it a transverse leaf spring. This solution has an extra gain; it is also providingan anti-roll bar action to the ride characteristics of the vehicle.The design of the transverse leaf spring is made for an automotive research vehicle atRoyal Institute of Technology (KTH). This vehicle is designed to represent a smallcity vehicle, weighing approximately 600 𝑘𝑔. The design of the original suspensionsystem is of the type Double Wishbone with push rod and coil springs with damper.The system is modular and exactly the same for the front and rear of the vehicle.Original mounting positions on the vehicle are to be kept intact. The design of thetransverse leaf spring is made in order to mimic the exact characteristics of theoriginal suspension system.First analytical optimizations are made in order to find an initial solution. This designis then implemented in FEM-software in order to further investigate thecharacteristics and design. A final design is found that is fulfilling the requirementsand a full scale version of the transverse leaf spring is built and examined withregards to its fulfilment of requirements.</p>

corrected abstract:
<p>In this project, the design of a transverse leaf spring for an automotive vehicle is investigated. A transverse leaf spring is a concept for implementing the traditional coil spring for the vehicle, into a spring operating through beam bending. There are different constructions and layouts of said leaf spring developed previously. One solution is where the spring is spanning from one side to the other of the vehicle, making it a transverse leaf spring. This solution has an extra gain; it is also providing an anti-roll bar action to the ride characteristics of the vehicle.</p><p>The design of the transverse leaf spring is made for an automotive research vehicle at <em>Royal Institute of Technology</em> (KTH). This vehicle is designed to represent a small city vehicle, weighing approximately 600 <em>kg</em>. The design of the original suspension system is of the type <em>Double Wishbone</em> with push rod and coil springs with damper. The system is modular and exactly the same for the front and rear of the vehicle. Original mounting positions on the vehicle are to be kept intact. The design of the transverse leaf spring is made in order to mimic the exact characteristics of the original suspension system.</p><p>First analytical optimizations are made in order to find an initial solution. This design is then implemented in FEM-software in order to further investigate the characteristics and design. A final design is found that is fulfilling the requirements and a full scale version of the transverse leaf spring is built and examined with regards to its fulfilment of requirements.</p>
----------------------------------------------------------------------
In diva2:1141679 - title missing spaces:
"Swing check valvecharacterization: 3D CFDvalidation of one dimensionalmodels used in RELAP5"
==>
"Swing check valve characterization: 3D CFD validation of one dimensional models used in RELAP5"

abstract is:
<p>In a previous thesis work a swing check valve was studied with CFD analysisin order to nd correlations that could provide a good input for a onedimensionalmodel of the same. In this document, starting from the previousthesis results and using the model by Li and Liou as the reference work, a checkvalve is investigated and the hydraulic torque coecients identied. In this wayit becomes possible to analyze the behavior of the same valve with a 1D codecalled RELAP5.The rst part of the work was dedicated to understanding the dynamics lyingbehind the movement of swing check vale, and to the construction of a suitable3D CFD model being able to nd the required coecients. The results weresubsequently elaborated and implemented in the RELAP5 code, in order to runthe 1D simulations. In the second part of the job, several transient simulations ofdierent pipelines were conducted with the 1D model, monitoring in particularthe closure of the valve over time.In the end, the data obtained in RELAP5 were compared to those fromequivalent 3D CFD analysis and from an alternative 1D approach by Adamkowski.Although not completely matching, the results showed that the 1D model byLi and Liou has a good ability to accurately simulate the valve. Further workcould be surely done on particular topics in order to improve and tune thismodel, making it a consistent and cheaper alternative to 3D simulations, ableto accurately simulate a wide range of cases.</p>

corrected abstract:
<p>In a previous thesis work a swing check valve was studied with CFD analysis in order to find correlations that could provide a good input for a one-dimensional model of the same. In this document, starting from the previous thesis results and using the model by Li and Liou as the reference work, a check valve is investigated and the hydraulic torque coefficients identified. In this way it becomes possible to analyze the behavior of the same valve with a 1D code called RELAP5.</p><p>The first part of the work was dedicated to understanding the dynamics lying behind the movement of swing check vale, and to the construction of a suitable 3D CFD model being able to find the required coefficients. The results were subsequently elaborated and implemented in the RELAP5 code, in order to run the 1D simulations. In the second part of the job, several transient simulations of different pipelines were conducted with the 1D model, monitoring in particular the closure of the valve over time.</p><p>In the end, the data obtained in RELAP5 were compared to those from equivalent 3D CFD analysis and from an alternative 1D approach by Adamkowski. Although not completely matching, the results showed that the 1D model by Li and Liou has a good ability to accurately simulate the valve. Further work could be surely done on particular topics in order to improve and tune this model, making it a consistent and cheaper alternative to 3D simulations, able to accurately simulate a wide range of cases.</p>
----------------------------------------------------------------------
In diva2:1120505 abstract is:
<p>This paper presents a locomotion system suitable for interactive usethat can plan realistic paths for small numbers of bipedal charactersin virtual worlds. Earlier approaches are extended by allowing animationsto be arbitrarily blended to increase the range of motions thatthe character can produce and our system also achieves greater performancecompared to the earlier approaches. The system uses a graphof valid footprints in the world in which is searched for a path thatthe character should traverse. The resulting sequence of footprints aresmoothed and refined to make them more similar to the character’soriginal animations. To make the motion smoother the curvature andother parameters of the path are estimated and those estimates areused to interpolate between different sets of similar animation clips. Asthe system is based on footprints it allows characters to navigate evenacross regions which are not directly connected, for example by jumpingover the gaps between disconnected regions. We have implementedthe system in C# using the Unity Game Engine and we evaluate it bymaking the character perform various actions such as walking, runningand jumping and study the visual result.Accompanying material can be found at http://arongranberg.com/research/thesis2017.</p>


corrected abstract:
<p>This paper presents a locomotion system suitable for interactive use that can plan realistic paths for small numbers of bipedal characters in virtual worlds. Earlier approaches are extended by allowing animations to be arbitrarily blended to increase the range of motions that the character can produce and our system also achieves greater performance compared to the earlier approaches. The system uses a graph of valid footprints in the world in which is searched for a path that the character should traverse. The resulting sequence of footprints are smoothed and refined to make them more similar to the character's original animations. To make the motion smoother the curvature and other parameters of the path are estimated and those estimates are used to interpolate between different sets of similar animation clips. As the system is based on footprints it allows characters to navigate even across regions which are not directly connected, for example by jumping over the gaps between disconnected regions. We have implemented the system in C# using the Unity Game Engine and we evaluate it by making the character perform various actions such as walking, running and jumping and study the visual result. Accompanying material can be found at http://arongranberg.com/research/thesis2017.</p>
----------------------------------------------------------------------
In diva2:1120039 abstract is:
<p>Recommender systems can be seen everywheretoday, having endless possibilities of implementation. However, operating inthe background, they can easily be passed without notice. Essentially, recommendersystems are algorithms that generate predictions by operating on a certain dataset. Each case of recommendation is environment sensitive and dependent on thecondition of the data at hand. Consequently, it is difficult to foresee whichmethod, or combination of methods, to apply in a particular situation forobtaining desired results. The area of recommender systems that this thesis isdelimited to is Collaborative filtering (CF) and can be split up into threedifferent categories, namely memory based, model based and hybrid algorithms.This thesis implements a CF algorithm for each of these categories and setsfocus on comparing their prediction accuracy and their dependency on the amountof available training data (i.e. as a function of sparsity). The results showthat the model based algorithm clearly performs better than the memory based,both in terms of overall accuracy and sparsity dependency. With an increasingsparsity level, the problem of having users without any ratings is encountered,which greatly impacts the accuracy for the memory based algorithm. A hybridbetween these algorithms resulted in a better accuracy than the model basedalgorithm itself but with an insignificant improvement.</p>

corrected abstract:
<p>Recommender systems can be seen everywhere today, having endless possibilities of implementation. However, operating in the background, they can easily be passed without notice. Essentially, recommender systems are algorithms that generate predictions by operating on a certain data set. Each case of recommendation is environment sensitive and dependent on the condition of the data at hand. Consequently, it is difficult to foresee which method, or combination of methods, to apply in a particular situation for obtaining desired results. The area of recommender systems that this thesis is delimited to is Collaborative filtering (CF) and can be split up into three different categories, namely memory based, model based and hybrid algorithms. This thesis implements a CF algorithm for each of these categories and sets focus on comparing their prediction accuracy and their dependency on the amount of available training data (i.e. as a function of sparsity). The results show that the model based algorithm clearly performs better than the memory based, both in terms of overall accuracy and sparsity dependency. With an increasing sparsity level, the problem of having users without any ratings is encountered, which greatly impacts the accuracy for the memory based algorithm. A hybrid between these algorithms resulted in a better accuracy than the model based algorithm itself but with an insignificant improvement.</p>
----------------------------------------------------------------------
In diva2:783984 abstract is:
<p>With the increasing global demand for energy and environmental awareness, the interestin sustainable energy solutions has grown over the last years including wave energy.In this thesis there is a literature study on Wave Energy Converters (WEC) and a theorychapter on the power in ocean waves. The thesis work was done in in collaboration withCorPower Ocean (CPO), an innovative company developing a WEC. Two buoy shapes,both with two dierent weights were investigated and a comparison made on the eectof latching on power absorption. The work can be separated into two main parts, anumerical simulation and experimental tests.A numerical model incorporated with a mathematical description of CPO Power Take-o(PTO) physics was used to simulate and obtain numerical results on the buoy behaviourin select sea states. The benet of latching was obtained by comparing passively heavingbuoys to latch controlled buoys. The simulation model was used for various analysis ofthe system.Experiments were performed at 1:30 scale on the same buoys in a tank facility. Informationabout the hydrodynamics of the buoy, motion and power absorption was obtainedand the eect of latching on the power absorption found.Results on natural period and radiation damping were obtained and a drag coecientwas estimated.The results show that phase control by latching can substantially increase the powerabsorption of a point absorber and broaden the range of waves it can operate in. Agreementwas found in the numerical model and the experiments when investigating thebenet of latching compared to passively heaving.</p>


corrected abstract:
<p>With the increasing global demand for energy and environmental awareness, the interest in sustainable energy solutions has grown over the last years including wave energy. In this thesis there is a literature study on Wave Energy Converters (WEC) and a theory chapter on the power in ocean waves. The thesis work was done in in collaboration with CorPower Ocean (CPO), an innovative company developing a WEC. Two buoy shapes, both with two different weights were investigated and a comparison made on the effect of latching on power absorption. The work can be separated into two main parts, a numerical simulation and experimental tests.</p><p>A numerical model incorporated with a mathematical description of CPO Power Take-off (PTO) physics was used to simulate and obtain numerical results on the buoy behaviour in select sea states. The benefit of latching was obtained by comparing passively heaving buoys to latch controlled buoys. The simulation model was used for various analysis of the system.</p><p>Experiments were performed at 1:30 scale on the same buoys in a tank facility. Information about the hydrodynamics of the buoy, motion and power absorption was obtained and the effect of latching on the power absorption found. Results on natural period and radiation damping were obtained and a drag coefficient was estimated.</p><p>The results show that phase control by latching can substantially increase the power absorption of a point absorber and broaden the range of waves it can operate in. Agreement was found in the numerical model and the experiments when investigating the benefit of latching compared to passively heaving.</p>
----------------------------------------------------------------------
In diva2:515592 abstract is:
<p>This study examines how formal mathematics can be taught in the Swedishsecondary school with its new curriculum for mathematics. The study examineswhat a teaching material in formal mathematics corresponding to the initialcontent of the course Mathematics 1c could look like, and whether formalmathematics can be taught to high school students.The survey was conducted with second year students from the science programme.The majority of these students studied the course Mathematics D.The students described themselves as not being motivated towards mathematics.The results show that the content of the curriculum can be presented withformal mathematics. This both in terms of requirements for content and studentsbeing able to comprehend this content. The curriculum also requires thatthis type of mathematics is introduced in the course Mathematics 1c.The results also show that students are open towards and want more formalmathematics in their ordinary education. They initially felt it was strangebecause they had never encountered this type of mathematics before, but somestudents found the formal mathematics to be easier than the mathematicsordinarily presented in class.The study finds no reason to postpone the meeting with the formal mathematicsto university level. Students’ commitment to proof and their comprehentionof content suggests that formal mathematics can be introduced inhigh school courses. This study thus concludes that the new secondary schoolcourse Mathematics 1c can be formalised and therefore makes possible a renewedmathematics education.</p>

corrected abstract:
<p>This study examines how formal mathematics can be taught in the Swedish secondary school with its new curriculum for mathematics. The study examines what a teaching material in formal mathematics corresponding to the initial content of the course Mathematics 1c could look like, and whether formal mathematics can be taught to high school students.</p><p>The survey was conducted with second year students from the science programme. The majority of these students studied the course Mathematics D. The students described themselves as not being motivated towards mathematics.</p><p>The results show that the content of the curriculum can be presented with formal mathematics. This both in terms of requirements for content and students being able to comprehend this content. The curriculum also requires that this type of mathematics is introduced in the course Mathematics 1c.</p><p>The results also show that students are open towards and want more formal mathematics in their ordinary education. They initially felt it was strange because they had never encountered this type of mathematics before, but some students found the formal mathematics to be easier than the mathematics ordinarily presented in class.</p><p>The study finds no reason to postpone the meeting with the formal mathematics to university level. Students’ commitment to proof and their comprehention of content suggests that formal mathematics can be introduced in high school courses. This study thus concludes that the new secondary school course Mathematics 1c can be formalised and therefore makes possible a renewed mathematics education.</p>
----------------------------------------------------------------------
In diva2:515494 title missing space:
"User Centred Design for adolescents withCerebral Palsy: Designing an eye controlled software to enhance mathematical activities"
==>
"User Centred Design for adolescents with Cerebral Palsy: Designing an eye controlled software to enhance mathematical activities"


abstract is:
<p>This study aims to find an answer to what prerequisites that needs to betaken into consideration when designing an eye controlled software formathematical activities carried out by children that suffers from cerebralpalsy. A user centred study was conducted at three habilitation centresaround Stockholm. This resulted in a high-fi prototype for columnarcalculation, in which findings from the study were incorporated.These findings included the need to be able to adjust colour, sizeand shape of interface elements, as the target group suffered from visualimpairments. The interface should have a simple and clean design, astoo appealing elements may draw the attention away from the task. Furthermore,it shouldn’t be too childish, despite the fact that the softwarecovers basic mathematics.The tasks should have various kinds of representation, such as readoutinstructions and visualizations. It is also theorized that by designingthe interface to have non-selectable elements, the user doesn’t needworry about clicking on buttons that affects the interface. Thus, thefocus can be on solving the task. The user should be encourage to solvetasks by getting feedback when a task is solved. This feedback shouldonly be given once per task, and should be customizable and optional.</p>

corrected abstract:
<p>This study aims to find an answer to what prerequisites that needs to be taken into consideration when designing an eye controlled software for mathematical activities carried out by children that suffers from cerebral palsy. A user centred study was conducted at three habilitation centres around Stockholm. This resulted in a high-fi prototype for columnar calculation, in which findings from the study were incorporated.</p><p>These findings included the need to be able to adjust colour, size and shape of interface elements, as the target group suffered from visual impairments. The interface should have a simple and clean design, as too appealing elements may draw the attention away from the task. Furthermore, it shouldn’t be too childish, despite the fact that the software covers basic mathematics.</p><p>The tasks should have various kinds of representation, such as readout instructions and visualizations. It is also theorized that by designing the interface to have non-selectable elements, the user doesn’t need worry about clicking on buttons that affects the interface. Thus, the focus can be on solving the task. The user should be encourage to solve tasks by getting feedback when a task is solved. This feedback should only be given once per task, and should be customizable and optional.</p>
----------------------------------------------------------------------
In diva2:1816888 abstract is:
<p>Accurately modeling aerodynamic forces and moments are crucial for understanding thebehavior of an aircraft when performing various maneuvers at different flight conditions.However, this task is challenging due to complex nonlinear dependencies on manydifferent parameters. Currently, Computational Fluid Dynamics (CFD), wind tunnel,and flight tests are the most common methods used to gather information about thecoefficients, which are both costly and time–consuming. Consequently, great efforts aremade to find alternative methods such as machine learning.</p><p>This thesis focus on finding machine learning models that can model the static and thedynamic aerodynamics coefficients for lift, drag, and pitching moment. Seven machinelearning models for static estimation were trained on data from CFD simulations.The main focus was on dynamic aerodynamics since these are more difficult toestimate. Here two machine learning models were implemented, Long Short–TermMemory (LSTM) and Gaussian Process Regression (GPR), as well as the ordinaryleast squares. These models were trained on data generated from simulated flighttrajectories of longitudinal movements.</p><p>The results of the study showed that it was possible to model the static coefficients withlimited data and still get high accuracy. There was no machine learning model thatperformed best for all three coefficients or with respect to the size of the training data.The Support vector regression was the best for the drag coefficients, while there wasno clear best model for the lift and moment. For the dynamic coefficients, the ordinaryleast squares performed better than expected and even better than LSTM and GPR forsome flight trajectories. The Gaussian process regression produced better results whenestimating a known trajectory, while the LSTM was better when predicting values ofa flight trajectory not used to train the models.</p>

corrected abstract:
<p>Accurately modeling aerodynamic forces and moments are crucial for understanding the behavior of an aircraft when performing various maneuvers at different flight conditions. However, this task is challenging due to complex nonlinear dependencies on many different parameters. Currently, Computational Fluid Dynamics (CFD), wind tunnel, and flight tests are the most common methods used to gather information about the coefficients, which are both costly and time–consuming. Consequently, great efforts are made to find alternative methods such as machine learning.</p><p>This thesis focus on finding machine learning models that can model the static and the dynamic aerodynamics coefficients for lift, drag, and pitching moment. Seven machine learning models for static estimation were trained on data from CFD simulations. The main focus was on dynamic aerodynamics since these are more difficult to estimate. Here two machine learning models were implemented, Long Short–Term Memory (LSTM) and Gaussian Process Regression (GPR), as well as the ordinary least squares. These models were trained on data generated from simulated flight trajectories of longitudinal movements.</p><p>The results of the study showed that it was possible to model the static coefficients with limited data and still get high accuracy. There was no machine learning model that performed best for all three coefficients or with respect to the size of the training data. The Support vector regression was the best for the drag coefficients, while there was no clear best model for the lift and moment. For the dynamic coefficients, the ordinary least squares performed better than expected and even better than LSTM and GPR for some flight trajectories. The Gaussian process regression produced better results when estimating a known trajectory, while the LSTM was better when predicting values of a flight trajectory not used to train the models.</p>
----------------------------------------------------------------------
In diva2:1757049 abstract is:
<p>This thesis is a case study in collaboration with the company Getswish AB. GetswishAB provides the mobile application and payment service Swish with the purpose ofdelivering smooth money transfers for individuals and companies in Sweden. About80 percent of the Swedish population are connected to Swish, and the majority seethe service as an apparent part of everyday life. This work studies a small part of alltransactions that take place daily between individuals and companies. Specifically, thispaper examines which factors affect the Swish transaction amount (TA) to companieswithin five different industries. The five industries studied are: Sports, leisure,and entertainment activities; Restaurant, catering, and bar activities; Retail trade,except for motor vehicles and motorcycles; Trade and repair of motor vehicles andmotorcycles; and Telecommunications. In combination with descriptive analysis andseasonality studies, a multiple linear regression model is used to evaluate patternsin the amount transferred to companies within the various industries. The responsevariable is the daily aggregated TA and the seven responding regressors examined are:i) The number of employees of the company, ii) The revenue of the company, iii) Thedate for registration to Swish service for companies, iv) The age of the customers, v) Thegender of the customers, vi) The number of transactions, and vii) The transaction date.The estimated parameters for each regressor are studied to evaluate correlations withthe TA. This thesis states that it is possible to construct a model from the regressorsanalyzed, which can predict the amount with an explanation degree of above 85% forfour of the five industries. The model constructed for the motor vehicle industry nevergives satisfactory results and must be further investigated to conclude.</p>

corrected abstract:
<p>This thesis is a case study in collaboration with the company Getswish AB (Swish). The company provides a mobile application and payment service Swish with the purpose to delivering smooth money transfers for individuals and companies in Sweden. About 80 percent of the Swedish population are connected to Swish, and the majority see the service as an apparent part of everyday life. This work studies a small part of all transactions that take place daily between individuals and companies. Specifically, this paper examines which factors affect the Swish transaction amount (TA) to companies within five different industries. The five industries studied are: Sports, leisure, and entertainment activities; Restaurant, catering, and bar activities; Retail trade, except for motor vehicles and motorcycles; Trade and repair of motor vehicles and motorcycles; and Telecommunications. In combination with descriptive analysis and seasonality studies, a multiple linear regression model is used to evaluate patterns in the amount transferred to companies within the various industries. The response variable is the daily aggregated TA and the seven responding regressors examined are: i) The number of employees of the company, ii) The revenue of the company, iii) The date for registration to Swish service for companies, iv) The age of the customers, v) The gender of the customers, vi) The number of transactions, and vii) The transaction date. The estimated parameters for each regressor are studied to evaluate correlations with the TA. This thesis states that it is possible to construct a model from the regressors analyzed, which can predict the amount with an explanation degree of above 85% for four of the five industries. The model constructed for the motor vehicle industry never gives satisfactory results and must be further investigated to conclude.</p>
----------------------------------------------------------------------
In diva2:1720701 abstract is:
<p>Transformer-based models are frequently used in natural language processing. These models are oftenlarge and pre-trained for general language understanding and then fine-tuned for a specific task. Becausethese models are large, they have a high memory requirement and have high inference time. Severalmodel compression techniques have been developed in order to reduce the mentioned disadvantageswithout significantly reducing the inference performance of the models. This thesis studies unstructuredpruning method, which are pruning methods that do not follow a predetermined pattern when removingparameters, to understand which parameters can be removed from language models and the impact ofremoving a significant portion of a model's parameters. Specifically, magnitude pruning, movementpruning, soft movement pruning, and $L_0$ regularization were applied to the pre-trained languagemodels BERT and M-BERT. The pre-trained models in turn were fine-tuned for sentiment classificationtasks, which refers to the task of classifying a given sentence to predetermined labels, such as positive ornegative. Magnitude pruning worked the best when pruning the models to a ratio of 15\% of the models'original parameters, while soft movement pruning worked the best for the weight ratio of 3\%. Formovement pruning, we were not able to achieve satisfying results for binary sentiment classification.From investigating the pruning patterns derived from soft movement pruning and $L_0$ regularization, itwas found that a large portion of the parameters from the last transformer blocks in the model architecturecould be removed without significantly reducing the model performance. An example of interestingfurther work is to remove the last transformer blocks altogether and investigate if an increase in inferencespeed is attained without significantly reducing the performance.</p>

corrected abstract:
<p>Transformer-based models are frequently used in natural language processing. These models are often large and pre-trained for general language understanding and then fine-tuned for a specific task. Because these models are large, they have a high memory requirement and have high inference time. Several model compression techniques have been developed in order to reduce the mentioned disadvantages without significantly reducing the inference performance of the models. This thesis studies unstructured pruning method, which are pruning methods that do not follow a predetermined pattern when removing parameters, to understand which parameters can be removed from language models and the impact of removing a significant portion of a model's parameters. Specifically, magnitude pruning, movement pruning, soft movement pruning, and <em>L<sub>0</sub></em> regularization were applied to the pre-trained language models BERT and M-BERT. The pre-trained models in turn were fine-tuned for sentiment classification tasks, which refers to the task of classifying a given sentence to predetermined labels, such as positive or negative. Magnitude pruning worked the best when pruning the models to a ratio of 15% of the models' original parameters, while soft movement pruning worked the best for the weight ratio of 3%. For movement pruning, we were not able to achieve satisfying results for binary sentiment classification. From investigating the pruning patterns derived from soft movement pruning and <em>L<sub>0</sub></em> regularization, it was found that a large portion of the parameters from the last transformer blocks in the model architecture could be removed without significantly reducing the model performance. An example of interesting further work is to remove the last transformer blocks altogether and investigate if an increase in inference speed is attained without significantly reducing the performance.</p>
----------------------------------------------------------------------
In diva2:1720668 abstract is:
<p>As Machine Learning (ML) methods have gained traction in recent years, someproblems regarding the construction of such methods have arisen. One such problem isthe collection and labeling of data sets. Specifically when it comes to many applicationsof Computer Vision (CV), one needs a set of images, labeled as either being of someclass or not. Creating such data sets can be very time consuming. This project setsout to tackle this problem by constructing an end-to-end system for searching forobjects in images (i.e. an Object Based Image Retrieval (OBIR) method) using an objectdetection framework (You Only Look Once (YOLO) [16]). The goal of the project wasto create a method that; given an image of an object of interest q, search for that sameor similar objects in a set of other images S. The core concept of the idea is to passthe image q through an object detection model (in this case YOLOv5 [16]), create a”fingerprint” (can be seen as a sort of identity for an object) from a set of feature mapsextracted from the YOLOv5 [16] model and look for corresponding similar parts of aset of feature maps extracted from other images. An investigation regarding whichvalues to select for a few different parameters was conducted, including a comparisonof performance for a couple of different similarity metrics. In the table below,the parameter combination which resulted in the highest F_Top_300-score (a measureindicating the amount of relevant images retrieved among the top 300 recommendedimages) in the parameter selection phase is presented.</p><p>Layer: 23Pool Methd: maxSim. Mtrc: eucFP Kern. Sz: 4</p><p>Evaluation of the method resulted in F_Top_300-scores as can be seen in the table below.</p><p>Mouse: 0.820Duck: 0.640Coin: 0.770Jet ski: 0.443Handgun: 0.807Average: 0.696</p>


corrected abstract:
<p>As Machine Learning (ML) methods have gained traction in recent years, some problems regarding the construction of such methods have arisen. One such problem is the collection and labeling of data sets. Specifically when it comes to many applications of Computer Vision (CV), one needs a set of images, labeled as either being of some class or not. Creating such data sets can be very time consuming. This project sets out to tackle this problem by constructing an end-to-end system for searching for objects in images (i.e. an Object Based Image Retrieval (OBIR) method) using an object detection framework (You Only Look Once (YOLO) [16]). The goal of the project was to create a method that; given an image of an object of interest <em>q</em>, search for that same or similar objects in a set of other images S. The core concept of the idea is to pass the image <em>q</em> through an object detection model (in this case YOLOv5 [16]), create a ”fingerprint” (can be seen as a sort of identity for an object) from a set of feature maps extracted from the YOLOv5 [16] model and look for corresponding similar parts of a set of feature maps extracted from other images. An investigation regarding which values to select for a few different parameters was conducted, including a comparison of performance for a couple of different similarity metrics. In the table below, the parameter combination which resulted in the highest <em>F<sub>Top300</sub></em>-score (a measure indicating the amount of relevant images retrieved among the top 300 recommended images) in the parameter selection phase is presented.</p>


<table style="border-collapse: collapse; width: 50%;" border="1">
    <tbody>
        <tr>
            <td style="width: 25%;"><span style="color: #ff3232;">Layer Pool.</span></td>
            <td style="width: 25%;"><span style="color: #0000da;">Methd Sim.</span></td>
            <td style="width: 25%;"><span style="color: #008080;">Mtrc FP</span></td>
            <td style="width: 25%;"><span style="color: #912c91;">Kern. Sz</span></td>
        </tr>
        <tr>
            <td style="width: 25%;"><span style="color: #ff3232;">23</span></td>
            <td style="width: 25%;"><span style="color: #0000da;">max</span></td>
            <td style="width: 25%;"><span style="color: #008080;">euc</span></td>
            <td style="width: 25%;"><span style="color: #912c91;">4</span></td>
        </tr>
    </tbody>
</table>

Layer Pool. Methd Sim. Mtrc FP Kern. Sz
23 max euc 4
<p>”Pool. Methd” denotes ”Pooling Method”, ”Sim. Mtrc” denotes ”Similarity Metric” and ”FP Kern. Sz”
denotes ”Fingerprintization Kernel Size” (for image fingerprintization)</p>

<p>Evaluation of the method resulted in <em>F<sub>Top300</sub></em>-scores as can be seen in the table below.</p>
<table style="border-collapse: collapse; width: 100%;" border="1">
    <tbody>
        <tr>
            <td style="width: 10%;"><span style="color: #ff3232;">Layer Pool.</span></td>
            <td style="width: 10%;"><span style="color: #0000da;">Methd Sim.</span></td>
            <td style="width: 10%;"><span style="color: #008080;">Mtrc FP</span></td>
            <td style="width: 10%;"><span style="color: #912c91;">Kern. Sz</span></td>
            <td style="width: 10%;"><span style="color: black;">Mouse</span></td>
            <td style="width: 10%;"><span style="color: black;">Duck</span></td>
            <td style="width: 10%;"><span style="color: black;">Coin</span></td>
            <td style="width: 10%;"><span style="color: black;">Jet ski</span></td>
            <td style="width: 10%;"><span style="color: black;">Handgun</span></td>
            <td style="width: 10%;"><span style="color: black;">Average</span></td>
        </tr>
        <tr>
            <td style="width: 10%;"><span style="color: #ff3232;">23</span></td>
            <td style="width: 10%;"><span style="color: #0000da;">max</span></td>
            <td style="width: 10%;"><span style="color: #008080;">euc</span></td>
            <td style="width: 10%;"><span style="color: #912c91;">4</span></td>
            <td style="width: 10%;"><span style="color: black;">0.820</span></td>
            <td style="width: 10%;"><span style="color: black;">0.640</span></td>
            <td style="width: 10%;"><span style="color: black;">0.770</span></td>
            <td style="width: 10%;"><span style="color: black;">0.443</span></td>
            <td style="width: 10%;"><span style="color: black;">0.807</span></td>
            <td style="width: 10%;"><span style="color: black;">0.696</span></td>
        </tr>
    </tbody>
</table>
<p>Resulting <em>F<sub>Top300</sub></em> scores for the five classes (as well as the average) of the evaluation.</p>
----------------------------------------------------------------------
In diva2:1682170 abstract is:
<p>We manipulate two superconducting qubits using digital microwave electronics. Starting fromtheir characterization, we develop a real-time reset scheme and implement the iSwap gate. Thequbits’ parameters are obtained using standard single-qubit characterization techniques, such asRabi and Ramsey oscillations and frequency sweep of the resonators. We also characterized theexperimental setup, including finding the working point of a Josephson Parametric Amplifierand the coupler between the two qubits. We solve the linear differential equations that modelthe resonator, in order to design a high-fidelity, single-shot qubit-measurement pulse shape,which actively empties the cavity. Using this pulse, we achieve a readout assignment fidelity of99.9%. The readout is formed in real-time using template matching. In addition, we implementa conditional reset of the qubit’s state in 1.4 μs, which resets the excited state population from5.4% to 0.5%. We simulate the cavity using QuTip to further optimize the readout pulse.Furthermore, we characterize the third energy level of the qubit to implement a qutrit readoutand observe a second excited state population of 0.3%, in accordance with theory. Finally,we implement the iSwap gate that, together with single-qubit gates, constitute a set of universalquantum gates, where we swap the 95.4% of the quantum state between the qubits in 690 ns. Allexperiments, including the pulse events and synchronization of the readout and feedback, wereperformed using a digital microwave platform based on a radio-frequency-on-a-chip system,and implemented using a Python interface.</p>


corrected abstract:
<p>We manipulate two superconducting qubits using digital microwave electronics. Starting from their characterization, we develop a real-time reset scheme and implement the iSwap gate. The qubits’ parameters are obtained using standard single-qubit characterization techniques, such as Rabi and Ramsey oscillations and frequency sweep of the resonators. We also characterized the experimental setup, including finding the working point of a Josephson Parametric Amplifier and the coupler between the two qubits. We solve the linear differential equations that model the resonator, in order to design a high-fidelity, single-shot qubit-measurement pulse shape, which actively empties the cavity. Using this pulse, we achieve a readout assignment fidelity of 99.9%. The readout is formed in real-time using template matching. In addition, we implement a conditional reset of the qubit’s state in 1.4 µs, which resets the excited state population from 5.4% to 0.5%. We simulate the cavity using QuTip to further optimize the readout pulse. Furthermore, we characterize the third energy level of the qubit to implement a qutrit readout and observe a second excited state population of 0.3%, in accordance with theory. Finally, we implement the iSwap gate that, together with single-qubit gates, constitute a set of universal quantum gates, where we swap the 95.4% of the quantum state between the qubits in 690 ns. All experiments, including the pulse events and synchronization of the readout and feedback, were performed using a digital microwave platform based on a radio-frequency-on-a-chip system, and implemented using a Python interface.</p>
----------------------------------------------------------------------
In diva2:1673915 abstract is:
<p>There are constant natural disasters and conflicts around the world. In these areas, thereis a great demand to temporary housing. Better Shelter is a Swedish organization thatoffers temporary housing units for aid organizations. The product they invest in the mostis a metal frame on which roofs and walls must be complemented with local materials.The metal structure has a good strength characteristic but poses a challenge on howevertries to attach roofs and walls reliably. Therefore, the assigner wants to create a fasteningelement that can be sent with the rest of the structure, and which not only facilitatesattachment of the roof, but also handles the loads that the home is assumed to endure.This work includes the design process of a fastening element that facilitates the fasteningof a corrugated galvanized metal sheet roof (CGI) as well as a linear analysis in ANSYS ofthe load capacity of the roof and the fastening. The fastening element that was decided toproceed with is the so-called J-connector which is sized with standardized bolt materialsand dimensions in mind.Since the linear analysis can only consider the material within the linear elastic limit, themodel could not make the accurate prediction results as the stresses surpassed the yieldlimit locally. Mesh refining around the stress concentrations combined with including thenon-linear response of the material is required. The change in the contact status need tobe included at places where plasticizing and buckling can occur. Since the yield limitwas surpassed close to connection points and it could be tolerated in reality, the suggesteddevelopment is constructing the physical prototype base on the proposed fastening elementand performing the experimental analysis.</p>

corrected abstract:
<p>There are constant natural disasters and conflicts around the world. In these areas, there is a great demand to temporary housing. Better Shelter is a Swedish organization that offers temporary housing units for aid organizations. The product they invest in the most is a metal frame on which roofs and walls must be complemented with local materials. The metal structure has a good strength characteristic but poses a challenge on however tries to attach roofs and walls reliably. Therefore, the assigner wants to create a fastening element that can be sent with the rest of the structure, and which not only facilitates attachment of the roof, but also handles the loads that the home is assumed to endure. This work includes the design process of a fastening element that facilitates the fastening of a corrugated galvanized metal sheet roof (CGI) as well as a linear analysis in ANSYS of the load capacity of the roof and the fastening. The fastening element that was decided to proceed with is the so-called J-connector which is sized with standardized bolt materials and dimensions in mind.</p><p>Since the linear analysis can only consider the material within the linear elastic limit, the model could not make the accurate prediction results as the stresses surpassed the yield limit locally. Mesh refining around the stress concentrations combined with including the non-linear response of the material is required. The change in the contact status need to be included at places where plasticizing and buckling can occur. Since the yield limit was surpassed close to connection points and it could be tolerated in reality, the suggested development is constructing the physical prototype base on the proposed fastening element and performing the experimental analysis.</p>
----------------------------------------------------------------------
In diva2:1602688 abstract is:
<p>Metamaterials are artificially designed materials with desired electromagneticresponses for advanced wave manipulation. Their key constituent is often somenoble metal, thanks to its well localized plasmonic effects with highextinction cross section. In this project, a metamaterial based onmetal-insulator-metal (MIM) structure is investigated to create a compactplanar reflector which mimics the function of a parabolic mirror. In such ametamaterial, each MIM unit is essentially a sub-wavelength resonator whichexhibits magnetic-dipole resonance. To achieve focusing effect, phase shift onreflected wave by each MIM unit upon a plane-wave incidence is calculatedrigorously through finite-element method. By carefully selecting unitgeometries and thereby introducing a phase gradient along the reflector plane,one can control propagation direction of reflected wave at each reflectorposition. The principle can be explained in terms of either ray-optics theoryor generalized Snell’s law. As a particular demonstration, we have designed inthe thesis a planar reflector consisting of eleven MIM units with a totaldevice width of 5.5 µm. FEM simulation showed that the reflector focuses lightat 1.2 µm wavelength with a nominal focus length of 6 µm. Such compactmetamaterial devices can be potentially fabricated on chips for sensing andtelecom applications, circumventing many inconveniences of includingconventional lenses in an optical system.</p>

corrected abstract:
<p>Metamaterials are artificially designed materials with desired electromagnetic responses for advanced wave manipulation. Their key constituent is often some noble metal, thanks to its well localized plasmonic effects with high extinction cross section. In this project, a metamaterial based on metal-insulator-metal (MIM) structure is investigated to create a compact planar reflector which mimics the function of a parabolic mirror. In such a metamaterial, each MIM unit is essentially a sub-wavelength resonator which exhibits magnetic-dipole resonance. To achieve focusing effect, phase shift on reflected wave by each MIM unit upon a plane-wave incidence is calculated rigorously through finite-element method. By carefully selecting unit geometries and thereby introducing a phase gradient along the reflector plane, one can control propagation direction of reflected wave at each reflector position. The principle can be explained in terms of either ray-optics theory or generalized Snell’s law. As a particular demonstration, we have designed in the thesis a planar reflector consisting of eleven MIM units with a total device width of 5.5 µm. FEM simulation showed that the reflector focuses light at 1.2 µm wavelength with a nominal focus length of 6 µm. Such compact metamaterial devices can be potentially fabricated on chips for sensing and telecom applications, circumventing many inconveniences of including conventional lenses in an optical system.</p>
----------------------------------------------------------------------
In diva2:1595609 Note: no full text in DiVA

abstract is:
<p>Electric solutions for leisure craft propulsion are gaining popularity as global environmentalawareness increases and it also enables the experience of silent boating. The most criticalproblem with electric boats is the low range due to the limited energy density of batteries.Most of the energy is lost due to the resistance of the boat hull. Conventional electricpropulsion systems require mechanical gears and rudders which result in energy losses.To maximise the efficiency and thus the operating range/endurance of such displacementvessels, different propulsion systems are investigated by marine propulsor suppliers. Thisthesis investigates different aspects of pod propulsion systems for displacement boats. Amethod is presented to estimate the added resistance of the pod body and the strut based onrecommended procedures from ITTC and resistance coefficients from Hoerner. The coolingperformance of the system is investigated by a method considering approximated convectiveheat transfer from the housing to the seawater. A concept of a steerable pod is presentedaccording to the evaluated dimensions of a 50 kW electric motor by fellow thesis workerA. Bagaskara. General arrangement, attachment to an example hull and load carryingcapability of the structure were evaluated. The pod unit consists of different metallic parts,therefore it is subjected to galvanic corrosion. The possibilities of cathodic protection areconsidered. A comparison is carried out between podded and L-drive arrangements, and it isconcluded that pods are more beneficial in terms of space requirement, cooling, maintenanceand overall efficiency at low speeds. The building costs are estimated to increase mostlydue to the need for a custom made electric motor.</p>

corrected abstract:
<p>Electric solutions for leisure craft propulsion are gaining popularity as global environmentalawareness increases and it also enables the experience of silent boating. The most critical problem with electric boats is the low range due to the limited energy density of batteries. Most of the energy is lost due to the resistance of the boat hull. Conventional electricpropulsion systems require mechanical gears and rudders which result in energy losses. To maximise the efficiency and thus the operating range/endurance of such displacement vessels, different propulsion systems are investigated by marine propulsor suppliers. This thesis investigates different aspects of pod propulsion systems for displacement boats. Amethod is presented to estimate the added resistance of the pod body and the strut based on recommended procedures from ITTC and resistance coefficients from Hoerner. The cooling performance of the system is investigated by a method considering approximated convective heat transfer from the housing to the seawater. A concept of a steerable pod is presented according to the evaluated dimensions of a 50 kW electric motor by fellow thesis worker A. Bagaskara. General arrangement, attachment to an example hull and load carrying capability of the structure were evaluated. The pod unit consists of different metallic parts, therefore it is subjected to galvanic corrosion. The possibilities of cathodic protection are considered. A comparison is carried out between podded and L-drive arrangements, and it is concluded that pods are more beneficial in terms of space requirement, cooling, maintenance and overall efficiency at low speeds. The building costs are estimated to increase mostly due to the need for a custom made electric motor.</p>
----------------------------------------------------------------------
In diva2:1528136 abstract is:
<p>Optimization methods are commonly used to develop new products and are also an importantstep in more incremental design improvements. In the maritime industry, these methodsare often used to create more ecient vessels and to ful ll the environmental requirementsimposed by the IMO. In recent years, the adjoint method have been used more frequently.This method can be used to predict the inuence of some input parameters on a quantityin a Computational Fluid Dynamics (CFD) simulation.In this project, the adjoint method has been investigated and applied on a relevant case;how it can be used to reduce the drag of a twisted rudder by changing the twist angles.STAR-CCM+ has been used to perform the CFD and adjoint simulations. These resultshave been imported to CAESES, a CAD-modeler, which connects the adjoint results to thedesign parameters. The adjoint results indicate a possible change of the design parameter,the twist angle is modi ed based on these results and a new geometry of the rudder is constructedin CAESES. Furthermore, the numerical results indicates that the method can beused to reduce the drag on the rudder. One of the cases in the project achieved a reductionof the rudder drag by 3.35 % and the total drag decreased with 0.18 %. However, the othertwo cases did not achieve a reduction of the drag and hence further investigations needs tobe done.The adjoint method have the possibility to be a good optimization alternative for developmentof new products or in engineering-to-order processes. The option of connecting theadjoint results to design parameters is a great advantage. On the other hand, the method inthis project is not reliable and the reason for the contradictory results needs to be studiedfurther.</p>

corrected abstract:
<p>Optimization methods are commonly used to develop new products and are also an important step in more incremental design improvements. In the maritime industry, these methods are often used to create more efficient vessels and to fulfill the environmental requirements imposed by the IMO. In recent years, the adjoint method have been used more frequently. This method can be used to predict the influence of some input parameters on a quantity in a Computational Fluid Dynamics (CFD) simulation.</p><p>In this project, the adjoint method has been investigated and applied on a relevant case; how it can be used to reduce the drag of a twisted rudder by changing the twist angles. STAR-CCM+ has been used to perform the CFD and adjoint simulations. These results have been imported to CAESES, a CAD-modeler, which connects the adjoint results to the design parameters. The adjoint results indicate a possible change of the design parameter, the twist angle is modified based on these results and a new geometry of the rudder is constructed in CAESES. Furthermore, the numerical results indicates that the method can be used to reduce the drag on the rudder. One of the cases in the project achieved a reduction of the rudder drag by 3.35 % and the total drag decreased with 0.18 %. However, the other two cases did not achieve a reduction of the drag and hence further investigations needs to be done.</p><p>The adjoint method have the possibility to be a good optimization alternative for development of new products or in engineering-to-order processes. The option of connecting the adjoint results to design parameters is a great advantage. On the other hand, the method in this project is not reliable and the reason for the contradictory results needs to be studied further.</p>
----------------------------------------------------------------------
In diva2:1528127 abstract is:
<p>Knowledge of vessel responses to waves is of the utmost importance for vessel operations.The responses affect which routes a vessel can take, what cargo it can carry, theconditions it’s crew will experience and much more. This can pose a problem for performanceoptimisation companies such as GreenSteam, partners in this project, for whomthe vessel transfer functions are generally not available.This project aims to use bayesian machine learning methods to infer transfer functionsand predict vessel responses. Publically available directional wave spectra are combinedwith highfrequencymotion measurements from a vessel to train a model to create thetransfer functions, which can be integrated to get the motions. If successful, this would bea relatively inexpensive method for computing transfer functions on any vessel for whichthe required measurements are available. Though not many vessels measure this datacurrently, the industry is moving towards more data collection, so that number is likely torise.The results identify a number of issues in the available data which must be overcome toproduce usable results from these methods. Though results are not optimal, they show apromising start and a route is proposed for future research in this area.</p>

corrected abstract:
<p>Knowledge of vessel responses to waves is of the utmost importance for vessel operations. The responses affect which routes a vessel can take, what cargo it can carry, the conditions it’s crew will experience and much more. This can pose a problem for performance optimisation companies such as GreenSteam, partners in this project, for whom the vessel transfer functions are generally not available.</p><p>This project aims to use bayesian machine learning methods to infer transfer functions and predict vessel responses. Publically available directional wave spectra are combined with high-frequency motion measurements from a vessel to train a model to create the transfer functions, which can be integrated to get the motions. If successful, this would be a relatively inexpensive method for computing transfer functions on any vessel for which the required measurements are available. Though not many vessels measure this data currently, the industry is moving towards more data collection, so that number is likely to rise.</p><p>The results identify a number of issues in the available data which must be overcome to produce usable results from these methods. Though results are not optimal, they show a promising start and a route is proposed for future research in this area.</p>
----------------------------------------------------------------------
In diva2:1319886 abstract is:
<p>The interest in modeling non-maturing deposits has skyrocketed ever since thefinancial crisis 2008. Not only from a regulatory and legislative perspective,but also from an investment and funding perspective.Modeling of non-maturing deposits is a very broad subject. In this thesis someof the topics within the subject are investigated, where the greatest focus inon the modeling of the deposit volumes. The main objective is to providethe bank with an analysis of the majority of the topics that needs to be cov-ered when modeling non-maturing deposits. This includes short-rate model-ing using Vasicek’s model, deposit rate modeling using a regression approachand a method proposed by Jarrow and Van Deventer, volume modeling usingSARIMA, SARIMAX and a general additive model, a static replicating port-folio based on Maes and Timmerman’s to model the behaviour of the depositaccounts and finally a liquidity risk model that was suggested by Kalkbrenerand Willing. All of these models have been applied on three different accounttypes: private transaction accounts, savings accounts and corporate savingsaccounts.The results are that, due to the current market, the static replicating portfoliodoes not achieve the desired results. Furthermore, the best volume model forthe data provided is a SARIMA model, meaning the effect of the exogenousvariables are seemingly already embedded in the lagged volume. Finally, theliquidity risk results are plausible and thus deemed satisfactory.</p>


corrected abstract:
<p>The interest in modeling non-maturing deposits has skyrocketed ever since the financial crisis 2008. Not only from a regulatory and legislative perspective, but also from an investment and funding perspective.</p><p>Modeling of non-maturing deposits is a very broad subject. In this thesis some of the topics within the subject are investigated, where the greatest focus in on the modeling of the deposit volumes. The main objective is to provide the bank with an analysis of the majority of the topics that needs to be covered when modeling non-maturing deposits. This includes short-rate modeling using Vasicek’s model, deposit rate modeling using a regression approach and a method proposed by Jarrow and Van Deventer, volume modeling using SARIMA, SARIMAX and a general additive model, a static replicating portfolio based on Maes and Timmerman’s to model the behaviour of the deposit accounts and finally a liquidity risk model that was suggested by Kalkbrener and Willing. All of these models have been applied on three different account types: private transaction accounts, savings accounts and corporate savings accounts.</p><p>The results are that, due to the current market, the static replicating portfolio does not achieve the desired results. Furthermore, the best volume model for the data provided is a SARIMA model, meaning the effect of the exogenous variables are seemingly already embedded in the lagged volume. Finally, the liquidity risk results are plausible and thus deemed satisfactory.</p>
----------------------------------------------------------------------
In diva2:1293627 Note: no full text in DiVA

abstract is:
<p>Numerous civil and military applications are based on the detection of light in the infraredspectrum : night-vision goggle, thermography or spatial imaging. These complex and specializedapplications require a reliable and high performing technology of photodetectors. Acurrent trend is to increase throughput and detector size. One particular step in the process,is an annealing under saturating mercury pressure of the wafers. The experimental knowledgeof the annealing is insufficient, but it is also a crucial step : during the process, the samplesneed to remain inside their stability domain at all time, so they do not deteriorate.The goal of this thesis was to investigate numerically the annealing process under saturatingmercury pressure and extract information that were not accessible with experimentalmeasurements.We incrementally developed a model and refined it until it fitted well the experimentalmeasurements. We were then able to extract meaningful data : we discovered that the experimentalmeasurements were biased, that the cold point regulating the mercury pressure wasnot maintained at the right spot, and we brought to light that there were inhomogeneitiesbetween and along the substrates. Even though we found out these problems, we checked thatthe samples remained inside the stability diagram during the whole process.Finally, these results were transmitted to help establish the specifications of new and largerfurnaces.</p>

corrected abstract:
<p>Numerous civil and military applications are based on the detection of light in the infrared spectrum : night-vision goggle, thermography or spatial imaging. These complex and specialized applications require a reliable and high performing technology of photo detectors. A current trend is to increase throughput and detector size. One particular step in the process, is an annealing under saturating mercury pressure of the wafers. The experimental knowledge of the annealing is insufficient, but it is also a crucial step : during the process, the samples need to remain inside their stability domain at all time, so they do not deteriorate. The goal of this thesis was to investigate numerically the annealing process under saturating mercury pressure and extract information that were not accessible with experimental measurements.</p><p>We incrementally developed a model and refined it until it fitted well the experimental measurements. We were then able to extract meaningful data : we discovered that the experimental measurements were biased, that the cold point regulating the mercury pressure was not maintained at the right spot, and we brought to light that there were inhomogeneities between and along the substrates. Even though we found out these problems, we checked that the samples remained inside the stability diagram during the whole process. Finally, these results were transmitted to help establish the specifications of new and larger furnaces.</p>
----------------------------------------------------------------------
In diva2:1212110 abstract is:
<p>To maintain solvency intimes of severe economic downturns banks and financialinstitutions keep capital cushions that reflect the risks in the balance sheet.Broadly,how much capital that is being held is a combination of external requirementsfromregulators and internal assessments of credit risk. We discuss alternatives totheBasel Pillar II capital add-on based on multi-factor models for held capitaland howthese can be applied so that only concentration (or sector) risk affects theoutcome,even in a portfolio with prominent idiosyncratic risk. Further, the stabilityandreliability of these models are evaluated. We found that this idiosyncraticrisk canefficiently be removed both on a sector and a portfolio level and that themulti-factormodels tested converge.We introduce two new indices based on Risk Weighted Assets (RI) and EconomicCapital (EI). Both show the desired effect of an intuitive dependence on the PDand LGD. Moreover, EI shows a dependence on the inter-sector correlation. Inthesample portfolio, we show that the high concentration in one sector could be(better)justified by these methods when the low average LGD and PD of this sector weretaken into consideration.</p>

corrected abstract:
<p>To maintain solvency in times of severe economic downturns banks and financial institutions keep capital cushions that reflect the risks in the balance sheet. Broadly, how much capital that is being held is a combination of external requirements from regulators and internal assessments of credit risk. We discuss alternatives to the Basel Pillar II capital add-on based on multi-factor models for held capital and how these can be applied so that only concentration (or sector) risk affects the outcome, even in a portfolio with prominent idiosyncratic risk. Further, the stability and reliability of these models are evaluated. We found that this idiosyncratic risk can efficiently be removed both on a sector and a portfolio level and that the multi-factor models tested converge.</p><p>We introduce two new indices based on Risk Weighted Assets (RI) and Economic Capital (EI). Both show the desired effect of an intuitive dependence on the PD and LGD. Moreover, EI shows a dependence on the inter-sector correlation. In the sample portfolio, we show that the high concentration in one sector could be (better) justified by these methods when the low average LGD and PD of this sector were taken into consideration.</p>
----------------------------------------------------------------------
In diva2:1133508 abstract is:
<p>GKN Aerospace Engine Systems specializes in large load carrying static structuresfor aero engines and is, as part of a light weight strategy, developing design andmanufacturing technology to be able to complement the current metallic product offerwith composite fan guide vane structures. Fan vanes in modern engines are structuraland to meet the requirements for low weight in the aircraft industry, it is necessary todesign the vane as a sandwich structure. The objective with this thesis is to investigateand model the heating effect in the polymeric sandwich core during cyclic loadingand to assess the impact on fatigue life from heating both during structural testing aswell as in service.To model the heating in a FEM model, the damping in the material is measured withDynamic Mechanical Thermal Analysis (DMTA) and is used together with the cyclicamplitude and frequency to calculate the heating term for each element in the FEmodel. In order to validate the thermal analysis and see the effect of heating on thefatigue properties of the core material, fatigue tests are done at normal testingfrequencies and elevated frequencies with temperature elevations monitored as afunction of time.The predicted heating effect is shown to correlate well with the experimental datawhereas the maximum loading frequency of 40 Hz that could be applied was too lowto give any effect on the fatigue life. It is also shown that the effect of loadingfrequencies in the order of 300 Hz requires the fatigue amplitude to be lower than thefatigue strength to avoid excessive heating effects. For in service loadings, the highcycle fatigue is intermittent and as the heating is slow enough, the conclusion is thatfatigue strength is not affected by heating effects</p>

corrected abstract:
<p>GKN Aerospace Engine Systems specializes in large load carrying static structures for aero engines and is, as part of a light weight strategy, developing design and manufacturing technology to be able to complement the current metallic product offer with composite fan guide vane structures. Fan vanes in modern engines are structural and to meet the requirements for low weight in the aircraft industry, it is necessary to design the vane as a sandwich structure. The objective with this thesis is to investigate and model the heating effect in the polymeric sandwich core during cyclic loading and to assess the impact on fatigue life from heating both during structural testing as well as in service.</p><p>To model the heating in a FEM model, the damping in the material is measured with Dynamic Mechanical Thermal Analysis (DMTA) and is used together with the cyclic amplitude and frequency to calculate the heating term for each element in the FE model. In order to validate the thermal analysis and see the effect of heating on the fatigue properties of the core material, fatigue tests are done at normal testing frequencies and elevated frequencies with temperature elevations monitored as a function of time.</p><p>The predicted heating effect is shown to correlate well with the experimental data whereas the maximum loading frequency of 40 Hz that could be applied was too low to give any effect on the fatigue life. It is also shown that the effect of loading frequencies in the order of 300 Hz requires the fatigue amplitude to be lower than the fatigue strength to avoid excessive heating effects. For in service loadings, the high cycle fatigue is intermittent and as the heating is slow enough, the conclusion is that fatigue strength is not affected by heating effects</p>
----------------------------------------------------------------------
In diva2:1120605 abstract is:
<p>In this project a simulation of multiple quadcopters is created to solvethe problem of exploring and mapping an unknown environment. Thetwo major challengers are allocation of the route of the quadcopters tooptimize the mapping, as well as the merging of the three-dimensionalsub maps to get a final result. Occupancy grid maps will be used forthe allocation task to calculate the probability that a cell is occupiedby an obstacle. A cost and an utility function will also be calculatedfor every cell next to the unexplored areas to make a decisions for theroutes of the quadcopters. The cost makes the quadcopter travel tothe closest unexplored cell that is not occupied with some probabilitywhile the utility function works in a way that several quadcopters willnot travel to the same area. The combination of these two calculatesthe optimal path. The optimal merging of the sub maps is then to findan optimal rigid transformation that optimally aligns two sets of pointsin R3 in a least square sent. To do this an optimal translation andan optimal rotation using Singular Value Decomposition is calculated.Finally, algorithms which can be implemented in any suitable softwarefor simulation and demonstration of how this things can be handledin an practical environment, is described.</p>


corrected abstract:
<p>In this project a simulation of multiple quadcopters is created to solve the problem of exploring and mapping an unknown environment. The two major challengers are allocation of the route of the quadcopters to optimize the mapping, as well as the merging of the three-dimensional sub maps to get a final result. Occupancy grid maps will be used for the allocation task to calculate the probability that a cell is occupied by an obstacle. A cost and an utility function will also be calculated for every cell next to the unexplored areas to make a decisions for the routes of the quadcopters. The cost makes the quadcopter travel to the closest unexplored cell that is not occupied with some probability while the utility function works in a way that several quadcopters will not travel to the same area. The combination of these two calculates the optimal path. The optimal merging of the sub maps is then to find an optimal rigid transformation that optimally aligns two sets of points in ℝ<sup>3</sup> in a least square sent. To do this an optimal translation and an optimal rotation using Singular Value Decomposition is calculated. Finally, algorithms which can be implemented in any suitable software for simulation and demonstration of how this things can be handled in an practical environment, is described.</p>
----------------------------------------------------------------------
In diva2:1110830 - missing ligature in title:
"CFD modelling of ski-jump spillway in Stornnforsen"
==>
"CFD modelling of ski-jump spillway in Storfinnforsen"


abstract is:
<p>Traditionally when designing dams and spillways, experiments in physical scale models are conductedin order to determine whether or not the design fulls it purpose, and to identify and avoid undesiredproblems, such as unfavourable ow patterns and unwanted water splatter. Physical models can oftenbe expensive and time consuming to build, and often suer from scale eects that in uence the results.Uniper and Vattenfall have recently done experiments in a physical 1:50 scale model of the dam Stornnforsen,in order to test new solutions for the energy dissipation from the spillways. One of the testedsolutions is a ip bucket at the bottom of the right surface spillway.In this project the same solution is numerically modelled, using the CFD software ANSYS® Fluent®,and the results are compared to those from the experiments. The CFD simulations are done both in fullscale and model scale, in order to identify potential scale eects. The aspects that are compared are theheight and length of the jet from the ip bucket.In addition to the CFD simulations, the height and length are also calculated semi-empirically, usingtwo dierent methods.Altogether the results correspond quite well to the experimental values. Some possible scale eects areobserved, where the jet from the full scale simulation is more dispersed than the jet from the model scalesimulation. The jet trajectory from the full scale simulation is also a bit lower than the jet from theexperiments and model scale simulations.The grid independence for the simulations is not quite satisfactory, and the grid should be rened to getmore reliable results. Due to lack of time and computational power any further grid renement is notdone in this project.</p>

corrected abstract:
<p>Traditionally when designing dams and spillways, experiments in physical scale models are conducted in order to determine whether or not the design fulfils it purpose, and to identify and avoid undesired problems, such as unfavourable flow patterns and unwanted water splatter. Physical models can often be expensive and time consuming to build, and often suffer from scale effects that influence the results.</p><p>Uniper and Vattenfall have recently done experiments in a physical 1:50 scale model of the dam Storfinnforsen, in order to test new solutions for the energy dissipation from the spillways. One of the tested solutions is a flip bucket at the bottom of the right surface spillway.</p><p>In this project the same solution is numerically modelled, using the CFD software ANSYS® Fluent®, and the results are compared to those from the experiments. The CFD simulations are done both in full scale and model scale, in order to identify potential scale effects. The aspects that are compared are the height and length of the jet from the flip bucket.</p><p>In addition to the CFD simulations, the height and length are also calculated semi-empirically, using two different methods.</p><p>Altogether the results correspond quite well to the experimental values. Some possible scale effects are observed, where the jet from the full scale simulation is more dispersed than the jet from the model scale simulation. The jet trajectory from the full scale simulation is also a bit lower than the jet from the experiments and model scale simulations.</p><p>The grid independence for the simulations is not quite satisfactory, and the grid should be refined to get more reliable results. Due to lack of time and computational power any further grid refinement is not done in this project.</p>
----------------------------------------------------------------------
In diva2:1110762 abstract is:
<p>This paper presents a study on spacecraftRendezvous and Docking (RvD) through a comprehensiveliterature review, in addition to investigate the main phases andpossible control methods during the space rendezvous and docking.It presents a study of different energy efficient far rangerendezvous (e.g. Hoffman, bi-elliptic, phasing maneuver etc.). Aset of formation flying models (i.e. relative navigation) for twospacecraft operating in close proximity are examined. Oneapproach to depict the relative orbit’s dynamics model for closeproximity operations is to mathematically express it by thenonlinear equations of relative motion (NERM), that present thehighest accuracy and are valid for all types of orbit eccentricitiesand separations. Another approach is the Hill-Clohessy-Wiltshire(HCW). This method is only valid for two conditions, when thetarget satellite orbit is near circular and the distance between thechaser and target is small. The dynamics models in this paperdescribe the spacecraft formation flying in both unperturbed andperturbed environment, where only the Earth oblatenessperturbations are being treated.Furthermore, this paper presents a design of a control, guidance,and navigation (GNC) based on the aforementioned dynamicmodel. This will enable the chaser satellite to autonomouslyapproach towards the target satellite during the close proximitynavigation using some control techniques such Linear QuadraticRegulation (LQR) and Linear Quadratic Gaussian (LQG). Thesecontrol techniques aim to reduce both the duration and the ΔVcost of the entire mission.</p>


corrected abstract:
<p>This paper presents a study on spacecraft Rendezvous and Docking (RvD) through a comprehensive literature review, in addition to investigate the main phases and possible control methods during the space rendezvous and docking. It presents a study of different energy efficient far range rendezvous (e.g. Hoffman, bi-elliptic, phasing maneuver etc.). A set of formation flying models (i.e. relative navigation) for two spacecraft operating in close proximity are examined. One approach to depict the relative orbit’s dynamics model for close proximity operations is to mathematically express it by the nonlinear equations of relative motion (NERM), that present the highest accuracy and are valid for all types of orbit eccentricities and separations. Another approach is the Hill-Clohessy-Wiltshire (HCW). This method is only valid for two conditions, when the target satellite orbit is near circular and the distance between the chaser and target is small. The dynamics models in this paper describe the spacecraft formation flying in both unperturbed and perturbed environment, where only the Earth oblateness perturbations are being treated.</p><p>Furthermore, this paper presents a design of a control, guidance, and navigation (GNC) based on the aforementioned dynamic model. This will enable the chaser satellite to autonomously approach towards the target satellite during the close proximity navigation using some control techniques such Linear Quadratic Regulation (LQR) and Linear Quadratic Gaussian (LQG). These control techniques aim to reduce both the duration and the ΔV cost of the entire mission.</p>
----------------------------------------------------------------------
In diva2:937846 abstract is:
<p>Proton radiation therapy allows for delivering a high dose to a well-confinedregion of interest due to the characteristic proton dose deposition. Due to protonrange straggling, anatomic variations in patients and small patient setup errors,treatment plans needs to account for proton range uncertainties of up to 3.5% invivo.Therefore, it is highly desirable to measure the proton range on-line in orderto minimize margins in the treatment plan. Initially, the feasibility of on-linerange monitoring through prompt gamma imaging and Positron EmissionTomography (PET) at different proton energies is evaluated using GEANT4Application for Tomographic Emission (GATE) Monte Carlo (MC) simulations.In the second phase, the performance of a lead knife-edge slit system for promptgamma imaging was evaluated with MC simulations. Results from simulationsindicate that prompt gamma emission and PET isotope production is correlatedwith proton range, with discrete prompt gamma emission lines from Carbon (4.4MeV) showing good correlation. The evaluated system was able to image thepeak gamma emission location at three different slit positions with promisingprecision ± 1 mm, ± 0.7 mm and ± 1.3 mm, and average shifts of -2 mm, -3 mmand -4 mm, respectively. The proton range was resolved with mean profile shiftsof -12 ± 1 mm, -13 ± 0.7 mm and -14 ± 1.3 mm, following prompt gamma crosssectionbehavior with peak emission- and threshold energies. The results providean indication of the potential of the knife-edge slit system and future work willinclude more extensive MC simulations and experimental measurements at the Skandion clinic to determine its clinical validity.</p>

corrected abstract:
<p>Proton radiation therapy allows for delivering a high dose to a well-confined region of interest due to the characteristic proton dose deposition. Due to proton range straggling, anatomic variations in patients and small patient setup errors, treatment plans needs to account for proton range uncertainties of up to 3.5% <em>in vivo</em>. Therefore, it is highly desirable to measure the proton range on-line in order to minimize margins in the treatment plan. Initially, the feasibility of on-line range monitoring through prompt gamma imaging and Positron Emission Tomography (PET) at different proton energies is evaluated using GEANT4 Application for Tomographic Emission (GATE) Monte Carlo (MC) simulations. In the second phase, the performance of a lead knife-edge slit system for prompt gamma imaging was evaluated with MC simulations. Results from simulations indicate that prompt gamma emission and PET isotope production is correlated with proton range, with discrete prompt gamma emission lines from Carbon (4.4 MeV) showing good correlation. The evaluated system was able to image the peak gamma emission location at three different slit positions with promising precision ± 1 mm, ± 0.7 mm and ± 1.3 mm, and average shifts of -2 mm, -3 mm and -4 mm, respectively. The proton range was resolved with mean profile shifts of -12 ± 1 mm, -13 ± 0.7 mm and -14 ± 1.3 mm, following prompt gamma crosssection behavior with peak emission- and threshold energies. The results provide an indication of the potential of the knife-edge slit system and future work will include more extensive MC simulations and experimental measurements at the Skandion clinic to determine its clinical validity.</p>
----------------------------------------------------------------------
In diva2:624028 - missing space in title:
"Pricing With Uncertainty: The impact of uncertainty in the valuation models ofDupire and Black&Scholes"
==>
"Pricing With Uncertainty: The impact of uncertainty in the valuation models of Dupire and Black&Scholes"

abstract is:
<p>Theaim of this master-thesis is to study the impact of uncertainty in the local-and implied volatility surfaces when pricing certain structured products suchas capital protected notes and autocalls. Due to their long maturities, limitedavailability of data and liquidity issue, the uncertainty may have a crucialimpact on the choice of valuation model. The degree of sensitivity andreliability of two different valuation models are studied. The valuation models chosen for this thesis are the local volatility model of Dupire and the implied volatility model of Black&amp;Scholes. The two models are stress tested with varying volatilities within an uncertainty interval chosen to be the volatilities obtained from Bid and Ask market prices. The volatility surface of the Mid market prices is set as the relative reference and then successively scaled up and down to measure the uncertainty.The results indicates that the uncertainty in the chosen interval for theDupire model is of higher order than in the Black&amp;Scholes model, i.e. thelocal volatility model is more sensitive to volatility changes. Also, the pricederived in the Black&amp;Scholes modelis closer to the market price of the issued CPN and the Dupire price is closer tothe issued Autocall. This might be an indication of uncertainty in thecalibration method, the size of the chosen uncertainty interval or the constantextrapolation assumption.A further notice is that the prices derived from the Black&amp;Scholes model areoverall higher than the prices from the Dupire model. Another observation ofinterest is that the uncertainty between the models is significantly greaterthan within each model itself.</p>

corrected abstract:
<p>The aim of this master-thesis is to study the impact of uncertainty in the local- and implied volatility surfaces when pricing certain structured products such as capital protected notes and autocalls. Due to their long maturities, limited availability of data and liquidity issue, the uncertainty may have a crucial impact on the choice of valuation model. The degree of sensitivity and reliability of two different valuation models are studied.</p><p>The valuation models chosen for this thesis are the local volatility model of Dupire and the implied volatility model of Black&amp;Scholes. The two models are stress tested with varying volatilities within an uncertainty interval chosen to be the volatilities obtained from Bid and Ask market prices. The volatility surface of the Mid market prices is set as the relative reference and then successively scaled up and down to measure the uncertainty.</p><p>The results indicates that the uncertainty in the chosen interval for the Dupire model is of higher order than in the BlackScholes model, i.e. the local volatility model is more sensitive to volatility changes. Also, the price derived in the BlackScholes model is closer to the market price of the issued CPN and the Dupire price is closer to the issued Autocall. This might be an indication of uncertainty in the calibration method, the size of the chosen uncertainty interval or the constant extrapolation assumption.</p><p>A further notice is that the prices derived from the Black&amp;Scholes model are overall higher than the prices from the Dupire model. Another observation of interest is that the uncertainty between the models is significantly greater than within each model itself.</p>
----------------------------------------------------------------------
In diva2:408834 - missing space in title:
"Learning collaborators: A study in making experiencebased knowledge conscious"
==>
"Learning collaborators: A study in making experience based knowledge conscious"

abstract is:
<p>The Swedish steel manufacturer Oxelösund AB is a member of the SSAB SwedishSteel Group and is the largest Nordic manufacturer of heavy steel plate. This thesis isabout how selected employees at SSAB Oxelösund AB acquire and consolidate theirknowledge, when it is supposed that they, in the future, will pass their knowledge onwithin the company. The thesis focuses on how employees perceive and gain theirlearning.The employees expresses that their experience transforms into knowledge when theyreflect upon recently perceived events. They describe that learning is a continuousprocess that is achieved over time and that they learn in their meeting with other people.Those employees were consolidating their knowledge when they both formulated andexpressed themselves in words or when they practiced their skills. Thus they haveconfirmed and have made their knowledge visible, primarily for themselves but also fortheir surroundings.For SSAB Oxelösund AB, to gradually will be able to transfer the employees experienceand knowledge to other employees, it is essential to give them time to reflect upon theirown knowledge. They must become aware of both the knowledge they possess andwhat skills are to be shared, to succeed in sharing their knowledge.</p>

corrected abstract:
<p>The Swedish steel manufacturer Oxelösund AB is a member of the SSAB Swedish Steel Group and is the largest Nordic manufacturer of heavy steel plate. This thesis is about how selected employees at SSAB Oxelösund AB acquire and consolidate their knowledge, when it is supposed that they, in the future, will pass their knowledge on within the company. The thesis focuses on how employees perceive and gain their learning.</p><p>The employees expresses that their experience transforms into knowledge when they reflect upon recently perceived events. They describe that learning is a continuous process that is achieved over time and that they learn in their meeting with other people. Those employees were consolidating their knowledge when they both formulated and expressed themselves in words or when they practiced their skills. Thus they have confirmed and have made their knowledge visible, primarily for themselves but also for their surroundings.</p><p>For SSAB Oxelösund AB, to gradually will be able to transfer the employees experience and knowledge to other employees, it is essential to give them time to reflect upon their own knowledge. They must become aware of both the knowledge they possess and what skills are to be shared, to succeed in sharing their knowledge.</p>
----------------------------------------------------------------------
In diva2:408832 abstract is:
<p>In an ever more competitive environment Sandvik Materials Technology,SMT, runs a large-scale program to improve their processes for product development.The purpose with the program is to increase the company’s profitability by increasingthe number of new products in their product portfolio. The strategies SMT is usingfor reaching their goals are to change the way they work with product development byreducing lead time and learning how to prioritize products with the highestprofitability. In the R &amp; D Department the work of improvement pursue by adaptingLean R &amp; D to their way of working. Lean R &amp; D encourages working withcontinuous improvements and eliminating waste.The main purpose for this Master’s Thesis is to develop didactic support to make iteasier to implement Lean in SMT R &amp; D department. With use of interviews,observations and taking part in education at SMT and at Sandholm Associates has thisMaster’s Thesis resulted in modifying of the internal education material at Sandvik anddevelopment of a concept of Stickers for the computer screen. The main product ofthis Master’s Thesis project is a Lean game adapted for Sandvik SMT R &amp; Ddepartment.This report contains suggestion for Sandvik how to improve the communication ofknowledge between their departments with purpose to reduce external consultationsand to achieve a more pleasant working environment.</p>

corrected abstract:
<p>In an ever more competitive environment Sandvik Materials Technology, SMT, runs a large-scale program to improve their processes for product development. The purpose with the program is to increase the company’s profitability by increasing the number of new products in their product portfolio. The strategies SMT is using for reaching their goals are to change the way they work with product development by reducing lead time and learning how to prioritize products with the highest profitability. In the R &amp; D Department the work of improvement pursue by adapting Lean R &amp; D to their way of working. Lean R &amp; D encourages working with continuous improvements and eliminating waste.</p><p>The main purpose for this Master’s Thesis is to develop didactic support to make it easier to implement Lean in SMT R &amp; D department. With use of interviews, observations and taking part in education at SMT and at Sandholm Associates has this Master’s Thesis resulted in modifying of the internal education material at Sandvik and development of a concept of Stickers for the computer screen. The main product of this Master’s Thesis project is a Lean game adapted for Sandvik SMT R &amp; D department.</p><p>This report contains suggestion for Sandvik how to improve the communication of knowledge between their departments with purpose to reduce external consultations and to achieve a more pleasant working environment.</p>
----------------------------------------------------------------------
In diva2:1892290 abstract is:
<p>Characterizing the optical scattering properties of materials is usefulfor many purposes. There are also multiple ways to describe scatteringfrom matter. One way, from a macroscopic point of view, is to measurethe bidirectional scatter distribution function (BSDF) with an opticalscattering measurement device, such as a scatterometer. On the otherend, mathematical descriptions such as Mie theory provide an exact de-scription of scattering from well-defined objects. In this work we considerboth theoretical and practical aspects of optical scattering measurementsin the development of an optical scattering measurement device. Firstwe study electromagnetic scattering theory, especially Mie theory. Thenwe study optical models, from a geometrical-optics point of view, of thedetector system of an optical scattering measurement device. We developand configure a detector system and also a BSDF mockup device withwhich we perform some BSDF measurements on a sample of milk. Wecompare the experimental result with the result from a simulation of anoptical scattering model for milk based on Mie theory. The results seemto stay within the same order of magnitude but the uncertainties of themeasurements and the experimental setup are too high to find any clearcorrespondence between the results.</p>

corrected abstract:
<p>Characterizing the optical scattering properties of materials is useful for many purposes. There are also multiple ways to describe scattering from matter. One way, from a macroscopic point of view, is to measure the bidirectional scatter distribution function (BSDF) with an optical scattering measurement device, such as a scatterometer. On the other end, mathematical descriptions such as Mie theory provide an exact description of scattering from well-defined objects. In this work we consider both theoretical and practical aspects of optical scattering measurements in the development of an optical scattering measurement device. First we study electromagnetic scattering theory, especially Mie theory. Then we study optical models, from a geometrical-optics point of view, of the detector system of an optical scattering measurement device. We develop and configure a detector system and also a BSDF mockup device with which we perform some BSDF measurements on a sample of milk. We compare the experimental result with the result from a simulation of an optical scattering model for milk based on Mie theory. The results seem to stay within the same order of magnitude but the uncertainties of the measurements and the experimental setup are too high to find any clear correspondence between the results.</p>
----------------------------------------------------------------------
In diva2:1880821 abstract is:
<p>This thesis investigates an aerodynamic optimization of front lower wishbones on a FormulaStudent car using CFD simulations and windtunnel testing. The aim of the study is to de-termine weather a geometric optimization of wishbones is feasable and could enhance theaerodynamic performance of the vehicle.Siemens Star-CCM+ is utilized for the CFD simulations with focus on iterative design im-provments to optimize the initial generic airfoil shape. The CFD model is based on the modelprovided by KTH Formula Student. The model was also developed further in the initial stagesof the study. Turbulance transition model was investigated as well as the turbulance model toaccurately capture the near wall flow dynamics.In order to validate the results from the CFD model a set of windtunnel tests where doneinvolving a 13 scale model of the vehicle at various yaw angles.Some key findings from the analysis indicate that significant changes in the aerodynamicalcharacteristics can be made by making relatively small changes to the geometry of the vehicle.An increase of 2,1% was achieved which closely correlate to the accuired data which indic-ated a 2,2% increase.The optimization process highlighted the importance of developing CFD optimization tech-niques as they can make large contributions to the aerodynamic characteristics of vehicles.This can be used in order to increase the performance of of vehicles in motorsport or reducedrag in everyday vehicles to reduce the energy consumtion.</p>

corrected abstract:
<p>This thesis investigates an aerodynamic optimization of front lower wishbones on a Formula Student car using CFD simulations and wind tunnel testing. The aim of the study is to determine weather a geometric optimization of wishbones is feasable and could enhance the aerodynamic performance of the vehicle.</p><p>Siemens Star-CCM+ is utilized for the CFD simulations with focus on iterative design improvments to optimize the initial generic airfoil shape. The CFD model is based on the model provided by KTH Formula Student. The model was also developed further in the initial stages of the study. Turbulance transition model was investigated as well as the turbulance model to accurately capture the near wall flow dynamics.</p><p>In order to validate the results from the CFD model a set of wind tunnel tests where done involving a &frac13; scale model of the vehicle at various yaw angles.</p><p>Some key findings from the analysis indicate that significant changes in the aerodynamical characteristics can be made by making relatively small changes to the geometry of the vehicle. An increase of 2,1% was achieved which closely correlate to the accuired data which indicated a 2,2% increase.</p><p>The optimization process highlighted the importance of developing CFD optimization techniques as they can make large contributions to the aerodynamic characteristics of vehicles. This can be used in order to increase the performance of of vehicles in motorsport or reduce drag in everyday vehicles to reduce the energy consumtion.</p>

Note: I've used a vulgar fraction /13 rather than 1 over 3.


<math xmlns="http://www.w3.org/1998/Math/MathML" alttext="\frac{1}{3}" display="inline"><mfrac><mn>1</mn><mn>3</mn></mfrac></math>

corrected abstract with MathML:
<p>This thesis investigates an aerodynamic optimization of front lower wishbones on a Formula Student car using CFD simulations and wind tunnel testing. The aim of the study is to determine weather a geometric optimization of wishbones is feasable and could enhance the aerodynamic performance of the vehicle.</p><p>Siemens Star-CCM+ is utilized for the CFD simulations with focus on iterative design improvments to optimize the initial generic airfoil shape. The CFD model is based on the model provided by KTH Formula Student. The model was also developed further in the initial stages of the study. Turbulance transition model was investigated as well as the turbulance model to accurately capture the near wall flow dynamics.</p><p>In order to validate the results from the CFD model a set of wind tunnel tests where done involving a <math xmlns="http://www.w3.org/1998/Math/MathML" alttext="\frac{1}{3}" display="inline"><mfrac><mn>1</mn><mn>3</mn></mfrac></math> scale model of the vehicle at various yaw angles.</p><p>Some key findings from the analysis indicate that significant changes in the aerodynamical characteristics can be made by making relatively small changes to the geometry of the vehicle. An increase of 2,1% was achieved which closely correlate to the accuired data which indicated a 2,2% increase.</p><p>The optimization process highlighted the importance of developing CFD optimization techniques as they can make large contributions to the aerodynamic characteristics of vehicles. This can be used in order to increase the performance of of vehicles in motorsport or reduce drag in everyday vehicles to reduce the energy consumtion.</p>


----------------------------------------------------------------------
In diva2:1843030 abstract is:
<p>In this thesis, the main focus is to formulate and test a suitable model forexogenous fault detection in swarms containing unmanned aerial vehicles(UAVs), which are aerial autonomous systems. FOI Swedish DefenseResearch Agency provided the thesis project and research question. Inspiredby previous work, the implementation use behavioral feature vectors (BFVs)to simulate the movements of the UAVs and to identify anomalies in theirbehaviors.</p><p>The chosen algorithm for fault detection is the density-based cluster analysismethod known as the Local Outlier Factor (LOF). This method is built on thek-Nearest Neighbor(kNN) algorithm and employs densities to detect outliers.In this thesis, it is implemented to detect faulty agents within the swarm basedon their behavior. A confusion matrix and some associated equations are usedto evaluate the accuracy of the method.</p><p>Six features are selected for examination in the LOF algorithm. The firsttwo features assess the number of neighbors in a circle around the agent,while the others consider traversed distance, height, velocity, and rotation.Three different fault types are implemented and induced in one of the agentswithin the swarm. The first two faults are motor failures, and the last oneis a sensor failure. The algorithm is successfully implemented, and theevaluation of the faults is conducted using three different metrics. Several setsof experiments are performed to assess the optimal value for the LOF thresholdand to understand the model’s performance. The thesis work results in a strongLOF value which yields an acceptable F1 score, signifying the accuracy of theimplementation is at a satisfactory level.</p>

corrected abstract:
<p>In this thesis, the main focus is to formulate and test a suitable model for exogenous fault detection in swarms containing unmanned aerial vehicles (UAVs), which are aerial autonomous systems. FOI Swedish Defense Research Agency provided the thesis project and research question. Inspired by previous work, the implementation use behavioral feature vectors (BFVs) to simulate the movements of the UAVs and to identify anomalies in their behaviors.</p><p>The chosen algorithm for fault detection is the density-based cluster analysis method known as the Local Outlier Factor (LOF). This method is built on the k-Nearest Neighbor(kNN) algorithm and employs densities to detect outliers. In this thesis, it is implemented to detect faulty agents within the swarm based on their behavior. A confusion matrix and some associated equations are used to evaluate the accuracy of the method.</p><p>Six features are selected for examination in the LOF algorithm. The first two features assess the number of neighbors in a circle around the agent, while the others consider traversed distance, height, velocity, and rotation. Three different fault types are implemented and induced in one of the agents within the swarm. The first two faults are motor failures, and the last one is a sensor failure. The algorithm is successfully implemented, and the evaluation of the faults is conducted using three different metrics. Several sets of experiments are performed to assess the optimal value for the LOF threshold and to understand the model’s performance. The thesis work results in a strong LOF value which yields an acceptable F1 score, signifying the accuracy of the implementation is at a satisfactory level.</p>
----------------------------------------------------------------------
In diva2:1823869 abstract is:
<p>The purpose of this study was twofold: The first part was about analyzing possiblecorrelations between various weather factors and the demand for bike-sharing.The aim of the study was to investigate how the relationship between theseexplanatory variables and the response variable looks like. To investigate this,regression analysis was used where a multivariate linear model was built basedon real data. The study was based on the city of Seoul, where the city’s bike-sharing and weather data were used. What the study found was that all weatherfactors were statistically significant with the demand for bike-sharing and therelationship between them was exponential. The relationship was positive forthe weather factors temperature and visibility while the remaining, i.e. weatherfactors, humidity, wind speed, solar radiation, rain and snowfall, had a negativerelationship. The strongest relationship were temperature, humidity and rainwhile the weakest were wind speed, snowfall and visibility.</p><p>In the second part of the study a market analysis for bike-sharing was carried out.Also in this part, the city of Seoul was in focus. Within this market analysis, amongother things, the market situation, competing services, strengths and weaknessesswere analyzed. The conclusion based on this analysis is that the market for bike-sharing is complex and includes many factors. It has also shown that a city likeSeoul has succeeded in introducing bike-sharing despite many challenges. Finally,the study has shown that bike-sharing is a growing market that will become evenmore relevant in the future.</p>

corrected abstract:
<p>The purpose of this study was twofold: The first part was about analyzing possible correlations between various weather factors and the demand for bike-sharing. The aim of the study was to investigate how the relationship between these explanatory variables and the response variable looks like. To investigate this, regression analysis was used where a multivariate linear model was built based on real data. The study was based on the city of Seoul, where the city’s bike-sharing and weather data were used. What the study found was that all weather factors were statistically significant with the demand for bike-sharing and the relationship between them was exponential. The relationship was positive for the weather factors temperature and visibility while the remaining, i.e. weather factors, humidity, wind speed, solar radiation, rain and snowfall, had a negative relationship. The strongest relationship were temperature, humidity and rain while the weakest were wind speed, snowfall and visibility.</p><p>In the second part of the study a market analysis for bike-sharing was carried out. Also in this part, the city of Seoul was in focus. Within this market analysis, among other things, the market situation, competing services, strengths and weaknessess were analyzed. The conclusion based on this analysis is that the market for bike-sharing is complex and includes many factors. It has also shown that a city like Seoul has succeeded in introducing bike-sharing despite many challenges. Finally, the study has shown that bike-sharing is a growing market that will become even more relevant in the future.</p>
----------------------------------------------------------------------
In diva2:1816745 abstract is:
<p>The use of continuously reinforced thermoplastics have increased in recent years andoffer some significant advantages over their thermoset counterparts. The utilizationof these materials is however still limited due to the labour intensive processingand manufacturing. In the first part of this thesis, the aim is to investigate howcontinuously reinforced thermoplastics can be used in high volume manufacturingof complex components, specifically when combined with injection moulding. Thesecond part will attempt to develop a conceptual manufacturing process and designof a demonstrator part and perform a structural analysis of the component.To answer the first question, an extensive literature study on continuous fibre materialsand thermoplastics was conducted along with research on established and newlydeveloped manufacturing methods such as automated tape laying and fibre placement,tailored fibre placement and 3D printing combined with injection overmoulding. Theconceptual manufacturing process of the demonstrator part was then based on thisresearch and the design was modeled in using finite element analysis.The results shows that continuously reinforced thermoplastics can be used forhigh volume manufacturing of complex components when combined with injectionmoulding. While some of the processing methods are still in an early developmentstage, the techniques have been tested and implemented. The structural analysis of thedemonstrator part shows that the design can withstand the maximum external loadsthus and provide proof of concept.</p>

corrected abstract:
<p>The use of continuously reinforced thermoplastics have increased in recent years and offer some significant advantages over their thermoset counterparts. The utilization of these materials is however still limited due to the labour intensive processing and manufacturing. In the first part of this thesis, the aim is to investigate how continuously reinforced thermoplastics can be used in high volume manufacturing of complex components, specifically when combined with injection moulding. The second part will attempt to develop a conceptual manufacturing process and design of a demonstrator part and perform a structural analysis of the component.</p><p>To answer the first question, an extensive literature study on continuous fibre materials and thermoplastics was conducted along with research on established and newly developed manufacturing methods such as automated tape laying and fibre placement, tailored fibre placement and 3D printing combined with injection overmoulding. The conceptual manufacturing process of the demonstrator part was then based on this research and the design was modeled in using finite element analysis.</p><p>The results shows that continuously reinforced thermoplastics can be used for high volume manufacturing of complex components when combined with injection moulding. While some of the processing methods are still in an early development stage, the techniques have been tested and implemented. The structural analysis of the demonstrator part shows that the design can withstand the maximum external loads thus and provide proof of concept.</p>
----------------------------------------------------------------------
In diva2:1799888 abstract is:
<p>Self propulsion modelling is important in order to accurately simulate ships and submarinesusing Computational Fluid Dynamics (CFD). However, fully resolved simulations of hull andpropeller geometries are computationally heavy and time consuming. As such there is a greatinterest in lower order CFD models of propellers. This work investigates three lower ordermodels of a non-cavitating generic submarine propeller (INSEAN E1619) in OpenFOAM. Themodels investigated are Actuator Disk (AD). Rotor Disk (RD) and Actuator Line Model (ALM).The AD model applies a momentum change based on propeller performance coefficients overa disc cell set. The RD uses Blade Element Method (BEM) to calculate a more realistic thrustdistribution over the disk. Finally the ALM applies BEM over seven rotating lines within the cellset disc. The source code to the RD model was modified according to suggestions provided fromearlier studies on the model. The ALM used was originally designed for turbines which wasrectified by changing the force projection vectors in the source code to model propellers instead.There was not enough published data to directly utilize BEM on the E1619 propeller, thus thedata was generated by conducting 2D simulations on every element. The simulations were setup to replicate results provided in earlier works with higher order models in order to compareboth quantitative and qualitative results. It was found the ALM matched the reference databest out of the models tested in this work. The RD was qualitatively similar to the time averageof the ALM fields but numerically inaccurate. The AD results were poor, both quantitativelyand qualitatively.</p><p> </p>

corrected abstract:
<p>Self propulsion modelling is important in order to accurately simulate ships and submarines using Computational Fluid Dynamics (CFD). However, fully resolved simulations of hull and propeller geometries are computationally heavy and time consuming. As such there is a great interest in lower order CFD models of propellers. This work investigates three lower order models of a non-cavitating generic submarine propeller (INSEAN E1619) in OpenFOAM. The models investigated are Actuator Disk (AD). Rotor Disk (RD) and Actuator Line Model (ALM). The AD model applies a momentum change based on propeller performance coefficients over a disc cell set. The RD uses Blade Element Method (BEM) to calculate a more realistic thrust distribution over the disk. Finally the ALM applies BEM over seven rotating lines within the cell set disc. The source code to the RD model was modified according to suggestions provided from earlier studies on the model. The ALM used was originally designed for turbines which was rectified by changing the force projection vectors in the source code to model propellers instead. There was not enough published data to directly utilize BEM on the E1619 propeller, thus the data was generated by conducting 2D simulations on every element. The simulations were set up to replicate results provided in earlier works with higher order models in order to compare both quantitative and qualitative results. It was found the ALM matched the reference data best out of the models tested in this work. The RD was qualitatively similar to the time average of the ALM fields but numerically inaccurate. The AD results were poor, both quantitatively and qualitatively.</p>
----------------------------------------------------------------------
In diva2:1774376 abstract is:
<p>Predicting wall heat flux accurately in wall-bounded turbulent flows is critical fora variety of engineering applications, including thermal management systems andenergy-efficient designs. Traditional methods, which rely on expensive numericalsimulations, are hampered by increasing complexity and extremly high computationcost. Recent advances in deep neural networks (DNNs), however, offer an effectivesolution by predicting wall heat flux using non-intrusive measurements derivedfrom off-wall quantities. This study introduces a novel approach, the convolution-compacted vision transformer (ViT), which integrates convolutional neural networks(CNNs) and ViT to predict instantaneous fields of wall heat flux accurately based onoff-wall quantities including velocity components at three directions and temperature.Our method is applied to an existing database of wall-bounded turbulent flowsobtained from direct numerical simulations (DNS). We first conduct an ablationstudy to examine the effects of incorporating convolution-based modules into ViTarchitectures and report on the impact of different modules. Subsequently, we utilizefully-convolutional neural networks (FCNs) with various architectures to identify thedistinctions between FCN models and the convolution-compacted ViT. Our optimizedViT model surpasses the FCN models in terms of instantaneous field predictions,learning turbulence statistics, and accurately capturing energy spectra. Finally, weundertake a sensitivity analysis using a gradient map to enhance the understandingof the nonlinear relationship established by DNN models, thus augmenting theinterpretability of these models</p>

corrected abstract:
<p>Predicting wall heat flux accurately in wall-bounded turbulent flows is critical for a variety of engineering applications, including thermal management systems and energy-efficient designs. Traditional methods, which rely on expensive numerical simulations, are hampered by increasing complexity and extremly high computation cost. Recent advances in deep neural networks (DNNs), however, offer an effective solution by predicting wall heat flux using non-intrusive measurements derived from off-wall quantities. This study introduces a novel approach, the convolution-compacted vision transformer (ViT), which integrates convolutional neural networks (CNNs) and ViT to predict instantaneous fields of wall heat flux accurately based on off-wall quantities including velocity components at three directions and temperature. Our method is applied to an existing database of wall-bounded turbulent flows obtained from direct numerical simulations (DNS). We first conduct an ablation study to examine the effects of incorporating convolution-based modules into ViT architectures and report on the impact of different modules. Subsequently, we utilize fully-convolutional neural networks (FCNs) with various architectures to identify the distinctions between FCN models and the convolution-compacted ViT. Our optimized ViT model surpasses the FCN models in terms of instantaneous field predictions, learning turbulence statistics, and accurately capturing energy spectra. Finally, we undertake a sensitivity analysis using a gradient map to enhance the understanding of the nonlinear relationship established by DNN models, thus augmenting the interpretability of these models.</p>
----------------------------------------------------------------------
In diva2:1766585 abstract is:
<p>Cytokines are small, secreted proteins that are important for cell signalling in theimmune system. Interferon gamma (IFN-γ) is one of the most potent cytokines thatnatural killer (NK) cells of the innate immune system secrete with both antiviral,antibacterial, and antitumoral activity. Analysis of NK cells, such as that of secretionof IFN-γ, is important for studying the immune response to cancer and for developingeffective immunotherapies. In this master thesis project, a method was developedfor determining the amount of IFN-γ secreted by NK cells when being confinedwith cancer cells in deep microwells. Antibody-coated microbeads was used tocapture secreted IFN-γ, which was fluorescently labeled and detected by imaging usingfluorescence microscopy. Microbead seeding into small microwells for single cellassays and into large microwells for embedding of beads into 3D tumor spheroidswas investigated. An analytical model based on experimental standard curves wasdeveloped for straightforward quantification of the amount of bound IFN-γ, with ademonstrated detection down to 2.10−18 moles per bead. The detection of IFN-γ wasevaluated for primary NK cells stimulated by PMA/ionomycin for different incubationtimes. The secretion rate of IFN-γ by IL-2 activated NK cells under PMA/ionomycinstimulation was estimated at 184 molecules per second. IFN-γ detection was alsoevaluated in cell cytotoxicity assays where NK cells were confined over time togetherwith cancer cells in microwells. Both assays showed a successful detection of IFN-γ secretion, demonstrating the potential of the developed method for immune cellanalysis.</p>

corrected abstract:
<p>Cytokines are small, secreted proteins that are important for cell signalling in the immune system. Interferon gamma (IFN-γ) is one of the most potent cytokines that natural killer (NK) cells of the innate immune system secrete with both antiviral, antibacterial, and antitumoral activity. Analysis of NK cells, such as that of secretion of IFN-γ, is important for studying the immune response to cancer and for developing effective immunotherapies. In this master thesis project, a method was developed for determining the amount of IFN-γ secreted by NK cells when being confined with cancer cells in deep microwells. Antibody-coated microbeads was used to capture secreted IFN-γ, which was fluorescently labeled and detected by imaging using fluorescence microscopy. Microbead seeding into small microwells for single cell assays and into large microwells for embedding of beads into 3D tumor spheroids was investigated. An analytical model based on experimental standard curves was developed for straightforward quantification of the amount of bound IFN-γ, with a demonstrated detection down to 2.10<sup>−18</sup> moles per bead. The detection of IFN-γ was evaluated for primary NK cells stimulated by PMA/ionomycin for different incubation times. The secretion rate of IFN-<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&gamma;</mi></math> by IL-2 activated NK cells under PMA/ionomycin stimulation was estimated at 113 molecules per second. IFN-<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&gamma;</mi></math> detection was also evaluated in cell cytotoxicity assays where NK cells were confined over time together with cancer cells in microwells. Both assays showed a successful detection of IFN-γ secretion, demonstrating the potential of the developed method for immune cell analysis.</p>

Note that two instance of gamma (γ) use math mode - to match the use of the CMMI font used for these two instances in the original manuscript. Note that the same two instances occur in the Swedish version of the abstract. This same math style is used in the list of acronyms. If one looks at lines 2 and 3 on page 8, you can ee both forms being used! The first with the math gamma, is referring to the protean, while the second is referring to the interferon gamma cytokine. I've sent a note to Prof. Björn Önfelt - who was the examiner for this thesis to ask about this.
----------------------------------------------------------------------
In diva2:1707770 abstract is:
<p>Lithium-ion batteries (LiBs) are the most popular choice in the shift towardselectrification due to their high volumetric energy and power density. An importantaspect to study is the effect of swelling on the mechanical performance of LiBsas it plays an important role in determining the forces in the battery module.During charge/discharge a battery cell swells/shrinks and over the lifetime of thebattery, swelling becomes permanent. The swelling increases with cycling that causesincreasing forces in the module. Excessive pressure generated due to cycling in themodule may electrically short the cells and/or cause mechanical damage to the cells.Compression pads placed between cells in the battery module absorb the swelling. Thematerial properties and size of the compression pads used influence the module forcesat End of Life (EoL).In this study, a 1D phenomenological model is built to predict the swelling forces. Themodel differs from others in literature in a way that the swelling forces are predictedwith cycling rather than State of Charge (SoC) and a stress-strain based constitutivemodel is used rather than a spring model. A process to eliminate the need for multipletests is also proposed in the thesis to predict swelling forces for different compressionpads and preloads.The proposed model is relatively simple and can improve existing battery managementsystems by predicting the swelling and the magnitude of swelling forces for differentcompression pads and preloads during the operational life of the battery.</p>

corrected abstract:
<p>Lithium-ion batteries (LiBs) are the most popular choice in the shift towards electrification due to their high volumetric energy and power density. An important aspect to study is the effect of swelling on the mechanical performance of LiBs as it plays an important role in determining the forces in the battery module. During charge/discharge, a battery cell swells/shrinks and over the lifetime of the battery, swelling becomes permanent. The swelling increases with cycling that causes increasing forces in the module. Excessive pressure generated due to cycling in the module may electrically short the cells and/or cause mechanical damage to the cells. Compression pads placed between cells in the battery module absorb the swelling. The material properties and size of the compression pads used influence the module forces at End of Life (EoL).</p><p>In this study, a 1D phenomenological model is built to predict the swelling forces. The model differs from others in literature in a way that the swelling forces are predicted with cycling rather than State of Charge (SoC) and a stress-strain based constitutive model is used rather than a spring model. A process to eliminate the need for multiple tests is also proposed in the thesis to predict swelling forces for different compression pads and preloads.</p><p>The proposed model is relatively simple and can improve existing battery management systems by predicting the swelling and the magnitude of swelling forces for different compression pads and preloads during the operational life of the battery.</p>
----------------------------------------------------------------------
In diva2:1698347 abstract is:
<p>An aerodynamic shape optimization procedure was performed on a low observable engineintake duct. The intake duct was fixed in its throat and aerodynamic interface plane (AIP)sections, while leaving up to 7 design parameters free to deformation in the centroid curveand mid section profile. The optimization setup consisted of an optimizer block implementedin MATLAB, where the NSGA-II optimization algorithm was implemented, and a simulationblock using computational fluid dynamics (CFD). The objective functions for the optimizationprocess were the pressure recovery and the DC60 distortion coefficient in the AIP section.In total, four optimizations with gradually increasing degrees of deformation were conducted.The first optimization process was a validation case, performed on a test duct design, whilethe remaining optimizations were performed using a duct designed by the Swedish DefenceResearch Agency (FOI) as a starting point, for cruise and take-off conditions.</p><p>The connection of NSGA-II and the CFD setup proved useful, as the distortion was decreasedby up to 52.8% relative the original value while keeping the pressure recovery within 0.06% ofthe original duct. The algorithm was successful in finding an improvement for both consideredoperating conditions, with the largest improvement for the cruise case. In total 975 duct designswere evaluated in the four processes, using a uniform inflow boundary condition on a boundaryextruded one meter from the throat of the intake duct.</p><p>The importance of the handling of non-converged solutions in the automated optimizationprocess was also pointed out, as an oscillating solution affected the third optimization, therebyrendering that solution useless.</p>

corrected abstract:
<p>An aerodynamic shape optimization procedure was performed on a low observable engine intake duct. The intake duct was fixed in its throat and aerodynamic interface plane (AIP) sections, while leaving up to 7 design parameters free to deformation in the centroid curve and mid section profile. The optimization setup consisted of an optimizer block implemented in MATLAB, where the NSGA-II optimization algorithm was implemented, and a simulation block using computational fluid dynamics (CFD). The objective functions for the optimization process were the pressure recovery and the DC60 distortion coefficient in the AIP section. In total, four optimizations with gradually increasing degrees of deformation were conducted. The first optimization process was a validation case, performed on a test duct design, while the remaining optimizations were performed using a duct designed by the Swedish Defence Research Agency (FOI) as a starting point, for cruise and take-off conditions.</p><p>The connection of NSGA-II and the CFD setup proved useful, as the distortion was decreased by up to 52.8% relative the original value while keeping the pressure recovery within 0.06% of the original duct. The algorithm was successful in finding an improvement for both considered operating conditions, with the largest improvement for the cruise case. In total 975 duct designs were evaluated in the four processes, using a uniform inflow boundary condition on a boundary extruded one meter from the throat of the intake duct.</p><p>The importance of the handling of non-converged solutions in the automated optimization process was also pointed out, as an oscillating solution affected the third optimization, thereby rendering that solution useless.</p>
----------------------------------------------------------------------
In diva2:1698159  - Note: no full text in DiVA

abstract is:
<p>Improving the performance of aircraft is a challenging topic and a burning issue in a societywho strives to reduce as much as possible CO2 emissions. A better performance could leadto a reduction of the fuel burnt during flight and thus to a more sustainable aviation. It istherefore crucial to have a clear understanding of all the parameters that have an impact onaircraft performance.This project aims at estimating two unknown parameters for aircraft manufacturers: theCost Index and the take-off weight. These two parameters contain sensitive information forairlines, which prevents them from sharing the parameters with manufacturers. However,these parameters have a strong impact on aircraft performance and also need to be estimated.The performance parameters of in-service aircraft have been collected for several flights andstudied. Reverse engineering is used on each high speed phase of the flight (climb, cruise,descent) to deduce the Cost Index and the weight. The whole methodology has been testedon simulated flights where the Cost Index and weight were known.The Cost Index and take-off weight have been estimated for simulated flights. Regardingreal flights, it has been possible to either deduce both parameters for most of the flights, orone of them or otherwise a relationship between both parameters has been found.</p>

corrected abstract:
<p>Improving the performance of aircraft is a challenging topic and a burning issue in a society who strives to reduce as much as possible CO<sub>2</sub> emissions. A better performance could lead to a reduction of the fuel burnt during flight and thus to a more sustainable aviation. It is therefore crucial to have a clear understanding of all the parameters that have an impact on aircraft performance. This project aims at estimating two unknown parameters for aircraft manufacturers: the Cost Index and the take-off weight. These two parameters contain sensitive information for airlines, which prevents them from sharing the parameters with manufacturers. However, these parameters have a strong impact on aircraft performance and also need to be estimated. The performance parameters of in-service aircraft have been collected for several flights and studied. Reverse engineering is used on each high speed phase of the flight (climb, cruise, descent) to deduce the Cost Index and the weight. The whole methodology has been tested on simulated flights where the Cost Index and weight were known. The Cost Index and take-off weight have been estimated for simulated flights. Regarding real flights, it has been possible to either deduce both parameters for most of the flights, or one of them or otherwise a relationship between both parameters has been found.</p>
----------------------------------------------------------------------
In diva2:1673875 - unnecessary hyphen in title:
"Fluid flow features in swirl injectors for ethanol fueled rocket: - Analysis using computational fluid dynamics"
===>
"Fluid flow features in swirl injectors for ethanol fueled rocket: Analysis using computational fluid dynamics"

abstract is:
<p>A swirl injector for a rocket engine being developed by \emph{AESIR} (Association of EngineeringStudents in Rocketry) was simulated with different geometric parameters. The swirl injector is usedto atomize the ethanol used as fuel and to create a spray that mixes well with the oxidizer withinthe combustion chamber. Inlet slot angle (90, 75, 60 and 45 degrees), swirl chamber length (15, 20and 25 mm) and outlet orifice diameter (3, 6 and 9 mm) were examined.Previous studies in swirl injectors show that CFD can be used to analyze the flow in such aninjector, furthermore theoretical models exist that can predict some of the general characteristicsof the flow. Previous studies have also simulated transient behavior and flow features effectingbreakup of fuel flowing through a swirl injector.A steady state simulation using Volume of Fluid (VOF) multiphase modeling and $k$-$\omega$ \emph{SST}turbulence modeling was used to simulate the swirl injector intended for the rocket engine. It wasfound that a wider outlet orifice would give a wider cone angle of spray. This is desirable in thecurrent rocket engine design as it will promote greater mixing of fuel and oxidizer higher up in thecombustion chamber. No large variances was observed when different inlet slot angles was simulated. Ashorter swirl chamber length reduced the amount of losses in energy due to viscous forces. The flowafter the outlet orifice was not simulated so the effect of turbulence kinetic energy and energylosses outside of the swirl injector have not been analyzed, previous studies have indicated thatturbulent kinetic energy does have an effect on the breakup and atomization of the fuel.It was concluded that using a wider outlet orifice of 9 mm gave the best results out of the differentgeometric parameters analyzed and the swirl chamber length should be a short as possible.</p><p> </p>

corrected abstract:
<p>A swirl injector for a rocket engine being developed by <em>AESIR</em> (Association of Engineering Students in Rocketry) was simulated with different geometric parameters. The swirl injector is used to atomize the ethanol used as fuel and to create a spray that mixes well with the oxidizer within the combustion chamber. Inlet slot angle (90, 75, 60 and 45 degrees), swirl chamber length (15, 20 and 25 mm) and outlet orifice diameter (3, 6 and 9 mm) were examined.</p><p>Previous studies in swirl injectors show that CFD can be used to analyze the flow in such an injector, furthermore theoretical models exist that can predict some of the general characteristics of the flow. Previous studies have also simulated transient behavior and flow features effecting breakup of fuel flowing through a swirl injector.</p><p>A steady state simulation using Volume of Fluid (VOF) multiphase modeling and <em>k-&omega;</em> <em>SST</em> turbulence modeling was used to simulate the swirl injector intended for the rocket engine. It was found that a wider outlet orifice would give a wider cone angle of spray. This is desirable in the current rocket engine design as it will promote greater mixing of fuel and oxidizer higher up in the combustion chamber. No large variances was observed when different inlet slot angles was simulated. A shorter swirl chamber length reduced the amount of losses in energy due to viscous forces. The flow after the outlet orifice was not simulated so the effect of turbulence kinetic energy and energy losses outside of the swirl injector have not been analyzed, previous studies have indicated that turbulent kinetic energy does have an effect on the breakup and atomization of the fuel.</p><p>It was concluded that using a wider outlet orifice of 9 mm gave the best results out of the different geometric parameters analyzed and the swirl chamber length should be a short as possible.</p>
----------------------------------------------------------------------
In diva2:1671528 - Note: no full text in DiVA
abstract is:
<p>Studies estimate that as much as 95% of people would benefit from eyeglasses.This means having eye tracking work well with eyeglasses is key to making thetechnology universal. Unfortunately, eyeglasses make eye tracking challenging.One common issue is the large reflections caused by anti-reflective coatedeyeglasses. Though they may have good performance in the visible spectrum,anti-reflective coatings are highly reflective under near-infrared illumination.In this thesis, we investigate using circularly polarized illumination to combatthe size and intensity of the glasses reflections.</p><p>Polarization is one of the only properties of light that we cannot perceivewith the naked eye. Thus, we have built an imaging polarimeter that allows usto easily visualize polarization state. We then use a well-known optical system,the optical isolator, to separate the glasses reflections. We also looked at howincidence angle and specific coating affected our ability to reduce reflections.</p><p>Our results show that we can reduce the intensity of glasses reflectionsby 99%. However, in general, it is difficult to reduce the glasses reflectionswithout sacrificing the some of the integrity of the glint. The various coatingsalso behaved differently under circularly polarized illumination. There arespecific cases where glasses reflections were reduced, while the glint wasonly minimally affected. This thesis also serves as a jumping-off point usepolarization in future iterations of eye trackers.</p>

corrected abstract:
<p>Studies estimate that as much as 95% of people would benefit from eyeglasses. This means having eye tracking work well with eyeglasses is key to making the technology universal. Unfortunately, eyeglasses make eye tracking challenging. One common issue is the large reflections caused by anti-reflective coated eyeglasses. Though they may have good performance in the visible spectrum, anti-reflective coatings are highly reflective under near-infrared illumination. In this thesis, we investigate using circularly polarized illumination to combat the size and intensity of the glasses reflections.</p><p>Polarization is one of the only properties of light that we cannot perceive with the naked eye. Thus, we have built an imaging polarimeter that allows us to easily visualize polarization state. We then use a well-known optical system, the optical isolator, to separate the glasses reflections. We also looked at how incidence angle and specific coating affected our ability to reduce reflections.</p><p>Our results show that we can reduce the intensity of glasses reflections by 99%. However, in general, it is difficult to reduce the glasses reflections without sacrificing the some of the integrity of the glint. The various coatings also behaved differently under circularly polarized illumination. There are specific cases where glasses reflections were reduced, while the glint was only minimally affected. This thesis also serves as a jumping-off point use polarization in future iterations of eye trackers.</p>
----------------------------------------------------------------------
In diva2:1648276 - Note: no full text in DiVA
abstract is:
<p>In This Master thesis report, three-dimensional Direct Numerical Simulations are usedto characterize natural convection in a Rayleigh-Bénard cavity with roughness on thebottom hot plate made of square based blocks. The cavity is filled with water and DNSwere conducted for different Rayleigh numbers from 1.105 up to 1.1010.As stated in the literature, three different regimes of heat transfer are identified in thiscavity. The regime II present a particularly intense heat transfer efficiency compared toregime III. We highlight all along different approaches, differences between the two platesthat might explain such difference in heat transfer regimes. A particular attention will beput on plumes as they carry heat.To do so, a first analysis is performed to find typical regions of plumes formation alongthe z-axis of the cavity . An extended database is hence computed for a finer vision ofthe inner and the outer mixing zone. A first approach of the identification of plumes isperformed temporally, regime II is thus characterized with the heat advected by plumes,and explain the difference in heat transfer regimes.A proof of concept is then done to extract plumes spatially using Topology DataAnalysis for a finer identification and finer statistics.Several results allowed us to understand the differences in heat transfer from the tworegimes, like the scaling with the convective velocity and temperature. In an other hand,the plumes analysis highlighted how heat was exchanged between the two plates.</p>


corrected abstract:
<p>In This Master thesis report, three-dimensional Direct Numerical Simulations are used to characterize natural convection in a Rayleigh-Bénard cavity with roughness on the bottom hot plate made of square based blocks. The cavity is filled with water and DNS were conducted for different Rayleigh numbers from 1.105 up to 1.1010.As stated in the literature, three different regimes of heat transfer are identified in this cavity. The regime II present a particularly intense heat transfer efficiency compared to regime III. We highlight all along different approaches, differences between the two plates that might explain such difference in heat transfer regimes. A particular attention will be put on plumes as they carry heat. To do so, a first analysis is performed to find typical regions of plumes formation along the z-axis of the cavity . An extended database is hence computed for a finer vision of the inner and the outer mixing zone. A first approach of the identification of plumes is performed temporally, regime II is thus characterized with the heat advected by plumes, and explain the difference in heat transfer regimes. A proof of concept is then done to extract plumes spatially using Topology DataAnalysis for a finer identification and finer statistics. Several results allowed us to understand the differences in heat transfer from the two regimes, like the scaling with the convective velocity and temperature. In an other hand, the plumes analysis highlighted how heat was exchanged between the two plates.</p>
----------------------------------------------------------------------
In diva2:1640036 - Note: no full text in DiVA
abstract is:
<p>The focus of this thesis is the effect of planetary curvature on the heat transfer efficiencyaccross latitudes in planetary atmospheres and oceans. We investigate both theoreticallyand numerically a physically-based parametrization of baroclinic turbulence inthe Boussinesq Eady model to include variations of the Coriolis parameter with latitude.In this model, a rapidly rotating density-stratified fluid is subjected to a meridional temperaturegradient in thermal wind balance with a uniform vertically sheared zonal flowand the effect of planetary curvature is captured by the parameter β.A normal mode projection of the Eady model with β was inconclusive to properlydescribe meridional heat transfers and the zonal structures usually observed in planetaryflows. However, the DNS solver CORAL allows us to perform 3D high-Reynolds numericalsimulations to seek an extension of the ’vortex-gas’ scaling theory for baroclinic turbulence.Planetary curvature reduces heat transfer between latitudes through the emergenceof coherent zonal structures while the flow remain mainly quasi-geostrophic. The meridionalbuoyancy flux displays the same functional dependence on the control parametersthan for the two-layer model within the framework of quasi-geostrophy.With similar arguments than for the Eady problem, it is shown that in a perturbativefashion for small β, the vertical profiles of meridional buoyancy flux are no longer depthinvariant.The flux decreases at least exponentially with height. The buoyancy transportis shown to be along mean isopycnals, whereas potential vorticity is transported onlyalong instantaneous isopycnals. Overall, the vortex-gas theory and its extension to theβ-plane lead to good predictions for heat transfer in the quasi-geostrophy limit for 3Dflows and weak β. The theory becomes less precise as we increase β.</p>

corrected abstract:
<p>The focus of this thesis is the effect of planetary curvature on the heat transfer efficiency across latitudes in planetary atmospheres and oceans. We investigate both theoretically and numerically a physically-based parametrization of baroclinic turbulence inthe Boussinesq Eady model to include variations of the Coriolis parameter with latitude. In this model, a rapidly rotating density-stratified fluid is subjected to a meridional temperature gradient in thermal wind balance with a uniform vertically sheared zonal flowand the effect of planetary curvature is captured by the parameter β.A normal mode projection of the Eady model with β was inconclusive to properly describe meridional heat transfers and the zonal structures usually observed in planetary flows. However, the DNS solver CORAL allows us to perform 3D high-Reynolds numerical simulations to seek an extension of the ’vortex-gas’ scaling theory for baroclinic turbulence. Planetary curvature reduces heat transfer between latitudes through the emergence of coherent zonal structures while the flow remain mainly quasi-geostrophic. The meridional buoyancy flux displays the same functional dependence on the control parameters than for the two-layer model within the framework of quasi-geostrophy. With similar arguments than for the Eady problem, it is shown that in a perturbative fashion for small β, the vertical profiles of meridional buoyancy flux are no longer depth invariant. The flux decreases at least exponentially with height. The buoyancy transport is shown to be along mean isopycnals, whereas potential vorticity is transported only along instantaneous isopycnals. Overall, the vortex-gas theory and its extension to theβ-plane lead to good predictions for heat transfer in the quasi-geostrophy limit for 3Dflows and weak β. The theory becomes less precise as we increase β.</p>
----------------------------------------------------------------------
In diva2:1511011 - Note: no full text in DiVA
abstract is:
<p>X-ray phase-contrast imaging is a powerful technique, which, due to its high sensitivity to density alterations, allows carrying out high-contrast imaging of soft tissue. The lungs are especially suitable for being examined with phasecontrast due to their compartments being largely air-filled. Thus, phasecontrastimaging allows to visualize structures down to the alveolar level, providing microscopic information about lung anatomy and functionality. Asthe lungs are moving when conducting in-vivo experiments, the aim of thisthesis is to develop and implement respiration-gating into an already existinglaboratory small animal X-ray imaging setup to improve the image resolutionby compensating for respiratory motion. Additionally, the radiation dose formice is aimed to be suitable for longitudinal in-vivo imaging.</p><p>A prospective gating approach was implemented into the imaging setup by allowing the different hardware components (e.g., camera, rotation stage) to communicate with each other. A pressure sensor was used to register respiratorymotion and to trigger a microcontroller when exceeding a certainrespiratory threshold. The micro-controller then transmits pulses to the devicesin the imaging setup, triggering them sequentially and thus allowingto perform a tomographic scan. In addition to programming the microcontroller,a graphical user interface (GUI) was designed. The system was then tested, optimized and evaluated using two different phantoms developed inthe course of this thesis, whereupon in-vivo experiments on free breathingmice were conducted.</p><p>The evaluation of the gating implementation on the phantoms indicates a significantimprovement of the image quality and thus a successful compensation for respiratory motion. This effective execution could subsequently be appliedto free breathing mice, allowing to identify airway features in the murine lungdown to the 50-100 μm range. By only allowing X-ray exposure during imageacquisition, the dose could be reduced to a level where repeated imaging ofthe same mouse becomes possible.</p>

corrected abstract:
<p>X-ray phase-contrast imaging is a powerful technique, which, due to its high sensitivity to density alterations, allows carrying out high-contrast imaging of soft tissue. The lungs are especially suitable for being examined with phasecontrast due to their compartments being largely air-filled. Thus, phasecontrastimaging allows to visualize structures down to the alveolar level, providing microscopic information about lung anatomy and functionality. Asthe lungs are moving when conducting in-vivo experiments, the aim of this thesis is to develop and implement respiration-gating into an already existing laboratory small animal X-ray imaging setup to improve the image resolution by compensating for respiratory motion. Additionally, the radiation dose for mice is aimed to be suitable for longitudinal in-vivo imaging.</p><p>A prospective gating approach was implemented into the imaging setup by allowing the different hardware components (e.g., camera, rotation stage) to communicate with each other. A pressure sensor was used to register respiratory motion and to trigger a microcontroller when exceeding a certain respiratory threshold. The micro-controller then transmits pulses to the devices in the imaging setup, triggering them sequentially and thus allowing to perform a tomographic scan. In addition to programming the microcontroller, a graphical user interface (GUI) was designed. The system was then tested, optimized and evaluated using two different phantoms developed inthe course of this thesis, where upon in-vivo experiments on free breathing mice were conducted.</p><p>The evaluation of the gating implement ation on the phantoms indicates a significant improvement of the image quality and thus a successful compensation for respiratory motion. This effective execution could subsequently be applied to free breathing mice, allowing to identify airway features in the murine lung down to the 50-100 μm range. By only allowing X-ray exposure during image acquisition, the dose could be reduced to a level where repeated imaging of the same mouse becomes possible.</p>
----------------------------------------------------------------------
In diva2:1509441 - Note: no full text in DiVA
abstract is:
<p>development of innovative Adaptive Optics (AO) systems for the next generation of gianttelescopes and instruments poses new instrumental questions essential to achieving this astrophysicalgoal. This type of observation indeed involves the use of dedicated instruments,combining a large telescope, an extreme adaptive optics (XAO) system, a coronograph physicallyremoving the photons from the star, optimized instrumentation, but also an efficientdata processing methodology.A new wavefront measurement method, based on a self-referenced Mach-Zehner interfermoter,a Mach-Zehnder interferometer that creates its own reference wave using a spatialfilter referred to as pinehole, is being studied at CRAL. In order to study its behavior andhelp predict the results that could be achieved, a simulation of this interferometer has beenrealized. Developing an analytical model, we can argue that this method is supposed to beworking for small values of phase and pinehole radius. The first part of the simulation isto verify whether or not this statement is true. To simulate the atmospheric aberrations,Zernike polynomials are used (only the ones optically interpreted as tip/tilt and defocus).Then, in a second time, a process of modulation is implemented to try and enhance theperformance of the simulation towards the high phase values.The results obtained with this simulation are analyzed both qualitatively, with visual comparisonat each step of the process, and quantitatively by computing the Strehl ratio, ratiobetween the maximum intensity of the measured PSF and the PSF as if it were perfect withoutaberrations. Hence, the evolution of the Strehl ratio with respect to both the variationof the pinehole radius and the variation of the amplitude of the aberration can be plotted.</p>

corrected abstract:
<p>Development of innovative Adaptive Optics (AO) systems for the next generation of giant telescopes and instruments poses new instrumental questions essential to achieving this astrophysical goal. This type of observation indeed involves the use of dedicated instruments, combining a large telescope, an extreme adaptive optics (XAO) system, a coronograph physically removing the photons from the star, optimized instrumentation, but also an efficient data processing methodology. A new wavefront measurement method, based on a self-referenced Mach-Zehner interferometer, a Mach-Zehnder interferometer that creates its own reference wave using a spatial filter referred to as pinhole, is being studied at CRAL. In order to study its behavior and help predict the results that could be achieved, a simulation of this interferometer has been realized. Developing an analytical model, we can argue that this method is supposed to be working for small values of phase and pinhole radius. The first part of the simulation is to verify whether or not this statement is true. To simulate the atmospheric aberrations, Zernike polynomials are used (only the ones optically interpreted as tip/tilt and defocus). Then, in a second time, a process of modulation is implemented to try and enhance theperformance of the simulation towards the high phase values. The results obtained with this simulation are analyzed both qualitatively, with visual comparison at each step of the process, and quantitatively by computing the Strehl ratio, ratio between the maximum intensity of the measured PSF and the PSF as if it were perfect without aberrations. Hence, the evolution of the Strehl ratio with respect to both the variation of the pinhole radius and the variation of the amplitude of the aberration can be plotted.</p>
----------------------------------------------------------------------
In diva2:1285492 abstract is:
<p>There is a need for renewable energy sources that can replace the non-renewable energy sourcesthat we use today. This is on the agenda as one of the United Nations sustainable developmentgoals. Embracing new technologies is addressed as one of the ways of achieving aordable,reliable, sustainable and modern energy for all. Oshore wind power has great potential as anenergy source, and development of the oating solutions is of special importance.In this report, key design parameters of oating oshore wind turbines are identied based ona literature study on research projects as well as on-going test turbines and wind farms. Thekey design parameters should be used for determining the type of technology suitable for aproject, as well as for guidance in the design phase.Based on the key design parameters, a conceptual design of a semisubmersible substructurehas been made for the DTU 10 MW reference wind turbine for a site outside the island ofBarra, west of Scotland. The substructure is a three column semisubmersible connected witha closed shape pontoon and no bracing. The wind turbine is placed on top of one of the threecolumns to reduce number of columns and utilize more of the structure.Variation of the column diameter and distance between the columns has been studied to ndsuitable main dimensions. Mass estimations has been made and the required amount of ballasthas been calculated for a set of combinations to select a conguration for further analysis.Hydrostatic and hydrodynamic analysis has been performed on the design to understand itscharacteristics in the ocean environment. Intact stability is considered in the hydrostatic analysis,and the hydrodynamic analysis includes a study of the motions, loads and accelerationsof the structure.</p>

corrected abstract:
<p>There is a need for renewable energy sources that can replace the non-renewable energy sources that we use today. This is on the agenda as one of the United Nations sustainable development goals. Embracing new technologies is addressed as one of the ways of achieving affordable, reliable, sustainable and modern energy for all. Offshore wind power has great potential as an energy source, and development of the floating solutions is of special importance.</p><p>In this report, key design parameters of floating offshore wind turbines are identified based on a literature study on research projects as well as on-going test turbines and wind farms. The key design parameters should be used for determining the type of technology suitable for a project, as well as for guidance in the design phase.</p><p>Based on the key design parameters, a conceptual design of a semisubmersible substructure has been made for the DTU 10 MW reference wind turbine for a site outside the island of Barra, west of Scotland. The substructure is a three column semisubmersible connected with a closed shape pontoon and no bracing. The wind turbine is placed on top of one of the three columns to reduce number of columns and utilize more of the structure.</p><p>Variation of the column diameter and distance between the columns has been studied to find suitable main dimensions. Mass estimations has been made and the required amount of ballast has been calculated for a set of combinations to select a configuration for further analysis. Hydrostatic and hydrodynamic analysis has been performed on the design to understand its characteristics in the ocean environment. Intact stability is considered in the hydrostatic analysis, and the hydrodynamic analysis includes a study of the motions, loads and accelerations of the structure.</p>
----------------------------------------------------------------------
In diva2:1249681 abstract is:
<p>This master thesis uses applied mathematicalstatistics to analyse purchase behaviour based on customer data of the Swedishbrand Indiska. The aim of the study is to build a model that can helppredicting the sales quantities of different product classes and identify whichfactors are the most significant in the different models and furthermore, tocreate an algorithm that can provide suggested product combinations in thepurchasing process. Generalized linear models with a Negative binomial distributionare applied to retrieve the predicted sales quantity. Moreover, conditionalprobability is used in the algorithm which results in a product recommendationengine based on the calculated conditional probability that the suggestedcombinations are purchased.From the findings, it can be concluded that all variables considered in themodels; original price, purchase month, colour, cluster, purchase country andchannel are significant for the predicted outcome of the sales quantity foreach product class. Furthermore, by using conditional probability andhistorical sales data, an algorithm can be constructed which createsrecommendations of product combinations of either one or two products that canbe bought together with an initial product that a customer shows interest in.  </p>

corrected abstract:
<p>This master thesis uses applied mathematical statistics to analyse purchase behaviour based on customer data of the Swedish brand Indiska. The aim of the study is to build a model that can help predicting the sales quantities of different product classes and identify which factors are the most significant in the different models and furthermore, to create an algorithm that can provide suggested product combinations in the purchasing process. Generalized linear models with a Negative binomial distribution are applied to retrieve the predicted sales quantity. Moreover, conditional probability is used in the algorithm which results in a product recommendation engine based on the calculated conditional probability that the suggested combinations are purchased.</p><p>From the findings, it can be concluded that all variables considered in the models; original price, purchase month, colour, cluster, purchase country and channel are significant for the predicted outcome of the sales quantity for each product class. Furthermore, by using conditional probability and historical sales data, an algorithm can be constructed which creates recommendations of product combinations of either one or two products that can be bought together with an initial product that a customer shows interest in.</p>
----------------------------------------------------------------------
In diva2:1142918 abstract is:
<p>A video streaming service faces several difficultiesoperating. Hardware is expensive and it is crucial to prioritizecustomers in a way that will make them content with the serviceprovided. That is, deliver a sufficient frame rate and neverallocate too much, essentially waste, resources on a client. Thisallocation has to be done several times per second so readingdata from the client is out of the question, because the systemwould be adapting too slow. This raises the question whether it ispossible to predict the frame rate of a client using only variablesmeasured on the server and if it can be done efficiently. Which itcan [1]. To further build on the work of Yanggratoke et al [1], weevaluated several different machine learning methods on a dataset in terms of performance, training time and dependence on thesize of the data set. Neural networks, having the best adaptingcapabilities, resulted in the best performance but training is moretime consuming than for the linear model. Using neural networksis a good idea when the relationship between input and outputis not linear.</p>

corrected abstract:
<p>A video streaming service faces several difficulties operating. Hardware is expensive and it is crucial to prioritize customers in a way that will make them content with the service provided. That is, deliver a sufficient frame rate and never allocate too much, essentially waste, resources on a client. This allocation has to be done several times per second so reading data from the client is out of the question, because the system would be adapting too slow. This raises the question whether it is possible to predict the frame rate of a client using only variables measured on the server and if it can be done efficiently. Which it can [1]. To further build on the work of Yanggratoke et al [1], we evaluated several different machine learning methods on a data set in terms of performance, training time and dependence on the size of the data set. Neural networks, having the best adapting capabilities, resulted in the best performance but training is more time consuming than for the linear model. Using neural networks is a good idea when the relationship between input and output is not linear.</p>
----------------------------------------------------------------------
In diva2:1142914 abstract is:
<p>Machine learning and neural networks haverecently become hot topics in many research areas. They havealready proved to be useful in the fields of medicine andbiotechnology. In these areas, they can be used to facilitatecomplicated and time consuming analysis processes. Animportant application is image recognition of cells, tumours etc.,which also is the focus of this paper.Our project was to construct both Fully Connected NeuralNetworks and Convolutional Neural Networks with the ability torecognize pictures of muscular stem cells (MuSCs). We wanted toinvestigate if the intensity values in each pixel of the images weresufficient to use as indata for classification.By optimizing the structure of our networks, we obtained goodresults. Using only the pixel values as input, the pictures werecorrectly classified with up to 95.1% accuracy. If the image sizewas added to the indata, the accuracy was as best 97.9 %.The conclusion was that it is sensible and practical to use pixelintensity values as indata to classification programs. Importantrelationships exist and by adding some other easily accessiblecharacteristics, the success rate can be compared to a human’sability to classify these cells.</p>

corrected abstract:
<p>Machine learning and neural networks have recently become hot topics in many research areas. They have already proved to be useful in the fields of medicine and biotechnology. In these areas, they can be used to facilitate complicated and time consuming analysis processes. An important application is image recognition of cells, tumours etc., which also is the focus of this paper.</p><p>Our project was to construct both Fully Connected Neural Networks and Convolutional Neural Networks with the ability to recognize pictures of muscular stem cells (MuSCs). We wanted to investigate if the intensity values in each pixel of the images were sufficient to use as indata for classification.</p><p>By optimizing the structure of our networks, we obtained good results. Using only the pixel values as input, the pictures were correctly classified with up to 95.1% accuracy. If the image size was added to the indata, the accuracy was as best 97.9 %.</p><p>The conclusion was that it is sensible and practical to use pixel intensity values as indata to classification programs. Important relationships exist and by adding some other easily accessible characteristics, the success rate can be compared to a human’s ability to classify these cells.</p>
----------------------------------------------------------------------
In diva2:1088402 abstract is:
<p>This masters thesis has two aims. The first is to assemble literature regarding the calculation of crackwidths and crack control in reinforced concrete structures. The second aim is to develop a calculationmethodology to design required reinforcement from data produced by the calculation programBRIGADE.There are several reasons why cracks appear in reinforced concrete structures. Except for externalloads, they appear as a result of attack from frost, seawater or chemicals. Also shrinkage or changesin temperature as the concrete hardens can cause cracks in the structure. Crack control in structuresis desirable for a range of reasons including aesthetics, life-span and the ability to provide water orgas proofing. Crack widths can be controlled by providing sufficient reinforcement in a structure andby arranging the reinforcement in the way it has most effect. Additionally, while pouring theconcrete, there are ways to affect the number of cracks in a structure. Treating the concrete in thecorrect way after pouring and also the use of chemicals can reduce cracking.This thesis considers the calculation of crack widths according to British Standard, Eurocode, ACI andBBK 04. Through calculation of crack widths according to these four standards, where one parameterat a time is varied, general conclusions can be drawn. Reducing the reinforcement spacing makes amajor difference to the calculated crack width. Also larger bar diameters give better results, while asmaller cover within the boundaries that the standards permit results in no significant effect.</p>

corrected abstract:
<p>This masters thesis has two aims. The first is to assemble literature regarding the calculation of crack widths and crack control in reinforced concrete structures. The second aim is to develop a calculation methodology to design required reinforcement from data produced by the calculation program BRIGADE.</p><p>There are several reasons why cracks appear in reinforced concrete structures. Except for external loads, they appear as a result of attack from frost, seawater or chemicals. Also shrinkage or changes in temperature as the concrete hardens can cause cracks in the structure. Crack control in structures is desirable for a range of reasons including aesthetics, life-span and the ability to provide water or gas proofing. Crack widths can be controlled by providing sufficient reinforcement in a structure and by arranging the reinforcement in the way it has most effect. Additionally, while pouring the concrete, there are ways to affect the number of cracks in a structure. Treating the concrete in the correct way after pouring and also the use of chemicals can reduce cracking.</p><p>This thesis considers the calculation of crack widths according to British Standard, Eurocode, ACI and BBK 04. Through calculation of crack widths according to these four standards, where one parameter at a time is varied, general conclusions can be drawn. Reducing the reinforcement spacing makes a major difference to the calculated crack width. Also larger bar diameters give better results, while a smaller cover within the boundaries that the standards permit results in no significant effect.</p>
----------------------------------------------------------------------
In diva2:1083784 abstract is:
<p>Different aspects of numerical simulations of turbulent flows are assessed by consideringa fully-developed turbulent channel flow that is rotating in the spanwise direction.Differences between differential and explicit algebraic Reynolds-stress models(RSMs) are investigated theoretically and numerically. Simulation results are comparedwith existing DNS-data. Both families of RSMs are demonstrated to achievegood qualitative agreement with the DNS. The results constitutes a demonstration ofthe validity of the so called extended weak-equilibrium assumption for systems witha superimposed solid body rotation. An original derivation, based on sound physicalgrounds, of the extended weak-equilibrium assumption is presented.It is further examined if the roll-cell vortex pattern, that constitutes a secondaryflow field, has an influence on the averaged solutions obtained by application of theReynolds-Averaged Navier-Stokes equations. This is assessed by comparison of resultsobtained by either considering the secondary plane as homogeneous in the spanwisedirection or by accounting for a fully three-dimensional flow field. Simulationsdemonstrate that existence of roll-cells in the latter case yields results that are in closeragreement with DNS-data compared with if they are suppressed as for the former case.Aspects of numerical treatment of explicit source terms are also assessed in theframework of finite volume methods for collocated grids.</p>

corrected abstract:
<p>Different aspects of numerical simulations of turbulent flows are assessed by considering a fully-developed turbulent channel flow that is rotating in the spanwise direction. Differences between differential and explicit algebraic Reynolds-stress models (RSMs) are investigated theoretically and numerically. Simulation results are compared with existing DNS-data. Both families of RSMs are demonstrated to achieve good qualitative agreement with the DNS. The results constitutes a demonstration of the validity of the so called <em>extended</em> weak-equilibrium assumption for systems with a superimposed solid body rotation. An original derivation, based on sound physical grounds, of the <em>extended</em> weak-equilibrium assumption is presented.</p><p>It is further examined if the roll-cell vortex pattern, that constitutes a secondary flow field, has an influence on the averaged solutions obtained by application of the Reynolds-Averaged Navier-Stokes equations. This is assessed by comparison of results obtained by either considering the secondary plane as homogeneous in the spanwise direction or by accounting for a fully three-dimensional flow field. Simulations demonstrate that existence of roll-cells in the latter case yields results that are in closer agreement with DNS-data compared with if they are suppressed as for the former case.</p><p>Aspects of numerical treatment of explicit source terms are also assessed in the framework of finite volume methods for collocated grids.</p>
----------------------------------------------------------------------
In diva2:503959 - Note: no full text in DiVA
abstract is:
<p>In this Master’s thesis a method to automatically classify log files containing recorded vehicle data from Scania trucks is investigated.The files were recordings of data from the on-board controller area network(CAN)-buses and were gathered from field test vehicles during normal operating conditions. All the recordings were triggered by the embedded collision warning system. The test vehicles were equipped with a forward looking radar giving information about the surrounding traffic. Seven classes of traffic situations were proposed as classes to try to detect automatically. These classes were Queue, Vehicle ahead turns left,Vehicle ahead turns right, Roundabout, Catch up, Overtake and Overtakebicycle.Using Matlab, an evaluation tool was made to handle and analyse the log files. The tool converts the log files in all necessary steps in orderto analyse and classify the contents of each file. The results is presented in plots and also stored in a file for later review.To classify the files automatically, three methods have been tested and two of the methods have been used for this task. The first method uses a set of conditions on the signals to determine the class, the secondmethod uses a decision tree to differentiate the classes. A cluster analysisof the log files showed not to be an effective method of classifying thefiles and was therefore not fully developed.The result showed that the classification rules detected over 85%of the 192 available situations compared to the “decision tree”-methodwhich detected a maximum of 75% of the situations. The drawback ofthe first method was that the more correct detections also meant morefalse detections. The specificity was used as a measure of how well the method was able to leave the non-existing situations undetected. Thespecificity for the “classification rules”-method was approximately 85%and for the decision tree the specificity was approximately 95%.The conclusion is that both the “classification rules”- and the “decisiontree”-method can be used to classify the log files, however thefirst method produces more detections but less specific, and vice versafor the latter. The methods can be further developed using more of theavailable signals or by working with the specific conditions used by themethods.</p>

corrected abstract:
<p>In this Master’s thesis a method to automatically classify log files containing recorded vehicle data from Scania trucks is investigated. The files were recordings of data from the on-board controller area network (CAN)-buses and were gathered from field test vehicles during normal operating conditions. All the recordings were triggered by the embedded collision warning system. The test vehicles were equipped with a forward looking radar giving information about the surrounding traffic. Seven classes of traffic situations were proposed as classes to try to detect automatically. These classes were Queue, Vehicle ahead turns left, Vehicle ahead turns right, Roundabout, Catch up, Overtake and Overtake bicycle. Using Matlab, an evaluation tool was made to handle and analyse the log files. The tool converts the log files in all necessary steps in order to analyse and classify the contents of each file. The results is presented in plots and also stored in a file for later review. To classify the files automatically, three methods have been tested and two of the methods have been used for this task. The first method uses a set of conditions on the signals to determine the class, the second method uses a decision tree to differentiate the classes. A cluster analysis of the log files showed not to be an effective method of classifying the files and was therefore not fully developed. The result showed that the classification rules detected over 85% of the 192 available situations compared to the “decision tree”-method which detected a maximum of 75% of the situations. The drawback of the first method was that the more correct detections also meant more false detections. The specificity was used as a measure of how well the method was able to leave the non-existing situations undetected. The specificity for the “classification rules”-method was approximately 85% and for the decision tree the specificity was approximately 95%. The conclusion is that both the “classification rules” - and the “decision tree” - method can be used to classify the log files, however the first method produces more detections but less specific, and vice versa for the latter. The methods can be further developed using more of the available signals or by working with the specific conditions used by themethods.</p>
----------------------------------------------------------------------
In diva2:488441 abstract is:
<p>Optical Coherence Tomography (OCT) is a non-invasive high-resolutionmethod for measuring the reectance of scattering media in 1/2/3D, e.g.skin. The method has been used in a number of dierent medical elds andfor measurement of tissue optical properties.The software developed in this thesis is able to display features hidden ina shadowed volume by adding multiple OCT measurements taken at obliqueangles, a technique here called Multiple-Angle Oblique Optical CoherenceTomography (MAO-OCT).Three dierent objects with were measured at 5 to 9 angles. The measurementswere automatically and manually aligned in the software. They werealso tested with 6 dierent high pass intensity lters (HPIF) and reduced insize using 4 dierent methods to speed up calculations.The software's automatic alignment was tested with one tilted computergenerated test at 9 angles and with 5 dierent shadow strengths.With MAO-OCT it is possible to remove some eects of shadows in OCT,though it comes with a cost of reduced sharpness. The errors depend muchon the dierences in index of refraction in the sample.The software managed to automatically align 90% of the articial measurements,and 60% of the OCT measurements. The shadow strength andthe resize method had no noticeable eect on the automatic alignment of themeasurements.</p>

corrected abstract:
<p>Optical Coherence Tomography (OCT) is a non-invasive high-resolution method for measuring the reflectance of scattering media in 1/2/3D, e.g. skin. The method has been used in a number of different medical fields and for measurement of tissue optical properties.</p><p>The software developed in this thesis is able to display features hidden in a shadowed volume by adding multiple OCT measurements taken at oblique angles, a technique here called Multiple-Angle Oblique Optical Coherence Tomography (MAO-OCT).</p><p>Three different objects with were measured at 5 to 9 angles. The measurements were automatically and manually aligned in the software. They were also tested with 6 different high pass intensity filters (HPIF) and reduced in size using 4 different methods to speed up calculations.</p><p>The software's automatic alignment was tested with one tilted computer generated test at 9 angles and with 5 different shadow strengths.</p><p>With MAO-OCT it is possible to remove some effects of shadows in OCT, though it comes with a cost of reduced sharpness. The errors depend much on the differences in index of refraction in the sample.</p><p>The software managed to automatically align 90% of the artificial measurements, and 60% of the OCT measurements. The shadow strength and the resize method had no noticeable effect on the automatic alignment of the measurements.</p>
----------------------------------------------------------------------
In diva2:408838 - possible duplicate of diva2:408837 - one student is common to both, but the duplicate only lists one of the two students.

abstract is:
<p>This thesis is organized in three different parts. In the first part Ericsson’s methods fordeveloping and deploying the existing knowledge are analyzed. In the second part we analyzethe competence build-up for consultants within a technical domain that is constantly evolving.The third part is an evaluation, on an overview level, of a new organizational concept thatEricsson launched. The concept is a way to globally manage knowledge and competencewithin different technical domains. The concept is called Global Competence Center.The method applied was interviews, as a first step to learn about the subject and also forourselves to get to know the organization in which we conducted our research. Interviewswere also used in order to answer our research questions. The employees gave their view onwhat makes learning more efficient. Examples are hands-on, reality based and problemoriented tasks. This was combined with studies of literature and our own experiences oflearning.Our study shows that a combination of different methods for developing and deployingknowledge and building competence seems to be most suitable. This is based on theinterviewees’ experiences and the principles of a theory about adult learning called andragogy.From the interviews, we also received suggestions about work improvements for theconsultants’ work roles. A competence program was developed for the building ofcompetence amongst consultants. The program uses case studies and mentorship as two of themethods, which both apply to the theory of andragogy.</p>

corrected abstract:
<p>This thesis is organized in three different parts. In the first part Ericsson’s methods for developing and deploying the existing knowledge are analyzed. In the second part we analyze the competence build-up for consultants within a technical domain that is constantly evolving. The third part is an evaluation, on an overview level, of a new organizational concept that Ericsson launched. The concept is a way to globally manage knowledge and competence within different technical domains. The concept is called Global Competence Center.</p><p>The method applied was interviews, as a first step to learn about the subject and also for ourselves to get to know the organization in which we conducted our research. Interviews were also used in order to answer our research questions. The employees gave their view on what makes learning more efficient. Examples are hands-on, reality based and problem oriented tasks. This was combined with studies of literature and our own experiences of learning.</p><p>Our study shows that a combination of different methods for developing and deploying knowledge and building competence seems to be most suitable. This is based on the interviewees’ experiences and the principles of a theory about adult learning called andragogy. From the interviews, we also received suggestions about work improvements for the consultants’ work roles. A competence program was developed for the building of competence amongst consultants. The program uses case studies and mentorship as two of the methods, which both apply to the theory of andragogy.</p>
----------------------------------------------------------------------
In diva2:1880984 abstract is:
<p>The study of human locomotion, known as gait analysis, has for a long time been performed withexpensive equipment in laboratory settings. However, the emergence of machine learning sparkedinterest in integrating this technology in gait analysis, thus simplifying the process. This study’saim is to substitute the pressure insoles used during gait cycle analysis of a walking subject, with amachine learning model.To achieve this, a model based on Long-Short Term Memory networks that predicts vertical groundreaction force based on data from inertial measurement unit sensors was used. This serves as asubstitution for pressure insoles or pressure plates. The model was trained with time series datasetscontaining inertial measurement unit data and corresponding pressure insole data. Subsequently, itwas tested for intersubjective, out-of-sample data.The model was able to capture the periodicity of the gait cycle as well as predict the general shapeof the vertical ground reaction force curves, where the accuracy was quantified using normalisedroot mean squared error. The error was in a range between 17.8% and 13.4% and had an average of15.2%, when tested intersubjectively and out-of-sample. The most significant factor contributing tothe error was the model’s amplitude inaccuracies which was, most likely, due to information beinglost during the processing of the data, as well as simply having an insufficient amount of data.</p>

corrected abstract:
<p>The study of human locomotion, known as gait analysis, has for a long time been performed with expensive equipment in laboratory settings. However, the emergence of machine learning sparked interest in integrating this technology in gait analysis, thus simplifying the process. This study’s aim is to substitute the pressure insoles used during gait cycle analysis of a walking subject, with a machine learning model.</p><p>To achieve this, a model based on Long-Short Term Memory networks that predicts vertical ground reaction force based on data from inertial measurement unit sensors was used. This serves as a substitution for pressure insoles or pressure plates. The model was trained with time series datasets containing inertial measurement unit data and corresponding pressure insole data. Subsequently, it was tested for intersubjective, out-of-sample data.</p><p>The model was able to capture the periodicity of the gait cycle as well as predict the general shape of the vertical ground reaction force curves, where the accuracy was quantified using normalised root mean squared error. The error was in a range between 17.8% and 13.4% and had an average of 15.2%, when tested intersubjectively and out-of-sample. The most significant factor contributing to the error was the model’s amplitude inaccuracies which was, most likely, due to information being lost during the processing of the data, as well as simply having an insufficient amount of data.</p>
----------------------------------------------------------------------
In diva2:1877759 abstract is:
<p>Mobile cellular networks are widely integrated in today’s infrastructure. These networks are constantly evolving and continuously expanding, especially with the introduction of fifth-generation (5G). It is important to ensure the effectiveness of these expansions.Mobile networks consist of a set of radio nodes that are distributed in a geographicalregion to provide connectivity services. Each radio node is served by a set of cells. Thehandover relations between cells is determined by Software features such as AutomaticNeighbor Relations (ANR). The handover relations, also refereed as edges, betweenradio nodes in the mobile network graph are created through historical interactions between User Equipment (UE) and radio nodes. The method has the limitation of not being able to set the edges before the physical hardware is integrated. In this work, we usegraph-based deep learning methods to determine mobility relations (edges), trained onradio node configuration data and a set of reliable relations of ANR in stable networks.The report focuses on measuring the accuracy and precision of different graph baseddeep learning approaches applied to real-world mobile networks. The report considers four models. Our comprehensive experiments on Telecom datasets obtained fromoperational Telecom Networks demonstrate that graph neural network model and multilayer perceptron trained with Binary Cross Entropy (BCE) loss outperform all othermodels. The four models evaluation showed that considering graph structure improveresults. Additionally, the model investigates the use of heuristics to reduce the trainingtime based on distance between radio node to eliminate irrelevant cases. The use ofthese heuristics improved precision and accuracy.</p><p> </p>

corrected abstract:
<p>Mobile cellular networks are widely integrated in today’s infrastructure. These networks are constantly evolving and continuously expanding, especially with the introduction of fifth-generation (5G). It is important to ensure the effectiveness of these expansions.</p><p>Mobile networks consist of a set of radio nodes that are distributed in a geographical region to provide connectivity services. Each radio node is served by a set of cells. The handover relations between cells is determined by Software features such as Automatic Neighbor Relations (ANR). The handover relations, also refereed as edges, between radio nodes in the mobile network graph are created through historical interactions between User Equipment (UE) and radio nodes. The method has the limitation of not being able to set the edges before the physical hardware is integrated. In this work, we use graph-based deep learning methods to determine mobility relations (edges), trained on radio node configuration data and a set of reliable relations of ANR in stable networks. The report focuses on measuring the accuracy and precision of different graph based deep learning approaches applied to real-world mobile networks. The report considers four models. Our comprehensive experiments on Telecom datasets obtained from operational Telecom Networks demonstrate that graph neural network model and multilayer perceptron trained with Binary Cross Entropy (BCE) loss outperform all other models. The four models evaluation showed that considering graph structure improve results. Additionally, the model investigates the use of heuristics to reduce the training time based on distance between radio node to eliminate irrelevant cases. The use of these heuristics improved precision and accuracy.</p>
----------------------------------------------------------------------
In diva2:1832656 abstract is:
<p>This thesis aims to investigate the feasibility of using a Markovian approach toforecast short-term stock market movements. To assist traders in making soundtrading decisions, this study proposes a Markovian model using a selection ofthe latest closing prices. Assuming that each time step in the one-minute timeframe of the stock market is stochastically independent, the model eliminates theimpact of fundamental analysis and creates a feasible Markov model. The modeltreats the stock price’s movement as entirely randomly generated, which allowsfor a more simplified model that can be implemented with ease. The modelis intended to serve as a starting ground for more advanced technical tradingstrategies and act as useful guidance for a short-term trader when combinedwith other resources. The creation of the model involves Laplace smoothing toensure there are no zero-probabilities and calculating the steady-state probabilityvector of the smoothed matrix to determine the predicted direction of the nexttime step. The model will reset daily, reducing the impact of fundamental factorsoccurring outside trading hours and reducing the risk of carrying over bias fromprevious trading day. Any open positions will hence be closed at the end of theday. The study’s purpose is to research and test if a simple forecasting modelbased on Markov chains can serve as a useful tool for forecasting stock prices atshort time intervals. The result of the study shows that a Markov-based tradingstrategy is more profitable than a simple buy-and-hold strategy and that theprediction accuracy of the Markov model is relatively high.</p>

corrected abstract:
<p>This thesis aims to investigate the feasibility of using a Markovian approach to forecast short-term stock market movements. To assist traders in making sound trading decisions, this study proposes a Markovian model using a selection of the latest closing prices. Assuming that each time step in the one-minute time frame of the stock market is stochastically independent, the model eliminates the impact of fundamental analysis and creates a feasible Markov model. The model treats the stock price’s movement as entirely randomly generated, which allows for a more simplified model that can be implemented with ease. The model is intended to serve as a starting ground for more advanced technical trading strategies and act as useful guidance for a short-term trader when combined with other resources. The creation of the model involves Laplace smoothing to ensure there are no zero-probabilities and calculating the steady-state probability vector of the smoothed matrix to determine the predicted direction of the next time step. The model will reset daily, reducing the impact of fundamental factors occurring outside trading hours and reducing the risk of carrying over bias from previous trading day. Any open positions will hence be closed at the end of the day. The study’s purpose is to research and test if a simple forecasting model based on Markov chains can serve as a useful tool for forecasting stock prices at short time intervals. The result of the study shows that a Markov-based trading strategy is more profitable than a simple buy-and-hold strategy and that the prediction accuracy of the Markov model is relatively high.</p>
----------------------------------------------------------------------
In diva2:1781270 abstract is:
<p>According to the paradigm of lambda-CDM cosmology, the stellar halo ofour Galaxy has been built-up over time through the accretion of other galaxiesand star clusters. The remnants of some of these are still observable today asstellar streams, but are typically very faint and difficult to resolve amidst the farmore numerous foreground Milky Way stars. The VelHel-4 stream, discoveredby Helmi et al. [2017], consists of seven members selected based on their energiesand angular momenta. Further studies of these stars has shown evidence ofglobular cluster (GC) abundance patterns, suggesting that the stream originatedfrom a GC progenitor, but a larger sample is needed to verify this signature. Theobjective of this thesis is to find new candidate members of the VelHel-4 stellarstream in order to better characterize its properties and to confirm a possibleGC origin.The preliminary selection of stars was done kinematically, by computing theorbital actions and energies using astrometric data and radial velocities for abright subset of the Gaia DR3 database, and then analyzing the clustering ofstream members in different combinations of action space. The selected samplewas then cleaned by analyzing the positions of these stars in a colour-magnitudediagram. In total, 34 stars were included in the final selection. Follow-up high-resolution spectroscopy of these candidates is needed to study their stellar abun-dances and confirm the possible GC origin of this stream.</p>

corrected abstract:
<p>According to the paradigm of lambda-CDM cosmology, the stellar halo of our Galaxy has been built-up over time through the accretion of other galaxies and star clusters. The remnants of some of these are still observable today as stellar streams, but are typically very faint and difficult to resolve amidst the far more numerous foreground Milky Way stars. The VelHel-4 stream, discovered by Helmi et al. [2017], consists of seven members selected based on their energies and angular momenta. Further studies of these stars has shown evidence of globular cluster (GC) abundance patterns, suggesting that the stream originated from a GC progenitor, but a larger sample is needed to verify this signature. The objective of this thesis is to find new candidate members of the VelHel-4 stellar stream in order to better characterize its properties and to confirm a possible GC origin.</p><p>The preliminary selection of stars was done kinematically, by computing the orbital actions and energies using astrometric data and radial velocities for a bright subset of the Gaia DR3 database, and then analyzing the clustering of stream members in different combinations of action space. The selected sample was then cleaned by analyzing the positions of these stars in a colour-magnitude diagram. In total, 34 stars were included in the final selection. Follow-up high-resolution spectroscopy of these candidates is needed to study their stellar abundances and confirm the possible GC origin of this stream.</p>
----------------------------------------------------------------------
In diva2:1756983 - Note: no full text in DiVA
abstract is:
<p>Companies that are publicly traded can be valued in numerous ways. Thefactors affecting the price of a stock are also numerous. One factor that is ofparticular interest is analysts’ judgements of how a stock is expected to perform,also known as revisions. This report aims to explore if analyst’s revisions havean effect on the price of a stock, and if so, to what degree it does. The reportis confined to the AxJ-market, that is, the Asian financial market excludingJapan.Mathematically, the analysis was performed by creating a regression model.The regressors were 8 separate revision measures, and the response variablewas the change in stock price over a time-period of one year, expressed as apercentage.The results from the report indicate that analysts’ revision indeed have aneffect on the price movement of a stock. For the regressors that were incorpo-rated into the model, the contribution that each regressor had to the model fora given year, varied over the time.As a cautionary note, it should be mentioned that it is extremely difficult tomake correct financial decisions that solely are based on analysts’ revisions, asthere are many other factors that may affect the price movements of a stock.</p>

corrected abstract:
<p>Companies that are publicly traded can be valued in numerous ways. The factors affecting the price of a stock are also numerous. One factor that is of particular interest is analysts’ judgements of how a stock is expected to perform, also known as revisions. This report aims to explore if analyst’s revisions have an effect on the price of a stock, and if so, to what degree it does. The report is confined to the AxJ-market, that is, the Asian financial market excluding Japan. Mathematically, the analysis was performed by creating a regression model. The regressors were 8 separate revision measures, and the response variable was the change in stock price over a time-period of one year, expressed as a percentage. The results from the report indicate that analysts’ revision indeed have an effect on the price movement of a stock. For the regressors that were incorpo-rated into the model, the contribution that each regressor had to the model fora given year, varied over the time. As a cautionary note, it should be mentioned that it is extremely difficult to make correct financial decisions that solely are based on analysts’ revisions, as there are many other factors that may affect the price movements of a stock.</p>
----------------------------------------------------------------------
In diva2:1742205 - Note: no full text in DiVA

abstract is:
<p>Spontaneous parametric down-conversion generation (SPDC) is a resource to producetwin photons, exhibiting two-mode quantum correlations and quantum entanglement,two aspects routinely exploited in many quantum information protocols.Spontaneous parametric generation is a nonlinear phenomenon that requires materialswith high order dielectric susceptibilities to generate efficiently twin photons.In my project, I have studied two integrated optical devices intended to generate twinphotons through SPDC: SiN-based micro-ring resonators and periodically poled lithiumniobate (PPLN) waveguides.In the SiN micro-ring resonators, four-wave mixing (FWM), a third order nonlinearinteraction in SiN is used to produce twin-photons. During my project I characterizedexperimentally the micro-rings by studying their resonances and quantifying their qualityfactors Q. Indeed, high Q resonators are necessary to enhance the efficiency of FWM andproduce high-rate twin photons.In the PPLN waveguides, SPDC is based on a second order nonlinear interaction,where a pump photon is converted into twin photons. During my degree project, Ihave investigated Second Harmonic Generation (SHG) to determine the phase matchingwavelengths of different PPLN waveguides with different lengths and poling periods.Finally, I have characterized the SPDC spectral emission by pumping the PPLN with atunable laser.</p>

corrected abstract:
<p>Spontaneous parametric down-conversion generation (SPDC) is a resource to produce twin photons, exhibiting two-mode quantum correlations and quantum entanglement, two aspects routinely exploited in many quantum information protocols. Spontaneous parametric generation is a nonlinear phenomenon that requires materials with high order dielectric susceptibilities to generate efficiently twin photons. In my project, I have studied two integrated optical devices intended to generate twin photons through SPDC: SiN-based micro-ring resonators and periodically poled lithiumniobate (PPLN) waveguides. In the SiN micro-ring resonators, four-wave mixing (FWM), a third order nonlinear interaction in SiN is used to produce twin-photons. During my project I characterized experimentally the micro-rings by studying their resonances and quantifying their quality factors Q. Indeed, high Q resonators are necessary to enhance the efficiency of FWM and produce high-rate twin photons. In the PPLN waveguides, SPDC is based on a second order nonlinear interaction, where a pump photon is converted into twin photons. During my degree project, I have investigated Second Harmonic Generation (SHG) to determine the phase matching wavelengths of different PPLN waveguides with different lengths and poling periods. Finally, I have characterized the SPDC spectral emission by pumping the PPLN with a tunable laser.</p>
----------------------------------------------------------------------
In diva2:1698340 abstract is:
<p>Noise generation from underwater activities propagates into the marine environment.For marine vessels the propulsion system generates the most noise during itsoperations. Naval vessels that want to operate without being detected want to controlthe sound generating properties of the vessel. To control the sound generatingproperties this project has been looking into the existing propeller of the submergedcraft Carrier Seal that is produced by James Fisher Defense. Then a new and bespokenpropeller has been developed with theories applied to minimize its noise generatingproperties. The properties of the propeller that have been altered is the number ofblades, blade area ratio, pitch and skew angle. These properties have been alteredwith aid of the open-source software for Matlab named Openprop. From the finalpropeller design a prototype was later produced, tested and compared to the existingpropeller of the Seal Carrier. To test and compare these two propellers a test procedurewith inspiration from NATO and the Swedish Defense and Research Agency (FOI) wasdeveloped. The results from the comparison show that the sound pressure level fromthe propeller spectrum could be lowered with 3 dB re 1 μP a for the vessels design speedand several blade tones could be eliminated entirely. Simultaneously the efficiency ofthe vessel is increased throughout its speed range.In conclusions the recommendation to JFD is to change their existing propeller tothis bespoken propeller as it has proven itself to better in every way during thesetrials.</p>

corrected abstract:
<p>Noise generation from underwater activities propagates into the marine environment. For marine vessels the propulsion system generates the most noise during its operations. Naval vessels that want to operate without being detected want to control the sound generating properties of the vessel. To control the sound generating properties this project has been looking into the existing propeller of the submerged craft Carrier Seal that is produced by James Fisher Defense. Then a new and bespoken propeller has been developed with theories applied to minimize its noise generating properties. The properties of the propeller that have been altered is the number of blades, blade area ratio, pitch and skew angle. These properties have been altered with aid of the open-source software for Matlab named Openprop. From the final propeller design a prototype was later produced, tested and compared to the existing propeller of the Seal Carrier. To test and compare these two propellers a test procedure with inspiration from NATO and the Swedish Defense and Research Agency (FOI) was developed. The results from the comparison show that the sound pressure level from the propeller spectrum could be lowered with 3 dB re 1 μP a for the vessels design speed and several blade tones could be eliminated entirely. Simultaneously the efficiency of the vessel is increased throughout its speed range.</p><p>In conclusions the recommendation to JFD is to change their existing propeller to this bespoken propeller as it has proven itself to better in every way during these trials.</p>
----------------------------------------------------------------------
In diva2:1680240 abstract is:
<p>Historically, the earth’s fluctuation between interglacial and glacial climates has been observedto have a period of 105 years [1]. However, simulations of the global average temperature didn’tmanage to reproduce this cycle period until 1982, when Benzi et al. [2] introduced the combinationof long-term variations in incoming solar radiation and stochastic noise in an energy balancemodel. Using an energy balance model means that the change in global average temperature isset as proportional to the difference in ingoing and outgoing energy. The result of the simulationsdemonstrated so-called stochastic resonance, where small stochastic perturbations amplified thepattern of the variation in insolation, causing a pattern of large changes in the global averagetemperature, i.e. changes in the climate. The stochastic perturbations model unpredictable shorttime scale phenomena like the weather. Our study aimed to reproduce the result of Benzi et al.[2] and to investigate the model and its parameters. The presence of a 105-year climatic cycle insimulated data was found. The combination of both noise and varying incoming solar radiationwas necessary to observe the 105-year cycle. The characteristics of the climate cycle pattern did,however, vary greatly depending on the values of constants in the model, illustrating how themodel and constants were imprecise. Therefore, no conclusions can be drawn from this studyabout the earth’s current or future climate. However, the study still confirms that stochasticnoise is an important part of modeling the climate, and manages to simulate the earth’s observed105-year climate cycle.</p>

corrected abstract:
<p>Historically, the earth’s fluctuation between interglacial and glacial climates has been observed to have a period of 10<sup>5</sup> years [1]. However, simulations of the global average temperature didn’t manage to reproduce this cycle period until 1982, when Benzi et al. [2] introduced the combination of long-term variations in incoming solar radiation and stochastic noise in an energy balance model. Using an energy balance model means that the change in global average temperature is set as proportional to the difference in ingoing and outgoing energy. The result of the simulations demonstrated so-called stochastic resonance, where small stochastic perturbations amplified the pattern of the variation in insolation, causing a pattern of large changes in the global average temperature, i.e. changes in the climate. The stochastic perturbations model unpredictable short time scale phenomena like the weather. Our study aimed to reproduce the result of Benzi et al.[2] and to investigate the model and its parameters. The presence of a 10<sup>5</sup>-year climatic cycle in simulated data was found. The combination of both noise and varying incoming solar radiation was necessary to observe the 10<sup>5</sup>-year cycle. The characteristics of the climate cycle pattern did, however, vary greatly depending on the values of constants in the model, illustrating how the model and constants were imprecise. Therefore, no conclusions can be drawn from this study about the earth’s current or future climate. However, the study still confirms that stochastic noise is an important part of modeling the climate, and manages to simulate the earth’s observed 10<sup>5</sup>-year climate cycle.</p>
----------------------------------------------------------------------
In diva2:1679305 abstract is:
<p>The passive particle separation method of elasto-inertial microfluidics have greatpotential in the field of physics, biology and chemistry. The objective of thisdegree project was to understand particle behavior in curved microchannels fornon-Newtonian fluids. This in order to optimize the separation of 1 µm and 2 µmparticles where the end goal is to create an efficient sample preparation method fordiagnosing sepsis. Fluorescent beads were spiked into PEO solutions of differentconcentrations and used in microfluidic PDMS-glass chips with various radii toexamine the influence of curvature and elasticity as well as the flow rate. Theresult indicated an independence of both curvature and elasticity. Reynoldsnumber and Dean number are dependent on the flow rate which results in atrade-off between a high and low flow rate. A low Reynolds number is not enoughto create Dean vortices that can be used to separate particles while a highReynolds number creates strong Dean vortices that can obstruct the focusing.</p><p>Later, microfluidic silicon-glass chips were used to separate 1 µm and 2 µm beads.The 2 µm particles were able to focus in two different PEO concentrations whereasthe 1 µm particles did not have time to focus entirely. This makes it possible toseparate 2 µm particles along with some 1 µm particles towards one outlet whileleaving another outlet with only 1 µm particles. This is a promising start butfurther optimization is required before being applied to real bacteria separation.</p>

corrected abstract:
<p>The passive particle separation method of elasto-inertial microfluidics have great potential in the field of physics, biology and chemistry. The objective of this degree project was to understand particle behavior in curved microchannels for non-Newtonian fluids. This in order to optimize the separation of 1 µm and 2 µm particles where the end goal is to create an efficient sample preparation method for diagnosing sepsis. Fluorescent beads were spiked into PEO solutions of different concentrations and used in microfluidic PDMS-glass chips with various radii to examine the influence of curvature and elasticity as well as the flow rate. The result indicated an independence of both curvature and elasticity. Reynolds number and Dean number are dependent on the flow rate which results in a trade-off between a high and low flow rate. A low Reynolds number is not enough to create Dean vortices that can be used to separate particles while a high Reynolds number creates strong Dean vortices that can obstruct the focusing.</p><p>Later, microfluidic silicon-glass chips were used to separate 1 µm and 2 µm beads. The 2 µm particles were able to focus in two different PEO concentrations where as the 1 µm particles did not have time to focus entirely. This makes it possible to separate 2 µm particles along with some 1 µm particles towards one outlet while leaving another outlet with only 1 µm particles. This is a promising start but further optimization is required before being applied to real bacteria separation.</p>
----------------------------------------------------------------------
In diva2:1546793 abstract is:
<p>The automotive environment is quickly evolving due to increasingly stringent environmentalstandards and the gradual reduction of the volume of diesel motorized vehicles. The volumes of electrifiedvehicles are thus constantly growing and are brought to be more and more present in our streets. Thiselectrification of vehicles involves new specifics issues compared to conventional vehicles, depending on thedifferent levels of electrification, which includes notably Hybrid Electric Vehicles.Hybridization in cars is characterized by the addition of a electrical traction and/or electrical generation systemin addition to the conventional thermal engine. However, if the complexity of vehicles with thermal tractioncomes essentially from the internal combustion engine and its efficiency which are sometimes complex tooptimize, the complexity of electric traction is expressed on the other hand at the level of the battery whichsupplies high voltage electricity. Indeed, while an electric machine offers high efficiency and an easy control,the high voltage battery contains many issues linked to a complex chemistry which must be controlled, andcan be subject to overheating.This overheating phenomenon is particularly an issue on HEV, which have smaller batteries than BEV and PHEVapplications for a comparable power demand. The conception of an efficient high-voltage battery coolingsystem is therefore essential in order to avoid any danger of damaging the system or potential fires linked tothe overheating of the battery. The air cooling solution is the most common, but this could change with thenew standard of the Electric Vehicle Safety Global Technical Regulation (EVS-GTR) applicable in 2021prohibiting the rejection of the cooling air that was in contact with the battery cells inside the passengercompartment. Is this solution able to adapt in order to remain competitive with the water or air conditioningcooling solutions? This study's purpose is to bring an answer to this issue.</p>

corrected abstract:
<p>The automotive environment is quickly evolving due to increasingly stringent environmental standards and the gradual reduction of the volume of diesel motorized vehicles. The volumes of electrified vehicles are thus constantly growing and are brought to be more and more present in our streets. This electrification of vehicles involves new specifics issues compared to conventional vehicles, depending on the different levels of electrification, which includes notably Hybrid Electric Vehicles.</p><p>Hybridization in cars is characterized by the addition of a electrical traction and/or electrical generation system in addition to the conventional thermal engine. However, if the complexity of vehicles with thermal traction comes essentially from the internal combustion engine and its efficiency which are sometimes complex to optimize, the complexity of electric traction is expressed on the other hand at the level of the battery which supplies high voltage electricity. Indeed, while an electric machine offers high efficiency and an easy control, the high voltage battery contains many issues linked to a complex chemistry which must be controlled, and can be subject to overheating.</p><p>This overheating phenomenon is particularly an issue on HEV, which have smaller batteries than BEV and PHEV applications for a comparable power demand. The conception of an efficient high-voltage battery cooling system is therefore essential in order to avoid any danger of damaging the system or potential fires linked to the overheating of the battery. The air cooling solution is the most common, but this could change with the new standard of the Electric Vehicle Safety Global Technical Regulation (EVS-GTR) applicable in 2021 prohibiting the rejection of the cooling air that was in contact with the battery cells inside the passenger compartment. Is this solution able to adapt in order to remain competitive with the water or air conditioning cooling solutions? This study's purpose is to bring an answer to this issue.</p>
----------------------------------------------------------------------
In diva2:1528126 abstract is:
<p>The modern marine fuel system have a vital part in preparing the fuel before it can enter theengines. Hard particles and water are removed and the viscosity needs to meet limitationsof the engines, to prevent damage. The cost for operating the fuel system and wear on theengine varies depending on how the fuel system are operated and on surrounding parameters.This thesis explains the cost impact of di erent parameters on the total cost for operating afuel system and aims to create a total cost of ownership model that also considers the costsfor systems connected to the fuel system.The total cost of ownership model includes capital expenses for the equipment and the operationalexpenses in form of; energy, service, sludge storage, water production, risk and alsoincreased costs on connected systems.The result of this total cost of ownership research shows that even if there are an increasedcost for operating the fuel system itself for highest performance will the total cost of ownershipwhen considering connected systems be lower. The user are therefore always recommendedto aim for maximum separation of hard particles to decrease the cost for enginewear and to lower the total cost of risk for a breakdown. The total energy consumption forthe fuel system can be decreased by up to 15% when variable ow control are used on thesupply pumps and meanwhile increase the separation eciency.</p>


corrected abstract:
<p>The modern marine fuel system have a vital part in preparing the fuel before it can enter the engines. Hard particles and water are removed and the viscosity needs to meet limitations of the engines, to prevent damage. The cost for operating the fuel system and wear on the engine varies depending on how the fuel system are operated and on surrounding parameters. This thesis explains the cost impact of different parameters on the total cost for operating a fuel system and aims to create a total cost of ownership model that also considers the costs for systems connected to the fuel system.</p><p>The total cost of ownership model includes capital expenses for the equipment and the operational expenses in form of; energy, service, sludge storage, water production, risk and also increased costs on connected systems.</p><p>The result of this total cost of ownership research shows that even if there are an increased cost for operating the fuel system itself for highest performance will the total cost of ownership when considering connected systems be lower. The user are therefore always recommended to aim for maximum separation of hard particles to decrease the cost for engine wear and to lower the total cost of risk for a breakdown. The total energy consumption for the fuel system can be decreased by up to 15% when variable flow control are used on the supply pumps and meanwhile increase the separation efficiency.</p>
----------------------------------------------------------------------
In diva2:1465543 abstract is:
<p>In early design phase for new hybrid electric vehicle (HEV) powertrains, simulation isused for the estimation of vehicle fuel consumption. For hybrid electric powertrains,fuel consumption is highly related to powertrain efficiency. While powertrainefficiency of hybrid electric powertrain is not a linear product of efficiencies ofcomponents, it has to be analysed as a sequence of energy conversions includingcomponent losses and energy interaction among components.This thesis is aimed at studying the energy losses and flows and present them in theform of Sankey diagram, later, an adaptive energy management system is developedbased on current rule-based control strategy. The first part involves developing energycalculation block in GT-SUITE corresponding to the vehicle model, calculating allthe energy losses and flows and presenting them in Sankey diagram. The secondpart involves optimizing energy management system control parameters according todifferent representative driving cycles. The third part involves developing adaptiveenergy management system by deploying optimal control parameter based on drivingpattern recognition with the help of SVM (support vector machine).In conclusion, a sturctured way to generate the Sankey diagram has been successfullygenerated and it turns out to be an effective tool to study HEV powertrain efficiencyand fuel economy. In addition, the combination of driving pattern recognition andoptimized control parameters also show a significant potential improvement in fuelconsumption.</p>

corrected abstract:
<p>In early design phase for new hybrid electric vehicle (HEV) powertrains, simulation is used for the estimation of vehicle fuel consumption. For hybrid electric powertrains, fuel consumption is highly related to powertrain efficiency. While powertrain efficiency of hybrid electric powertrain is not a linear product of efficiencies of components, it has to be analysed as a sequence of energy conversions including component losses and energy interaction among components.</p><p>This thesis is aimed at studying the energy losses and flows and present them in the form of Sankey diagram, later, an adaptive energy management system is developed based on current rule-based control strategy. The first part involves developing energy calculation block in GT-SUITE corresponding to the vehicle model, calculating all the energy losses and flows and presenting them in Sankey diagram. The second part involves optimizing energy management system control parameters according to different representative driving cycles. The third part involves developing adaptive energy management system by deploying optimal control parameter based on driving pattern recognition with the help of SVM (support vector machine).</p><p>In conclusion, a sturctured way to generate the Sankey diagram has been successfully generated and it turns out to be an effective tool to study HEV powertrain efficiency and fuel economy. In addition, the combination of driving pattern recognition and optimized control parameters also show a significant potential improvement in fuel consumption.</p>
----------------------------------------------------------------------
In diva2:1293442 abstract is:
<p>Harry Markowitz work in the 50’s spring-boarded modernportfolio theory. It gives investors quantitative tools to compose and assessasset portfolios in a systematic fashion. The main idea of the Mean-Varianceframework is that composing an optimal portfolio is equivalent to solving aquadratic optimization problem.In this project we employ the Maximally Predictable Portfolio (MPP) frameworkproposed by Lo and MacKinlay, as an alternative to Markowitz’s approach, inorder to construct investment portfolios. One of the benefits of using theformer method is that it accounts for forecasting estimation errors. Ourinvestment strategy is to buy and hold these portfolios during a time periodand assess their performance. We show that it is indeed possible to constructportfolios with high rate of return and coefficient of determination based onhistorical data. However, despite their many promising features, the success ofMPP portfolios is short lived. Based on our assessment we conclude thatinvesting in the stock market solely on the basis of the optimization resultsis not a lucrative strategy</p>

corrected abstract:
<p>Harry Markowitz work in the 50’s spring-boarded modern portfolio theory. It gives investors quantitative tools to compose and assess asset portfolios in a systematic fashion. The main idea of the Mean-Variance framework is that composing an optimal portfolio is equivalent to solving a quadratic optimization problem.</p><p>In this project we employ the Maximally Predictable Portfolio (MPP) framework proposed by Lo and MacKinlay, as an alternative to Markowitz’s approach, in order to construct investment portfolios. One of the benefits of using the former method is that it accounts for forecasting estimation errors. Our investment strategy is to buy and hold these portfolios during a time period and assess their performance. We show that it is indeed possible to construct portfolios with high rate of return and coefficient of determination based on historical data. However, despite their many promising features, the success of MPP portfolios is short lived. Based on our assessment we conclude that investing in the stock market solely on the basis of the optimization results is not a lucrative strategy.</p>
----------------------------------------------------------------------
In diva2:1206952 - missing space in title:
"Test Method Optimization ofSemi-Automatic ParkingFunction"
==>
"Test Method Optimization of Semi-Automatic Parking Function"

abstract is:
<p>This is a Master of Science project performed at Volvo Cars in Gothenburg and at The RoyalInstitute of Technology KTH in Stockholm. The project is about optimization of the testmethod for semi-automatic parking. The current test method to verify the parking functions aredescribed in a document called design verification method, DVM. The test method in DVMconsiders each function’s parameter separately which takes a lot of test time and all of thefunctions cannot be tested because of time shortage. The aim of this project is to develop anoptimized test method which can solves this issue and can replace the current test method.There are also some other issues that the project need to deal with, such as the sensor’smeasuring error and the optimization of the distance from vehicle to the curb.The current test methods are based on principle of one factor at a time method, which is verytime consuming. Several other test method such as Factorial Design, Taguchi Design andPlacket Burman Design which are based on the principle of factorial design are thereforestudied. Amongst these the factorial design is chosen since it is an adequate design in term ofreduction of test time and other properties which are beneficial for the aim of this project.The proposed test method is evaluated by first performing a test version with a number ofrelevant inputs parameters for which the process is described in Chapter 3 and the evaluationof the method is described in Chapter 4. In Chapter 5 the process on how the proposed methodcan replace the current method is described.The result of this thesis work is a proposed and verified system verification test method forparking assistance which can also be used for other systems as well on some levels.</p>

corrected abstract:
<p>This is a Master of Science project performed at Volvo Cars in Gothenburg and at The Royal Institute of Technology KTH in Stockholm. The project is about optimization of the test method for semi-automatic parking. The current test method to verify the parking functions are described in a document called design verification method, DVM. The test method in DVM considers each function’s parameter separately which takes a lot of test time and all of the functions cannot be tested because of time shortage. The aim of this project is to develop an optimized test method which can solves this issue and can replace the current test method. There are also some other issues that the project need to deal with, such as the sensor’s measuring error and the optimization of the distance from vehicle to the curb.</p><p>The current test methods are based on principle of one factor at a time method, which is very time consuming. Several other test method such as Factorial Design, Taguchi Design and Placket Burman Design which are based on the principle of factorial design are therefore studied. Amongst these the factorial design is chosen since it is an adequate design in term of reduction of test time and other properties which are beneficial for the aim of this project.</p><p>The proposed test method is evaluated by first performing a test version with a number of relevant inputs parameters for which the process is described in Chapter 3 and the evaluation of the method is described in Chapter 4. In Chapter 5 the process on how the proposed method can replace the current method is described.</p><p>The result of this thesis work is a proposed and verified system verification test method for parking assistance which can also be used for other systems as well on some levels.</p>
----------------------------------------------------------------------
In diva2:1136781 - missing space in title:
"CFD Analysis of Cold Stage Centrifugal Pump for Cooling of Hot IsostaticPress with Validation Case Study"
==>
"CFD Analysis of Cold Stage Centrifugal Pump for Cooling of Hot Isostatic Press with Validation Case Study"

abstract is:
<p>Hot isostatic pressing (HIPing) has been a growing material treatment process for performance part manufacturingfor over 50 years. This process of using an inert gas at high temperature and pressure to densifymaterials leads to vastly improved material properties by removing pores and other micro- aws. Interest forHIP treatment has greatly increased in recent years due to the development of metal 3D printing technology.HIP treatment is very well suited for treating 3D printed and cast parts due to their relatively poor materialproperties.An important part of any HIP cycle is the cooling phase. New uniform and rapid cooling technology hasvastly reduced HIP cycle times, but room for further improvement exists. This study aims to accurately andtrustfully evaluate the performance of one of a pair of centrifugal pumps used in a Quintus Technologies ABHIP cooling system. Computational uid dynamics (CFD) software and techniques are used to achieve this.This paper is split into two main parts; the rst of which is a validation case study, and the second is theperformance analysis of a Quintus HIP cold gas pump. The validation case study is conducted to supportthe accuracy and reliability of results obtained in the Quintus cold gas pump performance analysis.The validation case study results show good agreement with experimental data and supports the accuracy ofCFD in the analysis of centrifugal pumps. Both detailed ow and macro ow characteristics are shown to beaccurately predicted. The pump curve generated for the Quintus Cold gas pump quanties its performanceover a range of rotational speeds and mass ow rates. The work done here lays the groundwork for furtheranalysis and improvement of Quintus HIP cooling systems.</p>

corrected abstract:
<p>Hot isostatic pressing (HIPing) has been a growing material treatment process for performance part manufacturing for over 50 years. This process of using an inert gas at high temperature and pressure to densify materials leads to vastly improved material properties by removing pores and other micro-flaws. Interest for HIP treatment has greatly increased in recent years due to the development of metal 3D printing technology. HIP treatment is very well suited for treating 3D printed and cast parts due to their relatively poor material properties.</p><p>An important part of any HIP cycle is the cooling phase. New uniform and rapid cooling technology has vastly reduced HIP cycle times, but room for further improvement exists. This study aims to accurately and trustfully evaluate the performance of one of a pair of centrifugal pumps used in a Quintus Technologies AB HIP cooling system. Computational fluid dynamics (CFD) software and techniques are used to achieve this. This paper is split into two main parts; the first of which is a validation case study, and the second is the performance analysis of a Quintus HIP cold gas pump. The validation case study is conducted to support the accuracy and reliability of results obtained in the Quintus cold gas pump performance analysis.</p><p>The validation case study results show good agreement with experimental data and supports the accuracy of CFD in the analysis of centrifugal pumps. Both detailed flow and macro flow characteristics are shown to be accurately predicted. The pump curve generated for the Quintus Cold gas pump quantifies its performance over a range of rotational speeds and mass flow rates. The work done here lays the groundwork for further analysis and improvement of Quintus HIP cooling systems.</p>
----------------------------------------------------------------------
In diva2:1120459 abstract is:
<p>Collaboration between humans and robots is becoming an increasingly commonoccurrence in both industry and homes, more so with every forthcomingtechnological advance. This paper examines the possibilities of performinghuman hand movement predictions on the fly, e.g. by only using informationup to the specific moment in time of which the prediction is carried out.Specifically, data will be collected using a Kinect (v.1).The model used for the predictor developed is the Minimum Jerk model,which states that certain multi-joint reaching movements are planned in sucha way that the hand is to follow a straight path while maximizing smoothness.Extent, direction and duration of the motion are main objectives for thepredictor to determine, with a Kalman filter and curve fitting as the mainconstituents. Another assumption in this work is that a reliable start detectoris available. An experiment where five volunteers were to perform differentreaching movements was conducted.This study shows that the approach is feasible in some cases, namelyusable predictions is acquired for long movements. In the case of shortmovements the alternative of not doing any prediction was by all meansbetter.</p>

corrected abstract:
<p>Collaboration between humans and robots is becoming an increasingly common occurrence in both industry and homes, more so with every forthcoming technological advance. This paper examines the possibilities of performing human hand movement predictions on the fly, e.g. by only using information up to the specific moment in time of which the prediction is carried out. Specifically, data will be collected using a Kinect (v.1).</p><p>The model used for the predictor developed is the Minimum Jerk model, which states that certain multi-joint reaching movements are planned in such a way that the hand is to follow a straight path while maximizing smoothness. Extent, direction and duration of the motion are main objectives for the predictor to determine, with a Kalman filter and curve fitting as the main constituents. Another assumption in this work is that a reliable start detector is available. An experiment where five volunteers were to perform different reaching movements was conducted.</p><p>This study shows that the approach is feasible in some cases, namely usable predictions is acquired for long movements. In the case of short movements the alternative of not doing any prediction was by all means better.</p>
----------------------------------------------------------------------
In diva2:1114153 abstract is:
<p>The study aims to investigate which factors that could potentially affectthe yearly card transaction volume for smaller or medium sized businesses, inthis study entitled merchants, not listed on the stock exchange. Thesemerchants are characterized by a low number of employees, and in many casessmaller businesses without any noted ambition to grow in size. They are alldirected towards consumers, and are therefore heavily reliable of peopleactually visiting their business in order to make revenues. These merchants donot offer any public financial information like previous yearly revenues,planned future investments or any debts the company might have, and hence thestudy is focused on factors an external actor could take part of. Byinvestigating non-financial factors, this investigation further contributeswith research on how a company can ensure growth and revenues. It alsoemphasizes specific industries or size of companies that generate larger cardtransaction volumes than others.</p><p>By mapping both possible quantitative andqualitative factors, that could potentially have an impact on the yearly cardtransaction volume of a merchant, a mathematical analysis has been conducted.The mathematical part of the study is based on multiple linear regressionanalysis, where the result is a model that can predict the card transactionvolume for a company. The study has been conducted on actors operating on theDanish market, but could with smaller adjustments also be applied on other geographicalmarkets</p>

corrected abstract:
<p>The study aims to investigate which factors that could potentially affect the yearly card transaction volume for smaller or medium sized businesses, in this study entitled merchants, not listed on the stock exchange. These merchants are characterized by a low number of employees, and in many cases smaller businesses without any noted ambition to grow in size. They are all directed towards consumers, and are therefore heavily reliable of people actually visiting their business in order to make revenues. These merchants do not offer any public financial information like previous yearly revenues, planned future investments or any debts the company might have, and hence the study is focused on factors an external actor could take part of. By investigating non-financial factors, this investigation further contributes with research on how a company can ensure growth and revenues. It also emphasizes specific industries or size of companies that generate larger card transaction volumes than others.</p><p>By mapping both possible quantitative and qualitative factors, that could potentially have an impact on the yearly card transaction volume of a merchant, a mathematical analysis has been conducted. The mathematical part of the study is based on multiple linear regression analysis, where the result is a model that can predict the card transaction volume for a company. The study has been conducted on actors operating on the Danish market, but could with smaller adjustments also be applied on other geographical markets.</p>
----------------------------------------------------------------------
In diva2:692743 abstract is:
<p>This thesis is intended to give an overview of creditvaluation adjustment (CVA) and adjacent concepts. Firstly, the historicalevents that preceded the initiative to reform the Basel regulations and tointroduce CVA as a core component of counterparty credit risk are illustrated.After some conceptual background material, a journey is taken through theregulatory aspects of CVA. The three most commonly used methods for calculatingthe regulatory CVA capital charge are explained in detail and potentialchallenges with the methods are addressed. Further, the document analyses ingreater depth two of the methods; the internal model method (IMM) and thecurrent exposure method (CEM). The differences between these two methods areexplained mathematically and analysed. This comparison is supported bysimulations of portfolios containing interest rate swap contracts with differenttime to maturity and of counterparties with varying credit ratings. Oneconcluding observations is that credit valuation adjustment is a measure of centralimportance within counterparty credit risk. Further, it is shown that IMM has someimportant advantages over CEM, especially when it comes to model connection withreality. Finally, some possible future work to be done within the topic area is suggested.</p>


corrected abstract:
<p>This thesis is intended to give an overview of credit valuation adjustment (CVA) and adjacent concepts. Firstly, the historical events that preceded the initiative to reform the Basel regulations and to introduce CVA as a core component of counterparty credit risk are illustrated. After some conceptual background material, a journey is taken through the regulatory aspects of CVA. The three most commonly used methods for calculating the regulatory CVA capital charge are explained in detail and potential challenges with the methods are addressed. Further, the document analyses in greater depth two of the methods; the internal model method (IMM) and the current exposure method (CEM). The differences between these two methods are explained mathematically and analysed. This comparison is supported by simulations of portfolios containing interest rate swap contracts with different time to maturity and of counterparties with varying credit ratings. One concluding observations is that credit valuation adjustment is a measure of central importance within counterparty credit risk. Further, it is shown that IMM has some important advantages over CEM, especially when it comes to model connection with reality. Finally, some possible future work to be done within the topic area is suggested.</p>
----------------------------------------------------------------------
In diva2:618592 - Note: no full text in DiVA

abstract is:
<p>In the quest of reducing emissions of passenger cars a trend in the car industry has recently been tointroduce lightweight composite materials to replace steel which has been the otherwise main materialto use. In this report the weight-optimising of a structural underbody for a passenger car using twodifferent manufacturing methods is described. The two methods are Advanced Sheet MouldingCompound (A-SMC) and Resin Transfer Moulding (RTM). A-SMC is characterised by a low cycle time andfast layup of the material resulting in lower cost. RTM is slower and thus more expensive but has bettermaterial properties which results in lower weight. The novel approach is to use A-SMC to construct thecomplete underbody as one piece which has not been done before. The geometry is FEM -optimised forminimum material thickness under a standard load case for torsional stiffness.The simulations showed that the underbody was not possible to be manufactured using A-SMC in itsoriginal shape without being reinforced. When reinforcing the structure it met the design constraintsand the weight was optimised to 53.9 kg with the possibility for further improvements. The RTMmethod resulted in 25.83 kg without any reinforcements but showed potential for further weightreduction by changing the geometrical design. A final analysis of the underbody combined the twomanufacturing methods and the weight was here optimised to 25.27 kg without any reinforcements.</p>

corrected abstract:
<p>In the quest of reducing emissions of passenger cars a trend in the car industry has recently been to introduce lightweight composite materials to replace steel which has been the otherwise main material to use. In this report the weight-optimising of a structural under body for a passenger car using twodifferent manufacturing methods is described. The two methods are Advanced Sheet Moulding Compound (A-SMC) and Resin Transfer Moulding (RTM). A-SMC is characterised by a low cycle time and fast layup of the material resulting in lower cost. RTM is slower and thus more expensive but has better material properties which results in lower weight. The novel approach is to use A-SMC to construct the complete under body as one piece which has not been done before. The geometry is FEM -optimised for minimum material thickness under a standard load case for torsional stiffness. The simulations showed that the under body was not possible to be manufactured using A-SMC in its original shape without being reinforced. When reinforcing the structure it met the design constraints and the weight was optimised to 53.9 kg with the possibility for further improvements. The RTM method resulted in 25.83 kg without any reinforcements but showed potential for further weight reduction by changing the geometrical design. A final analysis of the under body combined the two manufacturing methods and the weight was here optimised to 25.27 kg without any reinforcements.</p>
----------------------------------------------------------------------
In diva2:408828 abstract is:
<p>The digital development may be the biggest challenge that the school has faced since theinvention of the printing press. Several of the teachers conditions have already changed andtheir role will in the future change fundamentally. To be able to operate in the school of thefuture teachers need a digital literacy. How to develop teacher’s digital literacy and how to findthe factors that affect this competence is the subject of this master thesis. In the thesis adevelopment model for teachers' digital literacy is used. The model has been used together withphysics teachers. The results are based on literature studies and a field study.The field study shows that it is possible to develop a teacher’s digital literacy with simplemeans. There are three main factors affecting the development of teachers' digital literacy:access to computers, access to software customized for schools and training. The educationalfoundation for both students and teachers must be the focus of attention and it is important thatthis is not overshadowed by a focus on the technology. When teachers no longer have amonopoly on knowledge, the traditional teacher role will change. The teacher will become aguide with the function to support the students learning. A computer-aided school is positivelywelcomed by students and cautiously welcomed by teachers. A professional production ofsoftware for use in schools has to begin. All teachers must be trained so that they have anadequate digital literacy to be sufficient in the school of tomorrow.</p>

corrected abstract:
<p>The digital development may be the biggest challenge that the school has faced since the invention of the printing press. Several of the teachers conditions have already changed and their role will in the future change fundamentally. To be able to operate in the school of the future teachers need a digital literacy. How to develop teacher’s digital literacy and how to find the factors that affect this competence is the subject of this master thesis. In the thesis a development model for teachers' digital literacy is used. The model has been used together with physics teachers. The results are based on literature studies and a field study.</p><p>The field study shows that it is possible to develop a teacher’s digital literacy with simple means. There are three main factors affecting the development of teachers' digital literacy: access to computers, access to software customized for schools and training. The educational foundation for both students and teachers must be the focus of attention and it is important that this is not overshadowed by a focus on the technology. When teachers no longer have a monopoly on knowledge, the traditional teacher role will change. The teacher will become a guide with the function to support the students learning. A computer-aided school is positively welcomed by students and cautiously welcomed by teachers. A professional production of software for use in schools has to begin. All teachers must be trained so that they have an adequate digital literacy to be sufficient in the school of tomorrow.</p>
----------------------------------------------------------------------
In diva2:401124 - missing space in title:
"Flight Dynamics Modelling and Application to SatelliteDeorbitation Strategy"
==>
"Flight Dynamics Modelling and Application to Satellite Deorbitation Strategy"

abstract is:
<p>In this paper a study to determine the best strategic choice to deorbit satellites incase of system failures is conducted. An orbital mechanic analysis of the deorbitationis done and a matlab code based on Lagrange’s interplanetary equations is developed.The effects of the atmospheric drag and the solar activity are investigated and the fueland delta velocity consumptions are assessed for different deorbitation strategies. Thepaper also investigates the probability to hit or be hit by a piece of debris. A briefoverview of the acquisition and safe hold modes Bdot and Bspin is given. Then thepossibility to deorbit a satellite working in these attitude and orbit control modes fromlow Earth’s orbits is studied. An attitude and orbit simulator is used to analyze theevolution of the attitude during a manoeuvre, in particular the effect of the thrusters’misalignment is investigated. A strategy which combines manoeuvres and aerobrakingis chosen and its implementation on different types of orbits is studied. The paperconcludes that the necessary manoeuvres for a 25 years deorbiting time can be achieveif the spacecraft has a functional acquisition and safe hold mode and an operationalpropulsion system. In the end an alternate mode based on star trackers and reactionwheels is developed to avoid most of the activations of the acquisition and safe holdmode and to eventually perform the end of life disposal of a spacecraft.</p>

corrected abstract:
<p>In this paper a study to determine the best strategic choice to deorbit satellites in case of system failures is conducted. An orbital mechanic analysis of the deorbitation is done and a matlab code based on Lagrange’s interplanetary equations is developed. The effects of the atmospheric drag and the solar activity are investigated and the fuel and delta velocity consumptions are assessed for different deorbitation strategies. The paper also investigates the probability to hit or be hit by a piece of debris. A brief overview of the acquisition and safe hold modes <em>Bdot</em> and <em>Bspin</em> is given. Then the possibility to deorbit a satellite working in these attitude and orbit control modes from low Earth’s orbits is studied. An attitude and orbit simulator is used to analyze the evolution of the attitude during a manoeuvre, in particular the effect of the thrusters’ misalignment is investigated. A strategy which combines manoeuvres and aerobraking is chosen and its implementation on different types of orbits is studied. The paper concludes that the necessary manoeuvres for a 25 years deorbiting time can be achieve if the spacecraft has a functional acquisition and safe hold mode and an operational propulsion system. In the end an alternate mode based on star trackers and reaction wheels is developed to avoid most of the activations of the acquisition and safe hold mode and to eventually perform the end of life disposal of a spacecraft.</p>
----------------------------------------------------------------------
In diva2:1890962 abstract is:
<p>The objective of this research is to investigate bound-state beta decay (BSBD) and its influence on the half-life oflong-lived fission products (LLFPs). We reviewed the existing BSBD model by Takahashi and Yokoi, which isbased on plasma states at very high temperatures. Following this, we devised a systematic BSBD model assumingbare ions in a vacuum. Our BSBD study was then performed using this bare ion model. The research methodicallyexplores how BSBD impacts LLFP half-lives, by comparing theoretical calculations with experimental data tovalidate the model’s predictive accuracy. To calculate the half-life of LLFPs, we first computed the radialwavefunctions of electrons in the K and L orbitals, where electron creation is more likely, by solving the Diracequation. Subsequently, we used FLYCHK software to determine the bound-state decay energy and obtain thelepton phase volume. Additionally, we calculated the decay rate function by implementing finite size correctionsand the partial half-life for each LLFP, resulting in the total decay rate of the nuclide. The study investigated thehalf-life of fully stripped LLFPs compared to their neutral states for known transitions, and also examined variousnuclear-level couplings and their influence on the decay transitions. The study suggests that these unknowntransitions can lead to different types of allowed transitions, significantly shortening half-lives, with additionalcontributions from the continuum channel. Furthermore, the research reveals variations in branching patternsbetween neutral and bare atoms, including instances of branching flip observed in specific LLFP isotopes</p>

corrected abstract:
<p>The objective of this research is to investigate bound-state beta decay (BSBD) and its influence on the half-life of long-lived fission products (LLFPs). We reviewed the existing BSBD model by Takahashi and Yokoi, which is based on plasma states at very high temperatures. Following this, we devised a systematic BSBD model assuming bare ions in a vacuum. Our BSBD study was then performed using this bare ion model. The research methodically explores how BSBD impacts LLFP half-lives, by comparing theoretical calculations with experimental data to validate the model’s predictive accuracy. To calculate the half-life of LLFPs, we first computed the radial wavefunctions of electrons in the K and L orbitals, where electron creation is more likely, by solving the Dirac equation. Subsequently, we used FLYCHK software to determine the bound-state decay energy and obtain the lepton phase volume. Additionally, we calculated the decay rate function by implementing finite size corrections and the partial half-life for each LLFP, resulting in the total decay rate of the nuclide. The study investigated the half-life of fully stripped LLFPs compared to their neutral states for known transitions, and also examined various nuclear-level couplings and their influence on the decay transitions. The study suggests that these unknown transitions can lead to different types of allowed transitions, significantly shortening half-lives, with additional contributions from the continuum channel. Furthermore, the research reveals variations in branching patterns between neutral and bare atoms, including instances of branching flip observed in specific LLFP isotopes</p>
----------------------------------------------------------------------
In diva2:1879630 abstract is:
<p>This thesis concerns itself with word classes and their application to language modelling.Considering a purely statistical Markov model trained on sequences of word classes in theSwedish language different problems in language engineering are examined. Problemsconsidered are part-of-speech tagging, evaluating text modifiers such as translators withthe help of probability measurements and matrix norms, and lastly detecting differenttypes of text using the Fourier transform of cross entropy sequences of word classes.The results show that the word class language model is quite weak by itself but that itis able to improve part-of-speech tagging for 1 and 2 letter models. There are indicationsthat a stronger word class model could aid 3-letter and potentially even stronger models.For evaluating modifiers the model is often able to distinguish between shuffled andsometimes translated text as well as to assign a score as to how much a text has beenmodified. Future work on this should however take better care to ensure large enoughtest data. The results from the Fourier approach indicate that a Fourier analysis of thecross entropy sequence between word classes may allow the model to distinguish betweenA.I. generated text as well as translated text from human written text. Future work onmachine learning word class models could be carried out to get further insights into therole of word class models in modern applications. The results could also give interestinginsights in linguistic research regarding word classes.</p>

corrected abstract:
<p>This thesis concerns itself with word classes and their application to language modelling. Considering a purely statistical Markov model trained on sequences of word classes in the Swedish language different problems in language engineering are examined. Problems considered are part-of-speech tagging, evaluating text modifiers such as translators with the help of probability measurements and matrix norms, and lastly detecting different types of text using the Fourier transform of cross entropy sequences of word classes.  The results show that the word class language model is quite weak by itself but that it is able to improve part-of-speech tagging for 1 and 2 letter models. There are indications that a stronger word class model could aid 3-letter and potentially even stronger models. For evaluating modifiers the model is often able to distinguish between shuffled and sometimes translated text as well as to assign a score as to how much a text has been modified. Future work on this should however take better care to ensure large enough test data. The results from the Fourier approach indicate that a Fourier analysis of the cross entropy sequence between word classes may allow the model to distinguish between A.I. generated text as well as translated text from human written text. Future work on machine learning word class models could be carried out to get further insights into the role of word class models in modern applications. The results could also give interesting insights in linguistic research regarding word classes.</p>
----------------------------------------------------------------------
In diva2:1876262 - Note: no full text in DiVA
abstract is:
<p>This thesis presents a comprehensive methodology for modelling asymmetric silicon anodes within the COMSOL Multiphysics software, with the aim to enhance the design andevaluation of lithium-ion batteries. The research addresses the critical issue of swelling inamorphous silicon anodes, which can undergo up to 400% volumetric expansion duringcharging cycles. This effect can potentially lead to the degradation of battery performanceand shorten lifespan.The thesis provides in detail the implementation of Finite Element Method (FEM) simulations to investigate the mechanical and electrochemical responses of silicon anodes underoperational conditions. The modelling process is elaborated, starting from the theoreticalbasis of lithium diffusion and plastic deformation and then proceeding with the practical setup of simulation parameters and boundary conditions in COMSOL Multiphysics.Different anode geometries, including double-walled nanotube structures, have been modelled to study the ability to mitigate the effects of expansion.The results from the simulations provide insights into the stress distribution and deformation patterns within the anodes and highlight the effectiveness of certain geometricalconfigurations in reducing mechanical stresses and maintaining the integrity of the solidelectrolyte interface (SEI).The work concludes with suggestions for further enhancements in the simulation modeland proposes future research directions to explore alternative materials and configurations.Through detailed modelling and analysis, this thesis contributes to the ongoing efforts toimprove the performance and reliability of next-generation lithium-ion batteries.</p><p> </p>

corrected abstract:
<p>This thesis presents a comprehensive methodology for modelling asymmetric silicon anodes within the COMSOL Multiphysics software, with the aim to enhance the design and evaluation of lithium-ion batteries. The research addresses the critical issue of swelling in amorphous silicon anodes, which can undergo up to 400% volumetric expansion during charging cycles. This effect can potentially lead to the degradation of battery performance and shorten lifespan. The thesis provides in detail the implementation of Finite Element Method (FEM) simulations to investigate the mechanical and electrochemical responses of silicon anodes under operational conditions. The modelling process is elaborated, starting from the theoretical basis of lithium diffusion and plastic deformation and then proceeding with the practical setup of simulation parameters and boundary conditions in COMSOL Multiphysics. Different anode geometries, including double-walled nanotube structures, have been modelled to study the ability to mitigate the effects of expansion. The results from the simulations provide insights into the stress distribution and deformation patterns within the anodes and highlight the effectiveness of certain geometrical configurations in reducing mechanical stresses and maintaining the integrity of the solid electrolyte interface (SEI). The work concludes with suggestions for further enhancements in the simulation model and proposes future research directions to explore alternative materials and configurations. Through detailed modelling and analysis, this thesis contributes to the ongoing efforts toimprove the performance and reliability of next-generation lithium-ion batteries.</p>
----------------------------------------------------------------------
In diva2:1873390 - Note: The PDF file has a problem and cannot be openned.

abstract is:
<p>Metallic fuels were produced through arc-melting. As-cast phases, microstructuresand selected mechanical properties were investigated for UZr,U-Th, and U-Th-Zr systems. For each system, two compositions wereinvestigated, with approximately 5 at. % and 20 at. % solute material, for atotal of six alloys. As-cast alloy microstructures were assessed in the contextof their equilibrium systems and compared to relevant published works whereapplicable. Mechanical testing revealed increased hardness with increasingsolute concentration, compared to the reference materials. The results supportthe conclusion that solid solution strengthening is the primary mechanismenabling this change in each binary system.Additionally, (U,Zr)N fuel was synthesized. This work exemplified aprocess to produce fuel with a homogeneous distribution of zirconium in thefuel matrix, thus representing a simulated burn-up distribution of zirconium.Refinements can be made to further improve this process in future work. Thesefindings will support a broader separate effects testing campaign underway bythe SUNRISE centre</p>

corrected abstract:
<p>Metallic fuels were produced through arc-melting. As-cast phases, microstructures and selected mechanical properties were investigated for UZr,U-Th, and U-Th-Zr systems. For each system, two compositions were investigated, with approximately 5 at. % and 20 at. % solute material, for a total of six alloys. As-cast alloy microstructures were assessed in the context of their equilibrium systems and compared to relevant published works where applicable. Mechanical testing revealed increased hardness with increasing solute concentration, compared to the reference materials. The results support the conclusion that solid solution strengthening is the primary mechanism enabling this change in each binary system. Additionally, (U,Zr)N fuel was synthe sized. This work exemplified a process to produce fuel with a homogeneous distribution of zirconium in the fuel matrix, thus representing a simulated burn-up distribution of zirconium. Refinements can be made to further improve this process in future work. These findings will support a broader separate effects testing campaign underway by the SUNRISE centre</p>
----------------------------------------------------------------------
In diva2:1831429 - Note: no full text in DiVA

abstract is:
<p>This bachelor thesis is an attempt to apply mathematical optimization to the orderingprocedure of a small business selling a perishable product. This is carried out througha collaboration with an organization selling Christmas trees each winter in a small partof Stockholm. In order to optimize the order quantity the aim is to find a practicalway to apply the newsvendor problem when the distribution of demand is unknown.Two different approaches are identified and both applied as models to solve thisproblem.The first model predicts a trend line based on historical data and assumes a normallydistributed demand. The other model uses Sample Average Approximation. Theresults from both models are analyzed and compared to the actual demand and orderdecisions made historically by the organization. The result shows that the modelwith normal distribution is more accurate than the model using Sample AverageApproximation because of the trend line. In general, both models can be useful andhave advantages in different situations. In this specific case the first model producesimproved decisions some years while others not, indicating that there are other factorsthat the model does not include. In order to produce better results than the decisionsmade by the organization improvements to the model are required.</p>

corrected abstract:
<p>This bachelor thesis is an attempt to apply mathematical optimization to the ordering procedure of a small business selling a perishable product. This is carried out through a collaboration with an organization selling Christmas trees each winter in a small partof Stockholm. In order to optimize the order quantity the aim is to find a practical way to apply the news vendor problem when the distribution of demand is unknown. Two different approaches are identified and both applied as models to solve this problem. The first model predicts a trend line based on historical data and assumes a normally distributed demand. The other model uses Sample Average Approximation. The results from both models are analyzed and compared to the actual demand and order decisions made historically by the organization. The result shows that the model with normal distribution is more accurate than the model using Sample Average Approximation because of the trend line. In general, both models can be useful and have advantages in different situations. In this specific case the first model produces improved decisions some years while others not, indicating that there are other factors that the model does not include. In order to produce better results than the decisions made by the organization improvements to the model are required.</p>
----------------------------------------------------------------------
In diva2:1817012 abstract is:
<p>Paydrive is a pioneer in the Swedish auto insurance market. Being able to influence your insurancepremium through your driving is a concept that is still in its early stages. Throughout this thesis,an attempt to consolidate the vast amounts of data gathered while driving with neural networkshas been made, together with comparisons to the currently existing generalized linear models. Inthe end, a full analysis of the data yielded four distinct groupings of customer behavior but becauseof how the data is structured the results from the modeling became sub-optimal. Insurance datais typically very skewed and zero-heavy due to the absence of accidents. The original researchquestion is whether it is possible to use two neural networks, calculating the probability of anaccident, r, and the size of a potential claim, s respectively. These two factors could be multipliedto determine a final insurance premium as c = r · s.</p><p>Using statistical standards and tools such as the Gini-coefficient, R2 values, MSE, and MAE themodels were evaluated both individually and pairwise. However, previous research in the fieldshows there haven’t been big enough advancements in this area yet. This thesis comes to the sameconclusion that due to the volatile nature of neural networks and the skewness of the data, it isincredibly difficult to get good results. Future work in the field could result in fairer prices forcustomers on their insurance premiums.</p>

corrected abstract:
<p>Paydrive is a pioneer in the Swedish auto insurance market. Being able to influence your insurance premium through your driving is a concept that is still in its early stages. Throughout this thesis, an attempt to consolidate the vast amounts of data gathered while driving with neural networks has been made, together with comparisons to the currently existing generalized linear models. In the end, a full analysis of the data yielded four distinct groupings of customer behavior but because of how the data is structured the results from the modeling became sub-optimal. Insurance data is typically very skewed and zero-heavy due to the absence of accidents. The original research question is whether it is possible to use two neural networks, calculating the probability of an accident, <em>r</e>, and the size of a potential claim, <em>s</em> respectively. These two factors could be multiplied to determine a final insurance premium as <em>c = r · s</em>.</p><p>Using statistical standards and tools such as the Gini-coefficient, <em>R<sup>2</sup></em> values, MSE, and MAE the models were evaluated both individually and pairwise. However, previous research in the field shows there haven’t been big enough advancements in this area yet. This thesis comes to the same conclusion that due to the volatile nature of neural networks and the skewness of the data, it is incredibly difficult to get good results. Future work in the field could result in fairer prices for customers on their insurance premiums.</p>
----------------------------------------------------------------------
In diva2:1764265 abstract is:
<p>The effect of proton (H+) irradiation on uranium mononitride (UN) and UN compositefuel with 10 at.% ZrN (UN10at%ZrN) was examined. Protons of 2 MeV with fluences of1E17, 1E18, 1E19 and 1E20 ions/cm2 were accelerated towards the fabricated samples in orderto investigate the evolution of the micro-structure. Stopping and Range of Ions in Matter(SRIM) calculations were performed to determine the displacements per atom associatedwith the depth of the highest damage, for each fluence.X-Ray diffraction (XRD) was used in both samples to identify the chemical composition ofeach pellet, which revealed the low presence of oxygen. Based on scanning electron microscopy(SEM), deterioration of the samples surface was observed, as the proton fluence increased.The applied stress due to the irradiation, led to the cracking of the pellets at the highestfluences. Blisters and craters appear to surround the cracked region, which might originatefrom the significant levels of hydrogen implantation within the samples.From Electron backscatter diffraction (EBSD) analysis, the grain size of the UN10at%ZrNcomposite was found to be smaller than in UN, due to the nano-particle nature of the ZrNpowder. The latter technique was also used to observe the elevated irradiated regions, whichwere further investigated by atomic force microscopy (AFM). Nano-indentation detectedirradiation hardening for both samples in the irradiated regions. Focused ion beam (FIB)milling was applied to remove lamellas from the cracked regions in both UN and compositesamples in order to be analyzed by transmission electron microscopy (TEM). The latter mightreveals the formation of dislocation loops in the irradiated areas.</p>

corrected abstract:
<p>The effect of proton (H<sup>+</sup>) irradiation on uranium mononitride (UN) and UN composite fuel with 10 at.% ZrN (UN10at%ZrN) was examined. Protons of 2 MeV with fluences of 10<sup>17</sup>, 10<sup>18</sup>, 10<sup>19</sup> and 10<sup>20</sup> ions/cm<sup>2</sup> were accelerated towards the fabricated samples in order to investigate the evolution of the micro-structure. Stopping and Range of Ions in Matter (SRIM) calculations were performed to determine the displacements per atom associated with the depth of the highest damage, for each fluence.</p><p>X-Ray diffraction (XRD) was used in both samples to identify the chemical composition of each pellet, which revealed the low presence of oxygen. Based on scanning electron microscopy (SEM), deterioration of the samples surface was observed, as the proton fluence increased. The applied stress due to the irradiation, led to the cracking of the pellets at the highest fluences. Blisters and craters appear to surround the cracked region, which might originate from the significant levels of hydrogen implantation within the samples.</p><p>From Electron backscatter diffraction (EBSD) analysis, the grain size of the UN10at%ZrN composite was found to be smaller than in UN, due to the nano-particle nature of the ZrN powder. The latter technique was also used to observe the elevated irradiated regions, which were further investigated by atomic force microscopy (AFM). Nano-indentation detected irradiation hardening for both samples in the irradiated regions. Focused ion beam (FIB) milling was applied to remove lamellas from the cracked regions in both UN and composite samples in order to be analyzed by transmission electron microscopy (TEM). The latter might reveals the formation of dislocation loops in the irradiated areas.</p>
----------------------------------------------------------------------
In diva2:1745694 - Note: no full text in DiVA
abstract is:
<p>A nuclear power plant is composed of a set of systems whose objective is to producecarbon-free, controllable and safe electricity. Safety is assured by essential elementslike the ventilation of the premises. This makes it possible to regulate the temperatureof certain equipmens and to provide pleasant climatic conditions inside the enclosure.The loss of ventilation can have serious repercussions on the operation of the plant.In this study the consequences of a loss of electrical room ventilation (DVL) followinga fire in the electrical room for the 900MW pressurised water reactors (PWR) isdetermined. France has 32 units of this type of reactor. The electrical architectureof the DVL system is studied in details.Since the Three Miles Island accident, a new safety analysis methodology has beendeveloped to complement the deterministic approach: the probabilistic approach. Ananalysis in the form of a probabilistic safety study makes it possible to quantify therisk of core damage due to the loss of ventilation. The study carries out with RiskSpectrum software shows, first of all, a significant evolution of the risk resulting fromfire in certain electrical rooms. The accidental analysis allows the identification of themost important accidental scenarios. The approach is then to identify certaincountermeasures to limit the serious phenomena resulting from this loss of ventilation.At the end, the overall risk is limited thanks to the valuation of some human actions.</p>

corrected abstract:
<p>A nuclear power plant is composed of a set of systems whose objective is to produce carbon-free, controllable and safe electricity. Safety is assured by essential elements like the ventilation of the premises. This makes it possible to regulate the temperature of certain equipments and to provide pleasant climatic conditions inside the enclosure. The loss of ventilation can have serious repercussions on the operation of the plant. In this study the consequences of a loss of electrical room ventilation (DVL) following a fire in the electrical room for the 900MW pressurised water reactors (PWR) is determined. France has 32 units of this type of reactor. The electrical architecture of the DVL system is studied in details. Since the Three Miles Island accident, a new safety analysis methodology has been developed to complement the deterministic approach: the probabilistic approach. An analysis in the form of a probabilistic safety study makes it possible to quantify the risk of core damage due to the loss of ventilation. The study carries out with RiskSpectrum software shows, first of all, a significant evolution of the risk resulting from fire in certain electrical rooms. The accidental analysis allows the identification of the most important accidental scenarios. The approach is then to identify certain countermeasures to limit the serious phenomena resulting from this loss of ventilation. At the end, the overall risk is limited thanks to the valuation of some human actions.</p>
----------------------------------------------------------------------
In diva2:1739365 abstract is:
<p>This master thesis aims in first instance to provide insights about the groundoperations that are carried out by different space players in order to exploit theirreusable space launch vehicles. The CNES is currently developing the CALLISTO flightdemonstrator with the objective of demonstrating the recovery and reuse capabilityof a vertical take-off and landing vehicle, for the benefit of future operational systemdevelopments. This paper also presents the work achieved in support of the design ofCALLISTO ground operations, namely via the use of 3D representation.The methodology adopted to perform the search and participate to the definition ofoperations is first described. Lessons are learned from the process of benchmark,specifically in the area of operations. Moreover, a comparison of the duration ofthe operational timelines of several vehicles is performed. Concerning some specificvehicles especially the DC-X, the RVT and the Falcon 9, significant information relatedto the content of the main operation phases is gathered and shown. An analysisof SpaceX actions to achieve reusability is performed. Focus is also posed on themaintenance operations in the aeronautics field. Regarding the 3D representation ofoperations, the method and the benefits are underlined with support from pictures ofthe final deliverables.</p>

corrected abstract:
<p>This master thesis aims in first instance to provide insights about the ground operations that are carried out by different space players in order to exploit their reusable space launch vehicles. The CNES is currently developing the CALLISTO flight demonstrator with the objective of demonstrating the recovery and reuse capability of a vertical take-off and landing vehicle, for the benefit of future operational system developments. This paper also presents the work achieved in support of the design of CALLISTO ground operations, namely via the use of 3D representation.</p><p>The methodology adopted to perform the search and participate to the definition of operations is first described. Lessons are learned from the process of benchmark, specifically in the area of operations. Moreover, a comparison of the duration of the operational timelines of several vehicles is performed. Concerning some specific vehicles especially the DC-X, the RVT and the Falcon 9, significant information related to the content of the main operation phases is gathered and shown. An analysis of SpaceX actions to achieve reusability is performed. Focus is also posed on the maintenance operations in the aeronautics field. Regarding the 3D representation of operations, the method and the benefits are underlined with support from pictures of the final deliverables.</p>
----------------------------------------------------------------------
In diva2:1695944 abstract is:
<p>Neutron stars are stellar objects of extreme properties. The dense core enables usto study nuclear matter beyond saturation density. The exact composition of matterat such densities is not yet established, but the thermodynamic states of the matteris theoreticized by the Equation of State (EOS). The EOS cannot be derived analyt-ically and is dependent on constraints from neutron stars and nuclear experiments inlaboratories on earth. Recent advances in astrophysical experiments have probed newconstraints on the EOS by studying properties such as mass, radius and tidal deformabil-ity of neutron stars. Especially the possibility to detect gravitational waves from mergingbinary systems by the LIGO/VIRGO collaboration and the mass-radius measurementsby NICER have contributed a great deal. Constraints from terrestrial experiments havebeen derived by studying matter at supra saturation density in Heavy Ion Collisions andby determining the neutron skin thickness. In this work, an overview of neutron stars,dense matter and the EOS is presented. Further, results of studies aiming to determineand constrain the EOS are reviewed. Even though there is consensus about some neutronstar properties among different research groups, there are still major uncertainties as allresult depend on a relatively small set of observational data. Therefore, the EOS can stillbe considered to be far from precise and the knowledge of the true neutron star matterremains undisclosed.</p>

corrected abstract:
<p>Neutron stars are stellar objects of extreme properties. The dense core enables us to study nuclear matter beyond saturation density. The exact composition of matter at such densities is not yet established, but the thermodynamic states of the matter is theoreticized by the Equation of State (EOS). The EOS cannot be derived analytically and is dependent on constraints from neutron stars and nuclear experiments in laboratories on earth. Recent advances in astrophysical experiments have probed new constraints on the EOS by studying properties such as mass, radius and tidal deformability of neutron stars. Especially the possibility to detect gravitational waves from merging binary systems by the LIGO/VIRGO collaboration and the mass-radius measurements by NICER have contributed a great deal. Constraints from terrestrial experiments have been derived by studying matter at supra saturation density in Heavy Ion Collisions and by determining the neutron skin thickness. In this work, an overview of neutron stars, dense matter and the EOS is presented. Further, results of studies aiming to determine and constrain the EOS are reviewed. Even though there is consensus about some neutron star properties among different research groups, there are still major uncertainties as all result depend on a relatively small set of observational data. Therefore, the EOS can still be considered to be far from precise and the knowledge of the true neutron star matter remains undisclosed.</p>
----------------------------------------------------------------------
In diva2:1695518 abstract is: <p>In this paper, a periodic maintenance model is formulated assumingcontinuous monitoring, imperfect preventive maintenance (PM) and perfect correctivemaintenance (CM) using three decision variables, (I, N, Z). The model is derived in aninfinite horizon context where the mean cost per unit time is modelled. PM actionsare performed N − 1 times at time instants iT for i = 1, ..., N − 1, where T = ∆T · Iand ∆T is a fixed positive number representing the minimum time allowed betweenPM actions and I is a time interval multiple representing the decision of how oftenPM actions should be performed. The N:th maintenance activity is either a plannedreplacement (if Z = 0) or a corrective replacement from letting the component runto failure (if Z = 1). Imperfect PM is modelled using age reductions, either using aconstant r or a factor γ. Previous research on assumptions of these types has beenlimited as the assumptions yield models of high complexity which are not analyticallytractable. However, assumptions of this type are considered more realistic than othermore thoroughly researched assumptions, using e.g. minimal CM. Therefore, twocomplimentary optimisation methods are proposed and evaluated, namely, completeenumeration and a specially derived genetic algorithm which can be used for differentproblem sizes respectively. Carefully determined solution bounds enabled completeenumeration to be applicable for many input parameter values which is a great strengthof the proposed model.</p>


corrected abstract:
<p>In this paper, a periodic maintenance model is formulated assuming continuous monitoring, imperfect preventive maintenance (PM) and perfect corrective maintenance (CM) using three decision variables, (<em>I</em>, <em>N</em>, <em>Z</em>). The model is derived in an infinite horizon context where the mean cost per unit time is modelled. PM actions are performed <em>N − 1</em> times at time instants <em>iT<em> for <em>i = 1, &mldr;, N − 1</em>, where <em>T = ∆T · I</em> and <em>∆T</em> is a fixed positive number representing the minimum time allowed between PM actions and <em>I</em> is a time interval multiple representing the decision of how often PM actions should be performed. The <em>N</em>:th maintenance activity is either a planned replacement (if <em>Z = 0</em>) or a corrective replacement from letting the component run to failure (if <em>Z = 1</em>). Imperfect PM is modelled using age reductions, either using a constant <em>r</em> or a factor <em>γ</em>. Previous research on assumptions of these types has been limited as the assumptions yield models of high complexity which are not analytically tractable. However, assumptions of this type are considered more realistic than other more thoroughly researched assumptions, using e.g. minimal CM. Therefore, two complimentary optimisation methods are proposed and evaluated, namely, complete enumeration and a specially derived genetic algorithm which can be used for different problem sizes respectively. Carefully determined solution bounds enabled complete enumeration to be applicable for many input parameter values which is a great strength of the proposed model.</p>
----------------------------------------------------------------------
In diva2:1682167 abstract is:
<p>Parametric amplifiers are essential for analyzing and measuring the weak signals generated byquantum circuits at cryogenic temperatures. This project aims to realize a low noise travelingwave parametric amplifier (TWPA) by exploiting the nonlinear current dependence of thekinetic inductance of superconducting NbTiN nanowires. We fabricate an inductor in theform of a compact meandering nanostructure on small chips. We describe the microwavecircuit design, the simulations performed and the fabrication recipes. We present the resultsfrom the initial measurements at low-temperature (4.2 K - 0.3 K) performed in a 3He dipstickcryostat.We analyzed two different structures in this thesis. The first design implements a co-planarwaveguide structure that operates as a multi-modal cavity with resonances that can be modifiedby adjusting geometrical parameters. In contrast, the second design attempts to eliminate theseresonances by matching the impedance of the device with that of the input and output signallines. For this reason, we adopted a microstrip structure with a top-layered ground plane.In addition, the second design allows for phase matching of the signal and idler frequenciesinvolved in parametric amplification through dispersion engineering. Finally, we determineimportant parameters like the temperature dependence of the kinetic inductance, phase velocity,and characteristic impedance of the devices at cryogenic temperatures.</p>

corrected abstract:
<p>Parametric amplifiers are essential for analyzing and measuring the weak signals generated by quantum circuits at cryogenic temperatures. This project aims to realize a low noise traveling wave parametric amplifier (TWPA) by exploiting the nonlinear current dependence of the kinetic inductance of superconducting NbTiN nanowires. We fabricate an inductor in the form of a compact meandering nanostructure on small chips. We describe the microwave circuit design, the simulations performed and the fabrication recipes. We present the results from the initial measurements at low-temperature (4.2 K - 0.3 K) performed in a <sup>3</sup>He dipstick cryostat.</p><p>We analyzed two different structures in this thesis. The first design implements a co-planar waveguide structure that operates as a multi-modal cavity with resonances that can be modified by adjusting geometrical parameters. In contrast, the second design attempts to eliminate these resonances by matching the impedance of the device with that of the input and output signal lines. For this reason, we adopted a microstrip structure with a top-layered ground plane. In addition, the second design allows for phase matching of the signal and idler frequencies involved in parametric amplification through dispersion engineering. Finally, we determine important parameters like the temperature dependence of the kinetic inductance, phase velocity, and characteristic impedance of the devices at cryogenic temperatures.</p>
----------------------------------------------------------------------
In diva2:1673657 abstract is:
<p>The evolution of electronics in the vehicle industry has introduced the possibility for more X-by-wire systems in future vehicles. However, the use of steer-by-wire systems has not yet been widelyimplemented. This opens up an opportunity to explore strategies around the potential use of such asystem.The purpose of the project was to evaluate how to design a variable steering ratio which would give asuitable ratio in all speeds. This would, in turn, make it possible to reduce the need for large steeringwheel angles. Additionally, steering wheel designs which can be implemented with a steer-by-wiresystems are discussed and what possibilities there are to move certain interfaces to the steering wheel.The evaluation process consisted of driving a real car with a constant steering ratio in normal trafficand later modelling a variable steering ratio and testing it in a simulator. This was to get data on howlarge steering wheel angles that are needed in different driving scenarios to then be able to design asuitable variable steering ratio. The tests conducted on normal roads in a real car has shown that thedriver utilises the whole steering range (full lock-lock distance) at speeds below 30 km/h and about±10° on the steering wheel at high speeds. The tests conducted in the simulator show that the variablesteering ratio presented in this report decreases the workload on the driver most of all at speeds below30 km/h.The variable steering ratio presented has been compared with a fixed steering ratio in the simulatorand the tests show that the variable steering ratio works similar to the fixed steering ratio in the samescenarios. The variable steering ratio also decreases the need for a steering wheel angle greater than±180°.</p><p> </p>

corrected abstract:
<p>The evolution of electronics in the vehicle industry has introduced the possibility for more X-by-wire systems in future vehicles. However, the use of steer-by-wire systems has not yet been widely implemented. This opens up an opportunity to explore strategies around the potential use of such a system.</p><p>The purpose of the project was to evaluate how to design a variable steering ratio which would give a suitable ratio in all speeds. This would, in turn, make it possible to reduce the need for large steering wheel angles. Additionally, steering wheel designs which can be implemented with a steer-by-wire systems are discussed and what possibilities there are to move certain interfaces to the steering wheel.</p><p>The evaluation process consisted of driving a real car with a constant steering ratio in normal traffic and later modelling a variable steering ratio and testing it in a simulator. This was to get data on how large steering wheel angles that are needed in different driving scenarios to then be able to design a suitable variable steering ratio. The tests conducted on normal roads in a real car has shown that the driver utilises the whole steering range (full lock-lock distance) at speeds below 30 km/h and about ±10° on the steering wheel at high speeds. The tests conducted in the simulator show that the variable steering ratio presented in this report decreases the workload on the driver most of all at speeds below 30 km/h.</p><p>The variable steering ratio presented has been compared with a fixed steering ratio in the simulator and the tests show that the variable steering ratio works similar to the fixed steering ratio in the same scenarios. The variable steering ratio also decreases the need for a steering wheel angle greater than ±180°.</p>
----------------------------------------------------------------------
In diva2:1656357 abstract is:
<p>With the rise of artificial intelligence, applications for machine learning can be found in nearly everyaspect of modern life, from healthcare and transportation to software services like recommendationsystems. Consequently, there are now more developers engaged in the field than ever - with the numberof implementations rapidly increasing by the day. In order to meet the new demands, it would be usefulto provide services that allow for an easy orchestration of a large number of repositories. Enabling usersto easily share, access and search for source code would be beneficial for both research and industryalike. A first step towards this is to find methods for clustering source code by functionality.</p><p>The problem of clustering source code has previously been studied in the literature. However, theproposed methods have so far not leveraged the capabilities of deep neural networks (DNN). In thiswork, we investigate the possibility of using DNNs to learn embeddings of source code for the purpose ofclustering by functionality. In particular, we evaluate embeddings from Code2Vec and cuBERT modelsfor this specific purpose.</p><p>From the results of our work we conclude that both Code2Vec and cuBERT are capable of learningsuch embeddings. Among the different frameworks that we used to fine-tune cuBERT, we found thebest performance for this task when fine-tuning the model under the triplet loss criterion. With thisframework, the model was capable of learning embeddings that yielded the most compact and well-separated clusters. We found that a majority of the cluster assignments were semantically coherent withrespect to the functionalities implemented by the methods. With these results, we have found evidenceindicating that it is possible to learn embeddings of source code that encode the functional similaritiesamong the methods. Future research could therefore aim to further investigate the possible applicationsof the embeddings learned by the different frameworks.</p>

corrected abstract:
<p>With the rise of artificial intelligence, applications for machine learning can be found in nearly every aspect of modern life, from healthcare and transportation to software services like recommendation systems. Consequently, there are now more developers engaged in the field than ever - with the number of implementations rapidly increasing by the day. In order to meet the new demands, it would be useful to provide services that allow for an easy orchestration of a large number of repositories. Enabling users to easily share, access and search for source code would be beneficial for both research and industry alike. A first step towards this is to find methods for clustering source code by functionality.</p><p>The problem of clustering source code has previously been studied in the literature. However, the proposed methods have so far not leveraged the capabilities of deep neural networks (DNN). In this work, we investigate the possibility of using DNNs to learn embeddings of source code for the purpose of clustering by functionality. In particular, we evaluate embeddings from Code2Vec and cuBERT models for this specific purpose.</p><p>From the results of our work we conclude that both Code2Vec and cuBERT are capable of learning such embeddings. Among the different frameworks that we used to fine-tune cuBERT, we found the best performance for this task when fine-tuning the model under the triplet loss criterion. With this framework, the model was capable of learning embeddings that yielded the most compact and well-separated clusters. We found that a majority of the cluster assignments were semantically coherent with respect to the functionalities implemented by the methods. With these results, we have found evidence indicating that it is possible to learn embeddings of source code that encode the functional similarities among the methods. Future research could therefore aim to further investigate the possible applications of the embeddings learned by the different frameworks.</p>
----------------------------------------------------------------------
In diva2:1655431 abstract is:
<p>In recent years the number of Special Purpose Acquisition Companies that have entered financialmarkets has increased tremendously, especially in the United States. An important query to this ishow they are valued and priced on stock exchanges. Using multiple regression analysis, the thesisexamines how different factors in a Special Purpose Acquisition Company relates to the stock price atdifferent phases of the companies lifetime. Further, it analyzes if these factors can help determining thestock price of similar companies in the future by proposing a model. Our result presents two modelswhich describes the stock price at target announced date and merger complete date respectively. Thetarget announced model is dependent of the initial term of the company, how many days until a targetcompany is specified and also if the company is specified to acquire in tech, energy or healthcare.However, the presented model for the target announced stock price shows weak accuracy and shouldin our proposition not be used in practice — and if at all — with great caution. The merger completemodel that is presented shows that the merger complete stock price is highly dependent on the targetannounced stock price, and although this model is somewhat more accurate, its practical purposes isalso limited due to inaccuracy. In conclusion, despite some relationship between the stock price andexamined factors being detected, our models are not appropriate to be used for practical purposes.Although, the target announced stock price can be used as a indicator of in which direction the mergercomplete stock price is heading.</p>

corrected abstract:
<p>In recent years the number of Special Purpose Acquisition Companies that have entered financial markets has increased tremendously, especially in the United States. An important query to this is how they are valued and priced on stock exchanges. Using multiple regression analysis, the thesis examines how different factors in a Special Purpose Acquisition Company relates to the stock price at different phases of the companies lifetime. Further, it analyzes if these factors can help determining the stock price of similar companies in the future by proposing a model. Our result presents two models which describes the stock price at target announced date and merger complete date respectively. The target announced model is dependent of the initial term of the company, how many days until a target company is specified and also if the company is specified to acquire in tech, energy or healthcare. However, the presented model for the target announced stock price shows weak accuracy and should in our proposition not be used in practice — and if at all — with great caution. The merger complete model that is presented shows that the merger complete stock price is highly dependent on the target announced stock price, and although this model is somewhat more accurate, its practical purposes is also limited due to inaccuracy. In conclusion, despite some relationship between the stock price and examined factors being detected, our models are not appropriate to be used for practical purposes. Although, the target announced stock price can be used as a indicator of in which direction the merger complete stock price is heading.</p>
----------------------------------------------------------------------
In diva2:1613485 abstract is:
<p>Using autonomous underwater vehicles is a popular method of collecting samplesand conducting surveys, but the transportation of this information is not always easy.The underwater vehicle might be unable to transmit the information wirelessly, andsamples may be required to be transported a long distance. A possible solution tothese problems is a hybrid unmanned aerial vehicle, accompanying said underwatervehicle. After a submerged deployment, this vehicle could transport the informationover long distances, or conduct other operations at different locations in air or water.While quadcopters are an increasingly popular type of vehicle, conventional fixed­wingplanes are still superior when it comes to range. This thesis designs, builds and testssuch a vehicle, with the goal of a submerged deployment, performing vertical takeoff,and then transitioning to fixed­wing flight. To minimize the drone’s impact on thevehicle which it accompanies, it is nearly buoyancy neutral by flooding the hull withwater, which enters and exits the vehicle rapidly during dive and egress. To managethe pressure at the underwater vehicle’s operating depth, it utilizes a bladder ratherthan having a heavy rigid compartment. It floats as a tailsitter at the surface, usingtwo motors in a tractor configuration to pull itself out of the water. The vehicle builtproved capable of being submerged and taking off vertically, however there were nofixed­wing flights attempted.</p>


corrected abstract:
<p>Using autonomous underwater vehicles is a popular method of collecting samples and conducting surveys, but the transportation of this information is not always easy. The underwater vehicle might be unable to transmit the information wirelessly, and samples may be required to be transported a long distance. A possible solution to these problems is a hybrid unmanned aerial vehicle, accompanying said underwater vehicle. After a submerged deployment, this vehicle could transport the information over long distances, or conduct other operations at different locations in air or water. While quadcopters are an increasingly popular type of vehicle, conventional fixed-wing planes are still superior when it comes to range. This thesis designs, builds and tests such a vehicle, with the goal of a submerged deployment, performing vertical takeoff, and then transitioning to fixed­wing flight. To minimize the drone’s impact on the vehicle which it accompanies, it is nearly buoyancy neutral by flooding the hull with water, which enters and exits the vehicle rapidly during dive and egress. To manage the pressure at the underwater vehicle’s operating depth, it utilizes a bladder rather than having a heavy rigid compartment. It floats as a tailsitter at the surface, using two motors in a tractor configuration to pull itself out of the water. The vehicle built proved capable of being submerged and taking off vertically, however there were no fixed­wing flights attempted.</p>
----------------------------------------------------------------------
In diva2:1528147 abstract is:
<p>This paper presents the work from a Master thesisat Bauhaus Luftfahrt (Munich, Germany) about subsystemdesign and analysis for electric commercial aircraft, particularlythe assessment of a Solid Oxide Fuel Cell (SOFC) powering a fullelectric subsystem architecture. The different components of theSOFC system architecture are modelled and assessed with theOpenMDAO framework. They are then assembled together toassess the performance of the whole SOFC system architecture,the main goal being to replace the conventional Auxiliary PowerUnit (APU) on the ground and to provide energy to all thesubsystems (e.g. flight controls, air conditioning, ice and rainprotection among others) of the aircraft during flight andground operations. The mass of the different components of theSOFC system is calculated, and a 2% operational empty massincrease is assumed for subsystem electrification. The resultsshow a kerosene block fuel reduction of 2.1% compared to theconventional baseline aircraft.</p>

corrected abstract:
<p>This paper presents the work from a Master thesis at Bauhaus Luftfahrt (Munich, Germany) about subsystem design and analysis for electric commercial aircraft, particularly the assessment of a Solid Oxide Fuel Cell (SOFC) powering a full electric subsystem architecture. The different components of the SOFC system architecture are modelled and assessed with the OpenMDAO framework. They are then assembled together to assess the performance of the whole SOFC system architecture, the main goal being to replace the conventional Auxiliary Power Unit (APU) on the ground and to provide energy to all the subsystems (e.g. flight controls, air conditioning, ice and rain protection among others) of the aircraft during flight and ground operations. The mass of the different components of the SOFC system is calculated, and a 2% operational empty mass increase is assumed for subsystem electrification. The results show a kerosene block fuel reduction of 2.1% compared to the conventional baseline aircraft.</p>
----------------------------------------------------------------------
In diva2:1528135 - missing spaces in title:
"Development of an underwaterruler using an AUV-deployedbeacon and Matched filter CFARdetector"
==>
"Development of an underwater ruler using an AUV-deployed beacon and Matched filter CFAR detector"

abstract is:
<p>Our ocean covers more than 70% of our planet’s surface. It is a huge reservoir continuously supplying us with enormousvaluable resources. The autonomous aquaculture, deep sea mining, subsea oil / gas exploitation and marine biology etc.,are the increasingly important driving forces for the development of underwater robotics. For all underwater robotics,navigation and positioning system often endures a challenging problem due to the high attenuation of radio-frequencysignals and the lack of Global Positioning System (GPS). Normally, acoustic navigation is the only way for theunderwater robotics to have the accurate navigation. One core element for the acoustic localisation is the rangeestimation. To provide an accurate range estimate from the underwater vehicles to the fixed reference point, an dropbeaconneeds to be established. In the underwater environment, the acoustic signals suffer from multi-path effects,ambient noises etc., which together with its simplified hardware can impose restrictions on the development of a perfectsystem. This project employs a very simplified hardware to deal with the range estimation problem from the beacon tothe transmitter subjected to the above mentioned issues. The algorithm consists of three main components includingdownsampling, matched filtering and CFAR detection. The downsampling is comprised of three steps such asbasebanding, lowpass filtering and resampling with lower sampling rate. By simulating four different signal typesincluding sinusoidal signal, frequency-modulated signals and M-sequence signal, the sinusoidal waveform is selected tosuit the system’s objective both for simplicity and robustness. A series of tests including tube test, water tank test, nearfieldopen water tests as well as LoLo integration and far-field tests verified and validated the system’s capability toestimate the accurate range (error ≤ 1m) in near field cases (≤ 20 meters). For far-field tests, it proved some furtherimprovements need to be accomplished before it is able to carry out long range missions.</p>

corrected abstract:
<p>Our ocean covers more than 70% of our planet’s surface. It is a huge reservoir continuously supplying us with enormous valuable resources. The autonomous aquaculture, deep sea mining, subsea oil / gas exploitation and marine biology etc., are the increasingly important driving forces for the development of underwater robotics. For all underwater robotics, navigation and positioning system often endures a challenging problem due to the high attenuation of radio-frequency signals and the lack of Global Positioning System (GPS). Normally, acoustic navigation is the only way for the underwater robotics to have the accurate navigation. One core element for the acoustic localisation is the range estimation. To provide an accurate range estimate from the underwater vehicles to the fixed reference point, an drop-beacon needs to be established. In the underwater environment, the acoustic signals suffer from multi-path effects, ambient noises etc., which together with its simplified hardware can impose restrictions on the development of a perfect system. This project employs a very simplified hardware to deal with the range estimation problem from the beacon to the transmitter subjected to the above mentioned issues. The algorithm consists of three main components including downsampling, matched filtering and CFAR detection. The downsampling is comprised of three steps such as basebanding, lowpass filtering and resampling with lower sampling rate. By simulating four different signal types including sinusoidal signal, frequency-modulated signals and M-sequence signal, the sinusoidal waveform is selected to suit the system’s objective both for simplicity and robustness. A series of tests including tube test, water tank test, nearfield open water tests as well as LoLo integration and far-field tests verified and validated the system’s capability to estimate the accurate range (error ≤ 1m) in near field cases (≤ 20 meters). For far-field tests, it proved some further improvements need to be accomplished before it is able to carry out long range missions.</p>
----------------------------------------------------------------------
In diva2:1216693 - missing space in title:
Autonomous Overtaking Using Reachability Analysisand MPC
==>
"Autonomous Overtaking Using Reachability Analysis and MPC"

abstract is:
<p>The era of autonomous cars is on the rise. Asdrivers lose control of the steering wheel, it is crucial that thecars themselves can guarantee safety for all traffic participants.This study aims to design a complete control system that cansafely perform an overtaking maneuver. To guarantee safety ofthe maneuver, reachability calculations will be carried out andanalyzed. The overtaking will be planned by using the modelpredictive control, MPC, framework. To complete the controlsystem a modified proportional controller will be designed totrack the planned path. The control system is implemented inMATLAB and the entire overtaking maneuver is simulated. Theresults show that the designed control framework successfullyperforms the overtaking on a straight two-lane highway in asafe manner.</p>

corrected abstract:
<p>The era of autonomous cars is on the rise. As drivers lose control of the steering wheel, it is crucial that the cars themselves can guarantee safety for all traffic participants. This study aims to design a complete control system that can safely perform an overtaking maneuver. To guarantee safety of the maneuver, reachability calculations will be carried out and analyzed. The overtaking will be planned by using the model predictive control, MPC, framework. To complete the control system a modified proportional controller will be designed to track the planned path. The control system is implemented in MATLAB and the entire overtaking maneuver is simulated. The results show that the designed control framework successfully performs the overtaking on a straight two-lane highway in a safe manner.</p>
----------------------------------------------------------------------
In diva2:1210175
abstract is:
<p>This thesis investigates the use of Artificial Neural Networks (ANNs)for calculating present values, Value-at-Risk and Expected Shortfall ofoptions, both European call options and more complex rainbow options. Theperformance of the ANN is evaluated by comparing it to a second-order Taylorpolynomial using pre-calculated sensitivities to certain risk-factors. Amultilayer perceptron approach is chosen based on previous literature andapplied to both types of options. The data is generated from a financial risk-managementsoftware for both call options and rainbow options along with the relatedTaylor approximations. The study shows that while the ANN outperforms theTaylor approximation in calculating present values and risk measures forcertain movements in the underlying risk-factors, the general conclusion isthat an ANN trained and evaluated in accordance with the method in this studydoes not outperform a Taylor approximation even if it is theoretically possiblefor the ANN to do so. The important conclusion of the study is that the ANNseems to be able to learn to calculate present values that otherwise requireMonte Carlo simulation. Thus, the study is a proof of concept that requiresfurther development for implementation.</p>

corrected abstract:
<p>This thesis investigates the use of Artificial Neural Networks (ANNs) for calculating present values, Value-at-Risk and Expected Shortfall of options, both European call options and more complex rainbow options. The performance of the ANN is evaluated by comparing it to a second-order Taylor polynomial using pre-calculated sensitivities to certain risk-factors. A multilayer perceptron approach is chosen based on previous literature and applied to both types of options. The data is generated from a financial risk-management software for both call options and rainbow options along with the related Taylor approximations. The study shows that while the ANN outperforms the Taylor approximation in calculating present values and risk measures for certain movements in the underlying risk-factors, the general conclusion is that an ANN trained and evaluated in accordance with the method in this study does not outperform a Taylor approximation even if it is theoretically possible for the ANN to do so. The important conclusion of the study is that the ANN seems to be able to learn to calculate present values that otherwise require Monte Carlo simulation. Thus, the study is a proof of concept that requires further development for implementation.</p>
----------------------------------------------------------------------
In diva2:1156678 - Note: no full text in DiVA
abstract is:
<p>Many industries are currently making the transition to high strength steel due to advantages relatedto the high strength to weight ratio of high strength steel. The fatigue properties in high strength steelshow a large dependency of the weld quality in structural applications. Recent studies show a weakrelation between quality classifications and fatigue properties in the current international standard forweld quality assurance, ISO 5817. The objective of this project is to develop and assess a method forpredicting fatigue life in welded joints based on measured weld geometry and applied load.Two different materials (S355 and S960) and two different material thicknesses (2mm and 8mm) wereused in this study. Experiments on cruciform joints were conducted to evaluate the fatigueperformance for different types of weld geometries. A computational model based on FEM and linearelastic fracture mechanics was developed and adapted to fit the experimental results usingoptimization and surrogate models.A comparison between the fatigue behavior according to the standards and the fatigue behavior givenby the computational model was thereafter made. The conclusions that can be made are that thegeneral fatigue behavior differs for the different materials for the same variation in geometry, thefatigue performance depends on a combination of geometrical parameters, the developed computational model cannot be used for the 2mm specimens in this particular case, the use of FAT-curves accordingto the weld quality systems is insufficient to describe fatigue properties for welds in thin high strengthsteel, and different geometries within different weld quality classes can give the same fatigue behavior.</p>

corrected abstract:
<p>Many industries are currently making the transition to high strength steel due to advantages related  to the high strength to weight ratio of high strength steel. The fatigue properties in high strength steel show a large dependency of the weld quality in structural applications. Recent studies show a weak relation between quality classifications and fatigue properties in the current international standard for weld quality assurance, ISO 5817. The objective of this project is to develop and assess a method for predicting fatigue life in welded joints based on measured weld geometry and applied load. Two different materials (S355 and S960) and two different material thicknesses (2mm and 8mm) were used in this study. Experiments on cruciform joints were conducted to evaluate the fatigue performance for different types of weld geometries. A computational model based on FEM and linearelastic fracture mechanics was developed and adapted to fit the experimental results using optimization and surrogate models. A comparison between the fatigue behavior according to the standards and the fatigue behavior given by the computational model was thereafter made. The conclusions that can be made are that the general fatigue behavior differs for the different materials for the same variation in geometry, the fatigue performance depends on a combination of geometrical parameters, the developed computational model cannot be used for the 2mm specimens in this particular case, the use of FAT-curves according to the weld quality systems is insufficient to describe fatigue properties for welds in thin high strength steel, and different geometries within different weld quality classes can give the same fatigue behavior.</p>
----------------------------------------------------------------------
In diva2:1120538 missing spaces in title:
"Effective Sampling and Windowingfor an Artificial Neural Network Model Used in Currency Exchange Rate Forecasting"
==>
"Effective Sampling and Windowing for an Artificial Neural Network Model Used in Currency Exchange Rate Forecasting"

abstract is: <p>Financial forecasting is a field of great interest in academia and economy. The subfield of exchangerate prediction is of considerable value to practically every entity operating within the financialmarket. Ranging from private hedgers, speculators or arbitrageurs to entire financial institutions suchas international banks or insurance companies, the ability to predict exchange rate movementsprovides major benefits for organizations in contact with these. A great multitude of research has beenconducted to construct methods to aid firms and investors to better anticipate on potentialdevelopments in the foreign exchange market. Much of the research has been focusing on a promisingprediction model within computational intelligence developed in recent decades, namely the ArtificialNeural Network (ANN). However, a review of existing literature suggests that the time step, predictionhorizon and window size have not been of central essence. Hence, this paper attempts to provide amore formal analysis of the actual impact of the three mentioned parameters on the prediction resultsof ANNs. Through literature studies, modeling and experimentation it is found that no specificcombination of time step, prediction horizon and window size results in more exact forecasts, but thatcertain combinations of the three parameters generally result in superior performance.</p>


corrected abstract:
<p>Financial forecasting is a field of great interest in academia and economy. The subfield of exchange rate prediction is of considerable value to practically every entity operating within the financial market. Ranging from private hedgers, speculators or arbitrageurs to entire financial institutions such as international banks or insurance companies, the ability to predict exchange rate movements provides major benefits for organizations in contact with these. A great multitude of research has been conducted to construct methods to aid firms and investors to better anticipate on potential developments in the foreign exchange market. Much of the research has been focusing on a promising prediction model within computational intelligence developed in recent decades, namely the Artificial Neural Network (ANN). However, a review of existing literature suggests that the time step, prediction horizon and window size have not been of central essence. Hence, this paper attempts to provide a more formal analysis of the actual impact of the three mentioned parameters on the prediction results of ANNs. Through literature studies, modeling and experimentation it is found that no specific combination of time step, prediction horizon and window size results in more exact forecasts, but that certain combinations of the three parameters generally result in superior performance.</p>
----------------------------------------------------------------------
In diva2:1078070 abstract is:
<p>The preliminary design of Guidance, Navigation, and Control (GNC) algorithms andAttitude and Orbital Control Systems (AOCS) for spacecraft plays an important rolein the planning of a space mission. Many missions require accurate positioning andattitude control to be able to full the mission objectives. In an early phase of conceptualstudies, trade-os have to be made to the GNC/AOCS subsystem designand compromises with respect to other subsystem designs have to be taken intoaccount. This demands for the possibility of rapid prototyping where design parameters,such as the choice of sensors and actuators, can be changed easily to assessthe compliance to the mission requirements. This thesis presents the modelling ofGNC/AOCS components for a toolbox created in the MATLAB/Simulink environment.The resulting toolbox is a user-friendly tool, which simplies the creationof GNC/AOCS system simulations for conceptual studies. A number of completesimulations were constructed to demonstrate the capabilities of the toolbox.</p>

corrected abstract:
<p>The preliminary design of Guidance, Navigation, and Control (GNC) algorithms and Attitude and Orbital Control Systems (AOCS) for spacecraft plays an important role in the planning of a space mission. Many missions require accurate positioning and attitude control to be able to fulfil the mission objectives. In an early phase of conceptual studies, trade-offs have to be made to the GNC/AOCS subsystem design and compromises with respect to other subsystem designs have to be taken into account. This demands for the possibility of rapid prototyping where design parameters, such as the choice of sensors and actuators, can be changed easily to assess the compliance to the mission requirements. This thesis presents the modelling of GNC/AOCS components for a toolbox created in the MATLAB/Simulink environment. The resulting toolbox is a user-friendly tool, which simplifies the creation of GNC/AOCS system simulations for conceptual studies. A number of complete simulations were constructed to demonstrate the capabilities of the toolbox.</p>
----------------------------------------------------------------------
In diva2:941181 - missing space in title:
"MicroCT system software updateand implementation of a beamhardening correction method"
==>
"MicroCT system software update and implementation of a beam hardening correction method"

abstract is:
<p>The School of Technology and Health, STH, has been developing a micro-CTscanner for pre-clinical imaging. The scanner consists of an X-ray tube and aflat panel sensor fixated on a gantry rotating around the object to be imaged. Acomputer located on the gantry runs an acquisition software for communicationbetween the devices on the gantry as well as for controlling them. For this thesisthe acquisition software has been significantly improved in terms of functionality,performance, usability and user-adaptivity.Projection images acquired by the microCT are created by measuring theX-ray beam attenuation as a function of spatial location. Using all projectionimages, a 3-dimensional image can be reconstructed giving a map of the attenuationinside the object, thus providing information about its internal structure.A common artifact for CT-scanner images is the <em>cupping arti fact</em> which the attenuation in the middle of an object is underestimated due to beam hardening.For the second part of this thesis, a method for correcting for this artifact hasbeen implemented and tested. The cupping artifact was successfully removedin most cases, although it was shown that for some situation it is not applicable</p>

corrected abstract:
<p>The School of Technology and Health, STH, has been developing a micro-CT scanner for pre-clinical imaging. The scanner consists of an X-ray tube and a flat panel sensor fixated on a gantry rotating around the object to be imaged. A computer located on the gantry runs an acquisition software for communication between the devices on the gantry as well as for controlling them. For this thesis the acquisition software has been significantly improved in terms of functionality, performance, usability and user-adaptivity.</p><p>Projection images acquired by the microCT are created by measuring the X-ray beam attenuation as a function of spatial location. Using all projection images, a 3-dimensional image can be reconstructed giving a map of the attenuation inside the object, thus providing information about its internal structure. A common artifact for CT-scanner images is the <em>cupping&mdash;artifact</em>, in which the attenuation in the middle of an object is underestimated due to beam hardening. For the second part of this thesis, a method for correcting for this artifact has been implemented and tested. The cupping artifact was successfully removed in most cases, although it was shown that for some situation it is not applicable</p>

Note: The original abstract does not end with a period.
----------------------------------------------------------------------
In diva2:852543 abstract is:
<p>This first level thesis focuses on the design of a human poweredaircraft which theoretically can complete the Kremer InternationalMarathon prize. The competition awards the firstheavier-than-air aircraft human-power-driven aircraft that completesa circuit of 42 195 meter in less than one hour with £50000.</p><p>The paper first introduces the reader to the basics of aircraftmechanics. It continues with the presentation of the design processwith examples of evaluated airfoils and key aspects.</p><p>Our conclusion of the study is that the Kremer InternationalMarathon prize is fully feasible to complete with the technologyof today. However, the project is not economically viabledue to high prizes of selected composite materials and the relativesmall size of the prize money. An increment of the prizewould certainly increase the research into human powered highperformance aircraft, which could lead to a successful KremerInternational Marathon prize attempt.</p><p>The limitations of this thesis are to strictly stay within the aerodynamicfield of study. Therefore, solid mechanics has not beenexactly calculated, but has been taken into account when dimensioningthe aircraft in CAD, and neither has any calculatedturning performance due to academic level.</p>

corrected abstract:
<p>This first level thesis focuses on the design of a human powered aircraft which theoretically can complete the Kremer International Marathon prize. The competition awards the first heavier-than-air aircraft human-power-driven aircraft that completes a circuit of 42 195 meter in less than one hour with £50000.</p><p>The paper first introduces the reader to the basics of aircraft mechanics. It continues with the presentation of the design process with examples of evaluated airfoils and key aspects.</p><p>Our conclusion of the study is that the Kremer International Marathon prize is fully feasible to complete with the technology of today. However, the project is not economically viable due to high prizes of selected composite materials and the relative small size of the prize money. An increment of the prize would certainly increase the research into human powered highperformance aircraft, which could lead to a successful Kremer International Marathon prize attempt.</p><p>The limitations of this thesis are to strictly stay within the aerodynamic field of study. Therefore, solid mechanics has not been exactly calculated, but has been taken into account when dimensioning the aircraft in CAD, and neither has any calculated turning performance due to academic level.</p>
----------------------------------------------------------------------
In diva2:515563 abstract is:
<p>Recent studies show that the knowledge in and understanding of science, technology andmathematics of Swedish junior high school and high school students is decreasing. As a resultof this the Swedish government has initialized several studies, research and reforms. One ofthe theories for teaching that today is seen as effective is the socio-cultural perspective and itsdescendants, including scientific inquiry. Using the pedagogy of the socio-cultural perspectivefour assignments regarding the human body was constructed for junior high school studentsattending summer research school in the summer of 2008, hosted by AstraZeneca. Theassignments follow a structure (goal of assignment, preparations, experiment and follow-up)inspired by the concept of NTA – Naturvetenskap och Teknik för Alla (Science andTechnology for everyone). The assignments can be found (in Swedish) as an appendix to thispaper. The assignments were evaluated and assessed through the observations made by theteacher and two questionnaires separated by six months. Results showed that three of the fourassignments had fulfilled the aim of making the students interested and engaged in scientificquestions. The forth had only partially achieved this, mostly due to a silent, built-in structureexpecting to generate certain knowledge in the students. This contradicts the socio-culturalperspective where new knowledge should be derived from what students already know.Seeing as the socio-cultural perspective was effective when using the other three assignmentsthis study shows that assignments constructed with a socio-cultural perspective are usefulwhen teaching science in junior high school.Nyckelord</p>

corrected abstract:
<p>Recent studies show that the knowledge in and understanding of science, technology and mathematics of Swedish junior high school and high school students is decreasing. As a result of this the Swedish government has initialized several studies, research and reforms. One of the theories for teaching that today is seen as effective is the socio-cultural perspective and its descendants, including scientific inquiry. Using the pedagogy of the socio-cultural perspective four assignments regarding the human body was constructed for junior high school students attending summer research school in the summer of 2008, hosted by AstraZeneca. The assignments follow a structure (goal of assignment, preparations, experiment and follow-up) inspired by the concept of NTA – <span lang="sv">Naturvetenskap och Teknik för Alla</span> (Science and Technology for everyone). The assignments can be found (in Swedish) as an appendix to this paper. The assignments were evaluated and assessed through the observations made by the teacher and two questionnaires separated by six months. Results showed that three of the four assignments had fulfilled the aim of making the students interested and engaged in scientific questions. The forth had only partially achieved this, mostly due to a silent, built-in structure expecting to generate certain knowledge in the students. This contradicts the socio-cultural perspective where new knowledge should be derived from what students already know. Seeing as the socio-cultural perspective was effective when using the other three assignments this study shows that assignments constructed with a socio-cultural perspective are useful when teaching science in junior high school.</p>
----------------------------------------------------------------------
In diva2:515459 - missing space, unnecessary hyphen, and unnecessary period in (sub)title:
"Environmental training for municipality officers inBosnia and Herzegovina: - A needs analysis performed before the start of a postgraduate programme for environmentalofficers in municipalities in Bosnia and Herzegovina."
==>
"Environmental training for municipality officers in Bosnia and Herzegovina: - A needs analysis performed before the start of a postgraduate programme for environmental officers in municipalities in Bosnia and Herzegovina"

abstract is:
<p>This is a study that aims to help create a better world. As big as it sounds, it is the truth. Everyday municipality officers in Bosnia and Herzegovina work for a better environment, a betterworld. Despite their efforts the work they do is not as efficient as it could be and the supportavailable for them is meagre. For this reason a programme called Municipality EnvironmentalInfrastructure is under development in cooperation between the University of Sarajevo (Bosniaand Herzegovina) and the Royal Institute of Technology (Sweden) with financing from theSwedish International Development Cooperation Agency (Sida). This thesis is a part of the workto make the programme successful.Competence needs for municipality officers will be identified through a needs analysis based oninterviews with different stakeholders. The answers provided will create a picture of the needsthat is both univocal and diverse with competences in identifying and handling environmentalthreats as well as managing infrastructure projects.The thesis will also look at what pedagogical methods the teachers at the programme plan to useand how this affects the programme. Since the programme is held in a formal setting but intendspractical use of the knowledge this leads to high demands on the pedagogical methods. Theprogramme syllabus will be fount to not entirely encompass all competence needs butsuggestions will be made as to how to include the identified needs.</p>

corrected abstract:
<p>This is a study that aims to help create a better world. As big as it sounds, it is the truth. Every day municipality officers in Bosnia and Herzegovina work for a better environment, a better world. Despite their efforts the work they do is not as efficient as it could be and the support available for them is meagre. For this reason a programme called Municipality Environmental Infrastructure is under development in cooperation between the University of Sarajevo (Bosnia and Herzegovina) and the Royal Institute of Technology (Sweden) with financing from the Swedish International Development Cooperation Agency (Sida). This thesis is a part of the work to make the programme successful.</p><p>Competence needs for municipality officers will be identified through a needs analysis based on interviews with different stakeholders. The answers provided will create a picture of the needs that is both univocal and diverse with competences in identifying and handling environmental threats as well as managing infrastructure projects.</p><p>The thesis will also look at what pedagogical methods the teachers at the programme plan to use and how this affects the programme. Since the programme is held in a formal setting but intends practical use of the knowledge this leads to high demands on the pedagogical methods. The programme syllabus will be fount to not entirely encompass all competence needs but suggestions will be made as to how to include the identified needs.</p>
----------------------------------------------------------------------
In diva2:492864 abstract is:
<p>This thesis project focuses on the verification method of a safety function called PICthat stands for Post-Impact Control which controls the vehicle motion of passengercars after being exposed to external disturbances produced by a 1st impact, aiming atavoiding or mitigating secondary events.The main objective was to select a promising method, among several candidates, todevelop further for testing the function and the interaction with the driver. To do thisis was first necessary to map the real destabilized states of motion that are targeted bythe function. These states are referred as Post-Impact problem space and are acombination of variables that describes the host vehicles motion at the instant thedestabilizing force has ceased. Knowing which states are requested by the solutioncandidates, it is possible to grade the rig candidates based on the capability ofcovering the problem space. Then, simulating the proposed rig solutions withMatlab/Simulink models to investigate which candidate fulfils best the problem space.The result of the simulations and other criteria is that a moving base simulator(Simulator SIM4) is most fitted to research verification. The second mostadvantageous solution is the rig alternative called Built-in Actuators.</p>

corrected abstract:
<p>This thesis project focuses on the verification method of a safety function called PIC that stands for Post-Impact Control which controls the vehicle motion of passenger cars after being exposed to external disturbances produced by a 1st impact, aiming at avoiding or mitigating secondary events.</p><p>The main objective was to select a promising method, among several candidates, to develop further for testing the function and the interaction with the driver. To do this is was first necessary to map the real destabilized states of motion that are targeted by the function. These states are referred as Post-Impact problem space and are a combination of variables that describes the host vehicles motion at the instant the destabilizing force has ceased. Knowing which states are requested by the solution candidates, it is possible to grade the rig candidates based on the capability of covering the problem space. Then, simulating the proposed rig solutions with Matlab/Simulink models to investigate which candidate fulfils best the problem space.</p><p>The result of the simulations and other criteria is that a moving base simulator (Simulator SIM4) is most fitted to research verification. The second most advantageous solution is the rig alternative called Built-in Actuators.</p>
----------------------------------------------------------------------
In diva2:435361 abstract is:
<p>In one of the largest renewable energy sectors, the Wind, research on maintenance on windturbines is surprisingly nearly nonexistent. No meaningful work has been made on optimizingthe scheduled maintenance process. Filling this gap this thesis stems.Unplanned maintenance is commonly synonym to large energy loss since the wind turbine mustbe stopped nearly throughout the whole duration of the maintenance procedure. All partiesevolved in the sector are hindered by this fact. It is therefore not only in all the sectors’ playersbut also the general publics’ interest to optimize this process for a more sustainable world.Responsible for all the calculations is a model, which was fully developed for this thesis and ispart of it. Having considered several programming languages the choice was Excel (VBA); beingthis software spread worldwide it encourages the models’ global implementation. Easy to use,versatility and accurate and prompt results were the guidelines for its developments. A weatherforecast is the fundamental input.Running the model on two different wind farms gave conclusive results. The energy loss wasreduced up to 71%. Also in some cases the time frame was cut up to 62%.Even with these promising figures energy loss must be significant in order to have a realeconomical impact. Nevertheless the model unveils the most convenient schedule formaintenance and its implementation is exclusively beneficial.</p>

corrected abstract:
<p>In one of the largest renewable energy sectors, the Wind, research on maintenance on wind turbines is surprisingly nearly nonexistent. No meaningful work has been made on optimizing the scheduled maintenance process. Filling this gap this thesis stems.</p><p>Unplanned maintenance is commonly synonym to large energy loss since the wind turbine must be stopped nearly throughout the whole duration of the maintenance procedure. All parties evolved in the sector are hindered by this fact. It is therefore not only in all the sectors’ players but also the general publics’ interest to optimize this process for a more sustainable world. Responsible for all the calculations is a model, which was fully developed for this thesis and is part of it. Having considered several programming languages the choice was Excel (VBA); being this software spread worldwide it encourages the models’ global implementation. Easy to use, versatility and accurate and prompt results were the guidelines for its developments. A weather forecast is the fundamental input.</p><p>Running the model on two different wind farms gave conclusive results. The energy loss was reduced up to 71%. Also in some cases the time frame was cut up to 62%. Even with these promising figures energy loss must be significant in order to have a real economical impact. Nevertheless the model unveils the most convenient schedule for maintenance and its implementation is exclusively beneficial.</p>
----------------------------------------------------------------------
In diva2:1827845 abstract is:
<p>Modern investors are considering investments in sustainable companies more than ever and oneof the main metrics that are of use is ESG performance. This criterion provides a comprehensiveoverview over an organization’s ability to generate value for all stakeholders, including employees,customers and shareholders. As the use of ESG criterion increases, so does the amount ofstudies regarding ESG performance and a company’s profitability. Most of these studies showa non negative correlation between the two. This study will however aim to examine how ESGperformance impact the efficiency and productivity level of a company. This will be done bystudying the sales generated by each employee. Moreover, the project will focus on two industries,tech and industrial and will analyse their differences. The results will be of help to business ownersand decision makers with questions regarding if investing in the company’s ESG performancecan be a way of increasing the company’s internal performance. The results of this projectsuggest that there exist a statistically significant correlation between them for both industries.The project also presented an insight to which categories of ESG are most significant in theregression model in the two different industries. The adjusted R2 for the regression model forindustrials was higher than the tech industry, which means that it is a more extensive correlationbetween the regressors and the response variable for the industrial industry.</p>

corrected abstract:
<p>Modern investors are considering investments in sustainable companies more than ever and one of the main metrics that are of use is ESG performance. This criterion provides a comprehensive overview over an organization’s ability to generate value for all stakeholders, including employees, customers and shareholders. As the use of ESG criterion increases, so does the amount of studies regarding ESG performance and a company’s profitability. Most of these studies show a non negative correlation between the two. This study will however aim to examine how ESG performance impact the efficiency and productivity level of a company. This will be done by studying the sales generated by each employee. Moreover, the project will focus on two industries, tech and industrial and will analyse their differences. The results will be of help to business owners and decision makers with questions regarding if investing in the company’s ESG performance can be a way of increasing the company’s internal performance. The results of this project suggest that there exist a statistically significant correlation between them for both industries. The project also presented an insight to which categories of ESG are most significant in the regression model in the two different industries. The adjusted R<sup>2</sup> for the regression model for industrials was higher than the tech industry, which means that it is a more extensive correlation between the regressors and the response variable for the industrial industry.</p>
----------------------------------------------------------------------
In diva2:1820859 abstract is:
<p>Anderson localization occurs when an otherwise conductive solid becomes insulatingdue to a sufficiently large degree of disorder in the medium. The electron band energy(as a function of disorder) at which this transition between extended and localizedelectron states occur is called the mobility edge (ME) and is energy-dependent only in3-dimensional systems. In lower dimensional systems, energy-independent ME (allstates localized or all extended) has been demonstrated by replacing disorder withquasi-periodic potential. However, recent theoretical findings indicate that neitherdisorder nor quasi-periodic potential is necessary for a material to exhibit electronlocalization and existence of energy-dependent pseudo ME at finite system size.In this thesis work, we use light in coupled silicon nitride waveguides to simulatesingle-particle transport of a solid-state medium and investigate the coexistence ofdelocalized and localized states in disorder-free photonic lattices of finite systemsize. This was achieved by implementing a simulated linearly increasing electricpotential on even-numbered sites by varying the refractive index of the wave guide(ch. 3). Through our experimental setup, we successfully achieved a coexistence oflocalized and delocalized states, where the degree of localization varies depending onthe strength of the applied electric field.The findings have implications for the field of quantum technology, whereunderstanding and controlling quantum states is crucial. The ability to achievelocalization in the absence of disorder opens new possibilities for designing andengineering photonic devices for quantum information processing tasks.</p>

corrected abstract:
<p>Anderson localization occurs when an otherwise conductive solid becomes insulating due to a sufficiently large degree of disorder in the medium. The electron band energy (as a function of disorder) at which this transition between extended and localized electron states occur is called the mobility edge (ME) and is energy-dependent only in 3-dimensional systems. In lower dimensional systems, energy-independent ME (all states localized or all extended) has been demonstrated by replacing disorder with quasi-periodic potential. However, recent theoretical findings indicate that neither disorder nor quasi-periodic potential is necessary for a material to exhibit electron localization and existence of energy-dependent pseudo ME at finite system size.</p><p>In this thesis work, we use light in coupled silicon nitride waveguides to simulate single-particle transport of a solid-state medium and investigate the coexistence of delocalized and localized states in disorder-free photonic lattices of finite system size. This was achieved by implementing a simulated linearly increasing electric potential on even-numbered sites by varying the refractive index of the wave guide (ch. 3). Through our experimental setup, we successfully achieved a coexistence of localized and delocalized states, where the degree of localization varies depending on the strength of the applied electric field.</p><p>The findings have implications for the field of quantum technology, where understanding and controlling quantum states is crucial. The ability to achieve localization in the absence of disorder opens new possibilities for designing and engineering photonic devices for quantum information processing tasks.</p>
----------------------------------------------------------------------
In diva2:1794337 abstract is:
<p>Patents are a fundamental part of scientific and engineering work, ensuringprotection of inventions owned by individuals or organizations. Patents areusually made public 18 months after being filed to a patent office, whichmeans that current publicly available patent data only provides informationabout the past. Regression models applied on discrete time series can be usedas a prediction tool to counteract this, building a 18 month long bridge intothe future and beyond. While linear models are popular for their simplicity,Bayesian networks have statistical properties that can produce high forecastingquality. Improvements is also made by using signal processing as patentdata is naturally stochastic. This thesis implements wavelet-based signalprocessing and P CA to increase stability and reduce overfitting. A multiplelinear regression model and a Bayesian network model is then designed andapplied to the transformed data. When evaluated on each data set, the Bayesianmodel both performs better and exhibits greater stability and consistency inits predictions. As expected, the linear model is both smaller and faster toevaluate and train. Despite an increase in complexity and slower evaluationtimes, the Bayesian model is conclusively superior to the linear model. Futurework should focus on the signal processing method and additional layers inthe Bayesian network.</p>

corrected abstract:
<p>Patents are a fundamental part of scientific and engineering work, ensuring protection of inventions owned by individuals or organizations. Patent families are usually made public 18 months after being filed to a patent office, which means that current publicly available patent data only provides information about the past. Regression models applied on discrete time series can be used as a prediction tool to counteract this, building a 18 month long bridge into the future and beyond. While linear models are popular for their simplicity, Bayesian networks have statistical properties that can produce high forecasting quality. Improvements is also made by using signal processing as patent data is naturally stochastic. This thesis implements wavelet-based signal processing and <em>PCA</em> to increase stability and reduce overfitting. A multiple linear regression model and a Bayesian network model is then designed and applied to the transformed data. When evaluated on each data set, the Bayesian model both performs better and exhibits greater stability and consistency in its predictions. As expected, the linear model is both smaller and faster to evaluate and train. Despite an increase in complexity and slower evaluation times, the Bayesian model is conclusively superior to the linear model. Future work should focus on the signal processing method and additional layers in the Bayesian network.</p>
----------------------------------------------------------------------
In diva2:1768905 abstract is:
<p>In this thesis the effect of an active suspension system versus a passive suspensionsystem utilized on a main battle tank/combat vehicle is studied. The effect oncomfort, mobility, accuracy and power draw is discussed. This is done with thegoal to study and understand if an improvement can be made using active sys-tems. This is done mainly by simulation of a main battle tank/combat vehicle inSimulink/Matlab.</p><p>The active suspension system chosen was the skyhook suspension system whichwas compared to a regular passive suspension. The results show that the activesuspension system has positive effects on comfort, mobility and accuracy. Thisthesis discusses the viability of such a system. This is done through analysing thepower draw of the active system and through reasoning about use in field situations.Besides comfort, mobility and accuracy, the results also show that in some cases theactive suspension would need more power than the engine can generate. In additionan active suspension system would be very sensitive due to the complexity of thecomponents needed. If it is possible to control the power draw and the sensitivityof an active suspension system it would vastly improve the comfort, mobility andaccuracy of main battle tanks/combat vehicles.</p>

corrected abstract:
<p>In this thesis the effect of an active suspension system versus a passive suspension system utilized on a main battle tank/combat vehicle is studied. The effect on comfort, mobility, accuracy and power draw is discussed. This is done with the goal to study and understand if an improvement can be made using active systems. This is done mainly by simulation of a main battle tank/combat vehicle in Simulink/Matlab.</p><p>The active suspension system chosen was the skyhook suspension system which was compared to a regular passive suspension. The results show that the active suspension system has positive effects on comfort, mobility and accuracy. This thesis discusses the viability of such a system. This is done through analysing the power draw of the active system and through reasoning about use in field situations. Besides comfort, mobility and accuracy, the results also show that in some cases the active suspension would need more power than the engine can generate. In addition an active suspension system would be very sensitive due to the complexity of the components needed. If it is possible to control the power draw and the sensitivity of an active suspension system it would vastly improve the comfort, mobility and accuracy of main battle tanks/combat vehicles.</p>
----------------------------------------------------------------------
In diva2:1740119 abstract is:
<p>As emissions of polluting substances due to shipping are growing, newsolutions for ship propulsion are emerging. One of them is the suctionwing, a vertical wing designed to generate lift forward, and made even moreefficient than classical wings by the suction of its boundary layer. Thisproject allowed for the participation, within a team of mechanical engineers,in the early design of a pilot model of such a system, ultimately intendedto be industrialised. The design process was followed in order to define thesolutions to match the performances of each function of the wing, basedon a set of specifications, and to verify the sustainability of those solutions.Also, a particular interest in the furniture and/or manufacturing of thosesolutions was given. For those instances, the suction system was defined withthe collaboration of manufacturers. Also, the wing orientation system wasdefined and its integration was verified with respect to the deformation ofthe components at its interfaces. Then, The flap was designed in order todefine solutions to allow mobility transmission, taking into account its stressconditions. The solution for the sealing of the system was also worked upon.Finally, the main mast of the wing was designed so to maintain its integrity,with respect to its stress charges and its strong stress concentrations.</p>

corrected abstract:
<p>As emissions of polluting substances due to shipping are growing, new solutions for ship propulsion are emerging. One of them is the suction wing, a vertical wing designed to generate lift forward, and made even more efficient than classical wings by the suction of its boundary layer. This project allowed for the participation, within a team of mechanical engineers, in the early design of a pilot model of such a system, ultimately intended to be industrialised. The design process was followed in order to define the solutions to match the performances of each function of the wing, based on a set of specifications, and to verify the sustainability of those solutions. Also, a particular interest in the furniture and/or manufacturing of those solutions was given. For those instances, the suction system was defined with the collaboration of manufacturers. Also, the wing orientation system was defined and its integration was verified with respect to the deformation of the components at its interfaces. Then, The flap was designed in order to define solutions to allow mobility transmission, taking into account its stress conditions. The solution for the sealing of the system was also worked upon. Finally, the main mast of the wing was designed so to maintain its integrity, with respect to its stress charges and its strong stress concentrations.</p>
----------------------------------------------------------------------
In diva2:1737092 abstract is:
<p>Nanoparticles (NPs) have been part of our daily lives whether we are aware of it ornot and are continuously shaping our world of tomorrow. They are employed in awide range of applications such as in sunscreens, breaking down microplastics inthe ocean and scaling down beyond Moore’s law. Superparamagnetic iron oxidenanoparticles (SPIONs) are one such NPs which have shown promising resultsin diverse fields, from magnetic resonance imagining (MRI) to drug delivery andmagnetic separation. In this work, SPIONs were synthesised following the conceptof green chemistry and self-assembled into nanoclusters (NCs) with a one-potsynthesis via microwave-assisted (MW) or conventional solvothermal (ST) route.The NC surface was engineered with several capping agents granting monodispersityand a flexible platform for bioconjugation. Furthermore, as-synthesised NCswere subsequently coated with a 55 nm thin silica shell leading to a core-shellarchitecture, which would prevent the probable iron dissolution in the biologicalenvironment as well as enhance their biocompatibility. After optimizing the NCsize and shape, a detailed characterization of their properties, behaviour, and potentialapplications was performed.</p>

corrected abstract:
<p>Nanoparticles (NPs) have been part of our daily lives whether we are aware of it or not and are continuously shaping our world of tomorrow. They are employed in a wide range of applications such as in sunscreens, breaking down microplastics in the ocean and scaling down beyond Moore’s law. Superparamagnetic iron oxide nanoparticles (SPIONs) are one such NPs which have shown promising results in diverse fields, from magnetic resonance imagining (MRI) to drug delivery and magnetic separation. In this work, SPIONs were synthesised following the concept of green chemistry and self-assembled into nanoclusters (NCs) with a one-pot synthesis <em>via</em> microwave-assisted (MW) or conventional solvothermal (ST) route. The NC surface was engineered with several capping agents granting monodispersity and a flexible platform for bioconjugation. Furthermore, as-synthesised NCs were subsequently coated with a 55 nm thin silica shell leading to a core-shell architecture, which would prevent the probable iron dissolution in the biological environment as well as enhance their biocompatibility. After optimizing the NC size and shape, a detailed characterization of their properties, behaviour, and potential applications was performed.</p>
----------------------------------------------------------------------
In diva2:1698025 abstract is:
<p>In a nuclear reactor, in case of a severe accident occurs, backup circuits are usedlike SIS and CSS, flooding the reactor with water. This might lead to many liquidleakages outside the reactor containment building. These leakages contain a lot offission products and especially iodine that is dangerous in terms of radiotoxicity oncereleased into the environment. Therefore, it is necessary to model correctly the liquidleakage in a reactor to size their impact properly. Their re-injection with a newpump implemented as part of the safety review for the extension lifetime of reactorsbeyond 40 years constitutes also a safety improvement to reduce the source term inthe environment.Modelling the transfer of fission products through liquid leakage and the re-injectionof said leakages is performed at IRSN with the ASTEC code. This thesis focuses onthe optimization of this model, and the consequences of modification in the code forfission products. Indeed, the model of pump used for leakages and re-injection hadsome issues that had to be circumvented.Also, the efficiency of re-injection will be tested with the new model, knowing thatthis implementation has already been proven to be effective in reducing releases tothe environment. Besides, this thesis studies the failure of the reinjection of liquidleaks that can lead to an accumulation of fission products in the auxiliary safeguardingbuilding. Other pH sensitivities in different areas are also studied.</p>

corrected abstract:
<p>In a nuclear reactor, in case of a severe accident occurs, backup circuits are used like SIS and CSS, flooding the reactor with water. This might lead to many liquid leakages outside the reactor containment building. These leakages contain a lot of fission products and especially iodine that is dangerous in terms of radiotoxicity once released into the environment. Therefore, it is necessary to model correctly the liquid leakage in a reactor to size their impact properly. Their re-injection with a new pump implemented as part of the safety review for the extension lifetime of reactors beyond 40 years constitutes also a safety improvement to reduce the source term in the environment.</p><p>Modelling the transfer of fission products through liquid leakage and the re-injection of said leakages is performed at IRSN with the ASTEC code. This thesis focuses on the optimization of this model, and the consequences of modification in the code for fission products. Indeed, the model of pump used for leakages and re-injection had some issues that had to be circumvented.</p><p>Also, the efficiency of re-injection will be tested with the new model, knowing that this implementation has already been proven to be effective in reducing releases to the environment. Besides, this thesis studies the failure of the reinjection of liquid leaks that can lead to an accumulation of fission products in the auxiliary safeguarding building. Other pH sensitivities in different areas are also studied.</p>
----------------------------------------------------------------------
In diva2:1640018 abstract is:
<p>The lattice Boltzmann method (LBM) is widely studied andused in the last years to replace the conventional numerical solvers forthe Navier-Stokes equations. In this work, a general introduction tofluid dynamics equations and the changes when the flow is a reactivemulti-species one will be given first. The lattice Boltzmann method andalgorithm will then be explained in details with the kinetic theorybehind, tested and validated for canonical test cases of doubly shearlayer flow in reactive and non-reactive flows. Finally the method willbe applied to simulate a non-reacting propane jet and the results willbe validated using experimental data.</p><p>The objectives of this thesis are mainly: first a better understandingof the LBM, the combustion reactions, the jets and how they work, secondthe use of this method to produce a simple code that works for a basictest case, third validate this code with more developed methods, andfinally apply this method to simulate a more complex configuration whichis the non-reacting propane jet flame into co-flowing air</p>

corrected abstract:
<p>The lattice Boltzmann method (LBM) is widely studied and used in the last years to replace the conventional numerical solvers for the Navier-Stokes equations. In this work, a general introduction to fluid dynamics equations and the changes when the flow is a reactive multi-species one will be given first. The lattice Boltzmann method and algorithm will then be explained in details with the kinetic theory behind, tested and validated for canonical test cases of doubly shear layer flow in reactive and non-reactive flows. Finally the method will be applied to simulate a non-reacting propane jet and the results will be validated using experimental data.</p><p>The objectives of this thesis are mainly: first a better understanding of the LBM, the combustion reactions, the jets and how they work, second the use of this method to produce a simple code that works for a basic test case, third validate this code with more developed methods, and finally apply this method to simulate a more complex configuration which is the non-reacting propane jet flame into co-flowing air.</p>
----------------------------------------------------------------------
In diva2:1528107 abstract is:
<p>Carbon Fibre Reinforced Plastic (CFRP) is a material with high specific propertiesand good fatigue and vibration dampening characteristics, and can potentiallybe used instead of steel and aluminium in heavy duty vehicles. This work focuseson testing methodology and the fatigue properties of a unidirectional (UD)material in the 0° and 90°orientation, reproducing and validating the method developedby Wanner[1]. While conducting a fatigue test of a CFRP composite intension-tension fatigue, in-situ strain measurements were performed to examinethe gradual elongation of the specimen (as this relates to stiffness loss, i.e. fatiguedamage). An imaging methodology capturing the specimen at peak loading hasbeen developed, including a trigger mechanism that activates the camera at the desiredtime and cycle count, as well as a method of extracting the photograph ofthe specimen at maximum displacement, allowing for peak-to-peak comparison.A method improving specimen production output and consistency has been developed.SN-curves have been produced for both 0° and 90° fibre orientations.Micrography of sectioned specimen has been conducted. The study finds the fatiguelimit of the 0° specimen to be as high as 80% of the material tensile failurestrength, while results from the 90° study indicate a lower but inconclusivevalue. An attempt at qualitatively determining the factors causing the material behaviourhas been made and is deliberated upon.</p>

corrected abstract:
<p>Carbon Fibre Reinforced Plastic (CFRP) is a material with high specific properties and good fatigue and vibration dampening characteristics, and can potentially be used instead of steel and aluminium in heavy duty vehicles. This work focuses on testing methodology and the fatigue properties of a unidirectional (UD) material in the 0° and 90° orientation, reproducing and validating the method developed by Wanner[1]. While conducting a fatigue test of a CFRP composite in tension-tension fatigue, in-situ strain measurements were performed to examine the gradual elongation of the specimen (as this relates to stiffness loss, i.e. fatigue damage). An imaging methodology capturing the specimen at peak loading has been developed, including a trigger mechanism that activates the camera at the desired time and cycle count, as well as a method of extracting the photograph of the specimen at maximum displacement, allowing for peak-to-peak comparison. A method improving specimen production output and consistency has been developed. SN-curves have been produced for both 0° and 90° fibre orientations. Micrography of sectioned specimen has been conducted. The study finds the fatigue limit of the 0° specimen to be as high as 80% of the material tensile failure strength, while results from the 90° study indicate a lower but inconclusive value. An attempt at qualitatively determining the factors causing the material behaviour has been made and is deliberated upon.</p>
----------------------------------------------------------------------
In diva2:1357359 abstract is:
<p>During the last decade, a novel methodology for wheel wear simulation has been developed in Sweden. The practical objective of this simulation procedure is to provide an integratedengineering tool to support rail vehicle design with respect to wheel wear performance and detailed understanding of wheel-rail interaction. The tool is integrated in a vehicle dynamicssimulation environment.The wear calculation is based on a set of dynamic simulations, representing the vehicle, the network, and the operating conditions. The wheel profile evolution is simulated in an iterativeprocess by adding the contribution from each simulation case and updating the profile geometry.The method is being validated against measurements by selected pilot applications. To strengthen the confidence in simulation results the scope of application should be as wide aspossible in terms of vehicle classes. The purpose of this thesis work has been to try to extend the scope of validation of this method into the light rail area, simulating the light rail vehicleA32 operating in Stockholm commuter service on the line Tvärbanan.An exhaustive study of the wear theory and previous work on wear prediction has been necessary to understand the wear prediction method proposed by KTH. The dynamicbehaviour of rail vehicles has also been deeply studied in order to understand the factors affecting wear in the wheel-rail contact.The vehicle model has been validated against previous studies of this vehicle. Furthermore new elements have been included in the model in order to better simulate the real conditionsof the vehicle.Numerous tests have been carried out in order to calibrate the wear tool and find the settings which better match the real conditions of the vehicle.Wheel and rail wear as well as profile evolution measurements were available before this work and they are compared with those results obtained from the simulations carried out.The simulated wear at the tread and flange parts of the wheel match quite well the measurements. However, the results are not so good for the middle part, since themeasurements show quite evenly distributed wear along the profile while the results from simulations show higher difference between extremes and middle part. More tests would benecessary to obtain an optimal solution.</p>

corrected abstract:
<p>During the last decade, a novel methodology for wheel wear simulation has been developed in Sweden. The practical objective of this simulation procedure is to provide an integrated engineering tool to support rail vehicle design with respect to wheel wear performance and detailed understanding of wheel-rail interaction. The tool is integrated in a vehicle dynamics simulation environment.</p><p>The wear calculation is based on a set of dynamic simulations, representing the vehicle, the network, and the operating conditions. The wheel profile evolution is simulated in an iterative process by adding the contribution from each simulation case and updating the profile geometry.</p><p>The method is being validated against measurements by selected pilot applications. To strengthen the confidence in simulation results the scope of application should be as wide as possible in terms of vehicle classes. The purpose of this thesis work has been to try to extend the scope of validation of this method into the light rail area, simulating the light rail vehicle A32 operating in Stockholm commuter service on the line Tvärbanan. An exhaustive study of the wear theory and previous work on wear prediction has been necessary to understand the wear prediction method proposed by KTH. The dynamic behaviour of rail vehicles has also been deeply studied in order to understand the factors affecting wear in the wheel-rail contact.</p><p>The vehicle model has been validated against previous studies of this vehicle. Furthermore new elements have been included in the model in order to better simulate the real conditions of the vehicle.</p><p>Numerous tests have been carried out in order to calibrate the wear tool and find the settings which better match the real conditions of the vehicle.</p><p>Wheel and rail wear as well as profile evolution measurements were available before this work and they are compared with those results obtained from the simulations carried out.</p><p>The simulated wear at the tread and flange parts of the wheel match quite well the measurements. However, the results are not so good for the middle part, since the measurements show quite evenly distributed wear along the profile while the results from simulations show higher difference between extremes and middle part. More tests would be necessary to obtain an optimal solution.</p>
----------------------------------------------------------------------
In diva2:1285502 abstract is:
<p>Vehicle pull is an issue that occurs when the driver has to exert a discerniblesteering torque (pull) for the vehicle to run straight, otherwisea lateral drift takes place. This thesis deals with the straight motion ofroad vehicles, with particular focus on the role played by tire characteristics,road cross slope and interactions between tires and vehicle.A thorough theoretical approach has been adopted, adjusting thePacejka’s formulation for effective axle characteristics and extendingthe linear handling diagram theory. This has allowed to obtain innovativeanalytical expressions, describing the straight-driving slip anglesand steering torque offsets.The analytical expressions have been validated, together with asingle-track model, by means of quasi-static and dynamic simulationsof a full-vehicle model. Moreover, a relationship between tire characteristicsand on-center handling has been described, that relates objectivemetrics with subjective feedback.The obtained analytical expressions can be used by vehicle OriginalEquipment Manufacturers (OEMs) or Tire Suppliers for productdevelopment.</p>

corrected abstract:
<p>Vehicle pull is an issue that occurs when the driver has to exert a discernible steering torque (pull) for the vehicle to run straight, otherwise a lateral drift takes place. This thesis deals with the straight motion of road vehicles, with particular focus on the role played by tire characteristics, road cross slope and interactions between tires and vehicle.</p><p>A thorough theoretical approach has been adopted, adjusting the Pacejka’s formulation for effective axle characteristics and extending the linear handling diagram theory. This has allowed to obtain innovative analytical expressions, describing the straight-driving slip angles and steering torque offsets.</p><p>The analytical expressions have been validated, together with a single-track model, by means of quasi-static and dynamic simulations of a full-vehicle model. Moreover, a relationship between tire characteristics and on-center handling has been described, that relates objective metrics with subjective feedback.</p><p>The obtained analytical expressions can be used by vehicle Original Equipment Manufacturers (OEMs) or Tire Suppliers for product development.</p>
----------------------------------------------------------------------
In diva2:1188292 - Note: no full text in DiVA
abstract is:
<p>Flexible hoses are used in the exhaust systems of trucks to decouple the vibrations betweenthe engine and chassis mounted components. These stainless steel parts have tosustain high thermal loads from the exhaust gas and vibrations from the engine and road.The combination of complex geometry and thermo-mechanical loading makes the fatiguelife prediction difficult in flexible hoses. This thesis focuses on developing a method for assessing the durability in these components. The method includes how to measure theload history in six degrees of freedom using the hexapod method, how to model the heattransfer and assess the thermal effects on the fatigue life using a parametric study, andhow to calculate the fatigue life from the measured load history by using the strain-basedlife approach. In addition, equivalent loads were defined for all degrees of freedom fromwhich the fatigue life was calculated and compared to the previous results. This approach saves the computation time considerably, while its accuracy is under questions.The exhaust gas temperature was observed to have the largest effect on the fatiguelife of the flexible hose. In complex stripwound hoses, plastic deformation occurs dueto a high thermal gradient through the wall thickness. This plastic deformation can contributeto stiffening of stripwound hoses. It was not possible to include the thermal stress in the calculations of the fatigue life from the load histories due to the linearity assumption,while they could be included when the equivalent loads were used. The calculated damage from the load histories was not comparable with the damage calculated fromthe equivalent loads. This is caused mainly by neglecting the effects of multiaxial loading in the equivalent loads. Further physical tests are required to validate and improvethe presented method herein.</p>

corrected abstract:
<p>Flexible hoses are used in the exhaust systems of trucks to decouple the vibrations between the engine and chassis mounted components. These stainless steel parts have to sustain high thermal loads from the exhaust gas and vibrations from the engine and road. The combination of complex geometry and thermo-mechanical loading makes the fatigue life prediction difficult in flexible hoses. This thesis focuses on developing a method for assessing the durability in these components. The method includes how to measure the load history in six degrees of freedom using the hexapod method, how to model the heattransfer and assess the thermal effects on the fatigue life using a parametric study, and how to calculate the fatigue life from the measured load history by using the strain-based life approach. In addition, equivalent loads were defined for all degrees of freedom from which the fatigue life was calculated and compared to the previous results. This approach saves the computation time considerably, while its accuracy is under questions. The exhaust gas temperature was observed to have the largest effect on the fatigue life of the flexible hose. In complex strip wound hoses, plastic deformation occurs due to a high thermal gradient through the wall thickness. This plastic deformation can contribute to stiffening of strip wound hoses. It was not possible to include the thermal stress in the calculations of the fatigue life from the load histories due to the linearity assumption, while they could be included when the equivalent loads were used. The calculated damage from the load histories was not comparable with the damage calculated from the equivalent loads. This is caused mainly by neglecting the effects of multiaxial loading in the equivalent loads. Further physical tests are required to validate and improve the presented method herein.</p>
----------------------------------------------------------------------
In diva2:1142942 - is 'diva2:1120520' a duplicate?

abstract is:
<p>SDR (Software Defined Radio) is a radio communicationsystem that has been of great interest and developmentover the last 20 years. It decreases communication costs significantlyas it replaces expensive analogue system components withcheap and flexible digital ones. In this article we describe anSDR implementation for communication with the SEAM (SmallExplorer for Advances Missions) satellite, a CubeSat satellitethat will perform high quality magnetic measurements in theEarth orbit. The project result consists of carefully chosen SDRtransceiver and software tools, integrated with parts of thecurrent satellite communication system. The implementation hasrequired studies within the field of digital processing, SDR andspecifications of the SEAM satellite communication system, andhas been developed and tested using a radio identical to an onboard satellite radio and coaxial cables as transmission media.The system is able to interpret incoming messages from theradio as hexadecimal data and will serve as a prototype whenimplementing SDR for another, more complex and expensivesystem that is used for communication with the SEAM satellite.</p>

corrected abstract:
<p>SDR (Software Defined Radio) is a radio communication system that has been of great interest and development over the last 20 years. It decreases communication costs significantly as it replaces expensive analogue system components with cheap and flexible digital ones. In this article we describe an SDR implementation for communication with the SEAM (Small Explorer for Advances Missions) satellite, a CubeSat satellite that will perform high quality magnetic measurements in the Earth orbit. The project result consists of carefully chosen SDR transceiver and software tools, integrated with parts of the current satellite communication system. The implementation has required studies within the field of digital processing, SDR and specifications of the SEAM satellite communication system, and has been developed and tested using a radio identical to an on board satellite radio and coaxial cables as transmission media. The system is able to interpret incoming messages from the radio as hexadecimal data and will serve as a prototype when implementing SDR for another, more complex and expensive system that is used for communication with the SEAM satellite.</p>
----------------------------------------------------------------------
In diva2:1142795 - is 'diva2:1120491' (with only one author) a duplicate?
abstract is:
<p>Electrohydrodynamic (EHD) thrusters hold promiseto provide more efficient thrust than propeller driven systems forsmall drones. However, the fact is that there is incomplete analysisin the area and that no work has yet studied the capacities ofbattery driven EHD thrusters. This study shows how a batterysystem from a commercial Arc-Lighter can produce a suitablevoltage for an EHD thruster, with 7.1 kV. Using the batterysystem to drive a single EHD thruster, the EHD thruster couldonly lift 13.3 ppm of the total weight of a battery system andEHD thruster. However, the low value is due to a low-qualitythruster. If the battery system would be used with suitable highqualityEHD thrusters from literature, approximately 5% of thetotal weight could be reached when at least 20% is needed forairplane drones to lift. In conclusion, the battery system is notsufficient for lifting the total weight. Still, in future, with morework on both EHD thruster and a battery system providing ahigher voltage, there might be possible to have small airplanedrones with battery driven EHD thrusters.</p>

corrected abstract:
<p>Electrohydrodynamic (EHD) thrusters hold promise to provide more efficient thrust than propeller driven systems for small drones. However, the fact is that there is incomplete analysis in the area and that no work has yet studied the capacities of battery driven EHD thrusters. This study shows how a battery system from a commercial Arc-Lighter can produce a suitable voltage for an EHD thruster, with 7.1 kV. Using the battery system to drive a single EHD thruster, the EHD thruster could only lift 13.3 ppm of the total weight of a battery system and EHD thruster. However, the low value is due to a low-quality thruster. If the battery system would be used with suitable high-quality EHD thrusters from literature, approximately 5% of the total weight could be reached when at least 20% is needed for airplane drones to lift. In conclusion, the battery system is not sufficient for lifting the total weight. Still, in future, with more work on both EHD thruster and a battery system providing a higher voltage, there might be possible to have small airplane drones with battery driven EHD thrusters.</p>
----------------------------------------------------------------------
In diva2:1127576 abstract is:
<p>This degree project is focused on the study of the tumble and the swirlmotions, which develop during the intake stroke, inside a cylinder of a Dieselengine.Nowadays, the reduction of fuel consumption and emissions is a primaryaspect for automotive companies, including Scania. Then, an efficient combustionprocess is required, and a fundamental role is played by tumble andswirl motion.These motions have been studied by means of the Particle Image Velocimetry(PIV) technique. In particular, two different cylinder head designshave been investigated, focusing on the structures present in the flow andtheir evolutions inside the cylinder. Finally, these results have been comparedwith LES results, in order to validate the latest. Analysing the swirl motion, it has been possible to identify three mainregions, along the cylinder, characterized by different vortex structures. Inaddition, the velocity field into the entire cylinder volume has been extractedby means of a three-dimensional three-component reconstruction.</p>

corrected abstract:
<p>This degree project is focused on the study of the tumble and the swirl motions, which develop during the intake stroke, inside a cylinder of a Diesel engine.</p><p>Nowadays, the reduction of fuel consumption and emissions is a primary aspect for automotive companies, including Scania. Then, an efficient combustion process is required, and a fundamental role is played by tumble and swirl motion.</p><p>These motions have been studied by means of the Particle Image Velocimetry (PIV) technique. In particular, two different cylinder head designs have been investigated, focusing on the structures present in the flow and their evolutions inside the cylinder. Finally, these results have been compared with LES results, in order to validate the latest.</p><p>Analysing the swirl motion, it has been possible to identify three main regions, along the cylinder, characterized by different vortex structures. In addition, the velocity field into the entire cylinder volume has been extracted by means of a three-dimensional three-component reconstruction.</p>
----------------------------------------------------------------------
In diva2:1078061 - missing space in title:
"PIV measurments and analysis of steady in-cylinderflow in tumble plane"
==>
"PIV measurments and analysis of steady in-cylinder flow in tumble plane"

abstract is:
<p>In this thesis the flow that is generated after passage through channels and inlet valveswithin a diesel engine cylinder is examined with the means of Particle Image Velocimetry(PIV). Experiments are carried out by le􀄴ing a thin laser sheet enter into the cylinder frombelow, and with short time intervals take pictures of smoke particles introduced into theair flowing through a given geometric channel shape, via inlet valves and down throughthe cylinder. Three valve lifts were tested and for all valve lifts six sample planes wereused, three parallel to the inlet valves and three orthogonal. Each case were measuredby taking 1500 PIV image pairs of the flow in the cylinder.After pre processing the PIV images they are evaluated by a cross-correlation algorithmwhich evaluates the displacements of the particles within a predetermined interrogationarea in order to find the velocity vector field. For each image pair an instantaneousvelocity field is obtained, and these are then used to calculate the mean velocity fieldof the stationary flow. The measured results are analyzed to find flow structures withinthe flow as well as turbulence properties. An identification scheme to detect tumble ismade in order to find large scale tumble motions as well as a Proper Orthogonal Decomposition(POD) analysis of the center plane for the three valve lifts. The velocity fieldsare found to be sensitive to the valve lift, which is most clearly seen in the difference ofswirl whereas no significant difference can be seen for the tumble motion.</p>

corrected abstract:
<p>In this thesis the flow that is generated after passage through channels and inlet valves within a diesel engine cylinder is examined with the means of Particle Image Velocimetry (PIV). Experiments are carried out by letting a thin laser sheet enter into the cylinder from below, and with short time intervals take pictures of smoke particles introduced into the air flowing through a given geometric channel shape, via inlet valves and down through the cylinder. Three valve lifts were tested and for all valve lifts six sample planes were used, three parallel to the inlet valves and three orthogonal. Each case were measured by taking 1500 PIV image pairs of the flow in the cylinder.</p><p>After pre processing the PIV images they are evaluated by a cross-correlation algorithm which evaluates the displacements of the particles within a predetermined interrogation area in order to find the velocity vector field. For each image pair an instantaneous velocity field is obtained, and these are then used to calculate the mean velocity field of the stationary flow. The measured results are analyzed to find flow structures within the flow as well as turbulence properties. An identification scheme to detect tumble is made in order to find large scale tumble motions as well as a Proper Orthogonal Decomposition (POD) analysis of the center plane for the three valve lifts. The velocity fields are found to be sensitive to the valve lift, which is most clearly seen in the difference of swirl whereas no significant difference can be seen for the tumble motion.</p>
----------------------------------------------------------------------
In diva2:726182 abstract is:
<p>Thispaper presents a mathematical optimization model concerning Prefab supplier selectionfor Spinactor AB with respect to the purchase price, volume discounts and transportationcost. Furthermore, it includes a qualitative study regarding supplier selectioncriteria. We identified the most important supplier criteria for Spinactor and developeda supplier selection model, based on the framework “total cost of ownership”(TCO), where those criteria were quantified. It was concluded that the threemost important supplier criteria for Spinactor AB, in the current situation, isdelivery performance, customer service and communication, and economicstability. The solution of the optimization problem was implemented using twodifferent methods, linear programming and integer programming, where linearprogramming was dominant in computational time. The optimization results showedthat the transportation cost is an important element in the total cost since anoptimal allocation suggests that five out of six potential suppliers should becontracted. Thus, our recommendation to Spinactor is to contract several localsuppliers for the initial purchasing of Prefab.</p>

corrected abstract:
<p>This paper presents a mathematical optimization model concerning Prefab supplier selection for Spinactor AB with respect to the purchase price, volume discounts and transportation cost. Furthermore, it includes a qualitative study regarding supplier selection criteria. We identified the most important supplier criteria for Spinactor and developed a supplier selection model, based on the framework “total cost of ownership” (TCO), where those criteria were quantified. It was concluded that the three most important supplier criteria for Spinactor AB, in the current situation, is delivery performance, customer service and communication, and economic stability.</p><p>The solution of the optimization problem was implemented using two different methods, linear programming and integer programming, where linear programming was dominant in computational time. The optimization results showed that the transportation cost is an important element in the total cost since an optimal allocation suggests that five out of six potential suppliers should be contracted. Thus, our recommendation to Spinactor is to contract several local suppliers for the initial purchasing of Prefab.</p>
----------------------------------------------------------------------
In diva2:618223 - Note: no full text in DiVA
abstract is:
<p>The aerospace industry becomes more and more accustomed with automated processing.Current manufacturing techniques are under development for higher automation. Within thiswork the behavior of prepreg material during forming is investigated, conducting simulationson previously performed experiments at SAAB AB. The aim is to implement materialbehavior in the AniForm Virtual Forming Tool simulation. The results of various experimentshave been combined to generate the basic material properties to assemble the materialbehavior and the experiment. With the use of investigated simplifications, the possibilities ofthe AniForm Virtual Forming Tool are used to compare two experiments conducted. Byrelating the simulations of wrinkled and non-wrinkled trials, the analysis of stresses andstrains within the simulation output is conducted. With the outcomes of these simulations,previously taken assumptions on fiber direction behavior are investigated. With the resultsshowing no geometrical wrinkling of the prepreg material itself, indicators for wrinkling areexamined on the output of the results. It can be seen that looking at various parametersexclusively does not necessarily explain the wrinkling of the prepreg material. Multiplefactors need to be taken into account at the same time. The results indicate that alignment ofstress- and strain behavior next to the fiber angle deviations play important parts in futureinvestigations regarding wrinkle development.</p>

corrected abstract:
<p>The aerospace industry becomes more and more accustomed with automated processing. Current manufacturing techniques are under development for higher automation. Within this work the behavior of prepreg material during forming is investigated, conducting simulations on previously performed experiments at SAAB AB. The aim is to implement material behavior in the AniForm Virtual Forming Tool simulation. The results of various experiments have been combined to generate the basic material properties to assemble the material behavior and the experiment. With the use of investigated simplifications, the possibilities of the AniForm Virtual Forming Tool are used to compare two experiments conducted. By relating the simulations of wrinkled and non-wrinkled trials, the analysis of stresses and strains within the simulation output is conducted. With the outcomes of these simulations, previously taken assumptions on fiber direction behavior are investigated. With the results showing no geometrical wrinkling of the prepreg material itself, indicators for wrinkling are examined on the output of the results. It can be seen that looking at various parameters exclusively does not necessarily explain the wrinkling of the prepreg material. Multiplefactors need to be taken into account at the same time. The results indicate that alignment of stress- and strain behavior next to the fiber angle deviations play important parts in future investigations regarding wrinkle development.</p>
----------------------------------------------------------------------
In diva2:558921 abstract is:
<p>During an emergency in a building complex, an effective evacuation is essentialto avoid crowd disasters. The evacuation efficiency could be enhanced both bychanging the layout of the building, and by changing the route guiding given tothe evacuating pedestrians. This thesis considers how to guide the evacuatingpedestrians so that the evacuation time is minimised.In this thesis, a dynamic network model, namely the point queue model, isused to form a linear programming problem whose solution is used to create anevacuation plan. By continuously updating the initial data in this model andsolving the problem with this new data, a feedback based control law is derivedbased on Model Predictive Control.The control law is tested on a simulation of the social force model for abuilding with five rooms and one respectively two exits. The result shows thatthe control law manages to efficiently guide the pedestrians out of the building,taking the varying distribution of pedestrians into account. The control lawfurther manages to handle minor errors in the layout information.                       Keywords. Evacuation modelling, pedestrian dynamics, optimal control</p>

corrected abstract:
<p>During an emergency in a building complex, an effective evacuation is essential to avoid crowd disasters. The evacuation efficiency could be enhanced both by changing the layout of the building, and by changing the route guiding given to the evacuating pedestrians. This thesis considers how to guide the evacuating pedestrians so that the evacuation time is minimised.</p><p>In this thesis, a dynamic network model, namely the point queue model, is used to form a linear programming problem whose solution is used to create an evacuation plan. By continuously updating the initial data in this model and solving the problem with this new data, a feedback based control law is derived based on Model Predictive Control.</p><p>The control law is tested on a simulation of the social force model for a building with five rooms and one respectively two exits. The result shows that the control law manages to efficiently guide the pedestrians out of the building, taking the varying distribution of pedestrians into account. The control law further manages to handle minor errors in the layout information.</p>
----------------------------------------------------------------------
In diva2:515496 abstract is:
<p>Lean Product Development is a knowledge-based business concept in order to maintain high quality, meetcustomer requirements and to make product development more efficient. An important part of the processis to add a lot of resources at an early stage and execute the development as an iterative process betweendepartments exploring many alternatives thoroughly. The work focuses on how Lean ProductDevelopment is carried out and explores how to manage interaction between different departments andexpertise with regard to cross-functional collaboration and knowledge sharing i.e. learning.The information for the studies was gathered at a major Swedish company from two projects. The projectswere carried out as cross-functional and possible key factors for cross-functional collaboration wasidentified.The results have been correlated with theories of Lean, Product development, Lean product developmentand Learning. The analysis shows that to carry out activities according to Lean product development willrequire more than to follow the concept’s framework for successful implementation. Factors that areidentified as important in cross-functional collaboration is also recognised in the Lean productdevelopment theory. However the theory does not indicate how the factors is implemented and carried outbut how it should be implemented. This will create opportunities and problems for companies that want towork and implement Lean product development.</p>

corrected abstract:
<p>Lean Product Development is a knowledge-based business concept in order to maintain high quality, meet customer requirements and to make product development more efficient. An important part of the process is to add a lot of resources at an early stage and execute the development as an iterative process between departments exploring many alternatives thoroughly. The work focuses on how Lean Product Development is carried out and explores how to manage interaction between different departments and expertise with regard to cross-functional collaboration and knowledge sharing i.e. learning.</p><p>The information for the studies was gathered at a major Swedish company from two projects. The projects were carried out as cross-functional and possible key factors for cross-functional collaboration was identified.</p><p>The results have been correlated with theories of Lean, Product development, Lean product development and Learning. The analysis shows that to carry out activities according to Lean product development will require more than to follow the concept’s framework for successful implementation. Factors that are identified as important in cross-functional collaboration is also recognised in the Lean product development theory. However the theory does not indicate how the factors is implemented and carried out but how it should be implemented. This will create opportunities and problems for companies that want to work and implement Lean product development.</p>
----------------------------------------------------------------------
In diva2:857656 abstract is:
<p>The collapse-behaviour of pipes was to be studied by use of Finite Element modelling.Existing analytical expressions for collapse were evaluated and especially the one used inDNV-OS-F101 was decided to be studied in comparison with FE-model results.Parameters that may influence the collapse capacity and are not included in the analyticalexpressions –flattening, peaking, eccentricity, local wall thickness variation, materialstress-strain curve, residual stresses - were defined and explained. A model was built inthe Finite Element software package Abaqus v6.9.1 and several articles on collapsetesting used to verify it. The aforementioned parameters were studied by use ofsensitivity studies and the results shown and discussed. Effective thickness definitions foruse in the DNV-formula and the DNV-yield stress criterion were discussed in the contextof the results. The results seemed to indicate that the transition between the elastic andplastic range of the material stress-strain curve was of great importance. The results werediscussed in the context of the different collapse-related parameters defined beforehandand some concluding remarks were made on possible further work related to thesefindings.</p>

corrected abstract:
<p>The collapse-behaviour of pipes was to be studied by use of Finite Element modelling. Existing analytical expressions for collapse were evaluated and especially the one used in DNV-OS-F101 was decided to be studied in comparison with FE-model results. Parameters that may influence the collapse capacity and are not included in the analytical expressions –flattening, peaking, eccentricity, local wall thickness variation, material stress-strain curve, residual stresses - were defined and explained. A model was built in the Finite Element software package Abaqus v6.9.1 and several articles on collapse testing used to verify it. The aforementioned parameters were studied by use of sensitivity studies and the results shown and discussed. Effective thickness definitions for use in the DNV-formula and the DNV-yield stress criterion were discussed in the context of the results. The results seemed to indicate that the transition between the elastic and plastic range of the material stress-strain curve was of great importance. The results were discussed in the context of the different collapse-related parameters defined beforehand and some concluding remarks were made on possible further work related to these findings.</p>
----------------------------------------------------------------------
In diva2:515487 abstract is:
<p>The teaching of mathematics has always interested me, especially after studies in engineeringand teaching. This genuine interest, together with the fact that Swedish students achieveworse grades in mathematics than for many years, is the reason why I wanted to immersemyself in this area. I hope, with this project, to make a contribution to the development of abetter education in mathematics.Living in an online world where youths spend lots of time with computer games andcommunication and having knowledge of the importance of involving users into the process ofdeveloping new systems, I found it interesting to integrate these natural driving forces intoeducation. This project has briefly compiled the biggest competitions in mathematics but alsocontemporary research, relevant to my subject, in system development. According to thesestudies, I have also designed a digital competition in mathematics and problem solving. Themain methods have been studies of different literature, communication with informedindividuals and two bigger surveys.My proposal to a competition has only been a pilot and is therefore hard to evaluate deeply.The fact that only two students participated is of course a failure but despite this I foundcomments and results from mainly the surveys very useful when developing biggercompetitions and new material to the teaching of mathematics.</p>

corrected abstract:
<p>The teaching of mathematics has always interested me, especially after studies in engineering and teaching. This genuine interest, together with the fact that Swedish students achieve worse grades in mathematics than for many years, is the reason why I wanted to immerse myself in this area. I hope, with this project, to make a contribution to the development of a better education in mathematics.</p><p>Living in an online world where youths spend lots of time with computer games and communication and having knowledge of the importance of involving users into the process of developing new systems, I found it interesting to integrate these natural driving forces into education. This project has briefly compiled the biggest competitions in mathematics but also contemporary research, relevant to my subject, in system development. According to these studies, I have also designed a digital competition in mathematics and problem solving. The main methods have been studies of different literature, communication with informed individuals and two bigger surveys.</p><p>My proposal to a competition has only been a pilot and is therefore hard to evaluate deeply. The fact that only two students participated is of course a failure but despite this I found comments and results from mainly the surveys very useful when developing bigger competitions and new material to the teaching of mathematics.</p>
----------------------------------------------------------------------
In diva2:1879348 abstract is:
<p>This project tries to solve higher-order nonlinear equations by using neural networks.When trying to solve the standard pairing problem, one is faced with higher-order nonlinear equations, these can be solved already but it takes a lot of computing power.With the help of machine learning, it might ease the computational needs to solve theseequations or to at least aid in finding good initial values to existing solvers.</p><p>The first implementation of neural networks was a model that solves higher-orderstandard polynomials with good results, from this the aim was to continue to developa model that could compute more complex equations. The development of this modelwas not a linear process due to the behavior of the equation, for smaller systems (fewerparticles) the model is often successful in finding the correct values. When trying to solvefor larger systems the model becomes more unstable and is more likely to converge toa local minimum instead of the global minimum. One might try to use more complexmodels when solving larger systems, however this was not done in this study.</p><p>The resultsare mildly promising and a model that can solve complex equations might be a possibility,even for larger systems.</p><p> </p>

corrected abstract:
<p>This project tries to solve higher-order nonlinear equations by using neural networks. When trying to solve the standard pairing problem, one is faced with higher-order nonlinear equations, these can be solved already but it takes a lot of computing power. With the help of machine learning, it might ease the computational needs to solve these equations or to at least aid in finding good initial values to existing solvers.</p><p>The first implementation of neural networks was a model that solves higher-order standard polynomials with good results, from this the aim was to continue to develop a model that could compute more complex equations. The development of this model was not a linear process due to the behavior of the equation, for smaller systems (fewer particles) the model is often successful in finding the correct values. When trying to solve for larger systems the model becomes more unstable and is more likely to converge to a local minimum instead of the global minimum. One might try to use more complex models when solving larger systems, however this was not done in this study. The results are mildly promising and a model that can solve complex equations might be a possibility, even for larger systems.</p>
----------------------------------------------------------------------
In diva2:1862229 abstract is:
<p>This report investigates a controlled reduction process (CRP) of triuraniumoctoxide (U3O8) powder into a hyperstoichiometric uranium dioxide (UO2+x)powder that could be mixed with the fresh uranium dioxide (UO2) powderwithout decreasing its sinterability. Before the CRP could be performed, scrapUO2 pellets were oxidized into U3O8 powder which was then reduced. Fiveversions of the CRP process were performed with different parameters. Thecharacteristics of the resulting CRP powders were investigated through X-raydiffraction (XRD), scanning electron microscope (SEM), and thermogravimetricanalysis (TGA). The powders were mixed with fresh UO2 powder, pressed andsintered to pellets. The density of the pellets was investigated, as was theirsurface through SEM.The results indicate that CRP powders with x below 0.25 had higher finalpellet density, suggesting that there is a benefit of reducing the U3O8 powder tothe fluorite crystal structure before mixing with the fresh UO2 powder. Furtherinvestigation and optimisation of the process is necessary, however its futureimplementation could lead to an increase in the weight fraction of recycledscrap material above the current 9 wt% maximum, among other benefits.</p>

corrected abstract:
<p>This report investigates a controlled reduction process (CRP) of triuranium octoxide (U<sub>3</sub>O<sub>8</sub>) powder into a hyperstoichiometric uranium dioxide (UO<sub>2+x</sub>) powder that could be mixed with the fresh uranium dioxide (UO<sub>2</sub>) powder without decreasing its sinterability. Before the CRP could be performed, scrap UO<sub>2</sub> pellets were oxidized into U<sub>3</sub>O<sub>8</sub> powder which was then reduced. Five versions of the CRP process were performed with different parameters. The characteristics of the resulting CRP powders were investigated through X-ray diffraction (XRD), scanning electron microscope (SEM), and thermogravimetric analysis (TGA). The powders were mixed with fresh UO<sub>2</sub> powder, pressed and sintered to pellets. The density of the pellets was investigated, as was their surface through SEM.</p><p>The results indicate that CRP powders with x below 0.25 had higher final pellet density, suggesting that there is a benefit of reducing the U<sub>3</sub>O<sub>8</sub> powder to the fluorite crystal structure before mixing with the fresh UO<sub>2</sub> powder. Further investigation and optimisation of the process is necessary, however its future implementation could lead to an increase in the weight fraction of recycled scrap material above the current 9 wt% maximum, among other benefits.</p>
----------------------------------------------------------------------
In diva2:1827778 abstract is:
<p>The purpose of this study was to identify factors contributing to graduates from thevocational education provider Nackademin, getting employed in sectors relevantto their field of study. This was done through logistic regression and the factorsexamined were: Field of study, Delivery mode of education, The graduatesrecommendation, Degree of final diploma, Amount of registered students andAmount of course points. The data consisted of surveys collected by Nackademinfrom 2015 throuh 2021. The final model was obtained through an exhaustivevariable selection on all possible models and then based on AIC. The finalmodel included the predictors: Design, Health, Education, Recommendationand Highest degree of final diploma. The model was then evaluated throughresidual analysis and a ROC curve. The ROC prediction test yielded an AUC of0.71 suggesting the final model has an acceptable predictive ability.</p>

corrected abstract:
<p>The purpose of this study was to identify factors contributing to graduates from the vocational education provider Nackademin, getting employed in sectors relevant to their field of study. This was done through logistic regression and the factors examined were: Field of study, Delivery mode of education, The graduates recommendation, Degree of final diploma, Amount of registered students and Amount of course points. The data consisted of surveys collected by Nackademin from 2015 through 2021. The final model was obtained through an exhaustive variable selection on all possible models and then based on AIC. The final model included the predictors: Design, Health, Education, Recommendation and Highest degree of final diploma. The model was then evaluated through residual analysis and a ROC curve. The ROC prediction test yielded an AUC of 0.71 suggesting the final model has an acceptable predictive ability.</p>
----------------------------------------------------------------------
In diva2:1817787 abstract is:
<p>Positron Emission Tomography (PET) is pivotal in medical imaging but is prone to artifactsfrom physiological movements, notably respiration. These motion artifacts both degradeimage quality and compromise precise attenuation correction. To counteract this, gatingstrategies partition PET data in synchronization with respiratory cycles, ensuring each gatenearly represents a static phase. Additionally, a 3D deep learning image registration modelcan be used for inter-gate motion correction, maximizing the use of the full acquired data. Thisstudy aimed to implement and evaluate two gating strategies: an external device-based approachand a data-driven centroid-of-distribution (COD) trace algorithm, and assess their impact on theperformance of the registration model. Analysis of clinical data from four subjects indicated thatthe external device approach outperformed its data-driven counterpart, which faced challengesin real-patient settings. Post motion compensation, both methods achieved results comparableto state-of-the-art reconstructions, suggesting the deep learning model addressed some data-driven method limitations. However, the motion corrected outputs did not exhibit significantimprovements in image quality over state-of-the-art standards.</p>

corrected abstract:
<p>Positron Emission Tomography (PET) is pivotal in medical imaging but is prone to artifacts from physiological movements, notably respiration. These motion artifacts both degrade image quality and compromise precise attenuation correction. To counteract this, gating strategies partition PET data in synchronization with respiratory cycles, ensuring each gate nearly represents a static phase. Additionally, a 3D deep learning image registration model can be used for inter-gate motion correction, maximizing the use of the full acquired data. This study aimed to implement and evaluate two gating strategies: an external device-based approach and a data-driven centroid-of-distribution (COD) trace algorithm, and assess their impact on the performance of the registration model. Analysis of clinical data from four subjects indicated that the external device approach outperformed its data-driven counterpart, which faced challenges in real-patient settings. Post motion compensation, both methods achieved results comparable to state-of-the-art reconstructions, suggesting the deep learning model addressed some data-driven method limitations. However, the motion corrected outputs did not exhibit significant improvements in image quality over state-of-the-art standards.</p>
----------------------------------------------------------------------
In diva2:1816881 abstract is:
<p>Gas turbines can experience various changes that affect their performance.Compressor fouling is one of the leading causes that deteriorate the gas turbineperformance. This research aims to investigate the impact of compressorfouling on the performance of gas turbines and the rotodynamic behaviorof gas turbines. Fouling was simulated as a reduction of mass flow and areduction of compressor isentropic efficiency by using Turbomatch software.A rotor–bearing model was created to analyze the vibration behavior dueto compressor fouling by using MADYN 2000 software and that particledeposition leads to rotor imbalance. The results show that the main variationsfor performance are power output, pressure ratio and EGT. For the rotodynamicmodel, the result illustrates an increase in vibration level for the first andsecond bearings and a decrease for the third bearing. The results also predictedthat parameters mass flow, compressor discharge temperature or specific fuelconsumption show a similar trend compared to the increase in vibrations. Thisresult can be used in conjunction with GPA analysis to predict the foulingcondition and help in identifying the severity of the fouling condition.</p>

corrected abstract:
<p>Gas turbines can experience various changes that affect their performance. Compressor fouling is one of the leading causes that deteriorate the gas turbine performance. This research aims to investigate the impact of compressor fouling on the performance of gas turbines and the rotodynamic behavior of gas turbines. Fouling was simulated as a reduction of mass flow and a reduction of compressor isentropic efficiency by using Turbomatch software. A rotor–bearing model was created to analyze the vibration behavior due to compressor fouling by using MADYN 2000 software and that particle deposition leads to rotor imbalance. The results show that the main variations for performance are power output, pressure ratio and EGT. For the rotodynamic model, the result illustrates an increase in vibration level for the first and second bearings and a decrease for the third bearing. The results also predicted that parameters mass flow, compressor discharge temperature or specific fuelconsumption show a similar trend compared to the increase in vibrations. This result can be used in conjunction with GPA analysis to predict the fouling condition and help in identifying the severity of the fouling condition.</p>
----------------------------------------------------------------------
In diva2:1800206 abstract is:
<p>The aim of this master thesis was to investigate the mechanical properties of ExpandedPolystyrene (EPS) through experimental testing. The strain was measured on thespecimen’s surface with Digital Image Correlation (DIC), in order to investigate howthe first principal strain at fracture initiation depends on the loading of the material.To impose different loading conditions various tests were performed, such as biaxialtension-compression, uniaxial tension and bending tests. The study was done withfour densities of EPS, 35, 50, 75 and 100 g/L. To investigate how strain and stressat fracture initiation depend on temperature, the bending tests were performed atthree different temperatures, -20, 20 and 50 °C. All tests were performed in the SolidMechanics lab at KTH, Stockholm. For every experiment, five repetitions were madeto reduce the influence of the inhomogeneous microstructure of EPS and the stochasticbehaviour of fracture mechanics. The results indicate that the fracture initiation straindecreases with increasing density of EPS, while the fracture strength increases withincreasing density of EPS. Further, the results indicate that the loading conditions havean influence on the fracture initiation strain.</p>

corrected abstract:
<p>The aim of this master thesis was to investigate the mechanical properties of Expanded Polystyrene (EPS) through experimental testing. The strain was measured on the specimen’s surface with Digital Image Correlation (DIC), in order to investigate how the first principal strain at fracture initiation depends on the loading of the material. To impose different loading conditions various tests were performed, such as biaxial tension-compression, uniaxial tension and bending tests. The study was done with four densities of EPS, 35, 50, 75 and 100 g/L. To investigate how strain and stress at fracture initiation depend on temperature, the bending tests were performed at three different temperatures, -20, 20 and 50 °C. All tests were performed in the Solid Mechanics lab at KTH, Stockholm. For every experiment, five repetitions were made to reduce the influence of the inhomogeneous microstructure of EPS and the stochastic behaviour of fracture mechanics. The results indicate that the fracture initiation strain decreases with increasing density of EPS, while the fracture strength increases with increasing density of EPS. Further, the results indicate that the loading conditions have an influence on the fracture initiation strain.</p>
----------------------------------------------------------------------
In diva2:1781251 abstract is:
<p>In order to increase the safety of all air travel, technologies that continueto augment the pilot's ability to avoid collisions and stay clear of danger areneeded. But, before these can be certified and deployed, their performance andpotential failure cases have to be understood. This requires evaluating a modelof the system on simulated encounters, consisting of different trajectoriesthat should replicate the real world.</p><p>This is commonly done using a statistical encounter model, which produces largeamounts of data but relies on the accuracy of the statistical model, thuslimited in its ability to produce realistic data. The goal with this project isto create an encounter dataset of real trajectories that would provide analternative to encounter models.</p><p>This is done using an ADS-B dataset from The OpenSky Network (provided byDaedalean AI), consisting of 226 billion air traffic data points from 2019.First, a solution to efficiently query and reconstruct trajectories from thedataset is designed and implemented. Using it, a NMAC (Near Mid-Air Collision)dataset is created to demonstrate the viability of ADS-B as a source forcreating an encounter dataset, and to prove the capabilities of the designedsolution.</p>

corrected abstract:
<p>In order to increase the safety of all air travel, technologies that continue to augment the pilot's ability to avoid collisions and stay clear of danger are needed. But, before these can be certified and deployed, their performance and potential failure cases have to be understood. This requires evaluating a model of the system on simulated encounters, consisting of different trajectories that should replicate the real world.</p><p>This is commonly done using a statistical encounter model, which produces large amounts of data but relies on the accuracy of the statistical model, thus limited in its ability to produce realistic data. The goal with this project is to create an encounter dataset of real trajectories that would provide an alternative to encounter models.</p><p>This is done using an ADS-B dataset from The OpenSky Network (provided by Daedalean AI), consisting of 226 billion air traffic data points from 2019. First, a solution to efficiently query and reconstruct trajectories from the dataset is designed and implemented. Using it, a NMAC (Near Mid-Air Collision) dataset is created to demonstrate the viability of ADS-B as a source for creating an encounter dataset, and to prove the capabilities of the designed solution.</p>
----------------------------------------------------------------------
In diva2:1780228 abstract is:
<p>DNA microscopy is a group of emerging technologies that can visualize subcellularstructures or spatial localization of biomolecules (RNA transcripts, proteins). Themethods do not rely on light, but instead rely on different ways to propagate DNAstrands through PCR followed by some kind of backtracing of their origin to recoverspatial information. The present work aims to improve on the annealing phase of aPCR simulation script previously written by the supervisor of this project. The hybridstructure between interacting DNA strands that are about to undergo replication isimportant as it determines the extension product, or whether is it possible to beelongated at all. This complex prediction is currently done with a bioinformaticalignment which is sufficiently realistic for optmimally orthogonally designed primersand adaptor regions that can be assumed to always anneal linearly and where we expectit to. NUPACK, a DNA/RNA complex prection package, was used to replace the currentalignment based approach. Comparisons of the two methods on randomly generatedDNA strings showed large differences in the predicted complexes which are interpretedas improvements.</p>

corrected abstract:
<p>DNA microscopy is a group of emerging technologies that can visualize subcellular structures or spatial localization of biomolecules (RNA transcripts, proteins). The methods do not rely on light, but instead rely on different ways to propagate DNA strands through PCR followed by some kind of backtracing of their origin to recover spatial information. The present work aims to improve on the annealing phase of a PCR simulation script previously written by the supervisor of this project. The hybrid structure between interacting DNA strands that are about to undergo replication is important as it determines the extension product, or whether is it possible to be elongated at all. This complex prediction is currently done with a bioinformatic alignment which is sufficiently realistic for optimally orthogonally designed primers and adaptor regions that can be assumed to always anneal linearly and where we expect it to. NUPACK, a DNA/RNA complex prediction package, was used to replace the current alignment based approach. Comparisons of the two methods on randomly generated DNA strings showed large differences in the predicted complexes which are interpreted as improvements.</p>
----------------------------------------------------------------------
In diva2:1757006 abstract is:
<p>The task of predicting start-up growth has been an item of institutional as wellas widespread individual research and acclaim of those successful. This workis an attempt to distill the alleged factors of prediction in the large body ofwork that has already been documented, as well as investigating reasonable butyet untested variables. Conclusions are built with a multiple regression model,exploring 7 regressors with data spanning 2014-2019 to avoid the potentiallyabnormal impact of the Covid-19 crisis.Due to the choice of non-predictive regressors, the final result is an explanatorymodel, highlighting the importance of rigorousness in the process of model-building and outlining of data collection in regression analysis. Most regres-sors had a non-significant or weak relationship with the response variable, butconcludes an explanatory degree of 51%. Even if it can not be utilised as apredictive model, it may provide some interesting insight. In the final model,every regressor except one had an unexpected beta value, contradicting earlierresearch.</p>

corrected abstract:
<p>The task of predicting start-up growth has been an item of institutional as well as widespread individual research and acclaim of those successful. This work is an attempt to distill the alleged factors of prediction in the large body of work that has already been documented, as well as investigating reasonable but yet untested variables. Conclusions are built with a multiple regression model, exploring 7 regressors with data spanning 2014-2019 to avoid the potentially abnormal impact of the Covid-19 crisis.</p><p>Due to the choice of non-predictive regressors, the final result is an explanatory model, highlighting the importance of rigorousness in the process of model-building and outlining of data collection in regression analysis. Most regressors had a non-significant or weak relationship with the response variable, but concludes an explanatory degree of 51%. Even if it can not be utilised as a predictive model, it may provide some interesting insight. In the final model, every regressor except one had an unexpected beta value, contradicting earlier research.</p>
----------------------------------------------------------------------
In diva2:1737438 abstract is:
<p>Electrification of vehicles is now one of the top priorities of automotive manufacturers in orderto comply with the sustainability goals and standards set by the United Nations. One of thedisadvantages faced by electric vehicles is a longer time required for charging the vehicles ascompared to refuelling a conventional vehicle. To speed up the charging, it is necessary toimprove the power rating to charge the vehicle at charging stations. Developing an efficient OnBoard Charger (OBC) that converts Alternating Current (AC) power to Direct Current (DC)power to charge the battery is one of the contributing factors for faster charging.</p><p>In order to protect the electricity grid from power failures, a bidirectional OBC can beimplemented in the vehicle. This bidirectional OBC is able to transfer power to the electricitygrid as well by discharging the battery. This technique helps to stabilise the grid during highpower demand. Having an efficient OBC ensures minimal losses during power conversion fromthe charging station to the battery in the vehicle.</p><p>Therefore, this thesis deals with the development of an OBC model to be incorporated ina Hardware in Loop (HIL) simulation. This model is only an emulation of the actual OBChardware for HIL test and is therefore limited. The model is mainly developed in Simulinkand consists of control logic and power conversion stages like Precharge and Power FactorCorrection that successfully imitate the actual hardware.</p><p>Further, in order to validate the model, it was run in Model in Loop (MIL) simulationwherein the model was tested under scenarios similar to the ones in the HIL setup. A successfulconversion of AC power to DC power was performed and a demonstration of the flexibility andthe adaptability of the model to different test cases is documented in the report. Some of theoutputs of the model are DC current for charging, model stateflow and AC current drawn fromthe grid.</p>

corrected abstract:
<p>Electrification of vehicles is now one of the top priorities of automotive manufacturers in order to comply with the sustainability goals and standards set by the United Nations. One of the disadvantages faced by electric vehicles is a longer time required for charging the vehicles as compared to refuelling a conventional vehicle. To speed up the charging, it is necessary to improve the power rating to charge the vehicle at charging stations. Developing an efficient On Board Charger (OBC) that converts Alternating Current (AC) power to Direct Current (DC) power to charge the battery is one of the contributing factors for faster charging.</p><p>In order to protect the electricity grid from power failures, a bidirectional OBC can be implemented in the vehicle. This bidirectional OBC is able to transfer power to the electricity grid as well by discharging the battery. This technique helps to stabilise the grid during high power demand. Having an efficient OBC ensures minimal losses during power conversion from the charging station to the battery in the vehicle.</p><p>Therefore, this thesis deals with the development of an OBC model to be incorporated in a Hardware in Loop (HIL) simulation. This model is only an emulation of the actual OBC hardware for HIL test and is therefore limited. The model is mainly developed in Simulink and consists of control logic and power conversion stages like Precharge and Power Factor Correction that successfully imitate the actual hardware.</p><p>Further, in order to validate the model, it was run in Model in Loop (MIL) simulation wherein the model was tested under scenarios similar to the ones in the HIL setup. A successful conversion of AC power to DC power was performed and a demonstration of the flexibility and the adaptability of the model to different test cases is documented in the report. Some of the outputs of the model are DC current for charging, model stateflow and AC current drawn from the grid.</p>
----------------------------------------------------------------------
In diva2:1698044 abstract is:
<p>The drone industry is growing and the need for increased autonomy will be required if large fleetof drones will be able to fly without a single pilot per drone. A useful part of automating the flighten-route can be achieved with the upcoming standard of Direct Remote Id (DRI), which signalspositional data for drones and can be used as the perceptive part in a collision avoidance systembetween drones with the advantage of limited weight penalties and minimal financial cost.Simulations were carried out to understand different kinds of evasive maneuvers and develop asimple yet effective algorithm for avoiding obstacles and continue towards the next waypoint ona mission. Positional data can be retrieved with an ESP-32 board from a flight computer withMavlink protocol, which can then be broadcasted and received to an ESP-32 board using DirectRemote Id. The distances between the nearest drones can be computed, along with the shortest al-lowable distance and closest positions of the drones, if they were to continue on a straight course. Ifthe closest passing distance turned out closer than a set safety distance, an evasive maneuver is cal-culated and executed, with preliminary work focusing on evasion maneuvers on an horizontal plane.Flight tests showed that an evasive position could be calculated, and the drone successfully di-verted to it, while continuing with the mission after the evasion was completed. These resultsshowed the potential of using Direct Remote Id as a simple close proximity detection for use withcollision avoidance</p>

corrected abstract:
<p>The drone industry is growing and the need for increased autonomy will be required if large fleet of drones will be able to fly without a single pilot per drone. A useful part of automating the flight en-route can be achieved with the upcoming standard of Direct Remote Id (DRI), which signals positional data for drones and can be used as the perceptive part in a collision avoidance system between drones with the advantage of limited weight penalties and minimal financial cost.</p><p>Simulations were carried out to understand different kinds of evasive maneuvers and develop a simple yet effective algorithm for avoiding obstacles and continue towards the next waypoint on a mission. Positional data can be retrieved with an ESP-32 board from a flight computer with Mavlink protocol, which can then be broadcasted and received to an ESP-32 board using Direct Remote Id. The distances between the nearest drones can be computed, along with the shortest allowable distance and closest positions of the drones, if they were to continue on a straight course. If the closest passing distance turned out closer than a set safety distance, an evasive maneuver is calculated and executed, with preliminary work focusing on evasion maneuvers on an horizontal plane.</p><p>Flight tests showed that an evasive position could be calculated, and the drone successfully diverted to it, while continuing with the mission after the evasion was completed. These results showed the potential of using Direct Remote Id as a simple close proximity detection for use with collision avoidance</p>
----------------------------------------------------------------------
In diva2:1681378 abstract is:
<p>This project has investigated and formed a basis for the thermal- and flow-induceddynamic loads in the lower parts of a preheater situated in a nuclear power plant.Special attention has been given to the bottom plate that separates the heated andnon-heated secondary steam. Using computational fluid dynamics (CFD), steady RANSsimulations were first used to investigate how the flow into the preheater was affectedby the inlet boundary conditions. From there a steady RANS conjugate heat transfer(CHT) analysis was conducted as to obtain the temperature field within the preheaterand its solid components. This also investigated different approaches in modelling theheat transfer between the primary and secondary steam. Lastly, a scale-resolving LESwas conducted as to obtain the flow-induced dynamic loads on the bottom plate.</p><p>The results show that the modelling used in previous works gives a less uniformtemperature distribution as compared to when appropriate heat transfer coefficient(HTC) correlations are applied. Regardless of how the heat source is modelled, hot spotswith significantly larger temperatures are present in the bottom plate near the outletsof the bottom tube sections. The root mean square value, amplitude and frequencycontent of the fluctuating force acting on the bottom plate have also been obtained.The results of the analysis provide a good starting point for future work examining ifthe loads on their own or in combination may risk damaging the structure.</p>

corrected abstract:
<p>This project has investigated and formed a basis for the thermal- and flow-induced dynamic loads in the lower parts of a preheater situated in a nuclear power plant. Special attention has been given to the bottom plate that separates the heated and non-heated secondary steam. Using computational fluid dynamics (CFD), steady RANS simulations were first used to investigate how the flow into the preheater was affected by the inlet boundary conditions. From there a steady RANS conjugate heat transfer (CHT) analysis was conducted as to obtain the temperature field within the preheater and its solid components. This also investigated different approaches in modelling the heat transfer between the primary and secondary steam. Lastly, a scale-resolving LES was conducted as to obtain the flow-induced dynamic loads on the bottom plate.</p><p>The results show that the modelling used in previous works gives a less uniform temperature distribution as compared to when appropriate heat transfer coefficient (HTC) correlations are applied. Regardless of how the heat source is modelled, hot spots with significantly larger temperatures are present in the bottom plate near the outlets of the bottom tube sections. The root mean square value, amplitude and frequency content of the fluctuating force acting on the bottom plate have also been obtained. The results of the analysis provide a good starting point for future work examining if the loads on their own or in combination may risk damaging the structure.</p>
----------------------------------------------------------------------
In diva2:1588547 abstract is:
<p>Some physical systems exhibit topological properties in the form of topological invariants— features of the system that remain constant unless the system undergoessignificant changes i.e. changes that require closing the energy gap of the Hamiltonian.This work studies one example of a system with topological properties — a Kitaevchain. Here, this model is studied when it is coupled to an environment. We studythe effect of the coupling on the topology of the system and attempt to find signaturesof topological phases in the dynamics of the system. By using the Lindblad equationdefined in the formalism of third quantization, we study the time evolution of thesystem numerically by using the Euler method. We find that the dynamics of theentanglement spectrum of half of the chain is different in the topological and trivialphases: if the system undergoes a quench from trivial to topological phase, the entanglementspectrum exhibits crossings as the system evolves in time. We also studythe topological phases when disorder is added to the system. We test the stabilityof the topological phases of the system against disorder and find that the topologicalphases are not affected by a weak disorder. Moreover, by studying the statistics of theminimum entanglement spectrum gap, we find that, in general, a stronger disordermakes the crossings less likely to appear in the topological phase and more likely toappear in the trivial phase.</p>

corrected abstract:
<p>Some physical systems exhibit topological properties in the form of topological invariants—features of the system that remain constant unless the system undergoes significant changes i.e. changes that require closing the energy gap of the Hamiltonian. This work studies one example of a system with topological properties — a Kitaev chain. Here, this model is studied when it is coupled to an environment. We study the effect of the coupling on the topology of the system and attempt to find signatures of topological phases in the dynamics of the system. By using the Lindblad equation defined in the formalism of third quantization, we study the time evolution of the system numerically by using the Euler method. We find that the dynamics of the entanglement spectrum of half of the chain is different in the topological and trivial phases: if the system undergoes a quench from trivial to topological phase, the entanglement spectrum exhibits crossings as the system evolves in time. We also study the topological phases when disorder is added to the system. We test the stability of the topological phases of the system against disorder and find that the topological phases are not affected by a weak disorder. Moreover, by studying the statistics of the minimum entanglement spectrum gap, we find that, in general, a stronger disorder makes the crossings less likely to appear in the topological phase and more likely to appear in the trivial phase.</p>
----------------------------------------------------------------------
In diva2:1528157 abstract is:
<p>Typical propulsion systems for space transportation involve the ejectionof mass for momentum gain. Solar sails remove the requirement forpropellant mass by obtaining their momentum from solar photons, whichrequires large surface area and very low mass. In this way solar sailcraftgenerate constant accelerations, in contrast with the impulsive thrust ofchemical rockets. This enables new families of orbits and presents a newchallenge for optimization and control. This study presents a summary ofproven solar sail technology and investigates minimum-time trajectoriesto and from Mars. This optimization is carried out in two phases, usingan energy rate-maximizing algorithm for planetary escape and sparse nonlinearprogramming for the interplanetary segment. The results provideupper bounds for minimum-time transfers and are then compared to possiblesail sizes and sailcraft masses. This in turn may inform the designand selection of future missions for materials exchange during explorationor settlement efforts.</p>

corrected abstract:
<p>Typical propulsion systems for space transportation involve the ejection of mass for momentum gain. Solar sails remove the requirement for propellant mass by obtaining their momentum from solar photons, which requires large surface area and very low mass. In this way solar sailcraft generate constant accelerations, in contrast with the impulsive thrust of chemical rockets. This enables new families of orbits and presents a new challenge for optimization and control. This study presents a summary of proven solar sail technology and investigates minimum-time trajectories to and from Mars. This optimization is carried out in two phases, using an energy rate-maximizing algorithm for planetary escape and sparse nonlinear programming for the interplanetary segment. The results provide upper bounds for minimum-time transfers and are then compared to possible sail sizes and sailcraft masses. This in turn may inform the design and selection of future missions for materials exchange during exploration or settlement efforts.</p>
----------------------------------------------------------------------
In diva2:1499290 abstract is:
<p>Three different void content calculation techniques using optical microscopy werecompared in multiple-user trials. The three methods studied comprised of a selection,thresholding, and semi-automatic machine learning method. The techniques wereapplied to micrographs of three carbon fiber-epoxy composite plates manufacturedin-house, where one plate had reduced void content by means of debulking priorto curing. The users performed the techniques on the sets of micrographs and thestandard deviation between the users void content results were measured.The advantages of the three methods were discussed and their practical applications wereproposed.</p><p>The trials showed agreement between users on what are voids and not as well asshowing that uncertainties in void content are specimen-specific and not attributed todifferent users or methods applied. All three methods showed satisfying precision incalculating void content compared to void content quality levels provided by literature.It was found that thresholding, which is the current standard method of void contentcalculation using microscopy, inhabits an unscientific bias which compromises the legitimacyof the method. The study formulates a manual selection-based method usingedge-detection selection tools intended to benchmark void content in images, as wellas proposing a route to the automation of void content analysis using microscopy.</p>

corrected abstract:
<p>Three different void content calculation techniques using optical microscopy were compared in multiple-user trials. The three methods studied comprised of a selection, thresholding, and semi-automatic machine learning method. The techniques were applied to micrographs of three carbon fiber-epoxy composite plates manufactured in-house, where one plate had reduced void content by means of debulking prior to curing. The users performed the techniques on the sets of micrographs and the standard deviation between the users void content results were measured. The advantages of the three methods were discussed and their practical applications were proposed.</p><p>The trials showed agreement between users on what are voids and not as well as showing that uncertainties in void content are specimen-specific and not attributed to different users or methods applied. All three methods showed satisfying precision in calculating void content compared to void content quality levels provided by literature. It was found that thresholding, which is the current standard method of void content calculation using microscopy, inhabits an unscientific bias which compromises the legitimacy of the method. The study formulates a manual selection-based method using edge-detection selection tools intended to benchmark void content in images, as well as proposing a route to the automation of void content analysis using microscopy.</p>
----------------------------------------------------------------------
In diva2:1244644 abstract is:
<p>Holms Industri AB currently manufactures a snow plow called Holms PD Plus, where thedisc is made of a 4 mm thick steelplate. The disc weighs 131 kg and since the mass of thesnow plow is closely related to the fuel consumption of the carrying vehicle it is of interest toreduce the mass of the plow. Therefore, the aim of this report is to investigate if it is possibleto manufacture the disc out of plastic instead, namely high density polyethylene (HDPE). Achange of material also causes the need to design new mountings since the current weldingmethod no longer would be an alternative.The analyses are based on eight different load cases that occur during snow clearance.However, only three of these are used to dimension the plow disc since the others are not ascritical. The current snow plow was analysed and used as a reference in the development ofalternative snow plows. All analyses were performed in the FEM-programme AnsysWorkbench 17.1.The results show that it is possible to use a plastic plow disc if it is 10 mm thick. In detail, thetop domed part should be 10 mm HDPE and the lower straight part should be 8 mm HDPEsupported on the back by a 2 mm thick steel plate. The suggested disc weighs 73 kg, meaninga reduction by 58 kg compared to the original. The attachments should be a combinationof welds (metal to metal) and bolted joints (plastic to metal). Further work should concernfatigue analyses of the plow disc and more in depth assessments of the mountings.</p>

corrected abstract:
<p>Holms Industri AB currently manufactures a snow plow called Holms PD Plus, where the disc is made of a 4 mm thick steelplate. The disc weighs 131 kg and since the mass of the snow plow is closely related to the fuel consumption of the carrying vehicle it is of interest to reduce the mass of the plow. Therefore, the aim of this report is to investigate if it is possible to manufacture the disc out of plastic instead, namely high density polyethylene (HDPE). A change of material also causes the need to design new mountings since the current welding method no longer would be an alternative.</p><p>The analyses are based on eight different load cases that occur during snow clearance. However, only three of these are used to dimension the plow disc since the others are not as critical. The current snow plow was analysed and used as a reference in the development of alternative snow plows. All analyses were performed in the FEM-programme Ansys Workbench 17.1.</p><p>The results show that it is possible to use a plastic plow disc if it is 10 mm thick. In detail, the top domed part should be 10 mm HDPE and the lower straight part should be 8 mm HDPE supported on the back by a 2 mm thick steel plate. The suggested disc weighs 73 kg, meaning a reduction by 58 kg compared to the original. The attachments should be a combination of welds (metal to metal) and bolted joints (plastic to metal). Further work should concern fatigue analyses of the plow disc and more in depth assessments of the mountings.</p>
----------------------------------------------------------------------
In diva2:1216849 abstract is:
<p>Competitive gaming or E-Sport is more popular than ever, this has resulted inan increase in the number of players and tournament prize pools. In traditionalsports demographic factors have been shown to have high predictive power whenit comes to determining a country's success in the Olympic Games. Similarresults have been found when it comes to E-sport which is why it is interestingto investigate whether there are any dierences between regions in Dota 2 aswell. The goal is to analyze factors that contribute to the success of a Dota 2team by building a multiple-regression model. All data is collected from opensources and contains 55 active Dota 2 teams that have been playing between2011 - 2018. The factors in the nal model is the sum of the individual playersestimated skill, the skill dierence between the highest and lowest rated playerson the team, the number of games the team has played and organization region.The result gives an insight in what a person or organization would want to lookat when researching a team as well as a model that can be used to predict howgood a team will perform.</p>

corrected abstract:
<p>Competitive gaming or E-Sport is more popular than ever, this has resulted in an increase in the number of players and tournament prize pools. In traditional sports demographic factors have been shown to have high predictive power when it comes to determining a country's success in the Olympic Games. Similar results have been found when it comes to E-sport which is why it is interesting to investigate whether there are any differences between regions in Dota 2 as well. The goal is to analyze factors that contribute to the success of a Dota 2 team by building a multiple-regression model. All data is collected from open sources and contains 55 active Dota 2 teams that have been playing between 2011 - 2018. The factors in the final model is the sum of the individual players estimated skill, the skill difference between the highest and lowest rated players on the team, the number of games the team has played and organization region. The result gives an insight in what a person or organization would want to look at when researching a team as well as a model that can be used to predict how good a team will perform.</p>
----------------------------------------------------------------------
In diva2:1142776 abstract is:
<p>With the increasing advances in the field of autonomousvehicles it is alluring to ask if a possible vehicularparadigm shift is in the near future. Maximizing road capacitywith Intelligent Traffic Intersections that communicate withautonomous vehicles could become a reality, where the needfor traffic lights and stop signs is excluded. In this paper, anAutonomous Intersection Management system is introduced thatutilizes trajectory-based prioritization and motion planning techniquesto manage traffic in an orthogonal single lane four-wayintersection. The developed system reduces the need for vehiclesto slow down or even stop before intersections, contrariwise, itlets all vehicles enter the intersection at the highest allowed speed.The proposed solution is shown to increase the capacity of intersectionscompared with contemporary intersections managedwith traffic lights.</p>

corrected abstract:
<p>With the increasing advances in the field of autonomous vehicles it is alluring to ask if a possible vehicular paradigm shift is in the near future. Maximizing road capacity with Intelligent Traffic Intersections that communicate with autonomous vehicles could become a reality, where the need for traffic lights and stop signs is excluded. In this paper, an Autonomous Intersection Management system is introduced that utilizes trajectory-based prioritization and motion planning techniques to manage traffic in an orthogonal single lane four-way intersection. The developed system reduces the need for vehicles to slow down or even stop before intersections, contrariwise, it lets all vehicles enter the intersection at the highest allowed speed. The proposed solution is shown to increase the capacity of intersections compared with contemporary intersections managed with traffic lights.</p>
----------------------------------------------------------------------
In diva2:1142764 abstract is:
<p>A formation of vehicles with close inter-vehicledistances is referred to as a vehicle platoon. Platooning is madepossible by implementing controllers that regulates vehicles’velocities based on information from the rest of the platoon,independently from human interaction. This report studies howthe stability of the system is affected by the choice of availableinformation for each vehicle. Vehicle models are implemented asblock diagrams in Simulink and the feedback gains are calculatedusing MATLAB. The vehicles’ behaviors are simulated using apredefined acceleration profile for the leader. The results showthat the system reacts differently to changes in the leader’sacceleration depending on which states each vehicle determinesits control signal on. Stability is best achieved by having thevehicles communicating with the vehicle in front and the leadingvehicle. It is also shown that time delays negatively affects thestability of the system, but stability is still best achieved whenvehicles communicate with the vehicle in front and the leadingvehicle.</p>

corrected abstract:
<p>A formation of vehicles with close inter-vehicle distances is referred to as a vehicle platoon. Platooning is made possible by implementing controllers that regulates vehicles’ velocities based on information from the rest of the platoon, independently from human interaction. This report studies how the stability of the system is affected by the choice of available information for each vehicle. Vehicle models are implemented as block diagrams in Simulink and the feedback gains are calculated using MATLAB. The vehicles’ behaviors are simulated using a predefined acceleration profile for the leader. The results show that the system reacts differently to changes in the leader’s acceleration depending on which states each vehicle determines its control signal on. Stability is best achieved by having the vehicles communicating with the vehicle in front and the leading vehicle. It is also shown that time delays negatively affects the stability of the system, but stability is still best achieved when vehicles communicate with the vehicle in front and the leading vehicle.</p>
----------------------------------------------------------------------
In diva2:1120852 - duplicate of In diva2:1142776?
abstract is:
<p>With the increasing advances in the field of autonomousvehicles it is alluring to ask if a possible vehicularparadigm shift is in the near future. Maximizing road capacitywith Intelligent Traffic Intersections that communicate withautonomous vehicles could become a reality, where the needfor traffic lights and stop signs is excluded. In this paper, anAutonomous Intersection Management system is introduced thatutilizes trajectory-based prioritization and motion planning techniquesto manage traffic in an orthogonal single lane four-wayintersection. The developed system reduces the need for vehiclesto slow down or even stop before intersections, contrariwise, itlets all vehicles enter the intersection at the highest allowed speed.The proposed solution is shown to increase the capacity of intersectionscompared with contemporary intersections managedwith traffic lights.</p>

corrected abstract:
<p>With the increasing advances in the field of autonomous vehicles it is alluring to ask if a possible vehicular paradigm shift is in the near future. Maximizing road capacity with Intelligent Traffic Intersections that communicate with autonomous vehicles could become a reality, where the need for traffic lights and stop signs is excluded. In this paper, an Autonomous Intersection Management system is introduced that utilizes trajectory-based prioritization and motion planning techniques to manage traffic in an orthogonal single lane four-way intersection. The developed system reduces the need for vehicles to slow down or even stop before intersections, contrariwise, it lets all vehicles enter the intersection at the highest allowed speed. The proposed solution is shown to increase the capacity of intersections compared with contemporary intersections managed with traffic lights.</p>
----------------------------------------------------------------------
In diva2:1120393 abstract is:
<p>This work explores the possibility of using a photon counting silicon detector developedfor Computer Tomography (CT) in Single Photon Emission Computed Tomography(SPECT) applications. This would allow for a more versatile and a much cheapersystem. The main focus is on determining the efficiency and resolution of such a system.This is done initially via an geometric model evaluating the solid angle of the collimator,showing promising results versus a standard Low Energy High Resolution (LEHR)collimator. Secondly a Monte-Carlo simulation is used for a more in depth analysis ofthe detector response by using two different radionuclides. The performance is measuredwith reference to efficiency and scatter to primary ratio (s/p). The energy thresholdsfor binning is evaluated with a Signal-Difference-to-Noise Ratio (SDNR). A Point SpreadFunction (PSF) is simulated with and without the impact from a human-like phantom.The work concludes that an implementation would not likely to be able to compete withspecialised myocardium SPECT due too the high noise from the detector response whena high efficiency threshold is set. Further investigations in general SPECT applicationsis recommended.Sammanfattning</p>

corrected abstract:
<p>This work explores the possibility of using a photon counting silicon detector developed for Computer Tomography (CT) in Single Photon Emission Computed Tomography (SPECT) applications. This would allow for a more versatile and a much cheaper system. The main focus is on determining the efficiency and resolution of such a system. This is done initially via an geometric model evaluating the solid angle of the collimator, showing promising results versus a standard Low Energy High Resolution (LEHR) collimator. Secondly a Monte-Carlo simulation is used for a more in depth analysis of the detector response by using two different radionuclides. The performance is measured with reference to efficiency and scatter to primary ratio (s/p). The energy thresholds for binning is evaluated with a Signal-Difference-to-Noise Ratio (SDNR). A Point Spread Function (PSF) is simulated with and without the impact from a human-like phantom. The work concludes that an implementation would not likely to be able to compete with specialised myocardium SPECT due too the high noise from the detector response when a high efficiency threshold is set. Further investigations in general SPECT applications is recommended.</p>
----------------------------------------------------------------------
In diva2:1110826 abstract is:
<p>As surprising as it may seem, accurate north finding, with an error of only several milli-radian, is still a very difficulttask and has been achieved only with very expensive systems. On the contrary, there are very simple systems that give theazimuth with an angular error five times superior but for a price a hundred times inferior. Moreover, these systems generally arenon-autonomous (i.e. they are environment dependent and can lose their precision in many situations). This assessment leads tothe following relevant question: Is it possible to design a north finding system with good precision, for a moderated cost and thatworks in any situation?This report presents and evaluates a solution which attempts to answer this problem. This solution is based on a gyrocompassingprinciple: a gyro measures the earth’s angular velocity in order to find the azimuth. This solution can be implementedfollowing several methods, this report presents and compares two of these implementations: Maytagging and Carouseling. Thecomparison is made thanks to a theoretical study, a computer simulation and tests on a real model designed for this report.Carouseling allows us, in theory, to reach an accurate azimuth, but puts mechanical constraints on the system. Maytaggingimplementation seems adapted considering trade-off between precision and cost. Further improvements on gyros will certainlymake systems based on gyro-compassing the most efficient autonomous systems for north finding.In this report, precisions reached by the different implementations are not made explicit for confidentiality reasons.</p>

corrected abstract:
<p>As surprising as it may seem, accurate north finding, with an error of only several milli-radian, is still a very difficult task and has been achieved only with very expensive systems. On the contrary, there are very simple systems that give the azimuth with an angular error five times superior but for a price a hundred times inferior. Moreover, these systems generally are non-autonomous (i.e. they are environment dependent and can lose their precision in many situations). This assessment leads to the following relevant question: Is it possible to design a north finding system with good precision, for a moderated cost and that works in any situation?</p><p>This report presents and evaluates a solution which attempts to answer this problem. This solution is based on a gyrocompassing principle: a gyro measures the earth’s angular velocity in order to find the azimuth. This solution can be implemented following several methods, this report presents and compares two of these implementations: Maytagging and Carouseling. The comparison is made thanks to a theoretical study, a computer simulation and tests on a real model designed for this report. Carouseling allows us, in theory, to reach an accurate azimuth, but puts mechanical constraints on the system. Maytagging implementation seems adapted considering trade-off between precision and cost. Further improvements on gyros will certainly make systems based on gyro-compassing the most efficient autonomous systems for north finding.<br>In this report, precisions reached by the different implementations are not made explicit for confidentiality reasons.</p>
----------------------------------------------------------------------
In diva2:1106332 - missing space in title:
"Coefficients and zeros of mixed characteristicpolynomials"
==>
"Coefficients and zeros of mixed characteristic polynomials"

abstract is:
<p>The mixed characteristic polynomial (MCP) was introduced in the papersof Marcus, Spielman and Srivastava from 2013 on Ramanujan graphs and the Kadison-Singerconjecture. Several known results and open problems can be formulated in termsof MCPs. The proofs of Marcus, Spielman and Srivastava involve bounding theroots of certain MCPs. Gurvits’ generalization of van der Waerden’s permanentconjecture bounds the constant term of MCPs using the capacity of an underlyingpolynomial.This thesis surveys selected results for MCPs. A counterexample to theHolens-Ðoković conjecture, due to Wanless, is discussed in the context of MCPs.It is used to show how a sequence of MCP coefficients is not monotoneand how the roots of associated Laguerre polynomials do not always majorizethose of other MCPs. Finally, we prove an analogue of the root bound in theproof of the Kadison-Singer conjecture. It applies to product polynomials ofdoubly stochastic matrices through classical results in graph theory due toGodsil, Mohar, Heilmann and Lieb.</p>

corrected abstract:
<p>The mixed characteristic polynomial (MCP) was introduced in the papers of Marcus, Spielman and Srivastava from 2013 on Ramanujan graphs and the Kadison-Singer conjecture. Several known results and open problems can be formulated in terms of MCPs. The proofs of Marcus, Spielman and Srivastava involve bounding the roots of certain MCPs. Gurvits’ generalization of van der Waerden’s permanent conjecture bounds the constant term of MCPs using the capacity of an underlying polynomial.</p><p>This thesis surveys selected results for MCPs. A counterexample to the Holens-Ðoković conjecture, due to Wanless, is discussed in the context of MCPs. It is used to show how a sequence of MCP coefficients is not monotone<br>and how the roots of associated Laguerre polynomials do not always majorize those of other MCPs. Finally, we prove an analogue of the root bound in the proof of the Kadison-Singer conjecture. It applies to product polynomials of<br>doubly stochastic matrices through classical results in graph theory due to Godsil, Mohar, Heilmann and Lieb.</p>

Note there are two strange breaks - but they are in the original abstract.
----------------------------------------------------------------------
In diva2:662361 abstract is:
<p>In this thesis it isshown how to measure the annual loss expectancy of computer networks due to therisk of cyber attacks. With the development of metrics for measuring theexploitation difficulty of identified software vulnerabilities, it is possibleto make a measurement of the annual loss expectancy for computer networks usingBayesian networks. To enable the computations, computer net-work vulnerabilitydata in the form of vulnerability model descriptions, vulnerable dataconnectivity relations and intrusion detection system measurements aretransformed into vector based numerical form. This data is then used to generatea probabilistic attack graph which is a Bayesian network of an attack graph.The probabilistic attack graph forms the basis for computing the annualizedloss expectancy of a computer network. Further, it is shown how to compute anoptimized order of vulnerability patching to mitigate the annual lossexpectancy. An example of computation of the annual loss expectancy is providedfor a small invented example network</p>

corrected abstract:
<p>In this thesis it is shown how to measure the annual loss expectancy of computer networks due to the risk of cyber attacks. With the development of metrics for measuring the exploitation difficulty of identified software vulnerabilities, it is possible to make a measurement of the annual loss expectancy for computer networks using Bayesian networks. To enable the computations, computer network vulnerability data in the form of vulnerability model descriptions, vulnerable data connectivity relations and intrusion detection system measurements are transformed into vector based numerical form. This data is then used to generate a probabilistic attack graph which is a Bayesian network of an attack graph. The probabilistic attack graph forms the basis for computing the annualized loss expectancy of a computer network. Further, it is shown how to compute an optimized order of vulnerability patching to mitigate the annual loss expectancy. An example of computation of the annual loss expectancy is provided for a small invented example network</p>
----------------------------------------------------------------------
In diva2:620469 abstract is:
<p>Disabilitybenefit is a publicly funded benefit in Sweden that provides financialprotection to individuals with permanent working ability impairments due todisability, injury, or illness. The eligibility requirements for disabilitybenefit were tightened June 1, 2008 to require that the working abilityimpairment be permanent and that no other factors such as age or local labormarket conditions can affect eligibility for the benefit. The goal of thispaper is to investigate risk factors for the incidence disability benefit andthe effects of the 2008 reform. This is the first study to investigate theimpact of the 2008 reform on the demographics of those that received disabilitybenefit. A logistic regression model was used to study the effect of the 2008law change. The regression results show that the 2008 reform did have astatistically significant effect on the demographics of the individuals whowere granted disability benefit. After the reform women were lessoverrepresented, the older age groups were more overrepresented, and peoplewith short educations were more overrepresented. Although the variables for SKLregions together were jointly statistically significant, their coefficientswere small and the group of variables had the least amount of explanatory valuecompared to the variables for age, education, gender and the interactionvariables.</p>

corrected abstract:
<p>Disability benefit is a publicly funded benefit in Sweden that provides financial protection to individuals with permanent working ability impairments due to disability, injury, or illness. The eligibility requirements for disability benefit were tightened June 1, 2008 to require that the working ability impairment be permanent and that no other factors such as age or local labor market conditions can affect eligibility for the benefit. The goal of this paper is to investigate risk factors for the incidence disability benefit and the effects of the 2008 reform. This is the first study to investigate the impact of the 2008 reform on the demographics of those that received disability benefit. A logistic regression model was used to study the effect of the 2008 law change. The regression results show that the 2008 reform did have a statistically significant effect on the demographics of the individuals who were granted disability benefit. After the reform women were less overrepresented, the older age groups were more overrepresented, and people with short educations were more overrepresented. Although the variables for SKL regions together were jointly statistically significant, their coefficients were small and the group of variables had the least amount of explanatory value compared to the variables for age, education, gender and the interaction variables.</p>
----------------------------------------------------------------------
In diva2:618600 - Note: no full text in DiVA
abstract is:
<p>Due to composite materials are widely used nowadays, it is important to understand manyareas concerning their behaviour that are not completely understood yet.This work focuses on the study of the strength of GRP-laminates with multiple randomlydistributed holes. For this purpose, percolation theory has been used in order totry to predict the normalized strength of laminates as function of only the hole density(the amount of hole area/specimen area).Three dierent laminates have been studied, DBLT800, DBL600 and DBL850, withmany simulations and tensile tests, in order to obtain results with good accuracy. Forsimulations, a suitable model was developed in FE-code ABAQUS in order to get thestrength of every model. For tensile tests, plates were manufactured previously by a vacuumassisted infusion process. Then, the plates were cut, drilled and tested. Once all theresults have been obtained, simulations and experiments were compared in order to knowif the strength obtained was good.These results obtained are very encouraging, showing that percolation theory can beused for predicting the strength of perforated composite plates, and the same curve obtainedmay be used for both DBLT800 and DBL600 laminates with suciently gooddelity.</p>

corrected abstract:
<p>Due to composite materials are widely used nowadays, it is important to understand many areas concerning their behaviour that are not completely understood yet. This work focuses on the study of the strength of GRP-laminates with multiple randomly distributed holes. For this purpose, percolation theory has been used in order to try to predict the normalized strength of laminates as function of only the hole density(the amount of hole area/specimen area).Three different laminates have been studied, DBLT800, DBL600 and DBL850, with many simulations and tensile tests, in order to obtain results with good accuracy. For simulations, a suitable model was developed in FE-code ABAQUS in order to get the strength of every model. For tensile tests, plates were manufactured previously by a vacuumassisted infusion process. Then, the plates were cut, drilled and tested. Once all the results have been obtained, simulations and experiments were compared in order to know if the strength obtained was good. These results obtained are very encouraging, showing that percolation theory can be used for predicting the strength of perforated composite plates, and the same curve obtained may be used for both DBLT800 and DBL600 laminates with suciently good fidelity.</p>
----------------------------------------------------------------------
In diva2:515581 abstract is:
<p>This essay is the report of my master's dissertation for Master of Engineering and Teacher Program atKTH and SU in Stockholm. The aims of the master’s dissertation are to increase the understandingand compose a map of how different presentation models affect the interest and ability to learnwhen the subject is resilient construction element is presented on a one day seminar.I have composed maps of three different presentation models in which the presentation models’communicative approach varied between authoritative and dialogic. The presentation models thathave been composed a map of are:• Seminar designed by problem-based learning• Seminar designed by the case method• Authoritative seminar The presentations models were tested and based on the tests, the feedback from participants and myparticipation studies, the presentation were surveyed and conclusions drawn about the learning and theinterest. This study was performed in the subject resilient construction element with the target groupadults with a basic skill in mechanics.The result of this study is that the potential for learning and interest increased if the communicativeapproach is in the dialogic direction. The communicative approach should neither be veryauthoritative or very dialogic. This is because the interest tends to decrease when the communicativeapproach is too authoritative and because the participants have problem with directing the seminarif communicative approach is too dialogic. Difficulties occur for the participants to control thedialogic communicative approach if they don’t have prior knowledge. The seminar model thatincreased learning and interests the most was the case-seminar format.</p>

corrected abstract:
<p>This essay is the report of my master's dissertation for Master of Engineering and Teacher Program at KTH and SU in Stockholm. The aims of the master’s dissertation are to increase the understanding and compose a map of how different presentation models affect the interest and ability to learn when the subject is resilient construction element is presented on a one day seminar. I have composed maps of three different presentation models in which the presentation models’ communicative approach varied between authoritative and dialogic. The presentation models that have been composed a map of are:<ul><li>Seminar designed by problem-based learning</li><li>Seminar designed by the case method</li><li>Authoritative seminar</li></ul></p><p>The presentations models were tested and based on the tests, the feedback from participants and my participation studies, the presentation were surveyed and conclusions drawn about the learning and the interest. This study was performed in the subject resilient construction element with the target group adults with a basic skill in mechanics.</p><p>The result of this study is that the potential for learning and interest increased if the communicative approach is in the dialogic direction. The communicative approach should neither be very authoritative or very dialogic. This is because the interest tends to decrease when the communicative approach is too authoritative and because the participants have problem with directing the seminar if communicative approach is too dialogic. Difficulties occur for the participants to control the dialogic communicative approach if they don’t have prior knowledge. The seminar model that increased learning and interests the most was the case-seminar format.</p>
----------------------------------------------------------------------
In diva2:1905032 abstract is:
<p>Studying White Matter (WM) lesions in premature babies is a societal challenge, aspremature mortality is decreasing while neurological morbidity remains the same.This master thesis is part of the p-HCP (Premature Human Connectome Project),which aims to study these lesions using extreme magnetic field MRI (11.7T) , startingwith the construction of a typical ex vivo brain atlas at mesoscopic scale and comparingit, in a second phase, to an atlas of damaged brains. This work presents a segmentationstrategy for the fetal brain structures to characterize them at different key stages ofthe development using MRI images with unprecedented resolution. We first manuallysegmented the structures of a 20-week gestation brain using a histological atlas, inorder to explore the information contained the nineteen MRI modalities available(quantitative, weighted and diffusion images). We then segmented the fetal brainstructures, without anatomical a priori, by using data engineering and automaticclustering algorithms. We have succeeded in designing a proof of concept for automaticsegmentation for the fetal brain and extracting, from our MRI images, groups ofstructures similar in molecular composition and cytoarchitecture.</p>

corrected abstract:
<p>Studying White Matter (WM) lesions in premature babies is a societal challenge, as premature mortality is decreasing while neurological morbidity remains the same. This master thesis is part of the p-HCP (Premature Human Connectome Project), which aims to study these lesions using extreme magnetic field MRI (11.7T) , starting with the construction of a typical ex vivo brain atlas at mesoscopic scale and comparing it, in a second phase, to an atlas of damaged brains. This work presents a segmentation strategy for the fetal brain structures to characterize them at different key stages of the development using MRI images with unprecedented resolution. We first manually segmented the structures of a 20-week gestation brain using a histological atlas, in order to explore the information contained the nineteen MRI modalities available (quantitative, weighted and diffusion images). We then segmented the fetal brain structures, without anatomical a priori, by using data engineering and automatic clustering algorithms. We have succeeded in designing a proof of concept for automatic segmentation for the fetal brain and extracting, from our MRI images, groups of structures similar in molecular composition and cytoarchitecture.</p>
----------------------------------------------------------------------
In diva2:1903464 abstract is:
<p>This thesis presents the theory, process, and results of creating a computational model for creep crackgrowth calculations. The model aims to enable industry customers to better plan their productions byestimating the remaining life affected by creep crack growth. The creep crack growth is modeledusing existing theories based on fracture mechanics, such as stress intensity factors 𝐾𝐼, and the 𝐶∗parameter. Replica testing was explored as the assumed method for identifying creep defects. Thedifficulty of where to place the replicas due to creep phenomena changing the critical locationscompared to elastic analyses is also discussed.</p><p>The model was developed using Python, the model allows users to input parameters such as geometry,material properties, load data, and settings. These settings include calculating creep crack growth forvarying depth-to-length ratios, applying different stress states at different points of the crack frontand whether R6 defect assessment analysis should be performed. The model is heavily based on theSwedish handbook SSM2018:18 but also relies on the British standard BS7910 and has been partiallyvalidated by comparing its results with those from commercially available software. Additionally, thesensitivity of the input parameters is analysed and presented.</p>

corrected abstract:
<p>This thesis presents the theory, process, and results of creating a computational model for creep crack growth calculations. The model aims to enable industry customers to better plan their productions by estimating the remaining life affected by creep crack growth. The creep crack growth is modeled using existing theories based on fracture mechanics, such as stress intensity factors 𝐾<sub>𝐼</sub>, and the 𝐶<sup>∗</sup> parameter. Replica testing was explored as the assumed method for identifying creep defects. The difficulty of where to place the replicas due to creep phenomena changing the critical locations compared to elastic analyses is also discussed.</p><p>The model was developed using Python, the model allows users to input parameters such as geometry, material properties, load data, and settings. These settings include calculating creep crack growth for varying depth-to-length ratios, applying different stress states at different points of the crack front and whether R6 defect assessment analysis should be performed. The model is heavily based on the Swedish handbook SSM2018:18 but also relies on the British standard BS7910 and has been partially validated by comparing its results with those from commercially available software. Additionally, the sensitivity of the input parameters is analysed and presented.</p>

Note that this abstract uses characters from Unicode's Mathematical Alphanumeric Symbols block.
----------------------------------------------------------------------
In diva2:1900919 abstract is:
<p>The subject of this thesis is the analysis and optimisation of cable harness variantsin Scania trucks. The growth in complexity of electrical systems on vehicles hasresulted in a number of different forms and lengths of their cable harnesses. Thecable harnesses of a truck consists of a numerous cable routings and for each truckvariant there are different cable routings. Selecting these for the formation of acomplete cable harness is currently a manual task and this thesis is an effort toaddress this issue.For the purpose of solving this problem, a method and a conceptual tool werecreated in order to establish an efficient procedure for cable routing selection. Thisis based on existing cable routing layouts in Scania’s design environment of SaberHarness/ Catia V5. The method exploits the tree in a 3D environment and usesDijkstra’s algorithm which is used to find the shortest path between node points.Findings prove that the proposed tool helps to decrease the time and human effortneeded for selecting the necessary cable routings for a cable harness. This improvesthe quality of the product and also optimises the number of length variants.</p>

corrected abstract:
<p>The subject of this thesis is the analysis and optimisation of cable harness variants in Scania trucks. The growth in complexity of electrical systems on vehicles has resulted in a number of different forms and lengths of their cable harnesses. The cable harnesses of a truck consists of a numerous cable routings and for each truck variant there are different cable routings. Selecting these for the formation of a complete cable harness is currently a manual task and this thesis is an effort to address this issue.</p><p>For the purpose of solving this problem, a method and a conceptual tool were created in order to establish an efficient procedure for cable routing selection. This is based on existing cable routing layouts in Scania’s design environment of Saber Harness/ Catia V5. The method exploits the tree in a 3D environment and uses Dijkstra’s algorithm which is used to find the shortest path between node points.</p><p>Findings prove that the proposed tool helps to decrease the time and human effort needed for selecting the necessary cable routings for a cable harness. This improves the quality of the product and also optimises the number of length variants.</p>
----------------------------------------------------------------------
In diva2:1896438 abstract is:
<p>Context. Over the last two decades, space-based photometric missions such asCoRot, Kepler/K2, and TESS have provided a wealth of photometric data of solar-like stars. Several methods have been used to extract the surface rotation period,using either the Lomb-Scargle periodogram, the autocorrelation function, wavelettransforms or Gaussian processes.Aims. We propose to evaluate the efficiency for measuring rotation period of arecently proposed method, called the Gradient Power Spectrum (GPS).Methods. The GPS is calculated in two steps. First, we calculate the globalwavelet power spectrum (GWPS) from the time series. Then the gradient is cal-culated from the logarithmic derivative of the GWPS. The maximum of the GPS in-dicates the position of the GWPS inflection point. Rotation period can then be foundby multiplying the inflection period by an empirically- constrained calibration fac-tor.Results. Through our analyzes it appeared that this method was strongly depen-dent on the stellar inclination. We decided to use this feature to feed a supervisedmachine learning algorithm to predict stellar inclination from the lightcurve.Conclusion. The GPS method is not reliable alone to measure surface rotation.However for inclination our results are promising. We retrieve almost 90% of sim-ulated stars inclination at ± 20°. For Kepler stars, we currently retrieve only 57.1%of their inclination compared to asteroseismic measurements. However the sampleused was only about 56 oscillating solar-like stars and their asteroseismic uncertain-ties are significant.</p>

corrected abstract:
<p><u>Context</u>. Over the last two decades, space-based photometric missions such as CoRot, Kepler/K2, and TESS have provided a wealth of photometric data of solar-like stars. Several methods have been used to extract the surface rotation period, using either the Lomb-Scargle periodogram, the autocorrelation function, wavelet transforms or Gaussian processes.</p><p><u>Aims</u>. We propose to evaluate the efficiency for measuring rotation period of a recently proposed method, called the Gradient Power Spectrum (GPS).</p><p><u>Methods</u>. The GPS is calculated in two steps. First, we calculate the global wavelet power spectrum (GWPS) from the time series. Then the gradient is calculated from the logarithmic derivative of the GWPS. The maximum of the GPS indicates the position of the GWPS inflection point. Rotation period can then be found by multiplying the inflection period by an empirically- constrained calibration factor.</p><p><u>Results</u>. Through our analyzes it appeared that this method was strongly dependent on the stellar inclination. We decided to use this feature to feed a supervised machine learning algorithm to predict stellar inclination from the lightcurve.</p><p><u>Conclusion</u>. The GPS method is not reliable alone to measure surface rotation.<br>However for inclination our results are promising. We retrieve almost 90% of simulated stars inclination at ± 20°. For Kepler stars, we currently retrieve only 57.1% of their inclination compared to asteroseismic measurements. However the sample used was only about 56 oscillating solar-like stars and their asteroseismic uncertainties are significant.</p>
----------------------------------------------------------------------
In diva2:1896340 abstract is:
<p>Proper orthogonal decomposition (POD) has been widely used to extract modes fromflow fields based on spatial and temporal correlations and ordered by energy, and canbe used to extract coherent structures in turbulence. Each POD time coefficient formsa time series where the samples are auto-correlated. Moreover, the time series ofdifferent coefficients are cross-correlated. Based on the above mentioned correlations,the present work develops a vector auto-regression (VAR) method to model thetime coefficients and predict them. Therefore, the flow field could be predicted byreconstruction using the original spatial modes and predicted time coefficients. As anexample, turbulent boundary layer flow over a flat plate Reθ = 790 is investigated outof the motivation of inflow generation. Prior to the modelling, the correlation betweenthe POD time coefficients is studied; energy transitions among the POD modes areinvestigated; and the modelling strategy is designed to capture such correlations. Asa result, the probability density function and correlations of POD time coefficients arepreserved. Moreover, dominant Reynolds stresses and power spectral density are alsoshown to be generally consistent with the original flow. Motivated by the findings, thepossibility of applying the method to higher Reynolds-numbers is also studied.</p>

corrected abstract:
<p>Proper orthogonal decomposition (POD) has been widely used to extract modes from flow fields based on spatial and temporal correlations and ordered by energy, and can be used to extract coherent structures in turbulence. Each POD time coefficient forms a time series where the samples are auto-correlated. Moreover, the time series of different coefficients are cross-correlated. Based on the above mentioned correlations, the present work develops a vector auto-regression (VAR) method to model the time coefficients and predict them. Therefore, the flow field could be predicted by reconstruction using the original spatial modes and predicted time coefficients. As an example, turbulent boundary layer flow over a flat plate Re<sub>θ</sub> = 790 is investigated out of the motivation of inflow generation. Prior to the modelling, the correlation between the POD time coefficients is studied; energy transitions among the POD modes are investigated; and the modelling strategy is designed to capture such correlations. As a result, the probability density function and correlations of POD time coefficients are preserved. Moreover, dominant Reynolds stresses and power spectral density are also shown to be generally consistent with the original flow. Motivated by the findings, the possibility of applying the method to higher Reynolds-numbers is also studied.</p>
----------------------------------------------------------------------
In diva2:1888131 abstract is:
<p>Two-phase flows occur in many industrial applications, including light-water nuclear reactors,where accurate modeling of interactions between the liquid and vapor phases is crucial for safetyand efficiency.In this project, a simplified, one-dimensional, two-fluid model, consisting of mass, energyand momentum conservation equations for the liquid and vapor phases each, is derived andimplemented. Due to its simplicity and ease of use, the model is suitable for educationalpurposes and early-stage research, particularly for model development based on separate effectstests with simple geometries.The implemented code solves the six field equations in combination with simplified, yet easilymodifiable, constitutive equations. The current model can account for thermal non-equilibria,such as sub-cooled and post-CHF boiling, as well as interfacial momentum coupling betweenthe liquid and vapor phases. The systems code TRACE predicts comparable results for verticalupward flow, despite the numerous assumptions made in the simplified model.</p>

corrected abstract:
<p>Two-phase flows occur in many industrial applications, including light-water nuclear reactors, where accurate modeling of interactions between the liquid and vapor phases is crucial for safety and efficiency.</p><p>In this project, a simplified, one-dimensional, two-fluid model, consisting of mass, energy and momentum conservation equations for the liquid and vapor phases each, is derived and implemented. Due to its simplicity and ease of use, the model is suitable for educational purposes and early-stage research, particularly for model development based on separate effects tests with simple geometries.</p><p>The implemented code solves the six field equations in combination with simplified, yet easily modifiable, constitutive equations. The current model can account for thermal non-equilibria, such as sub-cooled and post-CHF boiling, as well as interfacial momentum coupling between the liquid and vapor phases. The systems code TRACE predicts comparable results for vertical upward flow, despite the numerous assumptions made in the simplified model.</p>
----------------------------------------------------------------------
In diva2:1880347 abstract is:
<p>By placing millions of space sunshades, of the order of 104 m2 at the sub-Lagrangian point L1',between the sun and Earth, solar radiation can be reduced enough to achieve the necessary temper-ature reduction to enable a slow down of the global warming. The vast amount of space sunshadesposes significant challenges on the communication system, as the probability of interference, whichcan distort information, increases with the number of simultaneously communicating units.This thesis aims to design a potential structure for the communication system that minimizesinterference as much as possible. To reduce the number of simultaneously communicating units, thesunshades are arranged in cell formation, where a mother is placed in the center with daughtersaround that only communicate with their specific cell mother. Direct communication betweenthe Earth and space sunshades is not possible as the interference from solar radiation can causesignificant distortion on the signals. Therefore, relay satellites are placed in orbit around thesub-Lagrangian point L1' at a sufficient distance to avoid the effects of solar radiation. Thus, thecommunication between the mothers and Earth is instead routed via the relay satellites. Sincecommunication between such a large number of entities in space has not been investigated before,this approach could provide a possible basic design framework for designing such infrastructure inthe future.</p>

corrected abstract:
<p>By placing millions of space sunshades, of the order of 104 m2 at the sub-Lagrangian point L1', between the sun and Earth, solar radiation can be reduced enough to achieve the necessary temper-ature reduction to enable a slow down of the global warming. The vast amount of space sunshades poses significant challenges on the communication system, as the probability of interference, which can distort information, increases with the number of simultaneously communicating units. This thesis aims to design a potential structure for the communication system that minimizes interference as much as possible. To reduce the number of simultaneously communicating units, the sunshades are arranged in cell formation, where a mother is placed in the center with daughters around that only communicate with their specific cell mother. Direct communication between the Earth and space sunshades is not possible as the interference from solar radiation can cause significant distortion on the signals. Therefore, relay satellites are placed in orbit around the sub-Lagrangian point L1' at a sufficient distance to avoid the effects of solar radiation. Thus, the communication between the mothers and Earth is instead routed via the relay satellites. Since communication between such a large number of entities in space has not been investigated before, this approach could provide a possible basic design framework for designing such infrastructure inthe future.</p>
----------------------------------------------------------------------
In diva2:1878787 abstract is:
<p>Reinforcement learning has emerged as a powerful paradigm in machinelearning, witnessing remarkable progress in recent years. Amongreinforcement algorithms, Q-learning stands out, enabling agents tolearn quickly from past actions. This study aims to investigate andenhance Q-learning methodologies, with a specific focus on tabularQ-learning. In particular, it addresses Q-learning with an actionspace containing actions that require different amounts of time toexecute. With such an action space the algorithm might convergeto a suboptimal solution when using a constant discount factor sincediscounting occurs per action and not per time step. We refer to thisissue as the non-temporal discounting (NTD) problem. By introducinga time-normalised discounting function, we were able to address theissue of NTD. In addition, we were able to stabilise the solutionby implementing a cost for specific actions. As a result, the modelconverged to the expected solution. Building on these results it wouldbe wise to implement time-normalised discounting in a state-of-the-artreinforcement learning model such as deep Q-learning.</p><p> </p>

corrected abstract:
<p>Reinforcement learning has emerged as a powerful paradigm in machine learning, witnessing remarkable progress in recent years. Among reinforcement algorithms, Q-learning stands out, enabling agents to learn quickly from past actions. This study aims to investigate and enhance Q-learning methodologies, with a specific focus on tabular Q-learning. In particular, it addresses Q-learning with an action space containing actions that require different amounts of time to execute. With such an action space the algorithm might converge to a suboptimal solution when using a constant discount factor since discounting occurs per action and not per time step. We refer to this issue as the non-temporal discounting (NTD) problem. By introducing a time-normalised discounting function, we were able to address the issue of NTD. In addition, we were able to stabilise the solution by implementing a cost for specific actions. As a result, the model converged to the expected solution. Building on these results it would be wise to implement time-normalised discounting in a state-of-the-art reinforcement learning model such as deep Q-learning.</p>
----------------------------------------------------------------------
In diva2:1871461 abstract is:
<p>The aerospace sector’s increasing use of composites highlights the need for accuratecharacterization of the material’s properties, particularly regarding failure parameters.This paper goes through the process of physically testing a carbon epoxy composite interms of delamination strength using the ASTM standard in three different mode mixtures.Then, by implementing a Cohesive Zone Modelling approach with a bi-linear tractionseparation law, simulating the coupons in LS-Dyna and performing an iterative correlationprocess to find the material parameters that best replicate the results obtained from the labexperiments.The results showed how the failure load at which the delamination initiates matched theexperimental ones for reasonable values of the material properties, but the displacementat which this load were obtained are noticeably off.In the end, the reason behind this behaviour is discussed and possible solutions forfuture work are presented.</p>


corrected abstract:
<p>The aerospace sector’s increasing use of composites highlights the need for accurate characterization of the material’s properties, particularly regarding failure parameters.</p><p>This paper goes through the process of physically testing a carbon epoxy composite in terms of delamination strength using the ASTM standard in three different mode mixtures. Then, by implementing a Cohesive Zone Modelling approach with a bi-linear traction separation law, simulating the coupons in LS-Dyna and performing an iterative correlation process to find the material parameters that best replicate the results obtained from the lab experiments.</p><p>The results showed how the failure load at which the delamination initiates matched the experimental ones for reasonable values of the material properties, but the displacement at which this load were obtained are noticeably off.</p><p>In the end, the reason behind this behaviour is discussed and possible solutions for future work are presented.</p>
----------------------------------------------------------------------
In diva2:1833735 abstract is:
<p>This thesis is a case study in collaboration with SJ AB, a government owned railway companyin Sweden. The employees aboard the trains are an essential part of operating thetrains efficiently. Therefore, it is vital to forecast absences well in order to avoid havingto cancel train trips or having employees work over time. The current process SJ usesdivides the total amount of absences into 11 categories representing reasons for not beingpresent. This is done three months in advance, but the model is not based on mathematics.This study is going to examine how well the forecasts compare to reality in addition toinvestigating which variables are possible to estimate using regression analysis. Furthermore,the extent to which the staff on board the trains are affected will be investigatedin terms of having to work less overtime. The financial impact of an enhanced model willbe researched. “Free” days, Vacation and Sickness all have significant regressors and canpotentially be forecast using regression analysis. Future work includes finding more potentialregressor variables that could be significant for more response variables in addition tousing the results of this thesis in an actual estimation model for the total absence.</p>

corrected abstract:
<p>This thesis is a case study in collaboration with SJ AB, a government owned railway company in Sweden. The employees aboard the trains are an essential part of operating the trains efficiently. Therefore, it is vital to forecast absences well in order to avoid having to cancel train trips or having employees work over time. The current process SJ uses divides the total amount of absences into 11 categories representing reasons for not being present. This is done three months in advance, but the model is not based on mathematics. This study is going to examine how well the forecasts compare to reality in addition to investigating which variables are possible to estimate using regression analysis. Furthermore, the extent to which the staff on board the trains are affected will be investigated in terms of having to work less overtime. The financial impact of an enhanced model will be researched. “Free” days, Vacation and Sickness all have significant regressors and can potentially be forecast using regression analysis. Future work includes finding more potential regressor variables that could be significant for more response variables in addition to using the results of this thesis in an actual estimation model for the total absence.</p>
----------------------------------------------------------------------
In diva2:1830907 abstract is:
<p>The purpose of this study is to perform computational fluid dynamics (CFD) simulationfor an incompressible flow around an airfoil and investigate the aerodynamicproperties for different Reynolds numbers (400, 000 and 1, 000, 000) based on the chordlength, at the moderate angle of attack (AoA) of 5o . For this purpose a detailedevaluation of wall-resolved and wall-modelled large-eddy simulations (LES) wereconducted for the NACA4412 wing profile. The simulations were carried out with theopen-source CFD software OpenFOAM. The meshing of the computational domainwas constructed with the commercial software (student version) Ansys Icem while forthe post-processing of the different cases, a python package turbulucid was utilized.The results show good agreement compared to data and the aerodynamic properties ofthe flow captured accurately. However, for the case of the wall-modelled LES furtherinvestigation is required and is proposed for future campaigns.</p>

corrected abstract:
<p>The purpose of this study is to perform computational fluid dynamics (CFD) simulation for an incompressible flow around an airfoil and investigate the aerodynamic properties for different Reynolds numbers (400,000 and 1,000,000) based on the chord length, at the moderate angle of attack (AoA) of 5&deg;. For this purpose a detailed evaluation of wall-resolved and wall-modelled large-eddy simulations (LES) were conducted for the NACA4412 wing profile. The simulations were carried out with the open-source CFD software OpenFOAM. The meshing of the computational domain was constructed with the commercial software (student version) Ansys Icem while for the post-processing of the different cases, a python package turbulucid was utilized. The results show good agreement compared to data and the aerodynamic properties of the flow captured accurately. However, for the case of the wall-modelled LES further investigation is required and is proposed for future campaigns.</p>
----------------------------------------------------------------------
In diva2:1817109 - Note: no full text in DiVA
abstract is:
<p>A comparison between several numerical finite element methods to model progressive failurein composite materials is performed. The focus of the study is on the development of an efficientmethod to model crack growth phenomena in large-scale thermoplastic composite laminates.The analyses are performed in the commercial software Abaqus and include three discrete crackmethods and one smeared crack method, which are applied to model coupon tensile tests. Thethree discrete crack methods involve cohesive elements, cohesive contact properties, and theextended finite element method. The smeared crack method uses the Hashin damage initiationcriterion as already implemented in the software. The numerical results were compared bothto numerical and experimental references. A method to represent multiple failure modesin a single fracture surface is used both with cohesive elements and with a cohesive contactproperty. The cohesive contact method results in being the most efficient. To achieve a moreaccurate prediction of the damage evolution and of the final failure, a method using multiplesurfaces of cohesive elements is tested. A technique to scale the mesh by reducing the initiationstrength is evaluated. Both these last two studies show that more in-depth work is required tosuccessfully apply these methods.</p>

corrected abstract:
<p>A comparison between several numerical finite element methods to model progressive failure in composite materials is performed. The focus of the study is on the development of an efficient method to model crack growth phenomena in large-scale thermoplastic composite laminates. The analyses are performed in the commercial software Abaqus and include three discrete crack methods and one smeared crack method, which are applied to model coupon tensile tests. The three discrete crack methods involve cohesive elements, cohesive contact properties, and the extended finite element method. The smeared crack method uses the Hashin damage initiation criterion as already implemented in the software. The numerical results were compared both to numerical and experimental references. A method to represent multiple failure modes in a single fracture surface is used both with cohesive elements and with a cohesive contact property. The cohesive contact method results in being the most efficient. To achieve a more accurate prediction of the damage evolution and of the final failure, a method using multiple surfaces of cohesive elements is tested. A technique to scale the mesh by reducing the initiation strength is evaluated. Both these last two studies show that more in-depth work is required to successfully apply these methods.</p>
----------------------------------------------------------------------
In diva2:1781274 abstract is:
<p>This project aimed to evaluate the effectiveness of the Auto Regressive Exogenous(ARX) model in forecasting stock prices and contribute to research on statisticalmodels in predicting stock prices. An ARX model is a type of linear regression modelused in time series analysis to forecast future values based on past values and externalinput signals. In this study, the ARX model was used to forecast the closing pricesof stocks listed on the OMX Stockholm 30 (OMXS30*) excluding Essity, Evolution,and Sinch, using historical data from 2016-01-01 to 2020-01-01 obtained from YahooFinance.</p><p>The model was trained using the least squares approach with a control signal that filtersoutliers in the data. This was done by modeling the ARX model using optimizationtheory and then solving that optimization problem using Gurobi OptimizationSoftware. Subsequently, the accuracy of the model was tested by predicting prices in aperiod based on past values and the exogenous input variable.</p><p>The results indicated that the ARX model was not suitable for predicting stock priceswhile considering short time periods.</p><p> </p>

corrected abstract:
<p>This project aimed to evaluate the effectiveness of the Auto Regressive Exogenous (ARX) model in forecasting stock prices and contribute to research on statistical models in predicting stock prices. An ARX model is a type of linear regression model used in time series analysis to forecast future values based on past values and external input signals. In this study, the ARX model was used to forecast the closing prices of stocks listed on the OMX Stockholm 30 (OMXS30*) excluding Essity, Evolution, and Sinch, using historical data from 2016-01-01 to 2020-01-01 obtained from Yahoo Finance.</p><p>The model was trained using the least squares approach with a control signal that filters outliers in the data. This was done by modeling the ARX model using optimization theory and then solving that optimization problem using Gurobi Optimization Software. Subsequently, the accuracy of the model was tested by predicting prices in a period based on past values and the exogenous input variable.</p><p>The results indicated that the ARX model was not suitable for predicting stock prices while considering short time periods.</p>
----------------------------------------------------------------------
In diva2:1779355 abstract is:
<p>The main purpose for an aggressive investor is to maximize the return in theinvestments. But in order to do so the risk should be taken into consideration.In this thesis, we utilize Markowitz portfolio theory, one of the standard modelsfor maximizing return while considering risk. The model allows the investorto balance risk tolerance and expected return on the stock market based onhistorical data. Simply put, the goal is to allocate capital to stocks in a mannerthat maximizes expected return while considering risk. The tested cases involvedmodels utilizing historical data from five and ten years ago, respectively. Theresulting allocation distribution of these portfolios depended on the varianceconstraint and, to a large extent, on how many years of historical data were used.These results emphasize the significance of data in portfolio optimization.</p>

corrected abstract:
<p>The main purpose for an aggressive investor is to maximize the return in the investments. But in order to do so the risk should be taken into consideration. In this thesis, we utilize Markowitz portfolio theory, one of the standard models for maximizing return while considering risk. The model allows the investor to balance risk tolerance and expected return on the stock market based on historical data. Simply put, the goal is to allocate capital to stocks in a manner that maximizes expected return while considering risk. The tested cases involved models utilizing historical data from five and ten years ago, respectively. The resulting allocation distribution of these portfolios depended on the variance constraint and, to a large extent, on how many years of historical data were used. These results emphasize the significance of data in portfolio optimization.</p>
----------------------------------------------------------------------
In diva2:1779326 abstract is: <p>This report discusses the accuracy of blink detection in eye tracking, using machine learningalgorithms. Blink detection is used in a wide variety of medicinal and psychological applica-tions such as a controller for motor impaired individuals. Image classification has recentlybeen used in eye tracking and blink detection applications. The blink detection is appliedon data captured from the Pupil Invisible head-mounted eye tracker. The aim is that givenan image, the classifier can accurately determine the state of which the eye is in, blink oropen.These tests will be conducted on two SVM (support vector machine) models using differenttraining data, one trained on data from controlled environments, the other model also trainedon uncontrolled environments. For this project, data was captured in infrared disturbedenvironments to see how it affects the models performance. These models are evaluatedaccording to their accuracy using multiple different metrics. This rapport will discuss theresults of both classifiers in both tests, in addition to describing training methodology withan aim to find if blink detection is viable in infrared disturbed environments.</p>

corrected abstract:
<p>This report discusses the accuracy of blink detection in eye tracking, using machine learning algorithms. Blink detection is used in a wide variety of medicinal and psychological applications such as a controller for motor impaired individuals. Image classification has recently been used in eye tracking and blink detection applications. The blink detection is applied on data captured from the Pupil Invisible head-mounted eye tracker. The aim is that given an image, the classifier can accurately determine the state of which the eye is in, blink or open.</p><p>These tests will be conducted on two SVM (support vector machine) models using different training data, one trained on data from controlled environments, the other model also trained on uncontrolled environments. For this project, data was captured in infrared disturbed environments to see how it affects the models performance. These models are evaluated according to their accuracy using multiple different metrics. This rapport will discuss the results of both classifiers in both tests, in addition to describing training methodology with an aim to find if blink detection is viable in infrared disturbed environments.</p>
----------------------------------------------------------------------
In diva2:1776549 abstract is:
<p>Computed tomography (CT) is a medical imaging technique that usesX-rays to obtain a reconstruction of an object. The term acquisition ge-ometry refers to the arrangement of imaging sensors and the X-ray sourceas well as the procedure used for data collection. The quality of the re-construction is often limited by the acquisition geometry and parametervalues. In this thesis, we present a procedure for fine-tuning acquisitiongeometry parameters in CT by minimizing the difference between theforward projection of a known phantom and measured data, i.e. data dis-crepancies. We extend the ODL library in Python and create acquisitiongeometries where different parameters have been distorted. We utilizegradient descent in an attempt recover the true parameters of the acquisi-tion geometries. Our results show that the recovery of the true geometryis successful when one or, in some cases, two parameters are perturbed.The objective function becomes very sensitive when more parameters areperturbed, requiring a low learning rate and making convergence slow.Nevertheless, we are able to minimize the objective function in the for-ward projection for all perturbations. Although our algorithm performswell in some aspects relating to parameter recovery, there is potential forfurther research by implementing other optimization methods.</p>

corrected abstract:
<p>Computed tomography (CT) is a medical imaging technique that uses X-rays to obtain a reconstruction of an object. The term acquisition geometry refers to the arrangement of imaging sensors and the X-ray source as well as the procedure used for data collection. The quality of the reconstruction is often limited by the acquisition geometry and parameter values. In this thesis, we present a procedure for fine-tuning acquisition geometry parameters in CT by minimizing the difference between the forward projection of a known phantom and measured data, i.e. data discrepancies. We extend the ODL library in Python and create acquisition geometries where different parameters have been distorted. We utilize gradient descent in an attempt recover the true parameters of the acquisition geometries. Our results show that the recovery of the true geometry is successful when one or, in some cases, two parameters are perturbed. The objective function becomes very sensitive when more parameters are perturbed, requiring a low learning rate and making convergence slow. Nevertheless, we are able to minimize the objective function in the forward projection for all perturbations. Although our algorithm performs well in some aspects relating to parameter recovery, there is potential for further research by implementing other optimization methods.</p>
----------------------------------------------------------------------
In diva2:1768469 abstract is:
<p>Does the active membrane transporter, Na+,K+-ATPase dimerize? If it does, whatis the functional benefit? Does it increase or decrease the turnover rate? Theseare still unanswered questions and current research topics. Previous studies havedemonstrated dimerizations in closely related proteins of the P-type ATPase family.For the Na+, K+-ATPase a first indication of dimerization has been shown viaFluorescence lifetime imaging microscopy (FLIM) or Fluorescence resonance energytransfer - Fluorescence correlation spectroscopy (FRET-FCS) experiments. Theprecise dimer structure, dimerization process, and its ultimate functional effecthowever, remain to be found. This master thesis approaches those questions froma co-evolutionary standpoint. It predicts a possible dimer structure by starting with amultiple sequence alignment, direct coupling analysis, and structural contact filteringalgorithm. This model would strengthen the dimerization model of a decreasedturnover rate due to a competitive behavior of two Na+, K+-ATPases for its energysource ATP.</p>

corrected abstract:
<p>Does the active membrane transporter, Na<sup>+</sup>,K<sup>+</sup>-ATPase dimerize? If it does, what is the functional benefit? Does it increase or decrease the turnover rate? These are still unanswered questions and current research topics. Previous studies have demonstrated dimerizations in closely related proteins of the P-type ATPase family. For the Na<sup>+</sup>, K<sup>+</sup>-ATPase a first indication of dimerization has been shown via Fluorescence lifetime imaging microscopy (FLIM) or Fluorescence resonance energy transfer - Fluorescence correlation spectroscopy (FRET-FCS) experiments. The precise dimer structure, dimerization process, and its ultimate functional effect however, remain to be found. This master thesis approaches those questions from a co-evolutionary standpoint. It predicts a possible dimer structure by starting with a multiple sequence alignment, direct coupling analysis, and structural contact filtering algorithm. This model would strengthen the dimerization model of a decreased turnover rate due to a competitive behavior of two Na<sup>+</sup>, K<sup>+</sup>-ATPases for its energy source ATP.</p>
----------------------------------------------------------------------
In diva2:1739683 abstract is:
<p>An aircraft is exposed for a certain risk during warhead deployment. If the distance is too short between the aircraftand warhead during burst, fragments may hit the aircraft with fatal consequences. The warhead consists not onlyof fragments created during burst but also parts to attach it to the aircraft such as mounting plates and fasteningloops. These parts have a significantly larger mass than natural fragments and may travel far during trajectory.The problem for a potential hit of fragment on a fighter jet after its warhead has detonated has been presentsince several decades. It is of interest to analyse the problem to effectively reduce the warheads arming time andthe aircraft’s altitude during warhead deployment. The complexity consists of how the mounting plates and fas-tening loops behave during trajectory, which may affect the travelled distance if they rotate or tumble. Attemptsto solve this problem for the Gripen fighter jet has been made by Staffan Harling at FOI which this thesis is asubsequent work.</p><p>This thesis treats a risk perspective analyse of the distance between the aircraft and warhead named range safetydistance. Travelled distances for fragments are calculated with variation in velocity, drag coefficient and ejectionangle to analyse the problem to a wider extent.</p><p>The conclusion states that the time from warhead deployed until it burst should be at least seven seconds. Genericdata has been used in this master thesis due to classified information concerning real cases. Focus has been onthe method and to develop a Matlab code that hopefully can be used to estimate range safety distances from aappropriate risk perspective.</p>

corrected abstract:
<p>An aircraft is exposed for a certain risk during warhead deployment. If the distance is too short between the aircraft and warhead during burst, fragments may hit the aircraft with fatal consequences. The warhead consists not only of fragments created during burst but also parts to attach it to the aircraft such as mounting plates and fastening loops. These parts have a significantly larger mass than natural fragments and may travel far during trajectory.</p><p>The problem for a potential hit of fragment on a fighter jet after its warhead has detonated has been present since several decades. It is of interest to analyse the problem to effectively reduce the warheads arming time and the aircraft’s altitude during warhead deployment. The complexity consists of how the mounting plates and fastening loops behave during trajectory, which may affect the travelled distance if they rotate or tumble. Attempts to solve this problem for the Gripen fighter jet has been made by Staffan Harling at FOI which this thesis is a subsequent work.</p><p>This thesis treats a risk perspective analyse of the distance between the aircraft and warhead named range safety distance. Travelled distances for fragments are calculated with variation in velocity, drag coefficient and ejection angle to analyse the problem to a wider extent.</p><p>The conclusion states that the time from warhead deployed until it burst should be at least seven seconds. Generic data has been used in this master thesis due to classified information concerning real cases. Focus has been on the method and to develop a Matlab code that hopefully can be used to estimate range safety distances from a appropriate risk perspective.</p>
----------------------------------------------------------------------
In diva2:1678918 abstract is:
<p>The study of the instabilities in boiling water reactors is of significant importance to the safety withwhich they can be operated, as they can cause damage to the reactor posing risks to both equipmentand personnel. The instabilities that concern this paper are progressive growths in the oscillatingpower of boiling-water reactors. As thermal power is oscillatory is important to be able to identifywhether or not the power amplitude is stable.</p><p>The main focus of this paper has been the development of a neural network estimator of these insta-bilities, fitting a non-linear model function to data by estimating it’s parameters. In doing this, theambition was to optimize the networks to the point that it can deliver near ”best-guess” estimationsof the parameters which define these instabilities, evaluating the usefulness of these networks whenapplied to problems like this.</p><p>The goal was to design both MLP(Multi-Layer Perceptron) and SVR/KRR(Support Vector Regres-sion/Kernel Rigde Regression) networks and improve them to the point that they provide reliableand useful information about the waves in question. This goal was accomplished only in part asthe SVR/KRR networks proved to have some difficulty in ascertaining the phase shift of the waves.Overall, however, these networks prove very useful in this kind of task, succeeding with a reasonabledegree of confidence to calculating the different parameters of the waves studied.</p><p> </p>

corrected abstract:
<p>The study of the instabilities in boiling water reactors is of significant importance to the safety with which they can be operated, as they can cause damage to the reactor posing risks to both equipment and personnel. The instabilities that concern this paper are progressive growths in the oscillating power of boiling-water reactors. As thermal power is oscillatory is important to be able to identify whether or not the power amplitude is stable.</p><p>The main focus of this paper has been the development of a neural network estimator of these instabilities, fitting a non-linear model function to data by estimating it’s parameters. In doing this, the ambition was to optimize the networks to the point that it can deliver near ”best-guess” estimations of the parameters which define these instabilities, evaluating the usefulness of these networks when applied to problems like this.</p><p>The goal was to design both MLP(Multi-Layer Perceptron) and SVR/KRR(Support Vector Regression/Kernel Rigde Regression) networks and improve them to the point that they provide reliable and useful information about the waves in question. This goal was accomplished only in part as the SVR/KRR networks proved to have some difficulty in ascertaining the phase shift of the waves. Overall, however, these networks prove very useful in this kind of task, succeeding with a reasonable degree of confidence to calculating the different parameters of the waves studied.</p>

Note that there are no space before the opening left parenthesis for the expanded acronyms. Note also that "Rigde" should be spelled "Ridge" (as it is in the body of the thesis); however, it is misspelled in the original thesis abstracts (both the English and Swedish abstracts). Additionally, "it's" is incorrect, and should be "its" - as the latter is the posive form; however, there error is in the original abstract.
----------------------------------------------------------------------
In diva2:1678908 abstract is:
<p>Gamma-ray bursts (GRBs) are the most luminous phenomena in the Universe, explosions whoseenergy is generated by supernovae or mergers of dense objects such as neutron stars. The GRBemission is divided into the prompt emission phase characterized by γ-ray radiation and the afterglowof lower energy radiation. The prompt emission phase is still not understood; as of now, there aretwo leading descriptions: the photospheric- and the synchrotron models. The synchrotron model hashad great success in describing GRB spectra, and specifically some of the brightest ones, although notwithout issues such as some observations being at odds with theory. On the other hand, photosphericmodels have had problems too of how to broaden the spectrum in order to explain the observeddata. One explanation for this broadening is that Radiation Mediated Shocks (RMSs) dissipate energybelow the photosphere. In this report, a time resolved spectral analysis of the prompt emission of GRB160625B – a very bright GRB known to produce synchrotron-like emission – is done. Komrad is animplementation of the Kompaneets RMS Approximation (KRA), which is a dissipative photosphericmodel. Komrad is then used to fit a photospheric model to the prompt emission of GRB 160625Bin order to explore whether photospheric models can account for synchrotron-like emission spectra.Great statistical support is found for the photospheric model in comparison to standard GRB fittingfunctions as well as a synchrotron function which is indicative of the photospheric model being able toexplain a synchrotron-like spectra.</p>

corrected abstract:
<p>Gamma-ray bursts (GRBs) are the most luminous phenomena in the Universe, explosions whose energy is generated by supernovae or mergers of dense objects such as neutron stars. The GRB emission is divided into the prompt emission phase characterized by γ-ray radiation and the afterglow of lower energy radiation. The prompt emission phase is still not understood; as of now, there are two leading descriptions: the photospheric- and the synchrotron models. The synchrotron model has had great success in describing GRB spectra, and specifically some of the brightest ones, although not without issues such as some observations being at odds with theory. On the other hand, photospheric models have had problems too of how to broaden the spectrum in order to explain the observed data. One explanation for this broadening is that Radiation Mediated Shocks (RMSs) dissipate energy below the photosphere. In this report, a time resolved spectral analysis of the prompt emission of GRB 160625B – a very bright GRB known to produce synchrotron-like emission – is done. Komrad is an implementation of the Kompaneets RMS Approximation (KRA), which is a dissipative photospheric model. <tt>Komrad</tt> is then used to fit a photospheric model to the prompt emission of GRB 160625B in order to explore whether photospheric models can account for synchrotron-like emission spectra. Great statistical support is found for the photospheric model in comparison to standard GRB fitting functions as well as a synchrotron function which is indicative of the photospheric model being able to explain a synchrotron-like spectra.</p>
----------------------------------------------------------------------
In diva2:1678462 abstract is:
<p>Cavitation has been known for a long time to cause damages in spillways athydropower plants, aerators are therefore often implemented to prevent this.The majority of the hydropower plants in Sweden are installed in the northernpart of the country. In this environment, construction details like vents areexposed to rain, snow, leaves, and other difficult nature conditions for a majorpart of the year. It is therefore of interest to see what will happen if the ventsare sealed.Similar experiments have earlier been investigated at Vattenfall AB, oneof the largest energy companies in Sweden. The investigation consists ofcase studies with multiple variables, closed and open vents, low and highwater levels. The calculations are done with help from computational fluiddynamics and the goal is to see how parameters such as pressure, spreading,and horizontal length change when the aerator is sealed compared to open.The calculations were also carried out in a way so tests in the future can bedone by Vattenfall to validate the results from the computational simulations.The geometry of the hydropower plant is taken from a plant in Skellefteälvenin northern Sweden.</p>

corrected abstract:
<p>Cavitation has been known for a long time to cause damages in spillways at hydropower plants, aerators are therefore often implemented to prevent this. The majority of the hydropower plants in Sweden are installed in the northern part of the country. In this environment, construction details like vents are exposed to rain, snow, leaves, and other difficult nature conditions for a major part of the year. It is therefore of interest to see what will happen if the vents are sealed.</p><p>Similar experiments have earlier been investigated at Vattenfall AB, one of the largest energy companies in Sweden. The investigation consists of case studies with multiple variables, closed and open vents, low and high water levels. The calculations are done with help from computational fluid dynamics and the goal is to see how parameters such as pressure, spreading, and horizontal length change when the aerator is sealed compared to open. The calculations were also carried out in a way so tests in the future can be done by Vattenfall to validate the results from the computational simulations. The geometry of the hydropower plant is taken from a plant in Skellefteälven in northern Sweden.</p>
----------------------------------------------------------------------
In diva2:1548349 abstract is:
<p>Computational Fluid Dynamics is a powerful and widely used tool for developing projectsthat concern flow motion, in very different fields. Industrial CFD solvers are continuouslydeveloped with the aim of improving accuracy and reducing the computational cost of thesimulations. Turbulent wall-flow cases are particular demanding as the presence of a solidsurfaceinterface generates steep gradients in the proximity of the wall. Resolving suchgradients can be crucial to obtain a consistent solution but also very expensive in terms ofgrid refinement, and hence computational time. Wall functions are widely used and offersignificant computational savings when it comes to near-wall flow resolution. Previous wallfunction implemented in the M-Edge solver suffered by poor performances in complex flowscharacterized by strong pressure-gradient phenomena, such as separation. A new formulationhas been developed and validated for k − omega and Spalart-Allmaras turbulence models. Testsimulations started from simple and near-ideal cases (2D zero pressure gradient flat plate)and advanced to always more complex flow cases and geometries (full 3D general fighter).Every case has been run coupling the wall-function boundary condition with three differentturbulence models: the Menter SST, the Menter BSL with an EARSM and the Spalart-Allmaras one-equation model. Overall results showed the upgraded performance of new wallfunction in flow resolution together with more agile grid requirements, faster and deeperconvergence of the residuals and a general reduction in computational time.</p>

corrected abstract:
<p>Computational Fluid Dynamics is a powerful and widely used tool for developing projects that concern flow motion, in very different fields. Industrial CFD solvers are continuously developed with the aim of improving accuracy and reducing the computational cost of the simulations. Turbulent wall-flow cases are particular demanding as the presence of a solid surface interface generates steep gradients in the proximity of the wall. Resolving such gradients can be crucial to obtain a consistent solution but also very expensive in terms of grid refinement, and hence computational time. Wall functions are widely used and offer significant computational savings when it comes to near-wall flow resolution. Previous wall function implemented in the M-Edge solver suffered by poor performances in complex flows characterized by strong pressure-gradient phenomena, such as separation. A new formulation has been developed and validated for <em>k − &omega;</em> and Spalart-Allmaras turbulence models. Test simulations started from simple and near-ideal cases (2D zero pressure gradient flat plate) and advanced to always more complex flow cases and geometries (full 3D general fighter). Every case has been run coupling the wall-function boundary condition with three different turbulence models: the Menter SST, the Menter BSL with an EARSM and the Spalart-Allmaras one-equation model. Overall results showed the upgraded performance of new wall function in flow resolution together with more agile grid requirements, faster and deeper convergence of the residuals and a general reduction in computational time.</p>
----------------------------------------------------------------------
In diva2:1546615 abstract is:
<p>This thesis work is part of a design process which aims to develop a four-seathybrid-electric aircraft at Smartflyer (Grenchen, Switzerland). In that scope,various mechanisms of the plane had to be developed, including the systemactuating the control surfaces. The objective of this thesis work is to designthe primary flight controls which will be implemented in the first prototypebuilt at Smartflyer.Firstly, the work investigates the calculation of the aerodynamic loads appliedto the control surfaces through the use of three different methods which areanalytical calculations, VLM analysis and CFD simulation. Then, the workconsists in defining the kinematic mechanisms of the flight control to handlethe deflection of the horizontal stabiliser, the ailerons and the rudder. Lastly,the calculation of the forces to which are submitted the components of theflight control is conducted. This step allows to determine the pilot controlforces and ensures to take into account the ergonomic aspect during the designphase. The results of this work highlight the limits of the different methodsused and serves as a basis for a future sizing work and detailed conception.</p>

corrected abstract:
<p>This thesis work is part of a design process which aims to develop a four-seat hybrid-electric aircraft at Smartflyer (Grenchen, Switzerland). In that scope, various mechanisms of the plane had to be developed, including the system actuating the control surfaces. The objective of this thesis work is to design the primary flight controls which will be implemented in the first prototype built at Smartflyer.</p><p>Firstly, the work investigates the calculation of the aerodynamic loads applied to the control surfaces through the use of three different methods which are analytical calculations, VLM analysis and CFD simulation. Then, the work consists in defining the kinematic mechanisms of the flight control to handle the deflection of the horizontal stabiliser, the ailerons and the rudder. Lastly, the calculation of the forces to which are submitted the components of the flight control is conducted. This step allows to determine the pilot control forces and ensures to take into account the ergonomic aspect during the design phase. The results of this work highlight the limits of the different methods used and serves as a basis for a future sizing work and detailed conception.</p>
----------------------------------------------------------------------
In diva2:1528143 abstract is:
<p>Climate change if a fact and the responsibility comes down to the footprintowner. With a possible limitation on carbon oxide emissions and climate demandsfrom customers, a change in the shipping industry is evident. Asolutionto this could be to use a renewable energy created by the nature, the wind, tomake the change over to a climate friendly shipping industry. This study aimsto find a wind powered solution for a car carriers propulsion. What is the mostsuitable rig design?Based on a review of the existing solutions for wind population this thesiswill use a conceptual design method to generate rig designs. The result of thisthesis is two concepts who stuck to be interesting throughout the thesis periodand a catalog of concepts that could be used for further work of finding theoptimal wing solution.</p>

corrected abstract:
<p>Climate change if a fact and the responsibility comes down to the footprint owner. With a possible limitation on carbon oxide emissions and climate demands from customers, a change in the shipping industry is evident. A solution to this could be to use a renewable energy created by the nature, the wind, to make the change over to a climate friendly shipping industry. This study aims to find a wind powered solution for a car carriers propulsion. What is the most suitable rig design?</p><p>Based on a review of the existing solutions for wind population this thesis will use a conceptual design method to generate rig designs. The result of this thesis is two concepts who stuck to be interesting throughout the thesis period and a catalog of concepts that could be used for further work of finding the optimal wing solution.</p>
----------------------------------------------------------------------
In diva2:1464101 abstract is:
<p>In recent years it has been discovered that the window installations on the Visby class corvette are prone to break. The aim of this report is to perform a structural analysis of three different windowinstallations. Thus, to find which window installation minimizes the risk of leaks and cracks in the attachment connecting the window glass and the hull. The three window installations consist of theexisting window installation and one of SAAB Kockums alternative solutions installed in two different ways, with and without an additional damping mass. The model that will be used in the simulationswill consist of a rectangular shaped carbon fibre composite sandwich plate that represents the side structure of the maneuver deck. Furthermore, it will include three windows where only the middlepositioned window will contain the given installation details in order to reduce the complexity of both the modelling and the simulations. Three load cases were simulated in ANSYS Workbench. Thefirst load case called "hogging" consisted of a bending moment along the vertical sides of the model. The second load case called "slamming" consisted of a vertical force pointing upwards along thebottom of the model. Lastly, a torsion load case was simulated. In all load cases the existing window installation were subjected to the largest strains along the inner edges of the attachment. In theslamming load case, the alternative solution without the double damping mass was exposed to least strains around the inner edges of attachment compared to the other window installations. For thehogging and the torsion load case the alternative solution with double damping mass produced least strains around the inner edges of the attachment. But the alternative solution without the doubledamping mass was also able to reduce the strains around the inner edges compared to the existing window installation. In conclusion, the alternative solution without an additional damping mass isin overall minimizing the strains along the inner edges of the attachment in which the most leaks and cracks have been observed. It has especially been efficient in reducing strains in the slammingload case. Even though the window installation with an additional damping mass best withstands hogging and torsion, the slamming load case is the most common scenario. Therefore, the windowinstallation that best withstands the slamming load case should be prioritized. Thus, the alternative window installation without the additional damping mass is the best alternative because it bestwithstands slamming but also reduces the strains along the inner edges compared to the existing window installation in the other load cases.</p>

corrected abstract:
<p>In recent years it has been discovered that the window installations on the Visby class corvette are prone to break. The aim of this report is to perform a structural analysis of three different window installations. Thus, to find which window installation minimizes the risk of leaks and cracks in the attachment connecting the window glass and the hull. The three window installations consist of the existing window installation and one of SAAB Kockums alternative solutions installed in two different ways, with and without an additional damping mass. The model that will be used in the simulations will consist of a rectangular shaped carbon fibre composite sandwich plate that represents the side structure of the maneuver deck. Furthermore, it will include three windows where only the middle positioned window will contain the given installation details in order to reduce the complexity of both the modelling and the simulations. Three load cases were simulated in ANSYS Workbench. The first load case called "hogging" consisted of a bending moment along the vertical sides of the model. The second load case called "slamming" consisted of a vertical force pointing upwards along the bottom of the model. Lastly, a torsion load case was simulated. In all load cases the existing window installation were subjected to the largest strains along the inner edges of the attachment. In the slamming load case, the alternative solution without the double damping mass was exposed to least strains around the inner edges of attachment compared to the other window installations. For the hogging and the torsion load case the alternative solution with double damping mass produced least strains around the inner edges of the attachment. But the alternative solution without the double damping mass was also able to reduce the strains around the inner edges compared to the existing window installation. In conclusion, the alternative solution without an additional damping mass is in overall minimizing the strains along the inner edges of the attachment in which the most leaks and cracks have been observed. It has especially been efficient in reducing strains in the slamming load case. Even though the window installation with an additional damping mass best withstands hogging and torsion, the slamming load case is the most common scenario. Therefore, the window installation that best withstands the slamming load case should be prioritized. Thus, the alternative window installation without the additional damping mass is the best alternative because it best withstands slamming but also reduces the strains along the inner edges compared to the existing window installation in the other load cases.</p>
----------------------------------------------------------------------
In diva2:1334273 abstract is:
<p>Additive Manufacturing (AM), also known as 3D-printing, is the process of joining materials layer by layerfrom a 3D-model data and has several advantages over traditional manufacturing techniques. AM is destinedto change the way products are designed and manufactured in the future. In recent years, the process hasrapidly gained interest in all industry segments due to its ability to create customized and complex geometriesfor no added costs. This study focuses on a rather unexplored area of application of AM, namely of a vehiclecomponent that traditionally is manufactured with conventional manufacturing methods. The purpose ofthis study is to investigate the potential of AM of a PTO-shaft used in Scania's trucks. With the help oftopology optimization and a developed cost estimation model, dierent design cases are compared to eachother. Three areas are investigated: design, mechanical performance, and cost. The study found that adesign with roughly 25 % weight reduction is realizable, and would today cost about 15 times the cost forseries production using traditional manufacturing methods. This have clearly suggested that the PTO-shaftis not suitable for AM. However, by forecasting the cost into the future, the study found that printing thePTO-shaft are likely to be cost eective in terms of prototype production in the future, with up to about200 e in cost savings per part.</p>

corrected abstract:
<p>Additive Manufacturing (AM), also known as 3D-printing, is the process of joining materials layer by layer from a 3D-model data and has several advantages over traditional manufacturing techniques. AM is destined to change the way products are designed and manufactured in the future. In recent years, the process has rapidly gained interest in all industry segments due to its ability to create customized and complex geometries for no added costs. This study focuses on a rather unexplored area of application of AM, namely of a vehicle component that traditionally is manufactured with conventional manufacturing methods. The purpose of this study is to investigate the potential of AM of a PTO-shaft used in Scania's trucks. With the help of topology optimization and a developed cost estimation model, different design cases are compared to each other. Three areas are investigated: design, mechanical performance, and cost. The study found that a design with roughly 25 % weight reduction is realizable, and would today cost about 15 times the cost for series production using traditional manufacturing methods. This have clearly suggested that the PTO-shaft is not suitable for AM. However, by forecasting the cost into the future, the study found that printing the PTO-shaft are likely to be cost effective in terms of prototype production in the future, with up to about 200 € in cost savings per part.</p>
----------------------------------------------------------------------
In diva2:1229892 abstract is:
<p>Lung cancer is one of the most vicious commonplace diseases around. The only wayto efficiently combat lung cancer is to detect tumors early using Computed Tomography(CT). In this procedure, a preliminary scan spots any suspicious tumors and then, aftera significant wait, a follow-up scan is taken to confirm whether the nodule is in fact atumor. Currently a new generation of CT scanners, called Spectral Photon CountingCT scanners, is being developed. These CT scanners offer a variety of improvementsincluding the possibility of reducing the pixel size in detectors. The purpose of this workis to investigate, through computational modeling, if a smaller pixel size combined witha smaller source size would improve the ability to determine lung tumor growth, and assuch reduce the wait time between visits for patients. The results showed that it waspossible to reduce the pixel size without significantly increasing the noise of the imageby using image processing. However, the smaller pixels did not improve the ability todetect tumor growth.</p>

corrected abstract:
<p>Lung cancer is one of the most vicious commonplace diseases around. The only way to efficiently combat lung cancer is to detect tumors early using Computed Tomography(CT). In this procedure, a preliminary scan spots any suspicious tumors and then, after a significant wait, a follow-up scan is taken to confirm whether the nodule is in fact a tumor. Currently a new generation of CT scanners, called Spectral Photon Counting CT scanners, is being developed. These CT scanners offer a variety of improvements including the possibility of reducing the pixel size in detectors. The purpose of this work is to investigate, through computational modeling, if a smaller pixel size combined with a smaller source size would improve the ability to determine lung tumor growth, and as such reduce the wait time between visits for patients. The results showed that it was possible to reduce the pixel size without significantly increasing the noise of the image by using image processing. However, the smaller pixels did not improve the ability to detect tumor growth.</p>
----------------------------------------------------------------------
In diva2:1229885 - Note: no full text in DiVA

abstract is:
<p>In this study, convolutional neural networks (CNNs) are used to classify simulatedimages of ultra high energy cosmic rays (UHECRs). The JEM-EUSO program aimsto place an imaging instrument on board the International Space Station that willdetect UHECRs in Earth’s atmosphere. The instrument will include software andhardware for analyzing the images. There are several different techniques for imageanalysis that could be used in the instrument.Different CNNs are implemented and tested on simulated data. The best-performingnetwork is further analyzed. Results show that CNNs can be used to identify cosmicrays in images. The accuracy is found to be highly dependent on the brightness ofthe cosmic ray tracks compared to the background photon noise. It is also shown thatthe networks are able to identify the angle of the cosmic rays relative to the imageframe. Accuracy is found to decrease with increasing angle resolution, with accuraciesranging from 100% down to pure guesses. More extensive studies with more complexsimulations could generate finer results.</p>

corrected abstract:
<p>In this study, convolutional neural networks (CNNs) are used to classify simulated images of ultra high energy cosmic rays (UHECRs). The JEM-EUSO program aims to place an imaging instrument on board the International Space Station that will detect UHECRs in Earth’s atmosphere. The instrument will include software and hardware for analyzing the images. There are several different techniques for image analysis that could be used in the instrument. Different CNNs are implemented and tested on simulated data. The best-performing network is further analyzed. Results show that CNNs can be used to identify cosmic rays in images. The accuracy is found to be highly dependent on the brightness of the cosmic ray tracks compared to the background photon noise. It is also shown that the networks are able to identify the angle of the cosmic rays relative to the image frame. Accuracy is found to decrease with increasing angle resolution, with accuracies ranging from 100% down to pure guesses. More extensive studies with more complex simulations could generate finer results.</p>
----------------------------------------------------------------------
In diva2:1229832 - Note: no full text in DiVA
abstract is:
<p>Urban transportation systems are a vital part ofeveryday life nowadays. One common, but difficult issue in thisarea is bus bunching which means that buses close in on eachother eventually ending up platooning when in operation. Thisleads to inefficiency and passenger delay. The intent of this reportis to develop a model for mitigating delays in urban transportby reducing bus bunching. Our approach is to have buses selforganizeand focus on maintaining a consistent headway betweenbuses. We propose using Markov chains in the algorithm. Byassigning control points along the bus line that have the abilityto evaluate the location of the buses. This dynamic propertyallows for a quick response to the unpredictability of urban trafficand an increase in effective use of the transportation system.Results show that self-organizing, headway-based control has thepotential to significantly increase efficiency in urban transport.</p>

corrected abstract:
<p>Urban transportation systems are a vital part of everyday life nowadays. One common, but difficult issue in this area is bus bunching which means that buses close in on each other eventually ending up platooning when in operation. This leads to inefficiency and passenger delay. The intent of this report is to develop a model for mitigating delays in urban transport by reducing bus bunching. Our approach is to have buses self organize and focus on maintaining a consistent headway between buses. We propose using Markov chains in the algorithm. By assigning control points along the bus line that have the ability to evaluate the location of the buses. This dynamic property allows for a quick response to the unpredictability of urban traffic and an increase in effective use of the transportation system. Results show that self-organizing, headway-based control has the potential to significantly increase efficiency in urban transport.</p>
----------------------------------------------------------------------
In diva2:1229169 - Note: no full text in DiVA
abstract is:
<p>Since the cost of personnel represents a big part of the budget of a call centre itis interesting to investigate how it can be minimzed via scheduling. Finding theglobal optima can be extremely hard, which make heuristic solutions an interestingalternative in order to nd a satisfactory solution relatively quickly. If the problemis linear it is appropriate to use the simplex method. This report reviews GeneticAlgorithm and the Cross-Entropy method for creating schedules that minimize thecost, and the two methods are then compared. The algorithms are reviewed forlinear problems when simplex is appropriate, and a comparison is made betweenall of the methods. The simplex method is a method where solutions is looked forin the vertex of the dened area, since the optimal solution will be found there.In the Genetic Algorithm a solution set is randomally generated, out of which thebest ones are picked out. The best solutions will in turn be combined in orderto generate a better solution set, and the process is repeated until a satisfactoryresult is reached. The Cross Entropy{method is Monte Carlo-based in which asolution set is generated from a probability density function. The solutions areevaluated and then the probability density function is tweaked in order to makebetter solutions more probable.</p>

corrected abstract:
<p>Since the cost of personnel represents a big part of the budget of a call centre itis interesting to investigate how it can be minimzed via scheduling. Finding the global optima can be extremely hard, which make heuristic solutions an interesting alternative in order to nd a satisfactory solution relatively quickly. If the problem is linear it is appropriate to use the simplex method. This report reviews GeneticAlgorithm and the Cross-Entropy method for creating schedules that minimize the cost, and the two methods are then compared. The algorithms are reviewed for linear problems when simplex is appropriate, and a comparison is made between all of the methods. The simplex method is a method where solutions is looked for in the vertex of the defined area, since the optimal solution will be found there. In the Genetic Algorithm a solution set is randomly generated, out of which the best ones are picked out. The best solutions will in turn be combined in orderto generate a better solution set, and the process is repeated until a satisfactory result is reached. The Cross-Entropy method is Monte Carlo-based in which a solution set is generated from a probability density function. The solutions are evaluated and then the probability density function is tweaked in order to make better solutions more probable.</p>
----------------------------------------------------------------------
In diva2:1216739 - missing spaces in title:
"A comparison between aconventional LSTM network and agrid LSTM network applied onspeech recognition"
==>
"A comparison between a conventional LSTM network and a grid LSTM network applied onspeech recognition"

abstract is:
<p>In this paper, a comparision between the conventional LSTM network and the one-dimensionalgrid LSTM network applied on single word speech recognition is conducted. The performanceof the networks are measured in terms of accuracy and training time. The conventional LSTMmodel is the current state of the art method to model speech recognition. However, thegrid LSTM architecture has proven to be successful in solving other emperical tasks such astranslation and handwriting recognition. When implementing the two networks in the sametraining framework with the same training data of single word audio files, the conventionalLSTM network yielded an accuracy rate of 64.8 % while the grid LSTM network yielded anaccuracy rate of 65.2 %. Statistically, there was no difference in the accuracy rate betweenthe models. In addition, the conventional LSTM network took 2 % longer to train. However,this difference in training time is considered to be of little significance when tralnslating it toabsolute time. Thus, it can be concluded that the one-dimensional grid LSTM model performsjust as well as the conventional one.</p>

corrected abstract:
<p>In this paper, a comparision between the conventional LSTM network and the one-dimensional grid LSTM network applied on single word speech recognition is conducted. The performance of the networks are measured in terms of accuracy and training time. The conventional LSTM model is the current state of the art method to model speech recognition. However, the grid LSTM architecture has proven to be successful in solving other emperical tasks such as translation and handwriting recognition. When implementing the two networks in the same training framework with the same training data of single word audio files, the conventional LSTM network yielded an accuracy rate of 64.8 % while the grid LSTM network yielded an accuracy rate of 65.2 %. Statistically, there was no difference in the accuracy rate between the models. In addition, the conventional LSTM network took 2 % longer to train. However, this difference in training time is considered to be of little significance when tralnslating it to absolute time. Thus, it can be concluded that the one-dimensional grid LSTM model performs just as well as the conventional one.</p>
----------------------------------------------------------------------
In diva2:1142922 - diva2:1120498 is also a possible duplicate (The PDF in this latter case is just the paper - rather than the 2017 book form)

abstract is:
<p>The fundamentals of secure systems can be presentedas the three cornerstones: authentication (trustworthinessof senders), integrity (inability to alter messages) and confidentiality(inability to read messages for anyone except theintended recipient). In this project, an android application hasbeen programmed by the author with the purpose of sendinggeographical data over a bluetooth connection in a secure peerto-peer manner. In the application, the three goals were metprimarily through the use of certificate validation, encryption anddigital signatures. The final application features a user-interfacewith a Google Maps interface, and query parameters that theuser can set when requesting data about a certain position.The final application would very much be considered secure inan environment where only two android devices are communicating.However, more steps would have to be taken if the applicationis to be deployed commercially. The primary questions arescalability, speed and further security complications.</p>

corrected abstract:
<p>The fundamentals of secure systems can be presented as the three cornerstones: authentication (trustworthiness of senders), integrity (inability to alter messages) and confidentiality (inability to read messages for anyone except the intended recipient). In this project, an android application has been programmed by the author with the purpose of sending geographical data over a bluetooth connection in a secure peerto-peer manner. In the application, the three goals were met primarily through the use of certificate validation, encryption and digital signatures. The final application features a user-interface with a Google Maps interface, and query parameters that the user can set when requesting data about a certain position.</p><p>The final application would very much be considered secure in an environment where only two android devices are communicating. However, more steps would have to be taken if the application is to be deployed commercially. The primary questions are scalability, speed and further security complications.</p>

Note that the capitalization of Android and Bluetooth are incorrect in the origianl abstract.
----------------------------------------------------------------------
In diva2:1140141 - mssing space in title:
"On the use of miniaturized delta wings tomodulate the Blasius boundary layer"
==>
"On the use of miniaturized delta wings to modulate the Blasius boundary layer"

abstract is:
<p>The present study represents an experimental proof-of-concept of the use offree stream vortices to modulate the Blasius boundary layer in the spanwisedirection. Miniaturized delta wings have been employed to generate pairs ofcounter rotating vortices that, through the lift-up mechanism, create velocitystreaks which modulate the boundary layer. This study has been dedicatedto the design and characterization of the experimental set up in the BL windtunnel of KTH Mechanics. A 3-component LDV system has been used tocharacterize the ow eld.By analyzing the ow eld, it has been assessed that a streaky strucutre isindeed generated inside the boundary layer though this methodology. Amongthe dierent parameters in uencing the streaks generated, the angle of attackof the delta wings has been varied. For angles of 9 and 12 streaks with higherintensities than those generated inside the boundary layer, documented in theliterature, have been generated. For an angle of 3, they have been successfullygenerated and embedded inside the boundary layer. Further development ofthis technique could lead to its use in passive ow control strategies aiming atdelaying boundary layer transition.</p>

corrected abstract:
<p>The present study represents an experimental proof-of-concept of the use of free stream vortices to modulate the Blasius boundary layer in the spanwise direction. Miniaturized delta wings have been employed to generate pairs of counter rotating vortices that, through the lift-up mechanism, create velocity streaks which modulate the boundary layer. This study has been dedicated to the design and characterization of the experimental set up in the BL wind tunnel of KTH Mechanics. A 3-component LDV system has been used to characterize the flow field.</p><p>By analyzing the flow field, it has been assessed that a streaky strucutre is indeed generated inside the boundary layer though this methodology. Among the different parameters influencing the streaks generated, the angle of attack of the delta wings has been varied. For angles of 9 and 12º streaks with higher intensities than those generated inside the boundary layer, documented in the literature, have been generated. For an angle of 3º, they have been successfully generated and embedded inside the boundary layer. Further development of this technique could lead to its use in passive flow control strategies aiming at delaying boundary layer transition.</p>
----------------------------------------------------------------------
In diva2:1110869 abstract is:
<p>With thorough literature studies as well as simulations, a way to minimize the exposure to radiationthat astronauts are at risk of encountering during a solar proton event is sought. The understanding ofwhere these particles come from, as well as the random nature of solar particle events is of importancein order to predict their occurrence. Different models used for predicting solar particle events based on aPoisson possibility distribution are presented, as well as real-time forecasts which give a warning of anapproaching event. Although the models used for real-time forecasts have a high accuracy rate, the averagewarning time is only approximately one hour. The downside with the predicted possible occurrence isthat this only gives a statistical probability of events that could possibly occur. For the real-time forecaststhe downside is that with an average warning time of only one hour, they do not give a lot of time forseeking shelter during the onset of an event. With simulations it is shown that the best way to minimizethe radiation dose obtained by astronauts is to use different materials of shielding. It is also shown that alower shielding thickness when encountering SPEs, for example when in a space suit, is useful as longas the total amount of time spent in this suit during the duration of a mission is planned thoroughly inorder to stay below the radiation dose limits. If an astronaut would be caught in an event with the samemagnitude and intensity as the solar particle event of August 1972, it is shown that the astronaut onlyhas nine minutes to seek shelter before exceeding the radiation dose limits and thereby risking radiationinduced sickness.</p>

corrected abstract:
<p>With thorough literature studies as well as simulations, a way to minimize the exposure to radiation that astronauts are at risk of encountering during a solar proton event is sought. The understanding of where these particles come from, as well as the random nature of solar particle events is of importance in order to predict their occurrence. Different models used for predicting solar particle events based on a Poisson possibility distribution are presented, as well as real-time forecasts which give a warning of an approaching event. Although the models used for real-time forecasts have a high accuracy rate, the average warning time is only approximately one hour. The downside with the predicted possible occurrence is that this only gives a statistical probability of events that could possibly occur. For the real-time forecasts the downside is that with an average warning time of only one hour, they do not give a lot of time for seeking shelter during the onset of an event. With simulations it is shown that the best way to minimize the radiation dose obtained by astronauts is to use different materials of shielding. It is also shown that a lower shielding thickness when encountering SPEs, for example when in a space suit, is useful as long as the total amount of time spent in this suit during the duration of a mission is planned thoroughly in order to stay below the radiation dose limits. If an astronaut would be caught in an event with the same magnitude and intensity as the solar particle event of August 1972, it is shown that the astronaut only has nine minutes to seek shelter before exceeding the radiation dose limits and thereby risking radiation induced sickness.</p>
----------------------------------------------------------------------
In diva2:1089907 - missing spaces in title:
"Stenotic Flows: Direct Numerical Simulation,Stability and Sensitivity to Asymmetric ShapeVariations"
==>
"Stenotic Flows: Direct Numerical Simulation, Stability and Sensitivity to Asymmetric Shape Variations"

abstract is:
<p>Flow through a sinuous stenosis with varying degrees of shape asymmetry andat Reynolds number ranging from 250 up to 800 is investigated using direct numericalsimulation (DNS), global linear stability analysis and sensitivity analysis.The shape asymmetry consists of an offset of the stenosis throat, quantifiedas the eccentricity parameter, E. At low Reynolds numbers in a symmetricgeometry, the flow is steady and symmetric. Our results show that when Reynoldsnumber is increased, the flow obtains two simultaneous linearly stablesteady states through a subcritical Pitchfork bifurcation: a symmetric stateand an asymmetric state. The critical Reynolds number for transition betweenthe states are found to be very sensitive to asymmetric shape variations, thusbifurcation can also occur with respect to eccentricity for a given Reynoldsnumber. The final state observed in the DNS can be either nearly symmetricor strongly asymmetric, depending on the initial condition. When eccentricityis increased from zero, the symmetric state becomes slightly asymmetric,flow asymmetry varying nearly linearly with eccentricity. When eccentricityis increased further, the nearly symmetric state becomes linearly unstable. Alinear global stability analysis shows that the eigenvalue sensitivity to eccentricityis of the second order, this is also confirmed by preliminary sensitivityanalysis. For higher Reynolds numbers, the asymmetric solution branch displaysregimes of periodic oscillations as well as intermittency. Comparisons aremade to earlier studies and a theory that attempts to explain and unite thedifferent numerical and experimental results within the field is presented.</p>

corrected abstract:
<p>Flow through a sinuous stenosis with varying degrees of shape asymmetry and at Reynolds number ranging from 250 up to 800 is investigated using direct numerical simulation (DNS), global linear stability analysis and sensitivity analysis. The shape asymmetry consists of an offset of the stenosis throat, quantified as the eccentricity parameter, <em>E</em>. At low Reynolds numbers in a symmetric geometry, the flow is steady and symmetric. Our results show that when Reynolds number is increased, the flow obtains two simultaneous linearly stable steady states through a subcritical Pitchfork bifurcation: a symmetric state and an asymmetric state. The critical Reynolds number for transition between the states are found to be very sensitive to asymmetric shape variations, thus bifurcation can also occur with respect to eccentricity for a given Reynolds number. The final state observed in the DNS can be either nearly symmetric or strongly asymmetric, depending on the initial condition. When eccentricity is increased from zero, the symmetric state becomes slightly asymmetric, flow asymmetry varying nearly linearly with eccentricity. When eccentricity is increased further, the nearly symmetric state becomes linearly unstable. A linear global stability analysis shows that the eigenvalue sensitivity to eccentricity is of the second order, this is also confirmed by preliminary sensitivity analysis. For higher Reynolds numbers, the asymmetric solution branch displays regimes of periodic oscillations as well as intermittency. Comparisons are made to earlier studies and a theory that attempts to explain and unite the different numerical and experimental results within the field is presented.</p>
----------------------------------------------------------------------
In diva2:1083783 abstract is:
<p>One of the governing sources of energy loss in a modern day jet engine is attributed to surfacedrag. This energy loss can be divided into friction loss and to surface geometry loss. Thefriction loss is the shear stress the fluid experience due to a no slip condition at the wall, whilethe surface geometry loss is due to pressure drop when the fuel passes an obstacle.The objective of this work is to study the drag coefficient of a plate for different types ofmilled tracks and for different kinds of flow conditions. The theories used to calculate thedrag coefficient are based on the momentum thickness theory including shear stress- andpressure integration. The computations were carried out with ANSYS CFX assuming a ShearStress Transport 𝑘 − 𝜔 turbulence model. The steady state flow conditions tested are varyingboundary layer thicknesses, milled track heights, milled track widths, Reynolds numbers overthe milled track height, Reynolds numbers over the plate length and free-stream angle ofattack. By knowing what affects the drag coefficient for different types of milled tracks, morepractical models can be developed making the prediction of surface drag inside the jet enginemore accurate.This report has resulted in a formula that predicts the drag coefficient for different types ofmilled surfaces. The formula is derived from the assumption that the CFD results on ANSYSCFX are correct. A physical test has not been made to verify those results, however this has tobe done to prove that this formula is valid.</p>

corrected abstract:
<p>One of the governing sources of energy loss in a modern day jet engine is attributed to surface drag. This energy loss can be divided into friction loss and to surface geometry loss. The friction loss is the shear stress the fluid experience due to a no slip condition at the wall, while the surface geometry loss is due to pressure drop when the fuel passes an obstacle.</p><p>The objective of this work is to study the drag coefficient of a plate for different types of milled tracks and for different kinds of flow conditions. The theories used to calculate the drag coefficient are based on the momentum thickness theory including shear stress- and pressure integration. The computations were carried out with ANSYS CFX assuming a Shear Stress Transport <em>k − &omega;</em> turbulence model. The steady state flow conditions tested are varying boundary layer thicknesses, milled track heights, milled track widths, Reynolds numbers over the milled track height, Reynolds numbers over the plate length and free-stream angle of attack. By knowing what affects the drag coefficient for different types of milled tracks, more practical models can be developed making the prediction of surface drag inside the jet engine more accurate.</p><p>This report has resulted in a formula that predicts the drag coefficient for different types of milled surfaces. The formula is derived from the assumption that the CFD results on ANSYS CFX are correct. A physical test has not been made to verify those results, however this has to be done to prove that this formula is valid.</p>
----------------------------------------------------------------------
In diva2:1083057 - missing spaces in title:
"Performance Evaluation of the EjectionSystem for the RAIN RocketExperiment"
==>
"Performance Evaluation of the Ejection System for the RAIN Rocket Experiment"

abstract is:
<p>This master's thesis work was performed at the department of Mechanics of the RoyalInstitute of Technology (KTH) in Stockholm as part of my Master of Science studiesin Aerospace Engineering at KTH. This thesis study has two major purposes: (1) toevaluate the performance of the spring-based ejection system used in the RAIN rocketexperiment and (2) to suggest improvements to reduce de-spin and tip-o of the ejectedprobes.To evaluate the performance of the ejection system two sets of data have been analyzed:on-ground tests data and ight data. Data from on-ground ejection tests have beenanalyzed by means of video analysis and inertial sensor analysis while for ight dataonly inertial sensor data were available. Moreover, simple mechanical analytical modelshave been created to model the behavior of the probes during the ejection.The results from data analysis and mechanical models are able to suggests some improvementsfor the ejection system. However, it is not possible to make any strongconclusion on what might have caused the de-spin and the tip-o of the probes.</p>

corrected abstract:
<p>This master's thesis work was performed at the department of Mechanics of the Royal Institute of Technology (KTH) in Stockholm as part of my Master of Science studies in Aerospace Engineering at KTH. This thesis study has two major purposes: (1) to evaluate the performance of the spring-based ejection system used in the RAIN rocket experiment and (2) to suggest improvements to reduce de-spin and tip-off of the ejected probes.</p><p>To evaluate the performance of the ejection system two sets of data have been analyzed: on-ground tests data and flight data. Data from on-ground ejection tests have been analyzed by means of video analysis and inertial sensor analysis while for flight data only inertial sensor data were available. Moreover, simple mechanical analytical models have been created to model the behavior of the probes during the ejection.</p><p>The results from data analysis and mechanical models are able to suggests some improvements for the ejection system. However, it is not possible to make any strong conclusion on what might have caused the de-spin and the tip-off of the probes.</p>
----------------------------------------------------------------------
The following results are based upon a comparison of two different hashes of the English abstracts

len(possible_duplicates)=24
possible_duplicates=[
{'diva2:1142785', 'diva2:1120402'}, # Bus Line Optimisation Using Autonomous Minibuses
{'diva2:1656340', 'diva2:1596326'}, # Combined Actuarial Neural Networks in Actuarial Rate Making
{'diva2:1478009', 'diva2:1487614'}, # Conceptual Design of an Air- launched Multi-stage Launch Vehicle
{'diva2:1596321', 'diva2:1610014'}, # Deep Bayesian Neural Networks for Prediction of Insurance Premiums
{'diva2:530870', 'diva2:539446'},   # Design of secondary air system and thermal models for triple spool jet engines
{'diva2:1833721', 'diva2:1781513'}, # Exploring the impact of economic and social factors on stock market performance
{'diva2:408838', 'diva2:408837'},   # Global Learning at Ericsson: how to improve knowledge managementand competence build-up
{'diva2:1120480', 'diva2:1120479'}, # Gravitational Waves and Coalescing Black Holes
{'diva2:408824', 'diva2:408825'},   # Hur förbättra utvärderingsverktyget på VUC
{'diva2:1142776', 'diva2:1120852'}, # Intelligent Traffic Intersection Management Using Motion Planning for Autonomous Vehicles
{'diva2:1867288', 'diva2:1867352'}, # Modeling of Time Series Dynamics through Nonequilibrium Statistical Mechanics
{'diva2:1083787', 'diva2:1071273'}, # Modelling coaxial jets relevat to turbofan jet engines
{'diva2:558593', 'diva2:562861'},   # Photoluminescence and AFM characterization of silicon nanocrystals prepared by low-temperature plasma enhanced chemical vapour depositon and annealing.
{'diva2:1880442', 'diva2:1879507'}, # Portable wind tunnel design​
{'diva2:1120322', 'diva2:1120317'}, # Quadcopter formation simulated in a choreographed dance to music
{'diva2:754257', 'diva2:753742'},   # Revision Of The Aircraft Engines Preliminary Design Platform Of First Level
{'diva2:1120520', 'diva2:1142942'}, # SDR Implementation for Satellite Communication
{'diva2:1120498', 'diva2:1142922'}, # Security and Privacy for Modern and Emerging Mobile Systems
{'diva2:1045047', 'diva2:1033220'}, # Simulating Group Formations Using RVO
{'diva2:865309', 'diva2:815818'},   # Stress simulation of the SEAM CubeSat structure during launch.
{'diva2:1142795', 'diva2:1120491'}, # Study of a Battery Driven Electrohydrodynamic Thruster
{'diva2:1221489', 'diva2:1219115'}, # The impact of missing data imputation on HCC survival prediction: Exploring the combination of missing data imputation with data-level methods such as clustering and oversampling
{'diva2:1142934', 'diva2:1120556'}, # Thermal Analysis and Control of MIST
{'diva2:1033208', 'diva2:1045056'} # Visualizing the Body Language of a Musical Conductor using Gaussian Process Latent Variable Models ---- Note difference in titles
]
----------------------------------------------------------------------
In diva2:853573 abstract is:
<p>This paper summarises the most important aspects of shipping today, describingstakeholders, markets and types of ships, and weighing the sustainability of this form oftransportation.</p><p>Furthermore, it follows an initial ship design project step by step: a new vessel is beingprojected to fulfill a transportation need. The vessel will safely and cost-effectively transport1.2 million cubic meters of raw logs from Port Metro in Vancouver to the Chinese harboursTianjin and Qingdao. During the initial design phase, the requirements and main limitingfactors of the ship’s specifications are identified and the system concept is created.</p><p>Also investigated in this paper is the need for a new generation of intact stability criteria forships, in light of the mechanics of a stability failure mode, parametric rolling, and cases ofparticular accidents at sea involving parametric rolling. The circumstances surroundingincidents of stability failure in cases where existing criteria are met, are explored. The existingcriteria are examined and possible motivations behind the development of new criteria areconsidered. Finally, new methods for assessing risk in the case of parametric rolling areproposed. Based on the importance of this topic for safety at sea, and the hypothesis that asecond generation of stability criteria is needed is examined critically.</p>

corrected abstract:
<p>This paper summarises the most important aspects of shipping today, describing stakeholders, markets and types of ships, and weighing the sustainability of this form of transportation.</p><p>Furthermore, it follows an initial ship design project step by step: a new vessel is being projected to fulfill a transportation need. The vessel will safely and cost-effectively transport 1.2 million cubic meters of raw logs from Port Metro in Vancouver to the Chinese harbours Tianjin and Qingdao. During the initial design phase, the requirements and main limiting factors of the ship’s specifications are identified and the system concept is created.</p><p>Also investigated in this paper is the need for a new generation of intact stability criteria for ships, in light of the mechanics of a stability failure mode, parametric rolling, and cases of particular accidents at sea involving parametric rolling. The circumstances surrounding incidents of stability failure in cases where existing criteria are met, are explored. The existing criteria are examined and possible motivations behind the development of new criteria are considered. Finally, new methods for assessing risk in the case of parametric rolling are proposed. Based on the importance of this topic for safety at sea, and the hypothesis that a second generation of stability criteria is needed is examined critically.</p>
----------------------------------------------------------------------
In diva2:852972 abstract is:
<p>The inhabitants of the earth are facing a global crisis unless the amount ofgreenhouse gases in the atmosphere is reduced to sustainable levels. In anincreasingly globalized world the aircraft industry contributes to an increasinglyamount of emissions that will only increase further if no drastic measures are taken.</p><p>This bachelor thesis aims to study the fuel reduction that in ideal cases can beobtained by utilizing the so-called distributed propulsion configuration. This meansthat a series of electrically driven propellers are placed at the trailing edge of the wingwith purpose to fill the arisen wake behind the wing. The method is well establishedin the marine technology and is referred as the Boundary Layer Ingestion. The resultin the paper is derived from mathematical models and existing data from a Cessna172 aircraft.</p><p>A possible fuel reduction of 11% is derived for a completely idealized situation whichis considered as a very good result which indicates that the BLI technology is worthstudying more closely and develop further.</p>

corrected abstract:
<p>The inhabitants of the earth are facing a global crisis unless the amount of greenhouse gases in the atmosphere is reduced to sustainable levels. In an increasingly globalized world the aircraft industry contributes to an increasingly amount of emissions that will only increase further if no drastic measures are taken.</p><p>This bachelor thesis aims to study the fuel reduction that in ideal cases can be obtained by utilizing the so-called distributed propulsion configuration. This means that a series of electrically driven propellers are placed at the trailing edge of the wing with purpose to fill the arisen wake behind the wing. The method is well established in the marine technology and is referred as the Boundary Layer Ingestion. The result in the paper is derived from mathematical models and existing data from a Cessna 172 aircraft.</p><p>A possible fuel reduction of 11% is derived for a completely idealized situation which is considered as a very good result which indicates that the BLI technology is worth studying more closely and develop further.</p>
----------------------------------------------------------------------
In diva2:784038 abstract is:
<p>A person falling of a ship can be difficult to locate since the wake behind the ship forms achaotic field, making it extremely difficult to predict the location of the victim even if thetime when they fell overboard is known. Survivability for humans immersed at sea is verydependent on the time spent in the water, and varies significantly with sea temperature; thismakes it imperative that the victim is retrieved rapidly. Our current research is aimed atreducing this time using several UAV's searching the ships wake simultaneously, as a swarm.Since the wake is chaotic, a simulation was developed to model different random motions of avictim based on a chaotic equation. Our current research is making use of an establishedsimulator environment and developing it further to investigate how different platforms mayaffect rescue time, varying on the size of the ship, the weather conditions and whether thesearch is operated during day or night. Two different search strategies were implemented inthe developed simulator; these are Expanding Square search and Parallel search. An overallconclusion based on the results obtained is that the expanding square search tends to be amore rigid and reliable search strategy. Also the results show that for any scenario, the soughtperson is detected within minutes.</p>

corrected abstract:
<p>A person falling of a ship can be difficult to locate since the wake behind the ship forms a chaotic field, making it extremely difficult to predict the location of the victim even if the time when they fell overboard is known. Survivability for humans immersed at sea is very dependent on the time spent in the water, and varies significantly with sea temperature; this makes it imperative that the victim is retrieved rapidly. Our current research is aimed at reducing this time using several UAV's searching the ships wake simultaneously, as a swarm. Since the wake is chaotic, a simulation was developed to model different random motions of a victim based on a chaotic equation. Our current research is making use of an established simulator environment and developing it further to investigate how different platforms may affect rescue time, varying on the size of the ship, the weather conditions and whether the search is operated during day or night. Two different search strategies were implemented in the developed simulator; these are Expanding Square search and Parallel search. An overall conclusion based on the results obtained is that the expanding square search tends to be a more rigid and reliable search strategy. Also the results show that for any scenario, the sought person is detected within minutes.</p>
----------------------------------------------------------------------
In diva2:784014 - Note: no full text in DiVA

missing space in title:
"Implementation of Transitional Laminar SeparationBubbles in a Navier-Stokes Code"
==>
"Implementation of Transitional Laminar Separation Bubbles in a Navier-Stokes Code"

abstract is:
<p>Laminar separation bubbles evolve on laminar wing at low Reynolds number.These bubbles have especially been observed on winglets or on wings with flapsor slats. They can stretch up to 30% chord, taking them into account is crucialsince their presence greatly modifies the forces acting on a wing. However, theestablishment of a model is extremely complex and many researches are stillundertaken on this subject. The aim of this work is to achieve automation ofdetecting and taking into account long bubbles in the Navier -Stokes code ofDassault Aviation named Aether. The key issue is the proper detection of thetransition position inside the bubble. This detection is possible by performing alinear stability calculation, but this method is not very robust in this case andvery costly in computation time, as conventional methods are not practicable inthe presence of separation. Finally, the chosen method uses a boundary layercode to predict the position of separation, then a criterion involving local integralquantities to determine the intermittent transition area. This method has beentested for various 2D airfoils and provided quite satisfactory results.</p>

corrected abstract:
<p>Laminar separation bubbles evolve on laminar wing at low Reynolds number. These bubbles have especially been observed on winglets or on wings with flapsor slats. They can stretch up to 30% chord, taking them into account is crucial since their presence greatly modifies the forces acting on a wing. However, the establishment of a model is extremely complex and many researches are still undertaken on this subject. The aim of this work is to achieve automation of detecting and taking into account long bubbles in the Navier -Stokes code of Dassault Aviation named Aether. The key issue is the proper detection of the transition position inside the bubble. This detection is possible by performing alinear stability calculation, but this method is not very robust in this case and very costly in computation time, as conventional methods are not practicable inthe presence of separation. Finally, the chosen method uses a boundary layer code to predict the position of separation, then a criterion involving local integral quantities to determine the intermittent transition area. This method has been tested for various 2D airfoils and provided quite satisfactory results.</p>
----------------------------------------------------------------------
In diva2:783979 - missing spaces in title:
"Launch and recovery systems for unmannedvehicles onboard ships. A study and initialconcepts."
==>
"Launch and recovery systems for unmanned vehicles onboard ships. A study and initial concepts."

abstract is:
<p>This master’s thesis paper is an exploratory study along with conceptual design of launch and recovery systems(LARS) for unmanned vehicles and RHIB:s, which has been conducted for ThyssenKrupp Marine System AB inKarlskrona, Sweden. Two concepts have been developed, one for aerial vehicles (UAV:s) and one for surfaceand underwater vehicles (USV, RHIB and UUV). The goal when designing the two LARS has been to meet thegrowing demand within the world navies for greater off-board capabilities. The two concepts have been designedto be an integrated solutions on a 90 m long naval ship and based on technology that will be proven in year2015-2020. To meet the goal of using technology that will be proven in year 2015-2020, existing and futurepossible solutions has been evaluated. From the evaluations one technique for each concept was chosen forfurther development.In the development of a LARS for aerial vehicles only fixed wing UAV:s have been considered. The conceptwas made for a reference UAV based on the UAV Shadow 200B, which has a weight of 170 kg. The conceptthat was developed is a parasail lifter that can both launch and recover the reference UAV effectively. In thedevelopment of a system for surface and underwater vehicles only vehicle lengths in the span 1-12 m have beenconsidered. The concept that has been developed is a stern ramp that uses a sled to launch and recover all threevehicle types. The two concepts that has been developed are in an early design state and the papers results shouldtherefore be seen as an estimation of what each system are capable of performing.</p>

corrected abstract:
<p>This master’s thesis paper is an exploratory study along with conceptual design of launch and recovery systems (LARS) for unmanned vehicles and RHIB:s, which has been conducted for <em>ThyssenKrupp Marine System AB</em> in Karlskrona, Sweden. Two concepts have been developed, one for aerial vehicles (UAV:s) and one for surface and underwater vehicles (USV, RHIB and UUV). The goal when designing the two LARS has been to meet the growing demand within the world navies for greater off-board capabilities. The two concepts have been designed to be an integrated solutions on a 90 m long naval ship and based on technology that will be proven in year 2015-2020. To meet the goal of using technology that will be proven in year 2015-2020, existing and future possible solutions has been evaluated. From the evaluations one technique for each concept was chosen for further development.</p><p>In the development of a LARS for aerial vehicles only fixed wing UAV:s have been considered. The concept was made for a reference UAV based on the UAV Shadow 200B, which has a weight of 170 kg. The concept that was developed is a parasail lifter that can both launch and recover the reference UAV effectively. In the development of a system for surface and underwater vehicles only vehicle lengths in the span 1-12 m have been considered. The concept that has been developed is a stern ramp that uses a sled to launch and recover all three vehicle types. The two concepts that has been developed are in an early design state and the papers results should therefore be seen as an estimation of what each system are capable of performing.</p>
----------------------------------------------------------------------
In diva2:650418 - Note: no full text in DiVA
abstract is:
<p>The master thesis is concerned with the investigation of void fraction distributionsacross BWR fuel assemblies without spacers in minor scale design and with theinvestigation of the influence of different axial heat flux distribution and variations ofmass flux on the void fraction using Computational Fluid Dynamic (CFD) commercialcode CFX.11. The code is validated against experimental data obtained by R.T Lahey etal. [1]. A reasonable agreement between predictions and experimental data is obtainedfor the quality. However, some discrepancies are observed for the relative mass fluxdistributions. Further investigation and appropriate modelling in CFD code would beneeded to understand these inconsistencies.</p><p>The CFD code is also tested for BWR and PWR operational conditions. In case ofBWR, CFX suggests constant average void fraction distribution at the cross-section ofrod bundle near the exit at low mass flux and this distribution disrupted at higher massflux for different axial heat flux profiles. While, in case of PWR, CFX suggests that theaverage temperature distribution at the cross-section of the rod bundle near the exit isconstant for specific mass flux, average heat flux and no influence of different axial heatflux profile is detected.</p>


corrected abstract:
<p>The master thesis is concerned with the investigation of void fraction distributions across BWR fuel assemblies without spacers in minor scale design and with the investigation of the influence of different axial heat flux distribution and variations of mass flux on the void fraction using Computational Fluid Dynamic (CFD) commercial code CFX.11. The code is validated against experimental data obtained by R.T Lahey etal. [1]. A reasonable agreement between predictions and experimental data is obtained for the quality. However, some discrepancies are observed for the relative mass flux distributions. Further investigation and appropriate modelling in CFD code would be needed to understand these inconsistencies.</p><p>The CFD code is also tested for BWR and PWR operational conditions. In case of BWR, CFX suggests constant average void fraction distribution at the cross-section of rod bundle near the exit at low mass flux and this distribution disrupted at higher mass flux for different axial heat flux profiles. While, in case of PWR, CFX suggests that the average temperature distribution at the cross-section of the rod bundle near the exit is constant for specific mass flux, average heat flux and no influence of different axial heatflux profile is detected.</p>
----------------------------------------------------------------------
In diva2:642316 abstract is:
<p>The aim of this paper is to document the development of a method for detectionand quantification of Polychlorinated biphenyls (PCB) in soil using GasChromatography (GC) connected to a Quadrupole Mass Spectrometer (MS) via aninternal standard (CB189). The method developed is performed in conjunction withthe information provided by the Swedish environmental agency (Svenskanaturvårdsverket, SNV) in regards to PCB limits for sensitive land usage. The steps ofthe method and maintenance of the GC/MS are used to create a user manual andan attempt at transformative learning is done in an effort to teach the staff atLjungaLab AB so that at the very least, independent analysis can be run and at best,new methods and application are independently developed for the GC/MS. Anevaluation of the teaching efforts is also done to assess what grade of learning isachieved.</p>

corrected abstract:
<p>The aim of this paper is to document the development of a method for detection and quantification of Polychlorinated biphenyls (PCB) in soil using Gas Chromatography (GC) connected to a Quadrupole Mass Spectrometer (MS) via an internal standard (CB189). The method developed is performed in conjunction with the information provided by the Swedish environmental agency (<span lang=sv">Svenska naturvårdsverket</span>, SNV) in regards to PCB limits for sensitive land usage. The steps of the method and maintenance of the GC/MS are used to create a user manual and an attempt at transformative learning is done in an effort to teach the staff at LjungaLab AB so that at the very least, independent analysis can be run and at best, new methods and application are independently developed for the GC/MS. An evaluation of the teaching efforts is also done to assess what grade of learning is achieved.</p>
----------------------------------------------------------------------
In diva2:618236 abstract is:
<p>High residual stresses are likely to develop in honeycomb sandwichparts after autoclave co-curing and can lead to manufacturing defects.By using finite element unit cell models, these stresses have been calculatedfor standard panels and for panels where different core blocksare joined with adhesive. Failure criteria are given for three types ofaluminum honeycombs under combined thermal and shear loads, allowingto calculate the residual strength of the cores. Residual stressvalues are also calculated for adhesive joints between different coreblocks, they being about the same order of magnitude as the strengthof the adhesive regardless of geometry or core combination. Last, it isshown that the effect of the sandwich plate chamfered edges in preventingthe core expansion during the heating cycle may cause corecrushing when high and low density honeycombs are combined.</p>

corrected abstract:
<p>High residual stresses are likely to develop in honeycomb sandwich parts after autoclave co-curing and can lead to manufacturing defects. By using finite element unit cell models, these stresses have been calculated for standard panels and for panels where different core blocks are joined with adhesive. Failure criteria are given for three types of aluminum honeycombs under combined thermal and shear loads, allowing to calculate the residual strength of the cores. Residual stress values are also calculated for adhesive joints between different core blocks, they being about the same order of magnitude as the strength of the adhesive regardless of geometry or core combination. Last, it is shown that the effect of the sandwich plate chamfered edges in preventing the core expansion during the heating cycle may cause core crushing when high and low density honeycombs are combined.</p>
----------------------------------------------------------------------
In diva2:515603 abstract is:
<p>This paper concerns my research on the culture of the IT-workforce and how it relates to otheroccupational cultures, as well as how the relation affects the communication between them. Iconducted an ethnographic study at an IT-consulting company. My focus was on theinteraction between the IT-consultants and their customers. In combination with previousresearch in the same subject area, the study shows that the IT-workforce has a distinctoccupational culture. It is influenced by the reverence of technical knowledge and how the ITworkforceis treated as a group by society in general. This reciprocal prejudice also leads tointercultural dysfunction. Since a lot of companies depend on the IT-workforce there is muchto gain if communication is improved. Some solutions are for the IT-workforce to be morepronounced and understanding in the intercultural communication and to find common goalsto strive towards. Also the lowest level of IT-knowledge among the general population mustbe raised to avoid a class society related to IT.</p>

corrected abstract:
<p>This paper concerns my research on the culture of the IT-workforce and how it relates to other occupational cultures, as well as how the relation affects the communication between them. I conducted an ethnographic study at an IT-consulting company. My focus was on the interaction between the IT-consultants and their customers. In combination with previous research in the same subject area, the study shows that the IT-workforce has a distinct occupational culture. It is influenced by the reverence of technical knowledge and how the IT-workforce is treated as a group by society in general. This reciprocal prejudice also leads to intercultural dysfunction. Since a lot of companies depend on the IT-workforce there is much to gain if communication is improved. Some solutions are for the IT-workforce to be more pronounced and understanding in the intercultural communication and to find common goals to strive towards. Also the lowest level of IT-knowledge among the general population must be raised to avoid a class society related to IT.</p>
----------------------------------------------------------------------
In diva2:416911 abstract is:
<p>Rolling contact fatigue is a problem encountered with many machine elements.In the current report a numerical study has been performed in order to predictthe crack path and crack propagation cycles of a surface initiated rolling contactfatigue crack. The implementation of the contact problem is based on theasperity point load mechanism for rolling contact fatigue. The practical studiedproblem is gear contact. Different loading types and models are studied andcompared to an experimental spall profile. Good agreement has been observedconsidering short crack lengths with a distributed loading model using normalloads on the asperity and for the cylindrical contact and a tangential load on theasperity. Several different crack propagation criteria have been implemented inorder to verify the validity of the dominant mode I crack propagation assumption.Some general characteristics of rolling contact fatigue cracks have beenhighlighted. A quantitative parameter study of the implemented model hasbeen performed. </p>

corrected abstract:
<p>Rolling contact fatigue is a problem encountered with many machine elements. In the current report a numerical study has been performed in order to predict the crack path and crack propagation cycles of a surface initiated rolling contact fatigue crack. The implementation of the contact problem is based on the asperity point load mechanism for rolling contact fatigue. The practical studied problem is gear contact. Different loading types and models are studied and compared to an experimental spall profile. Good agreement has been observed considering short crack lengths with a distributed loading model using normal loads on the asperity and for the cylindrical contact and a tangential load on the asperity. Several different crack propagation criteria have been implemented in order to verify the validity of the dominant mode I crack propagation assumption. Some general characteristics of rolling contact fatigue cracks have been highlighted. A quantitative parameter study of the implemented model has been performed.</p>
----------------------------------------------------------------------
In diva2:405938 abstract is:
<p>This report describes the process of developing a conceptual design of the Storm Bird, a long distance sailing cruiser. The starting point was a boat designed in the mid nineties by the famous Swedish naval architect Håkan Södergren and the aim with the project is to present an idea as to the renewal of the design in a more modern boat. The new Storm bird was supposed to be a full on blue water cruiser concept, a boat that the presumed owner would not have to change in order to set off on his trip.To get insight in the minds and the needs of long distance sailors an extensive market and customer analysis has been undergone. This together with experience in the design team is a base to the thoughts and the ideas incorporated in this design.The hull design was limited to the existing hull moulds meaning that no changes in the hull shape could be made. An alternative however was the transformation from negative to positive transom which proved a very effective way of making the boat feel bigger.The design and layout have been focused on making an effective, well planned but most of all social yacht. The clear boundary between the inside and outside has been removed thanks to a large opening to the cockpit with big windows and good connection. The cockpit and interior areas have been focused towards each other so as to create one big social area, boundary free.Further on the living quarters, as the rest of the boat, are focused on the main idea of the customer being mainly a cruising couple. Therefore an optimal interior layout with focus on the one master cabin has been developedIn the cockpit, seats are comfortable as well as facing forwards and everyone onboard can follow what is going on through the forward placed navigation central. The wide opening between cockpit and interior makes traditional rope handling impossible. All controls are led aft through a clever arrangement to clutches and winches placed on either side of the cockpit instead of on the deck house. This way all functions are in the right position, close to the helmsman. The ropes are later hidden in boxes to ensure a tangle free cockpit.An intelligent overall solution when it comes to onboard systems has been developed as well. Key words have been weight distribution, serviceability and ease of installation. Stowage space and tank volumes correspond to the yacht’s intended use.The structural design has been carried out focusing on arriving at a realistic weight calculation in order to be able to determine centers of gravity and place equipment and ballast to achieve a working concept. Material and manufacturing techniques have been chosen so as to fit the expertise available at the company.Appendage design has focused on modernizing the underwater body by incorporating a new keel and rudder. The performance of the boat has been increased significantly whilst not making it too extreme for its intended purpose.The finished design concept is believed to be a really attractive choice for a blue water sailor.</p>

corrected abstract:
<p>This report describes the process of developing a conceptual design of the Storm Bird, a long distance sailing cruiser. The starting point was a boat designed in the mid nineties by the famous Swedish naval architect Håkan Södergren and the aim with the project is to present an idea as to the renewal of the design in a more modern boat. The new Storm bird was supposed to be a full on blue water cruiser concept, a boat that the presumed owner would not have to change in order to set off on his trip.</p><p>To get insight in the minds and the needs of long distance sailors an extensive market and customer analysis has been undergone. This together with experience in the design team is a base to the thoughts and the ideas incorporated in this design.</p><p>The hull design was limited to the existing hull moulds meaning that no changes in the hull shape could be made. An alternative however was the transformation from negative to positive transom which proved a very effective way of making the boat feel bigger.</p><p>The design and layout have been focused on making an effective, well planned but most of all social yacht. The clear boundary between the inside and outside has been removed thanks to a large opening to the cockpit with big windows and good connection. The cockpit and interior areas have been focused towards each other so as to create one big social area, boundary free.</p><p>Further on the living quarters, as the rest of the boat, are focused on the main idea of the customer being mainly a cruising couple. Therefore an optimal interior layout with focus on the one master cabin has been developed</p><p>In the cockpit, seats are comfortable as well as facing forwards and everyone onboard can follow what is going on through the forward placed navigation central. The wide opening between cockpit and interior makes traditional rope handling impossible. All controls are led aft through a clever arrangement to clutches and winches placed on either side of the cockpit instead of on the deck house. This way all functions are in the right position, close to the helmsman. The ropes are later hidden in boxes to ensure a tangle free cockpit.</p><p>An intelligent overall solution when it comes to onboard systems has been developed as well. Key words have been weight distribution, serviceability and ease of installation. Stowage space and tank volumes correspond to the yacht’s intended use.</p><p>The structural design has been carried out focusing on arriving at a realistic weight calculation in order to be able to determine centers of gravity and place equipment and ballast to achieve a working concept. Material and manufacturing techniques have been chosen so as to fit the expertise available at the company.</p><p>Appendage design has focused on modernizing the underwater body by incorporating a new keel and rudder. The performance of the boat has been increased significantly whilst not making it too extreme for its intended purpose.</p><p>The finished design concept is believed to be a really attractive choice for a blue water sailor.</p>
----------------------------------------------------------------------
In diva2:1881888 abstract is:
<p>During the development of new small modular gen IV nuclear reactors, promising nuclear fuels such as uraniumnitride (UN) are investigated. Radiation damage and its effects on the fuel material have often been investigatedexperimentally through controlled irradiation methods. The aim of this thesis is to modify and develop existingMolecular Dynamics simulation methods for use in UN irradiation simulations and investigate its viability for suchuse. The present work primarily used the CRA to simulate irradiation damage and conducted Wigner-Seitz analysisto find defects. Pressure data for different probability values converged to approximately 150 kPa at a dose of 1 dpaand simulations indicated volumetric swelling of around 6% to 7%, suggesting that microstructural swelling due todefect accumulation could explain the experimental observations of cracking. Cluster analysis reveals that interstitialclusters increase to a peak, after which their number decreases towards a steady state, while DXA analysis returned ahandful of dislocation lines at 1d pa for both the uranium sublattice as well as for the nitrogen sublattice.</p><p> </p>

corrected abstract:
<p>During the development of new small modular gen IV nuclear reactors, promising nuclear fuels such as uranium nitride (UN) are investigated. Radiation damage and its effects on the fuel material have often been investigated experimentally through controlled irradiation methods. The aim of this thesis is to modify and develop existing Molecular Dynamics simulation methods for use in UN irradiation simulations and investigate its viability for such use. The present work primarily used the CRA to simulate irradiation damage and conducted Wigner-Seitz analysis to find defects. Pressure data for different probability values converged to approximately 150 kPa at a dose of 1 dpa and simulations indicated volumetric swelling of around 6% to 7%, suggesting that microstructural swelling due to defect accumulation could explain the experimental observations of cracking. Cluster analysis reveals that interstitial clusters increase to a peak, after which their number decreases towards a steady state, while DXA analysis returned a handful of dislocation lines at 1d pa for both the uranium sublattice as well as for the nitrogen sublattice.</p>
----------------------------------------------------------------------
In diva2:1881336 abstract is:
<p>Understanding the behavior of viscous incompressible fluids is essential for scientificapplications, yet when modeling them presents significant theoretical and practical chal-lenges. This study aimed to develop a numerical solver especially for the two-dimensionalNavier-Stokes equation, tailored for modeling the dynamics of a viscous incompressiblefluid, to conserve the enstrophy. The goal was to accurately simulate a physical sys-tem, and apply numerical methods such as Runge-Kutta 4, Forward Euler’s method,and pseudo-spectral methods to construct and solve the governing Partial DifferentialEquations (PDEs).</p><p>These methods were evaluated for their ability to conserve the enstrophy. Not onlyenhancing our understanding of the application of the equation in real physical systems,this research also contributes to expanding the understanding of numerical methodologiesfor complicated PDEs in physical simulations.</p><p>Using the aforementioned methods, together with strategically specific initial condi-tions, it is observable that the methods are sufficient for conserving the enstrophy whendealing with only the linear part of Navier-Stokes. To improve the numerical methodsconcerning the non-linear part of the Navier-Stokes, a perturbation method was imple-mented. Outcomes from this method appear promising however, implementation andmore detailed analysis are not included in this report due to time constraints. Thisrecovery strategy represents a foundation for further exploration in further research.</p>

corrected abstract:
<p>Understanding the behavior of viscous incompressible fluids is essential for scientific applications, yet when modeling them presents significant theoretical and practical challenges. This study aimed to develop a numerical solver especially for the two-dimensional Navier-Stokes equation, tailored for modeling the dynamics of a viscous incompressible fluid, to conserve the enstrophy. The goal was to accurately simulate a physical system, and apply numerical methods such as Runge-Kutta 4, Forward Euler’s method, and pseudo-spectral methods to construct and solve the governing Partial Differential Equations (PDEs).</p><p>These methods were evaluated for their ability to conserve the enstrophy. Not only enhancing our understanding of the application of the equation in real physical systems, this research also contributes to expanding the understanding of numerical methodologies for complicated PDEs in physical simulations.</p><p>Using the aforementioned methods, together with strategically specific initial conditions, it is observable that the methods are sufficient for conserving the enstrophy when dealing with only the linear part of Navier-Stokes. To improve the numerical methods concerning the non-linear part of the Navier-Stokes, a perturbation method was implemented. Outcomes from this method appear promising however, implementation and more detailed analysis are not included in this report due to time constraints. This recovery strategy represents a foundation for further exploration in further research.</p>
----------------------------------------------------------------------
In diva2:1875954 abstract is:
<p>This thesis addresses the challenge of denoising microscopy images captured under low-light conditionswith varying intensity levels. The study compares three deep learning models — N2V, CARE, andRCAN — against the collaborative filter BM4D, which serves as a reference point. The models weretrained on two distinct datasets: Endoplasmic Reticulum and Mitochondria datasets, both acquired witha lattice light-sheet microscope.Results show that BM4D maintains stable performance metrics and delivers superior visual quality,when compared to the noisy input. In contrast, the deep learning models exhibit poor performance onnoisy test images when trained on datasets with non-uniform noise levels. Additionally, a sensitivitycomparison of neural parameter between the same models was made. Revealing that supervised modelsare data-specific to some extent, whereas the self-supervised N2V demonstrates consistent neuralparameters, suggesting lower data specificity.</p>

corrected abstract:
<p>This thesis addresses the challenge of denoising microscopy images captured under low-light conditions with varying intensity levels. The study compares three deep learning models — N2V, CARE, and RCAN — against the collaborative filter BM4D, which serves as a reference point. The models were trained on two distinct datasets: Endoplasmic Reticulum and Mitochondria datasets, both acquired with a lattice light-sheet microscope.</p><p>Results show that BM4D maintains stable performance metrics and delivers superior visual quality, when compared to the noisy input. In contrast, the deep learning models exhibit poor performance on noisy test images when trained on dataosets with non-uniform noise levels. Additionally, a sensitivity comparison of neural parameter between the same models was made. Revealing that supervised models are data-specific to some extent, whereas the self-supervised N2V demonstrates consistent neural parameters, suggesting lower data specificity.</p>
----------------------------------------------------------------------
In diva2:1799867 abstract is:
<p>This master’s thesis explores the possibility of calculating and using the pressure andvelocity fields responsible for acoustophoresis. The goal was to use simulated data,similar to two-dimensional particle tracks from a specific microfluidic platform, toestimate the force potential and solve the partial differential equation (PDE) thatgoverns the relationship between the force potential and the acoustic fields. Lastly,the possibility of identifying the mechanical properties of unknown particles, once theacoustic field was known, was investigated. The thesis found that solving the PDE usingthe finite difference method was likely not possible and an alternative method has beensuggested. It also found that the use of particle tracks to measure the compressibilityand density of cells as a biomarker is promising, as most simulated particles wereaccurately measured.</p>

corrected abstract:
<p>This master’s thesis explores the possibility of calculating and using the <em>pressure and velocity fields</em> responsible for <em>acoustophoresis</em>. The goal was to use simulated data, similar to two-dimensional <em>particle tracks</em> from a specific <em>microfluidic</em> platform, to estimate the <em>force potential</em> and solve the <em>partial differential equation</em> (PDE) that governs the relationship between the force potential and the <em>acoustic fields</em>. Lastly, the possibility of identifying the mechanical properties of unknown particles, once the acoustic field was known, was investigated. The thesis found that solving the PDE using the <em>finite difference method</em> was likely not possible and an alternative method has been suggested. It also found that the use of particle tracks to measure the <em>compressibility</em> and <em>density</em> of cells as a <em>biomarker</em> is promising, as most simulated particles were accurately measured.</p>
----------------------------------------------------------------------
In diva2:1791909 abstract is:
<p>This thesis presents an estimator to assist or replace a fighter aircraft’s air datasystem (ADS). The estimator is based on machine learning and LSTM neuralnetworks and uses the statistical correlation between states to estimate the angleof attack, angle of sideslip and Mach number using only the internal sensorsof the aircraft. The model is trained and extensively tested on a fighter jetsimulation model and shows promising results. The methodology and accuracyof the estimator are discussed, together with how a real-world implementationwould work. The estimators presented should act as a proof of concept of thepower of neural networks in state estimation, whilst the report discusses theirstrengths and weaknesses. The estimators can estimate the three targets wellin a vast envelope of altitudes, speeds, winds and manoeuvres. However, thetechnology is quite far from real-world implementation as it lacks transparencybut shows promising potential for future development.</p>

corrected abstract:
<p>This thesis presents an estimator to assist or replace a fighter aircraft’s air data system (ADS). The estimator is based on machine learning and LSTM neural networks and uses the statistical correlation between states to estimate the angle of attack, angle of sideslip and Mach number using only the internal sensors of the aircraft. The model is trained and extensively tested on a fighter jet simulation model and shows promising results. The methodology and accuracy of the estimator are discussed, together with how a real-world implementation would work. The estimators presented should act as a proof of concept of the power of neural networks in state estimation, whilst the report discusses their strengths and weaknesses. The estimators can estimate the three targets well in a vast envelope of altitudes, speeds, winds and manoeuvres. However, the technology is quite far from real-world implementation as it lacks transparency but shows promising potential for future development.</p>
----------------------------------------------------------------------
In diva2:1774381 abstract is:
<p>Calcium ions are one of the most versatile signalling molecules. They are essential tothe proper functioning of various cellular processes in many different types of cells.The calcium signals have been studied in the past using ratiometric dyes like Fura-2.We showed that the genetically encoded calcium indicator GCaMP6m displays highersignal to noise ratio than Fura Red and therefore used it to study the calcium cytosolicconcentration in MDCK II cells. We use fluorescence microscopy to record and Pythonto analyse calcium signals in individual cells. Cellpose was used for automating thesegmentation. The cells were treated with ouabain, a cardiotonic steroid shown toincrease the intercellular communication between cells through gap junctions. Thecells were also transfected with connexins 43. We showed that ouabain does nothave an impact on the number of calcium peaks. We observed higher correlationsin the calcium between adjaçent transfected cells, but the results are not statisticallysignificant. We also observed clearly defined oscillations in one low confluencerecording with a period of around two minutes.</p>

corrected abstract:
<p>Calcium ions are one of the most versatile signalling molecules. They are essential to the proper functioning of various cellular processes in many different types of cells. The calcium signals have been studied in the past using ratiometric dyes like Fura-2. We showed that the genetically encoded calcium indicator GCaMP6m displays higher signal to noise ratio than Fura Red and therefore used it to study the calcium cytosolic concentration in MDCK II cells. We use fluorescence microscopy to record and Python to analyse calcium signals in individual cells. Cellpose was used for automating the segmentation. The cells were treated with ouabain, a cardiotonic steroid shown to increase the intercellular communication between cells through gap junctions. The cells were also transfected with connexins 43. We showed that ouabain does not have an impact on the number of calcium peaks. We observed higher correlations in the calcium between adjaçent transfected cells, but the results are not statistically significant. We also observed clearly defined oscillations in one low confluence recording with a period of around two minutes.</p>
----------------------------------------------------------------------
In diva2:1770544 abstract is:
<p>This thesis presents an analysis of misidentified leptons in the Higgs boson decaychannel H → W W ∗ → lνlν. Misidentified leptons, resulting from jets misidentifiedas leptons, mimic the signal of a Higgs boson decay, resulting in a backgroundcontribution to the signal. The analysis is performed on proton-proton collisions ata center-of-mass energy of 13.6 TeV recorded by the ATLAS experiment at CERN’sLarge Hadron Collider. The estimation of misidentified leptons is done using theso-called Fake Factor method, which is used to assess the contributed backgroundwith misidentified leptons in the signal region. The Fake Factor values increasewith increasing momenta for misidentified electrons while it remains constant formisidentified muons. An analysis of the impact parameters d0 and z0 show thatthe accuracy of the Monte Carlo simulation to correctly predict the contribution ofmisidentified leptons from heavy quarks is high.</p>

corrected abstract:
<p>This thesis presents an analysis of misidentified leptons in the Higgs boson decay channel H → WW<sup>∗</sup> → lνlν. Misidentified leptons, resulting from jets misidentified as leptons, mimic the signal of a Higgs boson decay, resulting in a background contribution to the signal. The analysis is performed on proton-proton collisions at a center-of-mass energy of 13.6 TeV recorded by the ATLAS experiment at CERN’s Large Hadron Collider. The estimation of misidentified leptons is done using the so-called Fake Factor method, which is used to assess the contributed background with misidentified leptons in the signal region. The Fake Factor values increase with increasing momenta for misidentified electrons while it remains constant for misidentified muons. An analysis of the impact parameters <em>d<sub>0</sub></e> and <em>z<sub>0</sub></em> show that the accuracy of the Monte Carlo simulation to correctly predict the contribution of misidentified leptons from heavy quarks is high.</p>
----------------------------------------------------------------------
In diva2:1679050 abstract is:
<p>Clean, drinkable water is nowadays taken for granted in most developed coun-tries. However, over two billion people in the world do not have access to drink-ing water. In an attempt to combat this, capacitive deionization (CDI) hasgained increased attention in recent years. CDI is an emergent method of de-salination through separation of ionic species in aqueous solutions. The perfor-mance of CDI is dependent on materials used and how the device is constructed.This paper investigates key metrics relating the efficiency and applicability oftwo different CDI materials, activated carbon (Zorflex FM10 Chemivron) andCobalt Prussian Blue Analogue (referred to as the active material), in regardsto the electrodes used. These metrics include energy consumption, energy re-covery and Faradaic efficiency. The results were gathered from building a circuitwith the CDI cell as the capacitor and switching the polarity of the cell when adefined threshold of the voltage (1.5 V) was reached. The energy consumptionof the activated carbon (0.450 kWh/m3) was found to be less than that of theactive material (1.45 kWh/m3). The energy recovery was found to be roughlyequal for both materials, 80.6 % for the activated carbon and 79.5 % for theactive material. Finally, the activated carbon had a Faradaic efficiency of 0.75while the active material had 1.8.</p>

corrected abstract:
<p>Clean, drinkable water is nowadays taken for granted in most developed countries. However, over two billion people in the world do not have access to drinking water [1]. In an attempt to combat this, capacitive deionization (CDI) has gained increased attention in recent years. CDI is an emergent method of desalination through separation of ionic species in aqueous solutions. The performance of CDI is dependent on materials used and how the device is constructed. This paper investigates key metrics relating the efficiency and applicability of two different CDI materials, activated carbon (Zorflex FM10 Chemivron) and Cobalt Prussian Blue Analogue (referred to as the active material), in regards to the electrodes used. These metrics include energy consumption, energy recovery and Faradaic efficiency. The results were gathered from building a circuit with the CDI cell as the capacitor and switching the polarity of the cell when a defined threshold of the voltage (1.5 V) was reached. The energy consumption of the activated carbon (0.450 kWh/m<sup>3</sup>) was found to be less than that of the active material (1.45 kWh/m<sup>3</sup>). The energy recovery was found to be roughly equal for both materials, 80.6 % for the activated carbon and 79.5 % for the active material. Finally, the activated carbon had a Faradaic efficiency of 0.75 while the active material had 1.8.</p>
----------------------------------------------------------------------
In diva2:1656073 abstract is:
<p>This study aims to see if it is possible to generate abnormal returns in the Swedishstock market through the use of three different trading strategies based on technicalindicators. As the indicators are based on historical price data only, the study assumesweak market efficiency according to the efficient market hypothesis. The study isconducted using daily prices for OMX Stockholm PI and STOXX 600 Europe from theperiod between 1 January 2010 and 31 December 2019. Trading positions has beentaken in the OMX Stockholm PI index while STOXX 600 Europe has been used torepresent the market portfolio. Abnormal returns has been defined as the Jensen’s αin a Fama French three factor model with Carhart ­extension. This period has beencharacterised by increasing prices (a bull market) which may have had an impact onthe results. Furthermore, a higher frequency of rebalancing for the Fama ­French andCarhart model could also increase the quality of the results. The results indicate thatall three strategies has generated abnormal returns during the period.</p>

corrected abstract:
<p>This study aims to see if it is possible to generate abnormal returns in the Swedish stock market through the use of three different trading strategies based on technical indicators. As the indicators are based on historical price data only, the study assumes weak market efficiency according to the efficient market hypothesis. The study is conducted using daily prices for OMX Stockholm PI and STOXX 600 Europe from the period between 1 January 2010 and 31 December 2019. Trading positions has been taken in the OMX Stockholm PI index while STOXX 600 Europe has been used to represent the market portfolio. Abnormal returns has been defined as the <em>Jensen’s α</em> in a Fama French three factor model with Carhart­extension. This period has been characterised by increasing prices (a <em>bull market</em>) which may have had an impact on the results. Furthermore, a higher frequency of rebalancing for the Fama­French and Carhart model could also increase the quality of the results. The results indicate that all three strategies has generated abnormal returns during the period.</p>
----------------------------------------------------------------------
In diva2:1571710 abstract is:
<p>In this thesis, I demonstrate a single­photon Light Detection And Ranging, (LiDAR)system operating at 1550 nm capable of reconstructing 3D environments live withmm resolution at a rate of 400 points per second using eye­safe laser pulses. Thesystem was built using off­-the-­shelf optical components and analysis was performedusing open-­source software. I utilise a single superconducting nanowire single photondetector (SNSPD) with 19 ps time jitter and 85% detection efficiency to achieve a 4 psdepth resolution in live measurements. I also show that by performing slightly moretime costly post analysis of the data it is possible to increase the details and smoothnessof the images.</p><p>Furthermore, I show that the same LiDAR system and much of the algorithms usedfor 3D LiDAR can be used to perform Optical Time Domain reflectrometry (OTDR)measurements. I demonstrate that the system can identify interfaces between differentrefractive mediums such as fibre to fibre or fibre to air couplings with a depth resolutionof 9 mm along a single line. Using these reflections, I also show that the systemcan identify flaws in optical fibres as well as measure certain characteristics suchas absorption coefficient due to Rayleigh scattering or thermal expansion. Lastly, Idemonstrate that the same OTDR principles used in fibres can be applied to free­s-paceoptical setups and that the system can identify specific optical elements as well asmeasure the quality of the alignment of an optical system.</p>

corrected abstract:
<p>In this thesis, I demonstrate a single­photon <em>Light Detection And Ranging</em>, (LiDAR) system operating at 1550 nm capable of reconstructing 3D environments live with mm resolution at a rate of 400 points per second using eye­safe laser pulses. The system was built using off-the-shelf optical components and analysis was performed using open-source software. I utilise a single <em>superconducting nanowire single photon detector</em> (SNSPD) with 19 ps time jitter and 85% detection efficiency to achieve a 4 ps depth resolution in live measurements. I also show that by performing slightly more time costly post analysis of the data it is possible to increase the details and smoothness of the images.</p><p>Furthermore, I show that the same LiDAR system and much of the algorithms used for 3D LiDAR can be used to perform <em>Optical Time Domain reflectrometry</em> (OTDR) measurements. I demonstrate that the system can identify interfaces between different refractive mediums such as fibre to fibre or fibre to air couplings with a depth resolution of 9 mm along a single line. Using these reflections, I also show that the system can identify flaws in optical fibres as well as measure certain characteristics such as absorption coefficient due to Rayleigh scattering or thermal expansion. Lastly, I demonstrate that the same OTDR principles used in fibres can be applied to free-space optical setups and that the system can identify specific optical elements as well as measure the quality of the alignment of an optical system.</p>
----------------------------------------------------------------------
In diva2:1571214 abstract is:
<p>Drinkable water is a necessary resource for human beings, unfortunately thesupply is not satisfying the increasing demand. Recent requirement for economicallysustainable and energy conservative desalination of water have revived the interestof a method first mentioned in the 1960s, know as “electrochemical demineralization”.Today is the method referred to as ”Capacitive Deionization” or CDI for short,and there are commercial solutions emerging on the market. The adoption isstill in the early stage and faces engineering challenges.The experiment performed in this thesis tries to quantify the performanceof CDI when optimizing the water consumption, but encounter problems withthe experimental setup. The problems affected the results negative such thatthe posed problem statement could not be reliable answered. However, theproblems are related to first time execution of the experiment and providesvaluable insight to successfully execute a identical or similar experiment.</p>

corrected abstract:
<p>Drinkable water is a necessary resource for human beings, unfortunately the supply is not satisfying the increasing demand. Recent requirement for economically sustainable and energy conservative desalination of water have revived the interest of a method first mentioned in the 1960s, know as “electrochemical demineralization”. Today is the method referred to as ”Capacitive Deionization” or CDI for short, and there are commercial solutions emerging on the market. The adoption is still in the early stage and faces engineering challenges. The experiment performed in this thesis tries to quantify the performance of CDI when optimizing the water consumption, but encounter problems with the experimental setup. The problems affected the results negative such that the posed problem statement could not be reliable answered. However, the problems are related to first time execution of the experiment and provides valuable insight to successfully execute a identical or similar experiment.</p>
----------------------------------------------------------------------
In diva2:1229878 abstract is:
<p>Circular dichroism (CD) spectroscopy, exploiting the wavelength-dependentdifferential absorption of left- and right-handed circularly polarized light, isa popular method of protein characterization. Theoretically computed CDspectra from quantum mechanical computer models of peptides can widen theapplicability of the method. In this work, the usefulness of Time-DependentDensity Functional Theory (TD-DFT) with the CAM-B3LYP functional and6-31+G(d) basis set in obtaining CD spectra of model alanine α-helices 3to 15 residues long is investigated.  It is found that including 10 excitedstates per residue in the TD-DFT calculation resolves the characteristic partof the spectra sufficiently.  However, the results suffer from blueshift andimproper weakness of the 222 nm peak of the characteristic α-helix spectracorresponding to the n → π∗ transition, despite extension of the basis set to6-311++G(d,p) and use of the Polarizable Dielectric Continuum Model totreat solvent effects. In this case, these issues are limiting the usefulness ofTD-DFT for prediction of peptide CD spectra. More advanced methods totreat solvent interaction and benchmarking the performance of the functionalwith higher-level ab initio methods are suggestions for future studies.</p><p>A introduction to electronic structure theory and the use of time-dependentperturbation theory to treat spectroscopy is also given in this work.</p>

corrected abstract:
<p>Circular dichroism (CD) spectroscopy, exploiting the wavelength-dependent differential absorption of left- and right-handed circularly polarized light, is a popular method of protein characterization. Theoretically computed CD spectra from quantum mechanical computer models of peptides can widen the applicability of the method. In this work, the usefulness of Time-Dependent Density Functional Theory (TD-DFT) with the CAM-B3LYP functional and 6-31+G(d) basis set in obtaining CD spectra of model alanine α-helices 3 to 15 residues long is investigated.  It is found that including 10 excited states per residue in the TD-DFT calculation resolves the characteristic part of the spectra sufficiently.  However, the results suffer from blueshift and improper weakness of the 222 nm peak of the characteristic α-helix spectra corresponding to the <em>n → π<sup>∗</sup></em> transition, despite extension of the basis set to 6-311++G(d,p) and use of the Polarizable Dielectric Continuum Model to treat solvent effects. In this case, these issues are limiting the usefulness of TD-DFT for prediction of peptide CD spectra. More advanced methods to treat solvent interaction and benchmarking the performance of the functional with higher-level <em>ab initio</e> methods are suggestions for future studies.</p><p>A introduction to electronic structure theory and the use of time-dependent perturbation theory to treat spectroscopy is also given in this work.</p>
----------------------------------------------------------------------
In diva2:1219135 abstract is:
<p>With computers being used for more applications where commands can be spoken it is useful to findalgorithms which can separate voices from each other so that software can turn spoken words intocommands. In this paper our goal is to describe how Independent Component Analysis (ICA) can beused for separation of voices in cases where we have at least the same number of microphones, atdifferent distances from the speakers, as speakers whose voices we wish to separate, the so called``cocktail party problem". This is done by implementing an ICA algorithm on voice recordingscontaining multiple persons and examining the results. The use of both ICA algorithms result in aclear separation of voices, the advantage of fastICA is that the computations take a fraction of thetime needed for the ML-ICA. Both algorithms can also successfully separate voices when recordingsare made by more microphones than speakers. The algorithms were also able to separate some ofthe voices when there were fewer microphones than speakers which was surprising as thealgorithms have no theoretical guarantee for this.</p>

corrected abstract:
<p>With computers being used for more applications where commands can be spoken it is useful to find algorithms which can separate voices from each other so that software can turn spoken words into commands. In this paper our goal is to describe how Independent Component Analysis (ICA) can be used for separation of voices in cases where we have at least the same number of microphones, at different distances from the speakers, as speakers whose voices we wish to separate, the so called ``cocktail party problem". This is done by implementing an ICA algorithm on voice recordings containing multiple persons and examining the results. The use of both ICA algorithms result in a clear separation of voices, the advantage of fastICA is that the computations take a fraction of the time needed for the ML-ICA. Both algorithms can also successfully separate voices when recordings are made by more microphones than speakers. The algorithms were also able to separate some of the voices when there were fewer microphones than speakers which was surprising as the algorithms have no theoretical guarantee for this.</p>
----------------------------------------------------------------------
In diva2:1216824 abstract is:
<p>The core of this project focuses on how to makeaerial vehicles fly autonomously from an initial position to agoal. This is done by making a mathematical model for the UAV,a brief study of the sensors needed to estimate the UAVs state,then designing an LQR controller for the trajectory trackingand finally using an artificial potential field function for thenavigation. The mathematical model is done by studying thekinematics and dynamics for a single UAV, it is then linearisedand the system’s observability and controllability are checked todevelop the LQR. We conduct computer simulations to test thetheoretical findings and evaluate the proposed methods. Finally,we conclude the paper with a discussion and results, and providedirections and ideas to do further research on the topic.</p>

corrected abstract:
<p>The core of this project focuses on how to make aerial vehicles fly autonomously from an initial position to a goal. This is done by making a mathematical model for the UAV, a brief study of the sensors needed to estimate the UAVs state, then designing an LQR controller for the trajectory tracking and finally using an artificial potential field function for the navigation. The mathematical model is done by studying the kinematics and dynamics for a single UAV, it is then linearised and the system’s observability and controllability are checked to develop the LQR. We conduct computer simulations to test the theoretical findings and evaluate the proposed methods. Finally, we conclude the paper with a discussion and results, and provide directions and ideas to do further research on the topic.</p>
----------------------------------------------------------------------
In diva2:1216812 abstract is:
<p>This bachelor's degree project is about designing an airplane for high-altitude rocket launch.The rocket shall carry the satellite PICARD weighing 143 kg and will be fired up to low earthorbit (LEO).The aircraft's payload, the mass of the rocket, is three tons. The rocket is separated above theBaltic Sea, therefore, a range of 600 𝑘𝑚 and 30 minute endurance is required. For the rocketto reach the low earth orbit as quickly as possible, it is released 10 𝑘𝑚 above sea level.The aircraft is designed to mimic and perform as a fighter aircraft with high maneuverability.Therefore, the aircraft is assumed to have a climb rate of 2 km/min as well as take-off andlanding distance 460 m and 700 m respectively. The airplane cruise speed is set to Mach 0.9.Stall speed is assumed to be 200 km/h.This resulted in a total airplane mass of 19 𝑡𝑜𝑛𝑠, 66.2 𝑚2 wing reference area, 14 mwingspan, tail area of 13.8 𝑚2 and 24.1 𝑚2 respectively. The engines of the aircraft resultedin 2x F404-GE-402 turbofan jet engines manufactured by General Electric and giving theaircraft 158 kN propulsion.</p>

corrected abstract:
<p>This bachelor's degree project is about designing an airplane for high-altitude rocket launch. The rocket shall carry the satellite PICARD weighing 143 kg and will be fired up to low earth orbit (LEO).</p><p>The aircraft's payload, the mass of the rocket, is three tons. The rocket is separated above the Baltic Sea, therefore, a range of 600 𝑘𝑚 and 30 minute endurance is required. For the rocket to reach the low earth orbit as quickly as possible, it is released 10 𝑘𝑚 above sea level.</p><p>The aircraft is designed to mimic and perform as a fighter aircraft with high maneuverability. Therefore, the aircraft is assumed to have a climb rate of 2 km/min as well as take-off and landing distance 460 m and 700 m respectively. The airplane cruise speed is set to Mach 0.9. Stall speed is assumed to be 200 km/h.</p><p>This resulted in a total airplane mass of 19 𝑡𝑜𝑛𝑠, 66.2 𝑚<sup>2</sup> wing reference area, 14 m wingspan, tail area of 13.8 𝑚<sup>2</sup> and 24.1 𝑚<sup>2</sup> respectively. The engines of the aircraft resulted in 2x F404-GE-402 turbofan jet engines manufactured by General Electric and giving the aircraft 158 kN propulsion.</p>

Note: A number of the units use characters from the Unicode Mathematical Alphanumeric Symbols block.
----------------------------------------------------------------------
In diva2:1216708 abstract is:
<p>In this project, we aim to find a method for obtainingthe factors in a bull/bear market factor model for asset returnand variance, given an optimal portfolio. The proposed methodwas derived using the Karush-Kuhn-Tucker (KKT) conditionsfor optimal solutions to the convex Markowitz portfolio selectionproblem. For synthetic data where all necessary parameters wereknown exactly, the method could give bounds on the factors. Theexact values of the factors were obtained when short selling wasallowed, and in some instances when short selling was forbidden.The method was evaluated on real-world data with varyingresults, possibly due to estimation errors and invalid assumptionsabout the model of the investor.I. INTRODUC</p>

corrected abstract:
<p>In this project, we aim to find a method for obtaining the factors in a bull/bear market factor model for asset return and variance, given an optimal portfolio. The proposed method was derived using the Karush-Kuhn-Tucker (KKT) conditions for optimal solutions to the convex Markowitz portfolio selection problem. For synthetic data where all necessary parameters were known exactly, the method could give bounds on the factors. The exact values of the factors were obtained when short selling was allowed, and in some instances when short selling was forbidden. The method was evaluated on real-world data with varying results, possibly due to estimation errors and invalid assumptions about the model of the investor.</p>
----------------------------------------------------------------------
In diva2:1188275 abstract is:
<p>Reinforced rubber is thanks to its elastic and dissipative properties found in industrialapplications such as isolators, flexible joints and tires. Its dissipative propertied comes from material related losses which have the effect that energy invested when deforming the material is not retained when returning it back to its initial state. The materiallosses are in turn caused by interactions in the material on a level below the micro scale.These interaction forms a macro stress strain response that is dependent on both strainamplitude, strain rate and temperature.It is thus a challenge to accurately model components made of reinforced rubber andand features of interest related to them, such as the rolling resistance for a tire. It is also difficult to device general design guide lines for such components due to rubbers many and complex dependencies and a simple accurate phenomenological model for modeling these properties are highly sought for in industry today.This thesis presents a method for modeling the strain amplitude and strain rate behavior for cyclically loaded rubber along with a method of choosing its material parameters.The proposed modeling technique results in a model with the same frequencydependency over all strain rates. An approximation which is shown to be valid over a few decades of strain amplitudes and rates and is believed useful for many industrialapplications. The material model presented can in addition be implemented in commercial FEsoftwares by using only pre-defined material models. This was achieved by implementationof the overlay method. The thesis also presents a method for how to implement the modeling technique in simulations with purpose to determine the rolling resistance of a truck tyre.</p>

corrected abstract:
<p>Reinforced rubber is thanks to its elastic and dissipative properties found in industrial applications such as isolators, flexible joints and tires. Its dissipative propertied comes from material related losses which have the effect that energy invested when deforming the material is not retained when returning it back to its initial state. The material losses are in turn caused by interactions in the material on a level below the micro scale. These interaction forms a macro stress strain response that is dependent on both strain amplitude, strain rate and temperature.</p><p>It is thus a challenge to accurately model components made of reinforced rubber and and features of interest related to them, such as the rolling resistance for a tire. It is also difficult to device general design guide lines for such components due to rubbers many and complex dependencies and a simple accurate phenomenological model for modeling these properties are highly sought for in industry today.</p><p>This thesis presents a method for modeling the strain amplitude and strain rate behavior for cyclically loaded rubber along with a method of choosing its material parameters. The proposed modeling technique results in a model with the same frequency dependency over all strain rates. An approximation which is shown to be valid over a few decades of strain amplitudes and rates and is believed useful for many industrial applications.</p><p>The material model presented can in addition be implemented in commercial FE-softwares by using only pre-defined material models. This was achieved by implementation of the overlay method. The thesis also presents a method for how to implement the modeling technique in simulations with purpose to determine the rolling resistance of a truck tyre.</p>
----------------------------------------------------------------------
In diva2:1142926 abstract is:
<p>In this paper deuterium abundances in plasmaenclosing wall materials are analyzed and visualized. The aim isto improve and find new ways of presenting counting statistics.A continuous extension of the sample variance is introducedto incorporate depth resolution when visualizing concentrationas function of depth in a continuous manner. Some ideas ofdesigning measurements of deuterium are discussed, in particularchoosing ion beam energies. By using multiple energies of thehelium-3 beam instead of one, more detector data can be used.With diverse ways of visualizing concentrations derived fromoptimal detector data, one can conveniently quantify deuteriumconcentrations in a surface.</p>

corrected abstract:
<p>In this paper deuterium abundances in plasma enclosing wall materials are analyzed and visualized. The aim is to improve and find new ways of presenting counting statistics. A continuous extension of the sample variance is introduced to incorporate depth resolution when visualizing concentration as function of depth in a continuous manner. Some ideas of designing measurements of deuterium are discussed, in particular choosing ion beam energies. By using multiple energies of the helium-3 beam instead of one, more detector data can be used. With diverse ways of visualizing concentrations derived from optimal detector data, one can conveniently quantify deuterium concentrations in a surface.</p>
----------------------------------------------------------------------
In diva2:1120571 abstract is:
<p>With a recent interest in quantum computers, the properties of quantum mechanicalcounterparts to classical algorithms have been studied in the hope of providing efficientalgorithms for quantum computers. Because of the success of classical random walks inproviding good algorithms on classical computers, attention has been turned to quantumrandom walks, since they may similarly be used to construct efficient probabilisticalgorithms on quantum computers. In this thesis we examine properties of the quantumwalk on the line, in particular the standard deviation and the shape of the probabilitydistribution, and the effect of potentials perturbing the walk. We model these potentialsas rectangular barriers between the walker’s positions and introduce a probability of thewalker failing to perform the step procedure, similar to that of Wong in Ref. [14]. We findthat a potential localized around the starting position leads to an increased standard deviationand makes the walk increasingly ballistic. We also find that uniformly distributedrandom potentials have the general effect of localizing the distribution, similar to that ofAnderson localization.</p>

corrected abstract:
<p>With a recent interest in quantum computers, the properties of quantum mechanical counterparts to classical algorithms have been studied in the hope of providing efficient algorithms for quantum computers. Because of the success of classical random walks in providing good algorithms on classical computers, attention has been turned to quantum random walks, since they may similarly be used to construct efficient probabilistic algorithms on quantum computers. In this thesis we examine properties of the quantum walk on the line, in particular the standard deviation and the shape of the probability distribution, and the effect of potentials perturbing the walk. We model these potentials as rectangular barriers between the walker’s positions and introduce a probability of the walker failing to perform the step procedure, similar to that of Wong in Ref. [14]. We find that a potential localized around the starting position leads to an increased standard deviation and makes the walk increasingly ballistic. We also find that uniformly distributed random potentials have the general effect of localizing the distribution, similar to that of Anderson localization.</p>
----------------------------------------------------------------------
In diva2:1120371 abstract is:
<p>In this thesis we study gravitational waves in the domain of linearised general relativity. Wepresent the fundamental ideas and theory of general relativity, then, using the traditionalmeans of quadrupole approximation, we derive an expression for the power radiated as gravitationalwaves in a binary system. We limit ourselves to binary systems in the Newtonianlimit, and can therefore use Kepler’s laws in our calculations. We then use the derived expressionto explicitly predict the power radiated by the Sun-planet systems in our solar systemspecifically, regarding each system as a binary consisting of the planet and the Sun. Finally,we calculate the resulting orbital decay and find an expression for the time it would take forthe bodies to come into contact if gravitational radiation were the only means by which thesystem lost energy. As expected, the power radiated by gravitational waves is found to bevery small for systems in the Newtonian limit, and the corresponding time until impact isfound to be on the order of many times the age of the Universe.</p>

corrected abstract:
<p>In this thesis we study gravitational waves in the domain of linearised general relativity. We present the fundamental ideas and theory of general relativity, then, using the traditional means of quadrupole approximation, we derive an expression for the power radiated as gravitational waves in a binary system. We limit ourselves to binary systems in the Newtonian limit, and can therefore use Kepler’s laws in our calculations. We then use the derived expression to explicitly predict the power radiated by the Sun-planet systems in our solar system specifically, regarding each system as a binary consisting of the planet and the Sun. Finally, we calculate the resulting orbital decay and find an expression for the time it would take for the bodies to come into contact if gravitational radiation were the only means by which the system lost energy. As expected, the power radiated by gravitational waves is found to be very small for systems in the Newtonian limit, and the corresponding time until impact is found to be on the order of many times the age of the Universe.</p>
----------------------------------------------------------------------
In diva2:1111560 - missing space in title:
"Design and models optimisation of asailing yacht dynamic simulator"
==>
"Design and models optimisation of a sailing yacht dynamic simulator"

abstract is:
<p>This master thesis is concerned with the design and optimisation of a dynamic velocity prediction programmefor high performance sailing yachts. The simulator uses response surfaces methods, maximising the computationaleciency. Insights are given on dynamic theoretical aspects and models are discussed with respect to thestudied ships, 100 feet oshore racing trimarans. The consistency of the currently used models is assessed, andfeasible solutions and methods are proposed and implemented as solutions to the identied defaults.The simulator is subsequently used on a concrete case with the objective of assessing the eects of precise geometricalfeatures of appendages (rudders, foils and boards) on the ship dynamical stability properties. Area, extension,cant angle and tip-shaft angle in particular are studied. Tests cases are developed and multi-parametersstudies carried out. Dynamic results are compared to usual static stability criteria. Simulations show someinconsistencies of dynamic responses with the behaviour expected from the static criteria. Such processes allowbetter understanding of the design and scantling of the studied appendages.</p>

corrected abstract:
<p>This master thesis is concerned with the design and optimisation of a dynamic velocity prediction programme for high performance sailing yachts. The simulator uses response surfaces methods, maximising the computational efficiency. Insights are given on dynamic theoretical aspects and models are discussed with respect to the studied ships, 100 feet offshore racing trimarans. The consistency of the currently used models is assessed, and feasible solutions and methods are proposed and implemented as solutions to the identified defaults.</p><p>The simulator is subsequently used on a concrete case with the objective of assessing the effects of precise geometrical features of appendages (rudders, foils and boards) on the ship dynamical stability properties. Area, extension, cant angle and tip-shaft angle in particular are studied. Tests cases are developed and multi-parameters studies carried out. Dynamic results are compared to usual static stability criteria. Simulations show some inconsistencies of dynamic responses with the behaviour expected from the static criteria. Such processes allow better understanding of the design and scantling of the studied appendages.</p>
----------------------------------------------------------------------
In diva2:1109484 abstract is:
<p>Chute aerators are constructed to protect spillways from cavitation damage.They function by launching the water ow as a jet and supplying airunderneath since having air entrained into the water is an eective way tomitigate cavitation. This report looks at Bergeforsen, a dam in northernSweden, and its spillway as a basis for investigating how altering the aerator'soutlet alters its performance. Five dierent designs are tested using CFD with ANSYS Fluent 17. The designs are evaluated with regard to totalair ow, air ow distribution, duct pressure distribution, cavity length, andair concentration in water jet.It was found that designs with more even duct pressure distribution, thattransported air further toward the spillway center, had somewhat reducedtotal air supply compared to designs that released more of its air in theearly part of the duct. Consequently there appear to be a trade o betweeneectively distributing the air and providing more air ow. The currentdesign used by Bergeforsen strikes this balance quite well, better than theother tested designs.</p>

corrected abstract:
<p>Chute aerators are constructed to protect spillways from cavitation damage. They function by launching the water flow as a jet and supplying air underneath since having air entrained into the water is an effective way to mitigate cavitation. This report looks at Bergeforsen, a dam in northern Sweden, and its spillway as a basis for investigating how altering the aerator's outlet alters its performance. Five different designs are tested using CFD with ANSYS Fluent 17. The designs are evaluated with regard to total air flow, air flow distribution, duct pressure distribution, cavity length, and air concentration in water jet.</p><p>It was found that designs with more even duct pressure distribution, that transported air further toward the spillway center, had somewhat reduced total air supply compared to designs that released more of its air in the early part of the duct. Consequently there appear to be a trade off between effectively distributing the air and providing more air flow. The current design used by Bergeforsen strikes this balance quite well, better than the other tested designs.</p>
----------------------------------------------------------------------
In diva2:1083258 abstract is:
<p>In the thesis, a lifting body has been designed aiming to generate lift force for the pentacopter,called TILT LR (Long Range), at higher velocities during flights to improve the aerodynamicperformances. The configuration, which is used as the skeleton of the long range drone for upto 75 kilometers flights, is based upon a tilting system allowing the rotors to rotate around theirown axis in both pitch and roll angles. This offers the possibility to the TILT LR flying withoutany vertical excess thrust at a proper angle of attack and velocity. This new drone can be directlyapplied to missions require long flight time or cover long distance, such as Search &amp; Rescue(SAR), power lines and off-shore structures inspection, fire monitoring or surveillance.Several main CAD models have been created during the process of design and presented in thereport together with the final design. For each model in the process, CFD simulations have beenapplied to observe the behaviors of the flows around the surfaces of the body during steadyflights, followed by a brief analysis for further modification. A series of simulations withvarying velocities and angle of attack have been performed for the final design, analyzing itsperformances under different air conditions. Flight envelope of the design has been presentedalso, together with some ideas of possible further studies on the pentacopter.</p>

corrected abstract:
<p>In the thesis, a lifting body has been designed aiming to generate lift force for the pentacopter, called TILT LR (Long Range), at higher velocities during flights to improve the aerodynamic performances. The configuration, which is used as the skeleton of the long range drone for up to 75 kilometers flights, is based upon a tilting system allowing the rotors to rotate around their own axis in both pitch and roll angles. This offers the possibility to the TILT LR flying without any vertical excess thrust at a proper angle of attack and velocity. This new drone can be directly applied to missions require long flight time or cover long distance, such as Search &amp; Rescue (SAR), power lines and off-shore structures inspection, fire monitoring or surveillance.</p><p>Several main CAD models have been created during the process of design and presented in the report together with the final design. For each model in the process, CFD simulations have been applied to observe the behaviors of the flows around the surfaces of the body during steady flights, followed by a brief analysis for further modification. A series of simulations with varying velocities and angle of attack have been performed for the final design, analyzing its performances under different air conditions. Flight envelope of the design has been presented also, together with some ideas of possible further studies on the pentacopter.</p>
----------------------------------------------------------------------
In diva2:852969 abstract is:
<p>This bachelor’s thesis aims to investigate the possibility of reconstructing the traditionalJetpack to a more environmental-friendly version, powered by an electric propulsion systemand Li-Ion batteries. Using existing methods of conceptual aircraft design the geometricalcharacteristics of the Jetpack are chosen. With the chosen geometry the four forces acting onan aircraft at any given moment can be obtained as a function of time. These are latersimulated for two different approaches of starting the Jetpack with the conclusion that the bestway of starting this Jetpack, standing on the ground, is with an initial climb angle. TheJetpack then continuously levels out to reach a desired cruise height.</p><p>Different batteries are compared from two different sources and it is shown that, with thebattery having the best energy density (370 Wh/kg), the total flight time is 4.7 minutes whenusing the maximum continuous power output of the engines, at a maximum velocity of 178m/s. Another alternative is to use the optimal velocity for the highest lift-to-drag ratio of 48m/s in order to achieve optimal range, after reaching the maximum velocity. In this case theflight time can be as long as 89 minutes.</p><p>The conclusion of the project is that the conceptual design of the Jetpack is successful andthat further work is to be made in order to design and construct it within a couple of years.</p>

corrected abstract:
<p>This bachelor’s thesis aims to investigate the possibility of reconstructing the traditional Jetpack to a more environmental-friendly version, powered by an electric propulsion system and Li-Ion batteries. Using existing methods of conceptual aircraft design the geometrical characteristics of the Jetpack are chosen. With the chosen geometry the four forces acting on an aircraft at any given moment can be obtained as a function of time. These are later simulated for two different approaches of starting the Jetpack with the conclusion that the best way of starting this Jetpack, standing on the ground, is with an initial climb angle. The Jetpack then continuously levels out to reach a desired cruise height.</p><p>Different batteries are compared from two different sources and it is shown that, with the battery having the best energy density (370 Wh/kg), the total flight time is 4.7 minutes when using the maximum continuous power output of the engines, at a maximum velocity of 178 m/s. Another alternative is to use the optimal velocity for the highest lift-to-drag ratio of 48 m/s in order to achieve optimal range, after reaching the maximum velocity. In this case the flight time can be as long as 89 minutes.</p><p>The conclusion of the project is that the conceptual design of the Jetpack is successful and that further work is to be made in order to design and construct it within a couple of years.</p>
----------------------------------------------------------------------
In diva2:784019 abstract is:
<p>The forces caused by the high frequency vehicle-track interaction have a great impact on thetrack maintenance. They should be represented in vehicle-track models in order to predicttheir impact. As a result, a multi-body system (MBS) should be extended with an advancedtrack model. The MBS represents the vehicle and the wheel-rail contact with a great accuracy.The track will be modeled in two different ways: a moving track model and a continuous trackmodel which is a finite element modeling (FEM). The first one will be a lumped-mass model.The most advanced system will be the second one, which is a MBS-FEM representation andoffers a great precision to represent the high frequency dynamical properties. The system willbe used to simulate pertinent phenomena such as a wheelflats, corrugations and rail joint.Based on the literature and the measurements, the model is validated in a wider frequencyrange than the one currently used (0-20Hz). The results given by both models are close tothe literature and the measurement.</p>

corrected abstract:
<p>The forces caused by the high frequency vehicle-track interaction have a great impact on the track maintenance. They should be represented in vehicle-track models in order to predict their impact. As a result, a multi-body system (MBS) should be extended with an advanced track model. The MBS represents the vehicle and the wheel-rail contact with a great accuracy. The track will be modeled in two different ways: a moving track model and a continuous track model which is a finite element modeling (FEM). The first one will be a lumped-mass model. The most advanced system will be the second one, which is a MBS-FEM representation and offers a great precision to represent the high frequency dynamical properties. The system will be used to simulate pertinent phenomena such as a wheelflats, corrugations and rail joint. Based on the literature and the measurements, the model is validated in a wider frequency range than the one currently used (0-20Hz). The results given by both models are close to the literature and the measurement.</p>
----------------------------------------------------------------------
In diva2:721675 abstract is:
<p>This study investigates the differences in calculationof exposure at default between the current exposure method (CEM) and the newstandardized approach for measuring counterparty credit risk exposures (SA-CCR)for over the counter (OTC) derivatives. The study intends to analyze theconsequence of the usage of different approaches for netting as well as the differencesin EAD between asset classes. After implementing both models and calculating EADon real trades of a Swedish commercial bank it was obvious that SA-CCR has ahigher level of complexity than its predecessor. The results from this studyindicate that SA-CCR gives a lower EAD than CEM because of the higherrecognition of netting but higher EAD when netting is not allowed. Foreignexchange derivatives are affected to a higher extent than interest ratederivatives in this particular study. Foreign exchange derivatives got lowerEAD both when netting was allowed and when netting was not allowed under SA-CCR.A change of method for calculating EAD from CEM to SA-CCR could result in lowerminimum capital requirements</p>

corrected abstract:
<p>This study investigates the differences in calculation of exposure at default between the current exposure method (CEM) and the new standardized approach for measuring counterparty credit risk exposures (SA-CCR) for over the counter (OTC) derivatives. The study intends to analyze the consequence of the usage of different approaches for netting as well as the differences in EAD between asset classes. After implementing both models and calculating EAD on real trades of a Swedish commercial bank it was obvious that SA-CCR has a higher level of complexity than its predecessor. The results from this study indicate that SA-CCR gives a lower EAD than CEM because of the higher recognition of netting but higher EAD when netting is not allowed. Foreign exchange derivatives are affected to a higher extent than interest rate derivatives in this particular study. Foreign exchange derivatives got lower EAD both when netting was allowed and when netting was not allowed under SA-CCR. A change of method for calculating EAD from CEM to SA-CCR could result in lower minimum capital requirements.</p>
----------------------------------------------------------------------
In diva2:704293 abstract is:
<p>The Rational Covariance Extension Problem is a problemin applied mathematics where one tries to find a rational spectral density thatmatches a finite covariance sequence. Applications of this can be used in areaslike speech- and image-processing. This problem has been studied intensivelyover the last decades and recently a related problem, the Circulant RationalCovariance Extension Problem, was solved. This version of the problem dealswith periodic stochastic sequences, and was shown to be a natural way toapproximate the solution to the original problem. Here we look at the specialcase when the process in question is skew-periodic, and show that also in thiscase a unique solution to the problem exists. Moreover we develop numerical solversfor both the periodic and the skew-periodic problem, and use these algorithms toapproximate the spectrum from a speech signal.</p>

corrected abstract:
<p>The Rational Covariance Extension Problem is a problem in applied mathematics where one tries to find a rational spectral density that matches a finite covariance sequence. Applications of this can be used in areas like speech- and image-processing. This problem has been studied intensively over the last decades and recently a related problem, the Circulant Rational Covariance Extension Problem, was solved. This version of the problem deals with periodic stochastic sequences, and was shown to be a natural way to approximate the solution to the original problem. Here we look at the special case when the process in question is skew-periodic, and show that also in this case a unique solution to the problem exists. Moreover we develop numerical solvers for both the periodic and the skew-periodic problem, and use these algorithms to approximate the spectrum from a speech signal.</p>
----------------------------------------------------------------------
In diva2:402293 abstract is:
<p>This thesis is concerned with the classification and effect of cold lap weld defect onthe fatigue strength of a welded structure. Cold lap is a type of weld defect whichoccurs when molten metal does not completely fuse with the cold plate surface. Thisproduces a crack like defect, often very small, which is parallel to the plate. The coldlap weld defect has been classified into three types namely spatter, overlap andspatter-overlap cold lap. Study showed that all three types of cold lap defects have thecorresponded lack of fusion in the interface, which could be considered as initialmacro cracks in different shapes where a possible fatigue crack growth could start.Fatigue life assessment of the above mentioned three types of cold lap defects wascarried out using finite element and crack growth analysis in 2D and 3D. In the 2Danalysis the cold lap defects were modeled as line crack (assuming a/c=0). Based onthe experiments the cold lap defects were visualized as having two probable crackshapes; penny shaped and part through, which required 3D crack growth analysis.Results showed that in 2D analysis the three types of cold lap defects have sameinfluence on fatigue life of the weld. In 3D analysis, the shape of the cold lap defectsdid not show any difference in fatigue life. Overall penny shaped and part throughcracks in 3D analysis predicted 1.75 times longer fatigue life as compared to line crackin 2D analysis.</p>

corrected abstract:
<p>This thesis is concerned with the classification and effect of cold lap weld defect on the fatigue strength of a welded structure. Cold lap is a type of weld defect which occurs when molten metal does not completely fuse with the cold plate surface. This produces a crack like defect, often very small, which is parallel to the plate. The cold lap weld defect has been classified into three types namely spatter, overlap and spatter-overlap cold lap. Study showed that all three types of cold lap defects have the corresponded lack of fusion in the interface, which could be considered as initial macro cracks in different shapes where a possible fatigue crack growth could start.</p><p>Fatigue life assessment of the above mentioned three types of cold lap defects was carried out using finite element and crack growth analysis in 2D and 3D. In the 2D analysis the cold lap defects were modeled as line crack (assuming a/c=0). Based on the experiments the cold lap defects were visualized as having two probable crack shapes; penny shaped and part through, which required 3D crack growth analysis. Results showed that in 2D analysis the three types of cold lap defects have same influence on fatigue life of the weld. In 3D analysis, the shape of the cold lap defects did not show any difference in fatigue life. Overall penny shaped and part through cracks in 3D analysis predicted 1.75 times longer fatigue life as compared to line crack in 2D analysis.</p>
----------------------------------------------------------------------
In diva2:1900955 abstract is:
<p>As the performance of a rocket launcher is closely linked to its flight control system, a significantchallenge in rocket science is the design of attitude control algorithms, to ensure the stabilityof the vehicle, while following a designated trajectory and rejecting external disturbances. Thisreport aims at describing a general method for designing such a controller and finally assessfor its performance. First, a state-of-the-art review of existing attitude control methods isprovided, along with an introduction to linear control theory. Important phenomena influencingthe vehicle, including the rigid-body dynamics, aerodynamics, inertia of the engines, sloshingmodes and bending modes are then introduced. Thereafter, through the example of a given studycase, the parameters describing all of these phenomena are estimated. The linear equationsof motion are then derived and a method for constructing the state-space representation ofthe vehicle and its actuators is presented. Based on this linear model, a step-by-step methodis described to compute a stable PID controller, designed to handle all of the consideredphenomena. Finally, a performance analysis including stability, time response, sensitivity androbustness is conducted to evaluate the behavior of the controller.</p>

corrected abstract:
<p>As the performance of a rocket launcher is closely linked to its flight control system, a significant challenge in rocket science is the design of attitude control algorithms, to ensure the stability of the vehicle, while following a designated trajectory and rejecting external disturbances. This report aims at describing a general method for designing such a controller and finally assess for its performance. First, a state-of-the-art review of existing attitude control methods is provided, along with an introduction to linear control theory. Important phenomena influencing the vehicle, including the rigid-body dynamics, aerodynamics, inertia of the engines, sloshing modes and bending modes are then introduced. Thereafter, through the example of a given study case, the parameters describing all of these phenomena are estimated. The linear equations of motion are then derived and a method for constructing the state-space representation of the vehicle and its actuators is presented. Based on this linear model, a step-by-step method is described to compute a stable PID controller, designed to handle all of the considered phenomena. Finally, a performance analysis including stability, time response, sensitivity and robustness is conducted to evaluate the behavior of the controller.</p>
----------------------------------------------------------------------
In diva2:1817036 abstract is:
<p>This thesis investigates the impact of vibration isolators on circuit boards during harsh vibrationenvironments that occur when they are mounted on the wings of a fighter jet. To examine thisphenomenon, a mathematical model and a simulated model were developed to determine theresonant frequencies of the circuit board under various boundary conditions. Subsequently, theresonant frequencies of the circuit board were validated through experimental tests, allowing forthe establishment of the material properties of the circuit board. In order to prevent structuralfailure, this thesis employs α-gel dampers as the damped attachments for the circuit board.These vibration isolators belong to the category of silicone gel dampers and were evaluatedthrough experimental vibration testing. The two employed vibration isolators are denoted asmodels A1 and A2, exhibiting respective damping ratios of 0.1 and 0.05. By utilizing thesevibration isolators during the experimental vibration tests, the structure demonstrated resilienceagainst natural frequency coupling, thereby preventing failure.</p>

corrected abstract:
<p>This thesis investigates the impact of vibration isolators on circuit boards during harsh vibration environments that occur when they are mounted on the wings of a fighter jet. To examine this phenomenon, a mathematical model and a simulated model were developed to determine the resonant frequencies of the circuit board under various boundary conditions. Subsequently, the resonant frequencies of the circuit board were validated through experimental tests, allowing for the establishment of the material properties of the circuit board. In order to prevent structural failure, this thesis employs α-gel dampers as the damped attachments for the circuit board. These vibration isolators belong to the category of silicone gel dampers and were evaluated through experimental vibration testing. The two employed vibration isolators are denoted as models A1 and A2, exhibiting respective damping ratios of 0.1 and 0.05. By utilizing these vibration isolators during the experimental vibration tests, the structure demonstrated resilience against natural frequency coupling, thereby preventing failure.</p>
----------------------------------------------------------------------
In diva2:1779448 abstract is:
<p>This study investigates the performance of two keypoint detection algorithms, SIFTand LoFTR, for vehicle re-recognition on a 2+1 road in Täby, utilizing three differentmethods: proportion of matches, ”gates” based on the values of the features andSupport Vector Machines (SVM). Data was collected from four strategically placedcameras, with a subset of the data manually annotated and divided into training,validation, and testing sets to minimize overfitting and ensure generalization. TheF1-score was used as the primary metric to evaluate the performance of the variousmethods. Results indicate that LoFTR outperforms SIFT across all methods, with theSVM method demonstrating the best performance and adaptability. The findings havepractical implications in security, traffic management, and intelligent transportationsystems, and suggest directions for future research in real-time implementation andgeneralization across varied camera placements.</p>

corrected abstract:
<p>This study investigates the performance of two keypoint detection algorithms, SIFT and LoFTR, for vehicle re-recognition on a 2+1 road in Täby, utilizing three different methods: proportion of matches, ”gates” based on the values of the features and Support Vector Machines (SVM). Data was collected from four strategically placed cameras, with a subset of the data manually annotated and divided into training, validation, and testing sets to minimize overfitting and ensure generalization. The F1-score was used as the primary metric to evaluate the performance of the various methods. Results indicate that LoFTR outperforms SIFT across all methods, with the SVM method demonstrating the best performance and adaptability. The findings have practical implications in security, traffic management, and intelligent transportation systems, and suggest directions for future research in real-time implementation and generalization across varied camera placements.</p>
----------------------------------------------------------------------
In diva2:1776821 abstract is:
<p>This report considers an application of mixed-integer disjunctive programming (MIDP)where a theoretical robot can jump from one point to another and where the number ofjumps is to be minimized. The robot is only able to jump to the north, south, east andwest. Furthermore, the robot should also be able to navigate and jump around or across anypotential obstacles on the way. The algorithm for solving this problem is set to terminatewhen the robot has reached a set of end coordinates. The goal of this report is to find amethod for solving this problem and to investigate the time complexity of such a method.The problem is converted to big-M representation and solved numerically. Gurobi is theoptimization solver used in this thesis. The model created and implemented with Gurobiyielded optimal solutions to problems of the form above of varying complexity. For most ofcases tested, the time complexity appeared to be linear, but this is likely due to presolvingperformed by Gurobi before running the optimization. Further tests are needed to determinethe time complexity of Gurobi’s optimization algorithm for this specific type of problem.</p>

corrected abstract:
<p>This report considers an application of mixed-integer disjunctive programming (MIDP) where a theoretical robot can jump from one point to another and where the number of jumps is to be minimized. The robot is only able to jump to the north, south, east and west. Furthermore, the robot should also be able to navigate and jump around or across any potential obstacles on the way. The algorithm for solving this problem is set to terminate when the robot has reached a set of end coordinates. The goal of this report is to find a method for solving this problem and to investigate the time complexity of such a method. The problem is converted to big-M representation and solved numerically. Gurobi is the optimization solver used in this thesis. The model created and implemented with Gurobi yielded optimal solutions to problems of the form above of varying complexity. For most of cases tested, the time complexity appeared to be linear, but this is likely due to presolving performed by Gurobi before running the optimization. Further tests are needed to determine the time complexity of Gurobi’s optimization algorithm for this specific type of problem.</p>
----------------------------------------------------------------------
In diva2:1764270 abstract is:
<p>Among the structural materials under consideration for future lead-cooled fastreactors, special attention is paid to ferritic Fe-10Cr-4Al due to its superior corrosionand erosion protective properties, as well as its insensitivity to liquid metalembrittlement in liquid lead. This thesis gives an inside look into the radiation damageproperties of the alloy and the possible embrittlement scenarios. The samples wereirradiated with 5.5 MeV protons and then tested with a slow strain rate testing rig at375oC and 450oC. The results showed that for Fe-10Cr-4Al irradiated to a peak doseof 0.14 dpa, the total elongation to failure was reduced by 3-5%, compared to theunirradiated samples. Moreover, the mechanical properties (yield strength, ultimatetensile strength, and fracture elongation) of the irradiated samples depend stronglyon temperature. The scanning electron microscopy images show no signs of liquidmetal embrittlement. However, the brittle structures at the edges of the samples couldindicate the existence of hydrogen embrittlement.</p>

corrected abstract:
<p>Among the structural materials under consideration for future lead-cooled fast reactors, special attention is paid to ferritic Fe-10Cr-4Al due to its superior corrosion and erosion protective properties, as well as its insensitivity to liquid metal embrittlement in liquid lead. This thesis gives an inside look into the radiation damage properties of the alloy and the possible embrittlement scenarios. The samples were irradiated with 5.5 MeV protons and then tested with a slow strain rate testing rig at 375ºC and 450ºC. The results showed that for Fe-10Cr-4Al irradiated to a peak dose of 0.14 dpa, the total elongation to failure was reduced by 3-5%, compared to the unirradiated samples. Moreover, the mechanical properties (yield strength, ultimate tensile strength, and fracture elongation) of the irradiated samples depend strongly on temperature. The scanning electron microscopy images show no signs of liquid metal embrittlement. However, the brittle structures at the edges of the samples could indicate the existence of hydrogen embrittlement.</p>
----------------------------------------------------------------------
In diva2:1757046 
abstract is:
<p>The relationship between performance for the Swedish industry and changesin prices and volatility of commodities has been examined using multiple linearregression. The study focuses on how commodity price fluctuations correlate withgross profit growth, measuring company performance. Gross profit as a performancemeasure is contrary to most previous studies that use stock performance as thedependent variable. This study has found two commodities whose prices have asignificant relationship with changes in gross profit for the Swedish industry sector,Brent oil, and platinum. The correlation with Brent oil is the most reliable one.Surprisingly, Brent oil has a positive relationship with gross profit, even thougha higher oil price is causing more expensive logistics and manufacturing operations,increasing costs of sold goods. This indicates a possible correlation between oil priceand demand for manufactured products; industrial companies can either increaseprices or produce at a high capacity. Regarding the volatility of commodities, nosignificant correlation with gross profits has been found.</p>


corrected abstract:
<p>The relationship between performance for the Swedish industry and changes in prices and volatility of commodities has been examined using multiple linear regression. The study focuses on how commodity price fluctuations correlate with gross profit growth, measuring company performance. Gross profit as a performance measure is contrary to most previous studies that use stock performance as the dependent variable. This study has found two commodities whose prices have a significant relationship with changes in gross profit for the Swedish industry sector, Brent oil, and platinum. The correlation with Brent oil is the most reliable one.</p><p>Surprisingly, Brent oil has a positive relationship with gross profit, even though a higher oil price is causing more expensive logistics and manufacturing operations, increasing costs of sold goods. This indicates a possible correlation between oil price and demand for manufactured products; industrial companies can either increase prices or produce at a high capacity. Regarding the volatility of commodities, no significant correlation with gross profits has been found.</p>
----------------------------------------------------------------------
In diva2:1718318 
abstract is:
<p>The development of quantum machine learning is bridging the way to fault tolerant quantum computation by providing algorithms running on the current noisy intermediate scale quantum devices.However, it is difficult to find use-cases where quantum computers exceed their classical counterpart.The high energy physics community is experiencing a rapid growth in the amount of data physicists need to collect, store, and analyze within the more complex experiments are being conceived.Our work approaches the study of a particle physics event involving the Higgs boson from a quantum machine learning perspective.We compare quantum support vector machine with the best classical kernel method grounding our study in a new theoretical framework based on metrics observing at three different aspects: the geometry between the classical and quantum learning spaces, the dimensionality of the feature space, and the complexity of the ML models.We exploit these metrics as a compass in the parameter space because of their predictive power. Hence, we can exclude those areas where we do not expect any advantage in using quantum models and guide our study through the best parameter configurations.Indeed, how to select the number of qubits in a quantum circuits and the number of datapoints in a dataset were so far left to trial and error attempts.We observe, in a vast parameter region, that the used classical rbf kernel model overtakes the performances of the devised quantum kernels.We include in this study the projected quantum kernel - a kernel able to reduce the expressivity of the traditional fidelity quantum kernel by projecting its quantum state back to an approximate classical representation through the measurement of local quantum systems.The Higgs dataset has been proved to be low dimensional in the quantum feature space meaning that the quantum encoding selected is not enough expressive for the dataset under study.Nonetheless, the optimization of the parameters on all the kernels proposed, classical and quantum, revealed a quantum advantage for the projected kernel which well classify the Higgs boson events and surpass the classical ML model.</p>

corrected abstract:
<p>The development of quantum machine learning is bridging the way to fault tolerant quantum computation by providing algorithms running on the current noisy intermediate scale quantum devices. However, it is difficult to find use-cases where quantum computers exceed their classical counterpart. The high energy physics community is experiencing a rapid growth in the amount of data physicists need to collect, store, and analyze within the more complex experiments are being conceived. Our work approaches the study of a particle physics event involving the Higgs boson from a quantum machine learning perspective. We compare quantum support vector machine with the best classical kernel method grounding our study in a new theoretical framework based on metrics observing at three different aspects: the geometry between the classical and quantum learning spaces, the dimensionality of the feature space, and the complexity of the ML models. We exploit these metrics as a compass in the parameter space because of their predictive power. Hence, we can exclude those areas where we do not expect any advantage in using quantum models and guide our study through the best parameter configurations. Indeed, how to select the number of qubits in a quantum circuits and the number of datapoints in a dataset were so far left to trial and error attempts. We observe, in a vast parameter region, that the used classical rbf kernel model overtakes the performances of the devised quantum kernels. We include in this study the projected quantum kernel - a kernel able to reduce the expressivity of the traditional fidelity quantum kernel by projecting its quantum state back to an approximate classical representation through the measurement of local quantum systems. The Higgs dataset has been proved to be low dimensional in the quantum feature space meaning that the quantum encoding selected is not enough expressive for the dataset under study. Nonetheless, the optimization of the parameters on all the kernels proposed, classical and quantum, revealed a quantum advantage for the projected kernel which well classify the Higgs boson events and surpass the classical ML model.</p>
----------------------------------------------------------------------
In diva2:1674003 
abstract is: 
<p>The student inclusive Green Raven project of the KTH-Aero faculty requireda small blended wing model of their new flying wing design. The small scalemodel will be used for various flight tests. The goal of this specific project was tocreate an internal structure for the small scale model, including an outer shell.Two-dimensional drawings were created and tested in a simulation software.The model was then drawn in cad. Lastly the wing was strength tested inAnsys mechanical. The beams in the structure are made of Scots pine due toits accessibility and good strength to weight ratio. The outer shell is made outof fiberglass. A quick connection between the wing and the main body wasimplemented for easy transportation. All final testing indicate that the finaldesign had sufficient strength regarding the initial load requirements.</p><p> </p>

corrected abstract:
<p>The student inclusive Green Raven project of the KTH-Aero faculty required a small blended wing model of their new flying wing design. The small scale model will be used for various flight tests. The goal of this specific project was to create an internal structure for the small scale model, including an outer shell. Two-dimensional drawings were created and tested in a simulation software. The model was then drawn in cad. Lastly the wing was strength tested in Ansys mechanical. The beams in the structure are made of Scots pine due to its accessibility and good strength to weight ratio. The outer shell is made out of fiberglass. A quick connection between the wing and the main body was implemented for easy transportation. All final testing indicate that the final design had sufficient strength regarding the initial load requirements.</p>
----------------------------------------------------------------------
In diva2:1595164 
abstract is: 
<p>The electrification of automobiles has emerged as the sustainable powertrain solutionto meet United Nations sustainable development goals of sustainable cities andcommunities, affordable and clean energy, and climate action. The success of theelectrification depends on the efficiency of traction motors. Hence, the automobileindustry is dedicated to improving the performance of electrical traction machinesfor high performance and sustainability. The thesis aims to build various electricalmachine’s concept designs and quantify their behaviour on sustainability andperformance.</p><p>The thesis objective is to design Permanent Magnet Synchronous Motor (PMSM),Synchronous Reluctance Motor (SynRM), and Permanent Magnet SynchronousReluctance Motor (PM­SynRM). The thesis work comprises of accurate performanceestimation and optimisation of these electrical machines through a finite element based method. The in­house scripts are developed to estimate the performance, electrical losses, and efficiency of these electrical machines through flexible open-source tools.</p><p>The performance of PMSM with rare-­earth magnet Neodymium Ferrite Boron(NdFeB) and without rare­-earth magnet (ferrite) is done to evaluate the role of bothmagnets in producing torque density. The SynRM is evaluated and optimized usinggenetic algorithms in the thesis. The electrical machines are designed without the useof rare-­earth magnets to eliminate the degradation of the environment and reduce thecost and weight of the motor.</p>

corrected abstract:
<p>The electrification of automobiles has emerged as the sustainable powertrain solution to meet United Nations sustainable development goals of sustainable cities and communities, affordable and clean energy, and climate action. The success of the electrification depends on the efficiency of traction motors. Hence, the automobile industry is dedicated to improving the performance of electrical traction machines for high performance and sustainability. The thesis aims to build various electrical machine’s concept designs and quantify their behaviour on sustainability and performance.</p><p>The thesis objective is to design Permanent Magnet Synchronous Motor (PMSM), Synchronous Reluctance Motor (SynRM), and Permanent Magnet Synchronous Reluctance Motor (PM-SynRM). The thesis work comprises of accurate performance estimation and optimisation of these electrical machines through a finite element based method. The in­house scripts are developed to estimate the performance, electrical losses, and efficiency of these electrical machines through flexible open-source tools.</p><p>The performance of PMSM with rare-earth magnet Neodymium Ferrite Boron (NdFeB) and without rare-earth magnet (ferrite) is done to evaluate the role of both magnets in producing torque density. The SynRM is evaluated and optimized using genetic algorithms in the thesis. The electrical machines are designed without the use of rare-earth magnets to eliminate the degradation of the environment and reduce the cost and weight of the motor.</p>
----------------------------------------------------------------------
In diva2:1583515 
abstract is: 
<p>This project aimed to improve on an existing MATLAB tool used for real-time monitoring of flutterduring flight testing of the Gripen E fighter jet. Flutter is a dynamic aeroelastic phenomenon thatcauses undamped oscillations in a structure, which may lead to structural failure. Because of thepotentially catastrophic consequences of flutter in an aircraft, it must be proven, through flight fluttertesting, to be free of flutter within its flight envelope. To increase flight safety during flutter flighttesting, accelerometer data from the aircraft is monitored in real-time by flight test engineers on theground. Monitoring is done using different tools including the aforementioned MATLAB tool. Thetool was improved by increasing its robustness, correcting existing flaws, and refining its overallgraphical presentation of data. New analysis tools such as filtering of time signals, an airspeedindicator plot, and a custom data cursor were also implemented.</p>

corrected abstract:
<p>This project aimed to improve on an existing MATLAB tool used for real-time monitoring of flutter during flight testing of the Gripen E fighter jet. Flutter is a dynamic aeroelastic phenomenon that causes undamped oscillations in a structure, which may lead to structural failure. Because of the potentially catastrophic consequences of flutter in an aircraft, it must be proven, through flight flutter testing, to be free of flutter within its flight envelope. To increase flight safety during flutter flight testing, accelerometer data from the aircraft is monitored in real-time by flight test engineers on the ground. Monitoring is done using different tools including the aforementioned MATLAB tool. The tool was improved by increasing its robustness, correcting existing flaws, and refining its overall graphical presentation of data. New analysis tools such as filtering of time signals, an airspeed indicator plot, and a custom data cursor were also implemented.</p>
----------------------------------------------------------------------
In diva2:1572329 
abstract is: 
<p>Sustainable urbanisation is a pressing challenge in some parts of the world and cost-efficient and environmentally friendly building materials could become a solution to achieve sustainability. Bamboo has shown promising properties in tensile- and bending strength to be able to substitute conventional building materials. If reinforced concrete gets implemented inside the anisotropic bamboo it further increases compressive strength, raw durability and makes the material more homogeneous.This thesis report analyses and calculates the stress and deformation on these reinforced bamboo beams when used as a roof structure for a solar cell power charging station in Southeast Asia. The calculations were made in ANSYS Mechanical. Different structural designs were exposed to strong wind loads and the results were compared to optimize the usage of the materials.The results show low values of stress and deformation after implementing reinforced concrete in the whole bamboo which indicates that excessivelyreinforced concrete might have been used. The results also show that using bamboo only in the structure gives considerably higher stress and deformation values even reaching critical levels at the edges of the roof and where the roof connects with the structural pillars.By implementing reinforced concrete at only critical areas the amount of stress in the structure can be decreased to a manageable level. If this is done, no critical levels are reached and the arising stress levels in the bamboo fall below a safety factor of 3. With these results, one can argue to decrease the material usage by only using reinforced concrete at critical areas of the structure. By using the natural strength of bamboo and only complementing with concrete and steel where bamboo is weak, the overall environmental impact is kept low but also the costs for producing and transporting these materials.</p>

corrected abstract:
<p>Sustainable urbanisation is a pressing challenge in some parts of the world and cost-efficient and environmentally friendly building materials could become a solution to achieve sustainability. Bamboo has shown promising properties in tensile- and bending strength to be able to substitute conventional building materials. If reinforced concrete gets implemented inside the anisotropic bamboo it further increases compressive strength, raw durability and makes the material more homogeneous.</p><p>This thesis report analyses and calculates the stress and deformation on these reinforced bamboo beams when used as a roof structure for a solar cell power charging station in Southeast Asia. The calculations were made in ANSYS Mechanical. Different structural designs were exposed to strong wind loads and the results were compared to optimize the usage of the materials.</p><p>The results show low values of stress and deformation after implementing reinforced concrete in the whole bamboo which indicates that excessively reinforced concrete might have been used. The results also show that using bamboo only in the structure gives considerably higher stress and deformation values even reaching critical levels at the edges of the roof and where the roof connects with the structural pillars.</p><p>By implementing reinforced concrete at only critical areas the amount of stress in the structure can be decreased to a manageable level. If this is done, no critical levels are reached and the arising stress levels in the bamboo fall below a safety factor of 3. With these results, one can argue to decrease the material usage by only using reinforced concrete at critical areas of the structure. By using the natural strength of bamboo and only complementing with concrete and steel where bamboo is weak, the overall environmental impact is kept low but also the costs for producing and transporting these materials.</p>
----------------------------------------------------------------------
In diva2:1528145 - missing space in title
#Investigation and Analysis ofAircraft System Integration#
==>
#Investigation and Analysis of Aircraft System Integration#

abstract is: 
<p>Aircraft integration plays a pivotal role in the eective aggregation of varioussub-systems into a group of systems. The fundamental categorization of the aircraftintegration process points to technical and managerial aspects. The technical needsare to be managed eectively for an optimal solution. The objective of the thesiswas to investigate these two aspects as being interdependent. First, simulated theaircraft behavior at various congurations, and mapped the results with ight testdata. Subsequently, the conguration’s behavior was assessed as per certicationrequirements. Second, pre-Validation and Verication (V&amp;V) of the Stall WarningSystem (SWS) was conducted to ensure consistent design and performance as perrequirements. Third, managed one of the FCS system’s environmental tests anddeliverables schedule for its certication maturity by utilizing the Critical PathMethod (CPM).</p>

corrected abstract:
<p>Aircraft integration plays a pivotal role in the effective aggregation of various sub-systems into a group of systems. The fundamental categorization of the aircraft integration process points to technical and managerial aspects. The technical needs are to be managed effectively for an optimal solution. The objective of the thesis was to investigate these two aspects as being interdependent. First, simulated the aircraft behavior at various configurations, and mapped the results with flight test data. Subsequently, the configuration’s behavior was assessed as per certification requirements. Second, pre-Validation and Verification (V&amp;V) of the Stall Warning System (SWS) was conducted to ensure consistent design and performance as per requirements. Third, managed one of the FCS system’s environmental tests and deliverables schedule for its certification maturity by utilizing the Critical Path Method (CPM).</p>
----------------------------------------------------------------------
In diva2:1465541 
abstract is: 
<p>In this project, a chassis concept has been developed for a battery-powered autonomousvehicle. The vehicle is intended to be used at an airport for transporting people betweendifferent terminals. The objective is to develop a chassis which is anchored with modernrequirements and futuristic research based on conventional chassis design methods in orderto find an optimal solution for this specific vehicle. Literature studies have been conductedon future batteries, types of chassis, chassis materials, and optimal cross-sections. Thechassis materials have also been analyzed from an environmental perspective and life cycleanalysis (LCA). Based on this, it was found that the “skateboard” chassis model was optimalfor the intended vehicle while Advanced High Strength Steel (AHSS) proved to be the mostsuitable material for the load-bearing structure. It is essential to keep in mind that thisproject has been carried out on a conceptual level within the framework of a degree project.This master thesis project aims to provide a solid benchmark for further development andresearch within the subject.</p>

corrected abstract:
<p>In this project, a chassis concept has been developed for a battery-powered autonomous vehicle. The vehicle is intended to be used at an airport for transporting people between different terminals. The objective is to develop a chassis which is anchored with modern requirements and futuristic research based on conventional chassis design methods in order to find an optimal solution for this specific vehicle. Literature studies have been conducted on future batteries, types of chassis, chassis materials, and optimal cross-sections. The chassis materials have also been analyzed from an environmental perspective and life cycle analysis (LCA). Based on this, it was found that the “skateboard” chassis model was optimal for the intended vehicle while Advanced High Strength Steel (AHSS) proved to be the most suitable material for the load-bearing structure. It is essential to keep in mind that this project has been carried out on a conceptual level within the framework of a degree project. This master thesis project aims to provide a solid benchmark for further development and research within the subject.</p>
----------------------------------------------------------------------
In diva2:1365507 
abstract is: 
<p>Condition monitoring (CM) is widely used in industry, and there is a growing interest in applying CM on rail vehicle systems. Condition based maintenance has the possibility to increase system safety and availability while at the sametime reduce the total maintenance costs.This thesis investigates the feasibility of using condition monitoring of suspension element components, in this case dampers, in rail vehicles. There are different methods utilized to detect degradations, ranging from mathematicalmodelling of the system to pure "knowledge-based" methods, using only large amount of data to detect patterns on a larger scale. In this thesis the latter approach is explored, where acceleration signals are evaluated on severalplaces on the axleboxes, bogieframes and the carbody of a rail vehicle simulation model. These signals are picked close to the dampers that are monitored in this study, and frequency response functions (FRF) are computed between axleboxes and bogieframes as well as between bogieframes and carbody. The idea is that the FRF will change as the condition of the dampers change, and thus act as indicators of faults. The FRF are then fed to different classificationalgorithms, that are trained and tested to distinguish between the different damper faults.This thesis further investigates which classification algorithm shows promising results for the problem, and which algorithm performs best in terms of classification accuracy as well as two other measures. Another aspect explored is thepossibility to apply dimensionality reduction to the extracted indicators (features). This thesis is also looking into how the three performance measures used are affected by typical varying operational conditions for a rail vehicle,such as varying excitation and carbody mass. The Linear Support Vector Machine classifier using the whole feature space, and the Linear Discriminant Analysis classifier combined with Principal Component Analysis dimensionality reduction on the feature space both show promising results for the taskof correctly classifying upcoming damper degradations.</p>

corrected abstract:
<p>Condition monitoring (CM) is widely used in industry, and there is a growing interest in applying CM on rail vehicle systems. Condition based maintenance has the possibility to increase system safety and availability while at the same time reduce the total maintenance costs.</p><p>This thesis investigates the feasibility of using condition monitoring of suspension element components, in this case dampers, in rail vehicles. There are different methods utilized to detect degradations, ranging from mathematical modelling of the system to pure "knowledge-based" methods, using only large amount of data to detect patterns on a larger scale. In this thesis the latter approach is explored, where acceleration signals are evaluated on several places on the axleboxes, bogieframes and the carbody of a rail vehicle simulation model. These signals are picked close to the dampers that are monitored in this study, and frequency response functions (FRF) are computed between axleboxes and bogieframes as well as between bogieframes and carbody. The idea is that the FRF will change as the condition of the dampers change, and thus act as indicators of faults. The FRF are then fed to different classification algorithms, that are trained and tested to distinguish between the different damper faults.</p><p>This thesis further investigates which classification algorithm shows promising results for the problem, and which algorithm performs best in terms of classification accuracy as well as two other measures. Another aspect explored is the possibility to apply dimensionality reduction to the extracted indicators (features). This thesis is also looking into how the three performance measures used are affected by typical varying operational conditions for a rail vehicle, such as varying excitation and carbody mass. The Linear Support Vector Machine classifier using the whole feature space, and the Linear Discriminant Analysis classifier combined with Principal Component Analysis dimensionality reduction on the feature space both show promising results for the task of correctly classifying upcoming damper degradations.</p>
----------------------------------------------------------------------
In diva2:1341553 
abstract is: 
<p>The purpose of this bachelor thesis in applied mathematics and engineering physicsis to develop a model to describe the price of emissions allowances in EuropeanUnion Emission Trading Scheme, EU ETS, a scheme created to systematically re-duce carbon dioxide emissions with market forces in a free market. With the help ofstatistical regression, the core of the project is to explain the price of these emissionallowances by creating a model based on free available data from both microeconomicand macroeconomic factors in Europe. Stock indices, interest rates, electricity price,and inflation rates are examples of factors that were used. Two models were devel-oped, one with data points from mid-2010 to March 2019 and one with data pointsfrom mid-2010 until March 14, 2018, when a new directive was voted through re-garding the premises of the trading scheme. The models were developed throughstatistical methods such as identification of outliers, detection of multicollinearity,cross validation, and bootstrapping in order to create a model as suitable as pos-sible. Afterward, the model’s credibility and legitimacy were discussed, as well aspossible areas of improvement and further studies to improve the model and theunderstanding of the pricing.</p><p> </p>

corrected abstract:
<p>The purpose of this bachelor thesis in applied mathematics and engineering physics is to develop a model to describe the price of emissions allowances in European Union Emission Trading Scheme, EU ETS, a scheme created to systematically reduce carbon dioxide emissions with market forces in a free market. With the help of statistical regression, the core of the project is to explain the price of these emission allowances by creating a model based on free available data from both microeconomic and macroeconomic factors in Europe. Stock indices, interest rates, electricity price, and inflation rates are examples of factors that were used. Two models were developed, one with data points from mid-2010 to March 2019 and one with data points from mid-2010 until March 14, 2018, when a new directive was voted through regarding the premises of the trading scheme. The models were developed through statistical methods such as identification of outliers, detection of multicollinearity, cross validation, and bootstrapping in order to create a model as suitable as possible. Afterward, the model’s credibility and legitimacy were discussed, as well as possible areas of improvement and further studies to improve the model and the understanding of the pricing.</p>
----------------------------------------------------------------------
In diva2:1244654 
abstract is: 
<p>The aim of this project is to investigate accelerometer readings obtained from a modern smartphone,for use within the discipline of material mechanics. Problems that fall within the domain of classicalmechanics are addressed as well. To obtain data, the smartphone’s integrated accelerometer is used. Allmodern smartphones contain these devices. When conducting experiments, data files are created whichare then transferred to a computer for analysis.A series of experiments were conducted. The equipment for each experiment was chosen specifically forbeing common in households. The acceleration may be used directly for determining force, for obtainingvelocity and displacement upon integration, and for calculating stiffness of systems. Frequency analysisof the acceleration was carried out to find the natural frequencies of structures.The results of these experiments are promising. The accelerometer is well suited for use within thesedisciplines.</p>

corrected abstract:
<p>The aim of this project is to investigate accelerometer readings obtained from a modern smartphone, for use within the discipline of material mechanics. Problems that fall within the domain of classical mechanics are addressed as well. To obtain data, the smartphone’s integrated accelerometer is used. All modern smartphones contain these devices. When conducting experiments, data files are created which are then transferred to a computer for analysis.</p><p>A series of experiments were conducted. The equipment for each experiment was chosen specifically for being common in households. The acceleration may be used directly for determining force, for obtaining velocity and displacement upon integration, and for calculating stiffness of systems. Frequency analysis of the acceleration was carried out to find the natural frequencies of structures.</p><p>The results of these experiments are promising. The accelerometer is well suited for use within these disciplines.</p>
----------------------------------------------------------------------
In diva2:1229773 
abstract is: 
<p>This is a project in automated music analysis. The project was based on theunderlying mathematical relations in western tonal music and the discreteFourier transform. Two distinct but similar algorithms for automated analysisof the polyphonic note content in a given recording were developed.They were also implemented in an unoptimized form, being able to interpretand reproduce the polyphonic note content of simpler recorded material, butfailed at more complex recordings. This is partly because of the unoptimizednature of the implemented versions and partly because of shortcomingsin the algorithms. The algorithms presented are meant to be taken as buildingblocks for more refined versions, not necessarily as algorithms ready forimplementation.</p>

corrected abstract:
<p>This is a project in automated music analysis. The project was based on the underlying mathematical relations in western tonal music and the discrete Fourier transform. Two distinct but similar algorithms for automated analysis of the polyphonic note content in a given recording were developed. They were also implemented in an unoptimized form, being able to interpretand reproduce the polyphonic note content of simpler recorded material, but failed at more complex recordings. This is partly because of the unoptimized nature of the implemented versions and partly because of shortcomings in the algorithms. The algorithms presented are meant to be taken as building blocks for more refined versions, not necessarily as algorithms ready for implementation.</p>
----------------------------------------------------------------------
In diva2:1189537 
abstract is: 
<p>In the frame of increasing flight safety, finite element models are developed to computethe stresses in critical parts. The results obtained often complete the ones derived from fullscale experimental tests and analytical estimations. A finite element model is particularlyuseful to simulate many different flight configurations that can not be tested experimentally.This paper presents the different stages in the development of a finite element model ofa rotor flapping mass. On a helicopter the flapping mass makes the connection betweenblades and rotor hub. This study particularly focuses on the estimation of the fatiguelimit of a composite component. This component, called roving winding, is particularlycritical as it sustains most of the loads applied on the flapping mass. Getting an accuraterepresentation of the stress distribution in the roving was the main objective. The resultsderived from the model presented here were compared to experimental ones to ensure itsaccuracy. The confidence in the model obtained makes possible its use to evaluate theimpact of some material change or geometry modifications. The model also permitted toevaluate the impact of some productions defects on a composite part.</p>

corrected abstract:
<p>In the frame of increasing flight safety, finite element models are developed to compute the stresses in critical parts. The results obtained often complete the ones derived from full scale experimental tests and analytical estimations. A finite element model is particularly useful to simulate many different flight configurations that can not be tested experimentally. This paper presents the different stages in the development of a finite element model of a rotor flapping mass. On a helicopter the flapping mass makes the connection between blades and rotor hub. This study particularly focuses on the estimation of the fatigue limit of a composite component. This component, called roving winding, is particularly critical as it sustains most of the loads applied on the flapping mass. Getting an accurate representation of the stress distribution in the roving was the main objective. The results derived from the model presented here were compared to experimental ones to ensure its accuracy. The confidence in the model obtained makes possible its use to evaluate the impact of some material change or geometry modifications. The model also permitted to evaluate the impact of some productions defects on a composite part.</p>
----------------------------------------------------------------------
In diva2:1142924 
abstract is: 
<p>Time-of-flight elastic recoil detection analysis (ToFERDA)is a method for material analysis which has provenadvantageous when examining wall samples from fusion devicesas well as for tracking tracer isotopes such as deuterium, oxygen-18 and nitrogen-15. When ToF-ERDA data is processed tocalculate the composition of a material, the detection efficiencyis used to compensate for the lower detection rate of lighterelements. The aim of this project is to examine how the efficiencyof the time-of-flight detector in a ToF-ERDA experimental setupdepends on its settings and determine efficiency parameters touse when producing atomic composition depth profiles with ToFERDA.Experiments were performed at the Tandem Laboratoryin Uppsala. The outcome is a suggested set of changes toexperimental settings and a four-parameter fit to measureddetection efficiencies for efficiency compensation.</p>

corrected abstract:
<p>Time-of-flight elastic recoil detection analysis (ToF-ERDA) is a method for material analysis which has proven advantageous when examining wall samples from fusion devices as well as for tracking tracer isotopes such as deuterium, oxygen-18 and nitrogen-15. When ToF-ERDA data is processed to calculate the composition of a material, the detection efficiency is used to compensate for the lower detection rate of lighter elements. The aim of this project is to examine how the efficiency of the time-of-flight detector in a ToF-ERDA experimental setup depends on its settings and determine efficiency parameters to use when producing atomic composition depth profiles with ToFERDA. Experiments were performed at the Tandem Laboratory in Uppsala. The outcome is a suggested set of changes to experimental settings and a four-parameter fit to measured detection efficiencies for efficiency compensation.</p>
----------------------------------------------------------------------
In diva2:1120590 
abstract is: 
<p>The Swedish stock market consists of roughly 750 companies listed on fivedifferent markets. Out of all those companies a significant portion are rarelytraded. Stocks where the trading activity is low not only present a liquidityproblem to shareholders and potential investors but also affects the reputation ofthe traded company. A company whose shares are not actively traded does nothave a market that actively puts a value on the company.This study aims to interpret how daily trade volumes can be explained by bothcategorical and numerical variables associated with the companies listed inSweden.This study, contrary to popular belief, shows that the market of the listed stock isto a large degree irrelevant when explaining daily trade volumes of the stockslisted in Sweden. The study instead reveals the importance of factors such asshareholder structure, free float and number of outstanding shares in a company.</p>

corrected abstract:
<p>The Swedish stock market consists of roughly 750 companies listed on five different markets. Out of all those companies a significant portion are rarely traded. Stocks where the trading activity is low not only present a liquidity problem to shareholders and potential investors but also affects the reputation of the traded company. A company whose shares are not actively traded does not have a market that actively puts a value on the company.</p><p>This study aims to interpret how daily trade volumes can be explained by both categorical and numerical variables associated with the companies listed in Sweden.</p><p>This study, contrary to popular belief, shows that the market of the listed stock is to a large degree irrelevant when explaining daily trade volumes of the stocks listed in Sweden. The study instead reveals the importance of factors such as shareholder structure, free float and number of outstanding shares in a company.</p>
----------------------------------------------------------------------
In diva2:1120565 
abstract is: 
<p>This paper describes methods to compute the decomposition type of representationsof some quivers that appear in topological data analysis. Specically, these are theAn-quivers, and the quivers CL(f) and CL(f; f). A background is given to explain theappearance of these quivers in the data analysis setting. The theoretical backgroundcovers the basic category theoretical concepts used to dene decomposition of repre-sentations, as well as the necessary tools used to nd indecomposable representations.A brief review of how these tools can be applied on An-quivers is given. The mainresults in the paper are the matrices presented at the ends of sections 4 and 5, whichgive linear relations between vectors representing the decomposition types of repre-sentations of these quivers and vectors of numerical invariants of the representations.These invariants are simply the dimensions of some subspaces of the vector spaces inthe representation, and are therefore easily calculated. Finally, more general laddersand grids are considered, and it it proven that the only grids of nite representationtype are the ladders with at most four rungs.</p>

corrected abstract:
<p>This paper describes methods to compute the decomposition type of representations of some quivers that appear in topological data analysis. Specifically, these are the <emZ>A<sub>𝑛</sub></em>-quivers, and the quivers CL(𝑓) and CL(𝑓, 𝑓). A background is given to explain the appearance of these quivers in the data analysis setting. The theoretical background covers the basic category theoretical concepts used to define decomposition of representations, as well as the necessary tools used to find indecomposable representations. A brief review of how these tools can be applied on <em>A<sub>𝑛</sub></em>-quivers is given. The main results in the paper are the matrices presented at the ends of sections 4 and 5, which give linear relations between vectors representing the decomposition types of representations of these quivers and vectors of numerical invariants of the representations. These invariants are simply the dimensions of some subspaces of the vector spaces in the representation, and are therefore easily calculated. Finally, more general ladders and grids are considered, and it it proven that the only grids of finite representation type are the ladders with at most four rungs.</p>
----------------------------------------------------------------------
In diva2:644628 - missing space in title:
"High performance adaptive finite elementmodeling of complex CAD geometry"
==>
"High performance adaptive finite element modeling of complex CAD geometry"


abstract is: 
<p>CAD (Computer Aided Design) and finite elementanalysis are of fundamental importance for numerical simulations. The generalapproach is to design a model using CAD software, create a mesh of a domainthat includes this model and use finite element analysis to perform simulationson that mesh. When using more advanced simulation techniques, like adaptivefinite element methods, it is more and more desired to use CAD information, notonly for the creation of the initial mesh but also during the simulation. Inthis thesis, an approach is presented how to use CAD data during adaptive mesh refinementin a finite element simulation. An error indicator is presented to find theelements in a mesh, which need to be improved for a better geometricapproximation and it is shown how to integrate the different approaches into anexisting high performance finite element solver</p>

corrected abstract:
<p>CAD (Computer Aided Design) and finite element analysis are of fundamental importance for numerical simulations. The general approach is to design a model using CAD software, create a mesh of a domain that includes this model and use finite element analysis to perform simulations on that mesh. When using more advanced simulation techniques, like adaptive finite element methods, it is more and more desired to use CAD information, not only for the creation of the initial mesh but also during the simulation.</p><p>In this thesis, an approach is presented how to use CAD data during adaptive mesh refinement in a finite element simulation. An error indicator is presented to find the elements in a mesh, which need to be improved for a better geometric approximation and it is shown how to integrate the different approaches into an existing high performance finite element solver</p>
----------------------------------------------------------------------
In diva2:642757 
abstract is: 
<p>In this master’s thesis a model for transcription analysis for a one-to-one Relationship ofInquiry were constructed and presented. The model was modified from the model fortranscript analysis in Community of Inquiry. The original three presences from Community ofInquiry, teaching, cognitive and social presence, were chosen to be adapted to Relationship ofInquiry together with a fourth presence, emotional presence. In this study the online coachingproject Math coach were used for the construction and testing of the model.A total of 60 conversations ranging over more than 3000 message units were in this thesisanalysed to test the model. From the data collected it was seen that the coaches and thecoachees had an almost 50-50 share of the message units. Furthermore the presences in thedata collected where distributed so that the most units where coded to cognitive presence, thenin descending order: teaching, emotional, and social presence. In this master’s thesis themodel for transcript analysis is presented and the results are discussed.</p>

corrected abstract:
<p>In this master’s thesis a model for transcription analysis for a one-to-one Relationship of Inquiry were constructed and presented. The model was modified from the model for transcript analysis in Community of Inquiry. The original three presences from Community of Inquiry, teaching, cognitive and social presence, were chosen to be adapted to Relationship of Inquiry together with a fourth presence, emotional presence. In this study the online coaching project Math coach were used for the construction and testing of the model.</p><p>A total of 60 conversations ranging over more than 3000 message units were in this thesis analysed to test the model. From the data collected it was seen that the coaches and the coachees had an almost 50-50 share of the message units. Furthermore the presences in the data collected where distributed so that the most units where coded to cognitive presence, then in descending order: teaching, emotional, and social presence. In this master’s thesis the model for transcript analysis is presented and the results are discussed.</p>
----------------------------------------------------------------------
In diva2:618333  missing spaces in title:
"Wave Energy Propulsion forPure Car and Truck Carriers(PCTCs)"
==>
"Wave Energy Propulsion for Pure Car and Truck Carriers (PCTCs)"

abstract is: 
<p>Wave Energy Propulsion for Pure Car and Truck Carriers (PCTC's)</p><p>The development of ocean wave energy technology has in recent years seena revival due to increased climate concerns and interest in sustainable energy.This thesis investigates whether ocean wave energy could also beused for propulsion of commercial ships, with Pure Car and Truck Carriers(PCTC's) being the model ship type used. Based on current wave energyresearch four technologies are selected as candidates for wave energy propulsion:bow overtopping, thrust generating foils, moving multi-point absorberand turbine-tted anti-roll tanks.Analyses of the selected technologies indicate that the generated propulsivepower does not overcome the added resistance from the system at the shipdesign speed and size used in the study. Conclusions are that further waveenergy propulsion research should focus on systems for ships that are slowerand smaller than current PCTC's.</p>

corrected abstract:
<p><strong>Wave Energy Propulsion for Pure Car and Truck Carriers (PCTC's)<strong></p><p>The development of ocean wave energy technology has in recent years seen a revival due to increased climate concerns and interest in sustainable energy. This thesis investigates whether ocean wave energy could also be used for propulsion of commercial ships, with Pure Car and Truck Carriers (PCTC's) being the model ship type used. Based on current wave energy research four technologies are selected as candidates for wave energy propulsion: bow overtopping, thrust generating foils, moving multi-point absorber and turbine-fitted anti-roll tanks.</p><p>Analyses of the selected technologies indicate that the generated propulsive power does not overcome the added resistance from the system at the ship design speed and size used in the study. Conclusions are that further wave energy propulsion research should focus on systems for ships that are slower and smaller than current PCTC's.</p>
----------------------------------------------------------------------
In diva2:539446 
abstract is: 
<p>This master thesis deals with the understanding of the secondary air system of athree spool turbofan. The main purpose is the creation of secondary air systemand thermal models to evaluate the behavior of this kind of engine architectureand estimate the pros and cons in comparison with a typical two spool turbofan. Afinite element model of the secondary air system of the engine has been designedbased on the experience of typical jet engines manufactured by Snecma. Theinner thermodynamic pattern and mass flow rates of the engine were obtained.Some local improvements were then made by making analogies with the enginesmanufactured by Snecma. After having communicated the results to theperformance unit to get updates thermodynamic cycles, a quite reliable model wasobtained and can be used as a reference for further studies of this kind of engineat Snecma.</p>

corrected abstract:
<p>This master thesis deals with the understanding of the secondary air system of a three spool turbofan. The main purpose is the creation of secondary air system and thermal models to evaluate the behavior of this kind of engine architecture and estimate the pros and cons in comparison with a typical two spool turbofan. A finite element model of the secondary air system of the engine has been designed based on the experience of typical jet engines manufactured by Snecma. The inner thermodynamic pattern and mass flow rates of the engine were obtained. Some local improvements were then made by making analogies with the engines manufactured by Snecma. After having communicated the results to theperformance unit to get updates thermodynamic cycles, a quite reliable model was obtained and can be used as a reference for further studies of this kind of engine at Snecma.</p>
----------------------------------------------------------------------
In diva2:530870 
abstract is: 
<p>This master thesis deals with the understanding of the secondary air system of athree spool turbofan. The main purpose is the creation of secondary air systemand thermal models to evaluate the behavior of this kind of engine architectureand estimate the pros and cons in comparison with a typical two spool turbofan. Afinite element model of the secondary air system of the engine has been designedbased on the experience of typical jet engines manufactured by Snecma. Theinner thermodynamic pattern and mass flow rates of the engine were obtained.Some local improvements were then made by making analogies with the enginesmanufactured by Snecma. After having communicated the results to theperformance unit to get updates thermodynamic cycles, a quite reliable model wasobtained and can be used as a reference for further studies of this kind of engineat Snecma.</p>

corrected abstract:
<p>This master thesis deals with the understanding of the secondary air system of a three spool turbofan. The main purpose is the creation of secondary air system and thermal models to evaluate the behavior of this kind of engine architecture and estimate the pros and cons in comparison with a typical two spool turbofan. A finite element model of the secondary air system of the engine has been designed based on the experience of typical jet engines manufactured by Snecma. The inner thermodynamic pattern and mass flow rates of the engine were obtained. Some local improvements were then made by making analogies with the engines manufactured by Snecma. After having communicated the results to the performance unit to get updates thermodynamic cycles, a quite reliable model was obtained and can be used as a reference for further studies of this kind of engine at Snecma.</p>
----------------------------------------------------------------------
In diva2:328051 
abstract is: 
<p>The phenomenon of drag reduction by polymers in turbulent flow has beenstudied over the last 60 years. New insight have been recently gained by meansof numerical simulation of dilute polymer solution at moderate values of theturbulent Reynolds number and elasticity. In this thesis, we track elastic parti-cles in Lagrangian frame in turbulent channel flow at Reτ = 180, by tracking,where the single particle obeys the FENE (finite extendible nonlinear elastic)formulation for dumbbel model. The feedback from polymers to the flow is notconsidered, while the Lagrangian approach enables us to consider high valuesof polymer elasticity. In addition, the finite time Lyapunov exponent (FTLE)of the flow is computed tracking infinitesimal material elements advected bythe flow. Following the large deviation theory, the Cramer’s function of theprobability density function of the FTLE for large values of time intervals isstudied at different wall-normal positions. The one-way effect of the turbulentflow on polymers is investigated by looking at the elongation and orientation ofthe polymers, with different relaxation times, across the channel. The confor-mation tensor of the polymers deformation which is an important contributionin the momentum balance equation is calculated by averaging in wall-parallelplanes and compared to theories available in the literature.</p>

corrected abstract:
<p>The phenomenon of drag reduction by polymers in turbulent flow has been studied over the last 60 years. New insight have been recently gained by means of numerical simulation of dilute polymer solution at moderate values of the turbulent Reynolds number and elasticity. In this thesis, we track elastic particles in Lagrangian frame in turbulent channel flow at <em>Re<sub>τ</sub></em> = 180, by tracking, where the single particle obeys the FENE (finite extendible nonlinear elastic) formulation for dumbbel model. The feedback from polymers to the flow is not considered, while the Lagrangian approach enables us to consider high values of polymer elasticity. In addition, the finite time Lyapunov exponent (FTLE) of the flow is computed tracking infinitesimal material elements advected by the flow. Following the large deviation theory, the Cramer’s function of the probability density function of the FTLE for large values of time intervals is studied at different wall-normal positions. The one-way effect of the turbulent flow on polymers is investigated by looking at the elongation and orientation of the polymers, with different relaxation times, across the channel. The conformation tensor of the polymers deformation which is an important contribution in the momentum balance equation is calculated by averaging in wall-parallel planes and compared to theories available in the literature.</p>
----------------------------------------------------------------------
In diva2:1900087 
abstract is: 
<p>LiDAR, which stands for Light Detection and Ranging, serves as a remote measurementtechnique for studying atmospheric properties, such as gas and particle concentration, aswell as wind speed [23]. The detectors employed in such measurements play a crucial rolein determining factors such as resolution, maximum range, and acquisition time. SuperconductingNanowire Single Photon Detectors (SNSPDs) exhibit exceptional performance characteristics,including high detection efficiency, minimal timing jitter, low dead time, a high maximum countrate, and a high Signal-to-Noise Ratio (SNR) even in the infrared [7]. Consequently, SNSPDsemerge as promising candidates for enhancing the quality of LiDAR signals within the infraredrange. In this work, I study the absorption and broadening mechanism of CO2 spectral linesaround 2¹m. Furthermore, I measured a backscatter signal from water and candle vapors whilea measurement of backscatter light from CO2 gas is still to come. Lastly, I perform a LiDARmeasurement with a Newtonian telescope co-operating with SNSPDs, to build a 3D image of adepth target.</p>

corrected abstract:
<p>LiDAR, which stands for Light Detection and Ranging, serves as a remote measurement technique for studying atmospheric properties, such as gas and particle concentration, as well as wind speed [23]. The detectors employed in such measurements play a crucial role in determining factors such as resolution, maximum range, and acquisition time. Superconducting Nanowire Single Photon Detectors (SNSPDs) exhibit exceptional performance characteristics, including high detection efficiency, minimal timing jitter, low dead time, a high maximum count rate, and a high Signal-to-Noise Ratio (SNR) even in the infrared [7]. Consequently, SNSPDs emerge as promising candidates for enhancing the quality of LiDAR signals within the infrared range. In this work, I study the absorption and broadening mechanism of CO<sub>2</sub> spectral lines around 2µm. Furthermore, I measured a backscatter signal from water and candle vapors while a measurement of backscatter light from CO<sub>2</sub> gas is still to come. Lastly, I perform a LiDAR measurement with a Newtonian telescope co-operating with SNSPDs, to build a 3D image of a depth target.</p>
----------------------------------------------------------------------
In diva2:1894660 
abstract is: 
<p>This study addresses the valuation challenges of private unlisted loan investmentsby implementing a valuation model to estimate the market value of such financialinstrument. In collaboration with Alecta, Sweden’s largest occupational pensionfund, a Monte Carlo-based valuation model was developed in Python. A sensitivityanalysis was also conducted in order to capture the dynamics between theinstrument’s value and the different input parameters.</p><p>Results from implementing the model demonstrate its ability to providevaluations under varying economic scenarios, highlighting the model’s sensitivityto changes in relevant economic variables.</p><p>The study establishes a foundational basis for additional research that could,for example, improve the model’s capability to capture the characteristics ofsuch financial instruments. This could include exploring additional parametersthat may be incorporated and examining the reasonableness of the underlyingassumptions.</p>

corrected abstract:
<p>This study addresses the valuation challenges of private unlisted loan investments by implementing a valuation model to estimate the market value of such financial instrument. In collaboration with Alecta, Sweden’s largest occupational pension fund, a Monte Carlo-based valuation model was developed in Python. A sensitivity analysis was also conducted in order to capture the dynamics between the instrument’s value and the different input parameters.</p><p>Results from implementing the model demonstrate its ability to provide valuations under varying economic scenarios, highlighting the model’s sensitivity to changes in relevant economic variables.</p><p>The study establishes a foundational basis for additional research that could, for example, improve the model’s capability to capture the characteristics of such financial instruments. This could include exploring additional parameters that may be incorporated and examining the reasonableness of the underlying assumptions.</p>
----------------------------------------------------------------------
In diva2:1880418 - missing space in title:
"How To Break the Second Law of Thermodynamics: Monte Carlo Simulation of Information Machine Realisation andTheory of Information"
==>
"How To Break the Second Law of Thermodynamics: Monte Carlo Simulation of Information Machine Realisation and Theory of Information"

abstract is: 
<p>In 1867, James Clerk Maxwell introduced a thought experiment involving a micro-scopic being (observer) capable of making precise measurements of microscopic quantitiesthrough observation of the micro-dynamics in a thermodynamic system. This observerlater became known as Maxwell’s demon due to its devious impact on thermodynamics,particularly the perceived violation of the second law. Subsequently, Leo Szilard pro-posed a machine, the so-called Szilard Machine, which, by utilising a Maxwell’s demon,successfully extracts work from thermal fluctuations in a closed system, seemingly vio-lating the second law.</p><p>This thesis re-evaluates the second law of thermodynamics in the context of the Szi-lard Machine and Maxwell demons. The study explores the intersection of informationtheory and thermal physics, both theoretically and practically, with the aid of MonteCarlo simulations. The results indicate that machines with information feedback control,such as those utilising a Maxwell demon, challenge classical statements of the second lawof thermodynamics. This is because classical formulations, such as Clausius’ and Kelvin’sstatements, do not account for the entropic content of information. Simulations of thesefeedback processes, in conjunction with the detailed fluctuation theorem, provide a basisfor understanding feedback processes in so-called information machines. Ultimately, thesecond law of thermodynamics is upheld by an alternative statement endorsed by thedetailed fluctuation theorem.</p>

corrected abstract:
<p>In 1867, James Clerk Maxwell introduced a thought experiment involving a microscopic being (observer) capable of making precise measurements of microscopic quantities through observation of the micro-dynamics in a thermodynamic system. This observer later became known as Maxwell’s demon due to its devious impact on thermodynamics, particularly the perceived violation of the second law. Subsequently, Leo Szilard proposed a machine, the so-called Szilard Machine, which, by utilising a Maxwell’s demon, successfully extracts work from thermal fluctuations in a closed system, seemingly violating the second law.</p><p>This thesis re-evaluates the second law of thermodynamics in the context of the Szilard Machine and Maxwell demons. The study explores the intersection of information theory and thermal physics, both theoretically and practically, with the aid of Monte Carlo simulations. The results indicate that machines with information feedback control, such as those utilising a Maxwell demon, challenge classical statements of the second law of thermodynamics. This is because classical formulations, such as Clausius’ and Kelvin's statements, do not account for the entropic content of information. Simulations of these feedback processes, in conjunction with the detailed fluctuation theorem, provide a basis for understanding feedback processes in so-called information machines. Ultimately, the second law of thermodynamics is upheld by an alternative statement endorsed by the detailed fluctuation theorem.</p>
----------------------------------------------------------------------
In diva2:1877676 
abstract is: 
<p>This paper discusses how Q-Learning and Deep Q-Networks (DQN) canbe applied to state-action problems described by a Markov decision process(MDP). These are machine learning methods for finding the optimal choiceof action at each time step, resulting in the optimal policy. The limitationsand advantages for the two methods are discussed, with the main limitationbeing the fact that Q-learning is unable to be used on problems with infinitestate spaces. Q-learning, however, has an advantage in the simplicity of thealgorithm, leading to a better understanding of what the algorithm is actuallydoing. Q-Learning did manage to find the optimal policy for the simpleproblem studied in this paper, but was unable to do so for the advancedproblem. The Deep Q-Network (DQN) approach was able to solve bothproblems, with a drawback in it being harder to understand what the algorithmactually is doing.</p>

corrected abstract:
<p>This paper discusses how Q-Learning and Deep Q-Networks (DQN) can be applied to state-action problems described by a Markov decision process (MDP). These are machine learning methods for finding the optimal choice of action at each time step, resulting in the optimal policy. The limitations and advantages for the two methods are discussed, with the main limitation being the fact that Q-learning is unable to be used on problems with infinite state spaces. Q-learning, however, has an advantage in the simplicity of the algorithm, leading to a better understanding of what the algorithm is actually doing. Q-Learning did manage to find the optimal policy for the simple problem studied in this paper, but was unable to do so for the advanced problem. The Deep Q-Network (DQN) approach was able to solve both problems, with a drawback in it being harder to understand what the algorithm actually is doing.</p>
----------------------------------------------------------------------
In diva2:1872800 
abstract is: 
<p>The inner crust of a neutron star is explored, using nuclear models andimplementing them in an open quantum system formalism. The purposeof the investigation is to extract valuable dynamics of the predictednuclear clusters that may exist in this region. We begin by setting up thenuclear models to extract the total energy per particle per system in orderto see if the results are corroborated with the works of others. Thereafter,the single-particle energies of the nuclear clusters are extracted. Theseenergies are then used in the Lindblad formalism to work out the timedependenceof the density matrix, which will allow us to extract thetime-dependence of the average energy of the system in interaction withan environment.</p>

corrected abstract:
<p>The inner crust of a neutron star is explored, using nuclear models and implementing them in an open quantum system formalism. The purpose of the investigation is to extract valuable dynamics of the predicted nuclear clusters that may exist in this region. We begin by setting up the nuclear models to extract the total energy per particle per system in order to see if the results are corroborated with the works of others. Thereafter, the single-particle energies of the nuclear clusters are extracted. These energies are then used in the Lindblad formalism to work out the time-dependence of the density matrix, which will allow us to extract the time-dependence of the average energy of the system in interaction with an environment.</p>
----------------------------------------------------------------------
In diva2:1827763 
abstract is: 
<p>The importance of sustainability in financial investments has increasedsignificantly in the previous years. With an ever-growing climate crisis andthe implementation of the EU Taxonomy it is very likely that this trendwill continue. It is however inconclusive if there is a positive correlationbetween environmental, social and governance (ESG) sustainability variablesand returns in listed companies. To deepen the knowledge of Erik PenserBank regarding this subject two regression models were constructed to try andcapture this relationship using return on assets (ROA) and 3 year stock returnsas the response variables in the respective models. The results of the studyindicate that such a relationship can not be generally established for Europeanlisted companies</p>

corrected abstract:
<p>The importance of sustainability in financial investments has increased significantly in the previous years. With an ever-growing climate crisis and the implementation of the EU Taxonomy it is very likely that this trend will continue. It is however inconclusive if there is a positive correlation between environmental, social and governance (ESG) sustainability variables and returns in listed companies. To deepen the knowledge of Erik Penser Bank regarding this subject two regression models were constructed to try and capture this relationship using return on assets (ROA) and 3 year stock returns as the response variables in the respective models. The results of the study indicate that such a relationship can not be generally established for European listed companies</p>
----------------------------------------------------------------------
In diva2:1817132 
abstract is: 
<p>This thesis investigates sound transmission loss through flat finite panels using composite materials, which incorporate both carbon and flax fibres. The study wascarried out by performing FEM simulations developed and validated in steps. Thevalidation process was time-consuming but crucial in ensuring the model’s reliability.The analysis demonstrates the relationship between the panels’ mass, stiffness, andsound reduction effectiveness by studying the sound transmission loss for a flat plate.The results show how by adjusting the plate’s stiffness, thickness, and mass one cancontrol its reaction to the incoming sound. This allows resonance frequencies to beshifted away from critical points and avoids coincidence frequencies. By understanding the acoustic behavior of composite panels, this research contributes to improvingsoundproofing properties while preserving their mechanical advantages. In an era ofgrowing demand for better acoustic performance in the automotive industry, exploring innovative methods to optimize the sound transmission loss of composite panelsis essential.</p>

corrected abstract:
<p>This thesis investigates sound transmission loss through flat finite panels using composite materials, which incorporate both carbon and flax fibres. The study was carried out by performing FEM simulations developed and validated in steps. The validation process was time-consuming but crucial in ensuring the model’s reliability. The analysis demonstrates the relationship between the panels’ mass, stiffness, and sound reduction effectiveness by studying the sound transmission loss for a flat plate. The results show how by adjusting the plate’s stiffness, thickness, and mass one can control its reaction to the incoming sound. This allows resonance frequencies to be shifted away from critical points and avoids coincidence frequencies. By understanding the acoustic behavior of composite panels, this research contributes to improving soundproofing properties while preserving their mechanical advantages. In an era of growing demand for better acoustic performance in the automotive industry, exploring innovative methods to optimize the sound transmission loss of composite panels is essential.</p>
----------------------------------------------------------------------
In diva2:1751971 - caannot select the characters of the abstract from the PDF
abstract is: 
<p>Nowadays, nanomedicine is one of the most important application areas fornanoparticles (NPs), where the design and synthesis of new hybrid nanostructures have attracted much interest, when combining properties and functionalities from different constituents. For example, they have been extensivelyused for dual-mode imaging. In this work, a water-based synthesis of hybridNPs with co-precipitation method was studied. To obtain superparamagnetichybrid NPs for X-ray fluorescence computed tomography (XFCT), severalhybridization mechanisms were followed, combining the superparamagneticbehavior of iron oxide NPs with active X-ray fluorescence (XRF) elements –Rh or Ru. The NP surface had to be engineered to improve the NP stabilityand dispersion in water, and to grant high biocompatibility. The resulting hybrid nanostructures exhibit promising characteristics for dual-mode imagingand hyperthermia treatments. We present on the details of the synthetic process, as well as the characterization of the synthesized nanomaterials</p>

corrected abstract:
<p>Nowadays, nanomedicine is one of the most important application areas for nanoparticles (NPs), where the design and synthesis of new hybrid nanostructures have attracted much interest, when combining properties and functionalities from different constituents. For example, they have been extensively used for dual-mode imaging. In this work, a water-based synthesis of hybrid NPs with co-precipitation method was studied. To obtain superparamagnetic hybrid NPs for X-ray fluorescence computed tomography (XFCT), several hybridization mechanisms were followed, combining the superparamagnetic behavior of iron oxide NPs with active X-ray fluorescence (XRF) elements –Rh or Ru. The NP surface had to be engineered to improve the NP stability and dispersion in water, and to grant high biocompatibility. The resulting hybrid nanostructures exhibit promising characteristics for dual-mode imaging and hyperthermia treatments. We present on the details of the synthetic process, as well as the characterization of the synthesized nanomaterials</p>
----------------------------------------------------------------------
In diva2:1739571 
abstract is: 
<p>Irradiation embrittlement is a problematic issue for internal structures, such as acore shroud, in a light water reactor. It is characterized by decreased ductility andtoughness, and increased yield strength and hardening due to increased dislocations.Before this thesis a core shroud from a BWR was cut into pieces based on the estimatedneutron damage on the welds. Two of the pieces cut out was used in this thesis, A2( 0.5dpa) and C2(&lt;0.1 dpa). In this thesis Vickers hardness tests and uniaxial tensile testswere used to give an estimation of the embrittlement of the welds. An estimation ofyield strength of A2 was also calculated based on the resulting hardness. The Vickerstests showed noticeable hardening in the weld, both at 0.1 dpa and 0.5 dpa. The ATTalso showed an increase in yield strength in C2. The calculated yield strength alsoshowed an increase. In conclusion, the core shroud has a decreased toughness, henceincreased risk of unexpected brittle fracture.</p>

corrected abstract:
<p>Irradiation embrittlement is a problematic issue for internal structures, such as a core shroud, in a light water reactors. It is characterized by decreased ductility and toughness, and increased yield strength and hardening due to increased dislocations. Before this thesis a core shroud from a BWR was cut into pieces based on the estimated neutron damage on the welds. Two of the pieces cut out was used in this thesis, A2 (0.5 dpa) and C2 (&lt;0.1 dpa). In this thesis Vickers hardness tests and uniaxial tensile tests were used to give an estimation of the embrittlement of the welds. An estimation of yield strength of A2 was also calculated based on the resulting hardness. The Vickers tests showed noticeable hardening in the weld, both at 0.1 dpa and 0.5 dpa. The ATT also showed an increase in yield strength in C2. The calculated yield strength also showed an increase. In conclusion, the core shroud has a decreased toughness, hence increased risk of unexpected brittle fracture.</p>
----------------------------------------------------------------------
In diva2:1739353 
abstract is: 
<p>To further reduce weight remains a constant goal within aviation. In this project an aircraftconsole is redesigned from aluminium to carbon fibre composite and analysed to see ifit is possible to reduce weight while maintaining the ability to withstand the emergencylanding loads. There are in total 9 different load cases, but the focus is on 9 and 18g forward.The design of the composite console is an iterative process, and the outcome is a sandwichsolution with a basic layup of 3 carbon fibre composite layers, PVC core and another 3carbon composite layers. Each iteration is followed by analysis in Ansys workbench usingFEA, finite element analysis, and if stresses or strains are too high reinforcement layers areadded to areas of concern and a new iteration of layup and analysis is performed.The aluminium console weighs 17 kg and after iterating the sandwich console fivetimes the end result is a console weighing 12.2 kg. Which means a weight reduction ofapproximately 30%.Overall, the new composite sandwich console can withstand higher loads than the oldaluminium one but the fastening to the floor requires further analysis.</p>

corrected abstract:
<p>To further reduce weight remains a constant goal within aviation. In this project an aircraft console is redesigned from aluminium to carbon fibre composite and analysed to see if it is possible to reduce weight while maintaining the ability to withstand the emergency landing loads. There are in total 9 different load cases, but the focus is on 9 and 18g forward. The design of the composite console is an iterative process, and the outcome is a sandwich solution with a basic layup of 3 carbon fibre composite layers, PVC core and another 3carbon composite layers. Each iteration is followed by analysis in Ansys workbench using FEA, finite element analysis, and if stresses or strains are too high reinforcement layers are added to areas of concern and a new iteration of layup and analysis is performed. The aluminium console weighs 17 kg and after iterating the sandwich console five times the end result is a console weighing 12.2 kg. Which means a weight reduction of approximately 30%.Overall, the new composite sandwich console can withstand higher loads than the old aluminium one but the fastening to the floor requires further analysis.</p>
----------------------------------------------------------------------
In diva2:1701469 
abstract is: 
<p>This thesis investigates the effects of power-law correlated disorder on a three-dimensional XY model and the Weinrib-Halperin disorder relevance criterion’s pre-dictive ability. Ising models are used as a map to realise disorder couplings. Simula-tions are conducted using hybrid Monte Carlo method constituting Metropolis’ andWolff’s algorithms. Two cases using two-dimensional and three-dimensional Isinggenerated disorder corresponding to (d + 1)- and d-dimensional models are tested.In addition, a superficial scaling analysis is performed to highlight the change ofuniversality class.It is shown that magnetisation, response functions and Binder ratio along withits temperature derivative display stark differences from the pure XY model case.The results agree with the Weinrib-Halperin criterion in terms of predicting achange of universality class but show a discrepancy in both qualitative and nu-merical results. The main new result is that power-law correlated disorder canintroduce two phase transitions at different critical couplings. This is in disagree-ment with prior established theory and predicts new physics to be investigated insuperconductors and superfluids with correlated disorder.</p>

corrected abstract:
<p>This thesis investigates the effects of power-law correlated disorder on a three-dimensional XY model and the Weinrib-Halperin disorder relevance criterion’s predictive ability. Ising models are used as a map to realise disorder couplings. Simulations are conducted using hybrid Monte Carlo method constituting Metropolis’ and Wolff’s algorithms. Two cases using two-dimensional and three-dimensional Ising generated disorder corresponding to (<em>d</em>+ 1)- and <em>d</em>-dimensional models are tested. In addition, a superficial scaling analysis is performed to highlight the change of universality class.</p><p>It is shown that magnetisation, response functions and Binder ratio along with its temperature derivative display stark differences from the pure XY model case. The results agree with the Weinrib-Halperin criterion in terms of predicting a change of universality class but show a discrepancy in both qualitative and numerical results. The main new result is that power-law correlated disorder can introduce two phase transitions at different critical couplings. This is in disagreement with prior established theory and predicts new physics to be investigated in superconductors and superfluids with correlated disorder.</p>
----------------------------------------------------------------------
In diva2:1698423 
abstract is: 
<p>The prediction of contrails have been studied since the 1930s primarily for militaryapplications. In present times, contrail avoidance strategies are gaining popularity insustainable aviation. In this project, the theory behind their formation, persistenceand the mitigation strategies are explored, studied and implemented.Several methods are evaluated in order to predict the critical temperature. Aircraftparameters are included in the prediction of contrails and implemented. This dynamicprogramming is contrasted against real case operational flight plans.The first implementation considers an operational flight plan for which contrails are pre-dicted along the trajectory. The second involves the prediction of contrails at the end ofthe vertical trajectory optimisation. The optimisation for contrail persistence avoidanceis also accomplished. Results show that contrail persistence can be avoided.</p>

corrected abstract:
<p>The prediction of contrails have been studied since the 1930s primarily for military applications. In present times, contrail avoidance strategies are gaining popularity in sustainable aviation. In this project, the theory behind their formation, persistence and the mitigation strategies are explored, studied and implemented. Several methods are evaluated in order to predict the critical temperature. Aircraft parameters are included in the prediction of contrails and implemented. This dynamic programming is contrasted against real case operational flight plans. The first implementation considers an operational flight plan for which contrails are predicted along the trajectory. The second involves the prediction of contrails at the end of the vertical trajectory optimisation. The optimisation for contrail persistence avoidance is also accomplished. Results show that contrail persistence can be avoided.</p>
----------------------------------------------------------------------
In diva2:1680720 
abstract is: 
<p>The aim of this thesis is derive a set of polynomials defined on simply connected domains, the Faberpolynomials, in which all analytic function on the domain can be uniformly approximated. Importantconcepts and theorems such as isomorphisms, automorphisms and the Riemann mapping theorem areintroduced. Examples and applications are also included. Furthermore, the thesis will aim to introducean important consequence of the Faber polynomials, the method of the Grunsky inequalities.</p><p>The first section introduces important properties of analytic functions and the concept of isomor-phisms, in particular the form of all automorphisms of the unit disc will be derived. The second sectionconsiders the Riemann mapping theorem, a theorem that relates any simply connected region that is notall of ℂ to the unit disc. A proof of the theorem beginning with the Arzelá-Ascoli theorem is provided.An application in constructing harmonic functions on arbitrary simply connected regions will be pre-sented. In the third section, definitions and properties of the Faber polynomials are developed; followedby simple examples. The section concludes with a proof and example of the statement that analyticfunctions can be approximated by Faber polynomials. In the fourth and last section of the thesis, themethod of Grunsky inequalities is presented. Starting off, the Grunsky coefficients are defined using theFaber polynomials. Properties of Grunsky coefficients such as the symmetry property and the Grunskyinequalities are then derived. To conclude it will be shown that the Grunsky inequalities provide aunivalence criterion for analytic functions defined on the unit disc.</p><p> </p>

corrected abstract:
<p>The aim of this thesis is derive a set of polynomials defined on simply connected domains, the Faber polynomials, in which all analytic function on the domain can be uniformly approximated. Important concepts and theorems such as isomorphisms, automorphisms and the Riemann mapping theorem are introduced. Examples and applications are also included. Furthermore, the thesis will aim to introduce an important consequence of the Faber polynomials, the method of the Grunsky inequalities.</p><p>The first section introduces important properties of analytic functions and the concept of isomorphisms, in particular the form of all automorphisms of the unit disc will be derived. The second section considers the Riemann mapping theorem, a theorem that relates any simply connected region that is not all of ℂ to the unit disc. A proof of the theorem beginning with the Arzelá-Ascoli theorem is provided. An application in constructing harmonic functions on arbitrary simply connected regions will be presented. In the third section, definitions and properties of the Faber polynomials are developed; followed by simple examples. The section concludes with a proof and example of the statement that analytic functions can be approximated by Faber polynomials. In the fourth and last section of the thesis, the method of Grunsky inequalities is presented. Starting off, the Grunsky coefficients are defined using the Faber polynomials. Properties of Grunsky coefficients such as the symmetry property and the Grunsky inequalities are then derived. To conclude it will be shown that the Grunsky inequalities provide a univalence criterion for analytic functions defined on the unit disc.</p>
----------------------------------------------------------------------
In diva2:1670542 
abstract is: 
<p>This thesis deals with fraud detection in a real-world environment with datasets coming from Svenska Handelsbanken. The goal was to investigate how well machine learning can classify fraudulent transactions and how new additional features affected classification. The models used were EFSVM, RUTSVM, CS-SVM, ELM, MLP, Decision Tree, Extra Trees, and Random Forests. To determine the best results the Mathew Correlation Coefficient was used as performance metric, which has been shown to have a medium bias for imbalanced datasets. Each model could deal with high imbalanced datasets which is common for fraud detection. Best results were achieved with Random Forest and Extra Trees. The best scores were around 0.4 for the real-world datasets, though the score itself says nothing as it is more a testimony to the dataset’s separability. These scores were obtained when using aggregated features and not the standard raw dataset. The performance measure recall’s scores were around 0.88-0.93 with an increase in precision by 34.4%-67%, resulting in a large decrease of False Positives. Evaluation results showed a great difference compared to test-runs, either substantial increase or decrease. Two theories as to why are discussed, a great distribution change in the evaluation set, and the sample size increase (100%) for evaluation could have lead to the tests not being well representing of the performance. Feature aggregation were a central topic of this thesis, with the main focus on behaviour features which can describe patterns and habits of customers. For these there were five categories: Sender’s fraud history, Sender’s transaction history, Sender’s time transaction history, Sender’shistory to receiver, and receiver’s history. Out of these, the best performance increase was from the first which gave the top score, the other datasets did not show as much potential, with mostn ot increasing the results. Further studies need to be done before discarding these features, to be certain they don’t improve performance. Together with the data aggregation, a tool (t-SNE) to visualize high dimension data was usedto great success. With it an early understanding of what to expect from newly added features would bring to classification. For the best dataset it could be seen that a new sub-cluster of transactions had been created, leading to the belief that classification scores could improve, whichthey did. Feature selection and PCA-reduction techniques were also studied and PCA showedgood results and increased performance. Feature selection had not conclusive improvements. Over- and under-sampling were used and neither improved the scores, though undersampling could maintain the results which is interesting when increasing the dataset.</p>

corrected abstract:
<p>This thesis deals with fraud detection in a real-world environment with datasets coming from Svenska Handelsbanken. The goal was to investigate how well machine learning can classify fraudulent transactions and how new additional features affected classification. The models used were EFSVM, RUTSVM, CS-SVM, ELM, MLP, Decision Tree, Extra Trees, and Random Forests. To determine the best results the Mathew Correlation Coefficient was used as performance metric, which has been shown to have a medium bias for imbalanced datasets. Each model could deal with high imbalanced datasets which is common for fraud detection. Best results were achieved with Random Forest and Extra Trees. The best scores were around 0.4 for the real-world datasets, though the score itself says nothing as it is more a testimony to the dataset’s separability. These scores were obtained when using aggregated features and not the standard raw dataset. The performance measure recall’s scores were around 0.88-0.93 with an increase in precision by 34.4%-67%, resulting in a large decrease of False Positives. Evaluation results showed a great difference compared to test-runs, either substantial increase or decrease. Two theories as to why are discussed, a great distribution change in the evaluation set, and the sample size increase (100%) for evaluation could have lead to the tests not being well representing of the performance.</p><p>Feature aggregation were a central topic of this thesis, with the main focus on behaviour features which can describe patterns and habits of customers. For these there were five categories: Sender’s fraud history, Sender’s transaction history, Sender’s time transaction history, Sender’s history to receiver, and receiver’s history. Out of these, the best performance increase was from the first which gave the top score, the other datasets did not show as much potential, with most not increasing the results. Further studies need to be done before discarding these features, to be certain they don’t improve performance.</p><p>Together with the data aggregation, a tool (t-SNE) to visualize high dimension data was used to great success. With it an early understanding of what to expect from newly added features would bring to classification. For the best dataset it could be seen that a new sub-cluster of transactions had been created, leading to the belief that classification scores could improve, which they did. Feature selection and PCA-reduction techniques were also studied and PCA showed good results and increased performance. Feature selection had not conclusive improvements. Over- and under-sampling were used and neither improved the scores, though undersampling could maintain the results which is interesting when increasing the dataset.</p>
----------------------------------------------------------------------
In diva2:1655933 
abstract is: 
<p>This thesis examines the correlation between some macroeconomic factors andthe monthly difference in gold price with the objective of examining if, and towhat extent they are related. This is done with the ambition to identify the mostinfluential drivers affecting the change in gold price. Multiple linear regression isused to model the relationship between the response variable, monthly change ingold price, and 13 regressor variables considered to be influential macroeconomicfactors on the gold market. The predictor variables include measures likeinflation, money supply and bond rates.</p><p>A final model is obtained, identifying the most significant regressor variablesto be USDX, EU CPI, China CPI, US Money Supply, Gold Futures, US 10yeargovernment bond yields and Gold Ores PPI. The plausibility of the resultsare discussed based on previous studies, macroeconomic theory and possibledelimitations. The thesis also provide suggestions for further studies on topicsnot covered in the study.</p>

corrected abstract:
<p>This thesis examines the correlation between some macroeconomic factors and the monthly difference in gold price with the objective of examining if, and to what extent they are related. This is done with the ambition to identify the most influential drivers affecting the change in gold price. Multiple linear regression is used to model the relationship between the response variable, monthly change in gold price, and 13 regressor variables considered to be influential macroeconomic factors on the gold market. The predictor variables include measures like inflation, money supply and bond rates.</p><p>A final model is obtained, identifying the most significant regressor variables to be <em>USDX</em>, <em>EU CPI</em>, <em>China CPI</em>, <em>US Money Supply</em>, <em>Gold Futures</em>, <em>US 10-year government bond yields</em> and <em>Gold Ores PPI</em>. The plausibility of the results are discussed based on previous studies, macroeconomic theory and possible delimitations. The thesis also provide suggestions for further studies on topics not covered in the study.</p>
----------------------------------------------------------------------
In diva2:1571175 
abstract is: 
<p>Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal form of cancerwith very few available treatment options of which none has great effect.Cancer cells and stromal cells such as stellate cells which exist in abundancein PDAC interact by crosstalk, resulting in a tumorigenic collective response.With the help of a previously developed 3D co-culture spheroid model theeffect of a CRISPR/cas9 knockout of the cellular communication cetworkfactor 1 (CCN1) gene together with gemcitabine (GEM) treatment has beeninvestigated in terms of Panc1 cell viability and gene expression. Spheroidsconsisting of wild-type and knockout cell lines, each identified by westernblots were cultured, imaged and treated. Viability assays and RNA extractionfollowed by PCR showed that the viability of the cancer cells in the spheroidswere higher for the cells with CCN1 knockout. Cancer cells were also coculturedwith stellate cells with the goal of investigating the effect of thecellular crosstalk on chemoresistance.</p>

corrected abstract:
<p>Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal form of cancer with very few available treatment options of which none has great effect. Cancer cells and stromal cells such as stellate cells which exist in abundance in PDAC interact by crosstalk, resulting in a tumorigenic collective response. With the help of a previously developed 3D co-culture spheroid model the effect of a CRISPR/cas9 knockout of the cellular communication cetwork factor 1 (CCN1) gene together with gemcitabine (GEM) treatment has been investigated in terms of Panc1 cell viability and gene expression. Spheroids consisting of wild-type and knockout cell lines, each identified by western blots were cultured, imaged and treated. Viability assays and RNA extraction followed by PCR showed that the viability of the cancer cells in the spheroids were higher for the cells with CCN1 knockout. Cancer cells were also co-cultured with stellate cells with the goal of investigating the effect of the cellular crosstalk on chemoresistance.</p>
----------------------------------------------------------------------
In diva2:1568279 
abstract is: 
<p>A promising new technology in medical imaging is photon-counting detectors (PCD). Itcould allow for images with higher resolution, less noise, improved material decomposi-tion while possibly reducing radiation exposure for patients. Recently, the possibility touse deep-learning denoising in tandem with PCD to increase image quality is starting tobe investigated. In this report we use a variety of standard image quality metrics suchas MSE, SSIM and MTF, on different image phantoms, to evaluate two ways of imple-menting neural networks in the reconstruction process: in the image domain and in thesinogram domain. We show that implementing the network in the image domain seemsto be the most promising choice to increase image quality, observing higher contrast,reduced noise and smaller errors than for the sinogram domain network. We also discusswhy this might be the case. Additionally, we study the effects of optimizing the networksand how well the neural networks generalize to types of phantoms other than the onesthey were trained on.</p>

corrected abstract:
<p>A promising new technology in medical imaging is photon-counting detectors (PCD). It could allow for images with higher resolution, less noise, improved material decomposition while possibly reducing radiation exposure for patients. Recently, the possibility to use deep-learning denoising in tandem with PCD to increase image quality is starting to be investigated. In this report we use a variety of standard image quality metrics such as MSE, SSIM and MTF, on different image phantoms, to evaluate two ways of implementing neural networks in the reconstruction process: in the image domain and in the sinogram domain. We show that implementing the network in the image domain seems to be the most promising choice to increase image quality, observing higher contrast, reduced noise and smaller errors than for the sinogram domain network. We also discuss why this might be the case. Additionally, we study the effects of optimizing the networks and how well the neural networks generalize to types of phantoms other than the ones they were trained on.</p>
----------------------------------------------------------------------
In diva2:1528146 
abstract is: 
<p>The cooling system is a crucial part for helicopter operations. Withoutit, hovering flight could not be operated. The cooling system for the maingearbox of a helicopter is composed of radiators and a fan. A fan is anaerodynamic body and as such it can be improved in terms of aerodynamicefficiency. Therefore di↵erent parameters need to be taken into account whendesigning a new axial fan to have good aerodynamic performance. Simulationshave been carried out to investigate the e↵ects of these parameters andcome up with an optimal design based on the study requirements. The fanhas to enable the cooling system to evacuate an amount of thermal power sothat the helicopter can take o↵ with high outside temperatures. This optimaldesign has shown an increase of the mass flow rate up to a factor of abouttwo for a given pressure loss compared to the original fan.</p>

corrected abstract:
<p>The cooling system is a crucial part for helicopter operations. Without it, hovering flight could not be operated. The cooling system for the main gearbox of a helicopter is composed of radiators and a fan. A fan is an aerodynamic body and as such it can be improved in terms of aerodynamic efficiency. Therefore di↵erent parameters need to be taken into account when designing a new axial fan to have good aerodynamic performance. Simulations have been carried out to investigate the e↵ects of these parameters and come up with an optimal design based on the study requirements. The fan has to enable the cooling system to evacuate an amount of thermal power so that the helicopter can take off with high outside temperatures. This optimal design has shown an increase of the mass flow rate up to a factor of about two for a given pressure loss compared to the original fan.</p>
----------------------------------------------------------------------
In diva2:1436915 
abstract is: 
<p>Technical solutions for diagnosis and treatment of heart diseases is of greatinterest due to it being one of the leading causes of death in the world. Thisstudy focuses on the valve between the left atrium and ventricle, the mitralvalve, and the blood flow in the left ventricle of the human heart. A paramet-ric model for patient-specific simulations of mitral valve dynamics developedby Hoffman J. et al. at KTH and Lucor D. at Universit´e Paris-Saclay is usedand further studied by focusing on the heart disease mitral valve stenosis.Solutions are produced with FEniCS-HPC HeartSolver using computationalfluid dynamics. Our results show that the more severe the disease is, thehigher velocities and vorticity in the ventricle, and the lower the pressure.Further studies of interest include non-Dirichlet boundary conditions by themitral valve, to allow mitral regurgitation that was observed in this study.</p>

corrected abstract:
<p>Technical solutions for diagnosis and treatment of heart diseases is of great interest due to it being one of the leading causes of death in the world. This study focuses on the valve between the left atrium and ventricle, the mitral valve, and the blood flow in the left ventricle of the human heart. A parametric model for patient-specific simulations of mitral valve dynamics developed by Hoffman J. et al. at KTH and Lucor D. at Université Paris-Saclay is used and further studied by focusing on the heart disease mitral valve stenosis. Solutions are produced with FEniCS-HPC HeartSolver using computational fluid dynamics. Our results show that the more severe the disease is, the higher velocities and vorticity in the ventricle, and the lower the pressure. Further studies of interest include non-Dirichlet boundary conditions by the mitral valve, to allow mitral regurgitation that was observed in this study.</p>
----------------------------------------------------------------------
In diva2:1357370 
abstract is: 
<p><strong></strong>The topic of this project is an experimental study of prepreg characteristics, such as tack, consolidation and temperature sensitivity during forming. The aim has first been tounderstand how the material reacts during different manufacturing processes. Secondly, to recommend suitable parameter settings, based on the findings, in order to get a good andreliable manufacturing process.In a literature study it was found that the prepreg tack is difficult to measure. It is debated by the scientific community today how to best describe prepreg tack, and the answer is affectedof what parameters that are sought to be reproduced. Consolidation tests have, in this study, been performed in an Instron machine. The relaxation of two different materials has beenmeasured in room temperature, 40 °C and 60 °C, with a maximum pressure of 2-10 bar. These limits are set to cover the temperature- and pressure scope in a robot forming process.Results show that neither of the materials will experience full consolidation during these tests, and therefore, neither in a robot forming process. It is therefore recommended toconsolidate the material in a separate process, if forming it with a robot. The material 6376/HTS is more temperature sensitive than the other tested material, an aerospacegraded prepreg with T800 fibres.Forming tests was carried out in a vacuum forming box with the goal to find a temperature where no forming defects can be seen by eye. This is found to be true at temperatures above50 °C for the material 6376/HTS when stacked in sequence [45, 0, -45, 90]4s.None of the materials are recommended to be robot formed in room temperature. Results show that one can see correlations between the forming tests and the consolidation tests.The tests are also assessed as a good way to gain basic understanding of the characteristics of a specific material.</p>

corrected abstract:
<p>The topic of this project is an experimental study of prepreg characteristics, such as tack, consolidation and temperature sensitivity during forming. The aim has first been to understand how the material reacts during different manufacturing processes. Secondly, to recommend suitable parameter settings, based on the findings, in order to get a good and reliable manufacturing process.</p><p>In a literature study it was found that the prepreg tack is difficult to measure. It is debated by the scientific community today how to best describe prepreg tack, and the answer is affected of what parameters that are sought to be reproduced. Consolidation tests have, in this study, been performed in an Instron machine. The relaxation of two different materials has been measured in room temperature, 40 °C and 60 °C, with a maximum pressure of 2-10 bar. These limits are set to cover the temperature- and pressure scope in a robot forming process.</p><p>Results show that neither of the materials will experience full consolidation during these tests, and therefore, neither in a robot forming process. It is therefore recommended to consolidate the material in a separate process, if forming it with a robot. The material 6376/HTS is more temperature sensitive than the other tested material, an aerospace graded prepreg with T800 fibres.</p><p>Forming tests was carried out in a vacuum forming box with the goal to find a temperature where no forming defects can be seen by eye. This is found to be true at temperatures above 50 °C for the material 6376/HTS when stacked in sequence [45, 0, -45, 90]<sub<4s</sub>.</p><p>None of the materials are recommended to be robot formed in room temperature. Results show that one can see correlations between the forming tests and the consolidation tests. The tests are also assessed as a good way to gain basic understanding of the characteristics of a specific material.</p>
----------------------------------------------------------------------
In diva2:1333978 
abstract is: 
<p>Brown bears go into hibernation for several months during the winter period,but regain all bodily functions shortly after waking up, such as the strength ofbones. The aim of this thesis has been to characterise the material of activebear’s bones (tibiae) by destructive testing and then fitting a damage modelby the use of finite element simulations. Standard beam theory was used tocompare with the simulated results and supplemental compression testing wasconducted to verify elastic parameters. Examination of results show quite alarge distribution in both material parameters and determined stresses for thetested bones, with elastic moduli varying 3-10 GPa, Poisson’s ratio 0.3-0.45,strain for onset of damage 1-2%, damage rate factor of 25-40 and fracturestresses varying proportionally with stiffness between 50-190 MPa.</p>

corrected abstract:
<p>Brown bears go into hibernation for several months during the winter period, but regain all bodily functions shortly after waking up, such as the strength of bones. The aim of this thesis has been to characterise the material of active bear’s bones (tibiae) by destructive testing and then fitting a damage model by the use of finite element simulations. Standard beam theory was used to compare with the simulated results and supplemental compression testing was conducted to verify elastic parameters. Examination of results show quite a large distribution in both material parameters and determined stresses for the tested bones, with elastic moduli varying 3-10 GPa, Poisson’s ratio 0.3-0.45, strain for onset of damage 1-2%, damage rate factor of 25-40 and fracture stresses varying proportionally with stiffness between 50-190 MPa.</p>
----------------------------------------------------------------------
In diva2:1249325 
abstract is: 
<p>This thesis is about time accuracy on train delays in Sweden. The aim was topredict deviation from the scheduled time at the endstation for a given journey.The main mathematical model than has been used is linear regression with threestates of observation, ”infinitely” long before departure, at departure from firststation and when half of the stations has been passed. Data has been recievedfrom Swedish transport administration and contained around 70 000 data pointsfrom the period January 2016.The parameters that best described the aimed prediction was current delay at theobserved state, type of train and train company. There was a lot of uncertaintieswith the results due to strange contradictions in the data and both obvious andsuspected errors.i</p>

corrected abstract:
<p>This thesis is about time accuracy on train delays in Sweden. The aim was to predict deviation from the scheduled time at the end station for a given journey. The main mathematical model than has been used is linear regression with three states of observation, ”infinitely” long before departure, at departure from first station and when half of the stations has been passed. Data has been received from Swedish transport administration and contained around 70 000 data points from the period January 2016. The parameters that best described the aimed prediction was current delay at the observed state, type of train and train company. There was a lot of uncertaintieswith the results due to strange contradictions in the data and both obvious and suspected errors.</p>
----------------------------------------------------------------------
In diva2:1229785 
abstract is: 
<p>This project aimed to investigate some properties of a quantum annealer by simulatingit on a classical computer. We chose the problem Hamiltonian on the form of theIsing model with interaction energies between all qubits.The results of our investigation showed at what part of the quantum annealing theminimum energy gap typically occurs for randomly generated Hamiltonians. We sawhow the quantum annealing process should proceed in order to achieve a higher probabilityof obtaining the correct answer. Additionally, we noted that the annealing timeincreases with qubit count. Interesting phenomenon as “self-rescue” and wave-like convergencearose during the investigation. We also presented an algorithm for solving thegame Minesweeper in this paper.The results in this project can be used to find potential properties of larger systemsand also as a base for possible future extensions.</p>

corrected abstract:
<p>This project aimed to investigate some properties of a quantum annealer by simulating it on a classical computer. We chose the problem Hamiltonian on the form of the Ising model with interaction energies between all qubits.</p><p>The results of our investigation showed at what part of the quantum annealing the minimum energy gap typically occurs for randomly generated Hamiltonians. We saw how the quantum annealing process should proceed in order to achieve a higher probability of obtaining the correct answer. Additionally, we noted that the annealing time increases with qubit count. Interesting phenomenon as “self-rescue” and wave-like convergence arose during the investigation. We also presented an algorithm for solving the game Minesweeper in this paper.</p><p>The results in this project can be used to find potential properties of larger systems and also as a base for possible future extensions.</p>
----------------------------------------------------------------------
In diva2:1218988 
abstract is: 
<p>In Sweden there is currently a shortage of housing which requires a solution, a part of finding asolution lies in analyzing how people move between different regions. The aim of this report is toidentify factors that explain how many individuals move to and from different municipalities inSweden. By using regression analysis the key factors have been identified, analyzed and comparedwith previous studies.In this project models explaining moving behaviors have been developed with a coefficient ofdetermination reaching up to 68%. From these models a clear connection can be seen between movingand the number of reported crimes, the percentage of people working in the same municipalitythey live in, the percentage of students and the population of the municipality. This report canbe seen as a base analysis, that allows for further research to build upon. The project analyses asociological topic by using statistical methods. If the reader is not familiar with the mathematicalmethods, the conclusions from the report can still be understood and useful.</p>

corrected abstract:
<p>In Sweden there is currently a shortage of housing which requires a solution, a part of finding a solution lies in analyzing how people move between different regions. The aim of this report is to identify factors that explain how many individuals move to and from different municipalities in Sweden. By using regression analysis the key factors have been identified, analyzed and compared with previous studies.</p><p>In this project models explaining moving behaviors have been developed with a coefficient of determination reaching up to 68%. From these models a clear connection can be seen between moving and the number of reported crimes, the percentage of people working in the same municipality they live in, the percentage of students and the population of the municipality. This report can be seen as a base analysis, that allows for further research to build upon. The project analyses a sociological topic by using statistical methods. If the reader is not familiar with the mathematical methods, the conclusions from the report can still be understood and useful.</p>
----------------------------------------------------------------------
In diva2:1188300 
abstract is: 
<p>For a company like Scania CV AB, a vast number of laws and regulations has to be considered when developing a truck. In the constant struggle to keep the generated noise below the allowed levels, the gears are made more slender and flexible. The slenderness in combination with case hardening has brought a new type of gear fracture into the light.The Tooth Interior Fatigue Fracture, TIFF. A 2D-method, and a tool for engineers, was developed in the early 2000’s. However, this tool did not provide sufficient accuracy andcompatibility with the current design process to be adopted by the engineers at Scania.This thesis expands on the current 2D-model and attempts to improve the accuracy by bringing the analysis to 3D. Furthermore, the computational tool is developed in Pythonto allow for a more streamlined interface with the current workflow.The proposed method approximates the tooth as a cantilever-beam, and is only evaluated for this case. However, the stresses are computed with good accuracy. The onlydiscrepancy is one of the stress components, where the error is about 50%. This error isderived from the decision to, in torsion, model the cross-section of the gear tooth as an ellipse. The method has potential to be incorporated into the current design process, but the accuracy of the stresses due to torsion has to be improved, and some of the equations has to be adapted before real gear geometries can be considered.</p>

corrected abstract:
<p>For a company like Scania CV AB, a vast number of laws and regulations has to be considered when developing a truck. In the constant struggle to keep the generated noise below the allowed levels, the gears are made more slender and flexible. The slenderness in combination with case hardening has brought a new type of gear fracture into the light. The Tooth Interior Fatigue Fracture, TIFF. A 2D-method, and a tool for engineers, was developed in the early 2000’s. However, this tool did not provide sufficient accuracy and compatibility with the current design process to be adopted by the engineers at Scania. This thesis expands on the current 2D-model and attempts to improve the accuracy by bringing the analysis to 3D. Furthermore, the computational tool is developed in Python to allow for a more streamlined interface with the current workflow. The proposed method approximates the tooth as a cantilever-beam, and is only evaluated for this case. However, the stresses are computed with good accuracy. The only discrepancy is one of the stress components, where the error is about 50%. This error is derived from the decision to, in torsion, model the cross-section of the gear tooth as an ellipse. The method has potential to be incorporated into the current design process, but the accuracy of the stresses due to torsion has to be improved, and some of the equations has to be adapted before real gear geometries can be considered.</p>
----------------------------------------------------------------------
In diva2:1149193 
abstract is: 
<p>Counterparty credit risk is present in trades offinancial obligations. This master thesis investigates the up and comingtechnology blockchain and how it could be used to mitigate counterparty creditrisk. The study intends to cover essentials of the mathematical model expectedloss, along with an introduction to the blockchain technology. After modellinga simple smart contract and using historical financial data, it was evidentthat there is a possible opportunity to reduce counterparty credit risk withthe use of blockchain. From the market study of this thesis, it is obvious thatthe current financial market needs more education about blockchain technology.</p>

corrected abstract:
<p>Counterparty credit risk is present in trades of financial obligations. This master thesis investigates the up and coming technology blockchain and how it could be used to mitigate counterparty credit risk. The study intends to cover essentials of the mathematical model expected loss, along with an introduction to the blockchain technology. After modelling a simple smart contract and using historical financial data, it was evident that there is a possible opportunity to reduce counterparty credit risk with the use of blockchain. From the market study of this thesis, it is obvious that the current financial market needs more education about blockchain technology.</p>
----------------------------------------------------------------------
In diva2:1142908 
abstract is: 
<p>To enhance the control utilities of the energyprovider, smart meters are being installed nationwide to measurethe households energy consumption in real time. There is agroup of algorithms that is able to read these energy readingsand determines the states of the appliances. This opens up newpossibilities for someone who wants to exploit it and might lead toinfringement of privacy. It might lead to infringement of privacy.This paper will take a look on one approach of these algorithmsand counter measures to ensure privacy. The NIALM we chosewere working poorly in presence of more appliances hence adifferent algorithm should be used if a larger house with moreappliances desired.</p>

corrected abstract:
<p>To enhance the control utilities of the energy provider, smart meters are being installed nationwide to measure the households energy consumption in real time. There is a group of algorithms that is able to read these energy readings and determines the states of the appliances. This opens up new possibilities for someone who wants to exploit it and might lead to infringement of privacy. It might lead to infringement of privacy. This paper will take a look on one approach of these algorithms and counter measures to ensure privacy. The NIALM we chose were working poorly in presence of more appliances hence a different algorithm should be used if a larger house with more appliances desired.</p>
----------------------------------------------------------------------
In diva2:748438 
abstract is: 
<p>The characterization of the vibromixer principles, in particular FUNDAMIX® technology produced bythe Swiss company Dr.Mueller AG, is the focus of this study. Tests varying the vibration’s frequencies andamplitudes, as well as the mixing plate geometry, in terms of number of holes and their diameter, are done.Interesting results regarding these parameters are obtained, proving problem complexity and previousexperience. Higher amplitudes and frequencies result in a better fluid dynamic performance of thevibromixer, i.e. flow rate formed due to pumping capacity of the plate and creating the liquid recirculation.The available total area of the holes should be limited too. Different fluid viscosities (up to 1212mPa/s) aretested and possible carbon fiber improvements in the shaft production briefly discussed. Finally, aComputational Fluid Dynamic approach is done and possible further researches are covered.</p>

corrected abstract:
<p>The characterization of the vibromixer principles, in particular FUNDAMIX® technology produced by the Swiss company Dr.Mueller AG, is the focus of this study. Tests varying the vibration’s frequencies and amplitudes, as well as the mixing plate geometry, in terms of number of holes and their diameter, are done. Interesting results regarding these parameters are obtained, proving problem complexity and previous experience. Higher amplitudes and frequencies result in a better fluid dynamic performance of the vibromixer, i.e. flow rate formed due to pumping capacity of the plate and creating the liquid recirculation. The available total area of the holes should be limited too. Different fluid viscosities (up to 1212mPa/s) are tested and possible carbon fiber improvements in the shaft production briefly discussed. Finally, a Computational Fluid Dynamic approach is done and possible further researches are covered.</p>
----------------------------------------------------------------------
In diva2:705805 - Note: no full text in DiVA
abstract is: 
<p>This research project is part of a wider project called Smart Fault Detection and Diagnosis for HeatPump Systems currently under development by the Royal Institute of Technology (KTH).Generally, maintenance, diagnosis and repair of heat pumps are manual operations. The quality of the service relies almost exclusively on the skills, experience and motivation of the HVAC-Rtechnician. Moreover, professional technicians are only called up after a remarkable failure occursand not to perform routine follow up.The main objective of this master thesis will be to propose a method for fault detection of thebrine to water heat pump systems under operating conditions. It will be done by focusing into ninetests faults related to the ﬁrst boundary level which represents the heat pump unit, the brine andwater loop. A model based approach was developed to generate features and parameters capableof reading the status of the system. The fault detection was done by imposing test faults in themodel and evaluating the trend of the performance parameters. By comparing the predicted fault free values with the actual values (Residuals) from the model, several algorithms were proposed and conducted in order to obtain an online fault detection and diagnosis.It is concluded that the fault trend analysis can, in principle, provide a solution to detect faults inheat pump systems. The algorithms are considered user friendly tools, however more improvements needs to be done to include more faults and increase its resolution.</p>

corrected abstract:
<p>This research project is part of a wider project called Smart Fault Detection and Diagnosis for Heat Pump Systems currently under development by the Royal Institute of Technology (KTH).Generally, maintenance, diagnosis and repair of heat pumps are manual operations. The quality of the service relies almost exclusively on the skills, experience and motivation of the HVAC-R technician. Moreover, professional technicians are only called up after a remarkable failure occurs and not to perform routine follow up. The main objective of this master thesis will be to propose a method for fault detection of the brine to water heat pump systems under operating conditions. It will be done by focusing into nine tests faults related to the ﬁrst boundary level which represents the heat pump unit, the brine and water loop. A model based approach was developed to generate features and parameters capable of reading the status of the system. The fault detection was done by imposing test faults in themodel and evaluating the trend of the performance parameters. By comparing the predicted fault free values with the actual values (Residuals) from the model, several algorithms were proposed and conducted in order to obtain an online fault detection and diagnosis. It is concluded that the fault trend analysis can, in principle, provide a solution to detect faults in heat pump systems. The algorithms are considered user friendly tools, however more improvements needs to be done to include more faults and increase its resolution.</p>
----------------------------------------------------------------------
In diva2:633887 
abstract is: 
<p>Molecular dynamics simulations are a very usefultool to study the behavior and interaction of atoms and molecules in chemicaland bio-molecular systems. With the fast rising complexity of such simulationshybrid systems with both, multi-core processors (CPUs) and multiple graphics processingunits (GPUs), become more and more popular. To obtain an optimal performance thisthesis presents and evaluates two different hybrid algorithms, employing allavailable compute capacity from CPUs and GPUs. The presented algorithms can beapplied for short-range force calculations in arbitrary molecular dynamicssimulations</p>

corrected abstract:
<p>Molecular dynamics simulations are a very useful tool to study the behavior and interaction of atoms and molecules in chemical and bio-molecular systems. With the fast rising complexity of such simulations hybrid systems with both, multi-core processors (CPUs) and multiple graphics processing units (GPUs), become more and more popular. To obtain an optimal performance this thesis presents and evaluates two different hybrid algorithms, employing all available compute capacity from CPUs and GPUs. The presented algorithms can be applied for short-range force calculations in arbitrary molecular dynamics simulations.</p>
----------------------------------------------------------------------
In diva2:624555 
abstract is: 
<p>Inthis degree project, a solution on a coarse grid is recovered by fitting apartial differential equation to a few known data points. The PDE to consideris the heat equation and the Dupire’s equation with their synthetic data,including synthetic data from the Black-Scholes formula. The approach to fit aPDE is by optimal control to derive discrete approximations to regularized Hamiltoncharacteristic equations to which discrete stepping schemes, and parameters forsmoothness, are examined. By non-parametric numerical implementation thedervied method is tested and then a few suggestions on possible improvementsare given</p>

corrected abstract:
<p>In this degree project, a solution on a coarse grid is recovered by fitting a partial differential equation to a few known data points. The PDE to consider is the heat equation and the Dupire’s equation with their synthetic data, including synthetic data from the Black-Scholes formula. The approach to fit a PDE is by optimal control to derive discrete approximations to regularized Hamilton characteristic equations to which discrete stepping schemes, and parameters for smoothness, are examined. By non-parametric numerical implementation the dervied method is tested and then a few suggestions on possible improvements are given.</p>
----------------------------------------------------------------------
In diva2:618555 - Note: no full text in DiVA
abstract is: 
<p>Complex subsea oil and gas components installed on the seabed require protective structures,traditionally made of steel or glass fibre reinforced plastic – GRP. The most critical load case is trawlfishing, where trawl weights of several tons are dragged along the seabed.This master thesis work aims to develop a design methodology for such protective GRP covers and is acontinuation of the master theses work performed by REINERTSEN Jonas Elgered and Marco Nikolic in2011.The trawl load case, which is suitable for a dynamic analysis since much energy is transferred over ashort period of time, is modelled with LS DYNA, using one model built up with shell elements and onewith thick shell elements.The idea is for the methodology to work as a tool for evaluating lamina thicknesses and layups for earlycover-geometry designs.</p>

corrected abstract:
<p>Complex subsea oil and gas components installed on the seabed require protective structures, traditionally made of steel or glass fibre reinforced plastic – GRP. The most critical load case is trawl fishing, where trawl weights of several tons are dragged along the seabed. This master thesis work aims to develop a design methodology for such protective GRP covers and is a continuation of the master theses work performed by REINERTSEN Jonas Elgered and Marco Nikolic in 2011. The trawl load case, which is suitable for a dynamic analysis since much energy is transferred over a short period of time, is modelled with LS DYNA, using one model built up with shell elements and one with thick shell elements. The idea is for the methodology to work as a tool for evaluating lamina thicknesses and layups for early cover-geometry designs.</p>
----------------------------------------------------------------------
In diva2:492846 - Note: no full text in DiVA
abstract is: 
<p>SP has recognized that there today exists a need to develop structures at sea since much can be done to reduce the weight of ships by using lightweight structures hence being able to construct ships with reduced need for fuel consumption or increased load capacity. A great area of interest is then fibre composites as a means to reduce weight.To be able to be involved in the work of developing new lighter constructions, new mathematical models and metrics is needed. One step in acquiring this higher competence is to study how joints between fibre composites and steel can be done in an effective and reliable manner. One application of such joints can be a steel hull with a composite superstructure.Tests have been made on joints between steel and a carbon fibre sandwich panel. The joints are of the type fork joint where a sandwich panel is bonded in to a steel profile with the shape of a two fingered fork using structural adhesive. The tested joints has been manufactured by Kockums AB shipyard in Karlskrona.The tests have been performed under compressive loads in combination with bending.To accommodate the tests a test rig has been built.For measuring the strains and displacements in the joints a non contact optical measurement system called ARAMIS has been used.The results show that the joints were very little affected by the applied design loads implying that they are either over dimensioned or needs to be tested with stronger equipment to test the joints behaviour closer to their yield point. To study how ageing will affect the joints, and how large safety margin is needed for the joints to still be functionalafter many years at sea, would however require further investigations. The use of the ARAMIS system helped in getting a good picture of where in the joints different displacements occurred. A simple 3D FE (Finite Element) model was created and used for comparing against the measurements. The FE model showed results where the strain distribution correspondedto the tests although the strain levels not being an exact match.</p>

corrected abstract:
<p>SP has recognized that there today exists a need to develop structures at sea since much can be done to reduce the weight of ships by using lightweight structures hence being able to construct ships with reduced need for fuel consumption or increased load capacity. A great area of interest is then fibre composites as a means to reduce weight. To be able to be involved in the work of developing new lighter constructions, new mathematical models and metrics is needed. One step in acquiring this higher competence is to study how joints between fibre composites and steel can be done in an effective and reliable manner. One application of such joints can be a steel hull with a composite superstructure. Tests have been made on joints between steel and a carbon fibre sandwich panel. The joints are of the type fork joint where a sandwich panel is bonded in to a steel profile with the shape of a two fingered fork using structural adhesive. The tested joints has been manufactured by Kockums AB shipyard in Karlskrona. The tests have been performed under compressive loads in combination with bending. To accommodate the tests a test rig has been built. For measuring the strains and displacements in the joints a non contact optical measurement system called ARAMIS has been used. The results show that the joints were very little affected by the applied design loads implying that they are either over dimensioned or needs to be tested with stronger equipment to test the joints behaviour closer to their yield point. To study how ageing will affect the joints, and how large safety margin is needed for the joints to still be functional after many years at sea, would however require further investigations. The use of the ARAMIS system helped in getting a good picture of where in the joints different displacements occurred. A simple 3D FE (Finite Element) model was created and used for comparing against the measurements. The FE model showed results where the strain distribution corresponded to the tests although the strain levels not being an exact match.</p>
----------------------------------------------------------------------
In diva2:1900948 
abstract is: 
<p>The study of the flow around a car is of crucial importance in seeking energy efficiency and performance. With Volvo cars rapidly transitioning towards BEV-basedarchitecture (fully electrified fleet by 2030), newer avenues to further improve existing external vehicular aerodynamic knowledge are being explored. This requiresan update of the generic rules of thumb and an overall understanding of the fundamental principles. The goal of this work is to therefore develop an optimization andanalysis workflow that identifies the key contributors to vehicular aerodynamic drag.This will contribute to the creation of a generalized set of geometrical trends, whichis further backed up by physics and CFD results. The study’s findings align closelywith established literature, underlining the importance of adopting tophat-specificoptimization approaches. The guidelines and rules-of-thumb derived from this studymainly pertains to SUVs and select squareback configurations.</p>

corrected abstract:
<p>The study of the flow around a car is of crucial importance in seeking energy efficiency and performance. With Volvo cars rapidly transitioning towards BEV-based architecture (fully electrified fleet by 2030), newer avenues to further improve existing external vehicular aerodynamic knowledge are being explored. This requires an update of the generic rules of thumb and an overall understanding of the fundamental principles. The goal of this work is to therefore develop an optimization and analysis workflow that identifies the key contributors to vehicular aerodynamic drag. This will contribute to the creation of a generalized set of geometrical trends, which is further backed up by physics and CFD results. The study’s findings align closely with established literature, underlining the importance of adopting tophat-specific optimization approaches. The guidelines and rules-of-thumb derived from this study mainly pertains to SUVs and select squareback configurations.</p>
----------------------------------------------------------------------
In diva2:1879339 
abstract is: 
<p>This thesis explores the Turing model for pattern formation and its applicationin controlling reaction-diffusion systems. The goal is to simulate both linear andnonlinear reaction-diffusion models, to understand how patterns emerge and toinvestigate the controllability of these systems with boundary controls. Using thefinite difference method (FDM) and other numerical methods on a discretizedgrid, we generated patterns with nonlinear reaction functions, validating Turing’shypothesis that nonlinear models are more applicable for pattern formation. Thenonlinear models produced stable, organized patterns, whereas linear models re-sulted in divergence, creating unrealistic patterns. In the controllability study, wediscovered that full controllability is achieved when all control inputs are active.Our findings suggest that the placement of minimal control inputs, derived fromspecific patterns, ensures full controllability in small systems, though further re-search is needed to generalize this method to larger grids. This work underscoresthe potential of simplified models like Turing’s to provide insights into the com-plex mechanisms governing natural pattern formation.</p>

corrected abstract:
<p>This thesis explores the Turing model for pattern formation and its application in controlling reaction-diffusion systems. The goal is to simulate both linear and nonlinear reaction-diffusion models, to understand how patterns emerge and to investigate the controllability of these systems with boundary controls. Using the finite difference method (FDM) and other numerical methods on a discretized grid, we generated patterns with nonlinear reaction functions, validating Turing’s hypothesis that nonlinear models are more applicable for pattern formation. The nonlinear models produced stable, organized patterns, whereas linear models resulted in divergence, creating unrealistic patterns. In the controllability study, we discovered that full controllability is achieved when all control inputs are active. Our findings suggest that the placement of minimal control inputs, derived from specific patterns, ensures full controllability in small systems, though further research is needed to generalize this method to larger grids. This work underscores the potential of simplified models like Turing’s to provide insights into the complex mechanisms governing natural pattern formation.</p>
----------------------------------------------------------------------
In diva2:1823810 
abstract is: 
<p>This research paper revolves around the world’s oldest financial asset, gold, and whatdrives its price, which is of importance for all investors looking to be exposed to gold.The aim of this paper is to identify the main drivers behind the gold price, whichis done by performing a multiple linear regression analysis on the gold price and aset of explanatory variables. The results show that the real yield, measured as theTIPS-rate, has the largest impact on the gold price, followed by the inflation rate.The conclusion that is drawn in the paper is that it is reasonable that the real yield isthe main driver of the gold price, because the higher the real yield, the less attractiveit becomes for investors to own gold, as it is not an interest-bearing asset.</p>

corrected abstract:
<p>This research paper revolves around the world’s oldest financial asset, gold, and what drives its price, which is of importance for all investors looking to be exposed to gold. The aim of this paper is to identify the main drivers behind the gold price, which is done by performing a multiple linear regression analysis on the gold price and a set of explanatory variables. The results show that the real yield, measured as the TIPS-rate, has the largest impact on the gold price, followed by the inflation rate. The conclusion that is drawn in the paper is that it is reasonable that the real yield is the main driver of the gold price, because the higher the real yield, the less attractive it becomes for investors to own gold, as it is not an interest-bearing asset.</p>
----------------------------------------------------------------------
In diva2:1823788 
abstract is: 
<p>By analyzing the card properties of Magic the Gathering cards, models have beendeveloped to determine their impact on card prices. Previous studies have notfocused on gameplay properties, which distinguishes this work from previousresearch. To model the effect of gameplay properties, they have been quantifiedand examined using Least Squares Method and Lasso Regression, with the helpof the programming language R. The results indicate that factor directly relateradto collectability and playability have the greatest impact on the price of Magic theGathering cards. These results have been discussed from various perspectives,such as Wizards of the Coast (the publisher of Magic the Gathering), players,collectors, and investors. By focusing on gameplay properties, this study hascontributed to the field in a way that previous research has not, providing a morecomprehensive understanding of the value of Magic the Gathering cards.</p>

corrected abstract:
<p>By analyzing the card properties of Magic the Gathering cards, models have been developed to determine their impact on card prices. Previous studies have not focused on gameplay properties, which distinguishes this work from previous research. To model the effect of gameplay properties, they have been quantified and examined using Least Squares Method and Lasso Regression, with the help of the programming language R. The results indicate that factor directly relaterad to collectability and playability have the greatest impact on the price of Magic the Gathering cards. These results have been discussed from various perspectives, such as Wizards of the Coast (the publisher of Magic the Gathering), players, collectors, and investors. By focusing on gameplay properties, this study has contributed to the field in a way that previous research has not, providing a more comprehensive understanding of the value of Magic the Gathering cards.</p>
----------------------------------------------------------------------
In diva2:1780156 
abstract is: 
<p>This study aims to investigate the similarities in precision and performance between a James WebbSpace Telescope mirror actuator, the part controlling the motion of the primary mirror segments,and two 3D printed replicas. The two replicas were made from the plastics PLA and PETGand based on the designs of Zachary Tong. The plastics were examined concurrently and theresults were compared with simulations in COMSOL. The 3D printed parts were assembled and theresulting replicas were put in motion by an Arduino driven stepper motor. The result showed thatthe replica made from PLA had an average precision of 40 nm and the replica made from PETGhad an average precision of 32 nm. Compared to the James Webb Space Telescope actuator, bothreplicas were approximately one order of magnitude less accurate. The COMSOL simulations gavesimilar results. In conclusion, the study shows that the choice of material matters. The performanceof the James Webb Space Telescope actuator was more accurately emulated by the replica madefrom PETG than by the one made from PLA.</p>

corrected abstract:
<p>This study aims to investigate the similarities in precision and performance between a James Webb Space Telescope mirror actuator, the part controlling the motion of the primary mirror segments, and two 3D printed replicas. The two replicas were made from the plastics PLA and PETG and based on the designs of Zachary Tong<sup>1</sup>. The plastics were examined concurrently and the results were compared with simulations in COMSOL. The 3D printed parts were assembled and the resulting replicas were put in motion by an Arduino driven stepper motor. The result showed that the replica made from PLA had an average precision of 40 nm and the replica made from PETG had an average precision of 32 nm. Compared to the James Webb Space Telescope actuator, both replicas were approximately one order of magnitude less accurate. The COMSOL simulations gave similar results. In conclusion, the study shows that the choice of material matters. The performance of the James Webb Space Telescope actuator was more accurately emulated by the replica made from PETG than by the one made from PLA.</p>

<p><sup>1</sup>Zachary Tong. JWST Mirror Actuator. URL: https://www.thingiverse.com/thing:5232214/apps. (Date accessed: 2023-02-02).</p>

----------------------------------------------------------------------
In diva2:1757003 - Note: no full text in DiVA
abstract is: 
<p>Finding root causes for the differences between countries in life expectancy is relevant forhealth organisations and governments when prioritising public health projects. Previousstudies on the topic have seen a positive correlation between GDP per capita and lifeexpectancy, as well as levels of education and sanitary conditions. Using data from 183countries from the year 2015, this thesis uses multiple regression analysis to study 11different variables correlation to life expectancy at birth. The final result shows that the6 variables GDP per capita, access to drinking water and electricity, low average BMI,and concentration of doctors and pharmacists had significant positive correlations tolife expectancy at birth, while high average BMI had a significant negative correlation.Since the study is only based on one year, a more extensive study is proposed to backup the indications. Such a study could also incorporate health insurance system andlevel of economical inequality as variables.</p>

corrected abstract:
<p>Finding root causes for the differences between countries in life expectancy is relevant for health organisations and governments when prioritising public health projects. Previous studies on the topic have seen a positive correlation between GDP per capita and lifeexpectancy, as well as levels of education and sanitary conditions. Using data from 183countries from the year 2015, this thesis uses multiple regression analysis to study 11different variables correlation to life expectancy at birth. The final result shows that the 6 variables GDP per capita, access to drinking water and electricity, low average BMI, and concentration of doctors and pharmacists had significant positive correlations to life expectancy at birth, while high average BMI had a significant negative correlation. Since the study is only based on one year, a more extensive study is proposed to backup the indications. Such a study could also incorporate health insurance system and level of economical inequality as variables.</p>
----------------------------------------------------------------------
In diva2:1752017 
abstract is: 
<p>Understanding how a system behaves when exposed to different scenarios is key when improvingand developing complex structures. The amount of different approaches is immense and variesfrom case to case. One of the simpler approaches is black-box modelling as it only targetsan input and output to a system, and not necessarily the mathematical interpretations. Fora nonlinear system such as a remote controlled weapon station, this approach is appropriate,as it allows to only focus on a certain scenario and the results obtained for that case. In thisstudy, a remote controlled weapon station is further investigated when exposed to disturbancesfrom a combat vehicle. The data obtained is simulated on a platform and the results are usedin Matlab to analyze and find the best model from these tests. A Hammerstein-Wiener modelwith nonlinear wavelet networks is deemed the best as it gives the most accurate representationof the station’s behavior. The results obtained are considered to be moderately accurate dueto its precision and should only be used as reference point, rather than being interpreted as atrue representation of the system.</p>

corrected abstract:
<p>Understanding how a system behaves when exposed to different scenarios is key when improving and developing complex structures. The amount of different approaches is immense and varies from case to case. One of the simpler approaches is black-box modelling as it only targets an input and output to a system, and not necessarily the mathematical interpretations. For a nonlinear system such as a remote controlled weapon station, this approach is appropriate, as it allows to only focus on a certain scenario and the results obtained for that case. In this study, a remote controlled weapon station is further investigated when exposed to disturbances from a combat vehicle. The data obtained is simulated on a platform and the results are used in Matlab to analyze and find the best model from these tests. A Hammerstein-Wiener model with nonlinear wavelet networks is deemed the best as it gives the most accurate representation of the station’s behavior. The results obtained are considered to be moderately accurate due to its precision and should only be used as reference point, rather than being interpreted as a true representation of the system.</p>
----------------------------------------------------------------------
In diva2:1708047 
abstract is: 
<p>Lead Halide Perovskite is emerging quickly as a promising material for the future solar cellsthanks to their inherent good optoelectrical properties along with their cheap and facile fabri-cation. However, their main drawback before commercialization is their weak stability. In thiswork, a novel carbon-coated perovskite quantum dot has been synthesized, and is to the extentof our knowledge, for the first time. The coated perovskite quantum dots show a remarkable in-creased stability under different conditions while in solution. Their photoluminescence intensityalso increased as time went on, exceeding that of the uncoated perovskite quantum dots aftera few weeks. These coated perovskite quantum dots, while not fully characterized and thusnot fully understood show a promising way on how to combat the low stability in perovskites.Further, Copper/Copper(I)Iodide core/shell nanowires were synthesized as a transparent inte-grated hole transport layer/electrode for solar cells. While limited due to the low controlledfabrication process used, they providing a solid base for further research on the material to beused in solar cells.</p>

corrected abstract:
<p>Lead Halide Perovskite is emerging quickly as a promising material for the future solar cells thanks to their inherent good optoelectrical properties along with their cheap and facile fabrication. However, their main drawback before commercialization is their weak stability. In this work, a novel carbon-coated perovskite quantum dot has been synthesized, and is to the extent of our knowledge, for the first time. The coated perovskite quantum dots show a remarkable increased stability under different conditions while in solution. Their photoluminescence intensity also increased as time went on, exceeding that of the uncoated perovskite quantum dots after a few weeks. These coated perovskite quantum dots, while not fully characterized and thus not fully understood show a promising way on how to combat the low stability in perovskites. Further, Copper/Copper(I)Iodide core/shell nanowires were synthesized as a transparent integrated hole transport layer/electrode for solar cells. While limited due to the low controlled fabrication process used, they providing a solid base for further research on the material to be used in solar cells.</p>
----------------------------------------------------------------------
In diva2:1680714 
abstract is: 
<p>In this paper the Clifford Algebra is introduced and proposed as analternative to Gibbs' vector algebra as a unifying language for geometricoperations on vectors. Firstly, the algebra is constructed using a quotientof the tensor algebra and then its most important properties are proved,including how it enables division between vectors and how it is connected tothe exterior algebra. Further, the Clifford algebra is shown to naturallyembody the complex numbers and quaternions, whereupon its strength indescribing rotations is highlighted. Moreover, the wedge product, is shown asa way to generalize the cross product and reveal the true nature ofpseudovectors as bivectors. Lastly, we show how replacing the cross productwith the wedge product, within the Clifford algebra, naturally leads tosimplifying Maxwell's equations to a single equation.</p>

corrected abstract:
<p>In this paper the Clifford Algebra is introduced and proposed as an alternative to Gibbs' vector algebra as a unifying language for geometric operations on vectors. Firstly, the algebra is constructed using a quotient of the tensor algebra and then its most important properties are proved, including how it enables division between vectors and how it is connected to the exterior algebra. Further, the Clifford algebra is shown to naturally embody the complex numbers and quaternions, where upon its strength in describing rotations is highlighted. Moreover, the wedge product, is shown as a way to generalize the cross product and reveal the true nature of pseudovectors as bivectors. Lastly, we show how replacing the cross product with the wedge product, within the Clifford algebra, naturally leads to simplifying Maxwell's equations to a single equation.</p>
----------------------------------------------------------------------
In diva2:1510573 
abstract is: 
<p>This thesis focuses on the optimization of the electric aircraft propeller in order to increaseflight performance. Electric aircraft have limited energy, particularly the electricmotor torque compared to the fuel engine torque. For that, redesign of the propeller forelectric aircraft is important in order to improve the propeller efficiency. The airplanepropeller theory for Glauert is selected as a design method and incorporated with Brattimprovements of the theory. Glauert theory is a combination of the axial momentum andblade element theory. Pipistrel Alpha Electro airplane specifications have been chosen asa model for the design method. Utilization of variable pitch propeller and the influence ofnumber of blades has been investigated. The obtained design results show that the variablepitch propellers at cruise speed and altitude 3000 m reducing the power consumptionby 0.14 kWh and increase the propeller efficiency by 0.4% compared to the fixed pitchpropeller. Variable pitch propeller improvement was pretty good for electric aircraft. Theoptimum blade number for the design specifications is 3 blades.</p>

corrected abstract:
<p>This thesis focuses on the optimization of the electric aircraft propeller in order to increase flight performance. Electric aircraft have limited energy, particularly the electric motor torque compared to the fuel engine torque. For that, redesign of the propeller for electric aircraft is important in order to improve the propeller efficiency. The airplane propeller theory for Glauert is selected as a design method and incorporated with Bratt improvements of the theory. Glauert theory is a combination of the axial momentum and blade element theory. Pipistrel Alpha Electro airplane specifications have been chosen as a model for the design method. Utilization of variable pitch propeller and the influence of number of blades has been investigated. The obtained design results show that the variable pitch propellers at cruise speed and altitude 3000 m reducing the power consumption by 0.14 kWh and increase the propeller efficiency by 0.4% compared to the fixed pitch propeller. Variable pitch propeller improvement was pretty good for electric aircraft. The optimum blade number for the design specifications is 3 blades.</p>
----------------------------------------------------------------------
In diva2:1464096 
abstract is: 
<p>The lack of strong measures to avoid the possible fatal consequences of global warming is pushing researchers to look for other alternatives such as geoengineering.Within geoengineering, this study focuses on the space based solar radiation management methods. More precisely, the project evaluates the feasibilityof implementing a space sun shade near the first Lagrangian point in the Sun-Earth system within a thirty year period time from now. The study isstructured in three main blocks: spacecraft configuration, trajectory definition and launch. An analysis looking at the minimum cost system was carried out,starting with the definition of the mass and size of spacecraft. Furthermore, an optimization of the trajectory was developed in order to minimize the traveltime to the vicinity of the Lagrangian point. The shades will be formed by swarms of 10 000m2 solar sails that will cover an area of 6:3 x 1012 m2 witha total mass of around 5:7 x 1010 kg. The sails will be injected into a LEO and will start a trajectory to the vicinity of the first Lagrangian point that willtake around 2.3 years. The total cost of the project is approximated to be 10 trillion dollars. The mission appears to be feasible from a technological pointof view, with some development needed in the attitude control subsystem. The main challenge will be the launch of all the spacecraft. A space mission of thisdimensions has never been attempted before so it will require a big advance from the launch vehicle industry.</p>

corrected abstract:
<p>The lack of strong measures to avoid the possible fatal consequences of global warming is pushing researchers to look for other alternatives such as geoengineering. Within geoengineering, this study focuses on the space based solar radiation management methods. More precisely, the project evaluates the feasibility of implementing a space sun shade near the first Lagrangian point in the Sun-Earth system within a thirty year period time from now. The study is structured in three main blocks: spacecraft configuration, trajectory definition and launch. An analysis looking at the minimum cost system was carried out, starting with the definition of the mass and size of spacecraft. Furthermore, an optimization of the trajectory was developed in order to minimize the travel time to the vicinity of the Lagrangian point. The shades will be formed by swarms of 10 000 m<sup>2</sup> solar sails that will cover an area of 6.3 × 10<sup>12</sup> m<sup>2</sup> with a total mass of around 5.7 × 10<sup>10</sup> kg. The sails will be injected into a LEO and will start a trajectory to the vicinity of the first Lagrangian point that will take around 2.3 years. The total cost of the project is approximated to be 10 trillion dollars. The mission appears to be feasible from a technological point of view, with some development needed in the attitude control subsystem. The main challenge will be the launch of all the spacecraft. A space mission of this dimensions has never been attempted before so it will require a big advance from the launch vehicle industry.</p>
----------------------------------------------------------------------
In diva2:1242864 
abstract is: 
<p>In 2015, cardiovascular diseases caused the death of 17.7 million people - 31 % of all deaths globally - making it deadlier than any other cause.Cardiovascular diseases often result from Arteriosclerosis, a disease where plaque builds up and clogs arteries. As this occurs in the carotid arteries and the plaque material is subject to forces caused by blood pressure and flow, the stress that occurs within could lead to rupture of the material. The ruptured particles could then travel to the brain and cause a stroke.To identify and prevent these strokes, computed tomography and ultrasound scans are used today. With the help of such examination, it’s possible to see how much of the artery that is clogged and decide whether or not to operate on the patient. Another way of analyzing the plaques vulnerability of rupture is with the help of biomechanics. Based on the geometry and material composition of the plaque, and using a finite element analysis, the stresses and risk of rupture can be estimated.This thesis invetigates how different plaque geometries, with focus on plaque length and cross- section area coverage, affects the resulting stress in the material. The study was carried out in a total of 12 different plaque models. The models were based on two different lengths, 3 mm and 10 mm and four different cross-sectional coverages.With computed tomography as reference, models of the carotids and plaque were built using a computer-aided design program. The models were imported to the finite element program COMSOL and analyzed using fluid-mechanical and solid-mechanical simulations. The simula- tions were executed on non-linear solid-state simulations and linear-elastic tissue models and the Newtonian fluid assumption.based on the investigated plaque functionalities, this study found that the stress in the plaque tissue shows a peak at 50-60% plaque coverage of the cross-sectional area, such that the risk of rupture seems to be the highest at this area coverage.</p>

corrected abstract:
<p>In 2015, cardiovascular diseases caused the death of 17.7 million people - 31 % of all deaths globally - making it deadlier than any other cause.</p><p>Cardiovascular diseases often result from Arteriosclerosis, a disease where plaque builds up and clogs arteries. As this occurs in the carotid arteries and the plaque material is subject to forces caused by blood pressure and flow, the stress that occurs within could lead to rupture of the material. The ruptured particles could then travel to the brain and cause a stroke.</p><p>To identify and prevent these strokes, computed tomography and ultrasound scans are used today. With the help of such examination, it’s possible to see how much of the artery that is clogged and decide whether or not to operate on the patient. Another way of analyzing the plaques vulnerability of rupture is with the help of biomechanics. Based on the geometry and material composition of the plaque, and using a finite element analysis, the stresses and risk of rupture can be estimated.</p><p>This thesis invetigates how different plaque geometries, with focus on plaque length and cross-section area coverage, affects the resulting stress in the material. The study was carried out in a total of 12 different plaque models. The models were based on two different lengths, 3 mm and 10 mm and four different cross-sectional coverages.</p><p>With computed tomography as reference, models of the carotids and plaque were built using a computer-aided design program. The models were imported to the finite element program COMSOL and analyzed using fluid-mechanical and solid-mechanical simulations. The simulations were executed on non-linear solid-state simulations and linear-elastic tissue models and the Newtonian fluid assumption.</p><p>based on the investigated plaque functionalities, this study found that the stress in the plaque tissue shows a peak at 50-60% plaque coverage of the cross-sectional area, such that the risk of rupture seems to be the highest at this area coverage.</p>


Note: The last paragraph starts with a lower case letter, this is in the original abstract,
----------------------------------------------------------------------
In diva2:1229933 - missing space in title:
"Analysing ConformationalEnsembles using Machine Learning"
==>
"Analysing Conformational Ensembles using Machine Learning"

abstract is: 
<p>G-protein coupled receptors are involved in many diseases and act as target for severalpharmaceutical drugs. To develop methods to understand the dynamics of proteinsinvolved in the communication through the cell membrane is therefore crucial. In thisstudy, data from molecular dynamics trajectories of the !2-adrenergic receptor were studiedwith the aim to train a neural network to identify the receptor due to its bindingmechanism. Represented as conformational ensembles, the protein was examined to identifythe binding state of the receptor based on determinants of its conformation. Thethesis demonstrate that this conceptually simple method can be used to computationallyanalyze the molecular response on drug binding.</p>

corrected abstract:
<p>G-protein coupled receptors are involved in many diseases and act as target for several pharmaceutical drugs. To develop methods to understand the dynamics of proteins involved in the communication through the cell membrane is therefore crucial. In this study, data from molecular dynamics trajectories of the <em>&beta;</em><sub>2</sub>-adrenergic receptor were studied with the aim to train a neural network to identify the receptor due to its binding mechanism. Represented as conformational ensembles, the protein was examined to identify the binding state of the receptor based on determinants of its conformation. The thesis demonstrate that this conceptually simple method can be used to computationally analyze the molecular response on drug binding.</p>
----------------------------------------------------------------------
In diva2:1218565 
abstract is: 
<p>This paper analyzes a data set obtained from a recent study per ormed at The Karolinska Institute. The data set is comprised of 131 children with anxiety disorders, aged 8 - 12, who all underwent a novel treatment against their disorder called internet-delivered cognitive behavior therapy (ICBT). The data set contains standardized clinical severity ratings (CSR) of the patients before and and after the treatment, as well as 233 features for each patient (demographicinformation, symptom reports, information on other diagnoses etc). Before thetreatment, the clinicians also made a "guess", scored on a scale of 1 - 10, answering the question"How successful will ICBT treatment be for this particular patient?". Firstly, this studyfound that the clinicians predicted remission with an accuracy of approximately 50%. Secondly,this study employed machine learning algorithms designed to learn from the data setand make predictions based on the feature information of each particular patient. The topperforming algorithm predicted with an accuracy of 70%. This study therefore suggests thatmachine learning algorithms can predict outcome of ICBT treatmentwith a higher level of accuracythan clinicians. This study then addresses its weaknesses and limitations to this conclusion,most importantly the vagueness of the question and scale that the clinicians basedtheir guesses on.</p>

corrected abstract:
<p>This paper analyzes a data set obtained from a recent study performed at The Karolinska Institute. The data set is comprised of 131 children with anxiety disorders, aged 8 - 12, who all underwent a novel treatment against their disorder called internet-delivered cognitive behavior therapy (ICBT). The data set contains standardized clinical severity ratings (CSR) of the patients before and and after the treatment, as well as 233 features for each patient (demographic information, symptom reports, information on other diagnoses etc). Before the treatment, the clinicians also made a "guess", scored on a scale of 1 - 10, answering the question "How successful will ICBT treatment be for this particular patient?". Firstly, this study found that the clinicians predicted remission with an accuracy of approximately 50%. Secondly, this study employed machine learning algorithms designed to learn from the data set and make predictions based on the feature information of each particular patient. The top performing algorithm predicted with an accuracy of 70%. This study therefore suggests that machine learning algorithms can predict outcome of ICBT treatment with a higher level of accuracy than clinicians. This study then addresses its weaknesses and limitations to this conclusion, most importantly the vagueness of the question and scale that the clinicians based their guesses on.</p>
----------------------------------------------------------------------
In diva2:1215665 
abstract is: 
<p>The aim of this thesis is to investigate thehedging error in Credit Value Adjustment (CVA) produced by using a model forthe simulation of the risk factors different from the one used in the pricingof the derivative contract. The hypothesis is that this inconsistency betweensimulation and pricing models affects the CVA leading to an error in thehedging of credit counterparty risk. When computing the CVA, market factors aresimulated forward in time and the portfolio is priced in each scenario toobtain the Expected Positive Exposure (EPE). To hedge the market risk of CVA weuse a dynamic Delta-hedging strategy. We investigate the hedging error for adefault free portfolio and for its CVA and how it is affected by the mismatchbetween the models.</p>

corrected abstract:
<p>The aim of this thesis is to investigate the hedging error in Credit Value Adjustment (CVA) produced by using a model for the simulation of the risk factors different from the one used in the pricing of the derivative contract. The hypothesis is that this inconsistency between simulation and pricing models affects the CVA leading to an error in the hedging of credit counterparty risk. When computing the CVA, market factors are simulated forward in time and the portfolio is priced in each scenario to obtain the Expected Positive Exposure (EPE). To hedge the market risk of CVA we use a dynamic Delta-hedging strategy. We investigate the hedging error for a default free portfolio and for its CVA and how it is affected by the mismatch between the models.</p>
----------------------------------------------------------------------
In diva2:1142748 
abstract is: 
<p>This paper investigates a smart heat control systeminside a household using a family of dynamic controllers calledPID (Proportional Integrative Derivative), conducted withWireless Sensor Networks (WSN). The laws of thermodynamicsand other physical knowledge was used to model the effect that aheat exchanger has on the temperature of a room. Realisticsimulations have been run to illustrate how a PID control systemoutperforms the traditional on-off approach, in terms of energyefficiency. Simulations shows that the PID controlled system is28% more energy efficient than an on-off. This can make thefuture use of PIDs in home automation more common. Lastly afuture extension to other systems is discussed.</p>

corrected abstract:
<p>This paper investigates a smart heat control system inside a household using a family of dynamic controllers called PID (Proportional Integrative Derivative), conducted with Wireless Sensor Networks (WSN). The laws of thermodynamics and other physical knowledge was used to model the effect that a heat exchanger has on the temperature of a room. Realistic simulations have been run to illustrate how a PID control system outperforms the traditional on-off approach, in terms of energy efficiency. Simulations shows that the PID controlled system is 28% more energy efficient than an on-off. This can make the future use of PIDs in home automation more common. Lastly a future extension to other systems is discussed.</p>
----------------------------------------------------------------------
In diva2:1120495 
abstract is: 
<p>Text data mining is a growing research field where machine learning and NLP areimportant technologies. There are multiple applications concerning categorizinglarge sets of documents. Depending on the size of the documents the methodsdi↵er, when it comes to short text documents the information in individualones are scant. The aim of this paper is to show how well unsupervised textclustering reflects existing class assignments and how sensitive clustering is whencomparing di↵erent text representation and feature selection. The raw datawas collected from several national health surveys. Evaluation was made with aconditional entropy-based method called V-measure which connects the clustersto the categories. We present that some methods perform significantly betteragainst raw data then others.</p>

corrected abstract:
<p>Text data mining is a growing research field where machine learning and NLP are important technologies. There are multiple applications concerning categorizing large sets of documents. Depending on the size of the documents the methods differ, when it comes to short text documents the information in individual ones are scant. The aim of this paper is to show how well unsupervised text clustering reflects existing class assignments and how sensitive clustering is when comparing different text representation and feature selection. The raw data was collected from several national health surveys. Evaluation was made with a conditional entropy-based method called V-measure which connects the clusters to the categories. We present that some methods perform significantly better against raw data then others.</p>
----------------------------------------------------------------------
In diva2:1106219 
abstract is: 
<p>The modeling of non-maturity deposits is a topic that is highlyimportant to many banks because of the large amount of funding that comes fromthese products. It is also a topic that currently is in the focus oflegislators. Although a non-maturity deposit may seem to be a trivial product,it has several characteristics that make it rather complex. One of the twopurposes of this thesis is to compare different models for the deposit rate ofnon-maturity deposits and to investigate the strengths and weaknesses of themodels. The other purpose is to find a new model for the deposit rate ofnon-maturity deposits. Several different models that are suggested in the literatureare described and evaluated based on the four aspects; goodness of fit,stability, negative interest rate environment and simplicity. Three new modelsfor the deposit rate are suggested in this thesis, one of which shows a verygood performance compared to the models that can be found in the literature.</p>

corrected abstract:
<p>The modeling of non-maturity deposits is a topic that is highly important to many banks because of the large amount of funding that comes from these products. It is also a topic that currently is in the focus of legislators. Although a non-maturity deposit may seem to be a trivial product, it has several characteristics that make it rather complex. One of the two purposes of this thesis is to compare different models for the deposit rate of non-maturity deposits and to investigate the strengths and weaknesses of the models. The other purpose is to find a new model for the deposit rate of non-maturity deposits. Several different models that are suggested in the literature are described and evaluated based on the four aspects; goodness of fit, stability, negative interest rate environment and simplicity. Three new models for the deposit rate are suggested in this thesis, one of which shows a very good performance compared to the models that can be found in the literature.</p>
----------------------------------------------------------------------
In diva2:1078080 
abstract is: 
<p>A key aspect in Attitude and Orbital Control Systems (AOCS) is the ability to estimateaccurately a spacecraft’s attitude and angular rate. While a classic gyrometer and star-trackerhybridization features high performances, gyroless solutions using only star-trackers as attitudesensors are often chosen to reduce the mission’s cost, when possible. Such an estimation is improvedusing the knowledge of the torque commanded by the spacecraft’s reaction wheels. The torqueestimate is however imperfect, mostly due to friction occurring on the wheels’ shafts. This paperpresents the research undertaken to improve this knowledge by adding a secondary torqueestimator to an existing gyroless estimator. An additional filter is developed for that matter, tuned,and the global AOCS performance is tested in diverse operating conditions to assess the newmethod’s benefits. While converged state performances are not improved, this solution is shown tomake the estimator more robust to friction torque spikes occurring on some reaction wheels, and toimprove settling performances after spacecraft manoeuvers.</p>

corrected abstract:
<p>A key aspect in Attitude and Orbital Control Systems (AOCS) is the ability to estimate accurately a spacecraft’s attitude and angular rate. While a classic gyrometer and star-tracker hybridization features high performances, gyroless solutions using only star-trackers as attitude sensors are often chosen to reduce the mission’s cost, when possible. Such an estimation is improved using the knowledge of the torque commanded by the spacecraft’s reaction wheels. The torque estimate is however imperfect, mostly due to friction occurring on the wheels’ shafts. This paper presents the research undertaken to improve this knowledge by adding a secondary torque estimator to an existing gyroless estimator. An additional filter is developed for that matter, tuned, and the global AOCS performance is tested in diverse operating conditions to assess the new method’s benefits. While converged state performances are not improved, this solution is shown to make the estimator more robust to friction torque spikes occurring on some reaction wheels, and to improve settling performances after spacecraft manoeuvers.</p>
----------------------------------------------------------------------
In diva2:893788 
abstract is: 
<p>Even though there are no regulations on the interior noise level of passenger cars, it is a significant quality aspect both for customers and for car manufacturers. The reduction of many other car noise sources pushed tyre road noise to the forefront.What is more, well known phenomenon of the tyre acoustic cavity resonance (TCR), appearing around 225 Hz, makes the interior noise noticeably worse. Some techniques to mitigate this phenomenon right at the source are discussed in this thesis, however, these has not been adopted by the tyre nor car manufacturers yet.Therefore, there is a desire to minimise at least the transmission of the acoustic or vibration energy from the tyre to the compartment. This is where methods like TPA (Transfer Path Analysis) come into play.In this thesis, two different approaches to TPA are used to investigate transmission of the TCR energy.First, the coherence based road decomposition method is used to determine whether the TCR energy is transmitted by a structure-borne or an air-borne mechanism. The same method serves to identify if the TCR noise comes mainly from the front or the rear suspension.Second, the impedance matrix method was used to determine critical structure-borne transfer paths yielding clear results indicating two critical mounts at the rear suspension which dominate the transfer of vibro-acoustic energy. Subsequent physical modification of the critical mount was tested to verify the results of the transmission study.Moreover, deflection shape analysis of the tyre, rim, front and rear suspension was performed to identify possible amplification effects of the TCR phenomenon.</p>

corrected abstract:
<p>Even though there are no regulations on the interior noise level of passenger cars, it is a significant quality aspect both for customers and for car manufacturers. The reduction of many other car noise sources pushed tyre road noise to the forefront.</p><p>What is more, well known phenomenon of the tyre acoustic cavity resonance (TCR), appearing around 225 Hz, makes the interior noise noticeably worse. Some techniques to mitigate this phenomenon right at the source are discussed in this thesis, however, these has not been adopted by the tyre nor car manufacturers yet.</p><p>Therefore, there is a desire to minimise at least the transmission of the acoustic or vibration energy from the tyre to the compartment. This is where methods like TPA (Transfer Path Analysis) come into play.</p><p>In this thesis, two different approaches to TPA are used to investigate transmission of the TCR energy.</p><p>First, the coherence based road decomposition method is used to determine whether the TCR energy is transmitted by a structure-borne or an air-borne mechanism. The same method serves to identify if the TCR noise comes mainly from the front or the rear suspension.</p><p>Second, the impedance matrix method was used to determine critical structure-borne transfer paths yielding clear results indicating two critical mounts at the rear suspension which dominate the transfer of vibro-acoustic energy. Subsequent physical modification of the critical mount was tested to verify the results of the transmission study.</p><p>Moreover, deflection shape analysis of the tyre, rim, front and rear suspension was performed to identify possible amplification effects of the TCR phenomenon.</p>
----------------------------------------------------------------------
In diva2:623725 
abstract is: 
<p>The aim of this paper is to see whether one can improve on the naiveforecast of Euro Area inflation, where by naive forecast we mean theyear-over-year inflation rate one-year ahead will be the same as the past year.Various model selection procedures are employed on anautoregressive-moving-average model and several Phillips curvebasedmodels. We test also whether we can improve on the Euro Area inflation forecastby first forecasting the sub-components and aggregating them. We manage tosubstantially improve on the forecast by using a Phillips curve based model. Wealso find further improvement by forecasting the sub-components first andaggregating them to Euro Area inflation</p>

corrected abstract:
<p>The aim of this paper is to see whether one can improve on the naive forecast of Euro Area inflation, where by naive forecast we mean the year-over-year inflation rate one-year ahead will be the same as the past year. Various model selection procedures are employed on an autoregressive-moving-average model and several Phillips curve based models. We test also if we can improve on the Euro Area inflation forecast by first forecasting the sub-components and aggregating them. We manage to substantially improve on the forecast by using a Phillips curve based model. We also find further improvement by forecasting the sub-components first and aggregating them to Euro Area inflation</p>
----------------------------------------------------------------------
In diva2:622286 
abstract is: 
<p>Thisreport investigates whether there are sufficient differences between a bank'sdepositors to motivate price discrimination. This is done by looking at timeseries of individual depositors to try to find predictors by a regressionanalysis. To be able to conclude on the value of more stable deposits for thebank and hence deduce a price, one also needs to look at regulatory aspects ofdeposits and different depositors. Once these qualities of a deposit have beenassigned by both the bank and regulator, they need to be transformed into aprice. This is done by replicationwith market funding instruments.</p>


corrected abstract:
<p>This report investigates whether there are sufficient differences between a bank's depositors to motivate price discrimination. This is done by looking at timeseries of individual depositors to try to find predictors by a regression analysis. To be able to conclude on the value of more stable deposits for the bank and hence deduce a price, one also needs to look at regulatory aspects of deposits and different depositors. Once these qualities of a deposit have been assigned by both the bank and regulator, they need to be transformed into a price. This is done by replication with market funding instruments.</p>
----------------------------------------------------------------------
In diva2:610366 
abstract is: 
<p>The objective of this thesis is to analyse theconnection between test results and field claims of ECUs (electronic controlunits) at Scania in order to improve the acceptance criteria and evaluatesoftware testing strategies. The connection is examined through computation ofdifferent measures of dependencies such as the Pearson’s correlation, Spearman’srank correlation and Kendall’s tau. The correlations are computed from testresults in different ECU projects and considered in a predictive model based onlogistic regression. Numerical results indicate a weak connection between testresults and field claims. This is partly due to insufficient number of ECUprojects and the lack of traceability of field claims and test results. Themain conclusion confirms the present software testing strategy. Continuoussoftware release and testing results in a lower field claim and thus a betterproduct.</p>

corrected abstract:
<p>The objective of this thesis is to analyse the connection between test results and field claims of ECUs (electronic control units) at Scania in order to improve the acceptance criteria and evaluate software testing strategies. The connection is examined through computation of different measures of dependencies such as the Pearson’s correlation, Spearman’s rank correlation and Kendall’s tau. The correlations are computed from test results in different ECU projects and considered in a predictive model based on logistic regression. Numerical results indicate a weak connection between test results and field claims. This is partly due to insufficient number of ECU projects and the lack of traceability of field claims and test results. The main conclusion confirms the present software testing strategy. Continuous software release and testing results in a lower field claim and thus a better product.</p>
----------------------------------------------------------------------
In diva2:515572 
abstract is: 
<p>The aim of this study is to investigate how chemistry teachers at secondary school anduniversities work with learning strategies and models of perception. This thesis leans on theworks of Lev Vygotskij and his likes but also on the theory of conceptual metaphors by GeorgeLakoff and Mark Johnson as a tool to analyze the collected data.The Empirical data that this study is based upon have been collected through semi-structuredinterviews with teachers as primary respondents.The study has resulted in a two practical assignments for secondary school which are builtaround the concept of open assignments. This thesis has also generated a suggestion for a newstructure of the bachelor programme in chemistry at KTH.</p>

corrected abstract:
<p>The aim of this study is to investigate how chemistry teachers at secondary school and universities work with learning strategies and models of perception. This thesis leans on the works of Lev Vygotskij and his likes but also on the theory of conceptual metaphors by George Lakoff and Mark Johnson as a tool to analyze the collected data.</p><p>The Empirical data that this study is based upon have been collected through semi-structured interviews with teachers as primary respondents.</p><p>The study has resulted in a two practical assignments for secondary school which are built around the concept of open assignments. This thesis has also generated a suggestion for a new structure of the bachelor programme in chemistry at KTH.</p>
----------------------------------------------------------------------
In diva2:460410 
abstract is: 
<p>Within the course of the master thesis project, two thermal phase change models for direct contact conden-sation were developed with different modeling approaches, namely interfacial heat transfer and combustionanalysis approach.After understanding the OpenFOAM framework for two phase flow solvers with phase change capabilities,a new solver, including the two developed models for phase change, was implemented under the name ofinterPhaseChangeCondenseTempFoam and analyzed in a series of 18 tests in order to determine the physicalbehavior and robustness of the developed models. The solvers use a volume-of-fluid (VOF) approach withmixed fluid properties.It has been shown that the approach with inter-facial heat transfer shows physical behavior, a strong timestep robustness and good grid convergence properties. The solver can be used as a basis for more advancedsolvers within the phase change class.</p>

corrected abstract:
<p>Within the course of the master thesis project, two thermal phase change models for direct contact condensation were developed with different modeling approaches, namely interfacial heat transfer and combustion analysis approach.</p><p>After understanding the OpenFOAM framework for two phase flow solvers with phase change capabilities, a new solver, including the two developed models for phase change, was implemented under the name of interPhaseChangeCondenseTempFoam and analyzed in a series of 18 tests in order to determine the physical behavior and robustness of the developed models. The solvers use a volume-of-fluid (VOF) approach with mixed fluid properties.</p><p>It has been shown that the approach with inter-facial heat transfer shows physical behavior, a strong time step robustness and good grid convergence properties. The solver can be used as a basis for more advanced solvers within the phase change class.</p>
----------------------------------------------------------------------
In diva2:1900965 
abstract is: 
<p>This thesis looks into designing brake cooling ducts in a parametric wayas possible. This is absolutely necessary even though it is time-consumingdue to constant changes during the prototype phase of the car, the partssurrounding it are constantly changed which can affect the design of the ductas well. The study also looks into the computer fluid simulation optimizationby minimizing the pressure drop and looking at velocity distribution acrossa planar cross-section and flow separations inside of the duct by changingvarious parameters of it. After the design phase, the engineering aspects ofthe duct are done which involves the methods of mounting and design of thebrake ducts, packaging of the system, the materials used, and the carbon fiberlayup for the duct along with surface quality as per Koenigsegg standards, andthe methods to make the inside surface of the duct smooth.</p>

corrected abstract:
<p>This thesis looks into designing brake cooling ducts in a parametric way as possible. This is absolutely necessary even though it is time-consuming due to constant changes during the prototype phase of the car, the parts surrounding it are constantly changed which can affect the design of the duct as well. The study also looks into the computer fluid simulation optimization by minimizing the pressure drop and looking at velocity distribution across a planar cross-section and flow separations inside of the duct by changing various parameters of it. After the design phase, the engineering aspects of the duct are done which involves the methods of mounting and design of the brake ducts, packaging of the system, the materials used, and the carbon fiber layup for the duct along with surface quality as per Koenigsegg standards, and the methods to make the inside surface of the duct smooth.</p>
----------------------------------------------------------------------
In diva2:1881041 
abstract is: 
<p>This thesis details the creation and development of an experimental setup to test ground effectin the L2000 wind tunnel at KTH. Ground effect is an important aerodynamic phenomenonobserved in areas such as aviation and motorsports. The research includes a comprehen-sive literature study and design process, encompassing analytical calculations, finite elementmethod (FEM) simulations, and computational fluid dynamics (CFD) analyses.The project aimed to develop a sturdy and adjustable structure capable of investigatingground effect, despite various challenges and limitations. Improvements were suggested inareas such as floor length, setup dimensions, and structural rigidity. The study lays a foun-dation for future experimental research on ground effect, providing insights and a frameworkfor ongoing investigations in aeronautics and related fields.</p>

corrected abstract:
<p>This thesis details the creation and development of an experimental setup to test ground effect in the L2000 wind tunnel at KTH. Ground effect is an important aerodynamic phenomenon observed in areas such as aviation and motorsports. The research includes a comprehensive literature study and design process, encompassing analytical calculations, finite element method (FEM) simulations, and computational fluid dynamics (CFD) analyses.</p><p>The project aimed to develop a sturdy and adjustable structure capable of investigating ground effect, despite various challenges and limitations. Improvements were suggested in areas such as floor length, setup dimensions, and structural rigidity. The study lays a foundation for future experimental research on ground effect, providing insights and a framework for ongoing investigations in aeronautics and related fields.</p>
----------------------------------------------------------------------
In diva2:1878766 
abstract is: 
<p>When making an investment, it is desirable to maximize the profits while minimizingthe risk. The theory of portfolio optimization is the mathematical approach to choosingwhat assets to invest in, and distributing the capital accordingly. Usually, the objectiveof the optimization is to maximize the return or minimize the risk. This report aims toconstruct and analyze a robust optimization model with MILP in order to determine ifthat model is more suitable for portfolio optimization than earlier models. This is doneby creating a robust MILP model, altering its parameters, and comparing the resultingportfolios with portfolios from older models. Our conclusion is that the constructed modelis appropriate to use for portfolio optimization. In particular, a robust approach is wellsuited for portfolio optimization, and the added MILP-part allows users of the model tospecialize the portfolio to their own preferences.</p>

corrected abstract:
<p>When making an investment, it is desirable to maximize the profits while minimizing the risk. The theory of portfolio optimization is the mathematical approach to choosing what assets to invest in, and distributing the capital accordingly. Usually, the objective of the optimization is to maximize the return or minimize the risk. This report aims to construct and analyze a robust optimization model with MILP in order to determine if that model is more suitable for portfolio optimization than earlier models. This is done by creating a robust MILP model, altering its parameters, and comparing the resulting portfolios with portfolios from older models. Our conclusion is that the constructed model is appropriate to use for portfolio optimization. In particular, a robust approach is well suited for portfolio optimization, and the added MILP-part allows users of the model to specialize the portfolio to their own preferences.</p>
----------------------------------------------------------------------
In diva2:1877612 
abstract is: 
<p>Quantum Key Distribution (QKD) is a promising method for information exchange that relies on quantum mechanical principles to increase the security of encodedinformation in comparison with traditional cryptography. The objective of this degree project was to plan, build and validate a receiver system for Free-Space QKD. To testit, a simple state-preparation setup was built, using a 780 nm laser for the QKD channel and a 680 nm laser for tracking, and both were sent over a single free-spacequantum channel. On the receiver side, the beam is received by a telescope that focuses it into the eye-opening where the receiver is positioned. Here, the tracking laser isdeflected to a camera (intended for a tracking algorithm, to be implemented in the future) and the QKD laser beam is split, and its polarization measured in |H⟩/|V⟩ and|+⟩/|−⟩ basis. Finally, the system is validated by testing that the beam propagates over the free-space channel, that the polarization correction is properly applied and,finally, that each polarization is routed into the right detector. By replacing the test sender with another with a capacity for sending states random number generation, thedeveloped receiver should be able to take part in quantum communication via protocols such as BB84, B92 or Decoy-State.</p>

corrected abstract:
<p>Quantum Key Distribution (QKD) is a promising method for information exchange that relies on quantum mechanical principles to increase the security of encoded information in comparison with traditional cryptography. The objective of this degree project was to plan, build and validate a receiver system for Free-Space QKD. To test it, a simple state-preparation setup was built, using a 780 nm laser for the (QKD) channel and a 680 nm laser for tracking, and both were sent over a single free-space quantum channel. On the receiver side, the beam is received by a telescope that focuses it into the eye-opening where the receiver is positioned. Here, the tracking laser is deflected to a camera (intended for a tracking algorithm, to be implemented in the future) and the QKD laser beam is split, and its polarization measured in ❘𝐻〉/❘𝑉〉 and ❘+/❘−〉 basis. Finally, the system is validated by testing that the beam propagates over the free-space channel, that the polarization correction is properly applied and, finally, that each polarization is routed into the right detector. By replacing the test sender with another with a capacity for sending states random number generation, the developed receiver should be able to take part in quantum communication via protocols such as BB84, B92 or Decoy-State.</p>
----------------------------------------------------------------------
In diva2:1834483 
abstract is: 
<p>In the field of heavy-duty vehicles, fuel efficiency and environmental protection are factors that need to be focused on, while the aerodynamic drag generated during vehicle travelling is one of the most influential aspects. This thesis delves into the aerodynamic simulation of Scania trucks using the open-source Computational Fluid Dynamics (CFD) tool, OpenFOAM v2206. This study rigorously investigates the aerodynamics of two Scania truck models under different operating conditions, including scenarios with different crosswind environments at high speeds.The core of this study is to compare and analyse the computational results of OpenFOAM v2206 and its predecessor OpenFOAM v3.0+ in a number of aspects, in order to elucidate the evolution and improvement of CFD techniques and their practical impact on vehicle simulation performance. In order to save computational resources, the RANS method was used for the steady-state simulations. Preliminary comparisons were also made with results from PowerFLOW, another CFD software widely used within the Scania group.Another important part of this thesis is the exploration of an alternative meshing method (ANSA Hextreme Mesh) in CFD simulations. As a widely used pre-processing software in the Scania group today, analysing and comparing the advantages and disadvantages of ANSA and OpenFOAM in terms of meshing, such as the ease of meshing and the accuracy of aerodynamic predictions, can help to provide valuable guidance for the application of truck shape design and aerodynamic simulation.The results indicate that OpenFOAM v2206 excels in predicting aerodynamics and has utility in optimising truck design. Compared to OpenFOAM v3.0+, OpenFOAM v2206 shows smaller discrepancies in results with PowerFLOW. Further exploration is required regarding transient simulations using OpenFOAM. In terms of meshing methods, a simplified model (Allan Body) was investigated, and there is further research to be done on meshing the complete truck.In conclusion, this thesis presents a comprehensive and in-depth exploration of truck aerodynamics using advanced CFD tools. The results not only deepen the understanding of airflow dynamics around heavy vehicles, but also pave the way for the development of more aerodynamically efficient and environmentally friendly truck designs.</p><p> </p>

corrected abstract:
<p>In the field of heavy-duty vehicles, fuel efficiency and environmental protection are factors that need to be focused on, while the aerodynamic drag generated during vehicle travelling is one of the most influential aspects. This thesis delves into the aerodynamic simulation of Scania trucks using the open-source Computational Fluid Dynamics (CFD) tool, OpenFOAM v2206. This study rigorously investigates the aerodynamics of two Scania truck models under different operating conditions, including scenarios with different crosswind environments at high speeds.</p><p>The core of this study is to compare and analyse the computational results of OpenFOAM v2206 and its predecessor OpenFOAM v3.0+ in a number of aspects, in order to elucidate the evolution and improvement of CFD techniques and their practical impact on vehicle simulation performance. In order to save computational resources, the RANS method was used for the steady-state simulations. Preliminary comparisons were also made with results from PowerFLOW, another CFD software widely used within the Scania group.</p><p>Another important part of this thesis is the exploration of an alternative meshing method (ANSA Hextreme Mesh) in CFD simulations. As a widely used pre-processing software in the Scania group today, analysing and comparing the advantages and disadvantages of ANSA and OpenFOAM in terms of meshing, such as the ease of meshing and the accuracy of aerodynamic predictions, can help to provide valuable guidance for the application of truck shape design and aerodynamic simulation.</p><p>The results indicate that OpenFOAM v2206 excels in predicting aerodynamics and has utility in optimising truck design. Compared to OpenFOAM v3.0+, OpenFOAM v2206 shows smaller discrepancies in results with PowerFLOW. Further exploration is required regarding transient simulations using OpenFOAM. In terms of meshing methods, a simplified model (Allan Body) was investigated, and there is further research to be done on meshing the complete truck.</p><p>In conclusion, this thesis presents a comprehensive and in-depth exploration of truck aerodynamics using advanced CFD tools. The results not only deepen the understanding of airflow dynamics around heavy vehicles, but also pave the way for the development of more aerodynamically efficient and environmentally friendly truck designs.</p>
----------------------------------------------------------------------
In diva2:1817072 
abstract is: 
<p>The construction of high-performance vessels like the F50 catamaran has traditionally prioritized advanced composite materials and performance-driven design. However, there is a growing need to incorporate sustainable materials and practices, with their performance in marine applications remaining relatively unknown. This study aims to address this gap by investigating the feasibility of using flax laminates as an environmentally friendly alternative for frequently damaged components, specifically the stern extension.Mechanical testing of flax laminates revealed lower stiffness per fiber areal weight compared to literature values and supplier data sheets, primarily attributed to moisture uptake in the flax material. These findings highlight the significance of considering real-world environmental conditions and specific application requirements when evaluating the mechanical properties of flax composites. Despite the mechanical challenges, environmental analysis demonstrated that the flax alternative for the stern extension offers promising benefits. It exhibits a carbon-positive characteristic, resulting in lower energy consumption during production, and comparable waste production to the original carbon fiber extension. However, it is important to note that these advantages are based on idealized theoretical data, and further optimization is required to address variations in resin usage and the strength of the cured composite.To address the weight discrepancies among the fleet, currently rectified by corrector weights, a practical solution is proposed utilizing flax composite layups. Selective implementation of the flax stern extension on the lightest one-third of the fleet can effectively balance weight distribution without compromising overall yacht performance. This strategy allows SailGP to incorporate sustainable materials while maintaining uniformity and performance across all participating yachts. By considering the environmental impact and structural considerations, this study provides valuable insights for the development of sustainable marine composites and encourages further research in optimizing the performance and reliability of flax-based laminates in marine applications.</p>

corrected abstract:
<p>The construction of high-performance vessels like the F50 catamaran has traditionally prioritized advanced composite materials and performance-driven design. However, there is a growing need to incorporate sustainable materials and practices, with their performance in marine applications remaining relatively unknown. This study aims to address this gap by investigating the feasibility of using flax laminates as an environmentally friendly alternative for frequently damaged components, specifically the stern extension.</p><p>Mechanical testing of flax laminates revealed lower stiffness per fiber areal weight compared to literature values and supplier data sheets, primarily attributed to moisture uptake in the flax material. These findings highlight the significance of considering real-world environmental conditions and specific application requirements when evaluating the mechanical properties of flax composites. Despite the mechanical challenges, environmental analysis demonstrated that the flax alternative for the stern extension offers promising benefits. It exhibits a carbon-positive characteristic, resulting in lower energy consumption during production, and comparable waste production to the original carbon fiber extension. However, it is important to note that these advantages are based on idealized theoretical data, and further optimization is required to address variations in resin usage and the strength of the cured composite.</p><p>To address the weight discrepancies among the fleet, currently rectified by corrector weights, a practical solution is proposed utilizing flax composite layups. Selective implementation of the flax stern extension on the lightest one-third of the fleet can effectively balance weight distribution without compromising overall yacht performance. This strategy allows SailGP to incorporate sustainable materials while maintaining uniformity and performance across all participating yachts. By considering the environmental impact and structural considerations, this study provides valuable insights for the development of sustainable marine composites and encourages further research in optimizing the performance and reliability of flax-based laminates in marine applications.</p>
----------------------------------------------------------------------
In diva2:1816884 
abstract is: 
<p>Artificial satellites play a vital role throughout the world today. They providea broad range of services ranging from navigation to communication and reconnaissance. As antenna technology is evolving and ground based antennas are getting smaller and smaller, the demand for on-the-move solutions is growing.These antennas can be used whilst mounted on for example, a moving vehicle,where the mechanical performance of the antenna must be sufficient for thecurrent conditions. During this project, a computer based tool that can helpengineers when iterating and optimizing a two-axis gimbal type antenna designwas created. The tool uses simulated and recorded data from road vehicles andboats to calculate the required torque on the two axes necessary to sustain communication with a geostationary satellite. When completed, the tool was easy to use and configure whilst not requiring much computational power.</p>

corrected abstract:
<p>Artificial satellites play a vital role throughout the world today. They provide a broad range of services ranging from navigation to communication and reconnaissance. As antenna technology is evolving and ground based antennas are getting smaller and smaller, the demand for on-the-move solutions is growing. These antennas can be used whilst mounted on for example, a moving vehicle, where the mechanical performance of the antenna must be sufficient for the current conditions. During this project, a computer based tool that can help engineers when iterating and optimizing a two-axis gimbal type antenna design was created. The tool uses simulated and recorded data from road vehicles and boats to calculate the required torque on the two axes necessary to sustain communication with a geostationary satellite. When completed, the tool was easy to use and configure whilst not requiring much computational power.</p>
----------------------------------------------------------------------
In diva2:1752035 
abstract is: 
<p>This thesis investigates out-of-distribution recognition for time-series data of pulsedradar signals. The classifier is a naive Bayesian classifier based on Gaussian mixturemodels and Dirichlet process mixture models. In the mixture models, we model thedistribution of three pulse features in the time series, namely radio-frequency in thepulse, duration of the pulse, and pulse repetition interval which is the time betweenpulses. We found that simple thresholds on the likelihood can effectively determine ifsamples are out-of-distribution or belong to one of the classes trained on. In addition,we present a simple method that can be used for deinterleaving/pulse classification andshow that it can robustly classify 100 interleaved signals and simultaneously determineif pulses are out-of-distribution.</p>

corrected abstract:
<p>This thesis investigates out-of-distribution recognition for time-series data of pulsed radar signals. The classifier is a naive Bayesian classifier based on Gaussian mixture models and Dirichlet process mixture models. In the mixture models, we model the distribution of three pulse features in the time series, namely radio-frequency in the pulse, duration of the pulse, and pulse repetition interval which is the time between pulses. We found that simple thresholds on the likelihood can effectively determine if samples are out-of-distribution or belong to one of the classes trained on. In addition, we present a simple method that can be used for deinterleaving/pulse classification and show that it can robustly classify 100 interleaved signals and simultaneously determine if pulses are out-of-distribution.</p>
----------------------------------------------------------------------
In diva2:1750461 
abstract is: 
<p>The third data release in June 2022 from the Gaia mission (ESA) provides for the first time the non-single star solutions for hundreds of thousands sources. This brings a new exciting dataset to fully characterize new binary systems and contribute to enforce our global knowledge about stars.By combining the astrometry from the new Gaia non-single stars catalog with other datasets, the features of new star systems can be determined. From this combination, the mass and luminosity of each component can be determined with a new level of precision. </p><p>The estimation of star masses is a fundamental process to improve the understanding of their behaviour (luminosity, evolution, etc.). The stars that can be fully characterized are not that many but are crucial. They enable to calibrate fundamental physical relations, for instance the mass-luminosity relation. From this, the masses of single or less reachable stars can be estimated by only measuring their luminosity.</p><p>In this thesis, the orbital solutions from Gaia astrometry are combined with spectroscopy from the literature, using the data compilation SB9 and the data from the APOGEE experiment. This combination enables to calculate the masses and luminosity in the G band of new star systems : 43 for SB9 and 13 for APOGEE.This mass catalog will be very useful in the future since its mass is one of the main parameters to characterize a star. These results could be used for future studies.</p><p>A new calibration of the mass-luminosity relation in the G spectral band is also performed, using the calculated masses. The fit of this relation would need more masses to be improved, but it remains interesting and can be used as a starting point in the future.Moreover, a computational tool has been developed to improve how the fit is achieved.</p>

corrected abstract:
<p>The third data release in June 2022 from the Gaia mission (ESA) provides for the first time the non-single star solutions for hundreds of thousands sources. This brings a new exciting dataset to fully characterize new binary systems and contribute to enforce our global knowledge about stars. By combining the astrometry from the new Gaia non-single stars catalog with other datasets, the features of new star systems can be determined. From this combination, the mass and luminosity of each component can be determined with a new level of precision. </p><p>The estimation of star masses is a fundamental process to improve the understanding of their behaviour (luminosity, evolution, etc.). The stars that can be fully characterized are not that many but are crucial. They enable to calibrate fundamental physical relations, for instance the mass-luminosity relation. From this, the masses of single or less reachable stars can be estimated by only measuring their luminosity.</p><p>In this thesis, the orbital solutions from Gaia astrometry are combined with spectroscopy from the literature, using the data compilation SB9 and the data from the APOGEE experiment. This combination enables to calculate the masses and luminosity in the G band of new star systems : 43 for SB9 and 13 for APOGEE. This mass catalog will be very useful in the future since its mass is one of the main parameters to characterize a star. These results could be used for future studies.</p><p>A new calibration of the mass-luminosity relation in the G spectral band is also performed, using the calculated masses. The fit of this relation would need more masses to be improved, but it remains interesting and can be used as a starting point in the future. Moreover, a computational tool has been developed to improve how the fit is achieved.</p>
----------------------------------------------------------------------
In diva2:1705317 
abstract is: 
<p>This thesis focuses on the research of producing high purity UN fuel with the methodof fluoride ammonolysis. A UF4 powder was heated up to 800 °C under NH3 gasto produce UN2. Then the UN2 was heated up to 1100 °C under Ar, producing thefinal product of UN. During the experiments a variety of equipment was designed foraddressing the main constraints, the oxidation and the uncompleted reactions. Fur-thermore, a closed type rotated powder drum was designed from scratch, that wasintended to crash and mix the powder during the synthesis. Additionally, througha set of interrupted syntheses, the reaction’s stages where investigated and the fab-rication procedure redesigned with an improved eﬀiciency. Lastly, the reaction ofUF6 with NH3 was tested and UN synthesis was achieved with UF6.</p>

corrected abstract:
<p>This thesis focuses on the research of producing high purity UN fuel with the method of fluoride ammonolysis. A UF<ub>4</sub> powder was heated up to 800 °C under NH<sub>3</sub> gas to produce UN<sub>2</sub>. Then the UN<sub>2</sub> was heated up to 1100 °C under Ar, producing the final product of UN. During the experiments a variety of equipment was designed for addressing the main constraints, the oxidation and the uncompleted reactions. Furthermore, a closed type rotated powder drum was designed from scratch, that was intended to crash and mix the powder during the synthesis. Additionally, through a set of interrupted syntheses, the reaction’s stages where investigated and the fabrication procedure redesigned with an improved efficiency. Lastly, the reaction of UF<sub>6</sub> with NH<sub>3</sub> was tested and UN synthesis was achieved with UF<sub>6</sub>.</p>
----------------------------------------------------------------------
In diva2:1698165 
abstract is: 
<p>By working as an intern with the Quotation Officer of Sabena Technics Nîmes facility,an aircraft maintenance company, this degree project highlights the different factors totake into account in a quotation of civil planes (B737/B767/A320/A330) by analysingwork packages from airline company, applying a process to estimate the importantdata as the man hours or tools, and using my technical knowledge to understandmaintenance tasks. Management knowledge for commercial sale reviews was usefulas well, this job is in the middle of the commercial team and and the technicalteam. Finally, a critical opinion of the process has been made in order to propose animprovement of the next quotations.</p>

corrected abstract:
<p>By working as an intern with the Quotation Officer of Sabena Technics Nîmes facility, an aircraft maintenance company, this degree project highlights the different factors to take into account in a quotation of civil planes (B737/B767/A320/A330) by analysing work packages from airline company, applying a process to estimate the important data as the man hours or tools, and using my technical knowledge to understand maintenance tasks. Management knowledge for commercial sale reviews was useful as well, this job is in the middle of the commercial team and and the technical team. Finally, a critical opinion of the process has been made in order to propose an improvement of the next quotations.</p>
----------------------------------------------------------------------
In diva2:1679064 
abstract is: 
<p>In today’s lighter vehicles, EPS has had smaller breakthroughs, but not in heavyduty due to its high initial cost and lack of research. Even though there ispotential in the future when trucks may go electric. In this report, the theoreticalrequirements to implement an EPS system will be determined and investigatedalong with an in depth analysis of the electrical motor and planetary gearbox used.A preferred EPS system will consist of a Permanent magnet electric motor and aplanetary gearbox with optimized characteristics regarding voltage, cost, shape,dimensions and layout. The results lead to a theoretical EPS system that can befitted into a heavy duty truck’s engine compartment. Problems with implementingEPS, such as cost, electrical issues and limited space as well as possibilities forfuture work are discussed.</p>

corrected abstract:
<p>In today’s lighter vehicles, electric power steering, EPS, has had smaller breakthroughs, but not in heavy duty due to its high initial cost and lack of research. Even though there is potential in the future when trucks may go electric. In this report, the theoretical requirements to implement an EPS system will be determined and investigated along with an in depth analysis of the electrical motor and planetary gearbox used. A preferred EPS system will consist of a permanent magnet electric motor and a planetary gearbox with optimized characteristics regarding voltage, cost, shape, dimensions and layout. The results lead to a theoretical EPS system that can be fitted into a heavy duty truck’s engine compartment. Problems with implementing EPS, such as cost, electrical issues and limited space as well as possibilities for future work are discussed.</p>
----------------------------------------------------------------------
In diva2:1673882 
abstract is: 
<p>The focus of this thesis is to analyse the flight characteristics of the blended wingbody (BWB) unmanned aerial vehicle (UAV) Green Raven currently being developed by students at the Royal Institute of Technology (KTH) in Stockholm,Sweden. The purpose of evaluating a BWB aircraft is due to its potential increasein fuel efficiency and payload compared to conventional aircrafts which would enable more sustainable flights. The analysis is conducted in ANSYS Fluent 2020R2 where the goals are to extrapolate lift, drag and pitching moment coefficients,aerodynamic efficiency and evaluate stall patterns.</p><p>The analysis is conducted with free stream velocities from 5 m/s to 40 m/s with5 m/s increments at angles of attack from −4◦ to stall plus 4◦. The result of thisthesis is that an analysis have not been able to be conducted due to a lack ofcomputational power. Thusly, the conclusion to this thesis is that to be able toperform a complete analysis of the Green Raven, a more powerful computer needsto be used.</p>

corrected abstract:
<p>The focus of this thesis is to analyse the flight characteristics of the blended wing body (BWB) unmanned aerial vehicle (UAV) Green Raven currently being developed by students at the Royal Institute of Technology (KTH) in Stockholm, Sweden. The purpose of evaluating a BWB aircraft is due to its potential increase in fuel efficiency and payload compared to conventional aircrafts which would enable more sustainable flights. The analysis is conducted in ANSYS Fluent 2020 R2 where the goals are to extrapolate lift, drag and pitching moment coefficients, aerodynamic efficiency and evaluate stall patterns.</p><p>The analysis is conducted with free stream velocities from 5 𝑚/𝑠 to 40 𝑚/𝑠 with 5 𝑚/𝑠 increments at angles of attack from −4º to stall plus 4º. The result of this thesis is that an analysis have not been able to be conducted due to a lack of computational power. Thusly, the conclusion to this thesis is that to be able to perform a complete analysis of the Green Raven, a more powerful computer needs to be used.</p>
----------------------------------------------------------------------
In diva2:1334745 
abstract is: 
<p>One common metric used to determine the economic conditions of a country isthe gross domestic product (GDP) per capita. However, GDP is a somewhatnarrow metric, and other factors should be taken into account in economic anal-ysis. One way to further augment economic analysis is to take the geographicalpositioning of a country into account. Several studies in the past have con-cluded that distance to the equator might determine the GDP of a country, fordifferent underlying reasons. Thus, the aim of this thesis was to investigate thepotential relationship between the GDP per capita of a country and its latitudein relation to other explanatory variables. By using multiple linear regression,with latitude as one of the regressors, a predictive model for GDP per capitawas made. From this model a clear relationship between latitude and GDP percapita was found. Several theories are put forth to explain this yet no conclu-sive answer was found. Considering the fact that a linear relationship exists,policy decisions may benefit from further research into areas not immediatelyassociated with the usual aspects of economic analysis.</p>

corrected abstract:
<p>One common metric used to determine the economic conditions of a country is the gross domestic product (GDP) per capita. However, GDP is a somewhat narrow metric, and other factors should be taken into account in economic analysis. One way to further augment economic analysis is to take the geographical positioning of a country into account. Several studies in the past have concluded that distance to the equator might determine the GDP of a country, for different underlying reasons. Thus, the aim of this thesis was to investigate the potential relationship between the GDP per capita of a country and its latitude in relation to other explanatory variables. By using multiple linear regression, with latitude as one of the regressors, a predictive model for GDP per capita was made. From this model a clear relationship between latitude and GDP per capita was found. Several theories are put forth to explain this yet no conclusive answer was found. Considering the fact that a linear relationship exists, policy decisions may benefit from further research into areas not immediately associated with the usual aspects of economic analysis.</p>
----------------------------------------------------------------------
In diva2:1285512 
abstract is: 
<p>The engineering competition Formula Student has introduced a Driverless Vehicle (DV)class, which requires the students to develop a car that can autonomously make its wayaround a cone track. To ensure the safety of such a vehicle, an Emergency Brake System(EBS) is required. The EBS shall ensure transition to safe state for detection of a singlefailure mode. This thesis work covers the design of the EBS for KTH Formula Student(KTH FS).Due to the safety critical character of this system, the software part of the EBS, calledEBS Supervisor, has been analyzed in accordance with the safety standard ISO 26262 tosee if an improved safety could be achieved. The analysis has been perform according toPart 3: Concept phase of ISO 26262 with an item definition, Hazard Analysis and RiskAssessment (HARA), Functional Safety Concept (FSC) and Technical Safety Concept(TSC).The result of the analysis showed that the EBS Supervisor requires extensive redundanciesin order to follow ISO 26262. This includes an additional CPU as well as signal checksof inputs and outputs. Due to limited resources in terms of money and time within theKTH FS team, these redundancies will not be implemented. The process of working withthe safety standard did however inspire an increased safety mindset.</p>

corrected abstract:
<p>The engineering competition Formula Student has introduced a Driverless Vehicle (DV) class, which requires the students to develop a car that can autonomously make its way around a cone track. To ensure the safety of such a vehicle, an Emergency Brake System (EBS) is required. The EBS shall ensure transition to safe state for detection of a single failure mode. This thesis work covers the design of the EBS for KTH Formula Student (KTH FS).</p><p>Due to the safety critical character of this system, the software part of the EBS, called EBS Supervisor, has been analyzed in accordance with the safety standard ISO 26262 to see if an improved safety could be achieved. The analysis has been perform according to <em>Part 3: Concept phase of ISO 26262</em> with an item definition, Hazard Analysis and Risk Assessment (HARA), Functional Safety Concept (FSC) and Technical Safety Concept (TSC).</p><p>The result of the analysis showed that the EBS Supervisor requires extensive redundancies in order to follow ISO 26262. This includes an additional CPU as well as signal checks of inputs and outputs. Due to limited resources in terms of money and time within the KTH FS team, these redundancies will not be implemented. The process of working with the safety standard did however inspire an increased safety mindset.</p>
----------------------------------------------------------------------
In diva2:1237800 
abstract is: 
<p>The purpose of this thesis project is to develop a methodology that can be used to minimize the number of spot-welds in a mechanical structure, this is done in a reliable manner via optimization methods. The optimization considers fatigue life in spot-welds and also stiffness and eigenfrequency values. The ﬁrst chapter of this thesis presents a spot-weld fatigue model proposed by Rupp (1995), common FEmodels of spot-welds and also important aspects about structural optimization in general. The second chapter further describes how topology optimization and size (parameter) optimization are applied on a simple multi-weld model with respect to the aforementioned structural constraints. The topology optimization is later used on a full-size car model, while the size optimization is used to optimize the multiweld model by adding an non-linear structural constraint - a crash indentation constraint. The spot-weld fatigue model proposed by Rupp (1995), is also veriﬁed by comparing FE results using different FE-models of spot-welds compared to fatigue data by Long and Khanna (2007). Both optimization methods successfully minimize the total amount of spot-welds on the multi-weld model. The topology optimization,accompanied with thegradient based MFD algorithm,minimizes th etotal spot-welds with around 15% and 3% on the multi-weld model and car body respectively. The size optimization, using design of experiments and response surfaces, manage storeduce the number of welds in the multi-weldmodel by 25%. However, with the size optimization the computational time is several orders of magnitude longer-even without the formulation of the crash constraint. The fatiguespot-weld model fares reasonably well compared to the experimental fatigue data, regardless of the FE model of the spot-weld. It is concluded that the ACM model would be recommended based on its compatibility with fatigue and optimization methods, mesh-independence and also other studies have shown its ability to represent stiffness and eigenfrequency correctly.</p>

corrected abstract:
<p>The purpose of this thesis project is to develop a methodology that can be used to minimize the number of spot-welds in a mechanical structure, this is done in a reliable manner via optimization methods. The optimization considers fatigue life in spot-welds and also stiffness and eigenfrequency values. The first chapter of this thesis presents a spot-weld fatigue model proposed by Rupp (1995), common FE models of spot-welds and also important aspects about structural optimization in general. The second chapter further describes how topology optimization and size (parameter) optimization are applied on a simple multi-weld model with respect to the aforementioned structural constraints. The topology optimization is later used on a full-size car model, while the size optimization is used to optimize the multiweld model by adding an non-linear structural constraint - a crash indentation constraint. The spot-weld fatigue model proposed by Rupp (1995), is also verified by comparing FE results using different FE-models of spot-welds compared to fatigue data by Long and Khanna (2007). Both optimization methods successfully minimize the total amount of spot-welds on the multi-weld model. The topology optimization, accompanied with the gradient based MFD algorithm, minimizes the total spot-welds with around 15% and 3% on the multi-weld model and car body respectively. The size optimization, using design of experiments and response surfaces, manages to reduce the number of welds in the multi-weld model by 25%. However, with the size optimization the computational time is several orders of magnitude longer-even without the formulation of the crash constraint. The fatigue spot-weld model fares reasonably well compared to the experimental fatigue data, regardless of the FE model of the spot-weld. It is concluded that the ACM model would be recommended based on its compatibility with fatigue and optimization methods, mesh-independence and also other studies have shown its ability to represent stiffness and eigenfrequency correctly.</p>
----------------------------------------------------------------------
In diva2:1229776 
abstract is: 
<p>In this project we have studied the mathematics and physics of music. We have also usednumerical methods and programming in MATLAB to analyse sounds and simpler sequencesof music. The project has therefore resulted in both this report as well as the MATLAB codewe have written to perform our analyses.The report begins by giving a theoretical background to the mathematics and the theory thatthe project is based on. This includes a review of the Fourier Transform, the Discrete FourierTransform (DFT), the Fast Fourier Transform (FFT) and some musical theory. Furthermore itdescribes the process where we from simple to more advanced analyses and models havesucceeded in analysing melodies played on piano. The report is then concluded with adiscussion of possible ways to develop our models further to enable analyses of morecomplex pieces of music</p>

corrected abstract:
<p>In this project we have studied the mathematics and physics of music. We have also used numerical methods and programming in MATLAB to analyse sounds and simpler sequences of music. The project has therefore resulted in both this report as well as the MATLAB code we have written to perform our analyses. The report begins by giving a theoretical background to the mathematics and the theory that the project is based on. This includes a review of the Fourier Transform, the Discrete FourierTransform (DFT), the Fast Fourier Transform (FFT) and some musical theory. Furthermore it describes the process where we from simple to more advanced analyses and models have succeeded in analysing melodies played on piano. The report is then concluded with a discussion of possible ways to develop our models further to enable analyses of more complex pieces of music</p>

Note: There is no period at the end of the last sentence.
----------------------------------------------------------------------
In diva2:1174023 - missing space in title:
"Numerical Model of MeltingProblems"
==>
"Numerical Model of Melting Problems"

abstract is: 
<p>In the present study, a finite volume method is employed to modelthe advection-diffusion phenomenon during a pure substance meltingprocess. The exercise is limited to a benchmark problem consisting ofthe 2D melting from a vertical wall of a PCM driven by natural convectionin the melt. Numerical results, mainly the temporal evolutionof average Nusselt number at the hot wall and the average liquid fraction,are validated by available literature data and the effect of thermalinertia in the heat transfer is considered as well. Finally, motivatedby recent publications and the model presented here, possible new researchtopics are proposed.</p>

corrected abstract:
<p>In the present study, a finite volume method is employed to model the advection-diffusion phenomenon during a pure substance melting process. The exercise is limited to a benchmark problem consisting of the 2D melting from a vertical wall of a PCM driven by natural convection in the melt. Numerical results, mainly the temporal evolution of average Nusselt number at the hot wall and the average liquid fraction, are validated by available literature data and the effect of thermal inertia in the heat transfer is considered as well. Finally, motivated by recent publications and the model presented here, possible new research topics are proposed.</p>
----------------------------------------------------------------------
In diva2:1120599 
abstract is: 
<p>Inverse problems are a class of problems that often appear in science. They canbe thought of as calculating the cause that corresponds to measured data, andit is the opposite (i.e. inverse) of a direct problem, where data is calculated froma known source. In Computerized Tomography (CT), which is an importantnon-invasive imaging method in medicine, inverse problems arise when wewant to reconstruct the inner structure of the physical body from indirectmeasurements. In this thesis we investigate dierent methods for CT imagereconstruction. We compare the reconstruction quality of using Entropy asa regularizer to Tikhonov- and Total Variation (TV) regularization and theanalytic Filtered Backprojection approach, and also investigate the sensitivityof entropy regularization with respect to priors.</p>

corrected abstract:
<p>Inverse problems are a class of problems that often appear in science. They can be thought of as calculating the cause that corresponds to measured data, and it is the opposite (i.e. inverse) of a <em>direct problem</em>, where data is calculated from a known source. In <em>Computerized Tomography</em> (CT), which is an important non-invasive imaging method in medicine, inverse problems arise when we want to reconstruct the inner structure of the physical body from indirect measurements. In this thesis we investigate different methods for CT image reconstruction. We compare the reconstruction quality of using Entropy as a regularizer to <em>Tikhonov-</em> and <em>Total Variation</em> (TV) regularization and the analytic <em>Filtered Backprojection</em> approach, and also investigate the sensitivity of entropy regularization with respect to <em>priors</em>.</p>
----------------------------------------------------------------------
In diva2:1120480 
abstract is: 
<p>Gravitation is a manifestation of the space-time curvature and gravitational waves aredistortions traveling through the fabric of space-time. Scientists have tried to detectgravitational waves for decades, but the first direct detection of gravitational waves andthe first observation of a binary black hole merger was made in 2015. The purpose ofthis report is to introduce General Relativity and binary black holes and to calculate thetime to coalescence of two back holes with circular or elliptical orbits. We find that abinary black hole in an elliptical orbit with large eccentricity coalesces faster than in acircular orbit.</p>

corrected abstract:
<p>Gravitation is a manifestation of the space-time curvature and gravitational waves are distortions traveling through the fabric of space-time. Scientists have tried to detect gravitational waves for decades, but the first direct detection of gravitational waves and the first observation of a binary black hole merger was made in 2015. The purpose of this report is to introduce General Relativity and binary black holes and to calculate the time to coalescence of two back holes with circular or elliptical orbits. We find that a binary black hole in an elliptical orbit with large eccentricity coalesces faster than in a circular orbit.</p>
----------------------------------------------------------------------
In diva2:1120479 - duplicate of diva2:1120480 
abstract is: 
<p>Gravitation is a manifestation of the space-time curvature and gravitational waves aredistortions traveling through the fabric of space-time. Scientists have tried to detectgravitational waves for decades, but the first direct detection of gravitational waves andthe first observation of a binary black hole merger was made in 2015. The purpose ofthis report is to introduce General Relativity and binary black holes and to calculate thetime to coalescence of two back holes with circular or elliptical orbits. We find that abinary black hole in an elliptical orbit with large eccentricity coalesces faster than in acircular orbit.</p>

corrected abstract:
<p>Gravitation is a manifestation of the space-time curvature and gravitational waves are distortions traveling through the fabric of space-time. Scientists have tried to detect gravitational waves for decades, but the first direct detection of gravitational waves and the first observation of a binary black hole merger was made in 2015. The purpose of this report is to introduce General Relativity and binary black holes and to calculate the time to coalescence of two back holes with circular or elliptical orbits. We find that a binary black hole in an elliptical orbit with large eccentricity coalesces faster than in a circular orbit.</p>
----------------------------------------------------------------------
In diva2:1087206 
abstract is: 
<p>Aircraft engines are complex, value-added high-tech systems, which possess very restrictive technical specifications. This is especially the case for turbine parts which are submitted to very hostilethermo-mechanical environments. The technical properties, such as fatigue life and creep tolerance of these parts are therefore of primary concern.</p><p>The properties of the parts are also strongly linked to the manufacturing processes. Unfortunately, these processes possess inherent flaws that lead to the presence of unwanted abnormalities. Fordesign optimization and cost efficiency, great care and efforts are put by the aircraft engine manufacturers to consistently increase the knowledge on these irregularities and their impact on theparts functional characteristics.</p><p>These issues are still present in the actual industrial context of SAFRAN Aircraft Engines. After successfully completing the development and certification phases, the LEAP, the new enginegeneration that will progressively replace the CFM engine family, is now in its ramping-up production phase. To achieve what will be the greatest production rate increase of all aviation history, crucialcost reduction campaigns are conducted.</p><p>Among the many potential cost reduction levers, the scrapped parts rate is of primary importance as it represents a significant loss in production. Tolerances of the technical specifications, which hadpreliminarily been established with strong safety margins, are now looked into.</p><p>In this context, a joint project between the Development Department and the Material Department has been launched to understand more clearly how the metallurgical abnormalities impact the life ofcasting parts, and how this could help optimize the casting defect tolerances.</p><p> </p><p></p>

corrected abstract:
<p>Aircraft engines are complex, value-added high-tech systems, which possess very restrictive technical specifications. This is especially the case for turbine parts which are submitted to very hostile thermo-mechanical environments. The technical properties, such as fatigue life and creep tolerance of these parts are therefore of primary concern.</p><p>The properties of the parts are also strongly linked to the manufacturing processes. Unfortunately, these processes possess inherent flaws that lead to the presence of unwanted abnormalities. For design optimization and cost efficiency, great care and efforts are put by the aircraft engine manufacturers to consistently increase the knowledge on these irregularities and their impact on the parts functional characteristics.</p><p>These issues are still present in the actual industrial context of SAFRAN Aircraft Engines. After successfully completing the development and certification phases, the LEAP, the new engine generation that will progressively replace the CFM engine family, is now in its ramping-up production phase. To achieve what will be the greatest production rate increase of all aviation history, crucial cost reduction campaigns are conducted.</p><p>Among the many potential cost reduction levers, the scrapped parts rate is of primary importance as it represents a significant loss in production. Tolerances of the technical specifications, which had preliminarily been established with strong safety margins, are now looked into.</p><p>In this context, a joint project between the Development Department and the Material Department has been launched to understand more clearly how the metallurgical abnormalities impact the life of casting parts, and how this could help optimize the casting defect tolerances.</p>
----------------------------------------------------------------------
In diva2:1078075 missing ligature and spaces in title:
"The chaotic rotation of elongated particles in anoscillating Couette ow"
==>
"The chaotic rotation of elongated particles in an oscillating Couette flow"

abstract is: 
<p>Flows with suspended particles occur in a large range of engineering applicationsand biological systems. A fundamental understanding of the uidparticlecoupling can be obtained by analyzing single particles in simple ow situations. Nilsen and Anderson (2013) studied the dynamics of a prolatespheroid rotating in the ow-gradient plane of an oscillating creepingshear ow. They found that given a high enough particle inertia the rotationcould become chaotic. This thesis extends on this work by taking uid inertia into account and studying the rotation of an elliptical cylinder.Numerical simulations with the Lattice Boltzmann EBF method reveal alarge variety of rotational states. Further, chaotic solutions are identiedwith a new Lyapunov exponent based algorithm. These chaotic solutionsare found to be present for heavy particles (compared to the uid) and smalloscillation frequencies (compared to the shear rate in the channel).</p>

corrected abstract:
<p>Flows with suspended particles occur in a large range of engineering applications and biological systems. A fundamental understanding of the fluid-particle coupling can be obtained by analyzing single particles in simple flow situations. Nilsen and Anderson (2013) studied the dynamics of a prolate spheroid rotating in the flow-gradient plane of an oscillating creeping shear flow. They found that given a high enough particle inertia the rotation could become chaotic. This thesis extends on this work by taking fluid inertia into account and studying the rotation of an elliptical cylinder. Numerical simulations with the Lattice Boltzmann EBF method reveal a large variety of rotational states. Further, chaotic solutions are identified with a new Lyapunov exponent based algorithm. These chaotic solutions are found to be present for heavy particles (compared to the fluid) and small oscillation frequencies (compared to the shear rate in the channel).</p>
----------------------------------------------------------------------
In diva2:784003 
abstract is: 
<p>In the modern urban lifestyle, more and more people are exposed to noise pollution in form of traffic noise. As a response to this, the automotive OEMs (Original Equipment Manufacturer) are put under pressure to reduce the emitted noise from vehicles. To be able to meet the upcoming, stricter regulations, the automotive OEMs seeks new techniques to be able to front load the pass-by noise engineering in the vehicle development process and to identify and understand the different sources that contributes to the exterior noise.Earlier exterior sources ranking using ASQ (Airborn Source Quantification) with an energetic approach during pass-by noise test has yielded very good and reliable results for an ICE (Internal Combustion Engine) vehicle.In this Master Thesis, two exterior source ranking methods have been tested and evaluated for an electric vehicle during in-room pass-by noise test. The two methods were: ASQ and OPA (Operational Path Analysis). In total, five models were built from the two methods and each model was evaluated for, in total, three driving conditions corresponding to the current ISO362-1:2007 and the proposed, revised version.The results show that the ASQ models are not capable to correctly estimate the engine contribution due to its high tonality. Moreover, it was seen that the energetic ASQ model is very sensitive to small changes. Both ASQ models underestimated the tire noise.The OPA model on the other hand managed to estimate the total contribution very well. Both the engine contribution and the tire contributions are well estimated. Nevertheless, OPA as method has several weaknesses and building an OPA model is not a straightforward task. Its weaknesses and the process to reach a final OPA model are discussed in this thesis.It was seen that one of the most crucial steps in an OPA model is to have clean references to get meaningful results. A MIMO-FIR filter was therefore used to filter out engine harmonics from the tire references. Its principles and importance for the end results are also discussed.Included is also an overview of the basic principles in TPA (Transfer Path Analysis), ASQ, OPA and in room pass by noise test as well as a description of the test campaign.</p>

corrected abstract:
<p>In the modern urban lifestyle, more and more people are exposed to noise pollution in form of traffic noise. As a response to this, the automotive OEMs (Original Equipment Manufacturer) are put under pressure to reduce the emitted noise from vehicles. To be able to meet the upcoming, stricter regulations, the automotive OEMs seeks new techniques to be able to front load the pass-by noise engineering in the vehicle development process and to identify and understand the different sources that contributes to the exterior noise.</p><p>Earlier exterior sources ranking using ASQ (Airborn Source Quantification) with an energetic approach during pass-by noise test has yielded very good and reliable results for an ICE (Internal Combustion Engine) vehicle.</p><p>In this Master Thesis, two exterior source ranking methods have been tested and evaluated for an electric vehicle during in-room pass-by noise test. The two methods were: ASQ and OPA (Operational Path Analysis). In total, five models were built from the two methods and each model was evaluated for, in total, three driving conditions corresponding to the current ISO362-1:2007 and the proposed, revised version.</p><p>The results show that the ASQ models are not capable to correctly estimate the engine contribution due to its high tonality. Moreover, it was seen that the energetic ASQ model is very sensitive to small changes. Both ASQ models underestimated the tire noise.</p><p>The OPA model on the other hand managed to estimate the total contribution very well. Both the engine contribution and the tire contributions are well estimated. Nevertheless, OPA as method has several weaknesses and building an OPA model is not a straightforward task. Its weaknesses and the process to reach a final OPA model are discussed in this thesis.</p><p>It was seen that one of the most crucial steps in an OPA model is to have clean references to get meaningful results. A MIMO-FIR filter was therefore used to filter out engine harmonics from the tire references. Its principles and importance for the end results are also discussed.</p><p>Included is also an overview of the basic principles in TPA (Transfer Path Analysis), ASQ, OPA and in room pass by noise test as well as a description of the test campaign.</p>
----------------------------------------------------------------------
In diva2:783992 
abstract is: 
<p>A kayak is a very ancient way of riding in the water ; human-powered, it is propelled by means of a double bladepaddle. It is used nowadays to fish, to travel, or to compete at the Olympics. It is the 1,000 meters flat-waterrace that led this project, and particularly its improvement for Steven Ferguson, the actual New-Zealand kayakchampion. This report presents the different parts that have been investigated to improve the paddle efficiencyin the water, and therefore the speed of a kayak athlete on the water.Kayaks are 4,000 years old, and kept since the basic shapes of their hull and paddle. It is only in 1987 that aNorwegian team developed an improved paddle, which is still used nowadays. Every paddle currently availableon the market follow this design. Based on fashions and on the success of athletes, they were unable to suggestany major improvement since. This project suggests a way of significantly improve the performance of an athletein a flat-water kayaking competition.</p>

corrected abstract:
<p>A kayak is a very ancient way of riding in the water ; human-powered, it is propelled by means of a double blade paddle. It is used nowadays to fish, to travel, or to compete at the Olympics. It is the 1,000 meters flat-water race that led this project, and particularly its improvement for Steven Ferguson, the actual New-Zealand kayak champion. This report presents the different parts that have been investigated to improve the paddle efficiency in the water, and therefore the speed of a kayak athlete on the water.</p><p>Kayaks are 4,000 years old, and kept since the basic shapes of their hull and paddle. It is only in 1987 that a Norwegian team developed an improved paddle, which is still used nowadays. Every paddle currently available on the market follow this design. Based on fashions and on the success of athletes, they were unable to suggest any major improvement since. This project suggests a way of significantly improve the performance of an athlete in a flat-water kayaking competition.</p>

Note the "Introduction" - which appears in the place of an abstract, goes on with details of the steps and includes diagram of the design loop.
----------------------------------------------------------------------
In diva2:668149 
abstract is: 
<p>This thesis is theresult of a project aimed at developing a tool for allocation of risk capitalin catastrophe excess-of-loss reinsurance. Allocation of risk capital is animportant tool for measuring portfolio performance and optimizing the capitalrequirement. Here, two allocation rules are described and analyzed, Eulerallocation and Capital layer allocation. The rules are applied to two differentportfolios. The main conclusions is that the two methods can be used togetherto get a better picture of how the dependence structure between the contractsaffect the portfolio result. It is also illustrated how the RORAC of one of theportfolios can be increased by 1 % using the outcome from the analyses.</p>

corrected abstract:
<p>This thesis is the result of a project aimed at developing a tool for allocation of risk capital in catastrophe excess-of-loss reinsurance. Allocation of risk capital is an important tool for measuring portfolio performance and optimizing the capital requirement. Here, two allocation rules are described and analyzed, Euler allocation and Capital layer allocation. The rules are applied to two different portfolios. The main conclusions is that the two methods can be used together to get a better picture of how the dependence structure between the contracts affect the portfolio result. It is also illustrated how the RORAC of one of the portfolios can be increased by 1 % using the outcome from the analyses.</p>
----------------------------------------------------------------------
In diva2:620604 
abstract is: 
<p>CubeSat or Cubic Satellite is an effective method to study the space aroundthe Earth thanks to its low cost, easy maintenance and short lead time. However, a great challenge of small satellites lies in achieving technicaland scientific requirements during the design stage. In the present workprimary focus is given to dynamic characterization of the deployable tapespringboom in order to verify and study the boom deployment dynamiceffects on the satellite. The deployed boom dynamic characteristics werestudied through simulations and experimental testing. The gravity offloadingsystem was used to simulate weightlessness environment in theexperimental testing and simulations showed that the deployment of thesystem influence the results in a different way depending on the vibrationmode shape.</p>

corrected abstract:
<p>CubeSat or Cubic Satellite is an effective method to study the space around the Earth thanks to its low cost, easy maintenance and short lead time. However, a great challenge of small satellites lies in achieving technical and scientific requirements during the design stage. In the present work primary focus is given to dynamic characterization of the deployable tape-spring boom in order to verify and study the boom deployment dynamic effects on the satellite. The deployed boom dynamic characteristics were studied through simulations and experimental testing. The gravity off-loading system was used to simulate weightlessness environment in the experimental testing and simulations showed that the deployment of theo system influence the results in a different way depending on the vibration mode shape.</p>
----------------------------------------------------------------------
In diva2:618581 
abstract is: 
<p>This paper presents a new revolutionary design in commercial aircraft: the conven-tional vertical and horizontal tails are not present as generally known, and their contri-bution to the manoeuvering of the aircraft, namely the presence of the rudder and theelevators, is achieved by locating them at the wingtips and in the canard, respectively.Substituting the horizontal tail with the canard, the possibility of dividing the fuel be-tween the wing (where it is located conventionally) and the canard allows the pilot tochange the center of gravity during the ight with more freedom, while the eect of theelevators continues present. Locating the vertical stabilizers at the wingtips combinesthe eect of the vertical stabilizer and the winglet all in one, with the corresponding lostof weight. In this sense, the aerodynamic, stability and aeroelastic characteristics of anaircraft such as the one described have been analyzed using dierent modules belongingto CEASIOM program, and the results are very encouraging, showing that it is reallyfeasible to change the current concept of the commercial aircraft without penalizing theperformance.</p>

corrected abstract:
<p>This paper presents a new revolutionary design in commercial aircraft: the conventional vertical and horizontal tails are not present as generally known, and their contribution to the manoeuvering of the aircraft, namely the presence of the rudder and the elevators, is achieved by locating them at the wingtips and in the canard, respectively. Substituting the horizontal tail with the canard, the possibility of dividing the fuel between the wing (where it is located conventionally) and the canard allows the pilot to change the center of gravity during the flight with more freedom, while the effect of the elevators continues present. Locating the vertical stabilizers at the wingtips combines the effect of the vertical stabilizer and the winglet all in one, with the corresponding lost of weight. In this sense, the aerodynamic, stability and aeroelastic characteristics of an aircraft such as the one described have been analyzed using different modules belonging to CEASIOM program, and the results are very encouraging, showing that it is really feasible to change the current concept of the commercial aircraft without penalizing the performance.</p>
----------------------------------------------------------------------
In diva2:557257 
abstract is: 
<p>For a field</p><p>kand a grading of the polynomial ringk[t]with Hilbert functionh, weconsider the Quot functor</p><p>QuothV</p><p>, where V =</p><p>?</p><p>di</p><p>=1k[t]is a finitely generated and</p><p>free</p><p>k[t]-module. The Quot functor parametrizes, for any k-algebra B, homogeneous</p><p>B</p><p>[t]-submodulesN⊆B⊗kVsuch that the graded components of the quotient(</p><p>B⊗kV)/Nare locally freeB-modules of rank given byh. We find that it is locallyrepresentable by a polynomial ring over</p><p>kin a finite number of variables. Finally, weshow that there is a scheme that represents the Quot functor that is both smooth</p><p>and irreducible.</p>

corrected abstract:
<p>For a field 𝑘 and a grading of the polynomial ring 𝑘[𝑡] with Hilbert function ℎ, we consider the Quot functor Quot<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>ℎ</sup><sub>𝑉</sub></span></span>, where 𝑉 = <span style="font-size: 1.4rem;">&oplus;</span><span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>𝑑</sup><sub>𝑖=1</sub></span></span>𝑘[𝑡] is a finitely generated and free 𝑘[𝑡]-module. The Quot functor parametrizes, for any 𝑘-algebra 𝐵,  homogeneous 𝐵[𝑡]-submodules 𝑁 ⊆ 𝐵 ⊗<sub>𝑘</sub> 𝑉 such that the graded components of the quotient (𝐵 ⊗<sub>𝑘</sub> 𝑉)/𝑁 are locally free 𝐵-modules of rank given by ℎ. We find that it is locally representable by a polynomial ring over 𝑘 in a finite number of variables. Finally, we show that there is a scheme that represents the Quot functor that is both smooth and irreducible.</p>
----------------------------------------------------------------------
In diva2:516089 
abstract is: 
<p>This master thesis is the documented work of the development, implementation and documentationof a new method for identification of pharmaceuticals excipients using X-ray fluorescence. Themethod is supposed to be used at the laboratory for quality control at AstraZeneca, Södertälje. Thepurpose of the thesis was to develop a stout and effective method and to educate the users in theskills needed to perform fast and correct analyses. This thesis presents the technology behind XRFinstruments and which sources of errors that could affect the results as well as the pedagogicaltheories used to describe the practical work that takes place at the laboratory. The method and theexperiment that preceded the method are presented, and the results and decisions are discussed.How the education was planned, performed and evaluated is also presented.</p>

corrected abstract:
<p>This master thesis is the documented work of the development, implementation and documentation of a new method for identification of pharmaceuticals excipients using X-ray fluorescence. The method is supposed to be used at the laboratory for quality control at AstraZeneca, Södertälje. The purpose of the thesis was to develop a stout and effective method and to educate the users in the skills needed to perform fast and correct analyses. This thesis presents the technology behind XRF instruments and which sources of errors that could affect the results as well as the pedagogical theories used to describe the practical work that takes place at the laboratory. The method and the experiment that preceded the method are presented, and the results and decisions are discussed. How the education was planned, performed and evaluated is also presented.</p>
----------------------------------------------------------------------
In diva2:515607 
abstract is: 
<p>The aim of this thesis project is to produce an algorithm for finding a topologicalrepresentation of a polygon, called a straight skeleton, using floating pointarithmetic. Various straight skeleton algorithms are examined and discussed witha focus on time complexity and one is chosen for implementation. This implementationis then compared with the open source library CGAL with regards torunning time. The result is an algorithm which is based on an algorithm by Felkeland Obdrzalek and which, for polygons with more than five thousand vertices andthree significant digits representing points, runs around 25% faster than CGALsimplementation. Complications regarding the use of floating-point arithmetic arealso discussed.</p>

corrected abstract:
<p>The aim of this thesis project is to produce an algorithm for finding a topological representation of a polygon, called a straight skeleton, using floating point arithmetic. Various straight skeleton algorithms are examined and discussed with a focus on time complexity and one is chosen for implementation. This implementation is then compared with the open source library CGAL with regards to running time. The result is an algorithm which is based on an algorithm by Felkel and Obdrzalek and which, for polygons with more than five thousand vertices and three significant digits representing points, runs around 25% faster than CGALs implementation. Complications regarding the use of floating-point arithmetic are also discussed.</p>
----------------------------------------------------------------------
In diva2:513904 
abstract is: 
<p>Within the course of the master thesis project, two thermal phase change models for direct contact conden-sation were developed with different modeling approaches, namely interfacial heat transfer and combustionanalysis approach.After understanding the OpenFOAM framework for two phase flow solvers with phase change capabilities,a new solver, including the two developed models for phase change, was implemented under the name of interPhaseChangeCondenseTempFoam and analyzed in a series of 18 tests in order to determine the physicalbehavior and robustness of the developed models. The solvers use a volume-of-fluid (VOF) approach withmixed fluid properties.It has been shown that the approach with inter-facial heat transfer shows physical behavior, a strong timestep robustness and good grid convergence properties. The solver can be used as a basis for more advancedsolvers within the phase change class.</p>

corrected abstract:
<p>Within the course of the master thesis project, two thermal phase change models for direct contact condensation were developed with different modeling approaches, namely inter-facial heat transfer and combustion analysis approach.</p><p>After understanding the OpenFOAM framework for two phase flow solvers with phase change capabilities, a new solver, including the two developed models for phase change, was implemented under the name of interPhaseChangeCondenseTempFoam and analyzed in a series of 18 tests in order to determine the physical behavior and robustness of the developed models. The solvers use a volume-of-fluid (VOF) approach with mixed fluid properties.</p><p>It has been shown that the approach with inter-facial heat transfer shows physical behavior, a strong time step robustness and good grid convergence properties. The solver can be used as a basis for more advanced solvers within the phase change class.</p>
----------------------------------------------------------------------
In diva2:1881338 
abstract is: 
<p>This paper studies the feasibility of modelling the maximum stresses in a boltedjoint using analytical solutions, compared to using computed FEM solutions.The bolted joint is simplified to three basic load cases and modelled with Euler-Bernoulli and Timoshenko beam theory. Modelling is feasible for longer beams,but at aspect ratios &lt;1 the FEM solution becomes dominated by singularitiesfrom the FEM model’s sharp corners and boundary conditions. These singular-ities mirror real-life stress concentrations at the dimensional transition betweenshank and bolt head as well as the fixed nature of the threaded section, and mustbe accounted for in an analytical model.</p>

corrected abstract:
<p>This paper studies the feasibility of modelling the maximum stresses in a bolted joint using analytical solutions, compared to using computed FEM solutions. The bolted joint is simplified to three basic load cases and modelled with Euler-Bernoulli and Timoshenko beam theory. Modelling is feasible for longer beams, but at aspect ratios &lt;1 the FEM solution becomes dominated by singularities from the FEM model’s sharp corners and boundary conditions. These singularities mirror real-life stress concentrations at the dimensional transition between shank and bolt head as well as the fixed nature of the threaded section, and must be accounted for in an analytical model.</p>
----------------------------------------------------------------------
In diva2:1876753 - missing space in title:
"Optimizing Hydroelectric Power Production along the Skellefte River: A comparative study of linear programming andmixed-integer linear programming models"
==>
"Optimizing Hydroelectric Power Production along the Skellefte River: A comparative study of linear programming and mixed-integer linear programming models"


abstract is: 
<p>This thesis contains a comparative study of optimization models designed to maximize profits from 15interconnected hydroelectric power plants along the Skellefte River, using both linear programming (LP)and mixed integer linear programming (MILP) models.This project is based on previous work, which has utilized the LP model for this optimization problem.We expand upon this by employing and contrasting both LP and MILP models, solved using CPLEX inthe modeling software General Algebraic Modeling System (GAMS). This contrasts with earlier studies,that used the Spine software tool. In our approach, GAMS applies the simplex method for the LP modeland branch-and-cut for the MILP.The results indicate that the LP model is sufficiently accurate for this type of hydropower optimizationproblem, as there were no significant differences in the energy generated or the profit made using theMILP model compared to the original LP model.</p>

corrected abstract:
<p>This thesis contains a comparative study of optimization models designed to maximize profits from 15 interconnected hydroelectric power plants along the Skellefte River, using both linear programming (LP) and mixed integer linear programming (MILP) models.</p><p>This project is based on previous work, which has utilized the LP model for this optimization problem. We expand upon this by employing and contrasting both LP and MILP models, solved using CPLEX in the modeling software General Algebraic Modeling System (GAMS). This contrasts with earlier studies, that used the Spine software tool. In our approach, GAMS applies the simplex method for the LP model and branch-and-cut for the MILP.</p><p>The results indicate that the LP model is sufficiently accurate for this type of hydropower optimization problem, as there were no significant differences in the energy generated or the profit made using the MILP model compared to the original LP model.</p>
----------------------------------------------------------------------
In diva2:1871452 
abstract is: 
<p>This thesis aims to classify the Aerodynamics of a pigeon’s tail. In this thesis two differenttheoretical solutions are implemented to solve the aerodynamics. The first is by analysingequations created by Polhamus [1], and the second theoretical analysis is done by digitalizingmoment coefficient curves by Winter [2]. The equation for the moment coefficient cm is shownin equation 1, here M is the moment q the dynamic pressure, S the wings polanform area,and c the chord leangth. A practical experiment is conducted on the pigeon’s tail where theaerodynamic forces are measured in a wind tunnel. The experimental setup included motiontracking to analyze the Angle of Attack of the tail in the wind tunnel, a force sensor, anda servo to set the angle of attack. The resulting curves show that there should exist a peakaerodynamic performance at around 30◦ spread angle and 30◦ angle of attack. The peak is found in both the theoretical and the experimental analysis.</p>

partal corrected: diva2:1871452: <p>This thesis aims to classify the Aerodynamics of a pigeon’s tail. In this thesis two different theoretical solutions are implemented to solve the aerodynamics. The first is by analysing equations created by Polhamus [1], and the second theoretical analysis is done by digitalizing moment coefficient curves by Winter [2]. The equation for the moment coefficient cm is shown in equation 1, here M is the moment q the dynamic pressure, S the wings polanform area, and c the chord leangth. A practical experiment is conducted on the pigeon’s tail where the aerodynamic forces are measured in a wind tunnel. The experimental setup included motiontracking to analyze the Angle of Attack of the tail in the wind tunnel, a force sensor, anda servo to set the angle of attack. The result ing curves show that there should exist a peak aerodynamic performance at around 30◦ spread angle and 30◦ angle of attack. The peak is found in both the theoretical and the experimental analysis.</p>

corrected abstract:
<p>This thesis aims to classify the Aerodynamics of a pigeon’s tail. In this thesis two different theoretical solutions are implemented to solve the aerodynamics. The first is by analysing equations created by Polhamus [1], and the second theoretical analysis is done by digitalizing moment coefficient curves by Winter [2]. The equation for the moment coefficient 𝑐<sub>𝑚</sub> is shown in equation 1, here 𝑀 is the moment 𝑞 the dynamic pressure, 𝑆 the wings polanform area, and 𝑐 the chord leangth. A practical experiment is conducted on the pigeon’s tail where the aerodynamic forces are measured in a wind tunnel. The experimental setup included motion tracking to analyze the Angle of Attack of the tail in the wind tunnel, a force sensor, and a servo to set the angle of attack. The resulting curves show that there should exist a peak aerodynamic performance at around 30º spread angle and 30º angle of attack. The peak is found in both the theoretical and the experimental analysis.</p>
<p style="text-align: center;">𝑐<sub>𝑚</sub> = <span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;">𝑀</br><span style="text-decoration: overline;">𝑞𝑆𝑐</span></span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)</p>

----------------------------------------------------------------------
In diva2:1860537 
abstract is: 
<p>Helical tube bundles are usually used in the steam generator (SG) of High-Temperature Gas-Cooled Reactors (HTGRs) as the heat transfer area. The helical tube bundle is composed of multiple-layer helically coiled tubes, which are fixed by multiple sets of supporting structures. There are ideal flow paths separated by different layers of helical tubes. The velocity non-uniformity for different flow paths will affect the heat transfer tube temperature and the outlet steam temperature uniformity of different tube layers. In the shell side of the SG, turbulent cross flow over helical tube bundles are complicated and difficult to predict due to reverse pressure gradient and boundary layer separation. Due to the huge amount of computation resources consumption, there are few numerical simulation studies on the non-uniformity of cross flow over large-volume helical tube bundles.Two cases, namely the flow past a circular cylinder and cross flow over in-line tube bundles, are simulated to validate if Partially-Averaged Navier-Stokes (PANS) model is suitable for simulations of cross flow over helical tube bundles. The simulation results of k-ω SST PANS model are well agree with the average and local experimental data. Therefore, k-ω SST PANS model is used to investigate the influences of the supporting structure and helical diameter on the non-uniformity of cross flow over ideal helical tube bundles. The helix angle of helical tube bundle is neglected. The computational domain consists of 5 rows of helically coiled tubes in the streamwise direction. Periodic boundary conditions are used for the inlet and outlet to reduce the consumption of computing resources.For cross flow over helical tube bundles, there exists significant circumferential and radial velocities, which means there are secondary flows in the plane perpendicular to the streamwise direction. The radial velocity is about 16% of the streamwise velocity. Due to the presence of secondary flow, cross flow over individual tube is inclined, and the inclination direction changes at different circumferential positions. At the same circumferential position, the flow inclination direction is the same along the streamwise direction and radial direction. For helical tube bundles, the ratio of the blocking area to the flow area (blocking area ratio) of the inner, outer and middle flow paths are different. The blocking area ratio of the inner flow path is large, and the blocking area ratio of the outer flow path is small, resulting in non-uniform velocity distribution in different flow paths.Cross flow over helical tube bundles with three helical diameters (inner wall radius Ri is 0.02 m, 0.14 m and 0.26 m, respectively) are simulated. For small helical diameter tube bundle (Ri = 0.02 m), the maximum streamwise velocity non-uniformity is 16.6%. For tube bundles with middle and large helical diameters (Ri = 0.14 m and 0.26 m), the maximum streamwise velocity non-uniformity is 6.7% and 5.8%, respectively. The results show that the flow non-uniformity becomes more obvious for small helical diameter.The supporting structures results in more complex secondary flows. The secondary flows far from the supporting structures are larger than those in the region near the supporting structures. The supporting structures causes the blocking area ratio of inner, outer and middle flow paths vary with the helical diameter, and the blocking area ratio non-uniformity is larger than that without supporting structures. In the presence of supporting structures, the maximum streamwise velocity non-uniformities of small, middle and large helical diameter tube bundles are 22.0%, 8.8% and 6.3%, respectively. The effect of supporting structures on the flow non-uniformity increases as the helical diameter decreases.</p>

corrected abstract:
<p>Helical tube bundles are usually used in the steam generator (SG) of High-Temperature Gas-Cooled Reactors (HTGRs) as the heat transfer area. The helical tube bundle is composed of multiple-layer helically coiled tubes, which are fixed by multiple sets of supporting structures. There are ideal flow paths separated by different layers of helical tubes. The velocity non-uniformity for different flow paths will affect the heat transfer tube temperature and the outlet steam temperature uniformity of different tube layers. In the shell side of the SG, turbulent cross flow over helical tube bundles are complicated and difficult to predict due to reverse pressure gradient and boundary layer separation. Due to the huge amount of computation resources consumption, there are few numerical simulation studies on the non-uniformity of cross flow over large-volume helical tube bundles.</p><p>Two cases, namely the flow past a circular cylinder and cross flow over in-line tube bundles, are simulated to validate if Partially-Averaged Navier-Stokes (PANS) model is suitable for simulations of cross flow over helical tube bundles. The simulation results of 𝑘-<em>ω</em> SST PANS model are well agree with the average and local experimental data. Therefore, 𝑘-<em>ω</em> SST PANS model is used to investigate the influences of the supporting structure and helical diameter on the non-uniformity of cross flow over ideal helical tube bundles. The helix angle of helical tube bundle is neglected. The computational domain consists of 5 rows of helically coiled tubes in the streamwise direction. Periodic boundary conditions are used for the inlet and outlet to reduce the consumption of computing resources.</p><p>For cross flow over helical tube bundles, there exists significant circumferential and radial velocities, which means there are secondary flows in the plane perpendicular to the streamwise direction. The radial velocity is about 16% of the streamwise velocity. Due to the presence of secondary flow, cross flow over individual tube is inclined, and the inclination direction changes at different circumferential positions. At the same circumferential position, the flow inclination direction is the same along the streamwise direction and radial direction. For helical tube bundles, the ratio of the blocking area to the flow area (blocking area ratio) of the inner, outer and middle flow paths are different. The blocking area ratio of the inner flow path is large, and the blocking area ratio of the outer flow path is small, resulting in non-uniform velocity distribution in different flow paths.</p><p>Cross flow over helical tube bundles with three helical diameters (inner wall radius 𝑅<sub>𝑖</sub> is 0.02 m, 0.14 m and 0.26 m, respectively) are simulated. For small helical diameter tube bundle (𝑅<sub>𝑖</sub> = 0.02 m), the maximum streamwise velocity non-uniformity is 16.6%. For tube bundles with middle and large helical diameters (𝑅<sub>𝑖</sub> = 0.14 m and 0.26 m), the maximum streamwise velocity non-uniformity is 6.7% and 5.8%, respectively. The results show that the flow non-uniformity becomes more obvious for small helical diameter.</p><p>The supporting structures results in more complex secondary flows. The secondary flows far from the supporting structures are larger than those in the region near the supporting structures. The supporting structures causes the blocking area ratio of inner, outer and middle flow paths vary with the helical diameter, and the blocking area ratio non-uniformity is larger than that without supporting structures. In the presence of supporting structures, the maximum streamwise velocity non-uniformities of small, middle and large helical diameter tube bundles are 22.0%, 8.8% and 6.3%, respectively. The effect of supporting structures on the flow non-uniformity increases as the helical diameter decreases.</p>
----------------------------------------------------------------------
In diva2:1846658 
abstract is: 
<p>A distributed quantum computer holds the potential to emulate a larger quantumcomputer by being partitioned it into smaller modules where local operations (LO)can be applied, and classical communication (CC) can be utilized between thesemodules. Finding algorithms under LOCC restrictions is crucial for leveraging thecapabilities of distributed quantum computing, This thesis explores probabilisticexact LOCC supermaps, that maps 2-qubit bipartite unitary operations to its inver-sion and complex conjugation. Presented are LOCC unitary inversion and complexconjugation supermaps that use 3 calls of the operation, achieving success proba-bilities of 3/128 and 3/8, respectively. These supermaps are discovered through anexamination of the Kraus Cirac decomposition and its interaction with single qubitunitary inversion supermaps. These results can be used for time reversal of as welland noise reduction in closed distributed quantum systems</p>

corrected abstract:
<p>A distributed quantum computer holds the potential to emulate a larger quantum computer by being partitioned it into smaller modules where local operations (LO) can be applied, and classical communication (CC) can be utilized between these modules. Finding algorithms under LOCC restrictions is crucial for leveraging the capabilities of distributed quantum computing, This thesis explores probabilistic exact LOCC supermaps, that maps 2-qubit bipartite unitary operations to its inversion and complex conjugation. Presented are LOCC unitary inversion and complex conjugation supermaps that use 3 calls of the operation, achieving success probabilities of 3/128 and 3/8, respectively. These supermaps are discovered through an examination of the Kraus-Cirac decomposition and its interaction with single qubit unitary inversion supermaps. These results can be used for time reversal of as well and noise reduction in closed distributed quantum systems</p>
----------------------------------------------------------------------
In diva2:1816921 
abstract is: 
<p>This thesis explores the feasibility of clustering mixed data and unevenly spaced time series for customer segmentation. The proposed method implements the Gower dissimilarity as the local distance function in dynamic time warping to calculate dissimilarities between mixed data time series. The time series are then clustered with k−medoids and the clusters are evaluated with the silhouette score and t−SNE. The study further investigates the use of a time warping regularisation parameter. It is derived that implementing time as a feature has the same effect as penalising time warping, andtherefore time is implemented as a feature where the feature weight is equivalent to a regularisation parameter.</p><p>The results show that the proposed method successfully identifies clusters in customer transaction data provided by Nordea. Furthermore, the results show a decrease in the silhouette score with an increase in the regularisation parameter, suggesting that the time at which a transaction occurred might not be of relevance to the given dataset. However, due to the method’s high computational complexity, it is limited to relatively small datasets and therefore a need exists for a more scalable and efficient clustering technique.</p>

corrected abstract:
<p>This thesis explores the feasibility of clustering mixed data and unevenly spaced time series for customer segmentation. The proposed method implements the Gower dissimilarity as the local distance function in dynamic time warping to calculate dissimilarities between mixed data time series. The time series are then clustered with <em>k</em>−medoids and the clusters are evaluated with the silhouette score and <em>t</em>−SNE. The study further investigates the use of a time warping regularisation parameter. It is derived that implementing time as a feature has the same effect as penalising time warping, and therefore time is implemented as a feature where the feature weight is equivalent to a regularisation parameter.</p><p>The results show that the proposed method successfully identifies clusters in customer transaction data provided by Nordea. Furthermore, the results show a decrease in the silhouette score with an increase in the regularisation parameter, suggesting that the time at which a transaction occurred might not be of relevance to the given dataset. However, due to the method’s high computational complexity, it is limited to relatively small datasets and therefore a need exists for a more scalable and efficient clustering technique.</p>
----------------------------------------------------------------------
In diva2:1778733 
abstract is: 
<p>This thesis analyzes the mean variance optimization problem with respect to cardinalityconstraints. The aim of this thesis is to figure out how much of an impact transactionchanges has on the profit and risk of a portfolio. We solve the problem by implementingmixed integer programming (MIP) and solving the problem by using the Gurobi solver.In doing this, we create a mathematical model that enforces the amount of transactionchanges from the initial portfolio. Our results is later showed in an Efficient Frontier,to see how the profit and risk are changing depending on the transaction changes.Overall, this thesis demonstrates that the application of MIP is an effective approachto solve the mean variance optimization problem and can lead to improved investmentoutcomes.</p>


corrected abstract:
<p>This thesis analyzes the mean variance optimization problem with respect to cardinality constraints. The aim of this thesis is to figure out how much of an impact transaction changes has on the profit and risk of a portfolio. We solve the problem by implementing mixed integer programming (MIP) and solving the problem by using the Gurobi solver. In doing this, we create a mathematical model that enforces the amount of transaction changes from the initial portfolio. Our results is later showed in an Efficient Frontier, to see how the profit and risk are changing depending on the transaction changes. Overall, this thesis demonstrates that the application of MIP is an effective approach to solve the mean variance optimization problem and can lead to improved investment outcomes.</p>
----------------------------------------------------------------------
In diva2:1776606 
abstract is: 
<p>The aim of this thesis is to examine the connection between the Riemannhypothesis and the distribution of prime numbers. We first derive theanalytic continuation of the zeta function and prove some of its propertiesusing a functional equation. Results from complex analysis such asJensen’s formula and Hadamard factorization are introduced to facilitatea deeper investigation of the zeros of the zeta function. Subsequently, therelation between these zeros and the asymptotic distribution of primesis rendered explicit: they determine the error term when the prime-counting function π(x) is approximated by the logarithmic integral li(x).We show that this absolute error is O(x exp(−c√log x) ) and that the Riemannhypothesis implies the significantly improved upper bound O(√x log x).</p><p> </p>

corrected abstract:
<p>The aim of this thesis is to examine the connection between the Riemann hypothesis and the distribution of prime numbers. We first derive the analytic continuation of the zeta function and prove some of its properties using a functional equation. Results from complex analysis such as Jensen’s formula and Hadamard factorization are introduced to facilitate a deeper investigation of the zeros of the zeta function. Subsequently, the relation between these zeros and the asymptotic distribution of primes is rendered explicit: they determine the error term when the prime-counting function π(𝑥) is approximated by the logarithmic integral li(𝑥). We show that this absolute error is 𝒪(𝑥 e<sup>&minus;𝑐&radic;<span style="text-decoration: overline;">log 𝑥</span></sup>) and that the Riemann hypothesis implies the significantly improved upper bound 𝒪(&radic;<span style="text-decoration: overline;">𝑥</span> log 𝑥).</p>
----------------------------------------------------------------------
In diva2:1705120 
abstract is: 
<p>Industrial gas turbines constitute an integral part of today’s electricalpower production infrastructure. Reacting gas flows in these machines arevery interesting and complex in nature since they exhibit highly turbulentbehaviour which is strongly coupled to the chemical reaction dynamics.Thus developing accurate CFD models for such flows while keeping thecomputational expense reasonable is a non-trivial task. In this studyLarge-Eddy simulations of hydrogen-methane fuel mixture combustion inthe SGT800’s (Siemens Energy Gas turbine 800) burner, in atmosphericconditions are performed using the CFD code Starccm+ (versions 16.04and 16.06).</p>

corrected abstract:
<p>Industrial gas turbines constitute an integral part of today’s electrical power production infrastructure. Reacting gas flows in these machines are very interesting and complex in nature since they exhibit highly turbulent behaviour which is strongly coupled to the chemical reaction dynamics. Thus developing accurate CFD models for such flows while keeping the computational expense reasonable is a non-trivial task. In this study Large-Eddy simulations of hydrogen-methane fuel mixture combustion in the SGT800’s (Siemens Energy Gas turbine 800) burner, in atmospheric conditions are performed using the CFD code Starccm+ (versions 16.04 and 16.06).</p>
----------------------------------------------------------------------
In diva2:1683766 
abstract is: 
<p>Single-photon detectors are an essential tool for quantum photonics: one can harness the quantum properties of single photons for quantum communication as well as use the detectors in low illumination applications.Superconducting nanowire single-photon detectors offer high detection efficiency over a widerange of wavelengths, low dark counts, and a good timing resolution compared to single-photonavalanche diodes.The goal of this master thesis is to fabricate SNSPDs and to design a mount for an opticalfiber onto the SNSPD that can be fabricated inside the nanofabrication facility of KTH. Anamplification stage was designed to be placed inside the cryostat to lower the time uncertaintyof the detection peaks. We designed the fiber mount and the SNSPD using only lithographic processes. We designed a low-noise amplification and biasing system inside the cryostat to lower the amplifier noise compared to amplification at room temperature.</p>

corrected abstract:
<p>Single-photon detectors are an essential tool for quantum photonics: one can harness the quantum properties of single photons for quantum communication as well as use the detectors in low illumination applications.</p><p>Superconducting nanowire single-photon detectors offer high detection efficiency over a wide range of wavelengths, low dark counts, and a good timing resolution compared to single-photon avalanche diodes.</p><p>The goal of this master thesis is to fabricate SNSPDs and to design a mount for an optical fiber onto the SNSPD that can be fabricated inside the nanofabrication facility of KTH. An amplification stage was designed to be placed inside the cryostat to lower the time uncertainty of the detection peaks. We designed the fiber mount and the SNSPD using only lithographic processes.</p><p>We designed a low-noise amplification and biasing system inside the cryostat to lower the amplifier noise compared to amplification at room temperature.</p>
----------------------------------------------------------------------
In diva2:1679321 
abstract is: 
<p>In this work the susceptibility of Fe-10Cr-4Al steel to liquid metal embitterment (LME)in low oxygen environment was investigated. slow strain rate testing (SSRT) wereconducted on 10-4 FeCrAl steel in a stagnant lead from 340-480◦C, lead-bismutheutectic (LBE) from 140-450◦C and lead-bismuth mixture at 375◦C with increasingbismuth content from 0.1wt%-40wt%. The results showed that in the stagnant leadenvironment the FeCrAl steel showed no sign of LME with all samples being subjectedto around 25% strain before final break. In LBE the samples were affected by LMEespecially at 350-400◦C. The total elongation to failure reduced in LBE from 25%to 13.1% and a ductility trough from 190-400◦C was observed. In the lead-bismuthmixture there was a reduction in ductility at 5wt% going from 25% to 20% totalelongation, at 15wt% going from 20% to 16% total elongation and at 30wt% going from16% to 13% total elongation.</p>

corrected abstract:
<p>In this work the susceptibility of Fe-10Cr-4Al steel to liquid metal embitterment (LME) in low oxygen environment was investigated. slow strain rate testing (SSRT) were conducted on 10-4 FeCrAl steel in a stagnant lead from 340-480ºC, lead-bismuth eutectic (LBE) from 140-450ºC and lead-bismuth mixture at 375ºC with increasing bismuth content from 0.1wt%-40wt%. The results showed that in the stagnant lead environment the FeCrAl steel showed no sign of LME with all samples being subjected to around 25% strain before final break. In LBE the samples were affected by LME especially at 350-400ºC. The total elongation to failure reduced in LBE from 25% to 13.1% and a ductility trough from 190-400ºC was observed. In the lead-bismuth mixture there was a reduction in ductility at 5wt% going from 25% to 20% total elongation, at 15wt% going from 20% to 16% total elongation and at 30wt% going from 16% to 13% total elongation.</p>
----------------------------------------------------------------------
In diva2:1677530 
abstract is: 
<p>The purpose of this paper is to gain a deeper understanding of how biocomposites can beutilized to enable and simplify domestic waste management. The product designed in thisproject is built to compress waste and increase recycling rates. Through constructionalanalysis, calculations and simulations, the material DuraSense® was chosen, a sustainablebiocomposite based on recycled wood-fibres that can reduce carbon dioxide emissions by upto 51% compared to an identical model produced in PP plastic.The result of the project concludes that composites can be utilized to optimize structuralintegrity whilst reducing environmental impact and costs. The possibility of using a sandwichconstruction was analysed but decided against, mainly due to production difficulties andcosts.</p>

corrected abstract:
<p>The purpose of this paper is to gain a deeper understanding of how biocomposites can be utilized to enable and simplify domestic waste management. The product designed in thisproject is built to compress waste and increase recycling rates. Through constructional analysis, calculations and simulations, the material DuraSense® was chosen, a sustainable biocomposite based on recycled wood-fibres that can reduce carbon dioxide emissions by upto 51% compared to an identical model produced in PP plastic. The result of the project concludes that composites can be utilized to optimize structural integrity whilst reducing environmental impact and costs. The possibility of using a sandwich construction was analysed but decided against, mainly due to production difficulties and costs.</p>
----------------------------------------------------------------------
In diva2:1631314 
abstract is: 
<p>Transmitarrays antennas (TAs) can be seen as the planar counterpart of optical lenses. They are composed of thin radiating elements (unit cells) which introduce different local phase shifts on an incident electromagnetic wave, emitted by a primary source, and re-radiate it. By properly designing the unit cells and their distribution in the TA, the properties of the incident wave, e.g. wavefront and polarization, as well as the pattern of the radiated field can be tailored. Moreover, TAs are suited to low-cost multilayer fabrication processes, e.g. printed circuit board (PCB) technology, and can achieve electronic reconfiguration embedding diodes. Therefore, TAs are natural and cost-effective candidates for applications requiring to steer and shape the antenna beam, such as satellite communications (Satcom) and future terrestrial wireless networks. For instance, satellite antennas radiate contoured beams to cover specific Earth regions, whereas Satcom ground terminals and mobile base stations require very directive beams compliant with prescribed radiation masks. In many cases, the amplitude of the field impinging on the TA is fixed and the TA phase profile, i.e. the spatial distribution of the phase-shifting elements, is the only parameter that can be designed to generate the desired radiation pattern. Thus, versatile, efficient and robust phase-only synthesis methods are essential. Closed-form expressions for the phase profile can be derived only in a few cases and for specific targeted far-field patterns. On the other hand, synthesis approaches based on global optimization techniques, such as genetic algorithms, are general purpose but their convergence and accuracy is often poor, despite the long computation time. In this thesis, a mathematical approach for the phase-only synthesis of TAs using convex optimization is developed to solve diverse pattern shaping problems. The use of convex optimization ensures a good compromise between the generality, robustness and computational cost of the method.First, a model for the analysis of the TA is presented. It accurately predicts the antenna radiation pattern using the equivalence theorem and includes the impact of the spillover, i.e. the direct radiation from the TA feed. Then, the TA synthesis is formulated in terms of the far-field intensity pattern computed by the model. The phase-only synthesis problem is inherently non-convex. However, a sequential convex optimization procedure relying on proper relaxations is proposed to approximately solve it. The accuracy of these sub-optimal solutions is discussed and methods to enhance it are compared. The procedure is successfully applied to synthesize relatively large TAs, with symmetrical and non-symmetrical phase profiles, radiating either focused-beam or shaped-beam patterns, with challenging mask constraints.Finally, three millimeter-wave TAs, comprising different sets of unit cells, are designed using the synthesis procedure. The good agreement between the predicted radiation patterns and those obtained from full-wave simulations of the antennas demonstrates the precision and versatility of the proposed tool, within its range of validity. </p>

corrected abstract:
<p>Transmitarrays antennas (TAs) can be seen as the planar counterpart of optical lenses. They are composed of thin radiating elements (unit cells) which introduce different local phase shifts on an incident electromagnetic wave, emitted by a primary source, and re-radiate it. By properly designing the unit cells and their distribution in the TA, the properties of the incident wave, e.g.  wavefront and polarization, as well as the pattern of the radiated field can be tailored. Moreover, TAs are suited to low-cost multilayer fabrication processes using printed circuit board (PCB) technology, and can achieve electronic reconfiguration embedding diodes.</p><p>Therefore, TAs are natural and cost-effective candidates for applications requiring to steer and shape the antenna beam. For instance, satellite communication (Satcom) antennas should radiate contoured beams to cover specific Earth regions. Future mobile networks, on the other hand, require very directive beams compliant with prescribed radiation masks.</p><p>In many cases, the amplitude of the field impinging on the TA is fixed. Therefore, the TA phase profile is the only parameter that can be designed to optimize the radiation pattern.  Thus, versatile, efficient and robust phase-only synthesis methods are essential. Closed-form expressions for the phase profile can be derived only in a few cases. Synthesis approaches based on global optimization techniques, such as genetic algorithms, are general purpose but their convergence and accuracy is often poor and time-consuming. In this thesis, a mathematical approach for the phase-only synthesis of TAs using convex optimization is developed to solve diverse pattern shaping problems. The use of convex optimization ensures a good compromise between the generality, robustness and computational cost of the method.</p><p>First, a model for the analysis of the TA is presented. It accurately predicts the antenna radiation pattern using the equivalence theorem and includes the impact of the spillover. Then, the TA synthesis is formulated in terms of the far-field intensity computed by the model.  The phase-only synthesis problem is inherently non-convex. However, a sequential convex optimization procedure relying on proper relaxations is proposed to approximately solve it.  The procedure is successfully applied to synthesize relatively large focused- and shaped-beam TAs, with symmetrical and non-symmetrical phase profiles and challenging mask constraints.  Finally, three millimeter-wave TAs, comprising different sets of unit cells, are designed using the synthesis procedure. The good agreement of the results with full-wave simulations demonstrates the precision and versatility of the proposed tool.</p>


Note that the abstract in DiVA does not match the wording of the abstract in the thesis.
----------------------------------------------------------------------
In diva2:1567676 
abstract is: 
<p>An epidemiological model with vaccinations, testing and social distancing isproposed. The vaccinations are used as a control parameter to be optimised,using Pontryagin’s maximum principle and numerical methods to solve thecorresponding differential equations. Analysis of some of the parameters isdone, where the speed of the vaccination greatly decreases the amount of overallinfected, as does more social distancing. The analysis also shows that differentvalues for the associated costs of infected, treated, disease­induced deaths andvaccinations do not have as large impact on the system.</p><p> </p>

corrected abstract:
<p>An epidemiological model with vaccinations, testing and social distancing is proposed. The vaccinations are used as a control parameter to be optimised, using <em>Pontryagin’s maximum principle</em> and numerical methods to solve the corresponding differential equations. Analysis of some of the parameters is done, where the speed of the vaccination greatly decreases the amount of overall infected, as does more social distancing. The analysis also shows that different values for the associated costs of infected, treated, disease­induced deaths and vaccinations do not have as large impact on the system.</p>
----------------------------------------------------------------------
In diva2:1567321 
abstract is: 
<p>This paper aims to describe the entire processof developing and testing an underwater quadcopter using aflight chip (FC) as the steering computer. The problem withthe FC is that the physics differs between using an air droneand an underwater drone. Most importantly, buoyancy is notnegligible underwater, while it is negligible in the air. Ultimately,it proved to be possible to design and build a usable underwaterquadcopter using an FC. The main design requirements foundto be important to make this possible is to decrease buoyancyas much as possible while still creating a watertight enclosure.</p>


corrected abstract:
<p>This paper aims to describe the entire process of developing and testing an underwater quadcopter using a flight chip (FC) as the steering computer. The problem with the FC is that the physics differs between using an air drone and an underwater drone. Most importantly, buoyancy is not negligible underwater, while it is negligible in the air. Ultimately, it proved to be possible to design and build a usable underwater quadcopter using an FC. The main design requirements found to be important to make this possible is to decrease buoyancy as much as possible while still creating a watertight enclosure.</p>
----------------------------------------------------------------------
In diva2:1528148 
abstract is: 
<p>Airships were very popular 90 years ago with, for example, german Zeppelins. Now theyare back for several reasons, like their low energy consumption.But there are also still many problems to deal with like their sensitivity to wind gusts.In addition, the airships need more studies to improve their flight mechanics and sensitivityto the wind.This degree project, done with the French Aerospace Lab ONERA in Lille, studies a specificairship which is 5mlong and 1.7mwide. First, the airship is studied without wind to determineaerodynamic coefficients and added masses. Then, the model is confronted to experiments withwind gusts.</p>

corrected abstract:
<p>Airships were very popular 90 years ago with, for example, german Zeppelins. Now they are back for several reasons, like their low energy consumption.</p><p>But there are also still many problems to deal with like their sensitivity to wind gusts.</p><p>In addition, the airships need more studies to improve their flight mechanics and sensitivity to the wind.</p><p>This degree project, done with the French Aerospace Lab ONERA in Lille, studies a specific airship which is 5 m long and 1.7 m wide. First, the airship is studied without wind to determine aerodynamic coefficients and added masses. Then, the model is confronted to experiments with wind gusts.</p>
----------------------------------------------------------------------
In diva2:1527799 
abstract is: 
<p>The aim of this thesis was to make a practical tool for low frequency analysis in room acoustics.The need arises from Acad’s experience that their results from simulations using raytracing software deviate in the lower frequencies when compared to field measurements inrooms. The tool was programmed in Matlab and utilizes the Finite Difference Time Domain (FDTD) method, which is a form of rapid finite element analysis in the time domain.A number of tests have been made to investigate the practical limitations of the FDTD method, such as numerical errors caused by sound sources, discretization and simulation time. Boundary conditions, with and without frequency dependence, have been analysed bycomparing results from simulations of a virtual impedance tube and reverberation room to analytical solutions. These tests show that the use of the FDTD method appears well suited for the purpose of the tool.A field test was made to verify that the tool enables easy and relatively quick simulations of real rooms, with results well in line with measured acoustic parameters. Comparisons of the results from using the FDTD method, ray-tracing and finite elements (FEM) showed goodcorrelation. This indicates that the deviations Acad experience between simulated results and field measurements are most likely caused by uncertainties in the sound absorption data used for low frequencies rather than by limitations in the ray-tracing software. The FDTDtool might still come in handy for more complex models, where edge diffraction is a more important factor, or simply as a means for a “second opinion” to ray-tracing - in general FEM is too time consuming a method to be used on a daily basis.Auxiliary tools made for importing models, providing output data in the of room acoustic parameters, graphs and audio files are not covered in detail here, as these lay outside the scope of this thesis.</p>

corrected abstract:
<p>The aim of this thesis was to make a practical tool for low frequency analysis in room acoustics. The need arises from Acad’s experience that their results from simulations using raytracing software deviate in the lower frequencies when compared to field measurements in rooms. The tool was programmed in Matlab and utilizes the Finite Difference Time Domain (FDTD) method, which is a form of rapid finite element analysis in the time domain.</p><p>A number of tests have been made to investigate the practical limitations of the FDTD method, such as numerical errors caused by sound sources, discretization and simulation time. Boundary conditions, with and without frequency dependence, have been analysed by comparing results from simulations of a virtual impedance tube and reverberation room to analytical solutions. These tests show that the use of the FDTD method appears well suited for the purpose of the tool.</p><p>A field test was made to verify that the tool enables easy and relatively quick simulations of real rooms, with results well in line with measured acoustic parameters. Comparisons of the results from using the FDTD method, ray-tracing and finite elements (FEM) showed good correlation. This indicates that the deviations Acad experience between simulated results and field measurements are most likely caused by uncertainties in the sound absorption data used for low frequencies rather than by limitations in the ray-tracing software. The FDTD tool might still come in handy for more complex models, where edge diffraction is a more important factor, or simply as a means for a “second opinion” to ray-tracing - in general FEM is too time consuming a method to be used on a daily basis.</p><p>Auxiliary tools made for importing models, providing output data in the of room acoustic parameters, graphs and audio files are not covered in detail here, as these lay outside the scope of this thesis.</p>
----------------------------------------------------------------------
In diva2:1480880 
abstract is: 
<p>This thesis investigates semi-rigid cables exposed to a harsh vibration environment. Thethesis includes the creation of a simulation model of the semi-rigid cables as well asdeveloping a set-up and performing a experimental vibration test. It was shown that thecables could be modeled as a isotropic material with a fitted elastic modulus of 55 GPaand give accurate results in comparison with the experimental vibration test. However itis important that the boundary conditions of the cable are applied correctly. Regardingthe lifetime of the cables it was shown that pre-stress, due to a bending moment, inthe cables did not effect the lifetime of the cables. The amplitude moment, due to theoscillation in the cable from the random vibration, did however effect the lifetime of thecables. With data from the simulations and the vibration test an equation describingrelation between the amplitude moment and the number of cycles the cable was exposedto could be formulated.</p>

corrected abstract:
<p>This thesis investigates semi-rigid cables exposed to a harsh vibration environment. The thesis includes the creation of a simulation model of the semi-rigid cables as well as developing a set-up and performing a experimental vibration test. It was shown that the cables could be modeled as a isotropic material with a fitted elastic modulus of 55 GPa and give accurate results in comparison with the experimental vibration test. However it is important that the boundary conditions of the cable are applied correctly. Regarding the lifetime of the cables it was shown that pre-stress, due to a bending moment, in the cables did not effect the lifetime of the cables. The amplitude moment, due to the oscillation in the cable from the random vibration, did however effect the lifetime of the cables. With data from the simulations and the vibration test an equation describing relation between the amplitude moment and the number of cycles the cable was exposed to could be formulated.</p>
----------------------------------------------------------------------
In diva2:1464097 
abstract is: 
<p>Vehicle electrification plays a significant role in the effort to reduce the environmental impact of the automotive industry. Scania is one of the leading manufacturers ofheavy vehicles which is currently moving towards a sustainable transport system by manufacturing a new generation of heavy vehicles powered by batteries. Oneof the major concerns with these vehicles is related to the noise generated by the electric axial fans used in the cooling system. This project was conducted with thepurpose of investigating the factors that positively affect both noise and performance in the electric fans. Based on two different blade design methods and several noisecontrol techniques, 11 fan models were developed. The fan models created with design method 1 are equipped with cambered-plate blades, while the models madewith design method 2 consist of airfoil-shaped blades. Moreover, the performance of these models was analyzed by using theoretical methods and Computational FluidDynamics (CFD). In addition, two empirical approaches were used to estimate the acoustic energy emitted by the fan models. Furthermore, the developed modelswere compared with two commercially available fans. It was found that both design methods provide similar performance in low pressure differences. On the other hand,the efficiency and acoustic energy are influenced by the choice of the noise control methods.</p>

corrected abstract:
<p>Vehicle electrification plays a significant role in the effort to reduce the environmental impact of the automotive industry. Scania is one of the leading manufacturers of heavy vehicles which is currently moving towards a sustainable transport system by manufacturing a new generation of heavy vehicles powered by batteries. One of the major concerns with these vehicles is related to the noise generated by the electric axial fans used in the cooling system. This project was conducted with the purpose of investigating the factors that positively affect both noise and performance in the electric fans. Based on two different blade design methods and several noise control techniques, 11 fan models were developed. The fan models created with design method 1 are equipped with cambered-plate blades, while the models made with design method 2 consist of airfoil-shaped blades. Moreover, the performance of these models was analyzed by using theoretical methods and Computational Fluid Dynamics (CFD). In addition, two empirical approaches were used to estimate the acoustic energy emitted by the fan models. Furthermore, the developed models were compared with two commercially available fans. It was found that both design methods provide similar performance in low pressure differences. On the other hand, the efficiency and acoustic energy are influenced by the choice of the noise control methods.</p>
----------------------------------------------------------------------
In diva2:1380147 
abstract is: 
<p>Autonomous driving is one of the three new technologies that are disrupting the classical vehicle industry together with electriﬁcation and connectivity. All three are pieces in the puzzle to drastically reduce the number of fatalities and injuries from traﬃc accidents but also to reduce the total amount of cars, reduce the polluting greenhouse gases, reduce noise pollution and completely eliminate unwanted driving. For example would most people rather rest, read or do anything else instead of driving in congested traﬃc. It is not small steps to take and it will have to be done incrementally as many other things. Within the vehicle industry racing has always been the natural place to push the boundaries of what is possible. Here new technologies can be tested under controlled circumstances in order to faster ﬁnd the best solution to a problem.Autonomous driving is no exception, the international student competition ”Formula Student” has introduced a driverless racing class and Formula E are slowly implementing Robo Race. The fact that race cars aim to drive at the limits of what is possible enable engineers to develop algorithms that can handle these conditions even in the every day life. Because even though the situations when normal passenger cars need to perform at the limits are rare, it is at these times it can save peoples lives. When an unforeseen event occurs and a fast manoeuvre has to be done in order to avoid the accident, that is when the normal car is driving at the limits. But the other thing to take into considerations when taking new technology into the consumer market is that the cars cannot cost as much as a race car. This means simpler computers has to be used and this in turn puts a constraint on the algorithms in the car. They can not be too computationally heavy.In this thesis a controller is designed to drive as fast as possible around the track. But in contrast to existing research it is not about how much the limit of speed can be pushed but of how simple a controller can be. The controller was designed with a Model Predictive Controller (MPC) that is based on a point mass model, that resembles the Center of Gravity (CoG) of the car. A g-g diagram that describes the limits of the modeled car is used as the constraints and the cost function is to maximize the distance progressed along the track in a ﬁx time step. Together with constraints on the track boundaries an optimization problem is giving the best possible trajectory with respect to the derived model. This trajectory is then sent to a low level controller, based on a Pure Pursuit and P controller, that is following the predicted race trajectory. Everything is done online such that implementation is possible. This controller is then compared and evaluated to a similar successful controller from the literature but which has a more complicated model and MPC formulation. The comparison is made and some notable diﬀerences are that the point mass model is behaving similar to the more complex model from the literature. Though is the hypothesis not correct since the beneﬁts of the simpliﬁcation of the model, from bicycle to point mass model, is replaced when more complex constraints has to be set up, resulting in similar performance even in computational times.A combination of the two models would probably yield the best result with acceptable computational times, this is left as future work to research.</p>

corrected abstract:
<p>Autonomous driving is one of the three new technologies that are disrupting the classical vehicle industry together with electrification and connectivity. All three are pieces in the puzzle to drastically reduce the number of fatalities and injuries from traffic accidents but also to reduce the total amount of cars, reduce the polluting greenhouse gases, reduce noise pollution and completely eliminate unwanted driving. For example would most people rather rest, read or do anything else instead of driving in congested traffic. It is not small steps to take and it will have to be done incrementally as many other things. Within the vehicle industry racing has always been the natural place to push the boundaries of what is possible. Here new technologies can be tested under controlled circumstances in order to faster find the best solution to a problem.</p><p>Autonomous driving is no exception, the international student competition ”Formula Student” has introduced a driverless racing class and Formula E are slowly implementing Robo Race. The fact that race cars aim to drive at the limits of what is possible enable engineers to develop algorithms that can handle these conditions even in the every day life. Because even though the situations when normal passenger cars need to perform at the limits are rare, it is at these times it can save peoples lives. When an unforeseen event occurs and a fast manoeuvre has to be done in order to avoid the accident, that is when the normal car is driving at the limits. But the other thing to take into considerations when taking new technology into the consumer market is that the cars cannot cost as much as a race car. This means simpler computers has to be used and this in turn puts a constraint on the algorithms in the car. They can not be too computationally heavy.</p><p>In this thesis a controller is designed to drive as fast as possible around the track. But in contrast to existing research it is not about how much the limit of speed can be pushed but of how simple a controller can be. The controller was designed with a Model Predictive Controller (MPC) that is based on a point mass model, that resembles the Center of Gravity (CoG) of the car. A g-g diagram that describes the limits of the modeled car is used as the constraints and the cost function is to maximize the distance progressed along the track in a fix time step. Together with constraints on the track boundaries an optimization problem is giving the best possible trajectory with respect to the derived model. This trajectory is then sent to a low level controller, based on a Pure Pursuit and P controller, that is following the predicted race trajectory. Everything is done online such that implementation is possible. This controller is then compared and evaluated to a similar successful controller from the literature but which has a more complicated model and MPC formulation. The comparison is made and some notable differences are that the point mass model is behaving similar to the more complex model from the literature. Though is the hypothesis not correct since the benefits of the simplification of the model, from bicycle to point mass model, is replaced when more complex constraints has to be set up, resulting in similar performance even in computational times.</p><p>A combination of the two models would probably yield the best result with acceptable computational times, this is left as future work to research.</p>
----------------------------------------------------------------------
In diva2:1356933 
abstract is: 
<p>To keep up with technological as well as logistical challenges of the modern automobile market, major car manufacturing firms have resorted to virtual simulation tools. This enables the development as well as validation of vehicular models much before resources are invested into a new physical prototype.This project focuses on the development of a tool that would help in optimising the handling parameters of a vehicle. This is achieved by creating an optimization routine for tuning the various parameters of the Electronic Power Steering (EPAS). This process is usually done manually, by on-track testing, due to the difficulties in correlating Subjective Assessments (SA) with Objective Metrics (OM). Automating this process would help to reduce the overall research and development time, by providing a baseline tune for the EPAS parameters which could then be finely tweaked by manual track testing.The tool is built by interfacing various software in a multi-objective optimisation environment known as ModeFrontier. The modelling and simulations are performed in IPG CarMaker, with the post processing of the results taken care of by Sympathy for Data. Multiple optimization algorithms were tested to achieve the best optimisation routine. The EPAS parameters, namely the Basic Steering Torque, Active Return and Active Damping, act as the input to the optimization routine. The outputs of the model are the Objective Metrics, which provide a clear indication of the dynamic performance of a component. These metrics are optimized to _t the Steering DNA structure, which uniquely describes the attributes of a vehicle. The final optimised vehicle is manually tested at the track, to determine the real driving feel.</p>

corrected abstract:
<p>To keep up with technological as well as logistical challenges of the modern automobile market, major car manufacturing firms have resorted to virtual simulation tools. This enables the development as well as validation of vehicular models much before resources are invested into a new physical prototype.</p><p>This project focuses on the development of a tool that would help in optimising the handling parameters of a vehicle. This is achieved by creating an optimization routine for tuning the various parameters of the Electronic Power Steering (EPAS). This process is usually done manually, by on-track testing, due to the difficulties in correlating Subjective Assessments (SA) with Objective Metrics (OM). Automating this process would help to reduce the overall research and development time, by providing a baseline tune for the EPAS parameters which could then be finely tweaked by manual track testing.</p><p>The tool is built by interfacing various software in a multi-objective optimisation environment known as ModeFrontier. The modelling and simulations are performed in IPG CarMaker, with the post processing of the results taken care of by Sympathy for Data. Multiple optimisation algorithms were tested to achieve the best optimisation routine. The EPAS parameters, namely the Basic Steering Torque, Active Return and Active Damping, act as the input to the optimization routine. The outputs of the model are the Objective Metrics, which provide a clear indication of the dynamic performance of a component. These metrics are optimised to fit the Steering DNA structure, which uniquely describes the attributes of a vehicle. The final optimised vehicle is manually tested at the track, to determine the real driving feel.</p>
----------------------------------------------------------------------
In diva2:1334312 
abstract is: 
<p>The properties of a material stems from the properties of its' componentsand how they interact. Cellulose based materials have many currentand future applications, and understanding how the cellulose behaves onthe nanoscale is important for the development of new materials. Herethe thermal and mechanical properties of cellulose nanobrils were investigatedby means of molecular dynamics. Two types of brils were studied,unoxidised brils and tempo-oxidised brils in the presence Fe3+ions.Both the properties of the brils themselves and the interfaces betweenthem were studied. To do this a tool for generating relevant structuresfor simulation was created. The tool is general purpose and can generatea wide variety of structures besides those used in the simulations.</p>

corrected abstract:
<p>The properties of a material stems from the properties of its' components and how they interact. Cellulose based materials have many current and future applications, and understanding how the cellulose behaves onthe nanoscale is important for the development of new materials. Here the thermal and mechanical properties of cellulose nanofibrils were investigated by means of molecular dynamics. Two types of brils were studied, unoxidised brils and tempo-oxidised brils in the presence Fe3+ions.Both the properties of the brils themselves and the interfaces between them were studied. To do this a tool for generating relevant structures for simulation was created. The tool is general purpose and can generate a wide variety of structures besides those used in the simulations.</p>
----------------------------------------------------------------------
In diva2:1333956 
abstract is: 
<p>Aeronautical companies face well-known engineering challenges, suchas finding the best trade-off between weight and strength of planes.Numerical simulations have become a key tool to design lighter andstronger structures. The apparition of distributed computing has openednew possibilities for numerical computations. This master thesis aimsto improve methods that study crack propagation. Domain decompositionof the structure is used to take maximum advantage of parallelcomputing. This requires unusual solving methods, such as Schurcomplements methods, to correctly solve a given problem.</p>

corrected abstract:
<p>Aeronautical companies face well-known engineering challenges, such as finding the best trade-off between weight and strength of planes. Numerical simulations have become a key tool to design lighter and stronger structures. The apparition of distributed computing has opened new possibilities for numerical computations. This master thesis aims to improve methods that study crack propagation. Domain decomposition of the structure is used to take maximum advantage of parallel computing. This requires unusual solving methods, such as Schur complements methods, to correctly solve a given problem.</p>
----------------------------------------------------------------------
In diva2:1301731 
abstract is: 
<p>This study is evaluating Solid-Acoustic Finite Element modelling as a method for calculating structural vibration response in water. When designing for example vehicles, it is important to avoid vibrational resonance in any part of the structure, as this causes additional noise and reduced lifespan. It is known that vibration response can be affected by the surrounding medium, i.e. water for marine applications.Previous studies show that this effect is both material and geometry dependant why it is hard to apply standardised design rules. An alternative approach is direct calculation using full Fluid Structure Interaction (FSI) by Computational Fluid Dynamics (CFD) and Finite Element Methods (FEM) which is a powerful but slow and computationally costly method.Therefore, there exists a need for a faster and more efficient calculation method to predict how structures subjected to dynamic loads will respond when submerged in water. By modelling water as an acoustic medium, viscous effects are neglected and calculation time can be drastically reduced. Such an approximation is a linearization of the problem and can be suitable when all deformations are assumed to be small and there are no other nonlinear effects present.This study consists of experimental tests where vibrational response was measured for rod shaped test specimens which were suspended in a water filled test rig and excited using an electrodynamic shaker. A Solid-Acoustic Finite Element model of the same experiment was created, and the test and simulation results were compared. The numerical results were shown to agree well with experiments up to 450 Hz. Above 450 Hz differences occur which is probably due to a simplified rig geometry in the numerical model.</p>

corrected abstract:
<p>This study is evaluating Solid-Acoustic Finite Element modelling as a method for calculating structural vibration response in water. When designing for example vehicles, it is important to avoid vibrational resonance in any part of the structure, as this causes additional noise and reduced lifespan. It is known that vibration response can be affected by the surrounding medium, i.e. water for marine applications.</p><p>Previous studies show that this effect is both material and geometry dependant why it is hard to apply standardised design rules. An alternative approach is direct calculation using full Fluid Structure Interaction (FSI) by Computational Fluid Dynamics (CFD) and Finite Element Methods (FEM) which is a powerful but slow and computationally costly method.</p><p>Therefore, there exists a need for a faster and more efficient calculation method to predict how structures subjected to dynamic loads will respond when submerged in water. By modelling water as an acoustic medium, viscous effects are neglected and calculation time can be drastically reduced. Such an approximation is a linearization of the problem and can be suitable when all deformations are assumed to be small and there are no other nonlinear effects present.</p><p>This study consists of experimental tests where vibrational response was measured for rod shaped test specimens which were suspended in a water filled test rig and excited using an electrodynamic shaker. A Solid-Acoustic Finite Element model of the same experiment was created, and the test and simulation results were compared. The numerical results were shown to agree well with experiments up to 450 Hz. Above 450 Hz differences occur which is probably due to a simplified rig geometry in the numerical model.</p>
----------------------------------------------------------------------
In diva2:1270437 
abstract is: 
<p>This is the report of a master thesis in light weight design of a component in a system that harnesses wind power with a kite. The thesis is a degree project in Naval Architecture at KTH with the course code SD271X. The design work is mostly of a structural nature, but systems engineering, and conceptual design is also a major part of thestudy. The first part introduces the problem where the client, SkySails Power GmbH, is looking to design a new control pod for a system that carries 3 times the load as a previous design. The thesis is limited to the design of the load bearing chassis of the pod, but because at the time the other sub systems or components have not yetbeen designed, the study includes concept design of the entire pod system. The flight pattern and load cases of the kite are studied to get the right understanding of the forces that affect the system. The goal is to design achassis that is as affordable, light weight, and as strong as needed for the task.The requirements of the design problem are decided by the master student and the client together after a prestudy was made but they had minor changes further along the design process. It is a real life, organic iterative design process that has a goal from the start to use the opportunity of an outsider to reconsider the design of akey component of the client’s product.The result is a chassis design that is cheaper to produce and weighs less than if the old chassis would be linearly scaled up with the loads. This design has the same concept as the last but with a couple of modifications concerning some attachments to the rest of the system. The requirement of maintaining all previous functionsis achieved. A significant part of the thesis was to determine the boundaries between the areas of where FEM modelling is applicable and where hand calculations estimations are necessary. The results from this work will be used to build a prototype of the chassis, test it in a tensile testing machine, and finally integrate it into theentire system and flown.</p>

corrected abstract:
<p>This is the report of a master thesis in light weight design of a component in a system that harnesses wind power with a kite. The thesis is a degree project in Naval Architecture at KTH with the course code SD271X. The design work is mostly of a structural nature, but systems engineering, and conceptual design is also a major part of the study. The first part introduces the problem where the client, SkySails Power GmbH, is looking to design a new control pod for a system that carries 3 times the load as a previous design. The thesis is limited to the design of the load bearing chassis of the pod, but because at the time the other sub systems or components have not yet been designed, the study includes concept design of the entire pod system. The flight pattern and load cases of the kite are studied to get the right understanding of the forces that affect the system. The goal is to design a chassis that is as affordable, light weight, and as strong as needed for the task.</p><p>The requirements of the design problem are decided by the master student and the client together after a prestudy was made but they had minor changes further along the design process. It is a real life, organic iterative design process that has a goal from the start to use the opportunity of an outsider to reconsider the design of a key component of the client’s product.</p><p>The result is a chassis design that is cheaper to produce and weighs less than if the old chassis would be linearly scaled up with the loads. This design has the same concept as the last but with a couple of modifications concerning some attachments to the rest of the system. The requirement of maintaining all previous functions is achieved. A significant part of the thesis was to determine the boundaries between the areas of where FEM modelling is applicable and where hand calculations estimations are necessary. The results from this work will be used to build a prototype of the chassis, test it in a tensile testing machine, and finally integrate it into the entire system and flown.</p>
----------------------------------------------------------------------
In diva2:1247563 
abstract is: 
<p>In dynamical loaded welded structures fatigue life is often a limiting factor. Common practise is to reduce nominal stress by increased material thickness which leads to oversized, heavy structures. Those practises are replaced by improvement of production and designingtechniques to utilize better properties of high strength steels.When improving for example quality of welds, issues can occur that standards today did not consider and new test methods can be valuable.In this thesis a method for crack detection in non-load carrying T-joints have been tested. Specimens are made from 6mm Strenx 960MC high strength steel. DIC-analyses is made on frames captured be a high-speed camera during fatigue testing. Hypothesis is that crackpropagation starts when relative strain is higher than 10% per 10 000 cycles.Validation of the method is done by beachmarks from variable stress range during the fatigue test. With the same occurrence as framesets for DIC-analyse. Number of beachmarks agree well with the segment when relative strain indicates crackpropagation in a point at the weld toe.</p>

corrected abstract:
<p>In dynamical loaded welded structures fatigue life is often a limiting factor. Common practise is to reduce nominal stress by increased material thickness which leads to oversized, heavy structures. Those practises are replaced by improvement of production and designing techniques to utilize better properties of high strength steels.</p><p>When improving for example quality of welds, issues can occur that standards today did not consider and new test methods can be valuable.</p><p>In this thesis a method for crack detection in non-load carrying T-joints have been tested. Specimens are made from 6mm Strenx 960MC high strength steel. DIC-analyses is made on frames captured be a high-speed camera during fatigue testing. Hypothesis is that crack propagation starts when relative strain is higher than 10% per 10 000 cycles.</p><p>Validation of the method is done by beachmarks from variable stress range during the fatigue test. With the same occurrence as framesets for DIC-analyse.</p><p>Number of beachmarks agree well with the segment when relative strain indicates crack propagation in a point at the weld toe.</p>
----------------------------------------------------------------------
In diva2:1230637 
abstract is: 
<p>In this project we set up a high-resolution spectroscopy setup usinga fiber-based Fabry-P´erot interferometer to investigate epitaxial semiconductorquantum dots. We characterized the performance of the interferometerwith respect to four crucial parameters: reproducibility of measurements,resolution, transmission and stability. The setup is capable tomeasure linewidths of quantum dots with a resolution of up to 70 MHz.Using this setup, we performed high-resolution spectroscopy on a GaAsquantum dot measuring a linewidth of 5.9 ± 0.8 GHz, or 24.6 ± 3.2 μeV.Our results clearly show the applicability and challenges of fiber-basedFabry-P´erot interferometers for quantum dot spectroscopy.</p>

corrected abstract:
<p>In this project we set up a high-resolution spectroscopy setup using a fiber-based Fabry-Pérot interferometer to investigate epitaxial semiconductor quantum dots. We characterized the performance of the interferometer with respect to four crucial parameters: reproducibility of measurements, resolution, transmission and stability. The setup is capable to measure line widths of quantum dots with a resolution of up to 70 MHz. Using this setup, we performed high-resolution spectroscopy on a GaAs quantum dot measuring a linewidth of 5.9 ± 0.8 GHz, or 24.6 ± 3.2 μeV.Our results clearly show the applicability and challenges of fiber-based Fabry-Pérot interferometers for quantum dot spectroscopy.</p>
----------------------------------------------------------------------
In diva2:1216861 - missing space in title:
"High Level Motion Planningfor a Robot"
==>
"High Level Motion Planning for a Robot"

abstract is: 
<p>With high level motion planning a robot can betold what to do. The robot could be ordered to do tasks we donot want to do ourselves, such as cleaning, or to do tasks indangerous situations or even to explore new planets. The goal ofthis project was to construct a framework that returns a plan fora robot based on a description of the environment and a giventask. The given task is expressed in linear temporal logic (LTL).The framework was constructed by using a transition systemdescribing the environment and an automaton constructed fromthe task. The plan returned from the framework is discrete waypoints which the robot can follow and fulfills the given task.Simulations have been done which show that the frameworkworks as expected.</p>

corrected abstract:
<p>With high level motion planning a robot can be told what to do. The robot could be ordered to do tasks we do not want to do ourselves, such as cleaning, or to do tasks in dangerous situations or even to explore new planets. The goal of this project was to construct a framework that returns a plan for a robot based on a description of the environment and a given task. The given task is expressed in linear temporal logic (LTL). The framework was constructed by using a transition system describing the environment and an automaton constructed from the task. The plan returned from the framework is discrete way points which the robot can follow and fulfills the given task. Simulations have been done which show that the framework works as expected.</p>
----------------------------------------------------------------------
In diva2:1142934 - there seems to be a duplicate 'diva2:1120556'
abstract is: 
<p>A thermal analysis has been conducted on thesatellite MIST to learn how well different temperature requirementsare met in its latest configuration. In the process botha geometrical and a mathematical thermal model have beenrefined and updated and new information regarding internal heatdissipation has been added. The three thermally most extremecases have been simulated using the software Systema-Thermicaand the results show that several units aboard are not within theirtemperature limits. Different possibilities to resolve the issues,including the use of passive thermal control, have been discussed.</p>

corrected abstract:
<p>A thermal analysis has been conducted on the satellite MIST to learn how well different temperature requirements are met in its latest configuration. In the process both a geometrical and a mathematical thermal model have been refined and updated and new information regarding internal heat dissipation has been added. The three thermally most extreme cases have been simulated using the software Systema-Thermica and the results show that several units aboard are not within their temperature limits. Different possibilities to resolve the issues, including the use of passive thermal control, have been discussed.</p>
----------------------------------------------------------------------
In diva2:1083787 
abstract is: 
<p>Simulations of subsonic turbulent coaxial hot jets were conducted on two types ofunstructured grids within the framework of STAR-CCM+. The study case is based on atypical airliner turbofan engine model with a core nozzle and a fan nozzle, having a bypassratio of five. The two meshes used are a polyhedral one, suitable for complex surfaces, and atrimmed one mainly made of hexahedral cells. The sensitivity of the study case to variousinputs is attested using second and third order upwind schemes, modelling turbulence with aSST k-omega model. The project proves to be a valid feasibility study for a steady-statesolution on which an aeroacoustic analysis could be based in future works.</p>

corrected abstract:
<p>Simulations of subsonic turbulent coaxial hot jets were conducted on two types of unstructured grids within the framework of STAR-CCM+. The study case is based on a typical airliner turbofan engine model with a core nozzle and a fan nozzle, having a bypass ratio of five. The two meshes used are a polyhedral one, suitable for complex surfaces, and a trimmed one mainly made of hexahedral cells. The sensitivity of the study case to various inputs is attested using second and third order upwind schemes, modelling turbulence with a SST k-omega model. The project proves to be a valid feasibility study for a steady-state solution on which an aeroacoustic analysis could be based in future works.</p>

Note the abstract in the thesis says "k-omega model" and not "k-&omega; model".
----------------------------------------------------------------------
'diva2:1071273' is not present any longer
----------------------------------------------------------------------
In diva2:939506 
abstract is: 
<p>Several BWR utilities are adopting practices for determining the state of control rods based on the neutron sensitive LPRM and gamma sensitive TIP measurement systems of the nuclear reactor core. This method is in this study evaluated by quantitatively analyzing the detector sensitivity to several material and geometrical distortions in the detector vicinity, using the Los Alamos National Laboratory stochastic Monte Carlo code MCNP5 and the Westinghouse 3D core simulator POLCA7. These results are used to determine whether or not there are potential pitfalls that need to be considered when applying the in-core detector based diagnostic methods to determine the state of control rods. This study also addresses the utility concern about shutdown margin deterioration due to potential loss of neutron absorbing material from control rods.</p><p>It is found that indication of potential control rod leakage by means of the LPRM and the gamma TIP probe detector systems is feasible, provided that both detector systems show similar deviations that correspond to the potential loss of neutron absorbing material from the control rod. Furthermore, it is possible to correlate the observed detector signaldeviation to the magnitude of the distortion. The study lays out criteria that need to befulfilled for indication of control rod leakage to be reasonable, but it is to be noted that nodefinitive stand is or can be taken regarding the state of individual control rods at anyutility.</p><p>The shutdown margin is found not to be significantly deteriorated in any of the utility cases that are studied under an assumption of control rod loss of neutron absorbing material as apotential cause for anomalous detector readings. In order to provoke a significant effect, asection of the top fifth of the control rod must have lost neutron absorbing material next to a control rod position with already low shutdown margin.</p>

corrected abstract:
<p>Several BWR utilities are adopting practices for determining the state of control rods based on the neutron sensitive LPRM and gamma sensitive TIP measurement systems of the nuclear reactor core. This method is in this study evaluated by quantitatively analyzing the detector sensitivity to several material and geometrical distortions in the detector vicinity, using the Los Alamos National Laboratory stochastic Monte Carlo code MCNP5 and the Westinghouse 3D core simulator POLCA7. These results are used to determine whether or not there are potential pitfalls that need to be considered when applying the in-core detector based diagnostic methods to determine the state of control rods. This study also addresses the utility concern about shutdown margin deterioration due to potential loss of neutron absorbing material from control rods.</p><p>It is found that indication of potential control rod leakage by means of the LPRM and the gamma TIP probe detector systems is feasible, provided that both detector systems show similar deviations that correspond to the potential loss of neutron absorbing material from the control rod. Furthermore, it is possible to correlate the observed detector signal deviation to the magnitude of the distortion. The study lays out criteria that need to be fulfilled for indication of control rod leakage to be reasonable, but it is to be noted that no definitive stand is or can be taken regarding the state of individual control rods at any utility.</p><p>The shutdown margin is found not to be significantly deteriorated in any of the utility cases that are studied under an assumption of control rod loss of neutron absorbing material as a potential cause for anomalous detector readings. In order to provoke a significant effect, a section of the top fifth of the control rod must have lost neutron absorbing material next to a control rod position with already low shutdown margin.</p>
----------------------------------------------------------------------
In diva2:938784 
abstract is: 
<p>The main objective of this master thesis was to construct an automatic method for calibrating a projector to display images on a curved screen without the images looking deformed from a certain intended viewing position. Since the method was thought to be used in a flight simulator, where the intended viewing position has an occluded view of the screen, the method needed to be able to handle these occlusions in some way, and the proposed solution was to use two cameras for the calibration; one in the intended viewing position and one with a more clear sight of the screen.This thesis adds the multi-camera functionality to an existing algorithm for projector calibration using a single camera, which was developed in 2013. This algorithm performs well in calibrating projectors with respect to views that have a clear sight of the screen but lacks the functionality to do a calibration when its single camera cannot capture all parts of the screen from its viewing position.The algorithm developed uses point transfer between camera views to supply the camera in the viewing position with enough information to make a suitable calibration even for the regions of the screen it cannot capture itself.A program has been developed, showing that it is possible to do this projector calibration for situations where up to half of the screen is occluded from the intended viewing position, with a result that is not notably worse than when using the single camera algorithm for similar situations with clear sight of the screen. It might be possible to run the algorithm with less than half the screen visible from the viewing position, but an upper limit of how much of the screen can be occluded with an accepted result has not been found.The algorithm should be usable with any pair of cameras, and any projector, and does not assume that the cameras are stereo calibrated beforehand. However in the testing done in this thesis, camera images with resolution 640x480 have been used, and the displayed projector images have had the resolution 256x192 in the calibration.</p>

corrected abstract:
<p>The main objective of this master thesis was to construct an automatic method for calibrating a projector to display images on a curved screen without the images looking deformed from a certain intended viewing position. Since the method was thought to be used in a flight simulator, where the intended viewing position has an occluded view of the screen, the method needed to be able to handle these occlusions in some way, and the proposed solution was to use two cameras for the calibration; one in the intended viewing position and one with a more clear sight of the screen.</p><p>This thesis adds the multi-camera functionality to an existing algorithm for projector calibration using a single camera, which was developed in 2013. This algorithm performs well in calibrating projectors with respect to views that have a clear sight of the screen but lacks the functionality to do a calibration when its single camera cannot capture all parts of the screen from its viewing position.</p><p>The algorithm developed uses point transfer between camera views to supply the camera in the viewing position with enough information to make a suitable calibration even for the regions of the screen it cannot capture itself.</p><p>A program has been developed, showing that it is possible to do this projector calibration for situations where up to half of the screen is occluded from the intended viewing position, with a result that is not notably worse than when using the single camera algorithm for similar situations with clear sight of the screen. It might be possible to run the algorithm with less than half the screen visible from the viewing position, but an upper limit of how much of the screen can be occluded with an accepted result has not been found.</p><p>The algorithm should be usable with any pair of cameras, and any projector, and does not assume that the cameras are stereo calibrated beforehand. However in the testing done in this thesis, camera images with resolution 640 x 480 have been used, and the displayed projector images have had the resolution 256 x 192 in the calibration.</p>
----------------------------------------------------------------------
In diva2:902787 - missing spaces in title:
"Partial loudness assesment of windturbine sound through continuousjudgment by category-ratio scaling"
==>
"Partial loudness assesment of wind turbine sound through continuous judgment by category-ratio scaling"

abstract is: 
<p>The periodic and determinist nature of wind turbine sound is believed to be a key factor for the high degreeof audibility. The present study investigated how alterations in fluctuating frequency affect the loudnessperception and detectability of wind turbine sound with and without background noise. The instantaneousloudness was judged for wind turbine sounds of different sonic character and altered fluctuating frequenciesusing the method of continuous judgment by category-ratio (CJCR) scaling. The obtained results showedlimited effects on the audibility despite altered fluctuating frequencies, rather it was found that the spectralcues dominate the loudness perception. These results was consistent for both measurements situation ofloudness and partial loudness.</p>

corrected abstract:
<p>The periodic and determinist nature of wind turbine sound is believed to be a key factor for the high degree of audibility. The present study investigated how alterations in fluctuating frequency affect the loudness perception and detectability of wind turbine sound with and without background noise. The instantaneous loudness was judged for wind turbine sounds of different sonic character and altered fluctuating frequencies using the method of continuous judgment by category-ratio (CJCR) scaling. The obtained results showed limited effects on the audibility despite altered fluctuating frequencies, rather it was found that the spectral cues dominate the loudness perception. These results was consistent for both measurements situation of loudness and partial loudness.</p>
----------------------------------------------------------------------
In diva2:892115 
abstract is: 
<p>Vehicle handling evaluation is a crucial part of the vehicle development process. The evaluation can be done in two ways, subjectively; by expert test drivers or objectively; by performing repeatable standard manoeuvres usually by steering robots. Subjective testing is resource intensive as prototypes need to be built. Objective testing is less so, as it can be performed in a virtual environment in conjunction with physical testing. In an e˙ort to reduce resources and time used in vehicle development, manufacturers are looking to objective testing to assess vehicle behaviour.Vehicle handling testing in winter strongly relies on subjective testing. This thesis aims to investigate into the usage of objective test strategy to assess vehicle handling behaviour in winter conditions. Manoeuvres and metrics are defined for summer con-ditions, but not for winter. Hence the goal was to define new or modified metrics and manoeuvres custom to winter testing.Data from an objective winter test was obtained and analysed. The manoeuvres used were constant radius (CR), frequency response (FR), sine with dwell (SWD) and throt-tle release in turn (TRIT). The manoeuvres were compared to public standards from the International Organization for Standards (ISO) and National Highway Traÿc Safety Administration (NHTSA) as well as the vehicle manufacturer standards.The data from a reference vehicle is compared to that from three configuration vehicles, one without anti-roll bar in the front, one without rear anti-roll bar and a standard. The di˙erence in the signals between reference and configuration vehicles is compared to the spread in data of the reference vehicle to determine the signal-to-noise ratio in the manoeuvres. The spread of reference data is analysed to determine the distribu-tion and to di˙erentiate between the two test days. To replicate vehicle behaviour in simulation, winter tyre models using brush and Magic Formula model equations were investigated. These were used in a bicycle and a VI-CarRealTime model. The perfor-mance of these are checked and compared. The bicycle model is used in an unscented Kalman filter, to investigate potential improvements in signal processing. The metrics obtained from the study of standards are checked for robustness in winter conditions by analysis of variance (ANOVA) methods. The procedure of selection of metrics from the ANOVA results is explained. Further, the manoeuvres are modified virtually in VI-CarRealTime, from the results of a sensitivity analysis. The di˙erence in metrics between reference and configuration vehicles is maximized.The final results of the thesis were; a test plan with modified manoeuvres and a set of robust metrics. Also containing important information to aid in the execution of the tests. The conclusions drawn were that the noise in winter testing is high, but the di˙erence between vehicles is statistically significant for some robust metrics. The metrics related to yaw rate were in general more robust. Open-loop throttle and steering control in manoeuvres should be avoided as far as possible. A bicycle model is not complex enough to represent vehicle behaviour at high slip angles. Performance increase of a UKF is not justified as to the e˙ort involved.</p>


corrected abstract:
<p>Vehicle handling evaluation is a crucial part of the vehicle development process. The evaluation can be done in two ways, subjectively; by expert test drivers or objectively; by performing repeatable standard manoeuvres usually by steering robots. Subjective testing is resource intensive as prototypes need to be built. Objective testing is less so, as it can be performed in a virtual environment in conjunction with physical testing. In an effort to reduce resources and time used in vehicle development, manufacturers are looking to objective testing to assess vehicle behaviour.</p><p>Vehicle handling testing in winter strongly relies on subjective testing. This thesis aims to investigate into the usage of objective test strategy to assess vehicle handling behaviour in winter conditions. Manoeuvres and metrics are defined for summer conditions, but not for winter. Hence the goal was to define new or modified metrics and manoeuvres custom to winter testing.</p><p>Data from an objective winter test was obtained and analysed. The manoeuvres used were constant radius (CR), frequency response (FR), sine with dwell (SWD) and throttle release in turn (TRIT). The manoeuvres were compared to public standards from the International Organization for Standards (ISO) and National Highway Traffic Safety Administration (NHTSA) as well as the vehicle manufacturer standards.</p><p>The data from a reference vehicle is compared to that from three configuration vehicles, one without anti-roll bar in the front, one without rear anti-roll bar and a standard. The difference in the signals between reference and configuration vehicles is compared to the spread in data of the reference vehicle to determine the signal-to-noise ratio in the manoeuvres. The spread of reference data is analysed to determine the distribution and to differentiate between the two test days. To replicate vehicle behaviour in simulation, winter tyre models using brush and Magic Formula model equations were investigated. These were used in a bicycle and a VI-CarRealTime model. The performance of these are checked and compared. The bicycle model is used in an unscented Kalman filter, to investigate potential improvements in signal processing. The metrics obtained from the study of standards are checked for robustness in winter conditions by analysis of variance (ANOVA) methods. The procedure of selection of metrics from the ANOVA results is explained. Further, the manoeuvres are modified virtually in VI-CarRealTime, from the results of a sensitivity analysis. The difference in metrics between reference and configuration vehicles is maximized.</p><p>The final results of the thesis were; a test plan with modified manoeuvres and a set of robust metrics. Also containing important information to aid in the execution of the tests. The conclusions drawn were that the noise in winter testing is high, but the difference between vehicles is statistically significant for some robust metrics. The metrics related to yaw rate were in general more robust. Open-loop throttle and steering control in manoeuvres should be avoided as far as possible. A bicycle model is not complex enough to represent vehicle behaviour at high slip angles. Performance increase of a UKF is not justified as to the effort involved.</p>
----------------------------------------------------------------------
In diva2:852463 
abstract is: 
<p>The two most important physiological properties to determine an individuals endurance are maximumoxygen uptake and the anaerobic threshold. These properties can be obtained with a threshold test anda max test, which literature and an interview with an expert in the field showed.</p><p>The properties would then be linked to a Matlab model developed to simulate exhaustion for anindividual on a given track. To achieve this, a threshold test and a max test were performed on twoindividuals to measure the maximum oxygen uptake and the anaerobic threshold. The tests wereperformed in Solna by Aktivitus. Afterwards the max tests were simulated in the Matlab model for thetwo individuals.</p><p>It turned out that the anaerobic threshold is directly linked to the Matlab model which means that youcan put measured values directly into the model while the maximum oxygen uptake need furtherinvestigation.</p><p>The experiments have been performed as part of a project lead by Professor Anders Eriksson fromKTH and his colleagues from Karolinska Institutet and Mittuniversitetet. Technique which is animportant factor in endurance has been overlooked. To obtain better and more accurate results moretests should be performed.</p>

corrected abstract:
<p>The two most important physiological properties to determine an individuals endurance are maximum oxygen uptake and the anaerobic threshold. These properties can be obtained with a threshold test and a max test, which literature and an interview with an expert in the field showed.</p><p>The properties would then be linked to a Matlab model developed to simulate exhaustion for an individual on a given track. To achieve this, a threshold test and a max test were performed on two individuals to measure the maximum oxygen uptake and the anaerobic threshold. The tests were performed in Solna by Aktivitus. Afterwards the max tests were simulated in the Matlab model for the two individuals.</p><p>It turned out that the anaerobic threshold is directly linked to the Matlab model which means that you can put measured values directly into the model while the maximum oxygen uptake need further investigation.</p><p>The experiments have been performed as part of a project lead by Professor Anders Eriksson from KTH and his colleagues from Karolinska Institutet and Mittuniversitetet. Technique which is an important factor in endurance has been overlooked. To obtain better and more accurate results more tests should be performed.</p>
----------------------------------------------------------------------
In diva2:813822 
abstract is: 
<p>This study aims to investigate what impact restricted Boltzmann machines (RBMs) have when combined with a convolutional neural network (CNN) used for image classification. This is an interesting area of research which combines supervised and unsupervised training of neural networks and it has not been thoroughly examined yet.</p><p>Different versions of neural networks were trained and tested using two datasets consisting of 70 000 handwrittendigits and 60 000 natural images. The starting point was aregular CNN where the first layer then was replaced by two different kinds of RBMs. To evaluate the effect of RBMs the error rates and training times were compared.</p><p>The results show that the combination of RBMs and CNNs can work if implemented right and can be used in different applications. There is still much left to investigate, since this study was limited by the available computational power.</p>

corrected abstract:
<p>This study aims to investigate what impact restricted Boltzmann machines (RBMs) have when combined with a convolutional neural network (CNN) used for image classification. This is an interesting area of research which combines supervised and unsupervised training of neural networks and it has not been thoroughly examined yet.</p><p>Different versions of neural networks were trained and tested using two datasets consisting of 70 000 handwritten digits and 60 000 natural images. The starting point was a regular CNN where the first layer then was replaced by two different kinds of RBMs. To evaluate the effect of RBMs the error rates and training times were compared.</p><p>The results show that the combination of RBMs and CNNs can work if implemented right and can be used in different applications. There is still much left to investigate, since this study was limited by the available computational power.</p>
----------------------------------------------------------------------
In diva2:650438 
abstract is: 
<p>Thevehicle model validated in this thesis is developed by BorgWarner TTS to beused for control algorithm design. The validation process included measurementson the car in both winter and summer conditions. The model is simulated withMatlab Simulink and the measurement data is used as input to the model and alsofor comparison between result and simulated output. This was done as a master thesisat Kungliga Tekniska Högskolan in Stockholm</p>

corrected abstract:
<p>The vehicle model validated in this thesis is developed by BorgWarner TTS to be used for control algorithm design. The validation process included measurements on the car in both winter and summer conditions. The model is simulated with Matlab Simulink and the measurement data is used as input to the model and also for comparison between result and simulated output. This was done as a master thesis at Kungliga Tekniska Högskolan in Stockholm.</p>
----------------------------------------------------------------------
In diva2:642321 
abstract is: 
<p>This study investigates the difficulties that may arise for students as a consequence of differences that exist in mathematics education between different educational systems. The questions asked are when and why difficulties as a consequence of differences in writing or types of student problems may arise and in which way teachers and students attend to these difficulties. The focus is thus on the written parts of mathematics education and the questions are asked within the frame of general upper secondary education.In order to answer these questions, a sociocultural perspective based mainly on Sfard’s (2006) participationist theories together with Vygotsky’s (1999) general theories on development and learning is used. An important analytic tool is the discourse concept which in this context means a type of communication that brings some people together while excluding others from the communication. From this perspective the different rules that govern the way student problems are formulated and the ways of writing that will be accepted, may be described as parts of a specific discourse in the mathematics education rather than universally accepted truths.The study has been based on a qualitative research method and has included observations of lessons and semi-structured interviews with teachers and students. In order to give a global picture, four schools have participated in the study and one week has been spent at each school. The selection has consisted of one international school with IB education, two schools abroad (from different countries) and one Swedish upper secondary school.The study shows that difficulties occur for students and that these are mainly due to differences in the mathematics education between the different educational systems. Partly, the student problems have proved to differ in crucial ways and therefore it is difficult for students to interpret and find solutions to unknown student problems. Besides, the mathematical writing differs in many fields such as text editing, use of mathematical notation and algorithms, and this also causes difficulties. In order to overcome these difficulties, teachers and students use methods which aim at creating habits for the new way of working by showing the practices to the students who then repeat them on their own accord.Apart from the direct answers to the research questions, another great problem for students who start studying mathematics in a new educational system appeared to be lacking basic knowledge, which means that they lack certain essential knowledge and calculation proficiencies that are necessary prerequisites. This problem is specifically preponderant for students with Swedish educational background. Furthermore, it appears that the teachers at the international schools are much conscious of the fact that there exist differences in mathematics education and in general they consider it inspiring to be able to learn more from the knowledge of their students.By interpreting the differences in the mathematics education as representative for different discourses, it has been possible to understand that the difficulties that arise are due to the fact that the student has not yet individualized the new discourse. This means that the student is only able to carry out the discursive routines collectively, with aid from the teacher or the group, which explains the routine creating practices used by teachers and students. When the student has worked within the new discourse it will be individualized and the student will be able to work within the discourse in a deliberate and independent way. For the education, thisimplies that the students will first be able to perform certain routines within the discourse before a comprehension of the rules that govern them will be achieved.Since the problem of lacking basic knowledge is due to a total lack of the students’ necessary prerequisites, it is possible to interpret this problem with help of Vygotsky’s (1999) reasoning. The knowledge assumed by the new discourse does not lie within the zone of proximal development and therefore the conditions for learning are not fulfilled. The positive and reflective attitude of the teachers with respect to differences in the mathematics education differs in a decisive sense from the literature and appears to influence the possibilities to overcome the difficulties in a positive way.</p>

corrected abstract:
<p>This study investigates the difficulties that may arise for students as a consequence of differences that exist in mathematics education between different educational systems. The questions asked are when and why difficulties as a consequence of differences in writing or types of student problems may arise and in which way teachers and students attend to these difficulties. The focus is thus on the written parts of mathematics education and the questions are asked within the frame of general upper secondary education.</p><p>In order to answer these questions, a sociocultural perspective based mainly on Sfard’s (2006) participationist theories together with Vygotsky’s (1999) general theories on development and learning is used. An important analytic tool is the discourse concept which in this context means a type of communication that brings some people together while excluding others from the communication. From this perspective the different rules that govern the way student problems are formulated and the ways of writing that will be accepted, may be described as parts of a specific discourse in the mathematics education rather than universally accepted truths.</p><p>The study has been based on a qualitative research method and has included observations of lessons and semi-structured interviews with teachers and students. In order to give a global picture, four schools have participated in the study and one week has been spent at each school. The selection has consisted of one international school with IB education, two schools abroad (from different countries) and one Swedish upper secondary school.</p><p>The study shows that difficulties occur for students and that these are mainly due to differences in the mathematics education between the different educational systems. Partly, the student problems have proved to differ in crucial ways and therefore it is difficult for students to interpret and find solutions to unknown student problems. Besides, the mathematical writing differs in many fields such as text editing, use of mathematical notation and algorithms, and this also causes difficulties. In order to overcome these difficulties, teachers and students use methods which aim at creating habits for the new way of working by showing the practices to the students who then repeat them on their own accord.</p><p>Apart from the direct answers to the research questions, another great problem for students who start studying mathematics in a new educational system appeared to be lacking basic knowledge, which means that they lack certain essential knowledge and calculation proficiencies that are necessary prerequisites. This problem is specifically preponderant for students with Swedish educational background. Furthermore, it appears that the teachers at the international schools are much conscious of the fact that there exist differences in mathematics education and in general they consider it inspiring to be able to learn more from the knowledge of their students.</p><p>By interpreting the differences in the mathematics education as representative for different discourses, it has been possible to understand that the difficulties that arise are due to the fact that the student has not yet individualized the new discourse. This means that the student is only able to carry out the discursive routines collectively, with aid from the teacher or the group, which explains the routine creating practices used by teachers and students. When the student has worked within the new discourse it will be individualized and the student will be able to work within the discourse in a deliberate and independent way. For the education, this implies that the students will first be able to perform certain routines within the discourse before a comprehension of the rules that govern them will be achieved.</p><p>Since the problem of lacking basic knowledge is due to a total lack of the students’ necessary prerequisites, it is possible to interpret this problem with help of Vygotsky’s (1999) reasoning. The knowledge assumed by the new discourse does not lie within the zone of proximal development and therefore the conditions for learning are not fulfilled. The positive and reflective attitude of the teachers with respect to differences in the mathematics education differs in a decisive sense from the literature and appears to influence the possibilities to overcome the difficulties in a positive way.</p>
----------------------------------------------------------------------
In diva2:630453 
abstract is: 
<p>For a commutative ring k,we consider free k-modules E, endowing them with k[x_1,...,x_m]-module structuresthrough a ring homomorphism k[x_1,...,x_m] -&gt; End_Z(E). These structures arethen inspected by encoding the actions of the unknowns x_i in matricesX_1,...,X_m. We further introduce the concepts of lifts and formal smoothnessfor functors, and define the Quot_{F/A/k}^n functor acting on the category ofk-algebras, taking some k-algebra B to the set of quotients of the form (F ⊗_k B)/N, which are locallyfree as B-modules. Lastly, we find concrete examples of modules showing thatthe functors Hilb_{k[x,y,z]/k}^4 and Quot_{⊕^2 k[x,y]/k[x,y]/k}^2 are not formally smooth</p>

corrected abstract:

<p>For a commutative ring 𝑘 we consider free 𝑘-modules 𝐸, endowing them with 𝑘[x<sub>1</sub>,...,x<sub>𝑚</sub>]-module structures through a ring homomorphism 𝑘[x<sub>1</sub>,...,x<sub>m</sub>] &rightarrow; End<sub>&Zopf;</sub>(𝐸). These structures are then inspected by encoding the actions of the unknowns x<sub>i</sub> in matrices 𝑋<sub>1</sub>,...,𝑋<sub>𝑚</sub>. We further introduce the concepts of lifts and formal smoothness for functors, and define the Quot<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>𝑛</sup><sub>𝐹/𝐴/𝑘</sub></span></span> functor acting on the category of 𝑘-algebras, taking some 𝑘-algebra 𝐵 to the set of quotients of the form (𝐹 ⊗<sub>𝑘</sub> 𝐵)/𝑁, which are locally free as 𝐵-modules. Lastly, we find concrete examples of modules showing that the functors Hilb<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>4</sup><sub>𝑘[x,𝑦,𝑧]/𝑘</sub></span></span> and Quot<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>2</sup><sub>⊕<sup>2</sup> 𝑘[x,𝑦]/𝑘[x,𝑦]/𝑘</sub></span></span> are not formally smooth.</p>

Note: The equations are not optimal - but it will have to wait for MathJax support for LaTeX equations.
----------------------------------------------------------------------
In diva2:624029 
abstract is: 
<p>A regression-based method is presented in order toregenerate missing data points in stock return time series. The method usesonly complete time series of assets in optimal portfolios, in which the returnsof the underlying tend to correlate inadequately with each other. The studyshows that the method is able to replicate empirical VaR-backtesting resultswhere all data are available, even when up to 90% of the time series in half ofthe assets in the portfolios have been removed.</p>


corrected abstract:
<p>A regression-based method is presented in order to regenerate missing data points in stock return time series. The method uses only complete time series of assets in optimal portfolios, in which the returns of the underlying tend to correlate inadequately with each other. The study shows that the method is able to replicate empirical VaR-backtesting results where all data are available, even when up to 90% of the time series in half of the assets in the portfolios have been removed.</p>

Note: Cannot select the text from the PDF.
----------------------------------------------------------------------
In diva2:555823 
abstract is: 
<p>This master thesis report presents a model for leveling of the workload in preventivetrain maintenance, useful to improve workshop eciency and production quality. The recurring maintenance tasks for one trainset are arranged in relation to each other,ensuring that the tasks are performed often enough and that the workload is roughlythe same at every maintenance occasion. Literature of previous work has been sought for,but with limited success. Both a linear minimax optimization program and a quadraticprogram have been formulated, and a solution to the linear problem is presented. The quadratic model was not solvable in reasonable time. Further evaluation of the manhours actually needed for the tasks is recommended before implementation.</p>

corrected abstract:
<p>This master thesis report presents a model for leveling of the workload in preventive train maintenance, useful to improve workshop efficiency and production quality. The recurring maintenance tasks for one trainset are arranged in relation to each other, ensuring that the tasks are performed often enough and that the workload is roughly the same at every maintenance occasion. Literature of previous work has been sought for, but with limited success. Both a linear minimax optimization program and a quadratic program have been formulated, and a solution to the linear problem is presented. The quadratic model was not solvable in reasonable time. Further evaluation of the man hours actually needed for the tasks is recommended before implementation.</p>
----------------------------------------------------------------------
In diva2:408835 
abstract is: 
<p>What is creativity? Why is it important? How can you stimulate pupils to becomemore creative? These are some of the questions that are studied in this thesis.The discussion is focused on programming as the medium of creativity and theresult is a setup for a course book that teaches programming at the same time asit makes the reader more creative. The results are based on literature study aswell as observations and interviews with teachers and pupils from three differentschools in Stockholm, Sweden. The study shows that gets more important everyday as more and more jobs are replaced by computers and we live in an age ofabundance. The teachers would like the pupils to be more creative but don’tknow how achieve that at the same time as they have to teach them everythingthat they are supposed to know when the course ends.</p>

corrected abstract:
<p>What is creativity? Why is it important? How can you stimulate pupils to become more creative? These are some of the questions that are studied in this thesis. The discussion is focused on programming as the medium of creativity and the result is a setup for a course book that teaches programming at the same time as it makes the reader more creative. The results are based on literature study as well as observations and interviews with teachers and pupils from three different schools in Stockholm, Sweden. The study shows that gets more important every day as more and more jobs are replaced by computers and we live in an age of abundance. The teachers would like the pupils to be more creative but don’t know how achieve that at the same time as they have to teach them everything that they are supposed to know when the course ends.</p>


Note: Does not have standard cover.
----------------------------------------------------------------------
In diva2:407819 
abstract is: 
<p>The instability mechanism of the shear-thinning and shear-thickening fluids pasta circular cylinder is studied using linear theory. The shear-dependent viscosityis modeled by the Carreau-law where the rheological parameters, the power-index and the material time constant are chosen in the range 0.4 &lt; n &lt; 1.75 and0.1 &lt; λ &lt; 100. A second order finite-difference code is used for the simulationof cylinder flow in which the Immersed Boundary Technique is implemented torepresent the cylinder surface on a Cartesian mesh. The critical Reynolds num-ber for the onset of instability is reported for a range of rheological parameters.Structural sensitivity analysis based on the idea of ”wavemaker” is performedto identify the core of the instability. Perturbation kinetic energy budget is alsoconsidered to examine the physical mechanism of the instability. The charac-teristics of base-flow: drag coefficient, size of recirculation bubble and viscositydistribution are presented to provide useful knowledge about shear-thinning ef-fect in flow past a cylinder.</p>

corrected abstract:
<p>The instability mechanism of the shear-thinning and shear-thickening fluids past a circular cylinder is studied using linear theory. The shear-dependent viscosity is modeled by the Carreau-law where the rheological parameters, the power-index and the material time constant are chosen in the range 0.4 &lt; 𝑛 &lt; 1.75 and 0.1 &lt; λ &lt; 100. A second order finite-difference code is used for the simulation of cylinder flow in which the Immersed Boundary Technique is implemented to represent the cylinder surface on a Cartesian mesh. The critical Reynolds number for the onset of instability is reported for a range of rheological parameters. Structural sensitivity analysis based on the idea of ”wavemaker” is performed to identify the core of the instability. Perturbation kinetic energy budget is also considered to examine the physical mechanism of the instability. The characteristics of base-flow: drag coefficient, size of recirculation bubble and viscosity distribution are presented to provide useful knowledge about shear-thinning effect in flow past a cylinder.</p>
----------------------------------------------------------------------
In diva2:403154 
abstract is: 
<p>We give a general introduction to particle physics, and in particular to particlephysics in extra dimensions. Furthermore, we introduce the concept of dark matter(DM) and discuss some suggestions for what it originates from. Next, we calculatethe cross-section for DM annihilation in the framework of non-minimal universalextra dimensions. This process gives mono-energetic gamma-ray lines with energyclose to the DM particle mass. The calculations are performed for two different DMcandidates, U(1)Y-gaugebosonen B1 och den laddningsneutrala SU(2)L-gaugebosonen Z1. MM-kandidaten är alltid den lättaste partikeln inom den betraktade teorin. Störst  tvärsnitt får man när Z1 tas som den lättaste partikeln, och alltså som MM-kandidat, trots att den är tyngre än vad B1 är när den är tagen som teorins lättaste partikel. Anledningen till att tvärsnittet blir större för Z1 är att den har icke fösumbara självväxelverkningarr, vilket gör att många fler Feynmandiagram bidrar.</p>

corrected abstract:
<p>We give a general introduction to particle physics, and in particular to particle physics in extra dimensions. Furthermore, we introduce the concept of dark matter (DM) and discuss some suggestions for what it originates from. Next, we calculate the cross-section for DM annihilation in the framework of non-minimal universal extra dimensions. This process gives mono-energetic gamma-ray lines with energy close to the DM particle mass. The calculations are performed for two different DM candidates, the 𝑈(1)<sub>Y</sub> gauge boson 𝐵<sup>1</sup> and the electrically neutral 𝑆𝑈(2)<sub>L</sub> gauge boson 𝑍<sup>1</sup>. The DM candidate is always the lightest particle of the theory (LKP).  When the 𝑍<sup>1</sup> is the LKP, we get a larger cross-section, even though it is heavier that the 𝐵<sup>1</sup> is when it is the LKP. The reason why the cross-section is larger for the 𝑍<sup>1</sup> is that many more Feynman diagrams contribute to this process, since 𝑍<sup>1</sup> has non-negligible self-interactions.</p>


Note that the DiVA abstract seemed to mix some of the English and Swedish abstracts.
----------------------------------------------------------------------
In diva2:402374 
abstract is: 
<p>As a small step towards their long-term vision of one day producing emission free vessels, Wallenius em-ployed, in 2009, Mårten Silvanius to carry out his master thesis for them in which he studied five different concepts to reduce the overall fuel consumption using wind powered systems. The vessel on which his study was performed is the 230 m LCTC vessel M/V Fedora. One of the concepts studied was the bow wing which is thought to generate enough force in the ship direction to profitably reduce the overall wind resistance. His calculations showed that the wing would be the preferred method of the different concepts studied since it was determined cheapest to build, had good payback, had good global drag reducing ef-fects and had a predicted performance of a reduction in fuel cost between 3-5% on a worldwide route.This thesis is conducted mainly to verify the results of Silvanius numerical study. The method chosen is to perform a fully viscous 3-D CFD study on the entire flow around the above water portion of the ship in full scale. A 3-D model is created and the wing is placed using suggestions given by Silvanius.One major limitation in this project was the computational capacity available at the time this thesis was conducted. In order to run some of the viscous grids created the grids had to be severely coarsened. This had a negative impact on the reliability on some of the results.Since it has been difficult to obtain satisfactory solutions, no work has been done to optimize the shape and position of the wing.Nevertheless, one it has been shown that the wing does in fact affect the resistance in a positive way, however nowhere near as much as predicted by Silvanius. This effect needs to be further determined through further calculations, both using CFD and also through experimental wind tunnel testing where alternatives to the wing profile should be tested, e.g. replacing the wing with a vortex generator to further delay the point of separation.</p>

corrected abstract:
<p>As a small step towards their long-term vision of one day producing emission free vessels, Wallenius employed, in 2009, Mårten Silvanius to carry out his master thesis for them in which he studied five different concepts to reduce the overall fuel consumption using wind powered systems. The vessel on which his study was performed is the 230 m LCTC vessel M/V Fedora. One of the concepts studied was the bow wing which is thought to generate enough force in the ship direction to profitably reduce the overall wind resistance. His calculations showed that the wing would be the preferred method of the different concepts studied since it was determined cheapest to build, had good payback, had good global drag reducing effects and had a predicted performance of a reduction in fuel cost between 3-5% on a worldwide route.</p><p>This thesis is conducted mainly to verify the results of Silvanius numerical study. The method chosen is to perform a fully viscous 3-D CFD study on the entire flow around the above water portion of the ship in full scale. A 3-D model is created and the wing is placed using suggestions given by Silvanius.</p><p>One major limitation in this project was the computational capacity available at the time this thesis was conducted. In order to run some of the viscous grids created the grids had to be severely coarsened. This had a negative impact on the reliability on some of the results.</p><p>Since it has been difficult to obtain satisfactory solutions, no work has been done to optimize the shape and position of the wing.</p><p>Nevertheless, one it has been shown that the wing does in fact affect the resistance in a positive way, however nowhere near as much as predicted by Silvanius. This effect needs to be further determined through further calculations, both using CFD and also through experimental wind tunnel testing where alternatives to the wing profile should be tested, e.g. replacing the wing with a vortex generator to further delay the point of separation.</p>
----------------------------------------------------------------------
In diva2:1880367 
abstract is: 
<p>In quantum mechanics it is not uncommon to find analytically solved problems involvinga degree of math too advanced for most. It is often helpful to use a numerical approachto test solutions and deepen the understanding of such problems. In order to determine the validity of this approach, it is important to examine its accuracy. An exampleof this is the Landau-Zener problem, which is the topic of this thesis. It describes atwo-state quantum mechanical system that is applicable to many real world situations.The numerical method used involves propagating the wave function by calculating thetime evolution operator for numerous time steps. The accuracy using this method wasanalysed by comparing the results with the exact solution with varying parameters. Theconclusion is that the numerical solution does converge toward the known analytical solution. However, it does this with different accuracy, depending on the system parameters.</p>

corrected abstract:
<p>In quantum mechanics it is not uncommon to find analytically solved problems involving a degree of math too advanced for most. It is often helpful to use a numerical approach to test solutions and deepen the understanding of such problems. In order to determine the validity of this approach, it is important to examine its accuracy. An example of this is the Landau-Zener problem, which is the topic of this thesis. It describes a two-state quantum mechanical system that is applicable to many real world situations. The numerical method used involves propagating the wave function by calculating the time evolution operator for numerous time steps. The accuracy using this method was analysed by comparing the results with the exact solution with varying parameters. The conclusion is that the numerical solution does converge toward the known analytical solution. However, it does this with different accuracy, depending on the system parameters.</p>
----------------------------------------------------------------------
In diva2:1871601 
abstract is: 
<p>In 2018, the shipping industry accounted for 2.89% of worldwide greenhouse gas (GHG) emissions, with carbon dioxide (CO2) being the most significant GHG affecting global warming (IMO, 2020). This research aims to introduce a method for selecting propellers at the early design stage to lower CO2 emissions from ships.Traditionally, propeller design assumes that a ship travels in a straight line at a constant speed. However, real-sea conditions involve environmental forces and ship maneuvering, requiring frequent adjustments in speed and direction. These adjustments affect the flow angle and velocity at the propeller, causing it to operate outside its design point and, as a result, reducing its efficiency.To tackle this issue, a MATLAB-based simulation program was developed. Which predicts ship maneuvering motions in 4 degrees of freedom (DOF)—surge, sway, yaw, and roll—using the MMG model. The program applies wave and wind forces, calculates ship motion responses in each iteration, and employs two controllers to manage the rudder and propeller speed, ensuring the ship stays on course and arrives on time.The optimization process is iterative, using simulation outcomes to determine propeller speed, thrust, drift angle, etc., and then selecting an optimized propeller with the program's optimization tool that is more adequate for the operational condition.Building on Trodden's (2014) work, this approach improves maneuvering simulations' accuracy by incorporating ship rolling motion, more realistic wave modeling, and more accurate hydrodynamic coefficients. This offers a closer representation of operational conditions.Case studies comparing this method with the traditional approach to propeller selection have shown the simulation program's precision and its effectiveness in improving propeller open water efficiency by 1.65% and reducing CO2 emissions by 1.47% for a Pure Car Carrier (PCC) ship. The results of the research showed a promising potential for the program to predict ship maneuvering motions in real-sea conditions and optimize the propeller.</p>

corrected abstract:
<p>In 2018, the shipping industry accounted for 2.89% of worldwide greenhouse gas (GHG) emissions, with carbon dioxide (CO<sub>2</sub>) being the most significant GHG affecting global warming according to the International Maritime Organization’s (IMO) "Fourth GHG Study" in 2020. This research aims to introduce a method for selecting propellers at the early design stage to lower CO<sub>2</sub> emissions from ships.</p><p>Traditionally, propeller design assumes that a ship travels in a straight line at a constant speed. However, real-sea conditions involve environmental forces and ship maneuvering, requiring frequent adjustments in speed and direction. These adjustments affect the flow angle and velocity at the propeller, causing it to operate outside its design point and, as a result, reducing its efficiency.</p><p>To tackle this issue, a MATLAB-based simulation program was developed. Which predicts ship maneuvering motions in 4 degrees of freedom (DOF)—surge, sway, yaw, and roll—using the Manoeuvring Mathematical Modelling Group (MMG) model. The program applies wave and wind forces, calculates ship motion responses in each iteration, and employs two controllers to manage the rudder and propeller speed, ensuring the ship stays on course and arrives on time.</p><p>The optimization process is iterative, using simulation outcomes to determine propeller speed, thrust, drift angle, etc., and then selecting an optimized propeller with the program's optimization tool that is more adequate for the operational condition.</p><p>Building on the approach detailed in "Optimal Propeller Selection When Accounting for a Ship’s Manoeuvring Response Due to Environmental Loading" by David George Trodden, this approach improves maneuvering simulations' accuracy by incorporating ship rolling motion, more realistic wave modeling, and more accurate hydrodynamic coefficients. This offers a closer representation of operational conditions.</p><p>Case studies comparing this method with the traditional approach to propeller selection have shown the simulation program's precision and its effectiveness in improving propeller open water efficiency by 1.66% and reducing CO<sub>2</sub> emissions by 1.47% for a Pure Car Carrier (PCC) ship. The results of the research showed a promising potential for the program to predict ship maneuvering motions in real-sea conditions and optimize the propeller.</p>

Note that there were wording differences between the version in DiVA and the thesis.
----------------------------------------------------------------------
'diva2:1867352' no longer present
----------------------------------------------------------------------
In diva2:1867288 
abstract is: 
<p>Nonequilibrium Statistical Mechanics focuses on the branch of physics that studies large systems that are out of equilibrium. These systems are inherently difficult to model due to the complex paths available through state space depending on initial configurations and external force fields. Langevin dynamics is a stochastic differential equation used by physicists to model and capture these dynamics. By sampling individual particles from any given distribution, and allowing these particles to follow a Langevin path as t → ∞, we can demonstrate that the Boltzmann distribution is recovered for any stationary field. Thus Langevin dynamics can be seen as a path through state space that leads to equilibrium.In this master’s thesis, we use these properties of Langevin dynamics in a machine learning setting to perform time-series forecasting of large systems. Langevin dynamics gives us the ability to not only capture expectation values from datasets, but instead models an approximate distribution representing the data. This leads us to believe that using diffusion models inspired by Langevin dynamics will allow us to model complex multivariate datasets with temporal, spatial, local &amp; global correlations.Modeling the electric grid is an inherently challenging problem due to the compli- cated and subtle correlations that exist with for example historical usage, weather, and economic effects. Thanks to this, we chose to forecast the American power grid using a diffusion based machine learning model. In this thesis, a diffusion based architecture has been developed to forecast this problem and includes several components such as recur- rent neural networks, graph attention networks, and diffusion processes in conjunction.Our architecture has demonstrated superior performance, outperforming current state- of-the-art methods, including gradient boosting decision tree models. Detailed perfor- mance comparisons between our diffusion model and XGBoost have shown that our diffusion model consistently outperforms it.Looking ahead, our work suggests many potential areas of improvement to further advance the performance of diffusion based models in time-series forecasting. Examples of this include incorporating physics inspired conservation laws into the model, and im- proved sampling methods. We hope that these methods will be applied in industries that could benefit from more advanced forecasting, such as the increasingly renewable-based power grids.</p>

corrected abstract:
<p>Nonequilibrium Statistical Mechanics focuses on the branch of physics that studies large systems that are out of equilibrium. These systems are inherently difficult to model due to the complex paths available through state space depending on initial configurations and external force fields. Langevin dynamics is a stochastic differential equation used by physicists to model and capture these dynamics. By sampling individual particles from any given distribution, and allowing these particles to follow a Langevin path as 𝑡 → ∞, we can demonstrate that the Boltzmann distribution is recovered for any stationary field. Thus Langevin dynamics can be seen as a path through state space that leads to equilibrium.</p><p>In this master’s thesis, we use these properties of Langevin dynamics in a machine learning setting to perform time-series forecasting of large systems. Langevin dynamics gives us the ability to not only capture expectation values from datasets, but instead models an approximate distribution representing the data. This leads us to believe that using diffusion models inspired by Langevin dynamics will allow us to model complex multivariate datasets with temporal, spatial, local &amp; global correlations.</p><p>Modeling the electric grid is an inherently challenging problem due to the complicated and subtle correlations that exist with for example historical usage, weather, and economic effects. Thanks to this, we chose to forecast the American power grid using a diffusion based machine learning model. In this thesis, a diffusion based architecture has been developed to forecast this problem and includes several components such as recurrent neural networks, graph attention networks, and diffusion processes in conjunction.</p><p>Our architecture has demonstrated superior performance, outperforming current state-of-the-art methods, including gradient boosting decision tree models. Detailed performance comparisons between our diffusion model and XGBoost have shown that our diffusion model consistently outperforms it.</p><p>Looking ahead, our work suggests many potential areas of improvement to further advance the performance of diffusion based models in time-series forecasting. Examples of this include incorporating physics inspired conservation laws into the model, and improved sampling methods. We hope that these methods will be applied in industries that could benefit from more advanced forecasting, such as the increasingly renewable-based power grids.</p>
----------------------------------------------------------------------
In diva2:1857264 
abstract is: 
<p>The present work introduces a new strategy for performing an exact tuning of the parameters governing the Von Karman - Pao isotropic energy spectrum for generating a stochastic turbulent velocity field. The tuning is carried out by numerically solving a single-variable equation defined through the confluent hypergeometric function of second kind. This procedure effectively captures the entire turbulent energy content for each point of the domain, yielding distinct and optimized outcomes based on the local turbulence Reynolds number. This stands in contrast to the approach commonly adopted in the same applications, where the spectrum parameters are calculated under the assumption of infinite Reynolds number at each point, assumption not always valid and therefore potentially leading to significant errors, in particular close to physical walls. Additionally, the strategy used to select the parameters defining the numerical integration of the spectrum is innovative as well, enabling an optimal balance between error committed and computational cost.The synthesis of the turbulent velocity field represents the most peculiar step of the Stochastic Noise Generation and Radiation (SNGR) method, in which, starting from the results provided by a RANS or URANS simulation, the aeroacoustic sources are reconstructed and then the radiated acoustic field is computed to estimate the farfield broadband noise. Stochastic Noise Generation (SNG) represents a lower-fidelity but much more computationally efficient strategy compared to directly obtaining the acoustic sources through a LES simulation or a hybrid RANS-LES simulation such as DES/DDES.The first part of the present work illustrates the significant benefits brought by the overall tuning procedure for the cases of a round jet or a 2-D NACA0012 airfoil at AOA = 8°. Subsequently, a direct comparison between the temporal history of the velocity field generated through SNG post-processing (applied both to a RANS and a URANS) and that obtained via a DDES is presented for a 3-D NACA0021 airfoil at AOA = 17°. Despite having a good correspondence of the signals from a statistical point of view, the frequency behavior of those obtained with SNG remains the most limiting aspect of this method in its current version.The simulations have been performed with the open-source software SU2.</p>

corrected abstract:
<p>The present work introduces a new strategy for performing an exact tuning of the parameters governing the Von Karman - Pao isotropic energy spectrum for generating a stochastic turbulent velocity field. The tuning is carried out by numerically solving a single-variable equation defined through the confluent hypergeometric function of second kind. This procedure effectively captures the entire turbulent energy content for each point of the domain, yielding distinct and optimized outcomes based on the local turbulence Reynolds number. This stands in contrast to the approach commonly adopted in the same applications, where the spectrum parameters are calculated under the assumption of infinite Reynolds number at each point, assumption not always valid and therefore potentially leading to significant errors, in particular close to physical walls. Additionally, the strategy used to select the parameters defining the numerical integration of the spectrum is innovative as well, enabling an optimal balance between error committed and computational cost.</p><p>The synthesis of the turbulent velocity field represents the most peculiar step of the Stochastic Noise Generation and Radiation (SNGR) method, in which, starting from the results provided by a RANS or URANS simulation, the aeroacoustic sources are reconstructed and then the radiated acoustic field is computed to estimate the farfield broadband noise. Stochastic Noise Generation (SNG) represents a lower-fidelity but much more computationally efficient strategy compared to directly obtaining the acoustic sources through a LES simulation or a hybrid RANS-LES simulation such as DES/DDES.</p><p>The first part of the present work illustrates the significant benefits brought by the overall tuning procedure for the cases of a round jet or a 2-D NACA0012 airfoil at AOA = 8°. Subsequently, a direct comparison between the temporal history of the velocity field generated through SNG post-processing (applied both to a RANS and a URANS) and that obtained via a DDES is presented for a 3-D NACA0021 airfoil at AOA = 17°. Despite having a good correspondence of the signals from a statistical point of view, the frequency behavior of those obtained with SNG remains the most limiting aspect of this method in its current version.</p><p>The simulations have been performed with the open-source software SU2.</p>

Note: When there was no space after the terminal punctuation, I have assued this was the start of a new paragraph.
----------------------------------------------------------------------
In diva2:1841282 
abstract is: 
<p>A Multibody simulation model of Indian Railways (IR) freight wagon-BCNM1 design was developed on VI-Rail software package. The main purpose of the thesis was to develop and validate the model and utilise it to forecast the running behaviour of the vehicle and contribute to the ongoing research work. The multibody model was developed using the existing drawings of BCNM1 variant wagon. The model was validated by comparing simulation results with field trial data made available by Research Design and Standards Organisation (RDSO). Using this validated model, non-linear critical speed of both empty and loaded wagon was found. Values of non-linear critical linear speed obtained from simulation are close to nonlinear critical speed for three piece bogie reported by different scholars. Utilisingthe model, the influence of longitudinal clearance in combination with new design of elastomeric pads developed by RDSO was examined. The results show that the new pad and premium pad design developed do not significantly improve non-linear critical speed and further improvement in critical speed can be achieved by further increasing the lateral and longitudinal stiffness of the elastomeric pad. Curving performance of the new designs is nearly the same as that of the existing design. The curve chosen for simulation is the standard curve used by RDSO for testing of vehicle and in practical situation curve geometry neither of existing IR network or dedicated Freight Corridor (DFC) network is worse than the curve geometry chosen for simulations. The new designs do not adversely affect derailment coefficient, carbodyaccelerations and ride indices of the vehicle.</p>

corrected abstract:
<p>A Multibody simulation model of Indian Railways (IR) freight wagon-BCNM1 design was developed on VI-Rail software package. The main purpose of the thesis was to develop and validate the model and utilise it to forecast the running behaviour of the vehicle and contribute to the ongoing research work. The multibody model was developed using the existing drawings of BCNM1 variant wagon. The model was validated by comparing simulation results with field trial data made available by Research Design and Standards Organisation (RDSO). Using this validated model, non-linear critical speed of both empty and loaded wagon was found. Values of non-linear critical linear speed obtained from simulation are close to nonlinear critical speed for three piece bogie reported by different scholars. Utilising the model, the influence of longitudinal clearance in combination with new design of elastomeric pads developed by RDSO was examined. The results show that the new pad and premium pad design developed do not significantly improve non-linear critical speed and further improvement in critical speed can be achieved by further increasing the lateral and longitudinal stiffness of the elastomeric pad. Curving performance of the new designs is nearly the same as that of the existing design. The curve chosen for simulation is the standard curve used by RDSO for testing of vehicle and in practical situation curve geometry neither of existing IR network or dedicated Freight Corridor (DFC) network is worse than the curve geometry chosen for simulations. The new designs do not adversely affect derailment coefficient, carbody accelerations and ride indices of the vehicle.</p>
----------------------------------------------------------------------
In diva2:1830368 
abstract is: 
<p>Today, a large amount of time series data is being produced from a variety of different devices such as smart speakers, cell phones and vehicles. This data can be used to make inferences and predictions. Neural network based methods are among one of the most popular ways to model time series data. The field of neural networks is constantly expanding and new methods and model variants are frequently introduced. In 2018, a new family of neural networks was introduced. Namely, Neural Ordinary Differential Equations (Neural ODEs). Neural ODEs have shown great potential in modelling the dynamics of temporal data. Here we present an investigation into using Neural Ordinary Differential Equations for anomaly detection. We tested two model variants, LSTM-ODE and latent-ODE. The former model utilises a neural ODE to model the continuous-time hidden state in between observations of an LSTM model, the latter is a variational autoencoder that uses the LSTM-ODE as encoding and a Neural ODE as decoding. Both models are suited for modelling sparsely and irregularly sampled time series data. Here, we test their ability to detect anomalies on various sparsity and irregularity ofthe data. The models are compared to a Gaussian mixture model, a vanilla LSTM model and an LSTM variational autoencoder. Experimental results using the Human Activity Recognition dataset showed that the Neural ODEbased models obtained a better ability to detect anomalies compared to their LSTM based counterparts. However, the computational training cost of the Neural ODE models were considerably higher than for the models that onlyutilise the LSTM architecture. The Neural ODE based methods were also more memory consuming than their LSTM counterparts.</p>

corrected abstract:
<p>Today, a large amount of time series data is being produced from a variety of different devices such as smart speakers, cell phones and vehicles. This data can be used to make inferences and predictions. Neural network based methods are among one of the most popular ways to model time series data. The field of neural networks is constantly expanding and new methods and model variants are frequently introduced. In 2018, a new family of neural networks was introduced. Namely, Neural Ordinary Differential Equations (Neural ODEs). Neural ODEs have shown great potential in modelling the dynamics of temporal data. Here we present an investigation into using Neural Ordinary Differential Equations for anomaly detection. We tested two model variants, LSTM-ODE and latent-ODE. The former model utilises a neural ODE to model the continuous-time hidden state in between observations of an LSTM model, the latter is a variational autoencoder that uses the LSTM-ODE as encoding and a Neural ODE as decoding. Both models are suited for modelling sparsely and irregularly sampled time series data. Here, we test their ability to detect anomalies on various sparsity and irregularity of the data. The models are compared to a Gaussian mixture model, a vanilla LSTM model and an LSTM variational autoencoder. Experimental results using the Human Activity Recognition dataset showed that the Neural ODE based models obtained a better ability to detect anomalies compared to their LSTM based counterparts. However, the computational training cost of the Neural ODE models were considerably higher than for the models that only utilise the LSTM architecture. The Neural ODE based methods were also more memory consuming than their LSTM counterparts.</p>
----------------------------------------------------------------------
In diva2:1820981 
abstract is: 
<p>In this thesis, micro computed tomography (μ−CT) scans of a bio-glass scaffold produced by the robocasting technique was used to create finite element method (FEM) models with the purpose of determining its mechanical strength. Prior to this, a Matlab script was used to create several simplified geometries of the scaffold in an effort to determine the importance of scaffold design parameters (such as the fiber compenetration between two adjacent printing planes) on the strength of the scaffold. Furthermore, to assess the influence of micro-structural defects such as voids and micro-cracks that are intrinsic to the robocasting manufacturing process, the total number of voids and their respective volume was calculated using the μ-CT scan imagery and fitted to a statistical distribution. The distribution of voids was then used to create several scaffold models in Matlab with either spherical or ellipsoidal voids present. In the final two models, one scaled-down and one scaled-up FEM based on μ-CT scans were investigated.To model the crack initiation, propagation and final failure, the phase-field method was used. The method was implemented by the use of a publicly available Fortran user subroutine and was edited to account for asymmetric tension/compression energy degradation. The resulting strength of the produced models have been presented as non-dimensional values. The finite element analysis (FEA) of the Matlab produced scaffolds showed that the fiber shifting between two adjacent layers, porosity, and voids of ellipsoidal shape that were perpendicular to the loading direction had the highest effect on the strength of the scaffold. The resulting normalized strength values obtained from the μ-CT models was partially validated through a comparison with the literature available.The different failure modes and overall architectural arrangement of cracks also showed promising results.</p>

corrected abstract:
<p>In this thesis, micro computed tomography (µ−CT) scans of a bio-glass scaffold produced by the robocasting technique was used to create finite element method (FEM) models with the purpose of determining its mechanical strength. Prior to this, a Matlab script was used to create several simplified geometries of the scaffold in an effort to determine the importance of scaffold design parameters (such as the fiber compenetration between two adjacent printing planes) on the strength of the scaffold. Furthermore, to assess the influence of micro-structural defects such as voids and micro-cracks that are intrinsic to the robocasting manufacturing process, the total number of voids and their respective volume was calculated using the µ-CT scan imagery and fitted to a statistical distribution. The distribution of voids was then used to create several scaffold models in Matlab with either spherical or ellipsoidal voids present. In the final two models, one scaled-down and one scaled-up FEM based on µ-CT scans were investigated.</p><p>To model the crack initiation, propagation and final failure, the phase-field method was used. The method was implemented by the use of a publicly available Fortran user subroutine and was edited to account for asymmetric tension/compression energy degradation. The resulting strength of the produced models have been presented as non-dimensional values. The finite element analysis (FEA) of the Matlab produced scaffolds showed that the fiber shifting between two adjacent layers, porosity, and voids of ellipsoidal shape that were perpendicular to the loading direction had the highest effect on the strength of the scaffold. The resulting normalized strength values obtained from the µ-CT models was partially validated through a comparison with the literature available. The different failure modes and overall architectural arrangement of cracks also showed promising results.</p>
----------------------------------------------------------------------
In diva2:1799823 
abstract is: 
<p>Quantum computing has emerged as a new field that may have the potential to revolutionize the landscape of information processing and computational power, although physically constructing quantum hardware has proven difficult,and quantum computers in the current Noisy Intermediate Scale Quantum (NISQ) era are error prone and limited in the number of qubits they contain.A sub-field within quantum algorithms research which holds potential for the NISQ era, and which has seen increasing activity in recent years, is quantum machine learning, where researchers apply approaches from classical machine learning to quantum computing algorithms and explore the interplay between the two. This master thesis investigates feature selection and autoencoding algorithms for quantum computers. Our review of the prior art led us to focus on contributing to three sub-problems: A) Embedded feature selection on quantum annealers, B) short depth quantum autoencoder circuits, and C)embedded compressed feature representation for quantum classifier circuits.For problem A, we demonstrate a working example by converting ridge regression to the Quadratic Unconstrained Binary Optimization (QUBO) problem formalism native to quantum annealers, and solving it on a simulated backend. For problem B we develop a novel quantum convolutional autoencoder architecture and successfully run simulation experiments to study its performance.For problem C, we choose a classifier quantum circuit ansatz based on theoretical considerations from the prior art, and experimentally study it in parallel with a classical benchmark method for the same classification task,then show a method from embedding compressed feature representation onto that quantum circuit.</p>

corrected abstract:
<p>Quantum computing has emerged as a new field that may have the potential to revolutionize the landscape of information processing and computational power, although physically constructing quantum hardware has proven difficult, and quantum computers in the current Noisy Intermediate Scale Quantum (NISQ) era are error prone and limited in the number of qubits they contain. A sub-field within quantum algorithms research which holds potential for the NISQ era, and which has seen increasing activity in recent years, is quantum machine learning, where researchers apply approaches from classical machine learning to quantum computing algorithms and explore the interplay between the two. This master thesis investigates feature selection and autoencoding algorithms for quantum computers. Our review of the prior art led us to focus on contributing to three sub-problems: A) Embedded feature selection on quantum annealers, B) short depth quantum autoencoder circuits, and C) embedded compressed feature representation for quantum classifier circuits. For problem A, we demonstrate a working example by converting ridge regression to the Quadratic Unconstrained Binary Optimization (QUBO) problem formalism native to quantum annealers, and solving it on a simulated backend. For problem B we develop a novel quantum convolutional autoencoder architecture and successfully run simulation experiments to study its performance. For problem C, we choose a classifier quantum circuit ansatz based on theoretical considerations from the prior art, and experimentally study it in parallel with a classical benchmark method for the same classification task, then show a method from embedding compressed feature representation onto that quantum circuit.</p>
----------------------------------------------------------------------
In diva2:1781523 
abstract is: 
<p>This thesis aims to explore the implementation of a temporal convolution network (TCN)on gait event detection and gait phase detection. The performance of the model is eval-uated based on whether the task is gait event detection or gait phase detection. For gaitphase detection the TCN models performance is evaluated by comparing its results to theones of a simpler ML structure, namely a support vector machine (SVM).The TCN performance evaluation for gait phase detection is more nuanced as SVMmodels do not perform very well in these types of tasks. Therefor an approach where theperformance is evaluated using custom metrics and k-fold cross validation to check howconsistent the results are when training and testing data change. The effects of windowsliding and low pass filtering of data are also explored for gait event detection purposes.</p>

corrected abstract:
<p>This thesis aims to explore the implementation of a temporal convolution network (TCN) on gait event detection and gait phase detection. The performance of the model is evaluated based on whether the task is gait event detection or gait phase detection. For gait phase detection the TCN models performance is evaluated by comparing its results to the ones of a simpler ML structure, namely a support vector machine (SVM).</p><p>The TCN performance evaluation for gait phase detection is more nuanced as SVM models do not perform very well in these types of tasks. Therefor an approach where the performance is evaluated using custom metrics and k-fold cross validation to check how consistent the results are when training and testing data change. The effects of window sliding and low pass filtering of data are also explored for gait event detection purposes.</p>
----------------------------------------------------------------------
In diva2:1781501 
abstract is: 
<p>In this report, we study the enumeration of spanning trees in graphs, using two methods withinlinear algebra, Kirchhoff’s Matrix Tree Theorem and an alternative method, also referred to asLemma 1, derived by S. Klee and M.T Stamps in [KS20].</p><p>Along with introducing preliminary tools from linear algebra, we also study the Laplace matrix ofa graph and use its properties in the two methods to derive combinatorical expressions on spanningtree enumeration of different graph families. We discuss properties of the Laplace matrix obtainedfrom different graph structures, and determine which method is more suitable in each case, withrespect to linear algebra. Specifically, complete graphs, Ferrers graphs and Windmill graphs areconsidered.</p><p> </p>

corrected abstract:
<p>In this report, we study the enumeration of spanning trees in graphs, using two methods within linear algebra, Kirchhoff’s Matrix Tree Theorem and an alternative method, also referred to as Lemma 1, derived by S. Klee and M.T Stamps in [KS20].</p><p>Along with introducing preliminary tools from linear algebra, we also study the Laplace matrix of a graph and use its properties in the two methods to derive combinatorical expressions on spanning tree enumeration of different graph families. We discuss properties of the Laplace matrix obtained from different graph structures, and determine which method is more suitable in each case, with respect to linear algebra. Specifically, complete graphs, Ferrers graphs and Windmill graphs are considered.</p>
----------------------------------------------------------------------
In diva2:1781241 
abstract is: 
<p>The aim of this report is to study how machine learning can be used to reconstruct 2 dimensional computed tomography images from limited angle data. This could be used in a variety of applications where either the space or timeavailable for the CT scan limits the acquired data.In this study, three different types of models are considered. The first model uses filtered back projection (FBP) with a single learned filter, while the second uses a combination of multiple FBP:s with learned filters. The last model instead uses an FNO (Fourieer Neural Operator) layer to both inpaint and filter the limited angle data followed by a backprojection layer. The quality of the reconstructions are assessed both visually and statistically, using PSNR and SSIM measures.The results of this study show that while an FBP-based model using one or more trainable filter(s) can achieve better reconstructions than ones using an analytical Ram-Lak filter, their reconstructions still fail for small angle spans. Better results in the limited angle case can be achieved using the FNO-basedmodel.</p>

corrected abstract:
<p>The aim of this report is to study how machine learning can be used to reconstruct 2 dimensional computed tomography images from limited angle data.</p><p>This could be used in a variety of applications where either the space or time available for the CT scan limits the acquired data. In this study, three different types of models are considered. The first model uses filtered back projection (FBP) with a single learned filter, while the second uses a combination of multiple FBP:s with learned filters. The last model instead uses an FNO (Fourieer Neural Operator) layer to both inpaint and filter the limited angle data followed by a backprojection layer. The quality of the reconstructions are assessed both visually and statistically, using PSNR and SSIM measures.</p><p>The results of this study show that while an FBP-based model using one or more trainable filter(s) can achieve better reconstructions than ones using an analytical Ram-Lak filter, their reconstructions still fail for small angle spans. Better results in the limited angle case can be achieved using the FNO-based model.</p>

Note there are some errors in the original, such as the use of "FBP:s" and the incorrect spelling "Fourieer".
----------------------------------------------------------------------
In diva2:1779432 
abstract is: 
<p>Alan Turing explored how patterns emerged in embryos. The modelsexplained by Turing describe how two morphogens interact with eachother and develop certain patterns though different conditions. Usingthe differential equation expressed by Turing gives a lot of freedom inthe choice of variables depending on how the variables are chosen, whichwill be explored and simulated in this thesis. Later, a systems theoreticformulation will be used to explore the stability and controllability ofthe homogenous diffusion problem and Turing’s Reaction and Diffusionproblem. It will be found that under the assumptions made in this thesisboth models will be stable and controllable.</p>

corrected abstract:
<p>Alan Turing explored how patterns emerged in embryos. The models explained by Turing describe how two morphogens interact with each other and develop certain patterns though different conditions. Using the differential equation expressed by Turing gives a lot of freedom in the choice of variables depending on how the variables are chosen, which will be explored and simulated in this thesis. Later, a systems theoretic formulation will be used to explore the stability and controllability of the homogenous diffusion problem and Turing’s Reaction and Diffusion problem. It will be found that under the assumptions made in this thesis both models will be stable and controllable.</p>
----------------------------------------------------------------------
In diva2:1778618 
abstract is: 
<p>The convolution operation is a powerful tool which is widely used in many disciplines.Lately is has seen much use in the area of computer vision, particularly with convolutionalneural networks. For these use cases, convolutions need to be run repeatedly many timeswhich necessitates specialized hardware. Our work empirically investigates the efficiencyof some of the most prominent convolution methods used, such as the Fast FourierTransform and the Winograd method, and compares these to a baseline convolutionimplementation. These comparisons are made in both one and two dimensions, and forseveral different floating point data types.</p><p> </p>

corrected abstract:
<p>The convolution operation is a powerful tool which is widely used in many disciplines. Lately is has seen much use in the area of computer vision, particularly with convolutional neural networks. For these use cases, convolutions need to be run repeatedly many times which necessitates specialized hardware. Our work empirically investigates the efficiency of some of the most prominent convolution methods used, such as the Fast Fourier Transform and the Winograd method, and compares these to a baseline convolution implementation. These comparisons are made in both one and two dimensions, and for several different floating point data types.</p>
----------------------------------------------------------------------
In diva2:1776740 
abstract is: 
<p>In this paper, we present a study of elliptic curves, focusing on theirunderlying mathematical concepts, properties and applications in numbertheory. We begin by introducing elliptic curves and their unique features,discussing their algebraic structure, and exploring their group law, pro-viding examples and geometric interpretations. The core of our studyfocuses on the Elliptic Curve Method (ECM) for integer factorization.We present the motivation behind ECM and compare it to Pollard’s (p-1) method. A discussion on pseudocurves and the choice of an ellipticcurve and bound B is provided. We also address the differences betweenECM and Pollard’s (p-1) method and propose optimization techniques forECM, including the calculation of the least common multiple (LCM) ofthe first B integers using the Sieve of Eratosthenes.</p>

corrected abstract:
<p>In this paper, we present a study of elliptic curves, focusing on their underlying mathematical concepts, properties and applications in number theory. We begin by introducing elliptic curves and their unique features, discussing their algebraic structure, and exploring their group law, providing examples and geometric interpretations. The core of our study focuses on the Elliptic Curve Method (ECM) for integer factorization. We present the motivation behind ECM and compare it to Pollard’s (p-1) method. A discussion on pseudocurves and the choice of an elliptic curve and bound B is provided. We also address the differences between ECM and Pollard’s (p-1) method and propose optimization techniques for ECM, including the calculation of the least common multiple (LCM) of the first B integers using the Sieve of Eratosthenes.</p>
----------------------------------------------------------------------
In diva2:1721656  - missing space in title:
"On the emergent relative distancebetween quantum systems"
==>
"On the emergent relative distance between quantum systems"


abstract is: 
<p>In an emergent spacetime framework where relative distances between quantum systemsare determined by the mutual information between the systems, an entangled pairmust be shown to have a non-zero distance even though its mutual information is maximalby virtue of being maximally entangled. This report shows that in fact an entangledpair is only maximally entangled in some degrees of freedom such as spin, and when oneintroduces other quantum degrees of freedom the mutual information is no longer maximaland so a non-zero distance can be recovered. In light of a conjectured relationshipbetween Einstein–Rosen Bridges (ER) and entangled Einstein–Podolsky–Rosen (EPR)pairs called ER = EPR, what appeared as a wormhole forming between the two quantumsystems was an artefact of our ignorance of all quantum information associated with thesystems.</p>

corrected abstract:
<p>In an emergent spacetime framework where relative distances between quantum systems are determined by the mutual information between the systems, an entangled pair must be shown to have a non-zero distance even though its mutual information is maximal by virtue of being maximally entangled. This report shows that in fact an entangled pair is only maximally entangled in some degrees of freedom such as spin, and when one introduces other quantum degrees of freedom the mutual information is no longer maximal and so a non-zero distance can be recovered. In light of a conjectured relationship between Einstein–Rosen Bridges (ER) and entangled Einstein–Podolsky–Rosen (EPR) pairs called ER = EPR, what appeared as a wormhole forming between the two quantum systems was an artefact of our ignorance of all quantum information associated with the systems.</p>
----------------------------------------------------------------------
In diva2:1707859 
abstract is: 
<p>The transition from internal combustion engine to battery electric cars is accompanied by a shift on the NVH issues damaging the passenger comfort. The rolling noise generated by the wheels and tyres is in particular characterized by an increasing attention from OEMs and car manufacturers. Among the causes of the rolling noise are the vibrations generated at the wheel, which are transmitted to the vehicle interior through its structure. To limit these vibrations at their source, a new innovative concept has been proposed at Stellantis. This idea is based on the use of a specific type of vibration absorber known by the acronym MTMD (Multiple Tuned Mass Damper).First, the existing literature about vibrations absorbers and optimisation techniques has been reviewed. Then, initial simulations on a global model comprising the wheel and beam-like resonators have shown how this system can effectively reduce a resonant peak.Optimisation processes have then revealed an important attenuation of a wheel resonance, even when the peak frequency is shifted. This demonstrated how important the distribution of resonators resonance frequencies are and their damping ratios when designing a robust and efficient MTMD.Finally, local resonators have been designed and modelled by considering the constraints for an integration on a wheel. A satisfactory design for resonators able to vibrate at the wanted frequency has been found. The beam-like resonators used in the first global model have then been replaced by these real resonators, confirming the possible gain with this device in a vehicle.</p>

corrected abstract:
<p>The transition from internal combustion engine to battery electric cars is accompanied by a shift on the NVH issues damaging the passenger comfort. The rolling noise generated by the wheels and tyres is in particular characterized by an increasing attention from OEMs and car manufacturers.</p><p>Among the causes of the rolling noise are the vibrations generated at the wheel, which are transmitted to the vehicle interior through its structure. To limit these vibrations at their source, a new innovative concept has been proposed at Stellantis. This idea is based on the use of a specific type of vibration absorber known by the acronym MTMD (Multiple Tuned Mass Damper).</p><p>First, the existing literature about vibrations absorbers and optimisation techniques has been reviewed. Then, initial simulations on a global model comprising the wheel and beam-like resonators have shown how this system can effectively reduce a resonant peak.</p><p>Optimisation processes have then revealed an important attenuation of a wheel resonance, even when the peak frequency is shifted. This demonstrated how important the distribution of resonators resonance frequencies are and their damping ratios when designing a robust and efficient MTMD.</p><p>Finally, local resonators have been designed and modelled by considering the constraints for an integration on a wheel. A satisfactory design for resonators able to vibrate at the wanted frequency has been found. The beam-like resonators used in the first global model have then been replaced by these real resonators, confirming the possible gain with this device in a vehicle.</p>
----------------------------------------------------------------------
In diva2:1699779 
abstract is: 
<p>Bacterial chemotaxis refers to bacteria's way of moving towards more hospitable environment. The bacteria does this by taking point-wise measurements of the amount of chemical attractants/repellents in its surroundings and comparing them, using the bacteria's built in "memory" the methylation level, to determine if the current direction is good or not. One of the first models to simulate E.coli bacteria undergoing chemotaxis by solving a differential equation was created by Yuhai Tu.The subject of this paper is to study how quickly bacteria can react and adapt to changes in its environments and how that model is affected by changes of the methylation level.How quickly the bacteria can react and adapt will be studied by estimating the time constant of the system. How sensitive the model is, with regards to the methylation level, will be studied by constructing a phase plane and by doing a sensitivity analysis.The estimated time-constant comes between 0.4 and 1.4 seconds.The relative speed of reaction and adaptation comes from the bacteria using its memory to decide if the current direction is good or not. When using the model with high changes in concentration the model seems to coincide with experimental data and theoretical knowledge. With high changes in the amount of concentration, the model fits well with experimental data and theory. It has a stable state at 0.333 and has perfect adaptation. The long-term behavior of the model is not changed by small changes of the methylation level but it is sensitive with regards to uncertainty in how the methylation works. A small uncertainty in methylation process induces a lot of uncertainty in the output of the model.When working with smaller changes of concentrations, the model does not seem to fit with experimental data.The model seems too sensitive to small changes in chemical attractants, which has the effect of the bacteria never leaving the steady state. This problem is probably due to a lack of understanding of the methylation process.Overall further studies of methylation process is needed, either to strengthen our belief in the current estimation or to find a better estimation of the rate of change for the methylation. This holds especially true considering that the model doesn't coincide with experimental data if the change in concentration is too small.</p>

corrected abstract:
<p>Bacterial chemotaxis refers to bacteria's way of moving towards more hospitable environment. The bacteria does this by taking point-wise measurements of the amount of chemical attractants/repellents in its surroundings and comparing them, using the bacteria's built in "memory" the methylation level, to determine if the current direction is good or not. One of the first models to simulate E.coli bacteria undergoing chemotaxis by solving a differential equation was created by Yuhai Tu.</p><p>The subject of this paper is to study how quickly bacteria can react and adapt to changes in its environments and how that model is affected by changes of the methylation level.</p><p>How quickly the bacteria can react and adapt will be studied by estimating the time constant of the system. How sensitive the model is, with regards to the methylation level, will be studied by constructing a phase plane and by doing a sensitivity analysis.</p><p>The estimated time-constant comes between 0.4 and 1.4 seconds. The relative speed of reaction and adaptation comes from the bacteria using its memory to decide if the current direction is good or not. When using the model with high changes in concentration the model seems to coincide with experimental data and theoretical knowledge.</p><p>With high changes in the amount of concentration, the model fits well with experimental data and theory. It has a stable state at 0.333 and has perfect adaptation. The long-term behavior of the model is not changed by small changes of the methylation level but it is sensitive with regards to uncertainty in how the methylation works. A small uncertainty in methylation process induces a lot of uncertainty in the output of the model.</p><p>When working with smaller changes of concentrations, the model does not seem to fit with experimental data. The model seems too sensitive to small changes in chemical attractants, which has the effect of the bacteria never leaving the steady state. This problem is probably due to a lack of understanding of the methylation process.</p><p>Overall further studies of methylation process is needed, either to strengthen our belief in the current estimation or to find a better estimation of the rate of change for the methylation. This holds especially true considering that the model doesn't coincide with experimental data if the change in concentration is too small.</p>
----------------------------------------------------------------------
In diva2:1694953 
abstract is: 
<p>3D Bioprinting is considered the most promising method to produce engineered tissues for in vitro assays in drug discovery or toxicology and in vivo therapeutic approaches, such as cell therapy or regenerative medicine. It is a key element that has strongly impacted these sectors and contributed to the development of technologies for biological additive manufacturing, to reach the goal of printing three-dimensional substitutes of tissues and entireorgans. However, many current bioprinting technologies are restricted by their inadequate speed and resolution, which lead to decreased cell viability and difficulties recreating the true nature of the native tissues.</p><p>These shortcomings have been faced in this thesis by building and implementing a stereolithographic bioprinter that combines high-speed light-sheet illumination with an acousto-optic lithography patterning technique. With this technology, it is expected to enable bioprinting in an ultra-fast andhighly precise manner. The purpose of this thesis is to perform a proof of concept of the bioprinter and evaluate the obtained resolution, dosage, andcalculated cycle time by polymerizing a line with three different light-sensitive bioinks.</p><p>It was indicated that the proof of concept was successful. Polymerized lines with resolution down to 18 µm were observed with the bioprinter, where the resolution was highly dependent on the laser power and exposure time, i.e., the dosage. The calculated cycle time indicated that the acousto-optics technology decreases the time of printing.</p>

corrected abstract:
<p>3D Bioprinting is considered the most promising method to produce engineered tissues for in vitro assays in drug discovery or toxicology and in vivo therapeutic approaches, such as cell therapy or regenerative medicine. It is a key element that has strongly impacted these sectors and contributed to the development of technologies for biological additive manufacturing, to reach the goal of printing three-dimensional substitutes of tissues and entire organs. However, many current bioprinting technologies are restricted by their inadequate speed and resolution, which lead to decreased cell viability and difficulties recreating the true nature of the native tissues.</p><p>These shortcomings have been faced in this thesis by building and implementing a stereolithographic bioprinter that combines high-speed light-sheet illumination with an acousto-optic lithography patterning technique. With this technology, it is expected to enable bioprinting in an ultra-fast and highly precise manner. The purpose of this thesis is to perform a proof of concept of the bioprinter and evaluate the obtained resolution, dosage, and calculated cycle time by polymerizing a line with three different light-sensitive bioinks.</p><p>It was indicated that the proof of concept was successful. Polymerized lines with resolution down to 18 µm were observed with the bioprinter, where the resolution was highly dependent on the laser power and exposure time, i.e., the dosage. The calculated cycle time indicated that the acousto-optics technology decreases the time of printing.</p>
----------------------------------------------------------------------
In diva2:1614091 
abstract is: 
<p>The Rail Vehicle Research group at Kungliga Tekniska Högskolan (KTH) is on the path to design and build a scaled test rig called roller rig for research and educational purposes. A roller rig is a device simulating the track with rollers on which the test subject (a wheelset, bogie or even a full vehicle) canbe placed.This thesis report is part of a bigger project involving several team members and it explores the applicability of track irregularities on a scaled roller rig by means of computer simulations. A scaled roller rig model, capable of simulating track irregularities, is generated using the multibody simulation softwareSIMPACK. Track irregularity data, represented as Power Spectral Densities (PSD), are applied to the model created. The model created and implementation of track irregularities are assessed in order to validate the modelling steps.Comparison with a reference vehicle model is carried out to verify if results obtained on the test rig are representative of a vehicle running on track, taking into account roller rig intrinsic errors. Results obtained aim to support the design of the roller rig’s mechanical components from a dynamical standpoint.</p>

corrected abstract:
<p>The Rail Vehicle Research group at Kungliga Tekniska Högskolan (KTH) is on the path to design and build a scaled test rig called roller rig for research and educational purposes. A roller rig is a device simulating the track with rollers on which the test subject (a wheelset, bogie or even a full vehicle) can be placed.</p><p>This thesis report is part of a bigger project involving several team members and it explores the applicability of track irregularities on a scaled roller rig by means of computer simulations. A scaled roller rig model, capable of simulating track irregularities, is generated using the multibody simulation software SIMPACK. Track irregularity data, represented as Power Spectral Densities (PSD), are applied to the model created. The model created and implementation of track irregularities are assessed in order to validate the modelling steps.</p><p>Comparison with a reference vehicle model is carried out to verify if results obtained on the test rig are representative of a vehicle running on track, taking into account roller rig intrinsic errors. Results obtained aim to support the design of the roller rig’s mechanical components from a dynamical standpoint.</p>
----------------------------------------------------------------------
In diva2:1609997 
abstract is: 
<p>In this thesis a regression analysis of ten independent data sets is analysed in order toestimate losses and Key Risk Indicators (KRI). Each data set contains a list of objects,impacts that each object contains and revenue stream values (RSV) to each impact.The project investigates the data and simulate yearly losses as response variables in theregression modelling. The three regressors that influence the yearly losses are numberof objects, sum of revenue streams and expected aggregated losses. Given the responsevariable from each data set a percentage scale of KRI’s is determined indicating howlarge losses each set possess.</p>

corrected abstract:
<p>In this thesis a regression analysis of ten independent data sets is analysed in order to estimate losses and Key Risk Indicators (KRI). Each data set contains a list of objects, impacts that each object contains and revenue stream values (RSV) to each impact. The project investigates the data and simulate yearly losses as response variables in the regression modelling. The three regressors that influence the yearly losses are number of objects, sum of revenue streams and expected aggregated losses. Given the response variable from each data set a percentage scale of KRI’s is determined indicating how large losses each set possess.</p>
----------------------------------------------------------------------
In diva2:1599580 
abstract is: 
<p>The purpose of this study is to compare the life-cycle cost and environmental impact of the existing fuel-based propulsion system, on public commuter ferries in Stockholm, with a battery based propulsion system. The study is divided into multiple layers. First, the operating characteristics of the route Line 80 within Stockholm’s waterborne public transportation (WPT) are collected, such as fuel consumption, propulsion power output, speed, voyage time and propulsion system configuration. Second, based on the energy demand of the route, important parameters related to the existing fuel-based propulsion system and the battery-based propulsion system are accounted for and modeled. Third, Life Cycle Assessment (LCA) and the cost assessment methods are applied to examine the effectiveness of the electrification of commuter ferries on a financial and environmental scale. With the help of the software GaBi 2020, GREET 2020, and other literature studies, the environmental impacts at the construction, use and end-of-life (EOL) phase are evaluated. There are in total 8 scenarios considered, 4 for the fuel-based and 4 for the battery-based propulsion system. The environmental performance of these 8 scenarios are discussed in terms of Globalwarmingpotential(GWP), Acidificationpotential(AP), Eutrophicationpotential(EP) and Photo-chemical ozone creation potential (POCP). Themostpollutingphaseistheusephase for all scenarios. Propulsion system powered by diesel (scenario 1) is considered as a reference for comparative analysis of 7 other scenarios. The best performing system is the one powered by batteries with the assumption of an electricity mix based on hydro, wind and nuclear power, which is scenario 7 and 8 with a net reduction of GWP by more than 98%, AP by 90%, EP by 96%, and the POCP by 96%. If we consider the current Swedish electricity mix (scenario 5 and 6), the decrease in GWP, AP, EP and POCP are 90%, 80%, 82% and 91% respectively. Alternative fuels also present promising results for GWP in comparison to diesel (with the origin of the feed-stock creating mostly negative impacts) but the contribution to other impact categories is significantly higher. With inputs from the industry and the environmental evaluation, the cost assessment compares the costs related to fuel-based and battery-based propulsion systems with different energy sources. For the battery-based system, 3 scenarios are modeled for two different types of Li-ion batteries. The vessels in the developed scenarios are charged more frequently than the existing electric vessel and the number of charging stations is varied. The costs that are included in the assessment are the initial capital cost, the cost for fuel/electricity, maintenance cost, end-of-life cost and emissions cost. When concerning all the cost categories, the battery-based system is more cost-efficient than a fuel-based system, if run on the Swedish electricity mix, due to the lower cost for electricity and emissions. The reduction of cost is more than 68% when comparing traditional diesel with battery-based systems, but the source of the electricity is very important.</p>

corrected abstract:
<p>The purpose of this study is to compare the life-cycle cost and environmental impact of the existing fuel-based propulsion system, on public commuter ferries in Stockholm, with a battery based propulsion system. The study is divided into multiple layers. First, the operating characteristics of the route Line 80 within Stockholm’s waterborne public transportation (WPT) are collected, such as fuel consumption, propulsion power output, speed, voyage time and propulsion system configuration. Second, based on the energy demand of the route, important parameters related to the existing fuel-based propulsion system and the battery-based propulsion system are accounted for and modeled. Third, Life Cycle Assessment (LCA) and the cost assessment methods are applied to examine the effectiveness of the electrification of commuter ferries on a financial and environmental scale.</p><p>With the help of the software GaBi 2020, GREET 2020, and other literature studies, the environmental impacts at the construction, use and end-of-life (EOL) phase are evaluated. There are in total 8 scenarios considered, 4 for the fuel-based and 4 for the battery-based propulsion system. The environmental performance of these 8 scenarios are discussed in terms of Global warming potential (GWP), Acidification potential (AP), Eutrophication potential (EP) and Photo-chemical ozone creation potential (POCP). The most polluting phase is the use phase for all scenarios. Propulsion system powered by diesel (scenario 1) is considered as a reference for comparative analysis of 7 other scenarios. The best performing system is the one powered by batteries with the assumption of an electricity mix based on hydro, wind and nuclear power, which is scenario 7 and 8 with a net reduction of GWP by more than 98%, AP by 90%, EP by 96%, and the POCP by 96%. If we consider the current Swedish electricity mix (scenario 5 and 6), the decrease in GWP, AP, EP and POCP are 90%, 80%, 82% and 91% respectively. Alternative fuels also present promising results for GWP in comparison to diesel (with the origin of the feed-stock creating mostly negative impacts) but the contribution to other impact categories is significantly higher.</p><p>With inputs from the industry and the environmental evaluation, the cost assessment compares the costs related to fuel-based and battery-based propulsion systems with different energy sources. For the battery-based system, 3 scenarios are modeled for two different types of Li-ion batteries. The vessels in the developed scenarios are charged more frequently than the existing electric vessel and the number of charging stations is varied. The costs that are included in the assessment are the initial capital cost, the cost for fuel/electricity, maintenance cost, end-of-life cost and emissions cost. When concerning all the cost categories, the battery-based system is more cost-efficient than a fuel-based system, if run on the Swedish electricity mix, due to the lower cost for electricity and emissions. The reduction of cost is more than 68% when comparing traditional diesel with battery-based systems, but the source of the electricity is very important.</p>
----------------------------------------------------------------------
In diva2:1583518 
abstract is: 
<p>This paper presents the work done during a Master Thesis at Airbus Helicopters company in theaccident investigation team. The focus of that internship was the hydraulic systems on AS350 helicoptersthat assist flight controls and may have an impact on accidents. It led to a "lessons learned" analysis basedon accident rates and protective/ corrective measures taken by Airbus Helicopters after hydraulics relatedaccidents. A detailed protocol for hydraulic circuit expertise has been written and a tool-case for functionalpost-crash hydraulic tests has been developed. This paper will present how helicopters fly, are controlledand how hydraulics are involved in flight controls. The above cited missions will then be explained.</p>

corrected abstract:
<p>This paper presents the work done during a Master Thesis at Airbus Helicopters company in the accident investigation team. The focus of that internship was the hydraulic systems on AS350 helicopters that assist flight controls and may have an impact on accidents. It led to a "lessons learned" analysis based on accident rates and protective/ corrective measures taken by Airbus Helicopters after hydraulics related accidents. A detailed protocol for hydraulic circuit expertise has been written and a tool-case for functional post-crash hydraulic tests has been developed. This paper will present how helicopters fly, are controlled and how hydraulics are involved in flight controls. The above cited missions will then be explained.</p>
----------------------------------------------------------------------
In diva2:1570373 
abstract is: 
<p>A direct consequence of the world becoming more digital is that the amount of available data grows, which presents great opportunities for organizations, researchers and institutions alike.However, this places a huge demand on efficient and understandable algorithms for analyzing vast datasets.</p><p>This project is centered around using one of these algorithms for identifying groups of songs in a public dataset released by Spotify in 2018. This problem is part of a larger problem class, where one wish to assign data into groups, without the preexisting knowledge of what makes the different groups special, or how many different groups there are. This is typically solved using unsupervised machine learning.</p><p>The overall goal of this project was to use spectral clustering (a specific algorithm in the unsupervised machine learning family) to assign 50 704 songs from the dataset into different categories, where each category would be made up of similar songs. The algorithm rests upon graph theory, and a large emphasis was placed upon actuallyunderstanding the mathematical foundation and motivation behind the method before the actual implementation, which is reflected in the report.</p><p>The results achieved through applying spectral clustering were one large group consisting of 40 718 songs in combination with 22 smaller groups, all larger than 100 songs, with an average size of 430 songs. The groups found were not examined in depth, but the analysis done hints that certain groups were clearly different from the data as a whole in terms of the musical features. For instance, one groupwere deemed to be 54% more likely to be acoustic than the dataset as a whole.</p><p>As a conclusion, the largest cluster was deemed to be an artefact of the fact that when a sample of songs listened to on Spotify is taken, the likelihood of these songs mainly being popular songs would be high. This would explain the homogeneity that resulted in the fact that most songs were assigned into the same group, which also resulted in the limited success of spectral clustering for this specific project.</p><p> </p>

corrected abstract:
<p>A direct consequence of the world becoming more digital is that the amount of available data grows, which presents great opportunities for organizations, researchers and institutions alike. However, this places a huge demand on efficient and understandable algorithms for analyzing vast datasets.</p><p>This project is centered around using one of these algorithms for identifying groups of songs in a public dataset released by Spotify in 2018. This problem is part of a larger problem class, where one wish to assign data into groups, without the preexisting knowledge of what makes the different groups special, or how many different groups there are. This is typically solved using unsupervised machine learning.</p><p>The overall goal of this project was to use spectral clustering (a specific algorithm in the unsupervised machine learning family) to assign 50 704 songs from the dataset into different categories, where each category would be made up of similar songs. The algorithm rests upon graph theory, and a large emphasis was placed upon actually understanding the mathematical foundation and motivation behind the method before the actual implementation, which is reflected in the report.</p><p>The results achieved through applying spectral clustering were one large group consisting of 40 718 songs in combination with 22 smaller groups, all larger than 100 songs, with an average size of 430 songs. The groups found were not examined in depth, but the analysis done hints that certain groups were clearly different from the data as a whole in terms of the musical features. For instance, one group were deemed to be 54% more likely to be acoustic than the dataset as a whole.</p><p>As a conclusion, the largest cluster was deemed to be an artefact of the fact that when a sample of songs listened to on Spotify is taken, the likelihood of these songs mainly being popular songs would be high. This would explain the homogeneity that resulted in the fact that most songs were assigned into the same group, which also resulted in the limited success of spectral clustering for this specific project.</p>
----------------------------------------------------------------------
In diva2:1547552 
abstract is: 
<p>The current global scenario is such where impact on the environment is becoming a rising concern. Global automotive manufacturers have focused more towards hybrid and electric vehicles as both more aware customers and governmental legislation have begun demanding higher emission standards. One of the many ways that Volvo Car Group approaches this trend is by mild hybridization which is by assisting the combustion engine by a small electric motor and a battery pack. A smart energy management strategy is needed in order to get the most out of the benefits that hybrid electric vehicles offer. The main objective of this strategy is to utilize the electrical energy on-board in such a manner that the overall efficiency of the hybrid powertrain becomes as high as possible. The current implementation is such that the decision for using the on-board battery is non-predictive. This results in a sub-optimal utilization of the hybrid powertrain. In this thesis, a predictive energy optimization tool is developed to maximize the utility of hybridization and the practical implementation of this tool is investigated. The optimization considers both the capacity as well as the thermal loadconstraints of the battery. The developed optimization tool uses information about the route ahead together with convex optimization to produce optimal reference trajectories of the battery states. These trajectories are used in a real-time controller to determine the battery use by controlling the adjoint states in the Equivalent Consumption Minimization Strategy equation. This optimization tool is validated and compared with the baseline controller in a simulation environment based on Simulink. When perfect information about the road ahead is known, the average reduction in fuel consumption is 0.99% relative the baseline controller. Several issues occurring in the real implementation are explored, such as the limited computational speed and the length of the route ahead that can be predicted. For this reason the information input to the optimization tool is segmented and the resulting performance is investigated. For a 30 second segmentation of the future route information, the average saving in fuel consumption is 0.13% relative to the baseline controller. It is shown that the main factor limiting the amount of savings in fuel consumption is the introduction of the thermal load constraints on the battery.</p>

corrected abstract:
<p>The current global scenario is such where impact on the environment is becoming a rising concern. Global automotive manufacturers have focused more towards hybrid and electric vehicles as both more aware customers and governmental legislation have begun demanding higher emission standards. One of the many ways that Volvo Car Group approaches this trend is by mild hybridization which is by assisting the combustion engine by a small electric motor and a battery pack. A smart energy management strategy is needed in order to get the most out of the benefits that hybrid electric vehicles offer. The main objective of this strategy is to utilize the electrical energy on-board in such a manner that the overall efficiency of the hybrid powertrain becomes as high as possible. The current implementation is such that the decision for using the on-board battery is non-predictive. This results in a sub-optimal utilization of the hybrid powertrain. In this thesis, a predictive energy optimization tool is developed to maximize the utility of hybridization and the practical implementation of this tool is investigated. The optimization considers both the capacity as well as the thermal load constraints of the battery. The developed optimization tool uses information about the route ahead together with convex optimization to produce optimal reference trajectories of the battery states. These trajectories are used in a real-time controller to determine the battery use by controlling the adjoint states in the Equivalent Consumption Minimization Strategy equation. This optimization tool is validated and compared with the baseline controller in a simulation environment based on Simulink. When perfect information about the road ahead is known, the average reduction in fuel consumption is 0.99% relative the baseline controller. Several issues occurring in the real implementation are explored, such as the limited computational speed and the length of the route ahead that can be predicted. For this reason the information input to the optimization tool is segmented and the resulting performance is investigated. For a 30 second segmentation of the future route information, the average saving in fuel consumption is 0.13% relative to the baseline controller. It is shown that the main factor limiting the amount of savings in fuel consumption is the introduction of the thermal load constraints on the battery.</p>
----------------------------------------------------------------------
In diva2:1465511 
abstract is: 
<p>The subject of the thesis was proposed by the acoustic consultancy firm Brekke &amp; Strand, which is of relevance for their additional understanding and development of services and contribution to robust solutions. The aim with this thesis is to give indicative results and to dig deeper into the relation between environmental noise and the perception of the same, specially focusing on construction related noise but also permanent noise sources that originates from infrastructure. The area of interest to evaluate the study is a central location in Stockholm, specifically around Slussen.The importance of a study within this field is vital, especially when considering possible negative health effects that can be related to the perceived noise exposure. With that background it is worth to consider the amount of healthy-life-years lost in Europe each year, which is assumed to be one million. The correlation between lost life years and noise is one of many, but it is also proven to be a catalyst when it comes to stress related or cardiovascular diseases. For instance, if one is living closer than 50 meters from a major road the risk can be four times higher to be annoyed, which in extension can be coupled to the diseases mentioned above.The practical implementation of the study is conducted with a survey and field measurements, with a psychoacoustical perspective and within the frames of a case-control study. Partially this includes to inform the participants of the study at different stages regarding construction noise. The provided information, survey and field measurements aims to be a substantial part of the evaluation regarding the perceived noise whether it is due to construction work, infrastructure or other stochastic sources.The result is presented and distinguished based on distance to noise source, differentiated by gender as well as which type of source that tends to be most annoying. The implemented ranking is displayed as the ICBEN score of each category or source which aims to measure the perceived impact. Even though the results are indicative, the conclusion yields an information dependency as well as a noise source dependency. Further on, the result yields an interesting pattern between genders where women tend to be more disturbed by construction- and other-noise, whereas men tend to be more disturbed by rail- and road-noise.</p>

corrected abstract:
<p>The subject of the thesis was proposed by the acoustic consultancy firm Brekke &amp; Strand, which is of relevance for their additional understanding and development of services and contribution to robust solutions. The aim with this thesis is to give indicative results and to dig deeper into the relation between environmental noise and the perception of the same, specially focusing on construction related noise but also permanent noise sources that originates from infrastructure. The area of interest to evaluate the study is a central location in Stockholm, specifically around Slussen.</p><p>The importance of a study within this field is vital, especially when considering possible negative health effects that can be related to the perceived noise exposure. With that background it is worth to consider the amount of healthy-life-years lost in Europe each year, which is assumed to be one million. The correlation between lost life years and noise is one of many, but it is also proven to be a catalyst when it comes to stress related or cardiovascular diseases. For instance, if one is living closer than 50 meters from a major road the risk can be four times higher to be annoyed, which in extension can be coupled to the diseases mentioned above.</p><p>The practical implementation of the study is conducted with a survey and field measurements, with a psychoacoustical perspective and within the frames of a case-control study. Partially this includes to inform the participants of the study at different stages regarding construction noise. The provided information, survey and field measurements aims to be a substantial part of the evaluation regarding the perceived noise whether it is due to construction work, infrastructure or other stochastic sources.</p><p>The result is presented and distinguished based on distance to noise source, differentiated by gender as well as which type of source that tends to be most annoying. The implemented ranking is displayed as the ICBEN score of each category or source which aims to measure the perceived impact. Even though the results are indicative, the conclusion yields an information dependency as well as a noise source dependency. Further on, the result yields an interesting pattern between genders where women tend to be more disturbed by construction- and other-noise, whereas men tend to be more disturbed by rail- and road-noise.</p>
----------------------------------------------------------------------
In diva2:1464110 
abstract is: 
<p>Iron ore export remains a major player in the Swedish economy to date, with 90% of all iron ore produced in Europe stemming from the relatively small northern country. A large amount of this ore is transported from the mines to harbours for world-wide freight on railways. On such railway is the SwedishIron-ore Line running from Kiruna to Riksgränsen, connecting to the Norwegian Ofoten line which continues to Narvik. The line has the highest permissible axle-load in Europe at 30 tonnes, which poses challenges in its own. Historically, damage on the high rail of curves have been problematic, butremedies introduced in the form of wear adapted rail profiles has brought light to a new issue. Lowrail spalling damage, caused by rolling contact fatigue (RCF) has been problematic on the line, as it reduces the life of the rails and increases maintenancecosts. It is believed that a major factor to this damage is the track gauge width. The current limit values for maintenance of the track gauge is set at 1450mm, a figure derived from empirical studies. It is therefore the wish of the infrastructure manager Trafikverket to investigate the effect the gaugewidth has on this RCF induced damage, in order to review current maintenance practices. By applying current state of the art in rail vehicle dynamics simulations and contact mechanics, the current maintenance limit has been investigated.The outcome of said investigation has yielded a foundation of support for the current maintenance limit, as it closely aligns with where damage is calculated to form at a significantly higher rate than at lower gauges.</p>

corrected abstract:
<p>Iron ore export remains a major player in the Swedish economy to date, with 90% of all iron ore produced in Europe stemming from the relatively small northern country. A large amount of this ore is transported from the mines to harbours for world-wide freight on railways. On such railway is the Swedish Iron-ore Line running from Kiruna to Riksgränsen, connecting to the Norwegian Ofoten line which continues to Narvik. The line has the highest permissible axle-load in Europe at 30 tonnes, which poses challenges in its own. Historically, damage on the high rail of curves have been problematic, but remedies introduced in the form of wear adapted rail profiles has brought light to a new issue.</p><p>Low rail spalling damage, caused by rolling contact fatigue (RCF) has been problematic on the line, as it reduces the life of the rails and increases maintenance costs. It is believed that a major factor to this damage is the track gauge width. The current limit values for maintenance of the track gauge is set at 1450mm, a figure derived from empirical studies. It is therefore the wish of the infrastructure manager Trafikverket to investigate the effect the gauge width has on this RCF induced damage, in order to review current maintenance practices. By applying current state of the art in rail vehicle dynamics simulations and contact mechanics, the current maintenance limit has been investigated.</p><p>The outcome of said investigation has yielded a foundation of support for the current maintenance limit, as it closely aligns with where damage is calculated to form at a significantly higher rate than at lower gauges.</p>
----------------------------------------------------------------------
In diva2:1380341 
abstract is: 
<p>The aim of the M.Sc thesis work is to specify a measurement method suitable for deter-mining the sound power levels and especially to quantifying the levels at the compressor blade pass frequency of a turbocharger in the new turbo performance rig located at Scania CV AB, Södertälje.Intensity and pressure based methods are widely used to determine the sound power levels. The thesis work focuses on pressure based methods since intensity measurements has a limitation in high frequencies and the intensity scanning in the rig is not allowed when the test rig is being operated. Unlike the intensity based methods the major drawback of using the pressure based methods is the influence of test environment on the sound pressure measurements. Since the room is not completely anechoic and reflections from various objects in the room may lead to wrong estimation of sound power levels. In order to understand the influence of test environment at the four chosen microphone positions several measurements were performed both in compliance with international standards and also to test assumptions on the acoustics characteristics of the room.Other than the turbocharger itself the test environment also includes three main auxiliary equipments; a cooling fan, a burner and an oil conditioning system which may contribute to the background noise at the microphone locations. A detailed study has been conducted to understand the influence from these additional sound sources during the measurements. It was concluded that the background sound do not a˙ect the measured results in the frequency range of interest. Measures were taken to isolate radiation from connecting pipes by shielding them with sound absorbing material.Based on the results from the test environment measurements and the background noise analysis the international standard ISO 3744 (Determination of sound power levels in an essentially free field over a reflecting plane) is recommended to determine the sound power levels of the turbocharger.For a constant shaft speed it was found that the highest A-weighted sound power levels were observed when the turbocharger was running close to surge followed by peak eÿciency and choke conditions on the compressor map. There is one limitation associated with the calculated sound power level and that is, the estimated sound power level is uncertain since it is based on only 4 microphone positions and thereby is not capturing the details of the compressor directivity.As future work, a setup with a large number of microphones surrounding the test speci-men is recommended which would help to determine the directivity hence improving the accuracy of the measurements. Also further studies on the sensitivity of the microphone positions, the arrangement of the auxiliary equipment in the room and the influence by the inlet and outlet pipes used in the real installation is recommended.Keywords: Turbocharger, surge, choke, peak eÿciency, compressor map, compressor blade pass frequency, sound power levels.</p>

corrected abstract:
<p>The aim of the M.Sc thesis work is to specify a measurement method suitable for determining the sound power levels and especially to quantifying the levels at the compressor blade pass frequency of a turbocharger in the new turbo performance rig located at Scania CV AB, Södertälje.</p><p>Intensity and pressure based methods are widely used to determine the sound power levels. The thesis work focuses on pressure based methods since intensity measurements has a limitation in high frequencies and the intensity scanning in the rig is not allowed when the test rig is being operated. Unlike the intensity based methods the major drawback of using the pressure based methods is the influence of test environment on the sound pressure measurements. Since the room is not completely anechoic and reflections from various objects in the room may lead to wrong estimation of sound power levels. In order to understand the influence of test environment at the four chosen microphone positions several measurements were performed both in compliance with international standards and also to test assumptions on the acoustics characteristics of the room.</p><p>Other than the turbocharger itself the test environment also includes three main auxiliary equipments; a cooling fan, a burner and an oil conditioning system which may contribute to the background noise at the microphone locations. A detailed study has been conducted to understand the influence from these additional sound sources during the measurements. It was concluded that the background sound do not affect the measured results in the frequency range of interest. Measures were taken to isolate radiation from connecting pipes by shielding them with sound absorbing material.</p><p>Based on the results from the test environment measurements and the background noise analysis the international standard ISO 3744 (Determination of sound power levels in an essentially free field over a reflecting plane) is recommended to determine the sound power levels of the turbocharger.</p><p>For a constant shaft speed it was found that the highest A-weighted sound power levels were observed when the turbocharger was running close to surge followed by peak efficiency and choke conditions on the compressor map. There is one limitation associated with the calculated sound power level and that is, the estimated sound power level is uncertain since it is based on only 4 microphone positions and thereby is not capturing the details of the compressor directivity.</p><p>As future work, a setup with a large number of microphones surrounding the test specimen is recommended which would help to determine the directivity hence improving the accuracy of the measurements. Also further studies on the sensitivity of the microphone positions, the arrangement of the auxiliary equipment in the room and the influence by the inlet and outlet pipes used in the real installation is recommended.

There is a period after "M. Sc" missing in the originasl.
----------------------------------------------------------------------
In diva2:1357296 
abstract is: 
<p>Several statistical studies have suggested that the risk of injury is significantly higher in multiple event accidents (MEAs) than in single event crashes. Improper driving in such scenarios leads to hazardous vehicle heading angles and excessive lateral deviations from thevehicle path, resulting in severe secondary crashes. In these situations, the vehicle becomes highly prone to side impacts and such impacts are more harmful to the occupants since the sides of the vehicle have less crash energy absorbing structures than the front and rear ends.Significant advancements have been made in the area of automotive safety to ensure passenger safety. Active safety systems, in particular, are becoming more advanced by the day with vehicles becoming over actuated with electric propulsion and x-by-wire systems. Keepingthat in mind, in this master thesis a post impact vehicle motion control strategy for an electric vehicle is suggested based on a hierarchical control structure which regulates the lateral deviation of the affected vehicle while maintaining safe heading angles after an impact.Sliding Mode Control (SMC) has been utilized in the higher controller which generates a virtual output used as an input for a lower controller performing torque allocation. The allocation methods were based on optimization, aimed to minimize tire utilization, and anormal force based approach. The performance of the controller was first evaluated with single track and two track model of the vehicle because of their simplicity making them easy to debug and also since they allowed for quick simulations. This was followed with evaluationwith a high fidelity vehicle model in IPG CarMaker for fine tuning the controller. It was observed that the use of SMC strategy to generate virtual yaw moment to be used in torque vectoring for controlling vehicle trajectory post impact proved to be a robust strategymanaging to control the vehicle even in cases of actuator failure. So it can be concluded that the hierarchical control structure with the higher Sliding mode controller, generating a virtual yaw moment, and a lower controller doing torque allocation using a normal force basedstrategy and an optimization approach worked as intended.</p>

corrected abstract:
<p>Several statistical studies have suggested that the risk of injury is significantly higher in multiple event accidents (MEAs) than in single event crashes. Improper driving in such scenarios leads to hazardous vehicle heading angles and excessive lateral deviations from the vehicle path, resulting in severe secondary crashes. In these situations, the vehicle becomes highly prone to side impacts and such impacts are more harmful to the occupants since the sides of the vehicle have less crash energy absorbing structures than the front and rear ends.</p><p>Significant advancements have been made in the area of automotive safety to ensure passenger safety. Active safety systems, in particular, are becoming more advanced by the day with vehicles becoming over actuated with electric propulsion and x-by-wire systems. Keeping that in mind, in this master thesis a post impact vehicle motion control strategy for an electric vehicle is suggested based on a hierarchical control structure which regulates the lateral deviation of the affected vehicle while maintaining safe heading angles after an impact.</p><p>Sliding Mode Control (SMC) has been utilized in the higher controller which generates a virtual output used as an input for a lower controller performing torque allocation. The allocation methods were based on optimization, aimed to minimize tire utilization, and a normal force based approach. The performance of the controller was first evaluated with single track and two track model of the vehicle because of their simplicity making them easy to debug and also since they allowed for quick simulations. This was followed with evaluation with a high fidelity vehicle model in IPG CarMaker for fine tuning the controller. It was observed that the use of SMC strategy to generate virtual yaw moment to be used in torque vectoring for controlling vehicle trajectory post impact proved to be a robust strategy managing to control the vehicle even in cases of actuator failure. So it can be concluded that the hierarchical control structure with the higher Sliding mode controller, generating a virtual yaw moment, and a lower controller doing torque allocation using a normal force based strategy and an optimization approach worked as intended.</p>
----------------------------------------------------------------------
In diva2:1332273 
abstract is: 
<p>Systemic Lupus Erythematosus (SLE) is an autoimmune in ammatory disease causing tissuedamage in organs and can eventually be lethal. The immunopathogenesis of SLE is complexand production of bi-specic fusion proteins (amAbs) inhibiting the activity of both IFN-and C5 is one hope to cure the disease. Due to high cost related to drug development, a work ow for quick high throughput screening of stability and solubility of possible drug candidatesis desirable. Through rigorous testing, an optimized work- ow was devised. This work- owis described, tested on 10 amAbs and has proven to be an alternative that requires 600 gprotein which is 80% less protein needed than for an unoptimized work ow. The constructsexamined in this report varies with respect to antibody subtype and fusion point, where IgG1and HC-fusion appear to have higher stability. The thermal stability study suggests that thelead candidates are Construct 2 and 7 and ought to be included in the continuous developmentof SLE amAbs.</p>

corrected abstract:
<p>Systemic Lupus Erythematosus (SLE) is an autoimmune inflammatory disease causing tissue damage in organs and can eventually be lethal. The immunopathogenesis of SLE is complex and production of bi-specic fusion proteins (amAbs) inhibiting the activity of both IFN-and C5 is one hope to cure the disease. Due to high cost related to drug development, a work flow for quick high throughput screening of stability and solubility of possible drug candidates is desirable. Through rigorous testing, an optimized workflow was devised. This workflow is described, tested on 10 amAbs and has proven to be an alternative that requires 600 g protein which is 80% less protein needed than for an unoptimized work flow. The constructs examined in this report varies with respect to antibody subtype and fusion point, where IgG1 and HC-fusion appear to have higher stability. The thermal stability study suggests that the lead candidates are Construct 2 and 7 and ought to be included in the continuous development of SLE amAbs.</p>
----------------------------------------------------------------------
In diva2:1292412 
abstract is: 
<p>The aim of this report is the investigation of a hybrid vertical take-off and landing (VTOL) tilt wing aircraft which is in development at the company Dufour Aerospace. Using a model, programmed in MATLAB® different stages of flight can be simulated and investigated. Maininvestigation area of this report is the transition between cruise and hover conditions of the aircraft.The simulation is based on the six-degree-of-freedom nonlinear equations of motion for aircraft modified for tilt wing operation. The model characteristics have been determined using various CFD programs, wind tunnel data, as well as numerical and handbook methods.The main focus of modeling lies on the static longitudinal aerodynamic coefficients, the propeller and engine coefficients as well as a propeller slipstream model. Furthermore lateral directional aerodynamic coefficients and dynamic effects and a tail blower (Notar) system are modeled.As results, aerodynamic characteristics of the wing in the propeller slipstream are shown and discussed and the transition conditions are investigated by ’trimming’ the model at equilibrium points over its speed range and analyzing the resulting power requirements.</p>

corrected abstract:
<p>The aim of this report is the investigation of a hybrid vertical take-off and landing (VTOL) tilt wing aircraft which is in development at the company Dufour Aerospace. Using a model, programmed in MATLAB® different stages of flight can be simulated and investigated. Main investigation area of this report is the transition between cruise and hover conditions of the aircraft.</p><p>The simulation is based on the six-degree-of-freedom nonlinear equations of motion for aircraft modified for tilt wing operation. The model characteristics have been determined using various CFD programs, wind tunnel data, as well as numerical and handbook methods. The main focus of modeling lies on the static longitudinal aerodynamic coefficients, the propeller and engine coefficients as well as a propeller slipstream model. Furthermore lateral directional aerodynamic coefficients and dynamic effects and a tail blower (Notar) system are modeled.</p><p>As results, aerodynamic characteristics of the wing in the propeller slipstream are shown and discussed and the transition conditions are investigated by ’trimming’ the model at equilibrium points over its speed range and analyzing the resulting power requirements.</p>
----------------------------------------------------------------------
In diva2:1292396 
abstract is: 
<p>This project have looked into the possibility to develop a small autonomous expendable air vehicle to be used with other air vehicles to fly in a group or a swarm. The requirements of this project was that the air vehicle should be able to carry a payload of 8 kg, have arange of 100 km and fly at an subsonic speed at an altitude between 0 to 3000 meter. The air vehicle should also be launched from ground and be able to be stored for 3 to 5 years. The engine was predetermined to be a HAWK 240R. The method of designing the air vehicle asdescribed by Daniel P. Raymer in his book Aircraft design: A Conceptual Approach has been used with adaptation to this project’s specifications. A comparative air vehicle study has also been made. The focus for this project has been the air vehicle design, but launch method,manufacturing and cost estimation have been investigated.The result of this thesis project is the concept of an air vehicle that weights around 30 kg with optimal cruise condition, for maximum range, at sea level altitude and at Mach 0.2. But it was decided that the ability to reach the required range faster was more important than tofly as long as possible and thus Mach 0.6 was chosen for the same altitude. A suggestion to a ground launch device has been made. The manufacturing method for the larger part of the air vehicle was chosen to be 3D printed and an estimated cost analysis for the structure andthe components has been made.The conclusion of the project is that it should be possible to design a relative small vehicle, that is intended to fly together with other vehicles, for a reasonable cost. But before this idea can be realized, more research and testing must be done.</p>

corrected abstract:
<p>This project have looked into the possibility to develop a small autonomous expendable air vehicle to be used with other air vehicles to fly in a group or a swarm. The requirements of this project was that the air vehicle should be able to carry a payload of 8 kg, have a range of 100 km and fly at an subsonic speed at an altitude between 0 to 3000 meter. The air vehicle should also be launched from ground and be able to be stored for 3 to 5 years. The engine was predetermined to be a HAWK 240R. The method of designing the air vehicle as described by Daniel P. Raymer in his book <em>Aircraft design: A Conceptual Approach</em> has been used with adaptation to this project’s specifications. A comparative air vehicle study has also been made. The focus for this project has been the air vehicle design, but launch method, manufacturing and cost estimation have been investigated.</p><p>The result of this thesis project is the concept of an air vehicle that weights around 30 kg with optimal cruise condition, for maximum range, at sea level altitude and at Mach 0.2. But it was decided that the ability to reach the required range faster was more important than to fly as long as possible and thus Mach 0.6 was chosen for the same altitude. A suggestion to a ground launch device has been made. The manufacturing method for the larger part of the air vehicle was chosen to be 3D printed and an estimated cost analysis for the structure and the components has been made.</p><p>The conclusion of the project is that it should be possible to design a relative small vehicle, that is intended to fly together with other vehicles, for a reasonable cost. But before this idea can be realized, more research and testing must be done.</p>
----------------------------------------------------------------------
In diva2:1285536 
abstract is: 
<p>This thesis work presents a model predictive control (MPC) based trajectory planner forhigh speed lane change and low speed parking scenarios of autonomous four wheel steering(4WS) vehicle. A four wheel steering vehicle has better low speed maneuverabilityand high speed stability compared with normal front wheel steering(FWS) vehicles. TheMPC optimal trajectory planner is formulated in a curvilinear coordinate frame (Frenetframe) minimizing the lateral deviation, heading error and velocity error in a kinematicdouble track model of a four wheel steering vehicle. Using the proposed trajectory planner,simulations show that a four wheel steering vehicle is able to track different type ofpath with lower lateral deviations, less heading error and shorter longitudinal distance.</p>

corrected abstract:
<p>This thesis work presents a model predictive control (MPC) based trajectory planner for high speed lane change and low speed parking scenarios of autonomous four wheel steering (4WS) vehicle. A four wheel steering vehicle has better low speed maneuverability and high speed stability compared with normal front wheel steering(FWS) vehicles. The MPC optimal trajectory planner is formulated in a curvilinear coordinate frame (Frenet frame) minimizing the lateral deviation, heading error and velocity error in a kinematic double track model of a four wheel steering vehicle. Using the proposed trajectory planner, simulations show that a four wheel steering vehicle is able to track different type of path with lower lateral deviations, less heading error and shorter longitudinal distance.</p>
----------------------------------------------------------------------
In diva2:1263450 - missing spaces in title:
"Application of the open sourcecode Nemoh for modelling ofadded mass and damping in shipmotion simulations"
==>
"Application of the open source code Nemoh for modelling of added mass and damping in ship motion simulations"

abstract is: 
<p>Two different numerical tools were considered, the first one is a seakeeping method developed by KTH Ship Dynamics Research Program. It utilizes strip theory and Lewis forms and is further addressed as SMS. The second one, Nemoh is an open source code utilizing three-dimensional panel method for calculating first order hydrodynamic coefficients in the frequency domain.A comprehensive revision of Nemoh and SMS has been performed on behalf of the KTH Ship Dynamics Research Program. The background to the revision was the high interest in accurately capturing a ships dynamic response. The aim was to explore the prospect of making use of the open source code Nemoh for modelling of added mass and damping in terms of modelling, computational efforts and ship motion prediction improvements. Further, the thesis includes a well-described method on how to operate and pre-process data for Nemoh together with a validation study including results from commercial software´s and experimental studies. An approach with the aim to capture the speed-dependency of the hydrodynamic coefficients together with further potential development of Nemoh is addressed.The validation of Nemoh is showing diverse result. For two cases, the Response Amplitude in Heave is insufficiently modelled. In consequence it’s stated that further studies are required in order to establish whether it’s the case of inadequate input from the authors’ side or lack of robustness in Nemoh that is the cause. The approach to capture speed effects in the hydrodynamic coefficients is proven to be fairly accurate and is considered to be of further use for future development of Nemoh.With respect to identified computational efforts, it’s concluded that Nemoh requires much more computational time than SMS while the accuracy of result is lower. No major improvements may therefore be achieved by substituting or implementing parts of Nemoh into SMS. Nemoh is however of use for the KTH Ship Dynamics Research Program and of other users of Lewis Method when establishing whether a hull-geometry is considered to be “too” unconventional for a two-parameter mapping technique or not. The capability to calculate the RAO in surge is also of interest for KTH Ship Dynamics Research Program since it’s not a feature in SMS.</p>

corrected abstract:
<p>Two different numerical tools were considered, the first one is a seakeeping method developed by KTH Ship Dynamics Research Program. It utilizes strip theory and Lewis forms and is further addressed as <em>SMS</em>. The second one, <em>Nemoh</em> is an open source code utilizing three-dimensional panel method for calculating first order hydrodynamic coefficients in the frequency domain.</p><p>A comprehensive revision of <em>Nemoh</em> and <em>SMS</em> has been performed on behalf of the KTH Ship Dynamics Research Program. The background to the revision was the high interest in accurately capturing a ships dynamic response. The aim was to explore the prospect of making use of the open source code <em>Nemoh</em> for modelling of added mass and damping in terms of modelling, computational efforts and ship motion prediction improvements. Further, the thesis includes a well-described method on how to operate and pre-process data for <em>Nemoh</em> together with a validation study including results from commercial software´s and experimental studies. An approach with the aim to capture the speed-dependency of the hydrodynamic coefficients together with further potential development of <em>Nemoh</em> is addressed.</p><p>The validation of <em>Nemoh</em> is showing diverse result. For two cases, the Response Amplitude in Heave is insufficiently modelled. In consequence it’s stated that further studies are required in order to establish whether it’s the case of inadequate input from the authors’ side or lack of robustness in <em>Nemoh</em> that is the cause. The approach to capture speed effects in the hydrodynamic coefficients is proven to be fairly accurate and is considered to be of further use for future development of <em>Nemoh</em>.</p><p>With respect to identified computational efforts, it’s concluded that <em>Nemoh</em> requires much more computational time than <em>SMS</em> while the accuracy of result is lower. No major improvements may therefore be achieved by substituting or implementing parts of <em>Nemoh</em> into <em>SMS</em>. <em>Nemoh</em> is however of use for the KTH Ship Dynamics Research Program and of other users of Lewis Method when establishing whether a hull-geometry is considered to be “too” unconventional for a two-parameter mapping technique or not. The capability to calculate the RAO in surge is also of interest for KTH Ship Dynamics Research Program since it’s not a feature in <em>SMS</em>.</p>
----------------------------------------------------------------------
In diva2:1263425 
abstract is: 
<p>This paper deals with anomaly detection from collected flight data for a Boeing 757-200currently in use by Icelandair. Previous work on anomaly detection using generated datahas been shown to give good results using multiple linear regression models. Using a similarapproach the paper addresses how the amount of data effects the models ability to detectanomalies. Simple regression approach is compared to a quadratic one and practical use ofmaintenance data in this context is explored. Finally different model variations are investigatedto attempt further root cause analysis of faults.</p>

corrected abstract:
<p>This paper deals with anomaly detection from collected flight data for a Boeing 757-200 currently in use by Icelandair. Previous work on anomaly detection using generated data has been shown to give good results using multiple linear regression models. Using a similar approach the paper addresses how the amount of data effects the models ability to detect anomalies. Simple regression approach is compared to a quadratic one and practical use of maintenance data in this context is explored. Finally different model variations are investigated to attempt further root cause analysis of faults.</p>
----------------------------------------------------------------------
In diva2:1263422 
abstract is: 
<p>This thesis was conducted on behalf of Butong AB, who wanted to test and develop an environmental friendly, so called green sound barrier, which combines both art and science.Different configurations of the product were proposed by the company with various filling materials, as it was predicted that the filling materials would be the main sound absorbent among all parts of the structure.The thesis work started by selecting the best of the proposed fillings which could be of interest - that is those which were expected to have high sound absorption coefficients. The selection process was based on experience, reading and advice. The main idea behind the selection process was saving cost for the company as well as effort.Impedance tube method was used for performing the measurements on samples of the green sound barriers, in order to calculate the acoustical properties of each material and every construction, as it was considerably reliable, cheap and fast to use.The measurements were done according to a combination between standards described in ISO 10534-2:1998 and ASTM E2611-09, for performing test measurements using the impedance tube.This master thesis gives an explanation of the predicted absorption characteristics of the green sound barriers including the usage of different fillings, as well as the advantages and disadvantages of using it in real life applications.</p>

corrected abstract:
<p>This thesis was conducted on behalf of Butong AB, who wanted to test and develop an environmental friendly, so called green sound barrier, which combines both art and science.</p><p>Different configurations of the product were proposed by the company with various filling materials, as it was predicted that the filling materials would be the main sound absorbent among all parts of the structure.</p><p>The thesis work started by selecting the best of the proposed fillings which could be of interest - that is those which were expected to have high sound absorption coefficients. The selection process was based on experience, reading and advice. The main idea behind the selection process was saving cost for the company as well as effort.</p><p>Impedance tube method was used for performing the measurements on samples of the green sound barriers, in order to calculate the acoustical properties of each material and every construction, as it was considerably reliable, cheap and fast to use.</p><p>The measurements were done according to a combination between standards described in ISO 10534-2:1998 and ASTM E2611-09, for performing test measurements using the impedance tube.</p><p>This master thesis gives an explanation of the predicted absorption characteristics of the green sound barriers including the usage of different fillings, as well as the advantages and disadvantages of using it in real life applications.</p>
----------------------------------------------------------------------
In diva2:1236998 
abstract is: 
<p>Motor vehicle-traﬃc accidents are a common cause of traumatic brain injuries, resulting in severe and sustaining disabilities, or even fatality. In an eﬀort to mitigate injuries related to vehicle crashes, various safety systems such as the occupant airbag has been implemented. In angled impacts, occupant interaction with the airbags can lead to head rotation, and during recent years head rotation has been emphasized as an important contributor to head injury risk. Therefore, for prediction of head injury risk in crash simulations it is important to correctly model the friction force which arises in the contact between occupants and the car interior. The aim of this thesis is therefore to study the friction within such a system. More speciﬁcally, the analysis is focusing on dummy head to airbag interaction and to correlate a three parameter friction model for this contact pair, as well as a one parameter model currently used by Volvo Car Group, with measured laboratory test data in the software LS-DYNA.A preliminary study in LS-DYNA was conducted to determine the conﬁguration of the laboratory setup consisting of a statically inﬂated customized driver airbag and a crash dummy head being launched to impact the airbag. The laboratory test data was analyzed using linear regression and Students T-test to identify the inﬂuence of parameters on the measured responses. The simulation model was then modiﬁed to represent the laboratory setup, prior to an optimization study performed to correlate simulation and laboratory test data responses. Lastly, an evaluation study was made to test whether or not the proposed friction model could improve occupant crash simulations.It was found in the thesis study that the friction force had a large eﬀect on the rotation of the head around the vertical axis (z−axis in the anatomical coordinate system of the head). The experimental data showed that the internal pressure of the airbag had little eﬀect on the response. This was likely due to the studied pressures being large enough for the airbag to be so stiﬀ that no plowing eﬀect of the dummy head moving through the airbag fabric could be seen. Furthermore, results from the optimization study indicated that the model correlation was improved when a three parameter friction model with velocity dependence was used. This implies that the friction coeﬃcient is dependent on the velocity. It was also shown that material properties aﬀecting friction behavior vary between diﬀerent crash dummy heads, as well as diﬀerent surface coating. Both dummy T-shirt fabric and grease paint resulted in signiﬁcantly lower surface friction.Due to the diﬀerence in friction for diﬀerent dummy heads, a single set of friction model parameter values that describes the friction behavior of all crash dummy heads does not exist. The study ﬁnds that when sliding is present in a contact, a three parameter model for describing the friction improves the correlation, as it can account for the velocity dependence of the friction in the contact. In contrast, when sliding is not present the one parameter and the three parameter model give similar results.Keywords: friction, velocity dependent friction coeﬃcient, ﬁnite element analysis, car crash simulation, Volvo Cars, crash test dummy head, driver airbag, LS-DYNA, laboratory testing, optimization study.</p>

corrected abstract:
<p>Motor vehicle-traffic accidents are a common cause of traumatic brain injuries, resulting in severe and sustaining disabilities, or even fatality. In an effort to mitigate injuries related to vehicle crashes, various safety systems such as the occupant airbag has been implemented. In angled impacts, occupant interaction with the airbags can lead to head rotation, and during recent years head rotation has been emphasized as an important contributor to head injury risk. Therefore, for prediction of head injury risk in crash simulations it is important to correctly model the friction force which arises in the contact between occupants and the car interior. The aim of this thesis is therefore to study the friction within such a system. More specifically, the analysis is focusing on dummy head to airbag interaction and to correlate a three parameter friction model for this contact pair, as well as a one parameter model currently used by Volvo Car Group, with measured laboratory test data in the software LS-DYNA.</p><p>A preliminary study in LS-DYNA was conducted to determine the configuration of the laboratory setup consisting of a statically inflated customized driver airbag and a crash dummy head being launched to impact the airbag. The laboratory test data was analyzed using linear regression and Students T-test to identify the influence of parameters on the measured responses. The simulation model was then modified to represent the laboratory setup, prior to an optimization study performed to correlate simulation and laboratory test data responses. Lastly, an evaluation study was made to test whether or not the proposed friction model could improve occupant crash simulations.</p><p>It was found in the thesis study that the friction force had a large effect on the rotation of the head around the vertical axis (z−axis in the anatomical coordinate system of the head). The experimental data showed that the internal pressure of the airbag had little effect on the response. This was likely due to the studied pressures being large enough for the airbag to be so stiff that no plowing effect of the dummy head moving through the airbag fabric could be seen. Furthermore, results from the optimization study indicated that the model correlation was improved when a three parameter friction model with velocity dependence was used. This implies that the friction coefficient is dependent on the velocity. It was also shown that material properties affecting friction behavior vary between different crash dummy heads, as well as different surface coating. Both dummy T-shirt fabric and grease paint resulted in significantly lower surface friction.</p><p>Due to the difference in friction for different dummy heads, a single set of friction model parameter values that describes the friction behavior of all crash dummy heads does not exist. The study finds that when sliding is present in a contact, a three parameter model for describing the friction improves the correlation, as it can account for the velocity dependence of the friction in the contact. In contrast, when sliding is not present the one parameter and the three parameter model give similar results.</p>
----------------------------------------------------------------------
In diva2:1229928 
abstract is: 
<p>The purpose of this study is to analyze the possibility to age-determine blood spots with thehelp of enzymes which blood contains, in order to see if with these results it's possible to give anestimate of how long it has been since blood left the body. The main domain of this would be thatof a crime-scene in order to give a time-frame of when a crime has been commited. The analysisconsists of a serum that has been spiked with these enzymes gets to dry over time, and how theactivity of these enzymes, which are dependent on the enzymes denaturation change. Problemswhich arose, such as drying of the blood, what blood contains, as well as a brief review of thepossibility to conjugate quantum dots to the enzymes are also reviewed.</p>

corrected abstract:
<p>The purpose of this study is to analyze the possibility to age-determine blood spots with the help of enzymes which blood contains, in order to see if with these results it's possible to give an estimate of how long it has been since blood left the body. The main domain of this would be that of a crime-scene in order to give a time-frame of when a crime has been commited. The analysis consists of a serum that has been spiked with these enzymes gets to dry over time, and how the activity of these enzymes, which are dependent on the enzymes denaturation change. Problems which arose, such as drying of the blood, what blood contains, as well as a brief review of the possibility to conjugate quantum dots to the enzymes are also reviewed.</p>
----------------------------------------------------------------------
In diva2:1219151 
abstract is: 
<p>The current levels of space debris are critical and actions are needed to preventcollisions. In this paper it is examined whether an electrostatic ion thruster canbe powerful enough to slow down the debris in a sufficient manner. Furthermore,it is looked into whether the process can be repeated for a significant numberof pieces by maneuvering between them. We conclude that the removal processseems possible although some improvements are needed. Maneuvering is costlybut despite conservative assumptions, we estimate that about 800 pieces can beremoved in one journey made by a satellite weighing ten tonnes of which nine arexenon.</p>

corrected abstract:
<p>The current levels of space debris are critical and actions are needed to prevent collisions. In this paper it is examined whether an electrostatic ion thruster can be powerful enough to slow down the debris in a sufficient manner. Furthermore, it is looked into whether the process can be repeated for a significant number of pieces by maneuvering between them. We conclude that the removal process seems possible although some improvements are needed. Maneuvering is costly but despite conservative assumptions, we estimate that about 800 pieces can be removed in one journey made by a satellite weighing ten tonnes of which nine are xenon.</p>
----------------------------------------------------------------------
In diva2:1111891  - redundant colon and missing spaces in title:
"Analytical model for interlaminarstresses in unsymmetric laminates:: A study of laminated structural batteries duringLi-ion intercalation"
==>
"Analytical model for interlaminar stresses in unsymmetric laminates: A study of laminated structural batteries during Li-ion intercalation"

abstract is: 
<p>The aim of this thesis is to develop a mechanical model to simulate the behaviorof a structural battery cell during charging cycles. The cell can be modeledas a three layer laminate, composed of anode, separator and cathode. The layersexpand and contract differently during the charging cycle, when Li-ionsare transferred from the cathode to the anode. CLPT is used to obtain farfield stresses and strains and an analytical method, based on Kassapoglou’stheory, is developed to evaluate the risk of delamination due to interlaminarstresses. Depending on the required and allowable stresses and strains, designrecommendations are given for battery and mechanical actuators applications.</p>

corrected abstract:
<p>The aim of this thesis is to develop a mechanical model to simulate the behavior of a structural battery cell during charging cycles. The cell can be modeled as a three layer laminate, composed of anode, separator and cathode. The layers expand and contract differently during the charging cycle, when Li-ions are transferred from the cathode to the anode. CLPT is used to obtain farfield stresses and strains and an analytical method, based on Kassapoglou's theory, is developed to evaluate the risk of delamination due to interlaminar stresses. Depending on the required and allowable stresses and strains, design recommendations are given for battery and mechanical actuators applications.</p>
----------------------------------------------------------------------
In diva2:919302 - missing space in title:
"Cost Modelling of Composite Componentsfor Helicopter Applications"
==>
"Cost Modelling of Composite Components for Helicopter Applications"

abstract is: 
<p>Composite materials are becoming more popular than ever. The increasing envi-ronmental concerns results in new challenges, where one of the biggest is to reduce the emission of greenhouse gases. Both the aerospace industry and the automotive industry work hard to become more environmental friendly. To reduce the emissions of vehicles such as cars, planes and helicopters reduction of weight is important as lower weight reduces fuel consumptions. To save weight more of the structural and load-bearing components in such vehicles are manufactured out of composites.To meet the requirements are a lot of new manufacturing techniques developing. To make the new methods competitive with the techniques used today it is important to make the new methods as cost-eÿcient as possible.The purpose of this master thesis is to investigate, describe and analyze the manufacturing cost of composite components manufactured by 3D weaving, when the technique is widely used in the industry. The goal is to determine cost driving parameters and investigate how to make 3D weaving cost competitive. This is done through out the design of a basic technical cost model and comparison between di˙erent cases and with other manufacturing methods. The component evaluated is a stringer for a side shell panel of a helicopter. The method used as comparison in this thesis is a hand lay-up preform consolidated by High pressure RTM (HP-RTM).A fully automated manufacturing process is chosen for each method in order to manufacture large annual manufacturing volumes. The cost model is designed using a bottom-up perspective where each step of the manufacturing process is evaluated separately. The lead-time and the cost corresponding to each sub-step is calculated and passed on to the final sum of all steps. The calculated cost are investment cost, operator cost, material cost and fixed cost corresponding to plant cost such as electricity cost.The cost model shows that the cost is decreasing with increasing manufacturing volumes and can be divided in to two regions. The first region corresponds to lower manufacturing volumes and is highly dependent on the investment cost, The second region corresponds to higher manufacturing volumes and the cost driving parameter for this region is the manufacturing time.The model also shows that it should be possible to manufacture cost competitive stringers out of 3D woven preforms. To make the technique cost competitive it is important to manufacture 3D weaving machines with high weaving speed, since the speed is the factor influencing the total component cost mostly.</p>

corrected abstract:
<p>Composite materials are becoming more popular than ever. The increasing environmental concerns results in new challenges, where one of the biggest is to reduce the emission of greenhouse gases. Both the aerospace industry and the automotive industry work hard to become more environmental friendly. To reduce the emissions of vehicles such as cars, planes and helicopters reduction of weight is important as lower weight reduces fuel consumptions. To save weight more of the structural and load-bearing components in such vehicles are manufactured out of composites.</p><p>To meet the requirements are a lot of new manufacturing techniques developing. To make the new methods competitive with the techniques used today it is important to make the new methods as cost-efficient as possible.</p><p>The purpose of this master thesis is to investigate, describe and analyze the manufacturing cost of composite components manufactured by 3D weaving, when the technique is widely used in the industry. The goal is to determine cost driving parameters and investigate how to make 3D weaving cost competitive. This is done through out the design of a basic technical cost model and comparison between different cases and with other manufacturing methods. The component evaluated is a stringer for a side shell panel of a helicopter. The method used as comparison in this thesis is a hand lay-up preform consolidated by High pressure RTM (HP-RTM).</p><p>A fully automated manufacturing process is chosen for each method in order to manufacture large annual manufacturing volumes. The cost model is designed using a bottom-up perspective where each step of the manufacturing process is evaluated separately. The lead-time and the cost corresponding to each sub-step is calculated and passed on to the final sum of all steps. The calculated cost are investment cost, operator cost, material cost and fixed cost corresponding to plant cost such as electricity cost.</p><p>The cost model shows that the cost is decreasing with increasing manufacturing volumes and can be divided in to two regions. The first region corresponds to lower manufacturing volumes and is highly dependent on the investment cost, The second region corresponds to higher manufacturing volumes and the cost driving parameter for this region is the manufacturing time.</p><p>The model also shows that it should be possible to manufacture cost competitive stringers out of 3D woven preforms. To make the technique cost competitive it is important to manufacture 3D weaving machines with high weaving speed, since the speed is the factor influencing the total component cost mostly.</p>
----------------------------------------------------------------------
In diva2:893770 
abstract is: 
<p>One of the major challenges for helicopter pilots are low level flights and landings in degraded visual environments (DVE). Without proper assistance systems, the pilots are prone to lose their situational awareness (SA) when fog, heavy precipitation, limited sunlight and stirred-up sand or snow degrades their view. In recent years, various synthetic and enhanced vision systems were de-veloped so as to assist the pilots in these demanding situations. This work enhances the existing sys-tems by proposing a concept for the visualization of traffic information in helmet-/head-mounted displays (HMD). The intuitive representation provides additional cues about the environment and decreases the pilots’ workload, especially during flights in offshore windparks or while search and rescue operations with many other vehicles operating within a small range.The thesis, which was created at Airbus Defence &amp; Space Friedrichshafen, analyzes the strengths and weaknesses of conventional head-down cockpit displays of traffic information (CDTIs) and of a very basic head-up traffic cueing system. Based on the results of this assessment, a prototype implementation of an integrated traffic visualization concept is developed where both display types – HDD and HMD – complement each other in their roles. All traffic information is retrieved from the on-board ADS-B system, which provides data such as the position and velocity vector of ADS-B equipped vehicles in the vicinity.The main focus of the work is placed on the development of methods for de-cluttering the HMD and increasing the information content of the head-up symbology by coding important parame-ters visually. In order to create a de-cluttered representation, the high-dimensional traffic data is clustered with a derivation of DBSCAN so as to identify groups of traffic that can be visualized by a single symbol. Ellipsoidal shapes are applied to airborne traffic while rectangular forms high-light vehicles on ground. To avoid distraction to the pilot due to sudden symbology changes when clusters are merged or split up, a smooth transition visualization method is developed. For the graphical representation of important information on the HMD, a measure indicating the threat potential of an intruder is proposed. This parameter, which is derived from the classification ap-proach of the TCAS II, is visualized by the color of the traffic symbol. In analogy to the TCAS, it is faded from green via yellow to red when the time to a predicted conflict decreases.The realized functions are integrated into a flight simulator and tested with synthetic ADS-B data. Furthermore, the proposed algorithms are evaluated in terms of their effectiveness and run-time efficiency, and finally the advantages and limitations of the approach are discussed. Now, the application must be integrated into an operational synthetic vision system so as to conduct further tests.</p>

corrected abstract:
<p>One of the major challenges for helicopter pilots are low level flights and landings in degraded visual environments (DVE). Without proper assistance systems, the pilots are prone to lose their situational awareness (SA) when fog, heavy precipitation, limited sunlight and stirred-up sand or snow degrades their view. In recent years, various synthetic and enhanced vision systems were developed so as to assist the pilots in these demanding situations. This work enhances the existing systems by proposing a concept for the visualization of traffic information in helmet-/head-mounted displays (HMD). The intuitive representation provides additional cues about the environment and decreases the pilots’ workload, especially during flights in offshore windparks or while search and rescue operations with many other vehicles operating within a small range.</p><p>The thesis, which was created at Airbus Defence &amp; Space Friedrichshafen, analyzes the strengths and weaknesses of conventional head-down cockpit displays of traffic information (CDTIs) and of a very basic head-up traffic cueing system. Based on the results of this assessment, a prototype implementation of an integrated traffic visualization concept is developed where both display types – HDD and HMD – complement each other in their roles. All traffic information is retrieved from the on-board ADS-B system, which provides data such as the position and velocity vector of ADS-B equipped vehicles in the vicinity.</p><p>The main focus of the work is placed on the development of methods for de-cluttering the HMD and increasing the information content of the head-up symbology by coding important parameters visually. In order to create a de-cluttered representation, the high-dimensional traffic data is clustered with a derivation of DBSCAN so as to identify groups of traffic that can be visualized by a single symbol. Ellipsoidal shapes are applied to airborne traffic while rectangular forms highlight vehicles on ground. To avoid distraction to the pilot due to sudden symbology changes when clusters are merged or split up, a smooth transition visualization method is developed. For the graphical representation of important information on the HMD, a measure indicating the threat potential of an intruder is proposed. This parameter, which is derived from the classification approach of the TCAS II, is visualized by the color of the traffic symbol. In analogy to the TCAS, it is faded from green via yellow to red when the time to a predicted conflict decreases.</p><p>The realized functions are integrated into a flight simulator and tested with synthetic ADS-B data. Furthermore, the proposed algorithms are evaluated in terms of their effectiveness and run-time efficiency, and finally the advantages and limitations of the approach are discussed. Now, the application must be integrated into an operational synthetic vision system so as to conduct further tests.</p>
----------------------------------------------------------------------
In diva2:891537 
abstract is: 
<p>he global situation of personal and freight transport shows that the energy demand for transportation steadily increases, and prognoses indicate that the energy usage will double until 2050. The largest growth rates are expected in Asia, and China in particular will account for over 12 % of global transport energy usage in 2050. Over 81 % of passenger transport in passenger kilometre was produced by passenger cars in 2012 in the European Union, and new energy eÿcient and environmental friendly solutions have to be developed.PRT (Personal Rapid Transit) systems combine the benefits of traditional road systems (flexibility, accessibility, attractiveness) and rail systems (safety, capacity, environmental friendliness). This MSc thesis investigates a concept by SkyCab AB as a case study, which o˙ers an automated, non-stop and on-demand transportation service in a dedicated network and is supposed to fill a gap between personal cars and public transport. The focus is put on the energy usage of the vehicles in the operational phase.The objective is to identify the relevant parameters that determine the energy usage and their contributions. This request is addressed by setting up a simulation model, based on the SkyCab concept and estimations of vehicle parameters. A reference calculation and 16 variations of key parameters are conducted. The relation to greenhouse gas emissions is investigated and emissions are calculated for di˙erent electricity mixes.A second-order polynomial of running resistance for the vehicle is determined, includ-ing estimations of rolling resistance of small pneumatic tyres on straight track and in superelevated curves. The auxiliary power is estimated for the SkyCab vehicle on basis of a small electric passenger car.For the reference case the energy for rolling resistance is approx. 44 % of the energy usage, and auxiliary energy contributes by 33 %. Both o˙er potential for eÿciency im-provement. The auxiliary power is strongly dependent on the passengers’ comfort needs and the ambient conditions. Changes of acceleration rates have low impact on the energy usage, since a smaller proportion of energy is regenerated. An increase in top speed is a suÿcient measure to reduce trip time with comparably low increase in energy usage. Finally, suggestions are proposed to reduce the energy usage by improving key properties of the vehicle and guideway.</p>

corrected abstract:
<p>The global situation of personal and freight transport shows that the energy demand for transportation steadily increases, and prognoses indicate that the energy usage will double until 2050. The largest growth rates are expected in Asia, and China in particular will account for over 12 % of global transport energy usage in 2050. Over 81 % of passenger transport in passenger kilometre was produced by passenger cars in 2012 in the European Union, and new energy efficient and environmental friendly solutions have to be developed. PRT (Personal Rapid Transit) systems combine the benefits of traditional road systems (flexibility, accessibility, attractiveness) and rail systems (safety, capacity, environmental friendliness). This MSc thesis investigates a concept by SkyCab AB as a case study, which offers an automated, non-stop and on-demand transportation service in a dedicated network and is supposed to fill a gap between personal cars and public transport. The focus is put on the energy usage of the vehicles in the operational phase.</p><p>The objective is to identify the relevant parameters that determine the energy usage and their contributions. This request is addressed by setting up a simulation model, based on the SkyCab concept and estimations of vehicle parameters. A reference calculation and 16 variations of key parameters are conducted. The relation to greenhouse gas emissions is investigated and emissions are calculated for different electricity mixes.</p><p>A second-order polynomial of running resistance for the vehicle is determined, including estimations of rolling resistance of small pneumatic tyres on straight track and in superelevated curves. The auxiliary power is estimated for the SkyCab vehicle on basis of a small electric passenger car.</p><p>For the reference case the energy for rolling resistance is approx. 44 % of the energy usage, and auxiliary energy contributes by 33 %. Both offer potential for efficiency improvement. The auxiliary power is strongly dependent on the passengers’ comfort needs and the ambient conditions. Changes of acceleration rates have low impact on the energy usage, since a smaller proportion of energy is regenerated. An increase in top speed is a sufficient measure to reduce trip time with comparably low increase in energy usage. Finally, suggestions are proposed to reduce the energy usage by improving key properties of the vehicle and guideway.</p>
----------------------------------------------------------------------
In diva2:872213 
abstract is: 
<p>AbstractThe RCV is a four wheel drive and steer electrical vehicle developed and built by the Transport Lab at KTH Royal Institute of Technology. It is fully steer-by-wire and each wheel can be individually controlled, both with regard to steering angle, as well as camber and driving torque.The RCV is planned to be used as a common platform for diﬀerent ﬁelds of research, as a rolling laboratory to implement and evaluate research with.In this report the speciﬁcation and functionalities of the RCV are reviewed and its data collection capabilities are validated and evaluated through classic vehicle dynamic analyses such as circle tests, step steer and roll out tests. Also, some more experimental functionalities such as simple torque vectoring, toe sweep and steering by joystick, as proof of concept of the RCV as a research and prototyping platform.Finally, some suggestions for further developments for the RCV platform are presented.</p>

corrected abstract:
<p>The RCV is a four wheel drive and steer electrical vehicle developed and built by the Transport Lab at KTH Royal Institute of Technology. It is fully steer-by-wire and each wheel can be individually controlled, both with regard to steering angle, as well as camber and driving torque.</p><p>The RCV is planned to be used as a common platform for different fields of research, as a rolling laboratory to implement and evaluate research with.</p><p>In this report the specification and functionalities of the RCV are reviewed and its data collection capabilities are validated and evaluated through classic vehicle dynamic analyses such as circle tests, step steer and roll out tests. Also, some more experimental functionalities such as simple torque vectoring, toe sweep and steering by joystick, as proof of concept of the RCV as a research and prototyping platform.</p><p>Finally, some suggestions for further developments for the RCV platform are presented.</p>
----------------------------------------------------------------------
In diva2:864063 
abstract is: 
<p>This thesis analyzes one hour based energy disaggregation using Sparse Coding by exploiting temporal differences. Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having device-level energy information can cause users to conserve significant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In Energy Disaggregation or sometimes called Non- Intrusive Load Monitoring (NILM) most approaches are based on high frequent monitored appliances, while households only measure their consumption via smart-meters, which only account for one-hour measurements. This thesis aims at implementing key algorithms from J. Zico Kotler, Siddarth Batra and Andrew Ng paper "Energy Disaggregation via Discriminative Sparse Coding" and try to replicate the results by exploiting temporal differences that occur when dealing with time series data. The implementation was successful, but the results were inconclusive when dealing with large datasets, as the algorithm was too computationally heavy for the resources available. The work was performed at the Swedish company Greenely, who develops visualizations based on gamification for energy bills via a mobile application.</p>

corrected abstract:
<p>This thesis analyzes one hour based energy disaggregation using Sparse Coding by exploiting temporal differences. Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having device-level energy information can cause users to conserve significant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In Energy Disaggregation or sometimes called Non-Intrusive Load Monitoring (NILM) most approaches are based on high frequent monitored appliances, while households only measure their consumption via smart-meters, which only account for one-hour measurements. This thesis aims at implementing key algorithms from J. Zico Kotler, Siddarth Batra and Andrew Ng paper ”Energy Disaggregation via Discriminative Sparse Coding” and try to replicate the results by exploiting temporal differences that occur when dealing with time series data. The implementation was successful, but the results were inconclusive when dealing with large datasets, as the algorithm was too computationally heavy for the resources available. The work was performed at the Swedish company Greenely, who develops visualizations based on gamification for energy bills via a mobile application.</p>
----------------------------------------------------------------------
In diva2:853494 
abstract is: 
<p>The purpose of this project is to design a control law to control a groupof quadcopters ying in formation in a specied reference track.</p><p>To describe the movements of a quadcopter a mathematical model, basedon Newton-Euler equations, is formed and linearized into a linear statespacemodel. The control law is then designed by using feedback. Themodel was simulated in MATLABRO for dierent formations and numberof quadcopters, where the designed control law proved to be successfulin stabilizing the system. This report gives a basic investigation of formationying control where several assumptions and simplications havebeen made. For future work the model can be developed and more factorsbe taken into consideration.</p>

corrected abstract:
<p>The purpose of this project is to design a control law to control a group of quadcopters flying in formation in a specified reference track.</p><p>To describe the movements of a quadcopter a mathematical model, based on Newton-Euler equations, is formed and linearized into a linear statespace model. The control law is then designed by using feedback. The model was simulated in MATLAB® for different formations and number of quadcopters, where the designed control law proved to be successful in stabilizing the system. This report gives a basic investigation of formation flying control where several assumptions and simplifications have been made. For future work the model can be developed and more factors be taken into consideration.</p>
----------------------------------------------------------------------
In diva2:846130 
abstract is: 
<p>As the world’s natural resources are depleting while the environment constraints on society become om- nipresent, new technological alternatives for transports are sought.</p><p>Over the past years, the regulations in terms of pollutants stimulated the industries’ innovation in almost all regions of the world, using directives such as the European emission standards have managed to limit the carbon dioxide emissions up to 82% between Euro 1 and Euro 4.</p><p>Between research towards new fuels and the improvement of internal combustion engines (ICE), one solution stands out: combining an electrical machine with a traditional ICE to reduce the environmental impact and the fuel consumption. The most advanced level of hybridization is the Plug-In Hybrid Vehicle (PHEV), which usually has a battery capacity allowing it to drive all electric on short ranges and can be recharged at a power socket. This technological solution opens to a wide range of possibilities and improvements in terms of fuel consumption, as well as commercial opportunities.</p><p>This master thesis, conducted at Audi AG in Ingolstadt (Germany) will focus on one development feature of Plug-In Hybrid Vehicles: the predictive drive strategy. Driving all electric everywhere would be the best solution in terms of fuel economy. However, a PHEV does not always allow it, especially on long rides. Mainly three drive modes are available: an electric mode that will use the ICE as seldom as possible, a hybrid mode that allows electrical experience at low speed and uses the ICE for higher needs in power, and a charge mode aiming at recharging the battery through the ICE. Some enhanced algorithms are also able to detect the different environments the vehicle is expected to drive through, and compute fuel and electrical consumption estimates for each modes available. Based on this information, an optimizer will determine the best drive strategy - i.e the best temporal combination of modes - in terms of fuel consumption, but also taking into account some external requirements, such as Zero Emission Zones that might come up along the route.</p><p>The method used to compute the best drive strategy relies on four main stages. A first step is to identify the different driving environments constituting the route, based on long-range power predictions. This can be done using statistical comparison methods based on the assumption that one driving environment can be characterised by a range of values for a single parameter. The interval the parameter belongs to will determine the driving environment. The Kolmogorov-Smirnov test function is the second method used, which compares the cumulative distributions of two series of values and assesses their similarity. Once the itinerary is properly split into segments, it is possible to compute the expected fuel and electrical consumptions by taking the drivetrain efficiency chain, the regenerative braking and E-boost characteristics into account. For each section, a high- priority drive mode can also be given in order to allow a more flexible optimization in terms of coding and algorithmic. Mainly two prioritization options have been studied in this thesis. One pertains to the use a finite number of identified driving environments, to which a prioritization of the driving mode has been performed <em>offline </em>beforehand. The second option, the online priority, uses realtime computed values for a given segment and compares them in order to find the mode to be prioritized. Finally, two optimizers have been developed to compute an efficient drive strategy. The tree-based global optimizer considers the drive mode combinations for the whole route and selects the most efficient one, while a simplified optimizer would only improve a given basic strategy to reduce the fuel consumption.</p><p>The results of the comparison methods show interesting perspectives for the Kolmogorov-Smirnov test func- tion, as it only tells two segments apart without trying to identify the environment they belong to, hence a faster implementation. The parametric method proved its efficiency when working with a dual-environment condition, although an extension to more than two environments would make the whole interval computation process much more complex. Then, the consumption computation processes have been precisely defined. Fi- nally, both optimizers gave interesting results. The global optimizer is very complex algorithm which requires considerable CPU resources. However, it provides absolute optima to the optimization problem that will be used to observe the trend, behaviour and accuracy of some drive strategies. The simplified optimizer is a suggestion of a faster algorithm that does not computes the best solution but rather a good one. If the quality of the resulting solution is very route-dependant, it can provide drive strategies very close to the best ones computesby the global optimizer.</p>

corrected abstract:
<p>As the world’s natural resources are depleting while the environment constraints on society become omnipresent, new technological alternatives for transports are sought.</p><p>Over the past years, the regulations in terms of pollutants stimulated the industries’ innovation in almost all regions of the world, using directives such as the European emission standards have managed to limit the carbon dioxide emissions up to 82% between Euro 1 and Euro 4.</p><p>Between research towards new fuels and the improvement of internal combustion engines (ICE), one solution stands out: combining an electrical machine with a traditional ICE to reduce the environmental impact and the fuel consumption. The most advanced level of hybridization is the Plug-In Hybrid Vehicle (PHEV), which usually has a battery capacity allowing it to drive all electric on short ranges and can be recharged at a power socket. This technological solution opens to a wide range of possibilities and improvements in terms of fuel consumption, as well as commercial opportunities.</p><p>This master thesis, conducted at Audi AG in Ingolstadt (Germany) will focus on one development feature of Plug-In Hybrid Vehicles: the predictive drive strategy. Driving all electric everywhere would be the best solution in terms of fuel economy. However, a PHEV does not always allow it, especially on long rides. Mainly three drive modes are available: an electric mode that will use the ICE as seldom as possible, a hybrid mode that allows electrical experience at low speed and uses the ICE for higher needs in power, and a charge mode aiming at recharging the battery through the ICE. Some enhanced algorithms are also able to detect the different environments the vehicle is expected to drive through, and compute fuel and electrical consumption estimates for each modes available. Based on this information, an optimizer will determine the best drive strategy - i.e the best temporal combination of modes - in terms of fuel consumption, but also taking into account some external requirements, such as Zero Emission Zones that might come up along the route.</p><p>The method used to compute the best drive strategy relies on four main stages. A first step is to identify the different driving environments constituting the route, based on long-range power predictions. This can be done using statistical comparison methods based on the assumption that one driving environment can be characterised by a range of values for a single parameter. The interval the parameter belongs to will determine the driving environment. The Kolmogorov-Smirnov test function is the second method used, which compares the cumulative distributions of two series of values and assesses their similarity. Once the itinerary is properly split into segments, it is possible to compute the expected fuel and electrical consumptions by taking the drivetrain efficiency chain, the regenerative braking and E-boost characteristics into account. For each section, a high-priority drive mode can also be given in order to allow a more flexible optimization in terms of coding and algorithmic. Mainly two prioritization options have been studied in this thesis. One pertains to the use a finite number of identified driving environments, to which a prioritization of the driving mode has been performed <em>offline</em> beforehand. The second option, the online priority, uses realtime computed values for a given segment and compares them in order to find the mode to be prioritized. Finally, two optimizers have been developed to compute an efficient drive strategy. The tree-based global optimizer considers the drive mode combinations for the whole route and selects the most efficient one, while a simplified optimizer would only improve a given basic strategy to reduce the fuel consumption.</p><p>The results of the comparison methods show interesting perspectives for the Kolmogorov-Smirnov test function, as it only tells two segments apart without trying to identify the environment they belong to, hence a faster implementation. The parametric method proved its efficiency when working with a dual-environment condition, although an extension to more than two environments would make the whole interval computation process much more complex. Then, the consumption computation processes have been precisely defined. Finally, both optimizers gave interesting results. The global optimizer is very complex algorithm which requires considerable CPU resources. However, it provides absolute optima to the optimization problem that will be used to observe the trend, behaviour and accuracy of some drive strategies. The simplified optimizer is a suggestion of a faster algorithm that does not computes the best solution but rather a good one. If the quality of the resulting solution is very route-dependant, it can provide drive strategies very close to the best ones computes by the global optimizer.</p>
----------------------------------------------------------------------
In diva2:816955 
abstract is: 
<p>Email communication is valuable for any modern company, since it offers an easy mean for spreading important information or advertising new products, features or offers and much more. To be able to identify which customers that would be interested in certain information would make it possible to significantly improve a company's email communication and as such avoiding that customers start ignoring messages and creating unnecessary badwill. This thesis focuses on trying to target customers by applying statistical learning methods to historical data provided by the music streaming company Spotify.</p><p>An important aspect was the high-dimensionality of the data, creating certain demands on the applied methods. A binary classification model was created, where the target was whether a customer will open the email or not. Two approaches were used for trying to target the costumers, <em>logistic regression</em>, both with and without regularization, and <em>random forest classifier</em>, for their ability to handle the high-dimensionality of the data. Performance accuracy of the suggested models were then evaluated on both a training set and a test set using statistical validation methods, such as cross-validation, ROC curves and lift charts.</p><p>The models were studied under both large-sample and high-dimensional scenarios. The high-dimensional scenario represents when the number of observations, <em>N</em>, is of the same order as the number of features, <em>p</em> and the large sample scenario represents when <em>N </em>≫<em> p</em>. Lasso-based variable selection was performed for both these scenarios, to study the informative value of the features.</p><p>This study demonstrates that it is possible to greatly improve the opening rate of emails by targeting users, even in the high dimensional scenario. The results show that increasing the amount of training data over a thousand fold will only improve the performance marginally. Rather efficient customer targeting can be achieved by using a few highly informative variables selected by the Lasso regularization.</p>

corrected abstract:
<p>Email communication is valuable for any modern company, since it offers an easy mean for spreading important information or advertising new products, features or offers and much more. To be able to identify which customers that would be interested in certain information would make it possible to significantly improve a company's email communication and as such avoiding that customers start ignoring messages and creating unnecessary bad will. This thesis focuses on trying to target customers by applying statistical learning methods to historical data provided by the music streaming company Spotify.</p><p>An important aspect was the high-dimensionality of the data, creating certain demands on the applied methods. A binary classification model was created, where the target was whether a customer will open the email or not. Two approaches were used for trying to target the costumers, <em>logistic regression</em>, both with and without regularization, and <em>random forest classifier</em>, for their ability to handle the high-dimensionality of the data. Performance accuracy of the suggested models were then evaluated on both a training set and a test set using statistical validation methods, such as cross-validation, ROC curves and lift charts.</p><p>The models were studied under both large-sample and high-dimensional scenarios. The high-dimensional scenario represents when the number of observations, <em>𝑁</em>, is of the same order as the number of features, <em>𝑝</em> and the large sample scenario represents when <em>𝑁 ≫ 𝑝</em>. Lasso-based variable selection was performed for both these scenarios, to study the informative value of the features.</p><p>This study demonstrates that it is possible to greatly improve the opening rate of emails by targeting users, even in the high dimensional scenario. The results show that increasing the amount of training data over a thousand fold will only improve the performance marginally. Rather efficient customer targeting can be achieved by using a few highly informative variables selected by the Lasso regularization.</p>
----------------------------------------------------------------------
In diva2:784045 
abstract is: 
<p>Currently, traffic congestion often happens in big cities every day. People demand a new conceptual vehiclewhich has a slender shape to reduce space, lightweight structure to decrease the fuel consumption andinnovative technology to adapt for multiple transportation conditions. NEWT is such a conceptual amphibious vehicle that satisfies people's requirements. However, everything has two sides. Slender shape and high centre of gravity will result in instability. When NEWT runs in low speed, it easily gets rolled over. In order to make up for its drawback, gyro-stabilizer has been applied to the vehicle. By tilting the rotational gyro, it generates a counter torque counteracting the roll motion to make the vehicle recover to an upright position.Therefore this master thesis analyses the original stability of the vehicle and the possible improvement by adding the gyro system for both land and water-conditions. The model can handle the problem that thevehicle meets periodic disturbance forces, such as wave excitation force and wind force.</p>


corrected abstract:
<p>Currently, traffic congestion often happens in big cities every day. People demand a new conceptual vehicle which has a slender shape to reduce space, lightweight structure to decrease the fuel consumption and innovative technology to adapt for multiple transportation conditions. NEWT is such a conceptual amphibious vehicle that satisfies people's requirements. However, everything has two sides. Slender shape and high centre of gravity will result in instability. When NEWT runs in low speed, it easily gets rolled over. In order to make up for its drawback, gyro-stabilizer has been applied to the vehicle. By tilting the rotational gyro, it generates a counter torque counteracting the roll motion to make the vehicle recover to an upright position.</p><p>Therefore this master thesis analyses the original stability of the vehicle and the possible improvement by adding the gyro system for both land and water-conditions. The model can handle the problem that the vehicle meets periodic disturbance forces, such as wave excitation force and wind force.</p>
----------------------------------------------------------------------
In diva2:741109 
abstract is: 
<p>A major open question in astrophysics concerns the formation of highly collimated beams of matter and electromagnetic radiation, so-called relativistic jets, emerging from active galactic nuclei (AGN) in the center of certain galaxies. It is not known how, or why, these jets are formed nor how they maintain their collimated state for such large distances. For long it has been thought that jets can only be hosted in large, elliptical galaxies, though quiterecently AGNs have been discovered that contradict these beliefs. These objects are called radio-loud narrow-line</p><p>Seyfert 1 galaxies (RLNLSy1). In this thesis an AGN belonging to this class is analysed. Utilising an X-ray spectral fitting package (XSPEC), data from two separate observations obtained from the X-ray Multi-Mirror Mission observatory (XMM-Newton) of the radio-loud narrow-line Seyfert 1 galaxy SDSS J154817.92+351128.0 are fitted through the use of different models and analysed in the energy region of 0.3 keV to 10 keV. The purpose is to disentangle the emission from the light-emitting accretion disk surrounding the central supermassive black hole(SMBH) and the emission from a possible jet, as well as to determine their properties.</p><p>The results indicate a strong presence of a jet while still maintaining a visible spectrum from the accretion disk.</p><p>No sign of the Fe line, usually seen in radio-quiet NLSy1s, can be detected, even though the data is not of sufficientquality to exclude the possibility of detection. Furthermore, considerable amounts of enhanced emission below 2 keV is observed, a so-called soft excess. The soft excess could be explained by inverse comptonisation of the light emitted from the disk and/or reflections in the disk. Our results confirm the presence of powerful relativistic</p><p>jets emerging from J1548+3511. The wider implications of jet formation are further discussed.</p>

corrected abstract:
<p>A major open question in astrophysics concerns the formation of highly collimated beams of matter and electromagnetic radiation, so-called relativistic jets, emerging from active galactic nuclei (AGN) in the center of certain galaxies. It is not known how, or why, these jets are formed nor how they maintain their collimated state for such large distances. For long it has been thought that jets can only be hosted in large, elliptical galaxies, though quite recently AGNs have been discovered that contradict these beliefs. These objects are called radio-loud narrow-line Seyfert 1 galaxies (RLNLSy1). In this thesis an AGN belonging to this class is analysed. Utilising an X-ray spectral fitting package (XSPEC), data from two separate observations obtained from the X-ray Multi-Mirror Mission observatory (XMM-Newton) of the radio-loud narrow-line Seyfert 1 galaxy SDSS J154817.92+351128.0 are fitted through the use of different models and analysed in the energy region of 0.3 keV to 10 keV. The purpose is to disentangle the emission from the light-emitting accretion disk surrounding the central supermassive black hole (SMBH) and the emission from a possible jet, as well as to determine their properties.</p><p>The results indicate a strong presence of a jet while still maintaining a visible spectrum from the accretion disk. No sign of the Fe line, usually seen in radio-quiet NLSy1s, can be detected, even though the data is not of sufficient quality to exclude the possibility of detection. Furthermore, considerable amounts of enhanced emission below 2 keV is observed, a so-called soft excess. The soft excess could be explained by inverse comptonisation of the light emitted from the disk and/or reflections in the disk. Our results confirm the presence of powerful relativistic jets emerging from J1548+3511. The wider implications of jet formation are further discussed.</p>
----------------------------------------------------------------------
In diva2:723418 
abstract is: 
<p>We consider the problem of determining which conditions are necessaryfor cobordisms to admit Lorentzian metrics with certain properties. Inparticular, we prove a result originally due to Tipler without a smoothnesshypothesis necessary in the original proof. In doing this, we prove thatcompact horizons in a smooth spacetime satisfying the null energy condition aresmooth. We also prove that the ”generic condition” is indeed generic in the setof Lorentzian metrics on a given manifold</p>

corrected abstract:
<p>We consider the problem of determining which conditions are necessary for cobordisms to admit Lorentzian metrics with certain properties. In particular, we prove a result originally due to Tipler without a smoothness hypothesis necessary in the original proof. In doing this, we prove that compact horizons in a smooth spacetime satisfying the null energy condition are smooth.</p><p>We also prove that the ”generic condition” is indeed generic in the set of Lorentzian metrics on a given manifold.</p>
----------------------------------------------------------------------
In diva2:681967  - missing space in title:
"Low complexity algorithms for faster-than-Nyquistsign: Using coding to avoid an NP-hard problem"
==>
"Low complexity algorithms for faster-than-Nyquist signaling: Using coding to avoid an NP-hard problem"

abstract is: 
<p>This thesis is an investigation of what happens when communication links are pushed towards their limits and the data-bearing-pulses are packed tighter in time than previously done. This is called faster-than-Nyquist (FTN) signaling and it will violate the Nyquist inter-symbol interference criterion, implying that the data-pulsesare no longer orthogonal and thus that the samples at the receiver will be dependent on more than one of the transmitted symbols. Inter-symbol interference (ISI) has occurred and the consequences of it are studied for the AWGN-channel model. Here it is shown that in order to do maximum likelihood estimation on these samples the receiver will face an NP-hard problem. The standard algorithm to make good estimations in the ISI case is the Viterbi algorithm, but applied on a block with N bits and interference among K bits thecomplexity is O(N *2<sup>K</sup>), hence limiting the practical applicability. Here, a precoding scheme is proposed together with a decoding that reduce the estimation complexity. By applying the proposed precoding/decoding to a data block of length N the estimation can be done in O(N<sup>2</sup>) operations preceded by a single off-line O(N<sup>3</sup>) calculation. The precoding itself is also done in O(N<sup>2</sup>)operations, with a single o ff-line operation of O(N<sup>3</sup>) complexity.</p><p>The strength of the precoding is shown in simulations. In the first it was tested together with turbo codes of code rate 2/3 and block lengthof 6000 bits. When sending 25% more data (FTN) the non-precoded case needed about 2.5 dB higher signal-to-noise ratio (SNR) to have the same error rate as the precoded case. When the precoded case performed without any block errors, the non-precoded case still had a block error rate almost equal to 1.</p><p>We also studied the scenario of transmission with low latency and high reliability. Here, 600 bits were transmitted with a code rate of 2/3, and hence the target was to communicate 400 bits of data. Applying FTN with doublepacking, that is transmitting 1200 bits during the same amount of time, it was possible to lower the code rate to 1/3 since only 400 bits of data was to be communicated. This technique greatly improves the robustness. When the FTN case performed error free, the classical Nyquist case still had a block error rate of 0.19. To reach error free performance the Nyquist case needed 1.25 dB higher SNR compared to the precoded FTN case with lower code rate.</p>

corrected abstract:
<p>This thesis is an investigation of what happens when communication links are pushed towards their limits and the data-bearing-pulses are packed tighter in time than previously done. This is called faster-than-Nyquist (FTN) signaling and it will violate the Nyquist inter-symbol interference criterion, implying that the data-pulses are no longer orthogonal and thus that the samples at the receiver will be dependent on more than one of the transmitted symbols. Inter-symbol interference (ISI) has occurred and the consequences of it are studied for the AWGN-channel model. Here it is shown that in order to do maximum likelihood estimation on these samples the receiver will face an NP-hard problem. The standard algorithm to make good estimations in the ISI case is the Viterbi algorithm, but applied on a block with 𝑁 bits and interference among 𝐾 bits the complexity is 𝑂(𝑁·2<sup>𝐾</sup>), hence limiting the practical applicability. Here, a precoding scheme is proposed together with a decoding that reduce the estimation complexity. By applying the proposed precoding/decoding to a data block of length 𝑁 the estimation can be done in 𝑂(𝑁<sup>2</sup>) operations preceded by a single off-line 𝑂(𝑁<sup>3</sup>) calculation. The precoding itself is also done in 𝑂(𝑁<sup>2</sup>) operations, with a single off-line operation of 𝑂(𝑁<sup>3</sup>) complexity.</p><p>The strength of the precoding is shown in simulations. In the first it was tested together with turbo codes of code rate 2/3 and block length of 6000 bits. When sending 25% more data (FTN) the non-precoded case needed about 2.5 dB higher signal-to-noise ratio (SNR) to have the same error rate as the precoded case. When the precoded case performed without any block errors, the non-precoded case still had a block error rate almost equal to 1.</p><p>We also studied the scenario of transmission with low latency and high reliability. Here, 600 bits were transmitted with a code rate of 2/3, and hence the target was to communicate 400 bits of data. Applying FTN with double packing, that is transmitting 1200 bits during the same amount of time, it was possible to lower the code rate to 1/3 since only 400 bits of data was to be communicated. This technique greatly improves the robustness. When the FTN case performed error free, the classical Nyquist case still had a block error rate of 0.19. To reach error free performance the Nyquist case needed 1.25 dB higher SNR compared to the precoded FTN case with lower code rate.</p>
----------------------------------------------------------------------
In diva2:676710 - missing space in title:
"Transversal load introduction on sandwich RailwayCarbodies"
==>
"Transversal load introduction on sandwich Railway Carbodies"

abstract is: 
<p>Sandwich structures made up by foam core and carbon fiber face-sheets are now seen as a promising technical solution for the design of railway carbodies by many railway manufacturers. Despite their numerous advantages over the classical metallic structures, transversal load introduction usually result in high stresses in the core of a sandwich structure. Hence, the fastening of equipment on a sandwich carbody must be studied carefully. The focus of this thesis has been put on the seat fastenings.</p><p>Several technical solutions were compared. A metallic part, called a C-rail, adhesively bonded on the top of the sandwich panel represents the best solution according to the requirements. The aim of the C-rail is to spread the load and therefore lower the stress concentration in the core. The reaction loads at the seat fastenings, when the seats are loaded according to the European norms, where calculated using finite element (FE) analysis. Guidelines for the design of the C-rail were developed, based on 2D FE models. The focus was put on the reduction of stress concentration in the core. It was shown that an optimal C-rail cross section can be found for a specific sandwich panel.</p><p>An analytical solution of the residual shear stresses induced by thermal expansion was developed to facilitate the choice of adhesive. It was shown that thick adhesive layers with low shear stiffness have the best strength with respect to temperature changes. Based on the 2D FE model, efficient techniques were derived for the modeling of adhesive layers. The influence of the face-sheet and adhesive modelisation on the stress distribution in the adhesive was studied. For that purpose, several combinations of material properties for the face-sheet and adhesive were investigated. In particular a spring based model of the adhesive was derived in order to include the face-sheet out of plane stiffness. It was shown that the stiffness of the adhesive have a high influence on the stress distribution in the adhesive layer.</p><p>A 3D model was built up in order to investigate the load cases derived at the fastenings and to validate the qualitative studies based on the 2D models. The choice of the final sandwich panel depends on numerous issues such as sound insulation, dynamic properties or weight.Various sandwich configurations were investigated in order to determine the influence of issues related to equipment fastening on the final choice of sandwich panel. The face-sheet bending stiffness as well as the core stiffness were identified as the key parameters regarding the sandwich panel out of plane strength.</p><p>The panel configuration obtained by weight optimization of the carbody represents the weakest configuration with respect to out of plane load introduction. Note that the solution proposed fulfilled the requirements for that configuration.</p>

corrected abstract:
<p>Sandwich structures made up by foam core and carbon fiber face-sheets are now seen as a promising technical solution for the design of railway carbodies by many railway manufacturers. Despite their numerous advantages over the classical metallic structures, transversal load introduction usually result in high stresses in the core of a sandwich structure. Hence, the fastening of equipment on a sandwich carbody must be studied carefully. The focus of this thesis has been put on the seat fastenings.</p><p>Several technical solutions were compared. A metallic part, called a C-rail, adhesively bonded on the top of the sandwich panel represents the best solution according to the requirements. The aim of the C-rail is to spread the load and therefore lower the stress concentration in the core. The reaction loads at the seat fastenings, when the seats are loaded according to the European norms, where calculated using finite element (FE) analysis. Guidelines for the design of the C-rail were developed, based on 2D FE models. The focus was put on the reduction of stress concentration in the core. It was shown that an optimal C-rail cross section can be found for a specific sandwich panel.</p><p>An analytical solution of the residual shear stresses induced by thermal expansion was developed to facilitate the choice of adhesive. It was shown that thick adhesive layers with low shear stiffness have the best strength with respect to temperature changes. Based on the 2D FE model, efficient techniques were derived for the modeling of adhesive layers. The influence of the face-sheet and adhesive modelisation on the stress distribution in the adhesive was studied. For that purpose, several combinations of material properties for the face-sheet and adhesive were investigated. In particular a spring based model of the adhesive was derived in order to include the face-sheet out of plane stiffness. It was shown that the stiffness of the adhesive have a high influence on the stress distribution in the adhesive layer.</p><p>A 3D model was built up in order to investigate the load cases derived at the fastenings and to validate the qualitative studies based on the 2D models. The choice of the final sandwich panel depends on numerous issues such as sound insulation, dynamic properties or weight. Various sandwich configurations were investigated in order to determine the influence of issues related to equipment fastening on the final choice of sandwich panel. The face-sheet bending stiffness as well as the core stiffness were identified as the key parameters regarding the sandwich panel out of plane strength. The panel configuration obtained by weight optimization of the carbody represents the weakest configuration with respect to out of plane load introduction. Note that the solution proposed fulfilled the requirements for that configuration.</p>
----------------------------------------------------------------------
In diva2:652385 
abstract is: 
<p>Due to the growing demand for comfort, the noise generated by HVAC components should be considered by the designers. Flow induced noise is one of the major contributors to the noise in HVAC systems. By means of computational aero-acoustics(CAA), the mechanism of noise generationis able to be studiedin the design phase.</p><p>This research has been conducted based on a simplified HVAC duct, aiming to validate a hybrid method in CAA. The hybrid method is an affordable and promising approach for industrial applications. CFD simulation, as the first step of the hybrid method, has been performed. Due to the deficiency of RANS and high computational cost of LES, hybrid RANS-LES approaches are employed to combine sufficient accuracy and reduced cost. Delayed Detached Eddy Simulation (DDES) and Scale Adaptive Simulation (SAS), as the representative hybrid RANS-LES approaches, are performed and compared. CFD results are shown in detail and compared with the experimental measurements available in the literature. The comparison shows a good agreement for the time averaged flow field and a fairly good agreement for uns teady flow phenomena. Discrepancies between numerical results and measurements can be observe dregarding mainly unsteady pressure fluctuations.The influence study of grid, discretization schemes, time step as well as other parametersis also conducted. The insights gained here can serve as a guideline for future complex applications.</p>

corrected abstract:
<p>Due to the growing demand for comfort, the noise generated by HVAC components should be considered by the designers. Flow induced noise is one of the major contributors to the noise in HVAC systems. By means of computational aero-acoustics (CAA), the mechanism of noise generation is able to be studied in the design phase.</p><p>This research has been conducted based on a simplified HVAC duct, aiming to validate a hybrid method in CAA. The hybrid method is an affordable and promising approach for industrial applications. CFD simulation, as the first step of the hybrid method, has been performed. Due to the deficiency of RANS and high computational cost of LES, hybrid RANS-LES approaches are employed to combine sufficient accuracy and reduced cost. Delayed Detached Eddy Simulation (DDES) and Scale Adaptive Simulation (SAS), as the representative hybrid RANS-LES approaches, are performed and compared. CFD results are shown in detail and compared with the experimental measurements available in the literature. The comparison shows a good agreement for the time averaged flow field and a fairly good agreement for unsteady flow phenomena. Discrepancies between numerical results and measurements can be observed regarding mainly unsteady pressure fluctuations. The influence study of grid, discretization schemes, time step as well as other parameters is also conducted. The insights gained here can serve as a guideline for future complex applications.</p>
----------------------------------------------------------------------
In diva2:649862 
abstract is: 
<p> </p><p>Nordea, being the largest corporate group of its kind in Northern Europe, has</p><p>a great need of evaluating its customers ability to repay a debt as well as the</p><p>probability of bankruptcy. The evaluation is done by different statistically derived</p><p>internal rating models, based on logistic regression. The models have been</p><p>developed by the use of historical data and attain good predictiveness when a</p><p>lot of observational data is provided for each specific customer. In order to</p><p>ameliorate the rating models, Nordea wants to implement two new validation</p><p>methods, recommended by the reputable credit rating agency Moody’s:</p><p></p><p>information</p><p>entropy</p><p></p><p>and accuracy ratio with simulated defaults. A default is a</p><p>customer either being close to or being bankrupt.</p><p></p><p>Information entropy measures</p><p>how much information is included within a given variable, while</p><p></p><p>accuracy</p><p>ratio with simulated defaults</p><p></p><p>validates the ability of the model to discriminate</p><p>between "good" and "bad" customers when simulating default data. The simulation</p><p>is used when sufficient default data does not exist, which is the case</p><p>for large corporates. After the implementation of these validation methods, for</p><p>the same set of data that Moody’s were given, the results that they presented</p><p>could be confirmed by the chosen implementation method. This method was</p><p>then used for analysis of a general set of data and it could be concluded that</p><p>the use of each validation method, recommended by Moody’s, would improve</p><p>the validation of the model.</p>

corrected abstract:
<p>Nordea, being the largest corporate group of its kind in Northern Europe, has a great need of evaluating its customers ability to repay a debt as well as the probability of bankruptcy. The evaluation is done by different statistically derived internal rating models, based on logistic regression. The models have been developed by the use of historical data and attain good predictiveness when a lot of observational data is provided for each specific customer. In order to ameliorate the rating models, Nordea wants to implement two new validation methods, recommended by the reputable credit rating agency Moody’s: <em>information entropy</em> and <em>accuracy ratio with simulated defaults</em>. A default is a customer either being close to or being bankrupt. <em>Information entropy</em> measures how much information is included within a given variable, while <em>accuracy ratio with simulated defaults</em> validates the ability of the model to discriminate between "good" and "bad" customers when simulating default data. The simulation is used when sufficient default data does not exist, which is the case for large corporates. After the implementation of these validation methods, for the same set of data that Moody’s were given, the results that they presented could be confirmed by the chosen implementation method. This method was then used for analysis of a general set of data and it could be concluded that the use of each validation method, recommended by Moody’s, would improve the validation of the model.</p>
----------------------------------------------------------------------
In diva2:618571 
abstract is: 
<p>This is a master thesis conducted at KTH Centre for Naval Architecture in collaboration with Seaware AB and Wallenius Marine AB.Traditionally simulation of ship motion is divided into manoeuvring and seakeeping. In manoeuvring the plane motion in surge, sway and yaw degrees of freedom for a ship considered moving in calm water is simulated. For increased accuracy the roll degree of freedom can be included as it affects the plane motion. In seakeeping ship motion due to waves at a specific speed and course in 3 to 6 DOF (degrees of freedom), depending on the area of interest, is simulated.The motion that a ship undergoes at sea is however dependent on the interaction between the forces and moments due to waves as well as the forces and moments related to ship manoeuvring. Furthermore, analysis on e.g. methods for counteracting roll motion in waves with rudder movement requires the modelling of forces and moments due to waves and manoeuvring in several DOF. It is therefore desirable to develop a unified model that describes ship motion in several DOF with respect to waves and the effects of manoeuvring.To create a mathematical model, written in MATLAB script, for simulation of ship motion due to waves and the manoeuvring related forces in 5 DOF, a wave induced (Ovegård 2009) and manoeuvrability (Zachrissson 2011) ship motion model were integrated.The code was validated against a linear strip theory, a non-linear ship motion model as well as model experimental results. Results have shown that whilst the heave and pitch motions agree with the models and tank tests the roll motion is seen, in some cases, as creating larger response in comparison to other models and experimental tank tests.</p>

corrected abstract:
<p>This is a master thesis conducted at KTH Centre for Naval Architecture in collaboration with Seaware AB and Wallenius Marine AB.</p><p>Traditionally simulation of ship motion is divided into manoeuvring and seakeeping. In manoeuvring the plane motion in surge, sway and yaw degrees of freedom for a ship considered moving in calm water is simulated. For increased accuracy the roll degree of freedom can be included as it affects the plane motion. In seakeeping ship motion due to waves at a specific speed and course in 3 to 6 DOF (degrees of freedom), depending on the area of interest, is simulated.</p><p>The motion that a ship undergoes at sea is however dependent on the interaction between the forces and moments due to waves as well as the forces and moments related to ship manoeuvring. Furthermore, analysis on e.g. methods for counteracting roll motion in waves with rudder movement requires the modelling of forces and moments due to waves and manoeuvring in several DOF. It is therefore desirable to develop a unified model that describes ship motion in several DOF with respect to waves and the effects of manoeuvring.</p><p>To create a mathematical model, written in MATLAB script, for simulation of ship motion due to waves and the manoeuvring related forces in 5 DOF, a wave induced (Ovegård 2009) and manoeuvrability (Zachrissson 2011) ship motion model were integrated.</p><p>The code was validated against a linear strip theory, a non-linear ship motion model as well as model experimental results. Results have shown that whilst the heave and pitch motions agree with the models and tank tests the roll motion is seen, in some cases, as creating larger response in comparison to other models and experimental tank tests.</p>
----------------------------------------------------------------------
In diva2:612216 
abstract is: 
<p>Rotary machinery systems are widely used in different industries. As they have rotational movements, the resonances will be the function of rotational speed. Nowadays, there are different CAE (Computer Aided Engineering ) softwares that use finite element method to solve complex problems by simplifying the model to a limited DOF (Degree Of Freedom) system. Some of the CAE software have built-in module for rotary machinery to solve eigenvalue problems, unbalance response, stability, etc. COMSOL multiphysics 1 is a CAE software that can deal with structural mechanics. There is not any specific module for the rotor dynamics analysis in COMSOL multiphysics version 4.2a; however, It is possible to make a model based on 1D beam elements or even structural mechanics elements and solve the problem by using equivalent gyroscopic moment and unbalance force terms. The scope of this thesis is based on 1D beam element solution. Moreover, the result of the COMSOL solution has been compared with ANSYS based on 3D element 2.</p><p>In this thesis, the main purpose is implementing rotor dynamics concepts in COMSOL multiphysics based on beam element 3 . Second chapter is focused on the theory part of the rotordynamics where the equations in the stationary cases are derived which measn the rotational speed of the rotor does not change by time; moreover, a brief overview of finite element method has been mentioned. The third chapter covers the implementation of the rotor dynamics in COMSOL where a very brief overview of different COMSOL’s sections that are used in this thesis are described; Moreover, load implementation , 3D mapping based on extrusion integration and comparison a result data with ANSYS are mentioned. In the fourth chapter rotordynamics analysis is described based on Campbell plot 4, stability analysis and harmonic response from a model that is made in COMSOL.</p><p>Final chapter is the conclusion part about this thesis based on the results that have been shown.</p>

corrected abstract:
<p>Rotary machinery systems are widely used in different industries. As they have rotational movements, the resonances will be the function of rotational speed. Nowadays, there are different CAE (Computer Aided Engineering ) softwares that use finite element method to solve complex problems by simplifying the model to a limited DOF (Degree Of Freedom) system. Some of the CAE software have built-in module for rotary machinery to solve eigenvalue problems, unbalance response, stability, etc. COMSOL multiphysics<sup><a href="#fn1" id="ref1">1</a></sup> is a CAE software that can deal with structural mechanics. There is not any specific module for the rotor dynamics analysis in COMSOL multiphysics version 4.2a; however, It is possible to make a model based on 1D beam elements or even structural mechanics elements and solve the problem by using equivalent gyroscopic moment and unbalance force terms. The scope of this thesis is based on 1D beam element solution. Moreover, the result of the COMSOL solution has been compared with ANSYS based on 3D element<sup><a href="#fn2" id="ref2">2</a></sup>.</p><p>In this thesis, the main purpose is implementing rotor dynamics concepts in COMSOL multiphysics based on beam element<sup><a href="#fn3" id="ref3">3</a></sup>. Second chapter is focused on the theory part of the rotordynamics where the equations in the stationary cases are derived which measn the rotational speed of the rotor does not change by time; moreover, a brief overview of finite element method has been mentioned. The third chapter covers the implementation of the rotor dynamics in COMSOL where a very brief overview of different COMSOL’s sections that are used in this thesis are described; Moreover, load implementation , 3D mapping based on extrusion integration and comparison a result data with ANSYS are mentioned. In the fourth chapter rotordynamics analysis is described based on Campbell plot<sup><a href="#fn4" id="ref4">4</a></sup>, stability analysis and harmonic response from a model that is made in COMSOL. Final chapter is the conclusion part about this thesis based on the results that have been shown.</p>

<div id="footnotes">
    <ol>
        <li id="fn1">In this thesis COMSOL version 4.2a has been used <a href="#ref1" aria-label="Back to reference">↩</a></ĺi>
        <li id="fn2">ANSYS version 13 has been used for comparison the results with COMSOL <a href="#ref2" aria-label="Back to reference">↩</a></li>
        <li id="fn3">It is possible to use shell or solid elements to have a better result in dynamic behavior of the rotor which is out of the scope of this master thesis. <a href="#ref3 aria-label="Back to reference"">↩</a></li>
        <li id="fn4">Generating Campbell plot based on eigenvalue solutions , requires a post processing code that is done by a Matlab script available in the appendix part. <a href="#ref4" aria-label="Back to reference">↩</a></li>
    </ol>
</div>
----------------------------------------------------------------------
In diva2:516081 
abstract is: 
<p>Every organization goes through the same lifecycle. Through every step in the transformation to a better organization the communication structure changes. In this report there are examples of the communicational disadvantages that is connected to the communicational structure.By choosing to not have a formal communicational design, the manager takes a risk in spreading the information, the interaction, the control and the balance in creativity and constraint.This case organizations problem is that some parts of the organization has made more progress in the communicational structure than other parts. In this case the Collectivity structure has met the Formalized structure.Tha structural differences has came to affect the way of sharing information and it has become a situation where the information about the products is not stored in a safe way from a quality point of view. There is a gap between the two different ways of communicate.In this report I have created a model to identify problems in the communication structure. This is a tool to study the rest of the organization. This model is general and may be used on any organization.</p>

corrected abstract:
<p>Every organization goes through the same lifecycle. Through every step in the transformation to a better organization the communication structure changes. In this report there are examples of the communicational disadvantages that is connected to the communicational structure.</p><p>By choosing to not have a formal communicational design, the manager takes a risk in spreading the information, the interaction, the control and the balance in creativity and constraint.</p><p>This case organizations problem is that some parts of the organization has made more progress in the communicational structure than other parts. In this case the Collectivity structure has met the Formalized structure.</p><p>Tha structural differences has came to affect the way of sharing information and it has become a situation where the information about the products is not stored in a safe way from a quality point of view. There is a gap between the two different ways of communicate.</p><p>In this report I have created a model to identify problems in the communication structure. This is a tool to study the rest of the organization. This model is general and may be used on any organization.</p>
----------------------------------------------------------------------
In diva2:515507 
abstract is: 
<p>This study is based on the development of a new software, which is used as a recommendationtool for selecting the best product. As part of the software development has an external partnerbeen asked to assist. The report will process the expectations that different people have on anew project but also how information is communicated to an external partner.The results from this study show that a software's function is closely linked to its usability.Therefore shall usability been taken into account when a new resource is created. The study alsoshows the importance of being able to share information, on how the program will beconstituted, to someone outside the organization.</p>

corrected abstract:
<p>This study is based on the development of a new software, which is used as a recommendation tool for selecting the best product. As part of the software development has an external partner been asked to assist. The report will process the expectations that different people have on a new project but also how information is communicated to an external partner.</p><p>The results from this study show that a software's function is closely linked to its usability. Therefore shall usability been taken into account when a new resource is created. The study also shows the importance of being able to share information, on how the program will be constituted, to someone outside the organization.</p>
----------------------------------------------------------------------
In diva2:515492 
abstract is: 
<p>The user centered design process is a framework built on methods aiming at including the user in the design process. When designing a system that is going to be used by persons suffering from severe disabilities, a number of unique difficulties are introduced.This thesis investigates what particular issues that has to be considered when designing for eye tracking based interaction and how user inclusion can be achieved despite the presence of severe disabilities.Persons from three habilitation centers in the Stockholm area were included in the study which was implemented as an iterative design process, including a number of methods common within user centered design.The result of this study indicates that many of the well-recognized methods in user centered design can be used when including users with cerebral palsy. Focus is shifted slightly away from the user towards other persons in the vicinity. To achieve high usability the system must facilitate individual customizations of the interface regarding the colors, sizes and placements of objects. A method of two step selection, reducing the numbers of unintentional selections, is also presented.</p>

corrected abstract:
<p>The user centered design process is a framework built on methods aiming at including the user in the design process. When designing a system that is going to be used by persons suffering from severe disabilities, a number of unique difficulties are introduced.</p><p>This thesis investigates what particular issues that has to be considered when designing for eye tracking based interaction and how user inclusion can be achieved despite the presence of severe disabilities.</p><p>Persons from three habilitation centers in the Stockholm area were included in the study which was implemented as an iterative design process, including a number of methods common within user centered design.</p><p>The result of this study indicates that many of the well-recognized methods in user centered design can be used when including users with cerebral palsy. Focus is shifted slightly away from the user towards other persons in the vicinity. To achieve high usability the system must facilitate individual customizations of the interface regarding the colors, sizes and placements of objects. A method of two step selection, reducing the numbers of unintentional selections, is also presented.</p>
----------------------------------------------------------------------
In diva2:1904432 
abstract is: 
<p>This thesis explores the digital transformation of the Swedish railway industry, a critical component of the nation's undergoing significant changes in the digital era. It focuses on understanding the diverse perspectives of stakeholders involved in this transformation and the challenges and opportunities that arise from the integration of advanced digital technologies. The study employs Natural Language Processing (NLP) and text mining techniques to analyse unstructured text data, primarily from questionnaires, providing a nuanced understanding of stakeholder perceptions and expectations.</p><p>The research highlights the complexities of digital transformation in an industry traditionally rooted in safety and efficiency, emphasising the need for a balance between technological advancements and established protocols. It investigates the varying degrees of digital maturity across different organisations within the industry, examining how these variances impact the overall pace of digital transformation and the adoption of new technologies and tools.</p><p>A significant aspect of this study is its focus on the human and organisational elements of digital transformation, addressing the cultural shift required for successful implementation. It delves into the skill gaps and learning opportunities identified by stakeholders, underscoring the importance of upskilling and adaptability in a rapidly evolving digital landscape. The thesis also explores the implications of digital transformation for collaboration and innovation within the industry, considering the role of organisational structures and business models in facilitating this transition.</p><p>Moreover, the research emphasises the strategic imperative of digitalisation for the Swedish railway industry, not just for modernisation but for future-proofing the sector against evolving global trends towards sustainable and smart transportation systems. It argues for a comprehensive approach to digital transformation, one that is deeply understood and supported by all stakeholders, ensuring that the transition is not only technologically sound but also socially and economically viable.</p>

corrected abstract:
<p>This thesis explores the digital transformation of the Swedish railway industry, a critical component of the nation's undergoing significant changes in the digital era. It focuses on understanding the diverse perspectives of stakeholders involved in this transformation and the challenges and opportunities that arise from the integration of advanced digital technologies. The study employs Natural Language Processing (NLP) and text mining techniques to analyse unstructured text data, primarily from questionnaires, providing a nuanced understanding of stakeholder perceptions and expectations.</p><p>The research highlights the complexities of digital transformation in an industry traditionally rooted in safety and efficiency, emphasising the need for a balance between technological advancements and established protocols. It investigates the varying degrees of digital maturity across different organisations within the industry, examining how these variances impact the overall pace of digital transformation and the adoption of new technologies and tools.</p><p>A significant aspect of this study is its focus on the human and organisational elements of digital transformation, addressing the cultural shift required for successful implementation. It delves into the skill gaps and learning opportunities identified by stakeholders, underscoring the importance of upskilling and adaptability in a rapidly evolving digital landscape. The thesis also explores the implications of digital transformation for collaboration and innovation within the industry, considering the role of organisational structures and business models in facilitating this transition.</p><p>Moreover, the research emphasises the strategic imperative of digitalisation for the Swedish railway industry, not just for modernisation but for future-proofing the sector against evolving global trends towards sustainable and smart transportation systems. It argues for a comprehensive approach to digital transformation, one that is deeply understood and supported by all stakeholders, ensuring that the transition is not only technologically sound but also socially and economically viable.</p>
----------------------------------------------------------------------
In diva2:1900895 
abstract is: 
<p>This master thesis was performed in collaboration with Öhlins Racing AB, a company that develops advanced suspension technologies for automotive and motorcycle applications, mostly within the motorsport sector. Shock absorbers, which highly influence the vehicle behaviour and thus the global performances of the vehicle, are all the more important for off-road applications. In the past years, Öhlins Racing mostly focused on on-track applications for its automotive sector, but it recently showed its will to be a powerhouse in the off- road sector. The request from Öhlins Racing was to investigate and optimise the performances of off-road vehicle dampers.This research project started by looking into the modelling of off-road tracks and the modelling of dampers. The next step was to investigate which vehicle performances were relevant for off-roading and which metrics were necessary to evaluate them. Five tests were conducted in order to analyse the following performances: chassis stability, bump absorption, jump, acceleration/braking and steering. For each one of them, the optimised damper parameters were found and their effects on the vehicle’s performances were studied. A comparison has then been conducted in order to analyse the acceleration/braking and steering performances of a vehicle optimised for the chassis stability test and for the jump test. The influence of the soft soil was evaluated by conducting a test that involved the vehicle running on rigid soil and on soft soil (dry sand). The influence of the damper’s internal friction was also evaluated by conducting tests with and without friction.The chassis stability test, which consists of driving on an uneven “dune” profile, showed that a soft setup is better for pitch dynamics but much worse for roll dynamics. On the other hand, a setup deemed too stiff induces high damping forces. When facing a bump or a jump, a vehicle equipped with soft damper settings risks, in addition of having a longer settling time, to hit the bump stops. A setting that is too stiff results in high damping forces and high damper velocities and vertical acceleration of the chassis. Low compression damping is used to keep the damping forces low enough. For the jump, a high rebound damping was found necessary to optimise the landing and to avoid a bounce back. Concerning the acceleration and braking behaviour, it was found that the dampers almost exclusively work in the low-speed region and that a soft setup results in many oscillations while a stiff setup slows down the response. The same effect was established for the steering response: a comparison showed that the chassis stability optimisation is too soft and results in too many oscillations for the acceleration/braking and steering responses, while the jump setting seems to have a behaviour similar to the optimised settings for these tests. The soft soil test showed that the soft soil is acting as if an extra damping was added to the vehicle, with damping forces and velocities being greatly reduced. Finally, it was shown that the internal friction of the damper does not influence its performances, as the friction forces are kept low by Öhlins Racing’s technologies.The study and modelling of the soft soil was quite complicated due to complexity of the field and the lack of concrete studies about its application to vehicle simulations. The soft soil model developed in this thesis is limited and can thus be improved by future works.</p>

corrected abstract:
<p>This master thesis was performed in collaboration with Öhlins Racing AB, a company that develops advanced suspension technologies for automotive and motorcycle applications, mostly within the motorsport sector. Shock absorbers, which highly influence the vehicle behaviour and thus the global performances of the vehicle, are all the more important for off-road applications. In the past years, Öhlins Racing mostly focused on on-track applications for its automotive sector, but it recently showed its will to be a powerhouse in the off-road sector. The request from Öhlins Racing was to investigate and optimise the performances of off-road vehicle dampers.</p><p>This research project started by looking into the modelling of off-road tracks and the modelling of dampers. The next step was to investigate which vehicle performances were relevant for off-roading and which metrics were necessary to evaluate them. Five tests were conducted in order to analyse the following performances: chassis stability, bump absorption, jump, acceleration/braking and steering. For each one of them, the optimised damper parameters were found and their effects on the vehicle’s performances were studied. A comparison has then been conducted in order to analyse the acceleration/braking and steering performances of a vehicle optimised for the chassis stability test and for the jump test. The influence of the soft soil was evaluated by conducting a test that involved the vehicle running on rigid soil and on soft soil (dry sand). The influence of the damper’s internal friction was also evaluated by conducting tests with and without friction.</p><p>The chassis stability test, which consists of driving on an uneven “dune” profile, showed that a soft setup is better for pitch dynamics but much worse for roll dynamics. On the other hand, a setup deemed too stiff induces high damping forces. When facing a bump or a jump, a vehicle equipped with soft damper settings risks, in addition of having a longer settling time, to hit the bump stops. A setting that is too stiff results in high damping forces and high damper velocities and vertical acceleration of the chassis. Low compression damping is used to keep the damping forces low enough. For the jump, a high rebound damping was found necessary to optimise the landing and to avoid a bounce back. Concerning the acceleration and braking behaviour, it was found that the dampers almost exclusively work in the low-speed region and that a soft setup results in many oscillations while a stiff setup slows down the response. The same effect was established for the steering response: a comparison showed that the chassis stability optimisation is too soft and results in too many oscillations for the acceleration/braking and steering responses, while the jump setting seems to have a behaviour similar to the optimised settings for these tests. The soft soil test showed that the soft soil is acting as if an extra damping was added to the vehicle, with damping forces and velocities being greatly reduced. Finally, it was shown that the internal friction of the damper does not influence its performances, as the friction forces are kept low by Öhlins Racing’s technologies.</p><p>The study and modelling of the soft soil was quite complicated due to complexity of the field and the lack of concrete studies about its application to vehicle simulations. The soft soil model developed in this thesis is limited and can thus be improved by future works.</p>
----------------------------------------------------------------------
In diva2:1899604 
abstract is: 
<p>Topology optimization, the search for the perfect material distribution allows for new and better designs. In this thesis topology optimization was used to find a lighter and better performing powertrain suspension bracket. Next to finding such a solution this project shall also pave way for topology optimization in future design processes. For that, properties that could influence the resulting topologies were investigated regarding their influence on this bracket. To do so 14 comparisons were conducted where similar models which differ in one property were compared. Additionally, to the comparisons, two models were set up, one simple model that only used approximations, while the advanced model, included the most exact properties possible.The simple model resulted in a bracket that after cleaning was 42% lighter than the original bracket. This result showed too high stress concentrations in the validation, when including nonlinear contact. The advanced model resulted in a topology that after cleaning was still 20% lighter and performed as good as the original bracket in the validation. It was however shown that even though the advanced model resulted in a better performing topology the invested time and computational resources to set up and solve the model were too high to justify. The simple model upon an additional design step after validation could show similar performance in a shorter design frame. The comparisons showed that the dominant property for this bracket is the nonlinearity of the contact within the inside of the bracket. Other important properties identified were the member size and further manufacturing constraints as well as the objective function.With this thesis, further topology optimization processes of similar parts can be conducted in a faster and more efficient manner. Already the simple model showed a significant improvement over the original part. By reducing needs for resources and saving weight on the car, which makes driving the car more energy efficient the suggested designs will benefit the society economically and ecologically. Due to the large number of possible cars where the proposed design can be implemented, the overall saving potential is huge.</p>

corrected abstract:
<p>Topology optimization, the search for the perfect material distribution allows for new and better designs. In this thesis topology optimization was used to find a lighter and better performing powertrain suspension bracket. Next to finding such a solution this project shall also pave way for topology optimization in future design processes. For that, properties that could influence the resulting topologies were investigated regarding their influence on this bracket. To do so 14 comparisons were conducted where similar models which differ in one property were compared. Additionally, to the comparisons, two models were set up, one simple model that only used approximations, while the advanced model, included the most exact properties possible.</p><p>The simple model resulted in a bracket that after cleaning was 42% lighter than the original bracket. This result showed too high stress concentrations in the validation, when including nonlinear contact. The advanced model resulted in a topology that after cleaning was still 20% lighter and performed as good as the original bracket in the validation.</p><p>It was however shown that even though the advanced model resulted in a better performing topology the invested time and computational resources to set up and solve the model were too high to justify. The simple model upon an additional design step after validation could show similar performance in a shorter design frame.</p><p>The comparisons showed that the dominant property for this bracket is the nonlinearity of the contact within the inside of the bracket. Other important properties identified were the member size and further manufacturing constraints as well as the objective function.</p><p>With this thesis, further topology optimization processes of similar parts can be conducted in a faster and more efficient manner. Already the simple model showed a significant improvement over the original part. By reducing needs for resources and saving weight on the car, which makes driving the car more energy efficient the suggested designs will benefit the society economically and ecologically. Due to the large number of possible cars where the proposed design can be implemented, the overall saving potential is huge.</p>
----------------------------------------------------------------------
In diva2:1880872 
abstract is: 
<p>This bachelor’s thesis investigates the feasibility of designing a lightweight, aero-dynamic child bike trailer using composite materials for its main structural ele-ments. The project encompasses theoretical design, finite element analysis (FEA),and computational fluid dynamics (CFD) to optimize the structure for weight re-duction and improved aerodynamics while ensuring strength. Key considerationsinclude material selection, laminate stiffness, and manufacturing methods. Thestudy provides a plan for future prototype development. Findings suggest that acomposite-based design can significantly reduce weight and enhance aerodynamicperformance compared to traditional materials and structural design.</p>

corrected abstract:
<p>This bachelor’s thesis investigates the feasibility of designing a lightweight, aerodynamic child bike trailer using composite materials for its main structural elements. The project encompasses theoretical design, finite element analysis (FEA), and computational fluid dynamics (CFD) to optimize the structure for weight reduction and improved aerodynamics while ensuring strength. Key considerations include material selection, laminate stiffness, and manufacturing methods. The study provides a plan for future prototype development. Findings suggest that a composite-based design can significantly reduce weight and enhance aerodynamic performance compared to traditional materials and structural design.</p>
----------------------------------------------------------------------
In diva2:1877620 
abstract is: 
<p>Optimisation of molecular structures in solvation is important in many fields, such as drugdesign. In order to optimise geometries, one needs nuclear energy gradients. To optimisestructures efficiently, analytical gradients are required. In this work, the analytical nucleargradients within the Conductor-like Screening Model (COSMO) for modelling solvation arederived and implemented in the quantum chemistry software VeloxChem. By validation withnumerical gradients of varying accuracy, agreement with the implemented analytical gradientwas found, demonstrating internally consistent analytical expressions.</p>

corrected abstract:
<p>Optimisation of molecular structures in solvation is important in many fields, such as drug design. In order to optimise geometries, one needs nuclear energy gradients. To optimise structures efficiently, analytical gradients are required. In this work, the analytical nuclear gradients within the Conductor-like Screening Model (COSMO) for modelling solvation are derived and implemented in the quantum chemistry software VeloxChem. By validation with numerical gradients of varying accuracy, agreement with the implemented analytical gradient was found, demonstrating internally consistent analytical expressions.</p>
----------------------------------------------------------------------
In diva2:1858209 
abstract is: 
<p>As a degree project in Theoretical Physics, the variational MCMC-scheme aided by neural network quantum states was examined for the purpose ofsolving the nuclear pairing model. The method entailed minimization of the local energy sampled via the Born distribution obtained through the neural network output.Both the ground and excited states' energies were computed, where the latter case used an extended loss function which included the overlap to the former.The NNQS-ansatz worked well when emulating the ground state, in which case the Stochastic Reconfiguration optimization method was particularly effective. This optimization method resulted in relative fast convergence to low variance states, and did not require a large number of hyperparameter modifications. Ultimately, all resulting energy intervals encompassed the exact ground state solutions, and had relative errors equal to or near zero.For the excited states, the VMC-NNQS was less effective, as each individual occupation number state investigated required considerable hyperparameter testing before reasonably low lying energy eigenstates could be obtained. Moreover, the convergence properties were less distinguished than for the ground state, as the optimization struggled to maintain orthogonality to the ground state. Nonetheless, the final results included the nearest solutions of the first excited states for several systems and indicated correlation energies similar to those of the ground state.</p>

corrected abstract:
<p>As a degree project in Theoretical Physics, the variational MCMC-scheme aided by neural network quantum states was examined for the purpose of solving the nuclear pairing model. The method entailed minimization of the local energy sampled via the Born distribution obtained through the neural network output. Both the ground and excited states’ energies were computed, where the latter case used an extended loss function which included the overlap to the former. The NNQS-ansatz worked well when emulating the ground state, in which case the Stochastic Reconfiguration optimization method was particularly effective. This optimization method resulted in relative fast convergence to low variance states, and did not require a large number of hyperparameter modifications. Ultimately, all resulting energy intervals encompassed the exact ground state solutions, and had relative errors equal to or near zero. For the excited states, the VMC-NNQS was less effective, as each individual occupation number state investigated required considerable hyperparameter testing before reasonably low lying energy eigenstates could be obtained. Moreover, the convergence properties were less distinguished than for the ground state, as the optimization struggled to maintain orthogonality to the ground state. Nonetheless, the final results included the nearest solutions of the first excited states for several systems and indicated correlation energies similar to those of the ground state.</p>
----------------------------------------------------------------------
In diva2:1833665 
abstract is: 
<p>The purpose of this paper is to examine if there exists a statistically significantcorrelation between crime and the housing price in Stockholm. This was madeby applying and developing a multiple regression model. The data was gatheredfrom Svensk Mäklarstatistik and Brottsförebyggande Rådet. The analysis coversthe period 2019-2021 in the areas Kungsholmen, Rinkeby-Kista, Farsta andSkärholmen. The results showed a statistically significant correlation betweencrime and housing prices and it showed that crime had a negative effect.</p>

corrected abstract:
<p>The purpose of this paper is to examine if there exists a statistically significant correlation between crime and the housing price in Stockholm. This was made by applying and developing a multiple regression model. The data was gathered from <span lang="sv">Svensk Mäklarstatistik</span> and <span lang="sv">Brottsförebyggande Rådet</span>. The analysis covers the period 2019-2021 in the areas Kungsholmen, Rinkeby-Kista, Farsta and Skärholmen. The results showed a statistically significant correlation between crime and housing prices and it showed that crime had a negative effect.</p>
----------------------------------------------------------------------
In diva2:1833665 
abstract is: 
<p>The purpose of this paper is to examine if there exists a statistically significantcorrelation between crime and the housing price in Stockholm. This was madeby applying and developing a multiple regression model. The data was gatheredfrom Svensk Mäklarstatistik and Brottsförebyggande Rådet. The analysis coversthe period 2019-2021 in the areas Kungsholmen, Rinkeby-Kista, Farsta andSkärholmen. The results showed a statistically significant correlation betweencrime and housing prices and it showed that crime had a negative effect.</p>


corrected abstract:
<p>The purpose of this paper is to examine if there exists a statistically significant correlation between crime and the housing price in Stockholm. This was made by applying and developing a multiple regression model. The data was gathered from <span lang="sv">Svensk Mäklarstatistik</span> and <span lang="sv">Brottsförebyggande Rådet</span>. The analysis covers the period 2019-2021 in the areas Kungsholmen, Rinkeby-Kista, Farsta and Skärholmen. The results showed a statistically significant correlation between crime and housing prices and it showed that crime had a negative effect.</p>
----------------------------------------------------------------------
In diva2:1819058 
abstract is: 
<p>The increasing demand for energy, depletion of fossil fuels, rising global warming, and greenhouse gas emissions have stimulated the need for widespread development and adoption of renewable energy sources (RES) worldwide. Among these sources, solar energy has emerged as a major contender to meet the growing demand. It offers adaptable applications and provides an alternative to traditional energy sources. A brand-new application of solar panels is agrivoltaics. Agrivoltaics consists in installing solar panels above farming lands such as crops. The combination of solar energy production and farming on the same lands increases the overall yield of the land and brings several other opportunities. However, agrivoltaics is also very challenging. An improper installation of solar panels above crops may result in a dramatic drop of the farming yield. Thus, it is of major importance to understand how to maximize the solar energy production without harming the plants or decrease the farming yield. This master’s thesis focuses on the impact of agrivoltaic systems on the micro-climate close to the crop. The goal is to link the modified physical phenomena within an agrivoltaic system and their impact on the crops. The methodology is based on Computational Fluid Dynamics (CFD). The idea is to realize high fidelity simulations of the different physical phenomena and their coupling, and compare them to experimental data. Flow simulations coupled with radiative models and a surface model are realized in this perspective. The master’s thesis is divided in three parts. 1. Based on experimental data collected during three years at the EDF lab les Renardières, determine which physical phenomena impact the most the crop and what are the key parameters to study the growth of the plants. 2. Validate with experimental data from the atmospheric laboratory the SIRTA (Site Instrumental de Recherche par Télédétection Atmosphérique) of the engineering school Polytechnique, the radiative models and the surface model of the CFD software. 3. Study the impact of an agrivoltaic system on the identified physical phenomena with a simple geometry composed of one pitch of solar panel. The data study shows clearly that the plant temperature, the groundwater, and the radiation play crucial roles in the growth of the plant. A lack of radiation or groundwater will limit the growth of the crops. In addition, extreme temperatures can harm the crops. Consequently, this research project will firstly focus on capturing the impact of the solar panels on these three key parameters. Simulations are using a coupling of a 1D radiative model which is computationally fast and that can therefore be applied on a very large domain to compute the absorption of the atmospheric layers and the clouds, and a 3D radiative model which is able to capture the impact of an obstacle such as a solar panel. This coupling is validated for the shortwave radiation and the longwave radiation. Finally, full U-RANS simulations with the radiative models, the surface model and the - turbulence model are realized. The impact of the panels on the radiation field, the soil temperature, the specific humidity and on other fields such as the wind speed is well captured.</p>

corrected abstract:
<p>The increasing demand for energy, depletion of fossil fuels, rising global warming, and greenhouse gas emissions have stimulated the need for widespread development and adoption of renewable energy sources (RES) worldwide. Among these sources, solar energy has emerged as a major contender to meet the growing demand. It offers adaptable applications and provides an alternative to traditional energy sources. A brand-new application of solar panels is agrivoltaics. Agrivoltaics consists in installing solar panels above farming lands such as crops. The combination of solar energy production and farming on the same lands increases the overall yield of the land and brings several other opportunities.</p><p>However, agrivoltaics is also very challenging. An improper installation of solar panels above crops may result in a dramatic drop of the farming yield. Thus, it is of major importance to understand how to maximize the solar energy production without harming the plants or decrease the farming yield. This master’s thesis focuses on the impact of agrivoltaic systems on the micro-climate close to the crop. The goal is to link the modified physical phenomena within an agrivoltaic system and their impact on the crops. The methodology is based on Computational Fluid Dynamics (CFD). The idea is to realize high fidelity simulations of the different physical phenomena and their coupling, and compare them to experimental data. Flow simulations coupled with radiative models and a surface model are realized in this perspective.</p><p>The master’s thesis is divided in three parts. 1. Based on experimental data collected during three years at the EDF lab les Renardières, determine which physical phenomena impact the most the crop and what are the key parameters to study the growth of the plants. 2. Validate with experimental data from the atmospheric laboratory the SIRTA (Site Instrumental de Recherche par Télédétection Atmosphérique) of the engineering school Polytechnique, the radiative models and the surface model of the CFD software. 3. Study the impact of an agrivoltaic system on the identified physical phenomena with a simple geometry composed of one pitch of solar panel.</p><p>The data study shows clearly that the plant temperature, the groundwater, and the radiation play crucial roles in the growth of the plant. A lack of radiation or groundwater will limit the growth of the crops. In addition, extreme temperatures can harm the crops. Consequently, this research project will firstly focus on capturing the impact of the solar panels on these three key parameters. Simulations are using a coupling of a 1D radiative model which is computationally fast and that can therefore be applied on a very large domain to compute the absorption of the atmospheric layers and the clouds, and a 3D radiative model which is able to capture the impact of an obstacle such as a solar panel. This coupling is validated for the shortwave radiation and the longwave radiation. Finally, full U-RANS simulations with the radiative models, the surface model and the 𝑘-&#x1D700; turbulence model are realized. The impact of the panels on the radiation field, the soil temperature, the specific humidity and on other fields such as the wind speed is well captured.</p>
----------------------------------------------------------------------
In diva2:1800259 
abstract is: 
<p>This thesis investigates the robustness and stability of total return series for credit bond index investments. Dueto the challenges which arise for financial institutes and investors in achieving these objectives, we aim to createa forecasting model which matches the statistical properties of historical data, while remaining robust, stable andeasy to calibrate. To reach this goal, we implement autoregressive time-series models for credit spreads, a Vasicekmodel for the interest rate and use transformations to create total return series. We find that our autoregressivemodel performs well in terms of robustness and stability, while being statistically accurate for the Investment GradeIndex. The High Yield model has good statistical accuracy, but is lacking in stability and robustness.</p>

corrected abstract:
<p>This thesis investigates the robustness and stability of total return series for credit bond index investments. Due to the challenges which arise for financial institutes and investors in achieving these objectives, we aim to create a forecasting model which matches the statistical properties of historical data, while remaining robust, stable and easy to calibrate. To reach this goal, we implement autoregressive time-series models for credit spreads, a Vasicek model for the interest rate and use transformations to create total return series. We find that our autoregressive model performs well in terms of robustness and stability, while being statistically accurate for the Investment Grade Index. The High Yield model has good statistical accuracy, but is lacking in stability and robustness.</p>
----------------------------------------------------------------------
In diva2:1780218 
abstract is: 
<p>This report investigates how one can construct soliton solutions to the following solitonequations: the Korteweg-de Vries equation, the Benjamin-Ono equation, and the spinBenjamin-Ono equation by making rational pole ansätze, which are ansätze that dependon eponymous pole parameters moving in the complex plane. In doing so we demonstratea connection between these soliton solutions and the class of integrable dynamical systemsknown as Calogero-Moser systems. We find that the ansätze solves the given solitonequation if the motion of the poles is governed by an integrable dynamical system relatedto the Calogero-Moser systems. Additionally, we numerically implement and analyse thesoliton solutions constructed for the Korteweg-de Vries and Benjamin-Ono equations.</p><p> </p>

corrected abstract:
<p>This report investigates how one can construct soliton solutions to the following soliton equations: the Korteweg-de Vries equation, the Benjamin-Ono equation, and the spin Benjamin-Ono equation by making rational pole ansätze, which are ansätze that depend on eponymous pole parameters moving in the complex plane. In doing so we demonstrate a connection between these soliton solutions and the class of integrable dynamical systems known as Calogero-Moser systems. We find that the ansätze solves the given soliton equation if the motion of the poles is governed by an integrable dynamical system related to the Calogero-Moser systems. Additionally, we numerically implement and analyse the soliton solutions constructed for the Korteweg-de Vries and Benjamin-Ono equations.</p>
----------------------------------------------------------------------
In diva2:1775400 
abstract is: 
<p>This study aimed to explore the application of artificial intelligence (AI) and machine learning (ML) techniques in implementing a QRS detector forambulatory electrocardiography (ECG) monitoring devices. Three ML models, namely long short-term memory (LSTM), convolutional neural network (CNN), and multilayer perceptron (MLP), were compared and evaluated using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH noise stress test database (NSTDB). The MLP model consistently outperformed the other models, achieving high accuracy in R-peak detection. However, when tested on noisy data, all models faced challenges in accurately predicting R-peaks, indicating the need for further improvement.</p><p>To address this, the study emphasized the importance of iteratively refining the input data configurations for achieving accurate R-peak detection. By incorporating both the MITDB and NSTDB during training, the models demonstrated improved generalization to noisy signals. This iterative refinement process allowed for the identification of the best models and configurations, consistently surpassing existing ML-based implementations and outperforming the current ECG analysis system.</p><p>The MLP model, without shifting segments and utilizing both datasets, achieved an outstanding accuracy of 99.73 % in R-peak detection. This accuracy exceeded values reported in the literature, demonstrating the superior performance of this approach. Furthermore, the shifted MLP model, which considered temporal dependencies by incorporating shifted segments, showed promising results with an accuracy of 99.75 %. It exhibited enhanced accuracy, precision, and F1-score compared to the other models, highlighting the effectiveness of incorporating shifted segments.</p><p>For future research, it is important to address challenges such as overfitting and validate the models on independent datasets. Additionally, continuous refinement and optimization of the input data configurations will contribute to further advancements in ECG signal analysis and improve the accuracy of R-peak detection. This study underscores the potential of ML techniques in enhancing ECG analysis, ultimately leading to improved cardiac diagnostics and better patient care.</p>

corrected abstract:
<p>This study aimed to explore the application of artificial intelligence (AI) and machine learning (ML) techniques in implementing a QRS detector for ambulatory electrocardiography (ECG) monitoring devices. Three ML models, namely long short-term memory (LSTM), convolutional neural network (CNN), and multilayer perceptron (MLP), were compared and evaluated using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH noise stress test database (NSTDB). The MLP model consistently outperformed the other models, achieving high accuracy in R-peak detection. However, when tested on noisy data, all models faced challenges in accurately predicting R-peaks, indicating the need for further improvement.</p><p>To address this, the study emphasized the importance of iteratively refining the input data configurations for achieving accurate R-peak detection. By incorporating both the MITDB and NSTDB during training, the models demonstrated improved generalization to noisy signals. This iterative refinement process allowed for the identification of the best models and configurations, consistently surpassing existing ML-based implementations and outperforming the current ECG analysis system.</p><p>The MLP model, without shifting segments and utilizing both datasets, achieved an outstanding accuracy of 99.73 % in R-peak detection. This accuracy exceeded values reported in the literature, demonstrating the superior performance of this approach. Furthermore, the shifted MLP model, which considered temporal dependencies by incorporating shifted segments, showed promising results with an accuracy of 99.75 %. It exhibited enhanced accuracy, precision, and F1-score compared to the other models, highlighting the effectiveness of incorporating shifted segments.</p><p>For future research, it is important to address challenges such as overfitting and validate the models on independent datasets. Additionally, continuous refinement and optimization of the input data configurations will contribute to further advancements in ECG signal analysis and improve the accuracy of R-peak detection. This study underscores the potential of ML techniques in enhancing ECG analysis, ultimately leading to improved cardiac diagnostics and better patient care.</p>
----------------------------------------------------------------------
In diva2:1741662 
abstract is: 
<p>Future commercial applications making use of non-classical light sources require bright quantum emitters. Such emitters can be achieved by embedding self-assembled, optically active semiconductor quantum dots (QDs) in centered photonic structures. Generation of nearly pure single-photon states as well as highly indistinguishable and strongly entangled photon pairs has been demonstrated with self-assembled QDs. However, these QDs form at random locations on the samples during the growth process. QD positioning therefore becomes a necessity in order to obtain a high yield of centered photonic structures. The spatial matching allows for a strong interaction between individual QDs and greatly confined optical modes. This thesis focuses on optical positioning methods both at 800 nm and in the telecom C-band (1530 nm - 1565 nm), which is the ideal wavelength range for long distance quantum communication. A modular optical setup is built and the data acquisition is fully automated on LabVIEW. In its primary application, the setup can be used for cryogenic micro-photoluminescence (PL) measurements at 800 and 1550 nm. A scanning mirror showing low hysteresis characteristics, whose advantages over piezo actuators are discussed, is introduced in the setup to perform PL mapping and QD positioning on the sample. A telecentric optical system is also added to minimize the distorsions due to the slight misalignments induced by the use of the scanning mirror. PL mapping is then performed on QD samples on which metallic alignment marks have been fabricated, where the markers act as a reference for the positioning of the QDs. In our scanning approach, a PL spectrum is acquired at each scanned position, resulting in a PL map of the scanned surface. Data visualization techniques are thus required, and convenient contrast and map wavelength control are implemented as part of this work. Furthermore, a study is conducted to reach an optimal illumination scheme enabling simultaneous detection of the QDs and the alignment marks. A two-colour illumination scheme is chosen, consisting in a HeNe laser for QD excitation and an incoherent LED for marker detection. Once the PL maps are acquired, an advanced image analysis method based on cross-correlation calculations is employed for detection and positioning of the alignment marks, along with two-dimensional Gaussian fits for the positioning of the imaged QDs. The scanning microscope shows an overall positioning uncertainty under 10 nm in the near infrared (NIR) range, providing a proof of concept for telecom QD mapping and positioning, since the design is fully transposable at telecom wavelength. Additionally, as self-assembled QDs have random emission wavelengths, the acquisition of the positioned QDs' PL spectra presents a great benefit for individual photonic structure tuning and optimal mode overlap. Finally, a supplementary optical path for direct camera imaging is modularly added, in order to assess the positioning performances of the scanning microscope. The analysis of the camera images gives a positioning uncertainty around 3 nm, exceeding the precision of current state of the art positioning setups and setting a new standard in the NIR range.</p>

corrected abstract:
<p>Future commercial applications making use of non-classical light sources require bright quantum emitters. Such emitters can be achieved by embedding self-assembled, optically active semiconductor quantum dots (QDs) in centered photonic structures. Generation of nearly pure single-photon states as well as highly indistinguishable and strongly entangled photon pairs has been demonstrated with self-assembled QDs. However, these QDs form at random locations on the samples during the growth process. QD positioning therefore becomes a necessity in order to obtain a high yield of centered photonic structures. The spatial matching allows for a strong interaction between individual QDs and greatly confined optical modes. This thesis focuses on optical positioning methods both at 800 nm and in the telecom C-band (1530 nm - 1565 nm), which is the ideal wavelength range for long distance quantum communication. A modular optical setup is built and the data acquisition is fully automated on LabVIEW. In its primary application, the setup can be used for cryogenic micro-photoluminescence (PL) measurements at 800 and 1550 nm. A scanning mirror showing low hysteresis characteristics, whose advantages over piezo actuators are discussed, is introduced in the setup to perform PL mapping and QD positioning on the sample. A telecentric optical system is also added to minimize the distorsions due to the slight misalignments induced by the use of the scanning mirror. PL mapping is then performed on QD samples on which metallic alignment marks have been fabricated, where the markers act as a reference for the positioning of the QDs. In our scanning approach, a PL spectrum is acquired at each scanned position, resulting in a PL map of the scanned surface. Data visualization techniques are thus required, and convenient contrast and map wavelength control are implemented as part of this work. Furthermore, a study is conducted to reach an optimal illumination scheme enabling simultaneous detection of the QDs and the alignment marks. A two-colour illumination scheme is chosen, consisting in a HeNe laser for QD excitation and an incoherent LED for marker detection. Once the PL maps are acquired, an advanced image analysis method based on cross-correlation calculations is employed for detection and positioning of the alignment marks, along with two-dimensional Gaussian fits for the positioning of the imaged QDs. The scanning microscope shows an overall positioning uncertainty under 10 nm in the near infrared (NIR) range, providing a proof of concept for telecom QD mapping and positioning, since the design is fully transposable at telecom wavelength. Additionally, as self-assembled QDs have random emission wavelengths, the acquisition of the positioned QDs' PL spectra presents a great benefit for individual photonic structure tuning and optimal mode overlap. Finally, a supplementary optical path for direct camera imaging is modularly added, in order to assess the positioning performances of the scanning microscope. The analysis of the camera images gives a positioning uncertainty around 3 nm, exceeding the precision of current state of the art positioning setups and setting a new standard in the NIR range.</p>
----------------------------------------------------------------------
In diva2:1736958 
abstract is: 
<p>We study a class of It\={o} diffusion processes on domains with smooth boundary, at which the process is killed. Such a process, when conditioned on non-extinction, gives rise to a stationary state known as a \emph{quasistationary distribution} (QSD). Let $\mathcal{L}$ be the \emph{infinitesimal generator} of the \emph{Markov semigroup} of the process and let $\mathcal{L}^*$ denote the formal adjoint of $\mathcal{L}$, given by $\langle \mathcal{L} f, g \rangle=\langle f, \mathcal{L}^* g \rangle$ for suitable functions $f$ and $g$. For a killed It\={o} diffusion on a domain $G$, $\mathcal{L}^*$ is a differential operator that can be expressed analytically in terms of the drift and diffusion coefficients of the process. It is well-known that an eigenfunction of the Dirichlet eigenvalue problem\begin{align*}\mathcal{L}^* u(x) &amp;= \lambda u(x), ~~~~~~~~~~~ x \in G \\u(x) &amp;= 0, ~~~~~~~~~~~~~~~~~ x \in \partial G\end{align*}gives the unique (up to normalization) QSD of the process.</p><p>The goal of this thesis is to develop a Deep Learning-based numerical scheme to approx\hyp{}imate QSDs by exploiting the connection to the aforementioned eigenvalue problem. To this end we consider both methods based on minimizing a residual that takes account of the PDE dynamics---so called physics based methods---and schemes that use a stochastic representation of the PDE, with emphasis on the former.</p><p>As a basis for comparison, we derive analytical expressions for the QSD of a diffusion process with constant coefficients and for the Ornstein-Uhlenbeck (OU) process. In the constant coefficient case, we highlight a general problem with penalty-based methods that gives rise to \emph{spurious solutions} due to the numerical method solving a relaxed problem in practice.</p><p>Finally, we show empirically that, at least in the OU case, it is possible to obtain a good approximation of the QSD using Deep Learning.</p>

partal corrected: diva2:1736958: <p>We study a class of It\={o} diffusion processes on domains with smooth boundary, at which the process is killed. Such a process, when conditioned on non-extinction, gives rise to a stationary state known as a \emph{ quasistationary distribution} (QSD). Let $\mathcal{L}$ be the \emph{ infinitesimal generator} of the \emph{ Markov semigroup} of the process and let $\mathcal{L}^*$ denote the formal adjoint of $\mathcal{L}$, given by $\langle \mathcal{L} f, g \rangle=\langle f, \mathcal{L}^* g \rangle$ for suitable functions $f$ and $g$. For a killed It\={o} diffusion on a domain $G$, $\mathcal{L}^*$ is a differential operator that can be expressed analytically in terms of the drift and diffusion coefficients of the process. It is well-known that an eigenfunction of the Dirichlet eigenvalue problem\begin{align*}\mathcal{L}^* u(x) &amp;= \lambda u(x), ~~~~~~~~~~~ x \in G \\u(x) &amp;= 0, ~~~~~~~~~~~~~~~~~ x \in \partial G \end{align*} gives the unique (up to normalization) QSD of the process.</p><p>The goal of this thesis is to develop a Deep Learning-based numerical scheme to approx\hyp{}imate QSDs by exploiting the connection to the aforementioned eigenvalue problem. To this end we consider both methods based on minimizing a residual that takes account of the PDE dynamics---so called physics based methods---and schemes that use a stochastic representation of the PDE, with emphasis on the former.</p><p>As a basis for comparison, we derive analytical expressions for the QSD of a diffusion process with constant coefficients and for the Ornstein-Uhlenbeck (OU) process. In the constant coefficient case, we highlight a general problem with penalty-based methods that gives rise to \emph{ spurious solutions} due to the numerical method solving a relaxed problem in practice.</p><p>Finally, we show empirically that, at least in the OU case, it is possible to obtain a good approximation of the QSD using Deep Learning.</p>
w='OrnsteinUhlenbeck' val={'c': 'Ornstein-Uhlenbeck', 's': ['diva2:1761955', 'diva2:1736958', 'diva2:1761955', 'diva2:1736958', 'diva2:1859981', 'diva2:808180']}
w='OrnsteinUhlenbeck' val={'c': 'Ornstein-Uhlenbeck', 's': ['diva2:1761955', 'diva2:1736958', 'diva2:1761955', 'diva2:1736958', 'diva2:1859981', 'diva2:808180']}
w='approx\\hyp{}imate' val={'c': 'approximate', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='It\\={o}' val={'c': 'Itō', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='$\\mathcal{L}$' val={'c': 'ℒ', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='$\\mathcal{L}^*$' val={'c': 'ℒ<sup>*</sup>', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='$\\langle \\mathcal{L} f, g \\rangle=\\langle f, \\mathcal{L}^* g \\rangle$' val={'c': '〈ℒ <em>f</em>,<em>g</em>〉 = 〈<em>f</em>, ℒ<sup>*</sup><em>g</em>〉', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='$G$' val={'c': '<em>G</em>', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='\\mathcal{L}^* u(x) &amp;= \\lambda u(x)' val={'c': '<td>ℒ<sup>*</sup><em>u</em>(<em>x</em>) = λ <em>u</em>(<em>x</em>)', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='x \\in G' val={'c': '<dt><em>x</em> ∈ <em>G</em></td></tr>', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='\\u(x) &amp;= 0,' val={'c': '<em>u</em>(<em>x</em>) = 0', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='x \\in \\partial G' val={'c': '<td><em>x</em> ∈ ∂<em>G</em></td></tr>', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='\\begin{align*}' val={'c': '<table><tr>', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='\\end{align*}' val={'c': '</table>', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='~~~~~~~~~~~' val={'c': '</td>', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='~~~~~~~~~~~~~~~~~' val={'c': '</td>', 's': 'diva2:1736958', 'n': 'the latex seems to have been pasted in'}
w='dynamics---so' val={'c': 'dynamics&mdash;so', 's': 'diva2:1736958'}
w='methods---and' val={'c': 'methods&mdash;and', 's': 'diva2:1736958'}
w='\\rangle$' val={'c': '⟩', 's': 'diva2:1736958'}
w='$\\langle' val={'c': '⟨', 's': 'diva2:1736958'}
w='\\emph{quasistationary distribution}' val={'c': '<strong>quasistationary distribution</strong>', 's': 'diva2:1736958'}
w='\\emph{infinitesimal generator}' val={'c': '<strong>infinitesimal generator</strong>', 's': 'diva2:1736958'}
w='\\emph{Markov semigroup}' val={'c': '<strong>Markov semigroup</strong>', 's': 'diva2:1736958'}
w='\\emph{spurious solutions}' val={'c': '<strong>spurious solutions</strong>', 's': 'diva2:1736958'}
w='\\mathcal{L' val={'c': '&Laplacetrf;', 's': 'diva2:1736958'}
w='\\\\u' val={'c': '<br>u', 's': 'diva2:1736958'}

corrected abstract:
<p>We study a class of Itō diffusion processes on domains with smooth boundary, at which the process is killed. Such a process, when conditioned on non-extinction, gives rise to a stationary state known as a <em>quasistationary distribution</em> (QSD). Let  𝓛 be the <em>infinitesimal generator</em> of the <em>Markov semigroup</em> of the process and let 𝓛<sup>&lowast;</sup> denote the formal adjoint of 𝓛, given by &lang;𝓛𝑓, 𝑔&rang;= &lang;𝑓, 𝓛<sup>&lowast;</sup>𝑔&rang; for suitable functions 𝑓 and 𝑔. For a killed Itō diffusion on a domain 𝐺, 𝓛<sup>&lowast;</sup> is a differential operator that can be expressed analytically in terms of the drift and diffusion coefficients of the process. It is well-known that an eigenfunction of the Dirichlet eigenvalue problem</p>
<table>
    <tbody>
        <tr>
            <td style="text-align: center;"><span>𝓛</span><sup>&lowast;</sup>𝑢(𝑥) = &lambda;𝑢(𝑥),</td>
            <td>𝑥 &isin; 𝐺</td>
        </tr>
        <tr>
            <td style="text-align: center;">𝑢(𝑥) = 0,</td>
            <td>𝑥 &isin; &part;𝐺</td>
        </tr>
    </tbody>
</table>
<p>gives the unique (up to normalization) QSD of the process.</p>
<p>The goal of this thesis is to develop a Deep Learning-based numerical scheme to approximate QSDs by exploiting the connection to the aforementioned eigenvalue problem. To this end we consider both methods based on minimizing a residual that takes account of the PDE dynamics&mdash;so called physics based methods&mdash;and schemes that use a stochastic representation of the PDE, with emphasis on the former.</p>
<p>As a basis for comparison, we derive analytical expressions for the QSD of a diffusion process with constant coefficients and for the Ornstein-Uhlenbeck (OU) process. In the constant coefficient case, we highlight a general problem with penalty-based methods that gives rise to <em>spurious solutions</em> due to the numerical method solving a relaxed problem in practice.</p>
<p>Finally, we show empirically that, at least in the OU case, it is possible to obtain a good approximation of the QSD using Deep Learning.</p>

----------------------------------------------------------------------
In diva2:1729237 -- missing subscripts in title:
"Fabrication and characterization of GaAsxP1-x single junction solar cell on Si for III-V/Si tandem solar cell"
==> 
"Fabrication and characterization of GaAs<sub>x</sub>P<sub>1-x</sub> single junction solar cell on Si for III-V/Si tandem solar cell"

abstract is: 
<p>Silicon based solar cells have been used as photovoltaic devices for decades due to reasonable cost and environment- friendly nature of silicon. But the conversion efficiency of silicon solar cell is limited; for instance, the maximum conversion efficiency of a crystalline silicon solar cell available in the market developed by Kaneka Corporation is 26 % [1]. In comparison, III-V compound semiconductor multi-junction solar cells are the most efficient solar cells with efficiency of 47.1% [2]. However, due to high-cost substrate materials, III-V solar cells are not the best option for large scale production in real life. Therefore, integration of III-V compound semiconductors on silicon substrate has been studied to obtain III-V/Si multi junction solar cells with high conversion efficiency with reasonable price. To this end, we studied epitaxial growth of on GaAs deposited on Si.This thesis presents the characterization results of the above epitaxial layer and fabrication of a single junction solar cell on GaAs coated Si substrate and its performance.In the first part of the project, epitaxial layer grown by Hydride Vapor Phase Epitaxy (HVPE) on different kinds of substrates at different growth conditions are characterized to identify the optimized growth conditions and a suitable substrate. Samples are characterized by High Resolution X-ray Diffraction (HRXRD) and photoluminescence (PL) to determine the composition of and its crystalline quality and by optical microscope to assess the surface morphology. Scanning Electron Microscope (SEM) is used to study the depth of the dry etched structures.The second part of the project deals with the fabrication process consisting of 21 steps to obtain a single junction solar cell structure on GaAs/Si. This process flow will be explained in some detail along with a brief description of several tools in cleanroom that have been used for this purpose.Finally, in the third part, devices are characterized to investigate their performance. Transmission Line Method (TLM) is used to obtain important parameters such as specific contact resistance. Current- voltage (I-V) relation of solar cell is investigated to acquire its efficiency. The lowest specific contact resistance measured in this project is for p-contact (for 4041DV- cell 8) and the highest efficiency measured is 1.64% (for 4041DV- cell 6).In conclusion, although the results obtained are far from the state-of-the art results, this work has laid the foundation for future work that can lead to a breakthrough in fabricating multi-junction tandem solar cell on silicon.</p>

corrected abstract:
<p>Silicon based solar cells have been used as photovoltaic devices for decades due to reasonable cost and environment- friendly nature of silicon. But the conversion efficiency of silicon solar cell is limited; for instance, the maximum conversion efficiency of a crystalline silicon solar cell available in the market developed by Kaneka Corporation is 26 % [1]. In comparison, III-V compound semiconductor multi-junction solar cells are the most efficient solar cells with efficiency of 47.1% [2]. However, due to high-cost substrate materials, III-V solar cells are not the best option for large scale production in real life. Therefore, integration of III-V compound semiconductors on silicon substrate has been studied to obtain III-V/Si multi junction solar cells with high conversion efficiency with reasonable price. To this end, we studied epitaxial growth of GaAs<sub>z/sub>P<sub>1-x</sub> on GaAs deposited on Si.</p><p>This thesis presents the characterization results of the above GaAs<sub>z/sub>P<sub>1-x</sub> epitaxial layer and fabrication of a GaAs<sub>z/sub>P<sub>1-x</sub> single junction solar cell on GaAs coated Si substrate and its performance.</p><p>In the first part of the project, GaAs<sub>z/sub>P<sub>1-x</sub> epitaxial layer grown by Hydride Vapor Phase Epitaxy (HVPE) on different kinds of substrates at different growth conditions are characterized to identify the optimized growth conditions and a suitable substrate. Samples are characterized by High Resolution X-ray Diffraction (HRXRD) and photoluminescence (PL) to determine the composition of GaAs<sub>z/sub>P<sub>1-x</sub> and its crystalline quality and by optical microscope to assess the surface morphology. Scanning Electron Microscope (SEM) is used to study the depth of the dry etched structures.</p><p>The second part of the project deals with the fabrication process consisting of 21 steps to obtain a GaAs<sub>z/sub>P<sub>1-x</sub> single junction solar cell structure on GaAs/Si. This process flow will be explained in some detail along with a brief description of several tools in cleanroom that have been used for this purpose.</p><p>Finally, in the third part, devices are characterized to investigate their performance. Transmission Line Method (TLM) is used to obtain important parameters such as specific contact resistance. Current-voltage (I-V) relation of solar cell is investigated to acquire its efficiency. The lowest specific contact resistance measured in this project is 7.4 × 10<sup>-8</sup> Ω cm<sup>2</sup> for p-contact (for 4041DV- cell 8) and the highest efficiency measured is 1.64% (for 4041DV- cell 6).</p><p>In conclusion, although the results obtained are far from the state-of-the art results, this work has laid the foundation for future work that can lead to a breakthrough in fabricating multi-junction tandem solar cell on silicon.</p>
----------------------------------------------------------------------
In diva2:1668632 
abstract is: 
<p>Traditionally, practitioners use modern portfolio theory to invest optimally. Its appeal lies in its mathematical simplicity and elegance. However, despite its beauty, the theory it is plagued with many problems, which are in combination called the Markowitz curse. Lopéz de Prado introduced Hierarchical Risk Parity (HRP), which deals with the problems of Markwitz’s theory by introducing hierarchical structures into the portfolio allocation step.This thesis is a continuation of the HRP. In contrast to De Prado’s work, we build hierarchical clusters that do not have a predetermined structure and also use portfolio allocation methods that incorporates the mean estimates. We use an algorithm called community detection which is derived from graph theory. The algorithm generates clusters purely from the data without user specification. A problem to overcome is the correct identification of the market mode, whichis non-trivial for futures contracts. This is a serious problem since the specific clustering method we use hinges on correctly identifying this mode. Therefore, in this thesis we introduce a method of finding the market mode for futures data. Finally, we compare the portfolios constructed from the hierarchical clusters to traditional methods. We find that the hierarchical portfolios results in slightly worse performance than the traditional methods when we incorporate the mean and better performance for risk based portfolios. In general, we find that the hierarchical portfolios result in less extreme outcomes.</p>

corrected abstract:
<p>Traditionally, practitioners use modern portfolio theory to invest optimally. Its appeal lies in its mathematical simplicity and elegance. However, despite its beauty, the theory it is plagued with many problems, which are in combination called the Markowitz curse. Lopéz de Prado introduced Hierarchical Risk Parity (HRP), which deals with the problems of Markwitz’s theory by introducing hierarchical structures into the portfolio allocation step.</p><p>This thesis is a continuation of the HRP. In contrast to De Prado’s work, we build hierarchical clusters that do not have a predetermined structure and also use portfolio allocation methods that incorporates the mean estimates. We use an algorithm called community detection which is derived from graph theory. The algorithm generates clusters purely from the data without user specification. A problem to overcome is the correct identification of the market mode, which is non-trivial for futures contracts. This is a serious problem since the specific clustering method we use hinges on correctly identifying this mode. Therefore, in this thesis we introduce a method of finding the market mode for futures data. Finally, we compare the portfolios constructed from the hierarchical clusters to traditional methods. We find that the hierarchical portfolios results in slightly worse performance than the traditional methods when we incorporate the mean and better performance for risk based portfolios. In general, we find that the hierarchical portfolios result in less extreme outcomes.</p>
----------------------------------------------------------------------
In diva2:1595580 
abstract is: 
<p>Before vehicles can be placed in service it has to complete an authorisation process. At the moment,this process is largely depended on tests. This is, however, an expensive and long process. With new technologies and improved simulations this process can be shortened and the costs can be lowered. The validation of a vehicle model, however, is often limited by the available data. Often the measured rail profiles are not available and thus a new UIC60 profile is used for the simulations. The railway track often has been used and showssigns of wear and damages, therefore research has been done to investigate the influence of the rail profiles on the validation of a railway vehicle model. The current methods of validation in the European norm are used to compare simulated values with forces and accelerations available from vehicle measurements. In the first step,25 track sections with different curve radii have beensimulated with a measured rail profile every 100 meters. In the next step, the same sections have been simulated by using the standard UIC60 rail profile. The results show that the use of measured rail profiles does have a positive influence on the outcome of simulation.</p><p>In the final step, one single narrow curve has been simulated to show the effect of standard and worn rail profiles. Four different wear stages of the rail profile are simulated and compared to the available vehicle measurements available. These simulations show that the use of a medium worn rail profile gives the most accurate value.</p>

corrected abstract:
<p>Before vehicles can be placed in service it has to complete an authorisation process. At the moment, this process is largely depended on tests. This is, however, an expensive and long process. With new technologies and improved simulations this process can be shortened and the costs can be lowered. The validation of a vehicle model, however, is often limited by the available data. Often the measured rail profiles are not available and thus a new UIC60 profile is used for the simulations. The railway track often has been used and shows signs of wear and damages, therefore research has been done to investigate the influence of the rail profiles on the validation of a railway vehicle model.</p><p>The current methods of validation in the European norm are used to compare simulated values with forces and accelerations available from vehicle measurements. In the first step, 25 track sections with different curve radii have been simulated with a measured rail profile every 100 meters. In the next step, the same sections have been simulated by using the standard UIC60 rail profile. The results show that the use of measured rail profiles does have a positive influence on the outcome of simulation.</p><p>In the final step, one single narrow curve has been simulated to show the effect of standard and worn rail profiles. Four different wear stages of the rail profile are simulated and compared to the available vehicle measurements available. These simulations show that the use of a medium worn rail profile gives the most accurate value.</p>
----------------------------------------------------------------------
In diva2:1528039  - missing spaces in title:
"Development and verificationof a method to determine theshear properties of Hybrix core"
==>
"Development and verification of a method to determine the shear properties of Hybrix core"

abstract is: 
<p>This thesis helps develop a material model for a novel Fiber Core SandwichSheet construction. A test method was used to determine the mechanicalproperties of the sandwich material. Standard three point bendingtests coupled with digital image correlation was used. Results wereextracted from the digital image data. These results supplemented thedevelopment and tuning of an FE model of the sandwich material. Conclusionswere drawn about the feasibility of the method in studying sucha material.</p>

corrected abstract:
<p>This thesis helps develop a material model for a novel Fiber Core Sandwich Sheet construction. A test method was used to determine the mechanical properties of the sandwich material. Standard three point bending tests coupled with digital image correlation was used. Results were extracted from the digital image data. These results supplemented the development and tuning of an FE model of the sandwich material. Conclusions were drawn about the feasibility of the method in studying such a material.</p>
----------------------------------------------------------------------
In diva2:1527828 
abstract is: 
<p>Planing hull is one solution to break the speed barrier of conventional hull, but as the boat reaches a high speed, massive whisker spray will be developed and attached to the hull, which causes a notable resistance increase. A Swedish company Peterstep invented an innovative spray deflector that can deflect the spray backwards and harvest kinetic energy from the spray.In the 2019 spray deflector project, many tests were done in Davison Laboratory Towing Tank, and there is a trim angle difference between plated and non-plated hulls. To investigate possible reasons, more tests are implemented in this project. According to the test results, the reason is determined as the different roughness of the hull and bottom due to differences in materials. Also, the tape for sealing the seam between hull and bottom plate affects the sharpness of the hard chine, thereby hindering the flow separation.The model used in previous experiments is no longer suitable for the further investigation. The modular design caused the different running position of plated and non-plated hull. In addition, the hull is too slender for the wave test. Therefore, a new model is needed to satisfy the new objectives of experiments. In this paper, the detailed design is surrounded by design aims and restrictions, such as increase spray resistance and avoid porpoising.There are few researches on HSC free running model test. To investigate the performance of the spray deflector in the test environment more similar to reality, a free running model with propulsion and steering system is necessary. The preliminary design, component selection and testing plan are outlined in this paper.</p>

corrected abstract:
<p>Planing hull is one solution to break the speed barrier of conventional hull, but as the boat reaches a high speed, massive whisker spray will be developed and attached to the hull, which causes a notable resistance increase. A Swedish company Peterstep invented an innovative spray deflector that can deflect the spray backwards and harvest kinetic energy from the spray.</p><p>In the 2019 spray deflector project, many tests were done in Davison Laboratory Towing Tank, and there is a trim angle difference between plated and non-plated hulls. To investigate possible reasons, more tests are implemented in this project. According to the test results, the reason is determined as the different roughness of the hull and bottom due to differences in materials. Also, the tape for sealing the seam between hull and bottom plate affects the sharpness of the hard chine, thereby hindering the flow separation.</p><p>The model used in previous experiments is no longer suitable for the further investigation. The modular design caused the different running position of plated and non-plated hull. In addition, the hull is too slender for the wave test. Therefore, a new model is needed to satisfy the new objectives of experiments. In this paper, the detailed design is surrounded by design aims and restrictions, such as increase spray resistance and avoid porpoising.</p><p>There are few researches on HSC free running model test. To investigate the performance of the spray deflector in the test environment more similar to reality, a free running model with propulsion and steering system is necessary. The preliminary design, component selection and testing plan are outlined in this paper.</p>
----------------------------------------------------------------------
In diva2:1519872 - Note: no full text in DiVA
abstract is: 
<p>We will investigate the use of a model-based neural network for traffic reconstruction problem with and withoutmeasurement noise. The state of a traffic system is defined via the traffic density and can be modeled as the solution of a partial differential equation. The essence of the traffic flow problem isto estimate the density. There exist various methods for solving the traffic flow problem, however, none of which perform wellwith noise. The novelty of this paper is to present a method that is able to reduce the processes of identification, reconstruction,prediction and noise rejection into a single optimization problem.Index Terms—traffic state reconstruction, machine learning,noise rejection, physics informed deep learning.</p>

corrected abstract:
<p>We will investigate the use of a model-based neural network for traffic reconstruction problem with and without measurement noise. The state of a traffic system is defined via the traffic density and can be modeled as the solution of a partial differential equation. The essence of the traffic flow problem is to estimate the density. There exist various methods for solving the traffic flow problem, however, none of which perform well with noise. The novelty of this paper is to present a method that is able to reduce the processes of identification, reconstruction, prediction and noise rejection into a single optimization problem.</p>
----------------------------------------------------------------------
In diva2:1464278 
abstract is: 
<p>This thesis investigates the possibility of using Sequential Monte Carlo methods (SMC) to create an online algorithm to infer properties from a dataset, such as unknown model parameters. Statistical inference from data streams tends to be difficult, and this is particularly the case for parametric models, which will be the focus of this paper. We develop a sequential Monte Carlo algorithm sampling sequentially from the model's posterior distributions. As a key ingredient of this approach, unknown static parameters are jittered towards the shrinking support of the posterior on the basis of an artificial Markovian dynamic allowing for correct pseudo-marginalisation of the target distributions. We then test the algorithm on a simple Gaussian model, a Gausian Mixture Model (GMM), as well as a variable dimension GMM. All tests and coding were done using Matlab. The outcome of the simulation is promising, but more extensive comparisons to other online algorithms for static parameter models are needed to really gauge the computational efficiency of the developed algorithm.</p>

corrected abstract:
<p>This thesis investigates the possibility of using Sequential Monte Carlo methods (SMC) to create an online algorithm to infer properties from a dataset, such as unknown model parameters. Statistical inference from data streams tends to be difficult, and this is particularly the case for parametric models, which will be the focus of this paper. We develop a sequential Monte Carlo algorithm sampling sequentially from the model's posterior distributions. As a key ingredient of this approach, unknown static parameters are jittered towards the shrinking support of the posterior on the basis of an artificial Markovian dynamic allowing for correct pseudo-marginalisation of the target distributions. We then test the algorithm on a simple latent Gaussian model, latent Gausian Mixture Model (GMM), as well as a variable dimension laten GMM. All tests and coding were done using Matlab. The outcome of the simulation is promising, but more extensive comparisons to other online algorithms for static parameter models are needed to really gauge the computational efficiency of the developed algorithm.</p>
----------------------------------------------------------------------
In diva2:1380196 
abstract is: 
<p>Control and balancing of an inverted pendulum has gained a lot of attention over the past few decades due to its unstable properties. This has become a great challenge for control engineers to verify and test the control theory. To control and balance an inverted pendulum, proportional integrated derivative (PID) method or linear quadratic regulator (LQR) method can be used through which a lot of simulations can be done using the represented theories.Since urban population is increasing at a very alarming rate, there is a need to discover new ways of transportation to meet the future challenges and demands. Scania has come up with a new conceptual bus called NXT which aims to develop a modular vehicle that should configure and re-configure themselves between different transportation tasks. NXT vehicle has front and rear drive modules which can be represented as single axle, two-wheeled vehicles which in-turn can be viewed as an inverted pendulum with a huge Center of Gravity. Controlling and balancing of the pod or drive module precisely and accurately is an interesting challenge since it is an unstable inverted pendulum with huge center of gravity (COG). This behaviour of the system has created a research question whether the module is controllable or not.Therefore this thesis focuses on the possibility of controlling the pod which is a two-wheeled inverted pendulum vehicle with a COG offset. Also, the thesis focuses on the construction, mod-elling, testing and validation of a down-scaled model, what sensors are needed to balance the pod precisely, how the sensors must be integrated with the system and how the pod can be controlled remotely from a certain distance by a human. The developed pod houses the technologies like sensors, BLDC motor controllers, hoverboard, Arduino board and Bluetooth transmitters.The Master Thesis starts by presenting an introduction to the inverted pendulum theories, Scania NXT project, information about the research methods, thesis outline and structure . It continues by describing related literature about the inverted pendulums, segways, hoverboards, motor controllers and Arduino boards. Afterwards, the process of deriving a mathematical model, together with simulation in Matlab, Simulink and Simscape is described. Later, construction of the pod is made and lot of effort is put to run the pod. Since the pod needs to be controlled remotely by a human, a remote controlled systemis implemented via mobile phone using an app and finally the thesis is finished with a conclusion and ideas for future work.</p>

corrected abstract:
<p>Control and balancing of an inverted pendulum has gained a lot of attention over the past few decades due to its unstable properties. This has become a great challenge for control engineers to verify and test the control theory. To control and balance an inverted pendulum, proportional integrated derivative (PID) method or linear quadratic regulator (LQR) method can be used through which a lot of simulations can be done using the represented theories.</p><p>Since urban population is increasing at a very alarming rate, there is a need to discover new ways of transportation to meet the future challenges and demands. Scania has come up with a new conceptual bus called NXT which aims to develop a modular vehicle that should configure and re-configure themselves between different transportation tasks. NXT vehicle has front and rear drive modules which can be represented as single axle, two-wheeled vehicles which in-turn can be viewed as an inverted pendulum with a huge Center of Gravity. Controlling and balancing of the pod or drive module precisely and accurately is an interesting challenge since it is an unstable inverted pendulum with huge center of gravity (COG). This behaviour of the system has created a research question whether the module is controllable or not.</p><p>Therefore this thesis focuses on the possibility of controlling the pod which is a two-wheeled inverted pendulum vehicle with a COG offset. Also, the thesis focuses on the construction, modelling, testing and validation of a down-scaled model, what sensors are needed to balance the pod precisely, how the sensors must be integrated with the system and how the pod can be controlled remotely from a certain distance by a human. The developed pod houses the technologies like sensors, BLDC motor controllers, hoverboard, Arduino board and Bluetooth transmitters.</p><p>The Master Thesis starts by presenting an introduction to the inverted pendulum theories, Scania NXT project, information about the research methods, thesis outline and structure . It continues by describing related literature about the inverted pendulums, segways, hoverboards, motor controllers and Arduino boards. Afterwards, the process of deriving a mathematical model, together with simulation in Matlab, Simulink and Simscape is described. Later, construction of the pod is made and lot of effort is put to run the pod. Since the pod needs to be controlled remotely by a human, a remote controlled system is implemented via mobile phone using an app and finally the thesis is finished with a conclusion and ideas for future work.</p>
----------------------------------------------------------------------
In diva2:1360722 
abstract is: 
<p>A computational framework for linear static aeroelastic analyses is presented. The overall aeroelasticity model is applicable to conceptual aircraft design studies and other low-fidelity aero-structural analyses. A partitioned approach is used, i. e. separate solvers for aerodynamics and structure analyses are coupled in a suitable way, together forming a model for aeroelastic simulations. Aerodynamics are modelled using the vortexlattice method (VLM), a simple computational fluid dynamics (CFD) model based on potential flow. The structure is represented by a three-dimensional (3D) Euler-Bernoulli beam model in a finite element method (FEM) formulation. A particular focus was put on the modularity and loose coupling of aforementioned models. The core of the aeroelastic framework was abstracted, such that it does not depend on any specific details of the underlying aerodynamics and structure modules. The final aeroelasticity model constitutes independent software tools for the VLM and the beam FEM, as well as a framework enabling the aeroelastic coupling. These different tools have been developed as part of this thesis work. A wind tunnel experiment with a simple wing model is presented as a validation test case. An aero-structural analysis of a fully elastic unmanned aerial vehicle (UAV) (OptiMale) is described and results are compared with an existing higherfidelity study.</p>

corrected abstract:
<p>A computational framework for linear static aeroelastic analyses is presented. The overall aeroelasticity model is applicable to conceptual aircraft design studies and other low-fidelity aero-structural analyses. A partitioned approach is used, i. e. separate solvers for aerodynamics and structure analyses are coupled in a suitable way, together forming a model for aeroelastic simulations. Aerodynamics are modelled using the vortex-lattice method (VLM), a simple computational fluid dynamics (CFD) model based on potential flow. The structure is represented by a three-dimensional (3D) Euler-Bernoulli beam model in a finite element method (FEM) formulation. A particular focus was put on the modularity and loose coupling of aforementioned models. The core of the aeroelastic framework was abstracted, such that it does not depend on any specific details of the underlying aerodynamics and structure modules. The final aeroelasticity model constitutes independent software tools for the VLM and the beam FEM, as well as a framework enabling the aeroelastic coupling. These different tools have been developed as part of this thesis work. A wind tunnel experiment with a simple wing model is presented as a validation test case. An aero-structural analysis of a fully elastic unmanned aerial vehicle (UAV) (OptiMale) is described and results are compared with an existing higher-fidelity study.</p>
----------------------------------------------------------------------
In diva2:1357237 
abstract is: 
<p>The analysis of the reliability of CREO simulate will be done step by step. First of all, the precisionof the simulation has to be measured thus a comparison with simple theoretical computations willbe done. Then, the scope of the capacity of the software will be analyzed and if elements deemednecessary to thermal simulations are missing, back up solutions are to be found. Also, the influenceof the meshing will be studied and measured to ensure that the software guarantee convergence evenin the hand of persons unfamiliar with simulations.In parallel, one will experiment with realistic hardware that could be used to compare reality withthe simulations. Those experiments will be handmade using regular materials from the company.</p>

corrected abstract:
<p>The analysis of the reliability of CREO simulate will be done step by step. First of all, the precision of the simulation has to be measured thus a comparison with simple theoretical computations will be done. Then, the scope of the capacity of the software will be analyzed and if elements deemed necessary to thermal simulations are missing, back up solutions are to be found. Also, the influence of the meshing will be studied and measured to ensure that the software guarantee convergence even in the hand of persons unfamiliar with simulations. In parallel, one will experiment with realistic hardware that could be used to compare reality with the simulations. Those experiments will be handmade using regular materials from the company.</p>


The abstract does NOT appear in the thesis!
----------------------------------------------------------------------
In diva2:1341320 
abstract is: 
<p>In this article we analyze whether machine learning can be used to help solve problemsin physics. This is examined by implementing an artificial neural network which is trainedto find the energy levels for the quantum harmonic oscillator with and without an externalelectric field. Radial basis functions were used to make the neural network. Monte Carlomethods were used for heavy calculations. The method was shown to work well in somecases but had problems for large electric fields. The problems that occured were that theconvergence became unstable, with leaps in the energy and that the system did not alwaysconverge to the right energy level.</p>

corrected abstract:
<p>In this article we analyze whether machine learning can be used to help solve problems in physics. This is examined by implementing an artificial neural network which is trained to find the energy levels for the quantum harmonic oscillator with and without an external electric field. Radial basis functions were used to make the neural network. Monte Carlo methods were used for heavy calculations. The method was shown to work well in some cases but had problems for large electric fields. The problems that occured were that the convergence became unstable, with leaps in the energy and that the system did not always converge to the right energy level.</p>
----------------------------------------------------------------------
In diva2:1321152 - Note: no full text in DiVA
abstract is: 
<p>This thesis work considers the conceptual design of a mounting structure and two cross beams. The structure is intended to be placed on the frame of a truck and will be subjected to a standard set of load cases originating from vehicle maneuvering. This thesis work will also investigate how the optimization process can be included in the design process of heavy vehicles and how concepts can be generated for multiple configurations.The conceptual design was generated by first applying rough topology optimizations for each of the load cases, followed by topology optimization on combined load cases for each configuration and finally topology optimization with multiple configurations. Since the model was quite large, a static condensation was performed to investigate if the computations could be speeded up. A reference model was built with existing designs to study the responses and to compare the static analysis in the validation. For the process of minimizing mass, compliance responses were extracted from reference computations to use in the topology optimization.The optimization produced reasonable concepts for single and multiple configurations. However, more investigations should be done on the constraints of the two optimization problems that were set up. The reduced model showed a reduction in solution time of 60 % for multiple load cases and 81.5 % in storage size, which implies that the static condensation should be used when possible to save time when many iterations are to be expected.The concepts found can be further developed with fine tuning optimizations, such as size optimizations and shape optimizations. More concepts can also be generated by changing the constraints on the design variables or by increasing or decreasing the compliance constraints for the different subcases.</p><p></p>

corrected abstract:
<p>This thesis work considers the conceptual design of a mounting structure and two cross beams. The structure is intended to be placed on the frame of a truck and will be subjected to a standard set of load cases originating from vehicle maneuvering. This thesis work will also investigate how the optimization process can be included in the design process of heavy vehicles and how concepts can be generated for multiple configurations. The conceptual design was generated by first applying rough topology optimizations for each of the load cases, followed by topology optimization on combined load cases for each configuration and finally topology optimization with multiple configurations. Since the model was quite large, a static condensation was performed to investigate if the computations could be speeded up. A reference model was built with existing designs to study the responses and to compare the static analysis in the validation. For the process of minimizing mass, compliance responses were extracted from reference computations to use in the topology optimization. The optimization produced reasonable concepts for single and multiple configurations. However, more investigations should be done on the constraints of the two optimization problems that were set up. The reduced model showed a reduction in solution time of 60 % for multiple load cases and 81.5 % in storage size, which implies that the static condensation should be used when possible to save time when many iterations are to be expected. The concepts found can be further developed with fine tuning optimizations, such as size optimizations and shape optimizations. More concepts can also be generated by changing the constraints on the design variables or by increasing or decreasing the compliance constraints for the different subcases.</p><p></p>
----------------------------------------------------------------------
In diva2:1298377 
abstract is: 
<p>Noise pollution from road traffic is one of the greatest environmental issues in modern day, and the social cost for road traffic noise was estimated to over 16 billion SEK per year in Sweden in2014. Passive or active control methods can be used to reduce the noise. Active control methods or active noise control is more suitable for attenuating noise in lower frequencies. Active noise control reduces noise by eliminating the noise with a secondary source. There are different control strategies to construct an active noise control system, where the update of the secondary sourceis controlled by an algorithm. There are several different algorithms that are possible to use, and one option is to use a Feedforward Filtered-X Least-Mean-Square (FXLMS) algorithm. It uses control positions where the noise is meant to be reduced and reference signals that measure the noise upstream prior the secondary source. FXLMS also uses a model of the secondary source path to the control position in order to ensure convergence of the algorithm. Although the use of multiple reference signals increases the accuracy of the algorithm, it also increases the convergence time and the practical cost of such an installation. Unfortunately, it can require many reference signals to obtain a sufficient noise reduction when the unwanted noise source is complex and has multiple propagation paths.This study investigates the possibility of producing a new, reduced set of reference signals with a linear combination of the original reference signals that still contain the majority of information needed for suficient noise reduction. This new set of reference signals are sometimes called virtual reference signals. Three different methods of virtual reference signals are analysed; first a constant method using singular-value decomposition on the covariance of the reference signals, second another constant method using singular-value decomposition on the covariance of response estimate from each corresponding reference signal, third an adaptive algorithm updating the linear combination to adapt for incoming data. The different strategies are tested on road test measurements at three different constant speeds, 40km=h; 80km=h and 120km=h, and on data generated from a numerical vehicle model in COMSOL.The results from the analysis indicates that the virtual reference signals could sufficiently reproduce information from the original reference signals to obtain a similar noise reduction with fewer reference signals. However, the virtual reference signals with the adaptive algorithm could not manage to track a transient system where the signal amplitudes are varying over time. Further work is needed to analyse the limits and requirements to obtain virtual reference signals that can represent and track a system even for transient events.</p>

corrected abstract:
<p>Noise pollution from road traffic is one of the greatest environmental issues in modern day, and the social cost for road traffic noise was estimated to over 16 billion SEK per year in Sweden in 2014. Passive or active control methods can be used to reduce the noise. Active control methods or active noise control is more suitable for attenuating noise in lower frequencies. Active noise control reduces noise by eliminating the noise with a secondary source. There are different control strategies to construct an active noise control system, where the update of the secondary source is controlled by an algorithm. There are several different algorithms that are possible to use, and one option is to use a Feedforward Filtered-X Least-Mean-Square (FXLMS) algorithm. It uses control positions where the noise is meant to be reduced and reference signals that measure the noise upstream prior the secondary source. FXLMS also uses a model of the secondary source path to the control position in order to ensure convergence of the algorithm. Although the use of multiple reference signals increases the accuracy of the algorithm, it also increases the convergence time and the practical cost of such an installation. Unfortunately, it can require many reference signals to obtain a sufficient noise reduction when the unwanted noise source is complex and has multiple propagation paths.</p><p>This study investigates the possibility of producing a new, reduced set of reference signals with a linear combination of the original reference signals that still contain the majority of information needed for sufficient noise reduction. This new set of reference signals are sometimes called virtual reference signals. Three different methods of virtual reference signals are analysed; first a constant method using singular-value decomposition on the covariance of the reference signals, second another constant method using singular-value decomposition on the covariance of response estimate from each corresponding reference signal, third an adaptive algorithm updating the linear combination to adapt for incoming data. The different strategies are tested on road test measurements at three different constant speeds, 40 km/h, 80 km/h and 120 km/h, and on data generated from a numerical vehicle model in COMSOL.</p><p>The results from the analysis indicates that the virtual reference signals could sufficiently reproduce information from the original reference signals to obtain a similar noise reduction with fewer reference signals. However, the virtual reference signals with the adaptive algorithm could not manage to track a transient system where the signal amplitudes are varying over time. Further work is needed to analyse the limits and requirements to obtain virtual reference signals that can represent and track a system even for transient events.</p>
----------------------------------------------------------------------
In diva2:1216803 - missing spaces ini title:
"The (perhaps) causal brain: A comparison of attractor neural networks usingtemporally symmetric and antisymmetric synapticrules"
==>
"The (perhaps) causal brain: A comparison of attractor neural networks using temporally symmetric and antisymmetric synaptic rules"

abstract is: 
<p>The associative memory of the brain is thought to be well modelled by attractorneural networks. A sort of artificial neural network that may store memories and has the ability to associate them with distorted input. The memories may be stored in the system by changing the connecting weights depending on the activity pattern in the network, a process known as synaptic plasticity. There are several different theories of the conditions required for the strength of synaptic connection increase or decrease and which one of these that is the most likely is still an open issue. Many recent studies of the associative memory have used a model that only take correlated activity between neurons into account (BCPNN), but there is some experimental support for another one in which the exact timing of pre- and postsynaptic activity plays a role (STDP). There is, however, no conclusive evidence for either one and this study will, therefore, investigate the differences in an attractor neural network model using the two different rules for synaptic plasticity.In this study two simple attractor neural networks with 64 neurons werecreated, each using either STDP or BCPNN as a model for synaptic plasticity. Forcing the system into several states corresponding to different memories the connecting weights between the neurons changed. By stimulating the network with partial memory patterns its ability to recall memories remain stable could be tested. In several aspects, the system using STDP was found to perform better than BCPNN, and it is possible to conclude that the former synaptic rule was the better choice in this specific case.To draw any conclusions regarding which of STDP or BCPNN is more prob- able as a model for the synaptic plasticity in the brain more detailed studies would have to be undertaken. Preferably utilising more advanced and biologi- cally realistic models.</p>

corrected abstract:
<p>The associative memory of the brain is thought to be well modelled by attractor neural networks. A sort of artificial neural network that may store memories and has the ability to associate them with distorted input. The memories may be stored in the system by changing the connecting weights depending on the activity pattern in the network, a process known as synaptic plasticity. There are several different theories of the conditions required for the strength of synaptic connection increase or decrease and which one of these that is the most likely is still an open issue. Many recent studies of the associative memory have used a model that only take correlated activity between neurons into account (BCPNN), but there is some experimental support for another one in which the exact timing of pre- and postsynaptic activity plays a role (STDP). There is, however, no conclusive evidence for either one and this study will, therefore, investigate the differences in an attractor neural network model using the two different rules for synaptic plasticity.</p><p>In this study two simple attractor neural networks with 64 neurons were created, each using either STDP or BCPNN as a model for synaptic plasticity. Forcing the system into several states corresponding to different memories the connecting weights between the neurons changed. By stimulating the network with partial memory patterns its ability to recall memories remain stable could be tested. In several aspects, the system using STDP was found to perform better than BCPNN, and it is possible to conclude that the former synaptic rule was the better choice in this specific case.</p><p>To draw any conclusions regarding which of STDP or BCPNN is more probable as a model for the synaptic plasticity in the brain more detailed studies would have to be undertaken. Preferably utilising more advanced and biologically realistic models.</p>
----------------------------------------------------------------------
In diva2:1150909 
abstract is: 
<p>In modern society, many companies have large data records over their individual customers, containing information about attributes, such as name, gender, marital status, address, etc. These attributes can be used to link costumers together, depending on whether they share some sort of relationship with each other or not. In this thesis the goal is to investigate and compare methods to predict relationships between individuals in the terms of what we define as a household relationship, i.e. we wish to identify which individuals are sharing living expenses with one another. The objective is to explore the ability of three supervised statistical machine learning methods, namely, logistic regression (LR), artificial neural networks (ANN) and the support vector machine (SVM), to predict these household relationships and evaluate their predictive performance for different settings on their corresponding tuning parameters. Data over a limited population of individuals, containing information about household affiliation and attributes, were available for this task. In order to apply these methods, the problem had to be formulated on a form enabling supervised learning, i.e. a target <em>Y</em> and input predictors <strong><em>X</em></strong><em> = (X<sub>1</sub>, …, X<sub>p</sub>)</em>, based on the set of <em>p</em> attributes associated with each individual, had to be derived. We have presented a technique which forms pairs of individuals under the hypothesis <em>H<sub>0</sub></em>, that they share a household relationship, and then a test of significance is constructed. This technique transforms the problem into a standard binary classification problem. A sample of observations could be generated by randomly pair individuals and using the available data over each individual to code the corresponding outcome on <em>Y</em> and <strong><em>X</em></strong> for each random pair. For evaluation and tuning of the three supervised learning methods, the sample was split into a training set, a validation set and a test set.</p><p>We have seen that the prediction error, in term of misclassification rate, is very small for all three methods since the two classes, <em>H<sub>0</sub></em> is true, and <em>H<sub>0</sub></em> is false, are far away from each other and well separable. The data have shown pronounced linear separability, generally resulting in minor differences in misclassification rate as the tuning parameters are modified. However, some variations in the prediction results due to tuning have been observed, and if also considering computational time and requirements on computational power, optimal settings on the tuning parameters could be determined for each method. Comparing LR, ANN and SVM, using optimal tuning settings, the results from testing have shown that there is no significant difference between the three methods performances and they all predict well. Nevertheless, due to difference in complexity between the methods, we have concluded that SVM is the least suitable method to use, whereas LR most suitable. However, the ANN handles complex and non-linear data better than LR, therefore, for future application of the model, where data might not have such a pronounced linear separability, we find it suitable to consider ANN as well.</p><p>This thesis has been written at Svenska Handelsbanken, one of the large major banks in Sweden, with offices all around the world. Their headquarters are situated in Kungsträdgården, Stockholm. Computations have been performed using SAS software and data have been processed in SQL relational database management system.</p>

corrected abstract:
<p>In modern society, many companies have large data records over their individual customers, containing information about attributes, such as name, gender, marital status, address, etc. These attributes can be used to link costumers together, depending on whether they share some sort of relationship with each other or not. In this thesis the goal is to investigate and compare methods to predict relationships between individuals in the terms of what we define as a household relationship, i.e. we wish to identify which individuals are sharing living expenses with one another. The objective is to explore the ability of three supervised statistical machine learning methods, namely, logistic regression (LR), artificial neural networks (ANN) and the support vector machine (SVM), to predict these household relationships and evaluate their predictive performance for different settings on their corresponding tuning parameters. Data over a limited population of individuals, containing information about household affiliation and attributes, were available for this task. In order to apply these methods, the problem had to be formulated on a form enabling supervised learning, i.e. a target 𝑌 and input predictors <strong>X</strong> = (𝑋<sub>1</sub>, …, 𝑋<sub>p</sub>), based on the set of <em>p</em> attributes associated with each individual, had to be derived. We have presented a technique which forms pairs of individuals under the hypothesis 𝐻<sub>0</sub>, that they share a household relationship, and then a test of significance is constructed. This technique transforms the problem into a standard binary classification problem. A sample of observations could be generated by randomly pair individuals and using the available data over each individual to code the corresponding outcome on 𝑌 and <strong>X</strong> for each random pair. For evaluation and tuning of the three supervised learning methods, the sample was split into a training set, a validation set and a test set.</p><p>We have seen that the prediction error, in term of misclassification rate, is very small for all three methods since the two classes, 𝐻<sub>0</sub> is true, and 𝐻<sub>0</sub> is false, are far away from each other and well separable. The data have shown pronounced linear separability, generally resulting in minor differences in misclassification rate as the tuning parameters are modified. However, some variations in the prediction results due to tuning have been observed, and if also considering computational time and requirements on computational power, optimal settings on the tuning parameters could be determined for each method. Comparing LR, ANN and SVM, using optimal tuning settings, the results from testing have shown that there is no significant difference between the three methods performances and they all predict well. Nevertheless, due to difference in complexity between the methods, we have concluded that SVM is the least suitable method to use, whereas LR most suitable. However, the ANN handles complex and non-linear data better than LR, therefore, for future application of the model, where data might not have such a pronounced linear separability, we find it suitable to consider ANN as well.</p><p>This thesis has been written at Svenska Handelsbanken, one of the large major banks in Sweden, with offices all around the world. Their headquarters are situated in Kungsträdgården, Stockholm. Computations have been performed using SAS software and data have been processed in SQL relational database management system.</p>
----------------------------------------------------------------------
In diva2:1057240 
abstract is: 
<p>The aim of this Master thesis is to provide a statistical analysis of the factors inuencing the fuel consumption, with a focus on the separation of the drivers' performance. The study is focused on the long haulage trucks, which correspond to the application where the fuel consumption becomes of primary interest from the economical point of view. Further developments of the work leads to a graphical representation of the outcomes on a map, highlighting in particular the segments of the road network having the highest variation of the driver-inuenced fuel consumption. The analysis dataset created is the combination of data coming from di erent sources and additional features computed based on them. The datasources are providing respectively the vehicles' operating data and congurations, the road network's characteristics and the weather information. The results obtained prove that it is possible to isolate the driver factor from the overall fuel consumption. This can be achieved by training a model composed by variables statistically chosen through a regression procedure. Further in the analysis the di erent driver factors are used in order to determine the fuel saving potential of the road stretches where the factors are computed. The results are gathered in multiple stages, based on the dimension of the dataset considered and the method used. Two methods are used to train the model: the least squares regression and the ridge regression. First the whole Swedish road network composed by primary roads is analyzed with least squares. 1195 road stretches belonging to this network present a dened and di erent than zero fuel saving potential varying between 0.003 and 83.71 l/100km. Then, a smaller portion of the same road network is analyzed after being provided with road slope information. The fuel saving potential estimated using ridge regression present values between 0.002 and 24.39 l/100km.</p><p>From the geographical point of view little can be deduced from the analysis of the complete network. The E4 provided with slope data, on the contrary, allows a better insight, especially using ridge.</p>

corrected abstract:
<p>The aim of this Master thesis is to provide a statistical analysis of the factors influencing the fuel consumption, with a focus on the separation of the drivers' performance. The study is focused on the long haulage trucks, which correspond to the application where the fuel consumption becomes of primary interest from the economical point of view. Further developments of the work leads to a graphical representation of the outcomes on a map, highlighting in particular the segments of the road network having the highest variation of the driver-influenced fuel consumption.</p><p>The analysis dataset created is the combination of data coming from different sources and additional features computed based on them. The datasources are providing respectively the vehicles' operating data and configurations, the road network's characteristics and the weather information. The results obtained prove that it is possible to isolate the driver factor from the overall fuel consumption. This can be achieved by training a model composed by variables statistically chosen through a regression procedure. Further in the analysis the different driver factors are used in order to determine the fuel saving potential of the road stretches where the factors are computed. The results are gathered in multiple stages, based on the dimension of the dataset considered and the method used. Two methods are used to train the model: the least squares regression and the ridge regression. First the whole Swedish road network composed by primary roads is analyzed with least squares. 1195 road stretches belonging to this network present a defined and different than zero fuel saving potential varying between 0.003 and 83.71 l/100 km. Then, a smaller portion of the same road network is analyzed after being provided with road slope information. The fuel saving potential estimated using ridge regression present values between 0.002 and 24.39 l/100 km.</p><p>From the geographical point of view little can be deduced from the analysis of the complete network. The E4 provided with slope data, on the contrary, allows a better insight, especially using ridge.</p>
----------------------------------------------------------------------
In diva2:974146 
abstract is: 
<p>Vector autoregressive (VAR) models for time series analysis of high-dimensional data tend to suffer from overparametrisation as the number of parameters in a VAR model grows quadratically with the number of included predictors. In these cases, lower-dimensional structural assumptions are commonly imposed through factor models or regularisation. Factor models reduce the model dimension by projecting the observations onto a common lower-dimensional subspace, decomposing the variables into common and idiosyncratic terms, and might be preferred when predictors are highly collinear. Regularisation reduces overfitting by penalising certain features of the model estimates and might be preferred when, for example, only a few predictors are assumed important.</p><p>We propose a regularised factor model where factors are constructed by projection onto a common subspace and where the transition matrices in a time series model with the resulting factors are estimated with regularisation. By the subspace estimation we hope to uncover underlying latent factors that explain the predictor dynamics and the additional penalisation is used to encourage additional sparsity and to impose a priori structural knowledge into the estimate. We investigate unsupervised and supervised subspace extraction and extend earlier results on dynamic subspace extraction. Additionally, we investigate element-wise regularisation by the ridge and lasso penalties and two extensions of the lasso penalty that encourage structural sparsity. The performance of the model is tested by forecasting log returns of exchange rates.  </p>

corrected abstract:
<p>Vector autoregressive (VAR) models for time series analysis of high-dimensional data tend to suffer from overparametrisation as the number of parameters in a VAR model grows quadratically with the number of included predictors. In these cases, lower-dimensional structural assumptions are commonly imposed through factor models or regularisation. Factor models reduce the model dimension by projecting the observations onto a common lower-dimensional subspace, decomposing the variables into common and idiosyncratic terms, and might be preferred when predictors are highly collinear. Regularisation reduces overfitting by penalising certain features of the model estimates and might be preferred when, for example, only a few predictors are assumed important.</p><p>We propose a regularised factor model where factors are constructed by projection onto a common subspace and where the transition matrices in a time series model with the resulting factors are estimated with regularisation. By the subspace estimation we hope to uncover underlying latent factors that explain the predictor dynamics and the additional penalisation is used to encourage additional sparsity and to impose a priori structural knowledge into the estimate. We investigate unsupervised and supervised subspace extraction and extend earlier results on dynamic subspace extraction. Additionally, we investigate element-wise regularisation by the ridge and lasso penalties and two extensions of the lasso penalty that encourage structural sparsity. The performance of the model is tested by forecasting log returns of exchange rates.</p>
----------------------------------------------------------------------
In diva2:920005 
abstract is: 
<p>The monitoring of the worlds oceans is a fundamental requirement to understand and predict the ongoing climate change. Scientists need reliable long-term data series from regions that are of great interest for understanding the climate change and its consequences. An important area is the West Antarctic Ice Shelf (WAIS), which is still a great uncertainty of future sea level rise predictions. The vulnerable polar regions are, however, seldom sampled, mainly because of their remoteness, the cost and the possibility to reach them.The KTH Center of Naval Architecture recently formed a cooperation with the Department of Earth Science (GVC) at the University of Gothenburg to develop a cost effective sens-ing system, designed to acquire long term temperature data in remote polar regions. The oceanographers at GVC are researching if the sea bottom temperature in shelf regions like the WAIS can be used as a proxy for the whole oceanic heat content. This research makes it possible to use sensing systems that are moored close to the sea-ﬂoor and measure ocean bottom temperature only.In this thesis a low-cost system for long term underwater sensing (LoTUS) is developed. The LoTUS bottom lander system consists of a positive buoyant pressure hull equipped with a very precise temperature sensor, an Iridium satellite link, GPS and a deep hibernation functional-ity. The bottom landers are deployed from a ship or a helicopter and sample data for up to 10 years in water depths up to 1000 meter. As soon as the intended operation time is reached, the LoTUS lander releases its anchor, surfaces and transmits its temperature data to the user ashore via satellite. Afterwards the bottom lander reconﬁgures to a GPS beacon mode, i.e. it drifts and regularly sends out its position to observe direction and velocity of surface currents. The LoTUS system was successfully tested in the Baltic Sea and in the Petermann 2015 Ex-pedition in northern Greenland. The bottom landers proved to be working under harsh polar conditions and can be used as a new and innovative low cost sensing system to acquire long term temperature data.</p>

corrected abstract:
<p>The monitoring of the worlds oceans is a fundamental requirement to understand and predict the ongoing climate change. Scientists need reliable long-term data series from regions that are of great interest for understanding the climate change and its consequences. An important area is the West Antarctic Ice Shelf (WAIS), which is still a great uncertainty of future sea level rise predictions. The vulnerable polar regions are, however, seldom sampled, mainly because of their remoteness, the cost and the possibility to reach them.</p><p>The KTH Center of Naval Architecture recently formed a cooperation with the Department of Earth Science (GVC) at the University of Gothenburg to develop a cost effective sensing system, designed to acquire long term temperature data in remote polar regions. The oceanographers at GVC are researching if the sea bottom temperature in shelf regions like the WAIS can be used as a proxy for the whole oceanic heat content. This research makes it possible to use sensing systems that are moored close to the sea-floor and measure ocean bottom temperature only.</p><p>In this thesis a low-cost system for long term underwater sensing (LoTUS) is developed. The LoTUS bottom lander system consists of a positive buoyant pressure hull equipped with a very precise temperature sensor, an Iridium satellite link, GPS and a deep hibernation functionality. The bottom landers are deployed from a ship or a helicopter and sample data for up to 10 years in water depths up to 1000 meter. As soon as the intended operation time is reached, the LoTUS lander releases its anchor, surfaces and transmits its temperature data to the user ashore via satellite. Afterwards the bottom lander reconfigures to a GPS beacon mode, i.e. it drifts and regularly sends out its position to observe direction and velocity of surface currents. The LoTUS system was successfully tested in the Baltic Sea and in the Petermann 2015 Expedition in northern Greenland. The bottom landers proved to be working under harsh polar conditions and can be used as a new and innovative low cost sensing system to acquire long term temperature data.</p>
----------------------------------------------------------------------
In diva2:920001 
abstract is: 
<p>To enable the use of carbon ﬁbre reinforced polymer (CFRP) propellers for merchant vessels, the problem of cavitation must be solved. The matrix of the FRP is generally brittle and more sensitive to cavitation erosion than the more conventional metal propeller blades. Nakashima Propeller Co. wants to solve this problem by attaching a metal patch adhesively in the high-risk areas of the blade. The bonding of FRP and metal is a classic problem when manufacturing hybrid components, and even more diﬃcult when the bond is subject to fatigue loads, in a marine environment. The thin metal plate makes determining the crack resistance from the energy release rate diﬃcult and a novel approach is necessary.This report focuses on outlining the problem and establishes an experimental method for conducting the initial investigations of the robustness of an adhesive bond for this application. Four diﬀerent adhesives were tested in quasi-static mode I loading with constant cross-head velocity: Denatite 2204, co-curing, Newport 102 and Plexus AO420. All adhesives were epoxy based except for the Plexus. The co-cured system used resin Epolam 5015, the same was used for manufacturing the FRP in the other specimen. Instead of computing the energy release rate, the crack tip angles were compared between the diﬀerent adhesive systems. These measurements are only internally comparable. Based on the results from the quasi-static loading, the Denatite and the Plexus were brought into a smaller fatigue testing series. The two others were deemed as too weak to be of interest.The experiments showed that the methacrylate based Plexus AO420 had a signiﬁcantly higher strength, in the quasi-static loading it had a maximum recorded load almost ten times as high as the denatite. The fatigue test was inconclusive, although no drastic or unexpected behaviour was recorded. It was noted during the process (unsurprisingly) that the manufacturing process and the skill of the worker is very much aﬀecting the bond strength, sometimes even more than the material properties and the surface treatments applied.The experimental method, including the measurements of the crack tip angle could be concluded as a simple but eﬀective method to determine the basic adhesive strength and general characteristics of the diﬀerent types. The second series of fatigue testing can be performed to see general fatigue behaviour for the adhesives, but as the actual load case on the propeller is unknown it can be diﬃcult to directly relate the fatigue behaviour in the test to the real world. Both test series can be used to determine general behaviour, but due to the complexity of the geometry, deformation modes and loads on the real propeller, it is suggested to perform ﬁnal testing on a live propeller, rather than trying to draw to large conclusions from isolated experiments.</p>

corrected abstract:
<p>To enable the use of carbon fibre reinforced polymer (CFRP) propellers for merchant vessels, the problem of cavitation must be solved. The matrix of the FRP is generally brittle and more sensitive to cavitation erosion than the more conventional metal propeller blades. Nakashima Propeller Co. wants to solve this problem by attaching a metal patch adhesively in the high-risk areas of the blade. The bonding of FRP and metal is a classic problem when manufacturing hybrid components, and even more difficult when the bond is subject to fatigue loads, in a marine environment. The thin metal plate makes determining the crack resistance from the energy release rate difficult and a novel approach is necessary.</p><p>This report focuses on outlining the problem and establishes an experimental method for conducting the initial investigations of the robustness of an adhesive bond for this application. Four different adhesives were tested in quasi-static mode I loading with constant cross-head velocity: Denatite 2204, co-curing, Newport 102 and Plexus AO420. All adhesives were epoxy based except for the Plexus. The co-cured system used resin Epolam 5015, the same was used for manufacturing the FRP in the other specimen. Instead of computing the energy release rate, the crack tip angles were compared between the different adhesive systems. These measurements are only internally comparable. Based on the results from the quasistatic loading, the Denatite and the Plexus were brought into a smaller fatigue testing series. The two others were deemed as too weak to be of interest.</p><p>The experiments showed that the methacrylate based Plexus AO420 had a significantly higher strength, in the quasi-static loading it had a maximum recorded load almost ten times as high as the denatite. The fatigue test was inconclusive, although no drastic or unexpected behaviour was recorded. It was noted during the process (unsurprisingly) that the manufacturing process and the skill of the worker is very much affecting the bond strength, sometimes even more than the material properties and the surface treatments applied.</p><p>The experimental method, including the measurements of the crack tip angle could be concluded as a simple but effective method to determine the basic adhesive strength and general characteristics of the different types. The second series of fatigue testing can be performed to see general fatigue behaviour for the adhesives, but as the actual load case on the propeller is unknown it can be difficult to directly relate the fatigue behaviour in the test to the real world. Both test series can be used to determine general behaviour, but due to the complexity of the geometry, deformation modes and loads on the real propeller, it is suggested to perform final testing on a live propeller, rather than trying to draw to large conclusions from isolated experiments.</p>
----------------------------------------------------------------------
In diva2:919807 
abstract is: 
<p>The purpose of this thesis was to verify the simulated wheel-rail forces from the simulation soft-ware Simpack by comparing the results with on-track measurements. The report constitutes a literature study on wheel-rail contact modelling and techniques for measurements of track forces and track irregularities. Furthermore, it describes the veriﬁcation procedure itself and how the model was set up, and subsequently presents the results from the actual comparisons. Three versions of Simpack have been compared that utilize diﬀerent types of contact models. Investigations on alternative ways of ﬁltering track irregularities were conducted, and how the simulated forces were inﬂuenced by using worn wheel and rail proﬁles was also studied.The method was to realistically model the track sections in nine test cases with varying curve radii, considering the nominal track geometry and track irregularities. A vehicle model for the Regina 250 train was used for the simulations, which were then compared with the correspond-ing on-track measurements. Time histories and frequency contents were compared.It was found that the simulated wheel-rail forces correspond well to the measurements, at least for frequencies up to 5-10 Hz, and that no considerable deviations could be found when comparing the contact models. However, the simulation times diﬀered and the discrete contact model introduced in Simpack 9.8 was particularly time demanding.The results shows that it is beneﬁcial to ﬁlter the track irregularities before conducting simula-tions, since it saves simulation time while retaining an adequate accuracy of the track forces.The use of worn wheel and rail proﬁles could in this report not be shown to have any ma-jor impact on the wheel-rail forces, which is in contradiction with the hypothesis. However, it should be noted that the scope for this part was narrow and it merits a more thorough investigation.</p>

corrected abstract:
<p>The purpose of this thesis was to verify the simulated wheel-rail forces from the simulation software Simpack by comparing the results with on-track measurements. The report constitutes a literature study on wheel-rail contact modelling and techniques for measurements of track forces and track irregularities. Furthermore, it describes the verification procedure itself and how the model was set up, and subsequently presents the results from the actual comparisons. Three versions of Simpack have been compared that utilize different types of contact models. Investigations on alternative ways of filtering track irregularities were conducted, and how the simulated forces were influenced by using worn wheel and rail profiles was also studied.</p><p>The method was to realistically model the track sections in nine test cases with varying curve radii, considering the nominal track geometry and track irregularities. A vehicle model for the Regina 250 train was used for the simulations, which were then compared with the corresponding on-track measurements. Time histories and frequency contents were compared.</p><p>It was found that the simulated wheel-rail forces correspond well to the measurements, at least for frequencies up to 5-10 Hz, and that no considerable deviations could be found when comparing the contact models. However, the simulation times differed and the discrete contact model introduced in Simpack 9.8 was particularly time demanding.</p><p>The results shows that it is beneficial to filter the track irregularities before conducting simulations, since it saves simulation time while retaining an adequate accuracy of the track forces.</p><p>The use of worn wheel and rail profiles could in this report not be shown to have any major impact on the wheel-rail forces, which is in contradiction with the hypothesis. However, it should be noted that the scope for this part was narrow and it merits a more thorough investigation.</p>
----------------------------------------------------------------------
In diva2:878361 
abstract is: 
<p>Exposed subsea pipelines operating under high pressure and high temperature may experience a phenomena known as global buckling. Global buckling can be described as an extensive transversal movement of the pipeline once the critical load is reached. Although that global buckling is not a failure mode itself it may lead to other failure modes such as local buckling.</p><p>This master thesis aims to investigate the global buckling phenomena of exposed subsea pipeline in order to create a parametric 3D Finite Element (FE) model in ANSYS and a post processing script in Python for global buckling analyses according to Den Norske Veritas (DNV) standards. To do this, two FE-analyses are performed, the first one for verifying the ANSYS FE-model setup and the second one to evaluate the parametric 3D FE-model for global buckling on uneven seabed. The verification is carried out on a 10 km long pipeline resting on even seabed with a geometrical imperfection and is compared with an analytical solution. This, in order to verify the FE-model setup before it is used in a more complex model with an uneven seabed. The comparison is performed for two different sizes of initial imperfection amplitudes and two different set of orthotropic friction parameters.</p><p>The parametric 3D FE-model in ANSYS is developed according to offshore standard for global buckling, DNV-RP-F110. A 4 km long gas injection pipeline section is analyzed for different functional loads where the pipeline and the seabed is implemented from real routing coordinates. The pipeline is modeled by first order pipe element and the seabed with 4-node target elements. The pipeline initially is resting on link elements, only active in compression, to simplify the installation of the pipeline and is thereafter removed. To be able to describe the orthotropic friction, including lateral break-out-resistance, is three contact elements applied on each pipe element. An elasto-plastic material model including derating effects of the material parameters caused by temperature effects is used for the pipeline. A result post processing script is developed to evaluate the result and determine the utilization according to thecombined load criterion for local buckling in DNV-OS-F101.</p><p>The verification study results show a large difference for very small deformations where the FE-model is softer than the analytical model. This is caused by the analytical solution not considering pre-buckling. After the pre-buckling the FE-model is stiffer than the analytical model which probably is due to the update of the element stiffness matrix for each iteration in the FE-model. Although the large errors in the pre-buckling region the FE-model setup can be considered as valid. The result for the gas injection pipeline shows two lateral buckles, one mode four and one mode three, where the bends in the route act as initiators. Several vertical buckles are also obtained and they are located in free spans where the sag of the pipeline is acting as initiator. The utilization of the pipeline according to the combined load criterion is below required according to DNV-standard for all load cases and no further modification of the pipeline is needed regarding the local buckling. When evaluating the parametric 3D global buckling FE-model it was concluded that converge problems mainly arise from the installation phase. The model is sensitive to large global slopes and large magnitudes of unevenness of the seabed and may result in convergence problems. In those cases, an adjustment in the pipeline installation angle is required to achieve convergence. Furthermore, the multiple contact on each pipeline element for describing the lateral break-out-resistance increases the computational time. An implementation of a new friction model which describes the lateral friction as function of lateral sliding displacement would improve the performance of the model. Besides the convergence issue was the response of the evaluation satisfying and no unphysical behavior in the response could be observed. However, in the FE-model lay tension is introduced after installing the pipeline and may be a source of error, further investigations in this area is recommended. </p>

corrected abstract:
<p>Exposed subsea pipelines operating under high pressure and high temperature may experience a phenomena known as global buckling. Global buckling can be described as an extensive transversal movement of the pipeline once the critical load is reached. Although that global buckling is not a failure mode itself it may lead to other failure modes such as local buckling.</p><p>This master thesis aims to investigate the global buckling phenomena of exposed subsea pipeline in order to create a parametric 3D Finite Element (FE) model in ANSYS and a post processing script in Python for global buckling analyses according to Den Norske Veritas (DNV) standards. To do this, two FE-analyses are performed, the first one for verifying the ANSYS FE-model setup and the second one to evaluate the parametric 3D FE-model for global buckling on uneven seabed. The verification is carried out on a 10 km long pipeline resting on even seabed with a geometrical imperfection and is compared with an analytical solution. This, in order to verify the FE-model setup before it is used in a more complex model with an uneven seabed. The comparison is performed for two different sizes of initial imperfection amplitudes and two different set of orthotropic friction parameters.</p><p>The parametric 3D FE-model in ANSYS is developed according to offshore standard for global buckling, DNV-RP-F110. A 4 km long gas injection pipeline section is analyzed for different functional loads where the pipeline and the seabed is implemented from real routing coordinates. The pipeline is modeled by first order pipe element and the seabed with 4-node target elements. The pipeline initially is resting on link elements, only active in compression, to simplify the installation of the pipeline and is thereafter removed. To be able to describe the orthotropic friction, including lateral break-out-resistance, is three contact elements applied on each pipe element. An elasto-plastic material model including derating effects of the material parameters caused by temperature effects is used for the pipeline. A result post processing script is developed to evaluate the result and determine the utilization according to the combined load criterion for local buckling in DNV-OS-F101.</p><p>The verification study results show a large difference for very small deformations where the FE-model is softer than the analytical model. This is caused by the analytical solution not considering pre-buckling. After the pre-buckling the FE-model is stiffer than the analytical model which probably is due to the update of the element stiffness matrix for each iteration in the FE-model. Although the large errors in the pre-buckling region the FE-model setup can be considered as valid. The result for the gas injection pipeline shows two lateral buckles, one mode four and one mode three, where the bends in the route act as initiators. Several vertical buckles are also obtained and they are located in free spans where the sag of the pipeline is acting as initiator. The utilization of the pipeline according to the combined load criterion is below required according to DNV-standard for all load cases and no further modification of the pipeline is needed regarding the local buckling. When evaluating the parametric 3D global buckling FE-model it was concluded that converge problems mainly arise from the installation phase. The model is sensitive to large global slopes and large magnitudes of unevenness of the seabed and may result in convergence problems. In those cases, an adjustment in the pipeline installation angle is required to achieve convergence. Furthermore, the multiple contact on each pipeline element for describing the lateral break-out-resistance increases the computational time. An implementation of a new friction model which describes the lateral friction as function of lateral sliding displacement would improve the performance of the model. Besides the convergence issue was the response of the evaluation satisfying and no unphysical behavior in the response could be observed. However, in the FE-model lay tension is introduced after installing the pipeline and may be a source of error, further investigations in this area is recommended.</p>
----------------------------------------------------------------------
In diva2:872202 
abstract is: 
<p><strong>Abstact:</strong> In order to address the wind-industry's need for a new generation of more advanced wake models, which accurately quantify the mean flow characteristics within a reasonably CPU-time, the two-dimensional analytical approach by Belcher et al. (2003) has been extended to a three-dimensional wake model. Hereby, the boundary-layer approximation of the Navier-Stokes equations has been linearized around an undisturbed baseflow, assuming that the wind turbines provoke a small perturbation of the velocity field.</p><p>The conducted linearization of the well established actuator-disc theory brought valuable additional insights that could be used to understand the behavior (as well as the limitations) of a model based on linear methods. Hereby, one of the results was that an adjustment of the thrust coecient is necessary in order to get the same wake-velocity field within the used linear framework.</p><p>In this thesis, two different datasets from experiments conducted in two different wind-tunnel facilities were used in order to validate the proposed model against wind-farm and single-turbine cases. The developed model is, in contrary to current engineering wake models, able to account for effects occurring in the upstream flow region. The measurement, as well as the simulations, show that the presence of a wind farm affects the approaching flow even far upstream of the first turbine row, which is not considered in current industrial guidelines. Despite the model assumptions, several velocity statistics above wind farms have been properly estimated, providing insight about the transfer of momentum inside the turbine rows.</p><p>Overall, a promising preliminary version of a wake model is introduced, which can be extended arbitrarily depending on the regarded purpose.</p>

corrected abstract:
<p>In order to address the wind-industry's need for a new generation of more advanced wake models, which accurately quantify the mean flow characteristics within a reasonably CPU-time, the two-dimensional analytical approach by Belcher et al. (2003) has been extended to a three-dimensional wake model. Hereby, the boundary-layer approximation of the Navier-Stokes equations has been linearized around an undisturbed baseflow, assuming that the wind turbines provoke a small perturbation of the velocity field.</p><p>The conducted linearization of the well established actuator-disc theory brought valuable additional insights that could be used to understand the behavior (as well as the limitations) of a model based on linear methods. Hereby, one of the results was that an adjustment of the thrust coefficient is necessary in order to get the same wake-velocity field within the used linear framework.</p><p>In this thesis, two different datasets from experiments conducted in two different wind-tunnel facilities were used in order to validate the proposed model against wind-farm and single-turbine cases. The developed model is, in contrary to current engineering wake models, able to account for effects occurring in the upstream flow region. The measurement, as well as the simulations, show that the presence of a wind farm affects the approaching flow even far upstream of the first turbine row, which is not considered in current industrial guidelines. Despite the model assumptions, several velocity statistics above wind farms have been properly estimated, providing insight about the transfer of momentum inside the turbine rows.</p><p>Overall, a promising preliminary version of a wake model is introduced, which can be extended arbitrarily depending on the regarded purpose.</p>
----------------------------------------------------------------------
In diva2:872195 
abstract is: 
<p>The usage of Pipe-In-Pipe (PIP) solutions for offshore applications has increased during the lastyears. The solution gives high thermal insulation and protects the flow line from environmental impacts. One critical load case is buckling of the pipeline system due to thermal expansions of the inner pipe. This project intends to increase knowledge about PIP systems, investigate the impact of different parameters as well as update parameters in an existing SIMLA model. A FE-model of a PIP system was created in ANSYS with a refined section where pipes and centralizers are modelled with solid elements.The ANSYS-model was tested against a verified FE-model created in SIMLA. The global results obtained from ANSYS and SIMLA did not give a perfect match. The ANSYS model tended to buckle in another way, which is assumed to be related to different modelling of resistance between the pipeline and the seabed as well as unwished properties between the side section and the midsection.Local results obtained from ANSYS showed that there are discontinuities in bending moment and effective axial forces when passing a centralizer. The contact force between centralizer and pipes give rise to high friction forces that acts along the same line as the axial force in the pipes.Increased friction coefficient between centralizer and outer pipe resulted in increased discontinuity in axial force. Selection of a proper friction coefficient thus has significant influence on the results.Centralizer stiffness was evaluated by a local FE-model where a centralizer was compressed between the inner and the outer pipe. Displacement of inner pipe was evaluated as a function of applied force. The result showed that the force-displacement curve describing centralizer stiffness follows </p><p> Q (Δ)=(<img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?C%7B1%7D" /> Δ) <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cfrac%7B2%7D%7B3%7D" /></p><p>where <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?C%7B1%7D" />     is a constant depending on dimensions and material of the centralizer. Linearized indifferent sections and with a centralizer thickness of 0,1 meter the following expression gave stiffnesses in the range 100-1000 MN/m, which agrees with stiffnesses used in the SIMLA model.displacements up to 0.3 mm the radial stiffness used in SIMLA is still good to use.</p>

corrected abstract:
<p>The usage of Pipe-In-Pipe (PIP) solutions for offshore applications has increased during the last years. The solution gives high thermal insulation and protects the flow line from environmental impacts. One critical load case is buckling of the pipeline system due to thermal expansions of the inner pipe. This project intends to increase knowledge about PIP systems, investigate the impact of different parameters as well as update parameters in an existing SIMLA model. A FE-model of a PIP system was created in ANSYS with a refined section where pipes and centralizers are modelled with solid elements. The ANSYS-model was tested against a verified FE-model created in SIMLA. The global results obtained from ANSYS and SIMLA did not give a perfect match. The ANSYS model tended to buckle in another way, which is assumed to be related to different modelling of resistance between the pipeline and the seabed as well as unwished properties between the side section and the midsection. Local results obtained from ANSYS showed that there are discontinuities in bending moment and effective axial forces when passing a centralizer. The contact force between centralizer and pipes give rise to high friction forces that acts along the same line as the axial force in the pipes. Increased friction coefficient between centralizer and outer pipe resulted in increased discontinuity in axial force. Selection of a proper friction coefficient thus has significant influence on the results. Centralizer stiffness was evaluated by a local FE-model where a centralizer was compressed between the inner and the outer pipe. Displacement of inner pipe was evaluated as a function of applied force. The result showed that the force-displacement curve describing centralizer stiffness follows </p><p> Q (Δ)=(<img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?C%7B1%7D" /> Δ) <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cfrac%7B2%7D%7B3%7D" /></p><p>where <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?C%7B1%7D" /> is a constant depending on dimensions and material of the centralizer. Linearized indifferent sections and with a centralizer thickness of 0,1 meter the following expression gave stiffnesses in the range 100-1000 MN/m, which agrees with stiffnesses used in the SIMLA model. displacements up to 0.3 mm the radial stiffness used in SIMLA is still good to use.</p>
----------------------------------------------------------------------
In diva2:872172 
abstract is: 
<p>The ride comfort in heavy commercial trucks is an important property that requires detailed testing to investigate how different vehicle components affect the response to road input. Trucks come in many different configurations and a customer purchasing a new truck often has the choice to specify number of axles, drive line and cabin type, type of suspension and so on. This gives that there are many vehicle variations that have to be tested to ensure good ride comfort. Testing many combinations of, for example, dampers in different environments and road conditions might be time consuming. The work could be helped by pre-test computer simulations where the vehicle is simulated with the same conditions as in the tests. The simulation results could then be used to better understand how different components affect the vehicle response to a certain road input.</p><p>By using a MultiBody System (MBS) software, a full truck could be modeled and simulated to acquire accurate results. The simulation would however be computationally demanding and take long time. It also requires that the test engineer is familiar with the MBS software to be able to create the model and run the simulations. This thesis focuses on investigating if simplified dynamic truck models developed in Matlab could be an alternative to more complex models created in an MBS software.</p><p>Three different models are developed: a quarter car model, a 2D half truck model and a 3D truck model. The models are derived using the Lagrangian energy method and the dynamic response from a given road input is calculated numerically in Matlab. Different methods of solving the systems of differential equations are discussed and the implementation of the implicit Newmark -method is explained. To validate the truck models and solver, the models are replicated in the MBS software Adams View. The response of the Adams and Matlab models from an excitation on the wheels are compared to determine that the equations and solver are correctly derived and implemented. To test the models capabilities to predict the response in a real truck, tests on a road simulator are performed. A four-wheel Scania tractor is tested in a hydraulic road simulator rig. The road simulator excited the truck through the wheels with a sinus sweep from 0-20 Hz and the resulting accelerations in the tractor are measured. Three different setups of front axle dampers are tested to get a parameter variation to study with the models: a standard damper, harder damper and undamped front axle. The same tests are simulated in Matlab and the acceleration responses are compared to see how well the models predict the accelerations seen in the real truck.</p><p>The models in Matlab and Adams give the same results and are therefore reasoned to be mathematically correct. The Newmark -method is efficient and gives reasonable computing times. In the comparison with the road simulator test the models do not give the same results as measured on the truck. To be able to compare the results from the measurements and simulations, the tire stiffnesses have to be trimmed so that the correct eigenfrequency of the axles are found. The results with modified tire stiffnesses give better results but still with considerable deviations from the experimental results. The measurements on the truck show that the eigenfrequency of the front axle decrease when removing the front axle damper while the models show that the eigenfrequency increase. Also there are differences in the acceleration measured in the cabin and frame as the models do not predict many of the higher eigenfrequencies.</p><p>In conclusion it is discussed that the models have to be more complex to give useful information about the effects of variation of dampers on the axles. It is also discussed that using commercially available software to perform the same simulations might be a better alternative that gives the user more freedom to overlook and make changes to the model.</p>

corrected abstract:
<p>The ride comfort in heavy commercial trucks is an important property that requires detailed testing to investigate how different vehicle components affect the response to road input. Trucks come in many different configurations and a customer purchasing a new truck often has the choice to specify number of axles, drive line and cabin type, type of suspension and so on. This gives that there are many vehicle variations that have to be tested to ensure good ride comfort. Testing many combinations of, for example, dampers in different environments and road conditions might be time consuming. The work could be helped by pre-test computer simulations where the vehicle is simulated with the same conditions as in the tests. The simulation results could then be used to better understand how different components affect the vehicle response to a certain road input.</p><p>By using a MultiBody System (MBS) software, a full truck could be modeled and simulated to acquire accurate results. The simulation would however be computationally demanding and take long time. It also requires that the test engineer is familiar with the MBS software to be able to create the model and run the simulations. This thesis focuses on investigating if simplified dynamic truck models developed in Matlab could be an alternative to more complex models created in an MBS software.</p><p>Three different models are developed: a quarter car model, a 2D half truck model and a 3D truck model. The models are derived using the Lagrangian energy method and the dynamic response from a given road input is calculated numerically in Matlab. Different methods of solving the systems of differential equations are discussed and the implementation of the implicit Newmark &#x1D6FD;-method is explained. To validate the truck models and solver, the models are replicated in the MBS software Adams View. The response of the Adams and Matlab models from an excitation on the wheels are compared to determine that the equations and solver are correctly derived and implemented. To test the models capabilities to predict the response in a real truck, tests on a road simulator are performed. A four-wheel Scania tractor is tested in a hydraulic road simulator rig. The road simulator excited the truck through the wheels with a sinus sweep from 0-20 Hz and the resulting accelerations in the tractor are measured. Three different setups of front axle dampers are tested to get a parameter variation to study with the models: a standard damper, harder damper and undamped front axle. The same tests are simulated in Matlab and the acceleration responses are compared to see how well the models predict the accelerations seen in the real truck.</p><p>The models in Matlab and Adams give the same results and are therefore reasoned to be mathematically correct. The Newmark &#x1D6FD;-method is efficient and gives reasonable computing times. In the comparison with the road simulator test the models do not give the same results as measured on the truck. To be able to compare the results from the measurements and simulations, the tire stiffnesses have to be trimmed so that the correct eigenfrequency of the axles are found. The results with modified tire stiffnesses give better results but still with considerable deviations from the experimental results. The measurements on the truck show that the eigenfrequency of the front axle decrease when removing the front axle damper while the models show that the eigenfrequency increase. Also there are differences in the acceleration measured in the cabin and frame as the models do not predict many of the higher eigenfrequencies.</p><p>In conclusion it is discussed that the models have to be more complex to give useful information about the effects of variation of dampers on the axles. It is also discussed that using commercially available software to perform the same simulations might be a better alternative that gives the user more freedom to overlook and make changes to the model.</p>


Note: &#x1D6FD; is a Mathematical Italic Small Beta.
----------------------------------------------------------------------
diva2:818384 - .correct as is
----------------------------------------------------------------------
In diva2:814431 Note: no full text in DiVA
abstract is: 
<p>The last decades have seen an increasing use of databases storing sequence information for proteins. These databases have become large and are continually growing. Methods to reduce complexity and redundancy of these databases have been developed. Some methods employ the Self-Organizing Map (SOM) and its extension Median Self-Organizing Map (M-SOM) for clustering and visualisation of databases containing protein sequences. These methods have shown success in the clustering of protein sequences, but rely on the conversion of the symbolic protein sequences to real vectors in some stage of the training; either, using converted sequences during the whole training of the map in the case of the SOM, or using converted sequences during the initialization of the map in the case of the M-SOM. In this work, the feasibility of initializing the M-SOM with a method that avoids the conversionof the symbolic sequences to real vectors is investigated. This is doneby implementing an M-SOM and comparing the qualitative clustering of protein sequences when using either a random initialization, an initialization using a converted vectorial representation of the sequences, or a non-vectorial initialization that does not rely on conversion of the symbolic sequences. The different initialization methods are compared using topographic error, quantization error, frequency plots of mapped groups of categorized sequences, and U-matrices. Using these methods and measurements no denite reason to prefer either the vectorial or the non-vectorial initialization above the other could be determined, but the absence of a need for the conversion of sequences to real vectorscould favour the non-vectorial method.</p>

corrected abstract:
<p>The last decades have seen an increasing use of databases storing sequence information for proteins. These databases have become large and are continually growing. Methods to reduce complexity and redundancy of these databases have been developed. Some methods employ the Self-Organizing Map (SOM) and its extension Median Self-Organizing Map (M-SOM) for clustering and visualisation of databases containing protein sequences. These methods have shown success in the clustering of protein sequences, but rely on the conversion of the symbolic protein sequences to real vectors in some stage of the training; either, using converted sequences during the whole training of the map in the case of the SOM, or using converted sequences during the initialization of the map in the case of the M-SOM. In this work, the feasibility of initializing the M-SOM with a method that avoids the conversion of the symbolic sequences to real vectors is investigated. This is done by implementing an M-SOM and comparing the qualitative clustering of protein sequences when using either a random initialization, an initialization using a converted vectorial representation of the sequences, or a non-vectorial initialization that does not rely on conversion of the symbolic sequences. The different initialization methods are compared using topographic error, quantization error, frequency plots of mapped groups of categorized sequences, and U-matrices. Using these methods and measurements no definite reason to prefer either the vectorial or the non-vectorial initialization above the other could be determined, but the absence of a need for the conversion of sequences to real vectors could favour the non-vectorial method.</p>
----------------------------------------------------------------------
In diva2:814233 - Note: no full text in DiVA
abstract is: 
<p>The segmentation of magnetic resonance (MR) brain images into white matter (WM), gray matter (GM) and cerebrospinal uid (CSF) play an importantrole in diagnostics and research of brain diseases. A multitude of approaches to automatic brain tissue segmentation have been reported in literature. In this paper, we evaluate the effectiveness of a deep convolutional neural network (CNN) by reducing the segmentation task to a classication problem, an area in which CNNs have proven effective. Our CNN implementation analyses the neighborhood around each voxel to correctly classify the tissue type. The network was trained and evaluated on the IBSR in accordance with the comparison methodology for segmentation algorithms presented in Valverde et al. The results show promising performance and potential for the voxel based CNN approach to brain tissue segmentation.</p>

corrected abstract:
<p>The segmentation of magnetic resonance (MR) brain images into white matter (WM), gray matter (GM) and cerebrospinal uid (CSF) play an important role in diagnostics and research of brain diseases. A multitude of approaches to automatic brain tissue segmentation have been reported in literature. In this paper, we evaluate the effectiveness of a deep convolutional neural network (CNN) by reducing the segmentation task to a classication problem, an area in which CNNs have proven effective. Our CNN implementation analyses the neighborhood around each voxel to correctly classify the tissue type. The network was trained and evaluated on the IBSR in accordance with the comparison methodology for segmentation algorithms presented in Valverde et al. The results show promising performance and potential for the voxel based CNN approach to brain tissue segmentation.</p>
----------------------------------------------------------------------
In diva2:787508 
abstract is: 
<p>Future aircraft generations are required to have higher performance and capacities.</p><p>This achievement should be fulfilled with the minimum cost and environmental</p><p>impact. This calls for the design of new unconventional configurations, such as the</p><p>Blended Wing Body (BWB), a tailless aircraft which integrates the wing and the</p><p>fuselage into a single lifting surface. It has been proven in previously published</p><p>works that this concept is feasible, has an efficient economical performance and</p><p>is a promising candidate for solving the current air traffic problems, despite its</p><p>challenging control and stability features. Moreover, the size of the vertical surfaces,</p><p>such as the winglets, condition the radar detectability of the BWB model,</p><p>especially for military missions. The goal of the department of Aeronautical and</p><p>Vehicle Engineering at the Royal Institute of Technology (KTH) and of the department</p><p>of Air Transport Systems of the German Aerospace Centre (DLR) is to</p><p>investigate new ways to improve the conceptual design process of the aircrafts in</p><p>a multidisciplinary environment. In order to design future unconventional aircraft</p><p>configurations (such as the Blended Wing Body), the CEASIOM (Computerised</p><p>Environment for Aircraft Synthesis and Integrated Optimisation Methods) geometry</p><p>module, AcBuilder, is replaced and enhanced by implementing the Common</p><p>Parametric Aircraft Configuration Scheme (CPACS), developed by the DLR as</p><p>a basis technology. CPACS is meant to become a unified software framework to</p><p>allow the sharing of the work and information, making it accessible for every person.</p><p>It requires an implementation of the software modules in a framework using</p><p>a common language for all the tools, in order to make later alterations of this</p><p>framework easier. A detailed research of the latest developments and advances</p><p>in the BWB concept was performed in order to identify the main principles and</p><p>best design options. Afterwards, by using the implemented improved tool CPACSCreator</p><p>(CC) based on CPACS, instead of Acbuilder, a BWB aircraft baseline</p><p>was designed. The aerodynamic behaviour and performance of this model were</p><p>then analyzed with the aid of the improved CEASIOM platform, with an special</p><p>emphasis on its control and stability features. This analysis enables to improve</p><p>the baseline design and the allocation and size of the control surfaces was studied</p><p>and optimized. The minimum winglet required for a target flight performance was</p><p>identified, due to its importance for reducing the drag and the radar detectability</p><p>of the aircraft.</p>

corrected abstract:
<p>Future aircraft generations are required to have higher performance and capacities. This achievement should be fulfilled with the minimum cost and environmental impact. This calls for the design of new unconventional configurations, such as the Blended Wing Body (BWB), a tailless aircraft which integrates the wing and the fuselage into a single lifting surface. It has been proven in previously published works that this concept is feasible, has an efficient economical performance and is a promising candidate for solving the current air traffic problems, despite its challenging control and stability features. Moreover, the size of the vertical surfaces, such as the winglets, condition the radar detectability of the BWB model, especially for military missions. The goal of the department of Aeronautical and Vehicle Engineering at the Royal Institute of Technology (KTH) and of the department of Air Transport Systems of the German Aerospace Centre (DLR) is to investigate new ways to improve the conceptual design process of the aircrafts in a multidisciplinary environment. In order to design future unconventional aircraft configurations (such as the Blended Wing Body), the CEASIOM (Computerised Environment for Aircraft Synthesis and Integrated Optimisation Methods) geometry module, AcBuilder, is replaced and enhanced by implementing the Common Parametric Aircraft Configuration Scheme (CPACS), developed by the DLR as a basis technology. CPACS is meant to become a unified software framework to allow the sharing of the work and information, making it accessible for every person. It requires an implementation of the software modules in a framework using a common language for all the tools, in order to make later alterations of this framework easier. A detailed research of the latest developments and advances in the BWB concept was performed in order to identify the main principles and best design options. Afterwards, by using the implemented improved tool CPACSCreator (CC) based on CPACS, instead of Acbuilder, a BWB aircraft baseline was designed. The aerodynamic behaviour and performance of this model were then analyzed with the aid of the improved CEASIOM platform, with an special emphasis on its control and stability features. This analysis enables to improve the baseline design and the allocation and size of the control surfaces was studied and optimized. The minimum winglet required for a target flight performance was identified, due to its importance for reducing the drag and the radar detectability of the aircraft.</p>
----------------------------------------------------------------------
In diva2:784016 
abstract is: 
<p>This is a master’s thesis performed at the Department of Shipping and Marine Technology research group in Hydrodynamics at Chalmers University of Technology and is written for the Center for Naval Architecture at the Royal Institute of Technology, KTH.In order to meet increased requirements on efficient ship propulsions with low noise level, it is important to consider the complete system with both the hull and the propeller in the simulation.OpenFOAM (Open Field Operation and Manipulation) provides different techniques to simulate a rotating propeller with different physical and computational properties. MRF (The Multiple Reference Frame Model) is, perhaps, the easiest way but is a computationally efficient technique to model a rotating frame of reference. The sliding grid techniques provide the more complex way to simulate the propeller and its surrounding region, rotating and interpolate on interface for transient effects. AMI, Arbitrary Mesh Interface, is a sliding grid implementation which is available in the recent versions of OpenFOAM, introduced in the official releases after v2.1.0.In this study, the main objective is to compare these two techniques, MRF and AMI, to perform the open water characteristics of the propeller with the Reynolds-Averaged Navier-Stokes equation computations (RANS) and study the accuracy in parallel performance and the benefits of each approach.More specifically, a self-propelled ship is simulated to study the interaction between the hull and propeller. In order to simplify and decrease the computational complexity the free surface is not considered. The ship under investigation is a 7000 DWT chemical tanker which is subject of a collaborative R&amp;D project called STREAMLINE, strategic research for innovative marine propulsion concepts. In self-propelled condition, the transient forces on the propeller shall be evaluated. This study investigates the results of the experimental work with advanced CFD for accurate analysis and design of the propulsion. In this thesis, all simulations are conducted by using parallel computing. Therefore, a scalability analysis is studied to find out how to affect the average computational time by using different number of nodes.</p>

corrected abstract:
<p>This is a master’s thesis performed at the Department of Shipping and Marine Technology research group in Hydrodynamics at Chalmers University of Technology and is written for the Center for Naval Architecture at the Royal Institute of Technology, KTH.</p><p>In order to meet increased requirements on efficient ship propulsions with low noise level, it is important to consider the complete system with both the hull and the propeller in the simulation. OpenFOAM (Open Field Operation and Manipulation) provides different techniques to simulate a rotating propeller with different physical and computational properties. MRF (The Multiple Reference Frame Model) is, perhaps, the easiest way but is a computationally efficient technique to model a rotating frame of reference. The sliding grid techniques provide the more complex way to simulate the propeller and its surrounding region, rotating and interpolate on interface for transient effects. AMI, Arbitrary Mesh Interface, is a sliding grid implementation which is available in the recent versions of OpenFOAM, introduced in the official releases after v2.1.0. In this study, the main objective is to compare these two techniques, MRF and AMI, to perform the open water characteristics of the propeller with the Reynolds-Averaged Navier-Stokes equation computations (RANS) and study the accuracy in parallel performance and the benefits of each approach.</p><p>More specifically, a self-propelled ship is simulated to study the interaction between the hull and propeller. In order to simplify and decrease the computational complexity the free surface is not considered. The ship under investigation is a 7000 DWT chemical tanker which is subject of a collaborative R&amp;D project called STREAMLINE, strategic research for innovative marine propulsion concepts. In self-propelled condition, the transient forces on the propeller shall be evaluated. This study investigates the results of the experimental work with advanced CFD for accurate analysis and design of the propulsion. In this thesis, all simulations are conducted by using parallel computing. Therefore, a scalability analysis is studied to find out how to affect the average computational time by using different number of nodes.</p>
----------------------------------------------------------------------
In diva2:756124 
abstract is: 
<p>Gravitational lenses are unlike ordinary optical lenses, a phenomenon corrupting our optical view of the cosmos. When we look out intospace we see celestial bodies lying with astronomical distance from Earth. There is no guarantee that what we see out there in space is actually there where we are observing it, and it may be that we see multiple images of the same star! Studies have shown that thenumber of distorted images of an n-mass (n &gt; 1) plane lens can not exceed 5(n -1) for lighter lenses or 5n for more massive lens. This mathematical thesis covers gravitational lensing and their properties that distort light rays coming towards us from distant celestial bodies. In addition of gravitational lensing, its properties and the proof of the maximum number of distorted images, a bigger part of this work even cover two very interesting mathematical tools - the so-called Schwarz function and Bezout theorem.</p>

corrected abstract:
<p>Gravitational lenses are unlike ordinary optical lenses, a phenomenon corrupting our optical view of the cosmos. When we look out into space we see celestial bodies lying with astronomical distance from Earth. There is no guarantee that what we see out there in space is actually there where we are observing it, and it may be that we see multiple images of the same star! Studies have shown that the number of distorted images of an 𝑛-mass (𝑛 &gt; 1) plane lens can not exceed 5(𝑛 - 1) for lighter lenses or 5𝑛 for more massive lens. This mathematical thesis covers gravitational lensing and their properties that distort light rays coming towards us from distant celestial bodies. In addition of gravitational lensing, its properties and the proof of the maximum number of distorted images, a bigger part of this work even cover two very interesting mathematical tools - the so-called Schwarz function and Bézout theorem.</p>
----------------------------------------------------------------------
In diva2:741918 
abstract is: 
<p>For modeling thermal heat transfer, not only the effects of convection and conduction are relevant, but also thermal and visible radiation. Radiation is especially important for setups with large temperature differences, as well as for interaction with external light sources.Common computational fluid dynamic models usually treat radiation transport as a minor effect, that can be handled by simplified algorithms. All these normal models, e.g. surface to surface model, discrete transfer model, P_N method, discrete ordinates model, exhibit disadvantages in the computing performance and the physical modeling.</p><p>Hence, there are many technical applications, where the fluid simulation are limited both in accuracy and calculation time by the available radiation model. As exemplary cases combustion chambers, smoke and soot creation, solar power generation, UV water disinfection, condensation in car headlights, fusion and fission reactor chambers, electric arc movement, as well as low-emissivity glass windows can be named.</p><p>In the fields investigating radiation as main effect, e.g. cinematic 3d animation or illumination simulation for lamps and workspaces, the mentioned methods are not in use anymore as ray tracing is the first choice.</p><p>In this work, the existing methods for ray tracing were adapted and implemented with the goal to interact with fluid flow simulations and replace existing radiation modeling. This can be regarded as innovative, interdisciplinary method for the interaction of fluids and solids with radiation, incorporating physical effects that could not be included in previous simulations.</p><p>While in usual light calculations, the geometry exists solely in the form of surfaces and their triangulation, fluid flow requires volumetric calculation grids. Hence, methods are implemented that actually use the volumetric grid, and incorporate volumetric effects with little additional effort.</p><p>Spectral volumetric path tracing with Monte Carlo integrated, importance sampled emission was hence the method of choice for this work.</p><p>The implemented ray tracer is able to emit radiation from point sources, geometric surfaces, as well as from volumetric sources. Spectral dependence of material values is treated using radiation bands with hardly no increase of calculation time, whereas in all other models, the calculation time scales linearly with the amount of bands. Direct, diffuse and mixed surface reflection is modeled. The volumetric refraction index is implemented, so refraction is modeled, even including partial and total reflexion. The focusing of lenses or mirror systems can hence be simulated satisfactory, which cannot be treated sufficiently by any other radiation model. Surface and volumetric absorption are implemented, as well as surface and volumetric scattering effects.</p><p>The radiation emission can be caused by a temperature field at surfaces and volumes. These fields are imported from software calculating the fluid and the thermal system. Ray tracing results in volumetric and surface heat sources that can be returned to the original code, and their effect further calculations.</p><p>This coupling was implemented and tested with the commercial computational fluid dynamics code Fluent, using its plug-in interface. As most of Fluent's radiation models are only performed after a fixed number of implicit flow and turbulence iterations, no further disadvantages or limitations occur, that are not as well existing for the existing radiation simulations. A fully implicit treatment of radiation is unlikely to be performed, as stability is already sufficient for most applications. Of course, systems containing only heat sources caused by light and no secondary heat radiation can be treated by the implemented ray tracer with high performance.</p><p>The implemented ray tracer is validated with analytically solved systems, and compared to quantitative simulation results of other simulation methods. Also, the scattering effects are validated against experimental and simulation results from literature.</p><p>The observed calculation performance is similar or faster then for standard models with geometries of approximately 150000 volume elements, while the modeling is done more accurately. For larger models, even larger advantages can be expected.</p>

corrected abstract:
<p>For modeling thermal heat transfer, not only the effects of convection and conduction are relevant, but also thermal and visible radiation. Radiation is especially important for setups with large temperature differences, as well as for interaction with external light sources. Common computational fluid dynamic models usually treat radiation transport as a minor effect, that can be handled by simplified algorithms. All these normal models, e.g. surface to surface model, discrete transfer model, P<sub>N</sub> method, discrete ordinates model, exhibit disadvantages in the computing performance and the physical modeling.</p><p>Hence, there are many technical applications, where the fluid simulation are limited both in accuracy and calculation time by the available radiation model. As exemplary cases combustion chambers, smoke and soot creation, solar power generation, UV water disinfection, condensation in car headlights, fusion and fission reactor chambers, electric arc movement, as well as low-emissivity glass windows can be named.</p><p>In the fields investigating radiation as main effect, e.g. cinematic 3d animation or illumination simulation for lamps and workspaces, the mentioned methods are not in use anymore as ray tracing is the first choice.</p><p>In this work, the existing methods for ray tracing were adapted and implemented with the goal to interact with fluid flow simulations and replace existing radiation modeling. This can be regarded as innovative, interdisciplinary method for the interaction of fluids and solids with radiation, incorporating physical effects that could not be included in previous simulations.</p><p>While in usual light calculations, the geometry exists solely in the form of surfaces and their triangulation, fluid flow requires volumetric calculation grids. Hence, methods are implemented that actually use the volumetric grid, and incorporate volumetric effects with little additional effort.</p><p>Spectral volumetric path tracing with Monte Carlo integrated, importance sampled emission was hence the method of choice for this work.</p><p>The implemented ray tracer is able to emit radiation from point sources, geometric surfaces, as well as from volumetric sources. Spectral dependence of material values is treated using radiation bands with hardly no increase of calculation time, whereas in all other models, the calculation time scales linearly with the amount of bands. Direct, diffuse and mixed surface reflection is modeled. The volumetric refraction index is implemented, so refraction is modeled, even including partial and total reflexion. The focusing of lenses or mirror systems can hence be simulated satisfactory, which cannot be treated sufficiently by any other radiation model. Surface and volumetric absorption are implemented, as well as surface and volumetric scattering effects.</p><p>The radiation emission can be caused by a temperature field at surfaces and volumes. These fields are imported from software calculating the fluid and the thermal system. Ray tracing results in volumetric and surface heat sources that can be returned to the original code, and their effect further calculations.</p><p>This coupling was implemented and tested with the commercial computational fluid dynamics code Fluent, using its plug-in interface. As most of Fluent's radiation models are only performed after a fixed number of implicit flow and turbulence iterations, no further disadvantages or limitations occur, that are not as well existing for the existing radiation simulations. A fully implicit treatment of radiation is unlikely to be performed, as stability is already sufficient for most applications. Of course, systems containing only heat sources caused by light and no secondary heat radiation can be treated by the implemented ray tracer with high performance.</p><p>The implemented ray tracer is validated with analytically solved systems, and compared to quantitative simulation results of other simulation methods. Also, the scattering effects are validated against experimental and simulation results from literature.</p><p>The observed calculation performance is similar or faster then for standard models with geometries of approximately 150000 volume elements, while the modeling is done more accurately. For larger models, even larger advantages can be expected.</p>
----------------------------------------------------------------------
In diva2:730944 
abstract is: 
<p>A Hilbert space H is the abstraction of a nite-dimensional Eu-clidean space. The spectrum of a bounded linear operator A : H !H , denoted (A), is given by all numbers 2 C such that (A􀀀I) isnot invertible. The shift operators are one type of bounded linear op-erators. In this report we prove ve claims regarding the spectrum ofthe shifts. We work in the Hilbert space `2 which consists of all squaresummable sequences, both single sided (x0; x1; x2; :::) and double sided(:::x􀀀1; x0; x1; :::). One of the most general results proved applies tothe weighted unilateral shift S dened for (x0; x1; x2; :::) 2 `2 byS(x0; x1; x2; :::) = (0; 0x0; 1x1; 2x2; :::)where fng is a bounded arbitrary weight sequence with n &gt; 0 forall n 0.</p><p>Theorem. Let r(S) be the radius of the smallest disc which contain(S).</p><p>Then(S) = f : jj r(S)g:</p>

corrected abstract:
<p>A Hilbert space ℋ is the abstraction of a finite-dimensional Euclidean space. The spectrum of a bounded linear operator 𝐴 : ℋ &rarr; ℋ, denoted &sigma;(𝐴), is given by all numbers &lambda; &isin; ℂ such that (𝐴 - &lambda;𝐼) is not invertible. The shift operators are one type of bounded linear operators. In this report we prove five claims regarding the spectrum of the shifts. We work in the Hilbert space ℓ<sup>2</sup> which consists of all square summable sequences, both single sided (𝑥<sub>0</sub>, 𝑥<sub>1</sub>, 𝑥<sub>2</sub>, &hellip;) and double sided (&hellip;𝑥<sub>-1</sub>, 𝑥<sub>0</sub>, 𝑥<sub>1</sub>, &hellip;). One of the most general results proved applies to the weighted unilateral shift 𝑆<sub>&alpha;</sub> defined for (𝑥<sub>0</sub>, 𝑥<sub>1</sub>, 𝑥<sub>2</sub>, &hellip;) &isin; ℓ<sup>2</sup> by</p>
<p style="text-align: center;">S<sub>&alpha;</sub>(𝑥<sub>0</sub>, 𝑥<sub>1</sub>, 𝑥<sub>2</sub>, &hellip;) = (0, &alpha;<sub>0</sub>𝑥<sub>0</sub>, &alpha;<sub>1</sub>𝑥<sub>1</sub>, &alpha;<sub>2</sub>𝑥<sub>2</sub>, &hellip;)</p>
<p>where {&alpha;<sub>𝑛</sub>} is a bounded arbitrary weight sequence with &alpha;<sub>𝑛</sub> &gt; 0 forall 𝑛 &ge; 0.</p>
<p><strong>Theorem</strong>. <em>Let</em> 𝑟(S<sub>&alpha;</sub>) <em>be the radius of the smallest disc which contain</em> &sigma;(𝑆<sub>&alpha;</sub>). <em>Then </em></p>
<p style="text-align: center;">&sigma;(𝑆<sub>&alpha;</sub>) = {&lambda;: |&lambda;| &le; 𝑟(𝑆<sub>&alpha;</sub>)}.</p>
----------------------------------------------------------------------
In diva2:708379 - Note: no full text in DiVA
abstract is: 
<p>Diesel Engine has been the most powerful and relevant source of power in the automobile industry for decades due to their excellent performance, efficiency and power. On the contrary, there are numerous environmental issues of the diesel engines hampering the environment. It has been a great challenge for the researchers and scientists to minimize these issues. In the recent years, several strategies have been introduced to eradicate the emissions of the diesel engines. Among them, Partially Premixed Combustion (PPC) is one of the most emerging and reliable strategies. PPC is a compression ignited combustion process in which ignition delay is controlled. PPC is intended to endow with better combustion with low soot and NOx emission. The engine used in the present study is a single-cylinder research engine, installed in Aalto University Internal Combustion Engine Laboratory with the bore diameter of 200 mm. The thesis presents the validation of the measurement data with the simulated cases followed by the study of the spray impingement and fuel vapor mixing in PPC mode for different injection timing. A detailed study of the correlation of early injection with the fuel vapor distribution and wall impingement has been made. The simulations are carried out with the commercial CFD software STAR CD. Different injection parameters have been considered and taken into an account to lower the wall impingement and toproduce better air-fuel mixing with the purpose of good combustion and reduction of the emissions. The result of the penetration length of the spray and the fuel vapor distribution for different early injection cases have been illustrated in the study. Comparisons of different thermodynamic properties and spray analysis for different injection timing have been very clearly illustrated to get insight of effect of early injection. The parameters like injection timing, injection period, injection pressure, inclusion angle of the spray have an influence the combustion process in PPC mode. Extensive study has been made for each of these parameters to better understand their effects in the combustion process. Different split injection profiles have been implemented for the study of better fuel vapor distribution in the combustion chamber. The final part of the thesis includes the study of the combustion and implementation of EGR to control the temperature so as to get more prolonged ignition delay to accompany the PPC strategy for standard piston top and deep bowl piston top. With the injection optimization and implementation of EGR, NOx has been reduced by around 44%, CO by 60% and Soot by 66% in the standard piston top. The piston optimization resulted in more promising result with 58% reduction in NOx, 55% reduction in CO and 67% reduction in Soot. In both cases the percentage of fuel burnt was increased by around 8%.</p>

corrected abstract:
<p>Diesel Engine has been the most powerful and relevant source of power in the automobile industry for decades due to their excellent performance, efficiency and power. On the contrary, there are numerous environmental issues of the diesel engines hampering the environment. It has been a great challenge for the researchers and scientists to minimize these issues. In the recent years, several strategies have been introduced to eradicate the emissions of the diesel engines. Among them, Partially Premixed Combustion (PPC) is one of the most emerging and reliable strategies. PPC is a compression ignited combustion process in which ignition delay is controlled. PPC is intended to endow with better combustion with low soot and NOx emission. The engine used in the present study is a single-cylinder research engine, installed in Aalto University Internal Combustion Engine Laboratory with the bore diameter of 200 mm. The thesis presents the validation of the measurement data with the simulated cases followed by the study of the spray impingement and fuel vapor mixing in PPC mode for different injection timing. A detailed study of the correlation of early injection with the fuel vapor distribution and wall impingement has been made. The simulations are carried out with the commercial CFD software STAR CD. Different injection parameters have been considered and taken into an account to lower the wall impingement and to produce better air-fuel mixing with the purpose of good combustion and reduction of the emissions. The result of the penetration length of the spray and the fuel vapor distribution for different early injection cases have been illustrated in the study. Comparisons of different thermodynamic properties and spray analysis for different injection timing have been very clearly illustrated to get insight of effect of early injection. The parameters like injection timing, injection period, injection pressure, inclusion angle of the spray have an influence the combustion process in PPC mode. Extensive study has been made for each of these parameters to better understand their effects in the combustion process. Different split injection profiles have been implemented for the study of better fuel vapor distribution in the combustion chamber. The final part of the thesis includes the study of the combustion and implementation of EGR to control the temperature so as to get more prolonged ignition delay to accompany the PPC strategy for standard piston top and deep bowl piston top. With the injection optimization and implementation of EGR, NOx has been reduced by around 44%, CO by 60% and Soot by 66% in the standard piston top. The piston optimization resulted in more promising result with 58% reduction in NOx, 55% reduction in CO and 67% reduction in Soot. In both cases the percentage of fuel burnt was increased by around 8%.</p>
----------------------------------------------------------------------
In diva2:632708 - missing space in title:
"Computational methods to estimate error rates forpeptide identifications in mass spectrometry-based proteomics"
==>
"Computational methods to estimate error rates for peptide identifications in mass spectrometry-based proteomics"

abstract is: 
<p>In the field of proteomics, tandem mass spectrometry is the core technology which promises to identify peptide components within complex mixtures on a large scale. Currently the bottleneck is to reduce the error rates and assign accurate statistical estimates of peptide identifications.</p><p>In this work, we introduce the techniques of identifying chimeric spectra, where two or more precursor ions with similar mass and retention time are co-fragmented and sequenced by the MS/MS instrument. Based on this, we try to analyze the factor which leads to the high error rate of identifications. We show that chimeric spectra have high correlations with the ranking scores and can reduce the number of positive identifications.</p><p>Additionally, we address the problem of assigning a posterior error probability (PEP) to the individual peptide-spectrum matches (PSMs) that are obtained via search engines. This problem is computationally more difficult than estimating the error rate associated with a large collection of PSMs, such as false discovery rate (FDR). Existing methods rely on parametric or semiparametric models of the underlying score distribution as preassumption.We provide a so-called kernel logistic regression procedure without any explicit assumptions about the score distribution. Based on an appropriate positive definite Gaussian kernel, the resulting PEP estimate is proven to be robust by achieving a close correspondence between the PEP-derived q-values and FDR-derived q-values. Furthermore, we also accept at least 200 more significant PSMs with setting a threshold based on PEP-derived q-values compared to FDR-derived q-values. Finally, we show that this kernel logistic regression method is well established in the statistics literature and it can produce accurate PEP estimates for different types of PSM score functions and data.</p>

corrected abstract:
<p>In the field of proteomics, tandem mass spectrometry is the core technology which promises to identify peptide components within complex mixtures on a large scale. Currently the bottleneck is to reduce the error rates and assign accurate statistical estimates of peptide identifications.</p><p>In this work, we introduce the techniques of identifying chimeric spectra, where two or more precursor ions with similar mass and retention time are co-fragmented and sequenced by the MS/MS instrument. Based on this, we try to analyze the factor which leads to the high error rate of identifications. We show that chimeric spectra have high correlations with the ranking scores and can reduce the number of positive identifications.</p><p>Additionally, we address the problem of assigning a posterior error probability (PEP) to the individual peptide-spectrum matches (PSMs) that are obtained via search engines. This problem is computationally more difficult than estimating the error rate associated with a large collection of PSMs, such as false discovery rate (FDR). Existing methods rely on parametric or semiparametric models of the underlying score distribution as a preassumption. We provide a so-called kernel logistic regression procedure without any explicit assumptions about the score distribution. Based on an appropriate positive definite Gaussian kernel, the resulting PEP estimate is proven to be robust by achieving a close correspondence between the PEP-derived q-values and FDR-derived q-values. Furthermore, we also accept at least 200 more significant PSMs with setting a threshold based on PEP-derived q-values compared to FDR-derived q-values. Finally, we show that this kernel logistic regression method is well established in the statistics literature and it can produce accurate PEP estimates for different types of PSM score functions and data.</p>
----------------------------------------------------------------------
In diva2:618218 
abstract is: 
<p>By using industrialized building, pre-installed modules can be delivered to the construction site where they are easily mounted. This method has many advantages compared to on-site manufacturing, for example, the precision during the production process should be much higher than building each wall and floor on site. Furthermore, the quick erection procedure when using prefabricated elements and the possibility to use weather protection has the advantage of not being dependent on weather conditions, and many other factors.Masonite Beams AB is a Swedish company (a member of the Norwegian company, Byggma ASA) with a relatively new technology using light weight beams as bearing elements in the walls and a certain stiff board lamella slap which is mounted on the top of the floor structure creating an integrated stiff beam / board floor structure. The elements are flat which make it much easier to transport the elements from the factory to the building site where the building parts are easily mounted with special mounting stud.One of the main drawbacks of lightweight structure is the sound transmission. The main topic is low frequency annoyance, however the complex structural elements and their mutual sensitive connections other acoustical problems might appear. This master thesis describes how differentStegljud i balkelementsystem - flanktransmissionMaikel Rofailstages of buildings process of prefabricated element when mounted, influence the sound transmission and the sound quality in the buildings. A special two stories building were studied to achieve more understanding of the difficulties during the erection of different elements in the completion of the buildings. A process mapping with focus on the technical defects are made but also a risk analysis to evaluate when special care has to be taken during the buildings process.</p>

corrected abstract:
<p>By using industrialized building, pre-installed modules can be delivered to the construction site where they are easily mounted. This method has many advantages compared to on-site manufacturing, for example, the precision during the production process should be much higher than building each wall and floor on site. Furthermore, the quick erection procedure when using prefabricated elements and the possibility to use weather protection has the advantage of not being dependent on weather conditions, and many other factors.</p><p>Masonite Beams AB is a Swedish company (a member of the Norwegian company, Byggma ASA) with a relatively new technology using light weight beams as bearing elements in the walls and a certain stiff board lamella slap which is mounted on the top of the floor structure creating an integrated stiff beam / board floor structure. The elements are flat which make it much easier to transport the elements from the factory to the building site where the building parts are easily mounted with special mounting stud.</p><p>One of the main drawbacks of lightweight structure is the sound transmission. The main topic is low frequency annoyance, however the complex structural elements and their mutual sensitive connections other acoustical problems might appear. This master thesis describes how different stages of buildings process of prefabricated element when mounted, influence the sound transmission and the sound quality in the buildings. A special two stories building were studied to achieve more understanding of the difficulties during the erection of different elements in the completion of the buildings. A process mapping with focus on the technical defects are made but also a risk analysis to evaluate when special care has to be taken during the buildings process.</p>
----------------------------------------------------------------------
In diva2:578791 
abstract is: 
<p>The Basel II accord requires banks to put aside a capital buffer against unexpected operational losses, resulting from inadequate or failed internal processes, people and systems or from external events. Under the sophisticated Advanced Measurement Approach banks are given the opportunity to develop their own model to estimate operational risk.This report focus on a loss distribution approach based on a set of real data.</p><p>First a comprehensive data analysis was made which suggested that the observations belonged to a heavy tailed distribution. An evaluation of commonly used distributions was performed. The evaluation resulted in the choice of a compound Poisson distribution to model frequency and a piecewise defined distribution with an empirical body and a generalized Pareto tail to model severity. The frequency distribution and the severity distribution define the loss distribution from which Monte Carlo simulations were made in order to estimate the 99.9% quantile, also known as the the regulatory capital.</p><p>Conclusions made on the journey were that including all operational risks in a model is hard, but possible, and that extreme observations have a huge impact on the outcome.</p>

corrected abstract:
<p>The Basel II accord requires banks to put aside a capital buffer against unexpected operational losses, resulting from inadequate or failed internal processes, people and systems or from external events. Under the sophisticated Advanced Measurement Approach banks are given the opportunity to develop their own model to estimate operational risk. This report focus on a loss distribution approach based on a set of real data.</p><p>First a comprehensive data analysis was made which suggested that the observations belonged to a heavy tailed distribution. An evaluation of commonly used distributions was performed. The evaluation resulted in the choice of a compound Poisson distribution to model frequency and a piecewise defined distribution with an empirical body and a generalized Pareto tail to model severity. The frequency distribution and the severity distribution define the loss distribution from which Monte Carlo simulations were made in order to estimate the 99.9% quantile, also known as the the regulatory capital.</p><p>Conclusions made on the journey were that including all operational risks in a model is hard, but possible, and that extreme observations have a huge impact on the outcome.</p>

It seems the only probelm was that space missing after "operational risk."
----------------------------------------------------------------------
In diva2:515504 
abstract is: 
<p>Swedish school has for the last decades been using quality management as a method for improvement. Quality management is an enormous work and the whole school staff needs to be involved. The management consists of several parts like planning, surveys and interviews, analysis of the replies and setting up new goals. Parents, children and teachers are nowadays accustomed with quality surveys. The last few years these quality surveys have been computerized, but they still look almost the same, using little interaction with the respondent.I have had the opportunity to work with the company KMF Ventures, which has a vision that entails a more interactive web survey. This web survey gives both the school board and the pupils a possibility to compare their school with others. Hopefully this make them more interested in responding. The web survey is always open and the pupils can respond whenever they change opinion.This report describes the development process of the quality control tool for the web survey. The web tool has furthermore been evaluated in two Swedish gymnasium schools. Interviews have been held with teachers, school board and pupils. The purpose was to see how their quality management system works and how it has been changed when they starts to use this quality control tool. I also wanted to see how the schools handle problems with for example teachers.The questions in the web survey are few but they look practically the same as the ones already being used by the schools. They questions differ however from other surveys in the way it use only “yes” and “no” as responses.The quality web tool indicated that there were some problems in the schools. The lack of experience made however the schools to take little notice, even less to take action. This shows that education and planning of a new quality control tool is of great importance. The comparison option was nevertheless highly valued. This quality control tool could have a future, but it requires significant improvements. The company needs for example to consider the possibilities of a more well-known method for surveys. Most important is though that the company helps the school with the planning and the introduction of the new quality survey.</p>

corrected abstract:
<p>Swedish school has for the last decades been using quality management as a method for improvement. Quality management is an enormous work and the whole school staff needs to be involved. The management consists of several parts like planning, surveys and interviews, analysis of the replies and setting up new goals. Parents, children and teachers are nowadays accustomed with quality surveys. The last few years these quality surveys have been computerized, but they still look almost the same, using little interaction with the respondent.</p><p>I have had the opportunity to work with the company KMF Ventures, which has a vision that entails a more interactive web survey. This web survey gives both the school board and the pupils a possibility to compare their school with others. Hopefully this make them more interested in responding. The web survey is always open and the pupils can respond whenever they change opinion.</p><p>This report describes the development process of the quality control tool for the web survey. The web tool has furthermore been evaluated in two Swedish gymnasium schools. Interviews have been held with teachers, school board and pupils. The purpose was to see how their quality management system works and how it has been changed when they starts to use this quality control tool. I also wanted to see how the schools handle problems with for example teachers.</p><p>The questions in the web survey are few but they look practically the same as the ones already being used by the schools. They questions differ however from other surveys in the way it use only “yes” and “no” as responses.</p><p>The quality web tool indicated that there were some problems in the schools. The lack of experience made however the schools to take little notice, even less to take action. This shows that education and planning of a new quality control tool is of great importance. The comparison option was nevertheless highly valued. This quality control tool could have a future, but it requires significant improvements. The company needs for example to consider the possibilities of a more well-known method for surveys. Most important is though that the company helps the school with the planning and the introduction of the new quality survey.</p>
----------------------------------------------------------------------
In diva2:411675 
abstract is: 
<p>VIP aircrafts cabin interiors are refurbished according to specific needs of very demandingcustomers. In the mean time, international airworthiness authorities set high standard andinevitable certification procedures which do not provide the necessary freedom to fulfill suchrequests. This study points out four innovative ideas likely to lead to the drafting of approvednew methods of compliance.</p>


corrected abstract:
<p>VIP aircrafts cabin interiors are refurbished according to specific needs of very demanding customers. In the mean time, international airworthiness authorities set high standard and inevitable certification procedures which do not provide the necessary freedom to fulfill such requests. This study points out four innovative ideas likely to lead to the drafting of approved new methods of compliance.</p>
----------------------------------------------------------------------
In diva2:408555 
abstract is: 
<p>As a result of the accelerating demands for faster development within the heavy vehicle industry, computer aided simulations have become a more important tool in the development process. Simulations can offer faster evaluation of loads acting on the vehicle and more cost effective fatigue life predictions than physical testing, since physical prototypes are not needed for load measurements or fatigue tests. However, accurate fatigue life predictions without physical verification are today a difficult task with many uncertainties, yet simulations are still an important part of modern product development.The objective of this work is to investigate the accuracy of a virtual model of a physical truck. The thesis focuses only on load simulation accuracy, leaving the material uncertainties aside. The vehicle model is built using Adams/Car with two different complexities of the frame model. A part of the work is to investigate how the frame model complexity affects the accuracy of the results.The virtual truck is simulated in a virtual test rig that excites the model with displacement on the wheel hubs to represent the forces induced when the truck is driven on the test track. The process to make a drive signal to the test rig is iterative. Simulations are also performed with the virtual model equipped with tires and driven on a virtual 3D road.Model performance is evaluated using TDDI (Time Domain Discrepancy Index) and pseudo-damage. TDDI evaluates the results in the time domain and the pseudo-damage considers the potential fatigue damage in the time series. A value of the TDDI below 0.3 and between 0.5 and 2 for the pseudo-damage is found good. The accuracy is approximately the same as can be repeated by different test engineers driving the same test schedule with the same vehicle.When iterating using the cab and the front and rear end of the frame as response feedback, the results for the model with the simple frame model show good values of TDDI and pseudo damage for the front end of the frame and the cab. Though the axles and the mid of the frame show poor results. The rear end of the frame does not reach the model performance targets, getting a too low value of the pseudo-damage while the TDDI value is good. The vehicle model with the complex frame shows similar results, when using the same response feedback, although the frame model is not optimized.The full vehicle model driving on 3d-road does not, at present, deliver accurate results. However, the relative damping for the beams, representing the leaf springs, has turned out to highly affect the results. The leaf spring model thus need to be optimized. The complex frame model is not showing results good enough to justify the extra modelling time. The accuracy of the full-vehicle model can be considerably improved by optimizing the model/-s of the wheel suspension and the complex frame model.</p>

corrected abstract:
<p>As a result of the accelerating demands for faster development within the heavy vehicle industry, computer aided simulations have become a more important tool in the development process. Simulations can offer faster evaluation of loads acting on the vehicle and more cost effective fatigue life predictions than physical testing, since physical prototypes are not needed for load measurements or fatigue tests. However, accurate fatigue life predictions without physical verification are today a difficult task with many uncertainties, yet simulations are still an important part of modern product development.</p><p>The objective of this work is to investigate the accuracy of a virtual model of a physical truck. The thesis focuses only on load simulation accuracy, leaving the material uncertainties aside. The vehicle model is built using Adams/Car with two different complexities of the frame model. A part of the work is to investigate how the frame model complexity affects the accuracy of the results.</p><p>The virtual truck is simulated in a virtual test rig that excites the model with displacement on the wheel hubs to represent the forces induced when the truck is driven on the test track. The process to make a drive signal to the test rig is iterative. Simulations are also performed with the virtual model equipped with tires and driven on a virtual 3D road.</p><p>Model performance is evaluated using TDDI (Time Domain Discrepancy Index) and pseudo-damage. TDDI evaluates the results in the time domain and the pseudo-damage considers the potential fatigue damage in the time series. A value of the TDDI below 0.3 and between 0.5 and 2 for the pseudo-damage is found good. The accuracy is approximately the same as can be repeated by different test engineers driving the same test schedule with the same vehicle.</p><p>When iterating using the cab and the front and rear end of the frame as response feedback, the results for the model with the simple frame model show good values of TDDI and pseudo damage for the front end of the frame and the cab. Though the axles and the mid of the frame show poor results. The rear end of the frame does not reach the model performance targets, getting a too low value of the pseudo-damage while the TDDI value is good. The vehicle model with the complex frame shows similar results, when using the same response feedback, although the frame model is not optimized.</p><p>The full vehicle model driving on 3d-road does not, at present, deliver accurate results. However, the relative damping for the beams, representing the leaf springs, has turned out to highly affect the results. The leaf spring model thus need to be optimized. The complex frame model is not showing results good enough to justify the extra modelling time. The accuracy of the full-vehicle model can be considerably improved by optimizing the model/-s of the wheel suspension and the complex frame model.</p>
----------------------------------------------------------------------
In diva2:405993 
abstract is: 
<p>on mechanical brakes. The electric regenerative brakes can thus be used as normal service brake with minimum time loss.The first part of the study aims at developing a method to calculate wear on train brake pads. This is done by using a reformulated version of Archard’s wear equation with a temperature dependent wear coefficient and a temperature model to predict the brake pad temperature during braking. The temperature model is calibrated using trustworthy data from a brake system supplier and full-scale test results.By performing simulations in the program STEC (Simulation of Train Energy Consumption), energy consumption for different cases of high-speed train operations is procured and significant data for the wear calculations are found. Simulations include both “normal driving techniques” and “eco driving”. The driving styles were decided through interviews with train drivers and experts on energy optimized driving systems.The simulations show that more powerful drive systems reduce both energy consumption and travel time by permitting higher acceleration and energy regeneration while braking. Calculations show that since the electric motors could carry out more of the braking the wear of the mechanical brakes becomes lower.Eco driving techniques can help to further reduce the energy consumption and mechanical brake wear. This driving style can require some time margins though, since it takes slightly longer time to drive when using coasting and avoiding speed peaks. However, if used properly this should not have to affect the actual travel time, partly because some time margins are always included in the timetable.Even if new, more powerful, trains would have the ability to reduce energy consumption and brake wear it is also necessary to have an appropriate slip control system for the electric brakes, making it possible to use them also under slippery conditions. In this context it is important that the adhesion utilization is modest, about 12 – 15 % for speeds up to 100 km/h and lower at higher speeds.</p>

corrected abstract:
<p>This study is a part of “<span lang="sv">Gröna Tåget</span>” (Eng: “Green Train”) research and development programme that is preparing for new high-speed trains in Sweden. The purpose of this study is to investigate the effects of regenerative braking and eco driving with regard to energy consumption and wear of the mechanical brakes.</p><p>New sophisticated “eco driving” systems could help train drivers to run as energy efficient and economically as possible. Combined with more powerful drive systems this could lead to more regenerated energy and reduced wear on mechanical brakes. The electric regenerative brakes can thus be used as normal service brake with minimum time loss.</p><p>The first part of the study aims at developing a method to calculate wear on train brake pads. This is done by using a reformulated version of Archard’s wear equation with a temperature dependent wear coefficient and a temperature model to predict the brake pad temperature during braking. The temperature model is calibrated using trustworthy data from a brake system supplier and full-scale test results.</p><p>By performing simulations in the program STEC (Simulation of Train Energy Consumption), energy consumption for different cases of high-speed train operations is procured and significant data for the wear calculations are found. Simulations include both “normal driving techniques” and “eco driving”. The driving styles were decided through interviews with train drivers and experts on energy optimized driving systems.</p><p>The simulations show that more powerful drive systems reduce both energy consumption and travel time by permitting higher acceleration and energy regeneration while braking. Calculations show that since the electric motors could carry out more of the braking the wear of the mechanical brakes becomes lower.</p><p>Eco driving techniques can help to further reduce the energy consumption and mechanical brake wear. This driving style can require some time margins though, since it takes slightly longer time to drive when using coasting and avoiding speed peaks. However, if used properly this should not have to affect the actual travel time, partly because some time margins are always included in the timetable.</p><p>Even if new, more powerful, trains would have the ability to reduce energy consumption and brake wear it is also necessary to have an appropriate slip control system for the electric brakes, making it possible to use them also under slippery conditions. In this context it is important that the adhesion utilization is modest, about 12 – 15 % for speeds up to 100 km/h and lower at higher speeds.</p>
----------------------------------------------------------------------
In diva2:401391 - missing space in title:
"Sound transmission through double walls usingStatistical Energy Analysis"
==>
"Sound transmission through double walls using Statistical Energy Analysis"

abstract is: 
<p>A new Statistical Energy Analysis (SEA) formulation for double walls isderived. The new formulation uses three elements to describe the double wall,one for each plate and the third for the cavity including the mass-controlledmodes of the plates. This means that the influence of the double wall resonance(mass-air-mass resonance) can be predicted by SEA. Calculations aremade and compared to measurements, showing fair agreement.</p>

corrected abstract:
<p>A new Statistical Energy Analysis (SEA) formulation for double walls is derived. The new formulation uses three elements to describe the double wall, one for each plate and the third for the cavity including the mass-controlled modes of the plates. This means that the influence of the double wall resonance (mass-air-mass resonance) can be predicted by SEA. Calculations are made and compared to measurements, showing fair agreement.</p>
----------------------------------------------------------------------
In diva2:1896429 
abstract is: 
<p>This thesis focuses on a component of the NA64 experiment at CERN, which tests models trying to give an explanation to dark matter in hope to find an answer in agreement with experimental evidence. The goal of NA64 is to use a beam of highly energetic electrons to produce a dark photon, according to the processes enabled by a vector portal of the dark sector, and then detect the products arising from the decay of the dark photon. The component of the experiment studied here is the Synchrotron Radiation Detector, or SRD, which has the purpose of detecting photons emitted by the beam particles used in the experiment. The reason for this is that, starting from the energy distribution of the detected photons, it is possible to identify the generating beam particle. This allows for the selection of events, only accepting those generated by electrons. This is fundamental in order to min- imize background events that might mimick the sought dark matter experimental signature.Specifically, a new LYSO-based prototype of the SRD is tested, and the data recorded with it is compared to simulated data. A new prototype of the SRD is needed in order to improve performance, namely increasing electron efficiency and hadron rejection, improving energy resolution and reducing pileup. Its features are also studied, such as its efficiency or the effect of the intrinsic radiation emitted by its crystals on the final measurements.The results suggest that the new SRD possesses several features and characteris- tics that make it suitable for usage in NA64. However, the detector studied in this thesis is still at a prototypical stage, so many more improvements can still be done in order to obtain better performance, for example a different choice of parameters for the data taking or an improved calibration. The final plots show some of the expected characteristics, such as a peak in the electron run that can be identified as due to synchrotron radiation, but its energy does not exactly correspond to that of the simulated spectrum. The total electron efficiency was determined to be of 0.743 ± 0.001, while for hadrons 0.0009 ± 0.0002. This is different than what calculated from the simulated data, but still in line with theoretical considerations and expectations.</p>

corrected abstract:
<p>This thesis focuses on a component of the NA64 experiment at CERN, which tests models trying to give an explanation to dark matter in hope to find an answer in agreement with experimental evidence. The goal of NA64 is to use a beam of highly energetic electrons to produce a dark photon, according to the processes enabled by a vector portal of the dark sector, and then detect the products arising from the decay of the dark photon. The component of the experiment studied here is the Synchrotron Radiation Detector, or SRD, which has the purpose of detecting photons emitted by the beam particles used in the experiment. The reason for this is that, starting from the energy distribution of the detected photons, it is possible to identify the generating beam particle. This allows for the selection of events, only accepting those generated by electrons. This is fundamental in order to minimize background events that might mimick the sought dark matter experimental signature.</p><p>Specifically, a new LYSO-based prototype of the SRD is tested, and the data recorded with it is compared to simulated data. A new prototype of the SRD is needed in order to improve performance, namely increasing electron efficiency and hadron rejection, improving energy resolution and reducing pileup. Its features are also studied, such as its efficiency or the effect of the intrinsic radiation emitted by its crystals on the final measurements.</p><p>The results suggest that the new SRD possesses several features and characteristics that make it suitable for usage in NA64. However, the detector studied in this thesis is still at a prototypical stage, so many more improvements can still be done in order to obtain better performance, for example a different choice of parameters for the data taking or an improved calibration. The final plots show some of the expected characteristics, such as a peak in the electron run that can be identified as due to synchrotron radiation, but its energy does not exactly correspond to that of the simulated spectrum. The total electron efficiency was determined to be of 0.743 ± 0.001, while for hadrons 0.0009 ± 0.0002. This is different than what calculated from the simulated data, but still in line with theoretical considerations and expectations.</p>
----------------------------------------------------------------------
In diva2:1894669 
abstract is: 
<p>This thesis explores the critical external variables affecting the development of a forecasting model for the total volume of car advertisements on Blocket, a marketplace platform owned by the Nordic media and marketplace company Schibsted. The model was crafted using multiple linear regression, aligning with industry practices for selecting external variables, and was developed in close collaboration with industry experts from Schibsted. The dataset spans from May 2017 to December 2023, showcasing pronounced monthly seasonal trends, which were managed using dummy variables.An initial model was created, yielding an impressive R^2 of 0,94. This model included the average duration that car dealerships in Sweden hold their inventory. However, this variable proved unsuitable due to a scarcity of data points and challenges in forecasting it, leading to its exclusion from the final results. The definitive model features variables such as sales of privately used and new cars, the Volkswagen base rate, and disposable income, achieving an adjusted R^2 of 0,90 with all variables demonstrating significance. This model highlights the significant role of financing options for private car buyers, particularly shown by the substantial coefficient of the Volkswagen base rate. Determining whether this report shows causality would require further, more detailed research.However, the report acknowledges certain limitations, including a lack of data variability, which constrains the model's robustness and applicability across different economic conditions like recessions and booms. Additionally, the model, with its 83 data points, could benefit from an expanded dataset. Despite these shortcomings, the report offers valuable insights into the factors affecting consumer behaviour in the mobility market and outlines potential directions for future research.</p>

corrected abstract:
<p>This thesis explores the critical external variables affecting the development of a forecasting model for the total volume of car advertisements on Blocket, a marketplace platform owned by the Nordic media and marketplace company Schibsted. The model was crafted using multiple linear regression, aligning with industry practices for selecting external variables, and was developed in close collaboration with industry experts from Schibsted. The dataset spans from May 2017 to December 2023, showcasing pronounced monthly seasonal trends, which were managed using dummy variables.</p><p>An initial model was created, yielding an impressive R^2 of 0,94. This model included the average duration that car dealerships in Sweden hold their inventory. However, this variable proved unsuitable due to a scarcity of data points and challenges in forecasting it, leading to its exclusion from the final results. The definitive model features variables such as sales of privately used and new cars, the Volkswagen base rate, and disposable income, achieving an adjusted R<sup>2</sup> of 0,90 with all variables demonstrating significance. This model highlights the significant role of financing options for private car buyers, particularly shown by the substantial coefficient of the Volkswagen base rate. Determining whether this report shows causality would require further, more detailed research.</p><p>However, the report acknowledges certain limitations, including a lack of data variability, which constrains the model's robustness and applicability across different economic conditions like recessions and booms. Additionally, the model, with its 83 data points, could benefit from an expanded dataset. Despite these shortcomings, the report offers valuable insights into the factors affecting consumer behaviour in the mobility market and outlines potential directions for future research.</p>

Note that the original has the "R^2" string for the first R².
----------------------------------------------------------------------
In diva2:1891378 
abstract is: 
<p>The atmosphere and its dynamics are hard to model. Predicting how it evolves, at all altitudes, plays an important part in our everyday life for aspects such as meteorology, but it also becomes more and more crucial concerning optical communications with the Earth’s near space, where a lot of artificial satellites have been launched. This region, called the LEO (Low-Earth Orbit), lies right after the end of the thermosphere, at altitudes from 400 to 2000 kilometres.As written above, the atmosphere can strongly degrade the performance of the exchanges notably because of the optical phase perturbation. A solution to this problem is to use adaptive optics (AO), which corrects in real-time the effects of atmospheric turbulence. A lot of different AO systems have been designed since the 1980’s, but new turbulence prediction techniques as well as the steadily improving computers bring new opportunities in AO correction performance.At ONERA, the goal is to validate and demonstrate ground-to-LEO and groundto-GEO links, with a ground station called FEELINGS. This master thesis falls within this objective, as it consists in the characterization of cameras used in FEELINGS, and the study of a control law that will ultimately be implemented and validated on FEELINGS.In this master thesis, tests will be performed on a simulation that emulates an adaptive optics bench at ONERA. I first used a turbulence simulation model that enabled me to assess the phase perturbation predictability. I demonstrated that a substantial performance gain was theoretically achievable. This result was the motivation point to lead the rest of the study, by creating and characterizing the control law. The latter is a Linear Quadratic Gaussian control, whose performance was quantified, so that the simulated theoretical performance is now known.In conclusion, the first steps of the testing on the real adaptive optics bench at ONERA were carried out. It is composed of two different parts: PICOLO, an atmospheric turbulence simulator, and LISA, an adaptive optics bench. The combination of the two can efficiently model different situations and a specific section will be dedicated to detailing their parameters.</p>

corrected abstract:
<p>The atmosphere and its dynamics are hard to model. Predicting how it evolves, at all altitudes, plays an important part in our everyday life for aspects such as meteorology, but it also becomes more and more crucial concerning optical communications with the Earth’s near space, where a lot of artificial satellites have been launched. This region, called the LEO (Low-Earth Orbit), lies right after the end of the thermosphere, at altitudes from 400 to 2000 kilometres.</p><p>As written above, the atmosphere can strongly degrade the performance of the exchanges notably because of the optical phase perturbation. A solution to this problem is to use adaptive optics (AO), which corrects in real-time the effects of atmospheric turbulence. A lot of different AO systems have been designed since the 1980’s, but new turbulence prediction techniques as well as the steadily improving computers bring new opportunities in AO correction performance.</p><p>At ONERA, the goal is to validate and demonstrate ground-to-LEO and ground-to-GEO links, with a ground station called FEELINGS. This master thesis falls within this objective, as it consists in the characterization of cameras used in FEELINGS, and the study of a control law that will ultimately be implemented and validated on FEELINGS.</p><p>In this master thesis, tests will be performed on a simulation that emulates an adaptive optics bench at ONERA. I first used a turbulence simulation model that enabled me to assess the phase perturbation predictability. I demonstrated that a substantial performance gain was theoretically achievable. This result was the motivation point to lead the rest of the study, by creating and characterizing the control law. The latter is a Linear Quadratic Gaussian control, whose performance was quantified, so that the simulated theoretical performance is now known.</p><p>In conclusion, the first steps of the testing on the real adaptive optics bench at ONERA were carried out. It is composed of two different parts: PICOLO, an atmospheric turbulence simulator, and LISA, an adaptive optics bench. The combination of the two can efficiently model different situations and a specific section will be dedicated to detailing their parameters.</p>
----------------------------------------------------------------------
In diva2:1880788 
abstract is: 
<p>This Bachelors Thesis, conducted together with Öhlins racing AB, aims to develop a deeper understanding of the vehicle dynamics of snowmobiles, focusing particularly on the behavior of the rear suspension under various motions and applied forces. The rear suspension called the bogie, consists of several linkages, springs, and dampers whose geometry and parameters influence the movement of the bogie. The study aims to attain industry-standard knowledge of snowmobile dynamics by developing a simulation model in Matlab to further understand and examine the behavior of the bogie during heave and pitch, as well as consulting with professionals in snowmobile simulation to understand industry practices.The simulation model is built on two main components, the kinematic- and the dynamic calculations. The kinematics is determined by measuring existing snowmobiles to get data on how the points in the bogie are distanced. Subsequently, the motion of the bogie during heave is calculated by iteratively lifting the rail a small distance upwards from the initial points determined by the measurements. The motion will be dictated by the center arm since the rail is not allowed to rotate and the center arm can't be compressed. The dynamics of the bogie are then modeled by integrating springs to examine how the application of force varies throughout the motion. This, together with the forces exerted on the rail by the arms will result in a net heave force which is the force that is needed to initiate the heave motion.The heave simulation reveals that the application point of the heave force shifts forward during the compression of the bogie, a behavior that positively impacts the vehicle's turning ability by effectively shortening the wheelbase. Additionally, the motion ratios for the center and rear springs were analyzed, showing distinct variations. The motion ratio analysis for the center spring revealed only slight variations in the front, whereas the rear motion ratio exhibited substantial changes due to the rear arm and spring moving in opposite directions.Limitations of the study include the absence of empirical validation as well as simplifications of the suspension linkage, specifically the exclusion of coupled mechanisms. Future work should involve simultaneous pitch and heave movements and incorporate feedback from professional snowmobile drivers to refine the suspension settings. The insights gained from these simulations can guide the design of more efficient and responsive snowmobile bogies, ultimately enhancing the vehicle's performance and safety.</p>

corrected abstract:
<p>This Bachelors Thesis, conducted together with Öhlins racing AB, aims to develop a deeper understanding of the vehicle dynamics of snowmobiles, focusing particularly on the behavior of the rear suspension under various motions and applied forces. The rear suspension called the bogie, consists of several linkages, springs, and dampers whose geometry and parameters influence the movement of the bogie. The study aims to attain industry-standard knowledge of snowmobile dynamics by developing a simulation model in Matlab to further understand and examine the behavior of the bogie during heave and pitch, as well as consulting with professionals in snowmobile simulation to understand industry practices.</p><p>The simulation model is built on two main components, the kinematic- and the dynamic calculations. The kinematics is determined by measuring existing snowmobiles to get data on how the points in the bogie are distanced. Subsequently, the motion of the bogie during heave is calculated by iteratively lifting the rail a small distance upwards from the initial points determined by the measurements. The motion will be dictated by the center arm since the rail is not allowed to rotate and the center arm can't be compressed. The dynamics of the bogie are then modeled by integrating springs to examine how the application of force varies throughout the motion. This, together with the forces exerted on the rail by the arms will result in a net heave force which is the force that is needed to initiate the heave motion.</p><p>The heave simulation reveals that the application point of the heave force shifts forward during the compression of the bogie, a behavior that positively impacts the vehicle's turning ability by effectively shortening the wheelbase. Additionally, the motion ratios for the center and rear springs were analyzed, showing distinct variations. The motion ratio analysis for the center spring revealed only slight variations in the front, whereas the rear motion ratio exhibited substantial changes due to the rear arm and spring moving in opposite directions.</p><p>Limitations of the study include the absence of empirical validation as well as simplifications of the suspension linkage, specifically the exclusion of coupled mechanisms. Future work should involve simultaneous pitch and heave movements and incorporate feedback from professional snowmobile drivers to refine the suspension settings. The insights gained from these simulations can guide the design of more efficient and responsive snowmobile bogies, ultimately enhancing the vehicle's performance and safety.</p>
----------------------------------------------------------------------
In diva2:1876827 
abstract is: 
<p>In this work, we propose a regressional framework, built on the work ”Deep Order Flow Imbalance: Extracting Alpha at Multiple Horizons from the Limit Order Book” by Kolm, et al. (2023), for predicting short term returns of odds on binary betting exchange markets. Using the framework, we apply five different deep learning models that leverage order book data from tennis betting exchanges during the calendar month of July 2023 with the purpose of examining the predictive capabilities of deep learning models in this setting. We train each model on either raw limit order book states or order flow. The models predict the returns of the best available odds returns on five different short term time horizons on the four order book sides, back and lay for each of the two players in a given tennis match. Applying windowing, for each vector prediction we use the 100 latest market messages consisting of 81 features (odds and volumes per the ten first levels in the order book and time delta between market messages) in the case of the raw limit order book state and 41 features (order book flow per the ten first levels in the order book and time delta between market messages) in the case of the order book flow. All code is written in Python and run on Google Colab, leveraging cloud computing, off-the-shelf models and popular libraries, TensorFlow and Keras, for data processing and pipelining, model implementation, training and testing. The models are evaluated relative to a benchmark in the form of a naive predictor based on the average odds returns on the training set. The models do not converge towards an optimal parameter composition duringtraining, indicating low predictive capabilities of the input data. Despite this, we generally find all models to outperform the benchmark on the lay order book sides and while some perform better than others, we see similar relative performance distributions within each model across horizon-order book side combinations. To enhance discussion and suggest the direction of future research we examine relationships between key game characteristics such asthe variation of odds returns and the accuracy of predictions on a given market.</p>

corrected abstract:
<p>In this work, we propose a regressional framework, built on the work ”Deep Order Flow Imbalance: Extracting Alpha at Multiple Horizons from the Limit Order Book” by Kolm, et al. (2023), for predicting short term returns of odds on binary betting exchange markets. Using the framework, we apply five different deep learning models that leverage order book data from tennis betting exchanges during the calendar month of July 2023 with the purpose of examining the predictive capabilities of deep learning models in this setting. We train each model on either raw limit order book states or order flow. The models predict the returns of the best available odds returns on five different short term time horizons on the four order book sides, back and lay for each of the two players in a given tennis match. Applying windowing, for each vector prediction we use the 100 latest market messages consisting of 81 features (odds and volumes per the ten first levels in the order book and time delta between market messages) in the case of the raw limit order book state and 41 features (order book flow per the ten first levels in the order book and time delta between market messages) in the case of the order book flow. All code is written in Python and run on Google Colab, leveraging cloud computing, off-the-shelf models and popular libraries, TensorFlow and Keras, for data processing and pipelining, model implementation, training and testing. The models are evaluated relative to a benchmark in the form of a naive predictor based on the average odds returns on the training set. The models do not converge towards an optimal parameter composition during training, indicating low predictive capabilities of the input data. Despite this, we generally find all models to outperform the benchmark on the lay order book sides and while some perform better than others, we see similar relative performance distributions within each model across horizon-order book side combinations. To enhance discussion and suggest the direction of future research we examine relationships between key game characteristics such as the variation of odds returns and the accuracy of predictions on a given market.</p>
----------------------------------------------------------------------
In diva2:1871555 
abstract is: 
<p>Quick and reliable computational methods for optimized orbital transfers are crucial for projects at preliminary stages. They enable an initial, realistic sizing of the propulsion subsystem, one of the major components of satellite design. This thesis work, conducted at ReOrbit Oy, presents a minimum-time optimal trajectory for the orbit raising of a micro-satellite from GTO to GEO, assuming continuous firing by electric propulsion. The Delta-v requirements resulting from this simulation lead to the selection of an appropriate electric propulsion system, elaborating on the design of its configuration in terms of fuel and thrust requirements. This is done by taking into account, other than the major contribution given by the orbit raising, additions due to in-orbit maneuvers performed twice a day over a 10-year lifetime, like station-keeping corrections and reaction wheels desaturation. The optimization method is a direct-indirect hybrid for low-thrust orbital maneuvers, employing Pontryagin’s Minimum Principle for the transcription into a nonlinear programming problem. The initial guess required to start the optimizer is obtained with Lyapunov control theory. An orbital averaging technique is implemented, enabling fast computation of multiple trajectories during the optimization. Disturbances from the J2 zonal harmonic, solar radiation pressure, third-body effects of the Sun and Moon, and atmospheric drag up to 1500 km of altitude are included in the dynamic model. Eclipse conditions are assessed with a cylindrical shadow model, as the solar electric propulsion experiences a zero thrust period when in Earth’s shadow. The electric propulsion system configuration is determined with trade-off studies and comparisons between different suppliers. The chosen outline includes 4 Xenon thrusters, with complementary power processing units and propellant management systems, resulting in a total transfer time of less than 4 months. The same propulsion system is employed both for the transfer trajectory and the in-orbit maneuvers, by changing the thruster’s configuration once in GEO.</p>

corrected abstract:
<p>Quick and reliable computational methods for optimized orbital transfers are crucial for projects at preliminary stages. They enable an initial, realistic sizing of the propulsion subsystem, one of the major components of satellite design. This thesis work, conducted at ReOrbit Oy, presents a minimum-time optimal trajectory for the orbit raising of a micro-satellite from GTO to GEO, assuming continuous firing by electric propulsion. The &delta;𝑣 requirements resulting from this simulation lead to the selection of an appropriate electric propulsion system, elaborating on the design of its configuration in terms of fuel and thrust requirements. This is done by taking into account, other than the major contribution given by the orbit raising, additions due to inorbit maneuvers performed twice a day over a 10-year lifetime, like station-keeping corrections and reaction wheels desaturation. The optimization method is a direct-indirect hybrid for low-thrust orbital maneuvers, employing Pontryagin’s Minimum Principle for the transcription into a nonlinear programming problem. The initial guess required to start the optimizer is obtained with Lyapunov control theory. An orbital averaging technique is implemented, enabling fast computation of multiple trajectories during the optimization. Disturbances from the J<sub>2</sub> zonal harmonic, solar radiation pressure, third-body effects of the Sun and Moon, and atmospheric drag up to 1500 km of altitude are included in the dynamic model. Eclipse conditions are assessed with a cylindrical shadow model, as the solar electric propulsion experiences a zero thrust period when in Earth’s shadow. The electric propulsion system configuration is determined with tradeoff studies and comparisons between different suppliers. The chosen outline includes 4 Xenon thrusters, with complementary power processing units and propellant management systems, resulting in a total transfer time of less than 4 months. The same propulsion system is employed both for the transfer trajectory and the in-orbit maneuvers, by changing the thruster’s configuration once in GEO.</p>
----------------------------------------------------------------------
In diva2:1849034 
abstract is: 
<p>This study aims to investigate if taking individuals' chosen time of withdrawing occupational pension into account can be used to better predict the future mortality intensities which describes the probability of death in a given time interval. The main underlying hypothesis for why there would be a difference lies in the fact that some individuals could receive more pension benefits by choosing a withdrawal time based on his or her own expectation of remaining lifespan. In particular individuals that do not expect to live long after retirement could receive more benefits before death by choosing to withdraw his or her money as fast as possible.</p><p>To explore if such a relationship exists and is significant enough data provided by Alecta is analyzed. This data provides among others the chosen withdrawal time and recorded deaths. Based on this dataset three models are created each with their own advantages and disadvantages. </p><p>The first model is a pure empirical model which simply shows the processed data after calculations have been made, showing the exact data in a more comprehensive form. The second model is a Makeham model which assumes that the mortality intensity is distributed according to the Makeham formula in the studied age range. This model allows random noise to be smoothed out and allows predictions of the mortality intensity in ages not studied. The third model is a Logistic regression model. This model considers two cases, death or no death and based on observed data the model predicts the probability that an individual will die given an age and withdrawal time. The logistic regression model is a robust model which allows stable predictions even where data is scarce.</p><p>From the results of the three models it was concluded that for men there is a clear difference in mortality intensity between limited withdrawal times where shorter withdrawal times have higher mortality intensity and longer withdrawal times have lower mortality intensity. For women there were signs that the same is also true, however the evidence for this was weak. </p><p>The difference in mortality intensity was more pronounced in the first years after retirement and in particular longer withdrawal times had a very low mortality intensity. This was seen for both men and women. </p><p>When the mortality intensity was weighed based on the size of individuals' benefits the differences were much more pronounced for both men and women. Indicating that those with larger benefits to a greater extent chose their withdrawal time based on his or her own expectation of remaining lifespan.</p><p>Almost all results agreed with what would be expected based on the underlying hypothesis with one exception, lifelong withdrawal. For lifelong withdrawal both men and women had a significantly higher mortality intensity that would be expected if individuals aim to maximize their received benefits. This was likely explained by lifelong serving as a default or safe option for many individuals which would lead to the option being dominated by these individuals rather than those aiming to maximize their received benefits.</p>

corrected abstract:
<p>This study aims to investigate if taking individuals' chosen time of withdrawing occupational pension into account can be used to better predict the future mortality intensities which describes the probability of death in a given time interval. The main underlying hypothesis for why there would be a difference lies in the fact that some individuals could receive more pension benefits by choosing a withdrawal time based on his or her own expectation of remaining lifespan. In particular individuals that do not expect to live long after retirement could receive more benefits before death by choosing to withdraw his or her money as fast as possible.</p><p>To explore if such a relationship exists and is significant enough data provided by Alecta is analyzed. This data provides among others the chosen withdrawal time and recorded deaths. Based on this dataset three models are created each with their own advantages and disadvantages.</p><p>The first model is a pure empirical model which simply shows the processed data after calculations have been made, showing the exact data in a more comprehensive form. The second model is a Makeham model which assumes that the mortality intensity is distributed according to the Makeham formula in the studied age range. This model allows random noise to be smoothed out and allows predictions of the mortality intensity in ages not studied. The third model is a Logistic regression model. This model considers two cases, death or no death and based on observed data the model predicts the probability that an individual will die given an age and withdrawal time. The logistic regression model is a robust model which allows stable predictions even where data is scarce.</p><p>From the results of the three models it was concluded that for men there is a clear difference in mortality intensity between limited withdrawal times where shorter withdrawal times have higher mortality intensity and longer withdrawal times have lower mortality intensity. For women there were signs that the same is also true, however the evidence for this was weak.</p><p>The difference in mortality intensity was more pronounced in the first years after retirement and in particular longer withdrawal times had a very low mortality intensity. This was seen for both men and women.</p><p>When the mortality intensity was weighed based on the size of individuals' benefits the differences were much more pronounced for both men and women. Indicating that those with larger benefits to a greater extent chose their withdrawal time based on his or her own expectation of remaining lifespan.</p><p>Almost all results agreed with what would be expected based on the underlying hypothesis with one exception, lifelong withdrawal. For lifelong withdrawal both men and women had a significantly higher mortality intensity that would be expected if individuals aim to maximize their received benefits. This was likely explained by lifelong serving as a default or safe option for many individuals which would lead to the option being dominated by these individuals rather than those aiming to maximize their received benefits.</p>

Note: Only change was to delete the unnecessary space at the end of each paragraph.
----------------------------------------------------------------------
In diva2:1846355 
abstract is: 
<p>This thesis aims to cluster and classify mortgages issued by a financial institution. The aim is to apply machine learning techniques on historical data in order to discover a possible structure and predictability in prepaid mortgages. To discover the underlying structure of the data \textit{k}-means clustering on principal components is performed to cluster customers with mortgages.A logistic regression model is trained to predict how likely (future) customers with mortgages are to prepay their loans, hence moving them to another institution. The classification model is evaluated using confusion matrices for different levels of thresholds. The results show that based on historical data the model detects clusters which include a higher proportion of mortgages being prepaid. This indicating an underlying structure which can be used to determine a riskiness of leaving for customers within each cluster. The results from the logistic regression show a significant improvement in precision by using a high threshold in the classification.</p>

corrected abstract:
<p>This thesis aims to cluster and classify mortgages issued by a financial institution. The aim is to apply machine learning techniques on historical data in order to discover a possible structure and predictability in prepaid mortgages. To discover the underlying structure of the data <em>k</em>-means clustering on principal components is performed to cluster customers with mortgages. A logistic regression model is trained to predict how likely (future) customers with mortgages are to prepay their loans, hence moving them to another institution. The classification model is evaluated using confusion matrices for different levels of thresholds. The results show that based on historical data the model detects clusters which include a higher proportion of mortgages being prepaid. This indicating an underlying structure which can be used to determine a riskiness of leaving for customers within each cluster. The results from the logistic regression show a significant improvement in precision by using a high threshold in the classification.</p>

Note: Missing punctuation at the end of the last sentence in the original.
----------------------------------------------------------------------
In diva2:1842345 - text in abstract not selectable - appears to be images

missing subscripts in title:
"Uranium nitride synthesis by gas/gas reaction of UF6 and NH3"
==>
"Uranium nitride synthesis by gas/gas reaction of UF<sub>6</sub> and NH<sub>3</sub>"

abstract is: 
<p>This thesis project aims to develop an innovative technique for the production of high-purity uranium nitride (UN) through the ammonolysis of fluorides. The desired objective is to perform a controlled gas/gas reaction between uranium hexafluoride (UF6) and ammonia (NH3) at 800°C. The intermediate product thereby obtained (uranium dinitride, UN2) is subjected to further heating up to 1100°C under argon atmosphere, to ultimately produce UN. An inherent challenge faced in previous experiments was related to the dissociation of ammonia, which is alimiting factor for upscaling. Therefore, in this project a new setup is invented to address this challenge and it is proved experimentally: the idea is to achieve a coaxial laminar flow of UF6 and a carrier gas, where a central stream of the former is shielded by the latter so that the tworeacting gasses mix only in the hot point of the furnace, where the desired reaction can happen. To implement this approach, the ammonia dissociation has been studied, an apparatus for the controlled evaporation of UF6 has been designed and built, and two different injection nozzleshave been tested in different setup configurations. Eventually, the complete prototype has been tested altogether in a synthesis experiment at 800°C, and the products thus obtained have been converted into UN at 1100°C. Numerous auxiliary experiments have been performed using UF4as a reactant, as it is easier to handle and the results thus obtained can be largely extended to UF6. Lastly, a UF4 synthesis experiment has been performed, as educationally helpful to further dig into some chemistry features of this material, and a UN pellet has been sintered with Spark Plasma Sintering (SPS).</p>


corrected abstract:
<p>This thesis project aims to develop an innovative technique for the production of high-purity uranium nitride (UN) through the ammonolysis of fluorides. The desired objective is to perform a controlled gas/gas reaction between uranium hexafluoride (UF<sub>6</sub>) and ammonia (NH<sub>3</sub>) at 800°C. The intermediate product thereby obtained (uranium dinitride, UN2) is subjected to further heating up to 1100°C under argon atmosphere, to ultimately produce UN. An inherent challenge faced in previous experiments was related to the dissociation of ammonia, which is a limiting factor for upscaling. Therefore, in this project a new setup is invented to address this challenge and it is proved experimentally: the idea is to achieve a coaxial laminar flow of UF<sub>6</sub> and a carrier gas, where a central stream of the former is shielded by the latter so that the two reacting gasses mix only in the hot point of the furnace, where the desired reaction can happen. To implement this approach, the ammonia dissociation has been studied, an apparatus for the controlled evaporation of UF<sub>6</sub> has been designed and built, and two different injection nozzles have been tested in different setup configurations. Eventually, the complete prototype has been tested altogether in a synthesis experiment at 800°C, and the products thus obtained have been converted into UN at 1100°C. Numerous auxiliary experiments have been performed using UF<sub>4</sub> as a reactant, as it is easier to handle and the results thus obtained can be largely extended to UF<sub>6</sub>. Lastly, a UF<sub>4</sub> synthesis experiment has been performed, as educationally helpful to further dig into some chemistry features of this material, and a UN pellet has been sintered with Spark Plasma Sintering (SPS).</p>
----------------------------------------------------------------------
In diva2:1812797 
abstract is: 
<p>The LHCb experiment is one of the main detectors in function at the Large Hadron Collider (LHC) at CERN. In order to interpret the data recorded by the detector, Monte Carlo (MC) samples are used as a way to validate and test theories by comparison with the experimental data. This is especially true for Beyond the Standard Model (BSM) physics, for which Matrix Element Correction (MEC) generators, such as Madgraph, are needed in order to accurately simulate the hard process of a proton-proton collision.As the LHCb detector underwent a major upgrade for the Run 3 data-taking campaign, also its related software is being upgraded to cope with the increased amount of data being recorded by the detector. Specifically, the simulation software used by LHCb, called Gauss, has been updated to a version based on the new experiment-independent framework Gaussino. In order to produce BSM events for Run 3, Gauss, and therefore Gaussino, needs an interface to a MEC generator.This thesis proposes the implementation of an interface to Madgraph and other hard process generators for BSM physics in Gaussino, and an overall update and restructuring of its generation framework, concerning its production interfaces and tools, and the preparation of a multi-threaded environment for the production of BSM event samples. Although more updates in the Gaussino base framework are still needed to integrate all the implementations performed during the course of this thesis, the upgrade successfully interfaced Madgraph to Gaussino, simplified the overall generation framework of the LHCb software, and proved to be able to produce Monte Carlo simulations for Run 3.</p>

corrected abstract:
<p>The LHCb experiment is one of the main detectors in function at the Large Hadron Collider (LHC) at CERN. In order to interpret the data recorded by the detector, Monte Carlo (MC) samples are used as a way to validate and test theories by comparison with the experimental data. This is especially true for Beyond the Standard Model (BSM) physics, for which Matrix Element Correction (MEC) generators, such as Madgraph, are needed in order to accurately simulate the hard process of a proton-proton collision.</p><p>As the LHCb detector underwent a major upgrade for the Run 3 data-taking campaign, also its related software is being upgraded to cope with the increased amount of data being recorded by the detector. Specifically, the simulation software used by LHCb, called Gauss, has been updated to a version based on the new experiment-independent framework Gaussino. In order to produce BSM events for Run 3, Gauss, and therefore Gaussino, needs an interface to a MEC generator.</p><p>This thesis proposes the implementation of an interface to Madgraph and other hard process generators for BSM physics in Gaussino, and an overall update and restructuring of its generation framework, concerning its production interfaces and tools, and the preparation of a multi-threaded environment for the production of BSM event samples. Although more updates in the Gaussino base framework are still needed to integrate all the implementations performed during the course of this thesis, the upgrade successfully interfaced Madgraph to Gaussino, simplified the overall generation framework of the LHCb software, and proved to be able to produce Monte Carlo simulations for Run 3.</p>
----------------------------------------------------------------------
In diva2:1811859 
abstract is: 
<p>The accurate assessment of hemodynamics and its parameters play an important role when diagnosing cardiovascular diseases. In this context, 4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive measurement technique that facilitates hemodynamic parameter assessment as well as quantitative and qualitative analysis of three-directional flow over time. However, the assessment is limited by noise, low spatio-temporal resolution and long acquisition times. Consequently, in regions characterized by transient, rapid flow dynamics, such as the aorta and heart, capturing these rapid transient flows remains particularly challenging. Recent research has shown the feasibility of machine learning models to effectively denoise and increase the spatio-temporal resolution of 4D Flow MRI. However, temporal super-resolution networks, which can generalize on unseen domains and are independent on boundary segmentations, remain unexplored. </p><p>This study aims to investigate the feasibility of a neural network for temporal super-resolution and denoising of 4D Flow MRI data. To achieve this, we propose a residual convolutional neural network (based on the 4DFlowNet from Ferdian et al.) providing an end-to-end mapping from temporal low resolution space to high resolution space. The network is trained on patient-specific cardiac models created with computational-fluid dynamic (CFD) simulations covering a full cardiac cycle. For clinical contextualization, performance is assessed on clinical patient data. The study shows the potential of the 4DFlowNet for temporal-super resolution with an average relative error of 16.6 % on an unseen cardiac domain, outperforming deterministic methods such as linear and cubic interpolation. We find that the network effectively reduces noise and recovers high-transient flow by a factor of 2 on both in-silico and in-vivo cardiac datasets. The prediction results in a temporal resolution of 20 ms, going beyond the general clinical routine of 30-40 ms. This study exemplifies the performance of a residual CNN for temporal super-resolution of 4D flow MRI data, providing an option to extend evaluations to aortic geometries and to further develop different upsampling factors and temporal resolutions.</p>

corrected abstract:
<p>The accurate assessment of hemodynamics and its parameters play an important role when diagnosing cardiovascular diseases. In this context, 4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive measurement technique that facilitates hemodynamic parameter assessment as well as quantitative and qualitative analysis of three-directional flow over time. However, the assessment is limited by noise, low spatio-temporal resolution and long acquisition times. Consequently, in regions characterized by transient, rapid flow dynamics, such as the aorta and heart, capturing these rapid transient flows remains particularly challenging. Recent research has shown the feasibility of machine learning models to effectively denoise and increase the spatio-temporal resolution of 4D Flow MRI. However, temporal super-resolution networks, which can generalize on unseen domains and are independent on boundary segmentations, remain unexplored.</p><p>This study aims to investigate the feasibility of a neural network for temporal super-resolution and denoising of 4D Flow MRI data. To achieve this, we propose a residual convolutional neural network (based on the 4DFlowNet from Ferdian et al.) providing an end-to-end mapping from temporal low resolution space to high resolution space. The network is trained on patient-specific cardiac models created with computational-fluid dynamic (CFD) simulations covering a full cardiac cycle. For clinical contextualization, performance is assessed on clinical patient data. The study shows the potential of the 4DFlowNet for temporal-super resolution with an average relative error of 16.6% on an unseen cardiac domain, outperforming deterministic methods such as linear and cubic interpolation. We find that the network effectively reduces noise and recovers high-transient flow by a factor of 2 on both <em>in-silico</em> and <em>in-vivo</em> cardiac datasets. The prediction results in a temporal resolution of 20 ms, going beyond the general clinical routine of 30-40 ms. This study exemplifies the performance of a residual CNN for temporal super-resolution of 4D flow MRI data, providing an option to extend evaluations to aortic geometries and to further develop different upsampling factors and temporal resolutions.</p>
----------------------------------------------------------------------
In diva2:1761859 
abstract is: 
<p>The threaded fastener is by far the most common method for securing components together and plays a significant role in determining the quality of a product. Atlas Copco offers industrial tools for tightening these fasteners, which are today suffering from errors in the applied torque. These errors have been found to behave in periodic patterns which indicate that the errors can be predicted and therefore compensated for. However, this is only possible by knowing the rotational position of the tool. Atlas Copco is interested in the possibility of acquiring this rotational position without installing sensors inside the tools.</p><p>To address this challenge, the thesis explores the feasibility of estimating the rotational position by analysing the behaviour of the errors and finding periodicities in the data. The objective is to determine whether these periodicities can be used to accurately estimate the rotation of the torque errors of unknown data relative to errors of data where the rotational position is known. The tool analysed in this thesis exhibits a periodic pattern in the torque error with a period of 11 revolutions. </p><p>Two methods for estimating the rotational position were evaluated: a simple nearest neighbour method that uses mean squared error (MSE) as distance measure, and a more complex circular fully convolutional network (CFCN). The project involved data collection from a custom-built setup. However, the setup was not fully completed, and the models were therefore evaluated on a limited dataset.</p><p>The results showed that the CFCN method was not able to identify the rotational position of the signal. The insufficient size of the data is discussed to be the cause for this. The nearest neighbour method, however, was able to estimate the rotational position correctly with 100% accuracy across 1000 iterations, even when looking at a fragment of a signal as small as 40%. Unfortunately, this method is computationally demanding and exhibits slow performance when applied to large datasets. Consequently, adjustments are required to enhance its practical applicability. In summary, the findings suggest that the nearest neighbour method is a promising approach for estimating the rotational position and could potentially contribute to improving the accuracy of tools.</p>

corrected abstract:
<p>The threaded fastener is by far the most common method for securing components together and plays a significant role in determining the quality of a product. Atlas Copco offers industrial tools for tightening these fasteners, which are today suffering from errors in the applied torque. These errors have been found to behave in periodic patterns which indicate that the errors can be predicted and therefore compensated for. However, this is only possible by knowing the rotational position of the tool. Atlas Copco is interested in the possibility of acquiring this rotational position without installing sensors inside the tools.</p><p>To address this challenge, the thesis explores the feasibility of estimating the rotational position by analysing the behaviour of the errors and finding periodicities in the data. The objective is to determine whether these periodicities can be used to accurately estimate the rotation of the torque errors of unknown data relative to errors of data where the rotational position is known. The tool analysed in this thesis exhibits a periodic pattern in the torque error with a period of 11 revolutions.</p><p>Two methods for estimating the rotational position were evaluated: a simple nearest neighbour method that uses mean squared error (MSE) as distance measure, and a more complex circular fully convolutional network (CFCN). The project involved data collection from a custom-built setup. However, the setup was not fully completed, and the models were therefore evaluated on a limited dataset.</p><p>The results showed that the CFCN method was not able to identify the rotational position of the signal. The insufficient size of the data is discussed to be the cause for this. The nearest neighbour method, however, was able to estimate the rotational position correctly with 100% accuracy across 1000 iterations, even when looking at a fragment of a signal as small as 40%. Unfortunately, this method is computationally demanding and exhibits slow performance when applied to large datasets. Consequently, adjustments are required to enhance its practical applicability. In summary, the findings suggest that the nearest neighbour method is a promising approach for estimating the rotational position and could potentially contribute to improving the accuracy of tools.</p>


Note: Only change was the elimination of an unnecessary space at the end of a paragraph.
----------------------------------------------------------------------
In diva2:1756988 
abstract is: 
<p>During economic cycles throughout time organizations allocate their resources in accordance with overall market dynamics, shareholder make decisions based on market dynamics as well as how well a company allocate their resources. In addition, studies of resource allocation during specific market cycles are narrow due to limited data and opportunities to study. Therefore, this thesis aims to study the relation of financial ratios to change in market capitalization during the COVID-19 pandemic with multiple linear regression. Furthermore,the impact of the key financial ratios will be examined and discussed based on their relationto the market capitalization. Financial ratios and their respective market capitalization werecollected from 251 ticker symbols defined as Nordic large cap companies by Nasdaq. From these companies, the analysis was narrowed down to only include companies listed on Stockholm Stock Exchange. The financial ratios were defined from 2020 and the change in market capitalization was defined between 2020 and 2021. Ratios that proved to be significant in the model were narrowed down to five different ratios primarily derived from a company’s assets. The final model was validated at a 𝑅 2 = 0.2115. Furthermore, the model was significant in a mathematical sense, but further research is needed with more sophisticated methods.</p>

corrected abstract:
<p>During economic cycles throughout time organizations allocate their resources in accordance with overall market dynamics, shareholder make decisions based on market dynamics as well as how well a company allocate their resources. In addition, studies of resource allocation during specific market cycles are narrow due to limited data and opportunities to study. Therefore, this thesis aims to study the relation of financial ratios to change in market capitalization during the COVID-19 pandemic with multiple linear regression. Furthermore, the impact of the key financial ratios will be examined and discussed based on their relation to the market capitalization. Financial ratios and their respective market capitalization were collected from 251 ticker symbols defined as Nordic large cap companies by Nasdaq. From these companies, the analysis was narrowed down to only include companies listed on Stockholm Stock Exchange. The financial ratios were defined from 2020 and the change in market capitalization was defined between 2020 and 2021. Ratios that proved to be significant in the model were narrowed down to five different ratios primarily derived from a company’s assets. The final model was validated at a R<sup>2</sup> = 0.2115. Furthermore, the model was significant in a mathematical sense, but further research is needed with more sophisticated methods.</p>
----------------------------------------------------------------------
In diva2:1756981 
abstract is: 
<p>This paper examines the impact of several macroeconomic variables on property prices in Sweden. Linear regression is used to construct severalmathematical models relating the macroeconomic variables to property prices. Using methods of variables selection and goodness of fit measures,two final models are selected and subsequently compared, resulting in one final model. From this model, we conclude that GDP per capita, unemployment rate, inflation and repo interest rate have a significant relationship with property price changes in Sweden. Unemployment, GDPper capita, and inflation have positive relationships with property price changes, while repo interest rate has a negative relationship with propertyprice changes. However, as to what extent these variables affect property prices, no certain conclusions can be drawn from this study.</p>

corrected abstract:
<p>This paper examines the impact of several macroeconomic variables on property prices in Sweden. Linear regression is used to construct several mathematical models relating the macroeconomic variables to property prices. Using methods of variables selection and goodness of fit measures, two final models are selected and subsequently compared, resulting in one final model. From this model, we conclude that GDP per capita, unemployment rate, inflation and repo interest rate have a significant relationship with property price changes in Sweden. Unemployment, GDP per capita, and inflation have positive relationships with property price changes, while repo interest rate has a negative relationship with property price changes. However, as to what extent these variables affect property prices, no certain conclusions can be drawn from this study.</p>
----------------------------------------------------------------------
In diva2:1752047 
abstract is: 
<p>The industry of electronic commerce (e-commerce) constitutes a great part of the yearly retail consumption in Sweden. Looking at recent years, it has been seen that a rapidly growing sector within the mentioned field is the clothing industry for clothes and accessories for children and newborns. To get an overview of the items and help customers to find what they are looking for, many web stores have a system called a Recommendation System. The mechanics behind this service can look rather different depending on the method used. However, their unified goal is to provide a list of recommended items of interest to the customer. </p><p>A branch within this field is the Session Based Recommendation System (SBRS). These are models which are designed to work with the trace of products, called a session, that a user currently has visited on the web store. Based on that information they then formulate an output of recommended items. The SBRS models have been especially popularized since the majority of customers browse in an anonymous behavior, which means that they due to time efficiency often neglect the possibility of creating or logging into any personal web store account. This however limits the accessible information that a system can make use of to shape its item list. </p><p>It can be seen that the number of articles exploring SBRS within the fashion branch of clothing and accessories for children is very limited. This thesis is made to fill that gap. After a thorough literature study, three models were found to be of certain interest, the Short-Term Attention/Memory Priority (STAMP) model, Long Short-Term Memory (LSTM) model, and Gated Recurrent Unit (GRU) model. Further, the LSTM model is included as it is the collaborative company, BabyShop Group AB's current used method. </p><p>The results of this thesis show that the GRU model is a promising method, managing to predict the next item for a customer more consistently than any other of the evaluated models. Furthermore, it can also be seen that what embeddings the models use to represent the products plays a significant role in the learning and evaluation of the used data set. </p><p>Moreover, a benchmark model included in this thesis also shows the importance of filtering the data set of sessions. It can be seen that a majority of customers visit already-seen products, logged happenings most likely due to refreshing web pages or similar actions. This causes the session data set to be characterized by repeated items. For future work, it would therefore indeed be interesting to see how this data set can be filtered in a different way. To see how that affects the outcome of the used metrics in this thesis.</p>

corrected abstract:
<p>The industry of electronic commerce (e-commerce) constitutes a great part of the yearly retail consumption in Sweden. Looking at recent years, it has been seen that a rapidly growing sector within the mentioned field is the clothing industry for clothes and accessories for children and newborns.</p><p>To get an overview of the items and help customers to find what they are looking for, many web stores have a system called a <em>Recommendation System</em>. The mechanics behind this service can look rather different depending on the method used. However, their unified goal is to provide a list of recommended items of interest to the customer.</p><p>A branch within this field is the <em>Session Based Recommendation System</em> (SBRS). These are models which are designed to work with the trace of products, called a session, that a user currently has visited on the web store. Based on that information they then formulate an output of recommended items. The SBRS models have been especially popularized since the majority of customers browse in an anonymous behavior, which means that they due to time efficiency often neglect the possibility of creating or logging into any personal web store account. This however limits the accessible information that a system can make use of to shape its item list.</p><p>It can be seen that the number of articles exploring SBRS within the fashion branch of clothing and accessories for children is very limited. This thesis is made to fill that gap. After a thorough literature study, three models were found to be of certain interest, the Short-Term Attention/Memory Priority (STAMP) model, Long Short-Term Memory (LSTM) model, and Gated Recurrent Unit (GRU) model. Further, the LSTM model is included as it is the collaborative company, BabyShop Group AB's current used method.</p><p>The results of this thesis show that the GRU model is a promising method, managing to predict the next item for a customer more consistently than any other of the evaluated models. Furthermore, it can also be seen that what embeddings the models use to represent the products plays a significant role in the learning and evaluation of the used data set.</p><p>Moreover, a benchmark model included in this thesis also shows the importance of filtering the data set of sessions. It can be seen that a majority of customers visit already-seen products, logged happenings most likely due to refreshing web pages or similar actions. This causes the session data set to be characterized by repeated items. For future work, it would therefore indeed be interesting to see how this data set can be filtered in a different way. To see how that affects the outcome of the used metrics in this thesis.</p>
----------------------------------------------------------------------
In diva2:1741190 
abstract is: 
<p>Reducing the fuel consumption of vehicles is one of the main automotive industry's objectives. The obvious solution is to reduce the mass of the vehicle, so less mass has to be put into motion, which requires less energy. With this in mind, it was decided to look at anti-intrusion bars. This passive safety component installed in the doors plays a key role in side impacts, as it reduces the penetration of the bodywork into the passenger compartment. Thus, the lightening of vehicles should not be accompanied by a reduction in safety.The aim of this thesis is to explore the advantages of the hydroforming process, which has not yet been used to shape these parts. The work carried out focused on the design of hydroformed bars by considering the numerous constraints due to the implementation of such a part and the study of different usable materials. Finally, numerical simulations comparing a deep-drawn bar with the designed parts showed that hydroforming appears to produce anti-intrusion bars that are 10% lighter than and as effective as deep drawing. Simple simulations of the hydroforming process also showed that it appeared possible to shape the designed parts using this process.This work needs to be continued with more accurate calculations of the hydroforming process, simulations of crash tests with a full vehicle model and physical realisation of the part for real-world testing.</p>

corrected abstract:
<p>Reducing the fuel consumption of vehicles is one of the main automotive industry's objectives. The obvious solution is to reduce the mass of the vehicle, so less mass has to be put into motion, which requires less energy. With this in mind, it was decided to look at anti-intrusion bars. This passive safety component installed in the doors plays a key role in side impacts, as it reduces the penetration of the bodywork into the passenger compartment. Thus, the lightening of vehicles should not be accompanied by a reduction in safety.</p><p>The aim of this thesis is to explore the advantages of the hydroforming process, which has not yet been used to shape these parts. The work carried out focused on the design of hydroformed bars by considering the numerous constraints due to the implementation of such a part and the study of different usable materials. Finally, numerical simulations comparing a deep-drawn bar with the designed parts showed that hydroforming appears to produce anti-intrusion bars that are 10% lighter than and as effective as deep drawing. Simple simulations of the hydroforming process also showed that it appeared possible to shape the designed parts using this process.</p><p>This work needs to be continued with more accurate calculations of the hydroforming process, simulations of crash tests with a full vehicle model and physical realisation of the part for real-world testing.</p>

Note: Interpreted the periods without a space after them as probably a paragraph break.
----------------------------------------------------------------------
In diva2:1698134 
abstract is: 
<p>Financial markets have been crucial in driving capital investments across the world. Anessential piece of these markets is the presence of risk takers, or market speculators, who will hold financial portfolios in hopes of profit. Portfolios with cash flowsgenerated from floating interest rate derivatives will often be subjected to <em>fixing </em>risk, also called second-order basis risk, stemming from a discrepancy in time with the hedge and the original position.</p><p>Using data from a fixing risk mitigation service, named RESET, this thesis aims todeepen the understanding of accumulation of fixing risk on the the USD dollar market for 3-month interest rate swaps. This is done by modeling customer behavior using machine learning methods. Macroeconomic factors such as market volatility and the January effect amongst others were incorporated as variables into the set. The two models explored are logistic regression and neural networks, the first one chosen for interoperability and the latter for its generality.</p><p>Neither of the two models could accurately predict customer behavior, with a balanced accuracy short of 70 percent. The strongest influence of the final prediction turned out to be previous behavior, the January effect and how many of their financial positions the customer previously put into the service. </p>

corrected abstract:
<p>Financial markets have been crucial in driving capital investments across the world. An essential piece of these markets is the presence of risk takers, or market speculators, who will hold financial portfolios in hopes of profit. Portfolios with cash flows generated from floating interest rate derivatives will often be subjected to <em>fixing risk</em>, also called second-order basis risk, stemming from a discrepancy in time with the hedge and the original position.</p><p>Using data from a fixing risk mitigation service, named RESET, this thesis aims to deepen the understanding of accumulation of fixing risk on the the USD dollar market for 3-month interest rate swaps. This is done by modeling customer behavior using machine learning methods. Macroeconomic factors such as market volatility and the January effect amongst others were incorporated as variables into the set. The two models explored are logistic regression and neural networks, the first one chosen for interoperability and the latter for its generality.</p><p>Neither of the two models could accurately predict customer behavior, with a balanced accuracy short of 70 percent. The strongest influence of the final prediction turned out to be previous behavior, the January effect and how many of their financial positions the customer previously put into the service.</p>
----------------------------------------------------------------------
In diva2:1670939 - correct as is
----------------------------------------------------------------------
In diva2:1642450 
abstract is: 
<p>This  master  thesis  was  performed  together  with  the  Vehicle  Dynamics  department  at  Volvo  Cars,  Torslanda.  Volvo Cars is a global producer of passenger cars. With increasing demand of safety, vehicle stability and efficiency, coupled to  customer  demands  of  agility  and  driving  pleasure  from  passenger  cars,  there  is  a  need  of  active  systems  to  both enhance the vehicle dynamics while preserving or increase the efficiency of the vehicle. Therefore, it is of necessity to evaluate possible alternatives to existing systems in order to fulfil these demands.</p><p>The scope of this Master Thesis was to evaluate the control algorithm “G-vectoring”, used to control yaw response in transient manoeuvres by controlling the longitudinal acceleration and thereby the load transfer. Further, the control algorithm was to be used with the ICE-driveline as actuator to obtain the longitudinal acceleration. The controller was to be evaluated in terms of handling, comfort, applicability and efficiency.</p><p>The project consisted of two phases: first, a full factorial parameter study was performed using six vehicle specific factors  and  1  environmental  factor,  friction  coefficient,  to  evaluate  the  general  effects  of  the  driveline  controller. Further, to understand on which vehicle the controller would be most applicable on in terms of handling and comfort, and how it affected the general efficiency of the vehicles. The full factorial test was done using simulations, using a one-track model extended with a simple load transfer model, and magic-formula tyres. The driveline used was a Volvo T5  FWD  petrol  driveline  with  6  speed  automatic  Gearbox.  The  full  factorial  experiment  was  done  using  three manoeuvres, step steer, sine with dwell, and closed loop driving.</p><p>The  second  phase  was  to  answer  how  the  controller  affected  the  handling  and  comfort  of  the  vehicle  using subjective and objective assessment. In this stage, the controller was implemented to be used by real drivers in Volvo Cars driving simulator, using VI-CarRealTime as simulation tool. The controller was tuned for the nonlinear system with gain scheduling approach, and a linear description of the system was developed.</p><p>The subjective assessment was done over a set of two tests, the first consisting of free driving in the linear range on a 3km test track for 8 laps, the first 4 with the controller active, and second with the controller inactive. The second was a set of 10 consecutive ISO 3888-2 Double Lane changes   (DLC), the first 5 with the controller active, and the last 5 with the controller inactive. The subjective assessment was done with a 1-10 scale similar to SAE J1441. The objective evaluation was done only using the data obtained from the DLC test, as it poses a confined path for the driver to follow. The results from the full factorial test show that the controller can affect the yaw damping, yaw stability, steering angle variance and yaw response of a passenger car. Further, the results indicate that the controller can be expected to have the largest effect on a vehicle that has low mass, low inertia and a high centre of gravity. It was also found that the</p><p>controller poses a more fuel-efficient way of controlling yaw compared to a ESC equipped vehicle.</p><p>From the driving simulator test, it was found that the controller had better subjective rating  overall in  terms  of steering  and  handling  on  a  general  steering  feel  and  handling  feel  level.  On  a  more  detailed  level  the  subjective assessment saw the largest advantages of the controller in terms of straight ahead response, under/oversteer at power on/off, and transitional stability, controllability and capacity feel. The disadvantage is primarily less torque feedback and less controllability at power on/off. In the objective metrics, the effect of the controller indicates a general decrease of the steering variance and an a decrease of the steering peak-to-peak values.</p><p>The conclusion is that G-vectoring can enhance the lateral response and reduce the steering effort of a passenger</p><p>car, and that the effects from G-vectoring can be felt by the driver.</p>

corrected abstract:
<p>This master thesis was performed together with the Vehicle Dynamics department at Volvo Cars, Torslanda. Volvo Cars is a global producer of passenger cars. With increasing demand of safety, vehicle stability and efficiency, coupled to customer demands of agility and driving pleasure from passenger cars, there is a need of active systems to both enhance the vehicle dynamics while preserving or increase the efficiency of the vehicle. Therefore, it is of necessity to evaluate possible alternatives to existing systems in order to fulfil these demands.</p><p>The scope of this Master Thesis was to evaluate the control algorithm “G-vectoring”, used to control yaw response in transient manoeuvres by controlling the longitudinal acceleration and thereby the load transfer. Further, the control algorithm was to be used with the ICE-driveline as actuator to obtain the longitudinal acceleration. The controller was to be evaluated in terms of handling, comfort, applicability and efficiency.</p><p>The project consisted of two phases: first, a full factorial parameter study was performed using six vehicle specific factors and 1 environmental factor, friction coefficient, to evaluate the general effects of the driveline controller. Further, to understand on which vehicle the controller would be most applicable on in terms of handling and comfort, and how it affected the general efficiency of the vehicles. The full factorial test was done using simulations, using a one-track model extended with a simple load transfer model, and magic-formula tyres. The driveline used was a Volvo T5 FWD petrol driveline with 6 speed automatic Gearbox. The full factorial experiment was done using three manoeuvres, step steer, sine with dwell, and closed loop driving.</p><p>The second phase was to answer how the controller affected the handling and comfort of the vehicle using subjective and objective assessment. In this stage, the controller was implemented to be used by real drivers in Volvo Cars driving simulator, using VI-CarRealTime as simulation tool. The controller was tuned for the nonlinear system with gain scheduling approach, and a linear description of the system was developed.</p><p>The subjective assessment was done over a set of two tests, the first consisting of free driving in the linear range on a 3km test track for 8 laps, the first 4 with the controller active, and second with the controller inactive. The second was a set of 10 consecutive ISO 3888-2 Double Lane changes (DLC), the first 5 with the controller active, and the last 5 with the controller inactive. The subjective assessment was done with a 1-10 scale similar to SAE J1441. The objective evaluation was done only using the data obtained from the DLC test, as it poses a confined path for the driver to follow. The results from the full factorial test show that the controller can affect the yaw damping, yaw stability, steering angle variance and yaw response of a passenger car. Further, the results indicate that the controller can be expected to have the largest effect on a vehicle that has low mass, low inertia and a high centre of gravity. It was also found that the controller poses a more fuel-efficient way of controlling yaw compared to a ESC equipped vehicle.</p><p>From the driving simulator test, it was found that the controller had better subjective rating overall in terms of steering and handling on a general steering feel and handling feel level. On a more detailed level the subjective assessment saw the largest advantages of the controller in terms of straight ahead response, under/oversteer at power on/off, and transitional stability, controllability and capacity feel. The disadvantage is primarily less torque feedback and less controllability at power on/off. In the objective metrics, the effect of the controller indicates a general decrease of the steering variance and an a decrease of the steering peak-to-peak values.</p><p>The conclusion is that G-vectoring can enhance the lateral response and reduce the steering effort of a passenger car, and that the effects from G-vectoring can be felt by the driver.</p>
----------------------------------------------------------------------
In diva2:1640140 
abstract is: 
<p>According to the World Health Organisation, nine out of ten people live in places with sub par air quality. Therefore, the study of the treatment of polluted air is from a global point of view of great importance, and a matter of life quality for both humans and as well as animals. There are several di erent methods to treat polluted air, this study focuses on treatment of gas exhausts through ozonation. In the ozonation pro-cess, ozone is injected into the air stream, reacting and degrading the pollutants. The mixing phenomena of ozone in air is highly important for the process, being the focus of this study.To study the mixing, a computational fluid dynamic study was commit-ted on the injection process, with geometries base on industrial applica-tions. The Finite Volume Method was implemented in the simulation software OpenFOAM, on a highly turbulent incompressible flow. To depict the turbulence, Reynolds average RNG k − ε with wall functions and the Large Eddy Simulation (LES) with Wall-Adapting Local Eddy-viscosity (WALE) models were utilized and compared. The mixing is implemented through the transport equation for a passive scalar. For the steady state simulations the Simple-implicit Method for Pressure Linked Equation (SIMPLE) solver was used, and the PIMPLE algo-rithm for the transient cases.First, the flow through a 90 degree elbow pipe bend was investigated. A secondary flow, perpendicular to the pipe, was induced by the pipe bend making the flow three dimensional. Both turbulence models showed great similarities to previous experimental results, and were deemed suÿcient in terms of depicting the flow behaviour. The intensity of the secondary flow depends on the curvature radius, whereas a change in Reynold’s number has negligible e ects on the flow. After the valida-tion, a study of the injection after the pipe bend was conducted, where the position of the injection showed the greatest e ect on the mixing quality. Due to the position being important, it was shown that the secondary flow highly correlates with the quality of mixing, where a high intensity yields a better mixing.</p>

corrected abstract:
<p>According to the World Health Organisation, nine out of ten people live in places with sub par air quality. Therefore, the study of the treatment of polluted air is from a global point of view of great importance, and a matter of life quality for both humans and as well as animals. There are several different methods to treat polluted air, this study focuses on treatment of gas exhausts through ozonation. In the ozonation process, ozone is injected into the air stream, reacting and degrading the pollutants. The mixing phenomena of ozone in air is highly important for the process, being the focus of this study.</p><p>To study the mixing, a computational fluid dynamic study was committed on the injection process, with geometries base on industrial applications. The Finite Volume Method was implemented in the simulation software OpenFOAM, on a highly turbulent incompressible flow. To depict the turbulence, Reynolds average RNG 𝑘 − ε with wall functions and the Large Eddy Simulation (LES) with Wall-Adapting Local Eddy-viscosity (WALE) models were utilized and compared. The mixing is implemented through the transport equation for a passive scalar. For the steady state simulations the Simple-implicit Method for Pressure Linked Equation (SIMPLE) solver was used, and the PIMPLE algorithm for the transient cases.</p><p>First, the flow through a 90 degree elbow pipe bend was investigated. A secondary flow, perpendicular to the pipe, was induced by the pipe bend making the flow three dimensional. Both turbulence models showed great similarities to previous experimental results, and were deemed sufficient in terms of depicting the flow behaviour. The intensity of the secondary flow depends on the curvature radius, whereas a change in Reynold’s number has negligible effects on the flow. After the validation, a study of the injection after the pipe bend was conducted, where the position of the injection showed the greatest effect on the mixing quality. Due to the position being important, it was shown that the secondary flow highly correlates with the quality of mixing, where a high intensity yields a better mixing.</p>
----------------------------------------------------------------------
In diva2:1609991 
abstract is: 
<p>Following the global financial crisis, both risk-based and heuristic portfolio construction methods have received much attention from both academics and practitioners since these methods do not rely on the estimation of expected returns and as such are assumed to be more stable than Markowitz's traditional mean-variance portfolio. In 2016, Lopéz de Prado presented the Hierarchical Risk Parity (HRP), a new approach to portfolio construction which combines hierarchical clustering of assets with a heuristic risk-based allocation strategy in order to increase stability and improve out-of-sample performance. Using Monte Carlo simulations, Lopéz de Prado was able to demonstrate promising results.</p><p>This thesis attempts to evaluate HRP using walk-forward analysis and historical data from equity index and bond futures, against more realistic benchmark methods and using additional performance measures relevant to practitioners. The main conclusion is that applying hierarchical clustering to risk-based portfolio construction does indeed improve the out-of-sample return and Sharpe ratio. However, the resulting portfolio is also associated with a remarkably high turnover, which may indicate numerical instability and sensitivity to estimation errors. It is also identified that Lopéz de Prado's original HRP approach has an undesirable property and alternative approaches to HRP have consequently been developed. Compared to Lopéz de Prado's original HRP approach, these alternative approaches increase the Sharpe ratio with ~10% and reduce the turnover with 60-65%. However, it should be noted that compared to more mainstream portfolios the turnover is still rather high, indicating that these alternative approaches to HRP are still somewhat unstable and sensitive to estimation errors.</p>

corrected abstract:
<p>Following the global financial crisis, both risk-based and heuristic portfolio construction methods have received much attention from both academics and practitioners since these methods do not rely on the estimation of expected returns and as such are assumed to be more stable than Markowitz's traditional mean-variance portfolio. In 2016, Lopéz de Prado presented the Hierarchical Risk Parity (HRP), a new approach to portfolio construction which combines hierarchical clustering of assets with a heuristic risk-based allocation strategy in order to increase stability and improve out-of-sample performance. Using Monte Carlo simulations, Lopéz de Prado was able to demonstrate promising results.</p><p>This thesis attempts to evaluate HRP using walk-forward analysis and historical data from equity index and bond futures, against more realistic benchmark methods and using additional performance measures relevant to practitioners. The main conclusion is that applying hierarchical clustering to risk-based portfolio construction does indeed improve the out-of-sample return and Sharpe ratio. However, the resulting portfolio is also associated with a remarkably high turnover, which may indicate numerical instability and sensitivity to estimation errors. It is also identified that Lopéz de Prado's original HRP approach has an undesirable property and alternative approaches to HRP have consequently been developed. Compared to Lopéz de Prado's original HRP approach, these alternative approaches increase the Sharpe ratio with 10% and reduce the turnover with 60-65%. However, it should be noted that compared to more mainstream portfolios the turnover is still rather high, indicating that these alternative approaches to HRP are still somewhat unstable and sensitive to estimation errors.</p>

Note: The DiVA abstract showed ~10%, but the thesis does not have the "~".
----------------------------------------------------------------------
In diva2:1599594 
abstract is: 
<p>This thesis is about the development of a new numerical method for the analysis of composite shells. The present work is based on Reissner Mixed Variational Theorem (RMVT), the Sublaminate Generalized Unified Formulation (S-GUF), and the Ritz approximation. The present work investigates a more efficient way to compute transverse stresses (sigma_xz, sigma_yz, sigma_zz) based upon RMVT, allowing assigning their order of continuity a priori. This is a great advantage compared to a conventional displacement-based approach. In order to enable computing of both global and local responses (depending on the user’sneeds) the S-GUF framework was adopted. The Generalized Unified Formulation (GUF) enables the implementation of approximations with virtually unlimited algebraic order within a single code, and the order could also vary for different variables. In addition to the GUF, the concept of Sublaminate was utilized, allowing for sub-sectioning of the domain in the thickness direction into sublaminates, and it is then possible to apply different formulations in each of these sub-domains. The curvature of the shells is strictly defined by their radius-to- thickness ratio. The flexibility of S-GUF is helpful in the sense that curvature is only introduced and treated if needed by the particular case at hand. The governing equations obtained applying S-GUF to RMVT were solved in a weak formulation using the Ritz approximation. This choice was made to save computational time, which is one of the main benefits of the presented method. Validation of the code was made by comparing results from the present formulation with solutions available in the literature. Good to excellent agreement was found for several benchmark cases, supporting that the formulation is valid and provides reliable solutions.Finally, numerical and analytical considerations about the developed method were made: its numerical stability, how to tune its parameters, and which models result more correct from an analytical standpoint.</p>

corrected abstract:
<p>This thesis is about the development of a new numerical method for the analysis of composite shells. The present work is based on <em>Reissner Mixed Variational Theorem</em> (RMVT), the <em>Sublaminate Generalized Unified Formulation</em> (S-GUF), and the <em>Ritz approximation</em>. The present work investigates a more efficient way to compute transverse stresses (&sigma;<sub>xz</sub>, &sigma;<sub>yz</sub>, &sigma;<sub>zz</sub>) based upon RMVT, allowing assigning their order of continuity a priori. This is a great advantage compared to a conventional displacement-based approach.</p><p>In order to enable computing of both global and local responses (depending on the user’s needs) the S-GUF framework was adopted. The <em>Generalized Unified Formulation</em> (GUF) enables implementation of approximations with virtually unlimited algebraic order within a single code, and the order could also vary for different variables. In addition to the GUF, the concept of Sublaminate was utilized, allowing for sub-sectioning of the domain in the thickness direction into sublaminates, and it is then possible to apply different formulations in each of these sub-domains. The curvature of the shells is strictly defined by their radius-to-thickness ratio. The flexibility of S-GUF is helpful in the sense that curvature is only introduced and treated if needed by the particular case at hand.</p><p>The governing equations obtained applying S-GUF to RMVT were solved in a weak formulation using the <em>Ritz approximation</em>. This choice was made to save computational time, which is one of the main benefits of the presented method.</p><p>Validation of the code was made by comparing results from the present formulation with solutions available in the literature. Good to excellent agreement was found for several benchmark cases, supporting that the formulation is valid and provides reliable solutions.</p><p>Finally, numerical and analytical considerations about the developed method were made: its numerical stability, how to tune its parameters and which models result more correct from an analytical standpoint.</p>
----------------------------------------------------------------------
In diva2:1595175 
abstract is: 
<p>The problem of fleet design and fleet modelling; for decades problems regarding determining fleet sizes and optimized routing problems have formed the groundwork into the fleet design and fleet optimization for a wide range of business sectors. In most of these problems only single entities or fixed design resources are optimized for a certain route and delivery objective based on minimizing operational costs. In the naval industry, there has been a growing need for numerical methods that are able to predict what kind of fleets, in terms of size and capabilities, would be suited to achieve certain operational needs. Further than that, for shipbuilders and designers, what kind of design requirements the individual vessel platforms in such a fleet must contain constitutes the bridge in translating operational needs to ship design and system integration requirements. Especially in an era where technology advances more quickly than it takes to design a naval vessel, creating tools that are able to predict something about future fleet resilience could become an effective asset for future naval fleet development. For this, studies that contribute to developing methods that can evaluate the combined effect of individual vessel platforms from a fleet perspective are still fairly limited. The overall goal of this study was to determine how the development and application of fleet modelling tools can contribute to designing naval fleets that are more robust against future threats and missions. The objective was to extend and build on a fleet modelling method based on Systems Engineering, that is able to generate fleet compositions and produce basic individual platform design requirements for early-stage design phases of naval fleets through scripted naval scenario's.</p><p>The aim was to construct a functional numerical simulation model through Mixed Integer Linear programming and extend the abilities of the method to be able to include 'future' technologies, with UXV's receiving the main focus. The overall potential and results that the numerical model produces are interesting, through an optimization process it is able to build a fleet from a wide range of platform choices and deliver basic platform design requirements. Actual combat performance of the fleets that are generated, is debatable and needs to be further investigated and tested/verified through different means. The conclusion from the study is that to design future resilient fleets, more research and development is needed in the area of naval fleet modelling and simulation since the functionality of tools available can not overcome the amount of uncertainty that the future brings. Besides that, the method under review does make it able to generate interesting fleet combinations that could spark new ideas on how we could regard the future potential of uprising technology and their combined capabilities with naval vessel platforms.</p>

corrected abstract:
<p>The problem of fleet design and fleet modelling; for decades problems regarding determining fleet sizes and optimized routing problems have formed the groundwork into the fleet design and fleet optimization for a wide range of business sectors. In most of these problems only single entities or fixed design resources are optimized for a certain route and delivery objective based on minimizing operational costs. In the naval industry, there has been a growing need for numerical methods that are able to predict what kind of fleets, in terms of size and capabilities, would be suited to achieve certain operational needs. Further than that, for shipbuilders and designers, what kind of design requirements the individual vessel platforms in such a fleet must contain constitutes the bridge in translating operational needs to ship design and system integration requirements. Especially in an era where technology advances more quickly than it takes to design a naval vessel, creating tools that are able to predict something about future fleet resilience could become an effective asset for future naval fleet development. For this, studies that contribute to developing methods that can evaluate the combined effect of individual vessel platforms from a fleet perspective are still fairly limited. The overall goal of this study was to determine how the development and application of fleet modelling tools can contribute to designing naval fleets that are more robust against future threats and missions. The objective was to extend and build on a fleet modelling method, [1], based on Systems Engineering, that is able to generate fleet compositions and produce basic individual platform design requirements for early-stage design phases of naval fleets through scripted naval scenario's.</p><p>The aim was to construct a functional numerical simulation model through Mixed Integer Linear programming and extend the abilities of the method to be able to include 'future' technologies, with UXV's receiving the main focus. The overall potential and results that the numerical model produces are interesting, through an optimization process it is able to build a fleet from a wide range of platform choices and deliver basic platform design requirements. Actual combat performance of the fleets that are generated, is debatable and needs to be further investigated and tested/verified through different means. The conclusion from the study is that to design future resilient fleets, more research and development is needed in the area of naval fleet modelling and simulation since the functionality of tools available can not overcome the amount of uncertainty that the future brings. Besides that, the method under review does make it able to generate interesting fleet combinations that could spark new ideas on how we could regard the future potential of uprising technology and their combined capabilities with naval vessel platforms.</p>
----------------------------------------------------------------------
In diva2:1583445 - correct as is.
----------------------------------------------------------------------
In diva2:1574217  - correct as is.
----------------------------------------------------------------------
In diva2:1573626 
abstract is: 
<p>The increasing environmental issues and the measures taken to tackle them, is a topic ofhigh significance in today’s society. In light of this, the EU is underway with developinga taxonomy classifying sustainable economic activities in hopes to raise awareness, increase transparency regarding environmental impact, and motivate investors to invest sustainable. This paper aims to examine if the taxonomy is relevant to its cause, as well as if sustainability factors can be identified with linear regression connected to growth in a company’s value, which may motivate sustainable investments. Several interviews were conducted, along with the creation of a mathematical model. The conclusions drawn was that it is not viable to determine a company’s growth in value using solely sustainability factors. However, the results were promising regarding the implementation of sustainability factors in more comprehensive models. Furthermore, the impact of the taxonomy was hard to predict at this time, however, the consensus of the majority of the interviews conducted with experts on the subject, is that it has potential to impact sustainable investments in the future. Future research on the taxonomy may yield results of higher interest since more comprehensive data will be available, and the impact of the taxonomy will be more concrete.</p>

corrected abstract:
<p>The increasing environmental issues and the measures taken to tackle them, is a topic of high significance in today’s society. In light of this, the EU is underway with developing a taxonomy classifying sustainable economic activities in hopes to raise awareness, increase transparency regarding environmental impact, and motivate investors to invest sustainable. This paper aims to examine if the taxonomy is relevant to its cause, as well as if sustainability factors can be identified with linear regression connected to growth in a company’s value, which may motivate sustainable investments. Several interviews were conducted, along with the creation of a mathematical model. The conclusions drawn was that it is not viable to determine a company’s growth in value using solely sustainability factors. However, the results were promising regarding the implementation of sustainability factors in more comprehensive models. Furthermore, the impact of the taxonomy was hard to predict at this time, however, the consensus of the majority of the interviews conducted with experts on the subject, is that it has potential to impact sustainable investments in the future. Future research on the taxonomy may yield results of higher interest since more comprehensive data will be available, and the impact of the taxonomy will be more concrete.</p>
----------------------------------------------------------------------
In diva2:1569839 
abstract is: 
<p>This thesis aims at using novel machine learning techniques to test the dynamics of the Universe via the cosmological redshift-distance test. Currently, one of the most outstanding questions in cosmology is the physical cause of the accelerating cosmic expansion observed with supernovae. Simultaneously, tensions in measurements of the Hubble expansion parameter $H_0$ are emerging. Measuring the Universe expansion with next generation galaxy imaging surveys, such as provided by the Vera Rubin Observatory, offers the opportunity to discover new physics governing the Universe dynamics. In this thesis, with the long-term goal to unravel these peculiarities, we create a deep generative model in the form of a convolutional variational auto-encoder (VAE), trained with a "Variational Mixture of Posteriors" prior (VampPrior) and high-resolution galaxy images from the simulation project \texttt{TNG-50}. Our model is able to learn a prior on the visual features of galaxies and can generate synthetic galaxy images which preserve the coarse features (shape, size, inclination, and surface brightness profile), but not finer morphological features, such as spiral arms. The generative model for galaxy images is applicable to uses outside the scope of this thesis and is thus a contribution in itself. We next implement a cosmological pinhole camera model, taking angular diameter changes with redshift into account, to forward simulate the actual observation on a telescope detector. Building upon the hypothesis that certain features of galaxies should be of proper physical sizes, we use probabilistic triangulation to find the comoving distance $r(z,\Omega)$ to these in a flat ($K=0$) Universe. Using a sample of high-resolution galaxy images from redshifts $z\in[0.05,0.5]$ from \texttt{TNG-50}, we demonstrate that the implemented Bayesian inference approach successfully estimates $r(z)$ within $1\sigma$-error ($\Delta r_{\text{est}} = 140$ $(580)$ Mpc for $z=0.05$ $(0.5)$). Including the surface brightness attenuation and utilizing the avalanche of upcoming galaxy images could significantly lower the uncertainties. This thesis thus shows a promising path forward utilizing novel machine learning techniques and massive next-generation imaging data to improve and generalize the traditional cosmological angular-diameter test, which in turn has the potential to increase our understanding of the Universe.</p>

corrected abstract:
<p>This thesis aims at using novel machine learning techniques to test the dynamics of the Universe via the cosmological redshift-distance test. Currently, one of the most outstanding questions in cosmology is the physical cause of the accelerating cosmic expansion observed with supernovae. Simultaneously, tensions in measurements of the Hubble expansion parameter 𝐻<sub>0</sub> are emerging. Measuring the Universe expansion with next generation galaxy imaging surveys, such as provided by the Vera Rubin Observatory, offers the opportunity to discover new physics governing the Universe dynamics. In this thesis, with the long-term goal to unravel these peculiarities, we create a deep generative model in the form of a convolutional variational auto-encoder (VAE), trained with a "Variational Mixture of Posteriors" prior (VampPrior) and high-resolution galaxy images from the simulation project <tt>TNG-50</tt>. Our model is able to learn a prior on the visual features of galaxies and can generate synthetic galaxy images which preserve the coarse features (shape, size, inclination, and surface brightness profile), but not finer morphological features, such as spiral arms. The generative model for galaxy images is applicable to uses outside the scope of this thesis and is thus a contribution in itself. We next implement a cosmological pinhole camera model, taking angular diameter changes with redshift into account, to forward simulate the actual observation on a telescope detector. Building upon the hypothesis that certain features of galaxies should be of proper physical sizes, we use probabilistic triangulation to find the comoving distance 𝑟(𝑧, &Omega;) to these in a flat (𝐾=0) Universe. Using a sample of high-resolution galaxy images from redshifts 𝑧 &isin; [0.05,0.5] from <tt>TNG-50</tt>, we demonstrate that the implemented Bayesian inference approach successfully estimates 𝑟(𝑧) within 1&sigma;-error (&Delta;𝑟<sub><em>est</em></sub> = 140 (580) Mpc for 𝑧=0.05 (0.5)). Including the surface brightness attenuation and utilizing the avalanche of upcoming galaxy images could significantly lower the uncertainties. This thesis thus shows a promising path forward utilizing novel machine learning techniques and massive next-generation imaging data to improve and generalize the traditional cosmological angular-diameter test, which in turn has the potential to increase our understanding of the Universe.</p>
----------------------------------------------------------------------
In diva2:1568430 
abstract is: 
<p>This thesis deals with the following question: are there more eigenfunctions, other than the already known eigenfunctions, of the spin chain with elliptic interactions known as the Inozemtsev spin chain? The Inozemtsev spin chain interpolates between two quantum integrable spin chains, theHeisenberg spin chain and the Haldane-Shastry spin chain. Therefore it is interesting to explore eigenfunctions of the Inozemtsev spin chain in greater detail. Moreover, there exists connections between spin chains and their corresponding spinless continuum model, namely theCalogero-Sutherland models; a derivation of the connection between the Haldane-Shastry spin chain and the trigonometric interacting Calogero-Sutherland model is presented in this thesis. These connections state that the eigenfunctions of the Calogero-Sutherland model are also eigenfunctionsof the corresponding spin chain. An established connection between the Inozemtsev spin chain and the elliptic interacting Calogero-Sutherland model yields exact eigenfunctions with simple poles at coinciding arguments of the Inozemtsev spin chain. However, there are eigenfunctions of theelliptic Calogero-Sutherland model with second order zeros instead of simple poles at coinciding arguments. It is therefore interesting to see if a connection exists that relates the eigenfunctions of the elliptic Calogero-Sutherland model with second order zeros to eigenfunctionsof the Inozemtsev spin chain also with second order zeros. The main goal of this thesis is to explore eigenfunctions of the Inozemtsev spin chain with second order zeros for two magnons. This thesis uses analytical methods for finding these eigenfunctions and numerical methods have beenresorted to in the end. The numerical results indicate that the functions explored in this thesis fail to parametrise the eigenfunctions of the Inozemtsev spin chain, except for a few special cases.</p>

corrected abstract:
<p>This thesis deals with the following question: are there more eigenfunctions, other than the already known eigenfunctions, of the spin chain with elliptic interactions known as the Inozemtsev spin chain? The Inozemtsev spin chain interpolates between two quantum integrable spin chains, the Heisenberg spin chain and the Haldane-Shastry spin chain. Therefore it is interesting to explore eigenfunctions of the Inozemtsev spin chain in greater detail. Moreover, there exists connections between spin chains and their corresponding spinless continuum model, namely the Calogero-Sutherland models; a derivation of the connection between the Haldane-Shastry spin chain and the trigonometric interacting Calogero-Sutherland model is presented in this thesis. These connections state that the eigenfunctions of the Calogero-Sutherland model are also eigenfunctions of the corresponding spin chain. An established connection between the Inozemtsev spin chain and the elliptic interacting Calogero-Sutherland model yields exact eigenfunctions with simple poles at coinciding arguments of the Inozemtsev spin chain. However, there are eigenfunctions of the elliptic Calogero-Sutherland model with second order zeros instead of simple poles at coinciding arguments. It is therefore interesting to see if a connection exists that relates the eigenfunctions of the elliptic Calogero-Sutherland model with second order zeros to eigenfunctions of the Inozemtsev spin chain also with second order zeros. The main goal of this thesis is to explore eigenfunctions of the Inozemtsev spin chain with second order zeros for two magnons. This thesis uses analytical methods for finding these eigenfunctions and numerical methods have been resorted to in the end. The numerical results indicate that the functions explored in this thesis fail to parametrise the eigenfunctions of the Inozemtsev spin chain, except for a few special cases.</p>
----------------------------------------------------------------------
In diva2:1559774 
abstract is: 
<p>This thesis is part of a project that aims to develop a custom motocompressor for aeronautic cooling applications from an existing compressor that will no longer be manufactured. The main goals are to prevent obsolescence issues and better master all the motocompressor characteristics. In this context, various tasks are to be performed, including the design of the entire compressor froman existing model.</p><p>Firstly, the work is to understand how the existing compressor is built and how it can be redesigned and improved. Then, the work focuses on designing the new motocompressor, and particularly the scroll section. The others main tasks are improving the existing design, in particular concerning the sealing and the balancing system, as well as checking the mechanical resistance. Finally, the 2D technical drawings are realised, a functional dimensioning is performed on all the parts drawings and discussed with the manufacturer in order toensure a proper production quality. The main results of the design process will be discussed and the remaining tasks investigated.</p>

corrected abstract:
<p>This thesis is part of a project that aims to develop a custom motocompressor for aeronautic cooling applications from an existing compressor that will no longer be manufactured. The main goals are to prevent obsolescence issues and better master all the motocompressor characteristics. In this context, various tasks are to be performed, including the design of the entire compressor from an existing model.</p><p>Firstly, the work is to understand how the existing compressor is built and how it can be redesigned and improved. Then, the work focuses on designing the new motocompressor, and particularly the scroll section. The others main tasks are improving the existing design, in particular concerning the sealing and the balancing system, as well as checking the mechanical resistance. Finally, the 2D technical drawings are realised, a functional dimensioning is performed on all the parts drawings and discussed with the manufacturer in order to ensure a proper production quality. The main results of the design process will be discussed and the remaining tasks investigated.</p>
----------------------------------------------------------------------
In diva2:1528142 
abstract is: 
<p>There is a growing need for developing new technologies to drive the global shippinginto a carbon-neutral industry. This Master’s Thesis presents a generic route optimizationmodel for cargo vessels with wind assisted propulsion systems. An earlystage tool to predict environmental savings by finding optimal routes for cargo vesselsequipped with any sailing device.Systematic studies are developed for the optimization parameters in order to observetheir influence on the optimization performance. The route optimization model is usedfor a wind-assisted cargo vessel for route crossing the North Atlantic as an example application.</p>

corrected abstract:
<p>There is a growing need for developing new technologies to drive the global shipping into a carbon-neutral industry. This Master’s Thesis presents a generic route optimization model for cargo vessels with wind assisted propulsion systems. An early stage tool to predict environmental savings by finding optimal routes for cargo vessels equipped with any sailing device. Systematic studies are developed for the optimization parameters in order to observe their influence on the optimization performance. The route optimization model is used for a wind-assisted cargo vessel for route crossing the North Atlantic as an example application.</p>
----------------------------------------------------------------------
In diva2:1528083 
abstract is: 
<p>The work presented in this thesis focuses on the numerical assessment of defects effect on potential boltless design for fuselage panels longitudinal joints, typically identified as High Load Transfer (HLT) configuration.</p><p>Nowadays, the state-of-art joining technique for primary aircraft composite structures,</p><p>such as fuselage barrels, is still mechanical fastening. This conventional approach</p><p>limits the full exploitation of potential benefits, achievable by composite structures, in</p><p>terms of weight and cost reduction. Therefore, alternative boltless joining technologies open new possibilities to further improve the airframe design of future aircraft generations.</p><p>Two major techniques are currently under investigation, namely adhesive bonding and welding, which are employable to join thermosetting- and thermoplasticbased</p><p>laminated composites, respectively. Nevertheless, difficulties in assessing the</p><p>quality of joining line after manufacturing restrict the applicability of boltless joint to</p><p>non load-critical structural components.</p><p>This thesis aims to numerically evaluate the effect of manufacturing-induced defects,</p><p>such as weak bond or disbond, on the overall performances of the structural configuration.</p><p>Longitudinal joints of the fuselage of Airbus A350-XWB aircraft family are used as reference design, since they are currently carried out through single-lap bolted technique. To simulate an HLT joint, a Finite Element (FE) model of a Wide Single Lap Shear (WSLS) specimen, previously adopted as test setup during BOPACS (Boltless assembling Of Primary Aerospace Composite Structures) project, is implemented in the commercial FE software Abaqus. Damage modeling exploits the Cohesive Zone Model (CZM) approach, in which fracture energies drive damage initiation and evolution behavior. The computational loading scenario focuses on quasi-static non-linear analysis.</p><p>To assess the influence of defects on joint load-carrying capability, three different</p><p>classes of joining line are investigated, namely a brittle and a ductile adhesive for adhesive bonding application, and a thermoplastic polymer matrix for welding technology. Globally, all types of boltless joints experience ultimate strength reduction whenever damage occurs in the joining line, but each material exhibits a different decreasing trend depending on its inherent mechanical properties. In addition, two methodological approaches, which exploit only the numerical outcomes of FE quasi-static analyses, are proposed as predictive methods for fatigue response. Fatigue limits for a constant fatigue life are predicted by exploiting the Similarity Principle of stress peaks distribution. Constant Life Diagrams (CLDs) are numerically calculated by following a reverse algorithm based on experimental fatigue data extrapolated from BOPACS test campaign. On the other hand, fatigue initiation loads and fatigue lifetime are predicted by exploiting the Fatigue Crack Growth (FCG) approach and numerical integration of Paris’s law. Based on these investigations, it is finally concluded that predictions of fatigue response can be preliminarily assessed by exploiting numerical outcomes of quasi-static simulations, in conjunction with limited experimental data used as starting points.</p>

corrected abstract:
<p>The work presented in this thesis focuses on the numerical assessment of defects effect on potential boltless design for fuselage panels longitudinal joints, typically identified as High Load Transfer (HLT) configuration.</p><p>Nowadays, the state-of-art joining technique for primary aircraft composite structures, such as fuselage barrels, is still mechanical fastening. This conventional approach limits the full exploitation of potential benefits, achievable by composite structures, in terms of weight and cost reduction. Therefore, alternative boltless joining technologies open new possibilities to further improve the airframe design of future aircraft generations. Two major techniques are currently under investigation, namely adhesive bonding and welding, which are employable to join thermosetting- and thermoplastic based laminated composites, respectively. Nevertheless, difficulties in assessing the quality of joining line after manufacturing restrict the applicability of boltless joint to non load-critical structural components.</p><p>This thesis aims to numerically evaluate the effect of manufacturing-induced defects, such as weak bond or disbond, on the overall performances of the structural configuration. Longitudinal joints of the fuselage of Airbus A350-XWB aircraft family are used as reference design, since they are currently carried out through single-lap bolted technique. To simulate an HLT joint, a Finite Element (FE) model of a Wide Single Lap Shear (WSLS) specimen, previously adopted as test setup during BOPACS (Boltless assembling Of Primary Aerospace Composite Structures) project, is implemented in the commercial FE software Abaqus. Damage modeling exploits the Cohesive Zone Model (CZM) approach, in which fracture energies drive damage initiation and evolution behavior. The computational loading scenario focuses on quasi-static non-linear analysis.</p><p>To assess the influence of defects on joint load-carrying capability, three different classes of joining line are investigated, namely a brittle and a ductile adhesive for adhesive bonding application, and a thermoplastic polymer matrix for welding technology. Globally, all types of boltless joints experience ultimate strength reduction whenever damage occurs in the joining line, but each material exhibits a different decreasing trend depending on its inherent mechanical properties. In addition, two methodological approaches, which exploit only the numerical outcomes of FE quasi-static analyses, are proposed as predictive methods for fatigue response. Fatigue limits for a constant fatigue life are predicted by exploiting the Similarity Principle of stress peaks distribution. Constant Life Diagrams (CLDs) are numerically calculated by following a reverse algorithm based on experimental fatigue data extrapolated from BOPACS test campaign. On the other hand, fatigue initiation loads and fatigue lifetime are predicted by exploiting the Fatigue Crack Growth (FCG) approach and numerical integration of Paris’s law. Based on these investigations, it is finally concluded that predictions of fatigue response can be preliminarily assessed by exploiting numerical outcomes of quasi-static simulations, in conjunction with limited experimental data used as starting points.</p>
----------------------------------------------------------------------
In diva2:1528067 
abstract is: 
<p>This report looks at different ways to optimize a cross beam belonging to a base frame for the company Siemens. Siemens builds and analyses industry sized generators with high efficiency for industrial buyers. These generators can generate power up to 2235 MVA.All calculations have been made on the SGT-800 Split Skid model. The SGT-800 Split Skid base frame there is a crossbeam underneath the turbine. The crossbeam takes stress through a solid plate and pendulum supports on the top flange.The purpose of this report has been to see if the weight of the crossbeam can be optimized without compromising stress results. The load case used is a combination load consisting of gravity and blade loss. Blade loss is a load case where a rotor blade snaps during use.The cross beam was optimized using size, shape and topology optimization methods, and a material change was made. The optimization was done via the program optistruct after receiving a model in abaqus and converting it to optistruct. After the material change and a combination of size and topology optimization, the weight saved is 39 %.</p>

corrected abstract:
<p>This report looks at different ways to optimize a cross beam belonging to a base frame for the company Siemens. Siemens builds and analyses industry sized generators with high efficiency for industrial buyers. These generators can generate power up to 2235 MVA.</p><p>All calculations have been made on the SGT-800 Split Skid model. The SGT-800 Split Skid base frame there is a crossbeam underneath the turbine. The crossbeam takes stress through a solid plate and pendulum supports on the top flange.</p><p>The purpose of this report has been to see if the weight of the crossbeam can be optimized without compromising stress results. The load case used is a combination load consisting of gravity and blade loss. Blade loss is a load case where a rotor blade snaps during use.</p><p>The cross beam was optimized using size, shape and topology optimization methods, and a material change was made. The optimization was done via the program optistruct after receiving a model in abaqus and converting it to optistruct. After the material change and a combination of size and topology optimization, the weight saved is 39 %.</p>
----------------------------------------------------------------------
In diva2:1517478 
abstract is: 
<p>The imminent threat of global catastrophe due to climate change gets more real by each passing year. The Aviation trade association, IATA, claims that Aviation accounts for approximately 2% of the Greenhouse Gases (GHG) caused by human activities, and 3.5% of the total Radiative Forcing. With continuous increase in Aviation industry and subsequent drop in fossil fuel prices, these numbers are only expected to up with time. In Addition, these numbers do not include the effects of altitude of emission and many environmentalists believe that the number for some pollutants could be at least 2-3 times larger than IATA estimates. This rising concern engages the Aviation industry to investigate possible methods to alleviate their environmental impact. </p><p>The first part of this thesis provides a framework to support Airlines in monitoring their current environmental footprint during the process of scheduling. This objective is realised by developing a robust system for estimating the fuel consumed (ergo quantity of major Greenhouse Gases emitted) by a particular fleet type operating a certain leg, which is then employed in a Fleet Assignment (FA) Operation to reduce emissions and increase the Contribution. An emissions estimation model for Turbojet Aeroplane fleets is created for Industrial Optimizers AB’sMP2 software. The emissions estimation model uses historic fuel consumption data provided by ICAO for a given fleet type to estimate the quantity (in kg) of environmental pollutants during the Landing and Takeoff operation (below 3000 ft) and the Cruise, Climb and Descent operation (above 3000 ft). </p><p>The second part of this thesis concerns with assigning monetary weights to the pollutant estimates to calculate an emission cost. This emission cost is then added to MP2’s Fleet Assignment’s objective function as an additional Operational cost to perform a Contribution maximization optimization subjected to the legality constraints. The effects of these monetary weights levied on the results of Fleet Assignment are studied, and utilizing curve-fitting and mathematical optimization, monetary weights are estimated for the desired reduction in GHG emissions. </p><p>Finally, a recursive algorithm based on Newton-Raphson method is designed and tested for calculating pollutant weights for untested schedules.</p>

corrected abstract:
<p>The imminent threat of global catastrophe due to climate change gets more real by each passing year. The Aviation trade association, IATA, claims that Aviation accounts for approximately 2% of the Greenhouse Gases (GHG) caused by human activities, and 3.5% of the total Radiative Forcing. With continuous increase in Aviation industry and subsequent drop in fossil fuel prices, these numbers are only expected to up with time. In Addition, these numbers do not include the effects of altitude of emission and many environmentalists believe that the number for some pollutants could be at least 2-3 times larger than IATA estimates. This rising concern engages the Aviation industry to investigate possible methods to alleviate their environmental impact.</p><p>The first part of this thesis provides a framework to support Airlines in monitoring their current environmental footprint during the process of scheduling. This objective is realised by developing a robust system for estimating the fuel consumed (ergo quantity of major Greenhouse Gases emitted) by a particular fleet type operating a certain leg, which is then employed in a Fleet Assignment (FA) Operation to reduce emissions and increase the Contribution. An emissions estimation model for Turbojet Aeroplane fleets is created for Industrial Optimizers AB’s MP<sup>2</sup> software. The emissions estimation model uses historic fuel consumption data provided by ICAO for a given fleet type to estimate the quantity (in kg) of environmental pollutants during the Landing and Takeoff operation (below 3000 ft) and the Cruise, Climb and Descent operation (above 3000 ft).</p><p>The second part of this thesis concerns with assigning monetary weights to the pollutant estimates to calculate an emission cost. This emission cost is then added to MP<sup>2</sup>’s Fleet Assignment’s objective function as an additional Operational cost to perform a Contribution maximization optimization subjected to the legality constraints. The effects of these monetary weights levied on the results of Fleet Assignment are studied, and utilizing curve-fitting and mathematical optimization, monetary weights are estimated for the desired reduction in GHG emissions.</p><p>Finally, a recursive algorithm based on Newton-Raphson method is designed and tested for calculating pollutant weights for untested schedules.</p>
----------------------------------------------------------------------
In diva2:1516123 
abstract is: 
<p>One of the biggest obstacles to launching autonomous vehicles is the current legislation, which currently does not cover automation level higher than level 2. Work on developing the legal requirements takes place at UN level within WP29 (The UNECE World Forum for Harmonization of Vehicle Regulations). As a world-leading vehicle manufacturer, Scania is aspiring to pave the way for sustainable transport solutions. At Scania, well-established methodologies on certification of different systems exist, although the process of certification of autonomous driving systems needs to be developed.This master thesis investigates the current situation regarding the elaboration of regulations to cover autonomous vehicles, future certification methods related to these systems, and how this applies to Scania. Particular focus is being on the investigation of virtual certification methods. This can form the basis for various departments at Scania in their work with future autonomous systems and how to get these certified.The future certification work for autonomous vehicles will be based on a validation process based on a process called the ‘Multi-pillars approach’ / ‘Three-pillars approach’. The idea is that the autonomous vehicle should be certified based on a process where the basis for certification is made by validating and justifying its systems. This will be done through simulations and other methods to ensure that the systems are satisfactory. A less extensive work should then be done in the testing of the autonomous vehicle on the test track and in traffic, where only less demanding situations must be validated.The functional requirements of the autonomous vehicle will largely control the validation process that is carried out for the ‘Multi-pillars approach’ / ‘Three-pillars approach’. For example, the definition of ODD (Operational Design Domain) is crucial for the validation that the vehicle will undergo at a later stage.</p>

corrected abstract:
<p>One of the biggest obstacles to launching autonomous vehicles is the current legislation, which currently does not cover automation level higher than level 2. Work on developing the legal requirements takes place at UN level within WP29 (The UNECE World Forum for Harmonization of Vehicle Regulations). As a world-leading vehicle manufacturer, Scania is aspiring to pave the way for sustainable transport solutions. At Scania, well-established methodologies on certification of different systems exist, although the process of certification of autonomous driving systems needs to be developed.</p><p>This master thesis investigates the current situation regarding the elaboration of regulations to cover autonomous vehicles, future certification methods related to these systems, and how this applies to Scania. Particular focus is being on the investigation of virtual certification methods. This can form the basis for various departments at Scania in their work with future autonomous systems and how to get these certified.</p><p>The future certification work for autonomous vehicles will be based on a validation process based on a process called the ‘Multi-pillars approach’ / ‘Three-pillars approach’. The idea is that the autonomous vehicle should be certified based on a process where the basis for certification is made by validating and justifying its systems. This will be done through simulations and other methods to ensure that the systems are satisfactory. A less extensive work should then be done in the testing of the autonomous vehicle on the test track and in traffic, where only less demanding situations must be validated.</p><p>The functional requirements of the autonomous vehicle will largely control the validation process that is carried out for the ‘Multi-pillars approach’ / ‘Three-pillars approach’. For example, the definition of ODD (Operational Design Domain) is crucial for the validation that the vehicle will undergo at a later stage.</p>
----------------------------------------------------------------------
In diva2:1514717 
abstract is: 
<p>As intelligent access solutions begin to dominate the world, the statistical learning methods to answer for the behavior of these needs attention, as there is no clear answer to how an algorithm could learn and predict exactly how people move. This project aims at investigating if, with the help of unsupervised learning methods, it is possible to distinguish anomalies from normal events in an access system, and if the most probable choice of cylinder to be unlocked by a user can be calculated.Given to do this is a data set of the previous events in an access system, together with the access configurations - and the algorithms that were used consisted of an auto-encoder and a probabilistic generative model.The auto-encoder managed to, with success, encode the high-dimensional data set into one of significantly lower dimension, and the probabilistic generative model, which was chosen to be a Gaussian mixture model, identified clusters in the data and assigned a measure of unexpectedness to the events.Lastly, the probabilistic generative model was used to compute the conditional probability of which the user, given all the details except which cylinder that was chosen during an event, would choose a certain cylinder. The result of this was a correct guess in 65.7 % of the cases, which can be seen as a satisfactory number for something originating from an unsupervised problem.</p>

corrected abstract:
<p>As intelligent access solutions begin to dominate the world, the statistical learning methods to answer for the behavior of these needs attention, as there is no clear answer to how an algorithm could learn and predict exactly how people move. This project aims at investigating if, with the help of unsupervised learning methods, it is possible to distinguish anomalies from normal events in an access system, and if the most probable choice of cylinder to be unlocked by a user can be calculated.</p><p>Given to do this is a data set of the previous events in an access system, together with the access configurations - and the algorithms that were used consisted of an auto-encoder and a probabilistic generative model.</p><p>The auto-encoder managed to, with success, encode the high-dimensional data set into one of significantly lower dimension, and the probabilistic generative model, which was chosen to be a Gaussian mixture model, identified clusters in the data and assigned a measure of unexpectedness to the events.</p><p>Lastly, the probabilistic generative model was used to compute the conditional probability of which the user, given all the details except which cylinder that was chosen during an event, would choose a certain cylinder. The result of this was a correct guess in 65.7 % of the cases, which can be seen as a satisfactory number for something originating from an unsupervised problem.</p>
----------------------------------------------------------------------
In diva2:1509432 
abstract is: 
<p>Exhaust emissions in a vehicle has to flow through an exhaust aftertreatment in a diesel vehicle. In a diesel engine, the exhaust emissions are treated with Diesel Oxidation Catalyst (DOC), Diesel Particulate Filter (DPF), and Selective Catalytic Reduction (SCR). Every engine produces a different kind of soot depending on the drive cycle. In this thesis, a study was made on the soot oxidation in DPF so as to reduce the net fuel consumption and hence optimising the engine.This project focuses on DPF, where the soot and ash are trapped on the walls of the filter when the emissions flow through the DPF. Over a period of time, the soot accumulates and causes the pressure inside the filter to increase. To reduce the backpressure due to soot accumulation, soot has to be removed from the filter which is done by a regeneration process in which soot is oxidized. To understand the soot oxidation in the DPF, we study the chemical kinetics of the soot.The soot reacts with NO2, O2, and N2 in a Thermogravimetric Analysis (TGA) instrument, in isothermal conditions. Two soot samples, SORT-1 and FORCED 360 were analyzed with TGA, the rate equations were derived from using Arrhenius type kinetics and the data was processed by MATLAB. The rate at which the soot is oxidized by NO2 and O2 for SORT-1 is higher than for FORCED 360. This trend is observed similarly when both the soot samples react with only O2. When soot oxidation reaction takes place with O2 and NO2 they require a lower temperature of 250 °C-400 °C than compared to samples reacting with only O2 with a temperature of 350 °C - 500 °C. To understand the conditions that affect soot oxidation, the concentration of oxygen was varied and it was found that at higher oxygen concentration the soot oxidized is almost constant. Then soot kinetics were analysed by finding the rate of the reaction, the order of the reaction, and finally the activation energy. The order of the reaction for FORCED 360 and SORT-1 vary and slope of the graph, logarithm of reaction constant vs logarithm of mass shows a non-linearity in the former due to the slower rate of the reaction in SORT-1 than in FORCED 360. The activation energy was found to be 39.3 kJ/mol for SORT-1 and FORCED 360 is 60.8 kJ/mol.</p>

corrected abstract:
<p>Exhaust emissions in a vehicle has to flow through an exhaust aftertreatment in a diesel vehicle. In a diesel engine, the exhaust emissions are treated with Diesel Oxidation Catalyst (DOC), Diesel Particulate Filter (DPF), and Selective Catalytic Reduction (SCR). Every engine produces a different kind of soot depending on the drive cycle. In this thesis, a study was made on the soot oxidation in DPF so as to reduce the net fuel consumption and hence optimising the engine.</p><p>This project focuses on DPF, where the soot and ash are trapped on the walls of the filter when the emissions flow through the DPF. Over a period of time, the soot accumulates and causes the pressure inside the filter to increase. To reduce the backpressure due to soot accumulation, soot has to be removed from the filter which is done by a regeneration process in which soot is oxidized. To understand the soot oxidation in the DPF, we study the chemical kinetics of the soot.</p><p>The soot reacts with NO<sub>2</sub>, O<sub>2</sub>, and N<sub>2</sub> in a Thermogravimetric Analysis (TGA) instrument, in isothermal conditions. Two soot samples, SORT-1 and FORCED 360 were analyzed with TGA, the rate equations were derived from using Arrhenius type kinetics and the data was processed by MATLAB. The rate at which the soot is oxidized by NO<sub>2</sub> and O<sub>2</sub> for SORT-1 is higher than for FORCED 360. This trend is observed similarly when both the soot samples react with only O<sub>2</sub>. When soot oxidation reaction takes place with O<sub>2</sub> and NO<sub>2</sub> they require a lower temperature of 250 °C-400 °C than compared to samples reacting with only O<sub>2</sub> with a temperature of 350 °C - 500 °C. To understand the conditions that affect soot oxidation, the concentration of oxygen was varied and it was found that at higher oxygen concentration the soot oxidized is almost constant. Then soot kinetics were analysed by finding the rate of the reaction, the order of the reaction, and finally the activation energy. The order of the reaction for FORCED 360 and SORT-1 vary and slope of the graph, logarithm of reaction constant vs logarithm of mass shows a non-linearity in the former due to the slower rate of the reaction in SORT-1 than in FORCED 360. The activation energy was found to be 39.3 kJ/mol for SORT-1 and FORCED 360 is 60.8 kJ/mol.</p>
----------------------------------------------------------------------
In diva2:1464140 
abstract is: 
<p>To be able to ensure range safety and quickly recover the rocket and any on-board experiment, it is essential to be able to track the rocket. For sounding rocket launches at Esrange, this is typically done with radar or rocket telemetry, it has, however, recentlybecome of interest to do so with an optical system. This thesis develops the necessary methods for an optical system used to track and reconstruct the trajectory of a sounding rocket. It further discusses the limitations of such a system, given the hardware andinfrastructure available. Finally, some of the methods are tested with previous rocket footage showing that the suggested method works well for the available test cases. The full method uses foreground detection to detect the rocket in the image and using thisdata from multiple cameras combine to triangulate the 3D-coordinates of the rocket. A simple calibration method for the cameras is proposed, and future work to improve the capabilities of the system are proposed. The results show reliable tracking of the rocket,but raises concern in the viability of using current hardware for tracking the rocket during daytime due to difficulties in the visibility of the rocket in these conditions. The developed methods are foremost suggested to be used in nighttime conditions.</p>

corrected abstract:
<p>To be able to ensure range safety and quickly recover the rocket and any on-board experiment, it is essential to be able to track the rocket. For sounding rocket launches at Esrange, this is typically done with radar or rocket telemetry, it has, however, recently become of interest to do so with an optical system. This thesis develops the necessary methods for an optical system used to track and reconstruct the trajectory of a sounding rocket. It further discusses the limitations of such a system, given the hardware and infrastructure available. Finally, some of the methods are tested with previous rocket footage showing that the suggested method works well for the available test cases. The full method uses foreground detection to detect the rocket in the image and using this data from multiple cameras combine to triangulate the 3D-coordinates of the rocket. A simple calibration method for the cameras is proposed, and future work to improve the capabilities of the system are proposed. The results show reliable tracking of the rocket, but raises concern in the viability of using current hardware for tracking the rocket during daytime due to difficulties in the visibility of the rocket in these conditions. The developed methods are foremost suggested to be used in nighttime conditions.</p>
----------------------------------------------------------------------
In diva2:1440101 
abstract is: 
<p>Many space engineering and orbital mechanics applications seek for the usage of focused mathematical models, capable of providing useful insight onto particular phenomena or exploiting some theoretical and physical tools to reduce the computational costs and/or increase the level of accuracy reached. Orbital resonances are one of the phenomena that needs to be properly modelled, both for exploiting such features in the mission design phase and to predict possible resonant returns of threatening objects closely approaching a specified planet.</p><p>This work deals indeed with one of the possible models of orbital resonances, representing such a physical phenomenon in the B-plane reference frame with an analysis on the resonant trajectories performed at the moment of close encounter. Before this, flybys are an important source of uncertainty in the numerical simulations, which then need to be as accurate as possible to be used as benchmark. To this extent, a highly efficient method to account for general relativity effects in the N-body propagation is developed, tested and validated, to be then used as precise benchmark for the resonance analysis and application.</p><p>The B-plane resonance model is a strictly patched conics theory which does not account for perturbations. A semi-analytical extension of the current B-plane resonance model is proposed to account for perturbing effects inside the planet’s sphere of influence. Introducing a set of perturbing coefficients brings the model to match the simulation results at the B-plane point where such coefficients are computed, as well as to be a highly reliable approximation in its vicinity, performing a validation with Monte-Carlo simulated data.</p><p>An extension of the validation proposed would lead to a complete planetary protection or defence application, whereas in its final part the work will show the flexibility of the model by looking at it from a different perspective. A ballistic resonant flyby design application will be implemented by solving a multi-level optimisation problem, to modify an initial trajectory into a new one on the same Tisserand level.</p><p>Without dealing with the specific case of resonances, the B-plane reference frame embeds a smart geometrical framework where to express and design flyby deflections, whose power will be shown in terms of accuracy achieved and computational cost required.</p><p>Once completed by detaching from the patched conics approximation, such a model could bring remarkable simplifications in planetary protection applications, reducing the need of propagating a high number of Monte Carlo samples, and would increase the precision of the defence analyses against impacts from near-Earth threatening asteroids. About the application proposed here, internal and/or external integration could eventually lead to an enhanced efficiency of the current mission design strategies and could widen the internal proposed capabilities, providing high precision and almost optimal results with lowered computational costs.</p>

corrected abstract:
<p>Many space engineering and orbital mechanics applications seek for the usage of focused mathematical models, capable of providing useful insight into particular phenomena or exploiting some theoretical and physical tools to reduce the computational costs and/or increase the level of accuracy reached. Orbital resonances are one of the phenomena that needs to be properly modelled, both for exploiting such features in the mission design phase and to predict possible resonant returns of threatening objects closely approaching a specified planet.</p><p>This work deals indeed with one of the possible models of orbital resonances, representing such a physical phenomenon in the B-plane reference frame with an analysis on the resonant trajectories performed at the moment of close encounter. Before this, flybys are an important source of uncertainty in the numerical simulations, which then need to be as accurate as possible to be used as benchmark. To this extent, a highly efficient method to account for general relativity effects in the N-body propagation is developed, tested and validated, to be then used as precise benchmark for the resonance analysis and application.</p><p>The B-plane resonance model is a strictly patched conics theory which does not account for perturbations. A semi-analytical extension of the current B-plane resonance model is proposed to account for perturbing effects inside the planet’s sphere of influence. Introducing a set of perturbing coefficients brings the model to match the simulation results at the B-plane point where such coefficients are computed, as well as to be a highly reliable approximation in its vicinity, performing a validation with Monte-Carlo simulated data.</p><p>An extension of the validation proposed would lead to a complete planetary protection or defence application, whereas in its final part the work will show the flexibility of the model by looking at it from a different perspective. A ballistic resonant flyby design application will be implemented by solving a multi-level optimisation problem, to modify an initial trajectory into a new one on the same Tisserand level. Without dealing with the specific case of resonances, the B-plane reference frame embeds a smart geometrical framework where to express and design flyby deflections, whose power will be shown in terms of accuracy achieved and computational cost required.</p><p>Once completed by detaching from the patched conics approximation, such a model could bring remarkable simplifications in planetary protection applications, reducing the need of propagating a high number of Monte Carlo samples, and would increase the precision of the defence analyses against impacts from near-Earth threatening asteroids. About the application proposed here, internal and/or external integration could eventually lead to an enhanced efficiency of the current mission design strategies and could widen the internal proposed capabilities, providing high precision and almost optimal results with lowered computational costs.</p>
----------------------------------------------------------------------
In diva2:1431627 
abstract is: 
<p>This study evaluates noise robustness of convolutional autoencoders and neural networks for classification of Low Probability of Intercept (LPI) radar modulation type. Specifically, a number of different neural network architectures are tested in four different synthetic noise environments.</p><p>Tests in Gaussian noise show that performance is decreasing with decreasing Signal to Noise Ratio (SNR). Training a network on all SNRs in the dataset achieved a peak performance of 70.8 % at SNR=-6 dB with a denoising autoencoder and convolutional classifier setup. Tests indicate that the models have a difficult time generalizing to SNRs lower than what is provided in training data, performing roughly 10-20% worse than when those SNRs are included in the training data. If intermediate SNRs are removed from the training data the models can generalize and perform similarly to tests where, intermediate noise levels are included in the training data. When testing data is generated with different parameters to training data performance is underwhelming, with a peak performance of 22.0 % at SNR=-6 dB. The last tests done use telecom signals as additive noise instead of Gaussian noise. These tests are performed when the LPI and telecom signals appear at different frequencies. The models preform well on such cases with a peak performance of 80.3 % at an intermidiate noise level.</p><p>This study also contribute with a different, and more realistic, way of generating data than what is prevalent in literature as well as a network that performs well without the need for signal preprocessing. Without preprocessing a peak performance of 64.9 % was achieved at SNR=-6 dB. It is customary to generate data such that each sample always includes the start of its signals period which increases performance by around 20 % across all tests. In a real application however it is not certain that the start of a received signal can be determined.</p>

corrected abstract:
<p>This study evaluates noise robustness of convolutional autoencoders and neural networks for classification of Low Probability of Intercept (LPI) radar modulation type. Specifically, a number of different neural network architectures are tested in four different synthetic noise environments.</p><p>Tests in Gaussian noise show that performance is decreasing with decreasing Signal to Noise Ratio (SNR). Training a network on all SNRs in the dataset achieved a peak performance of 70.8% at SNR = -6 dB with a denoising autoencoder and convolutional classifier setup. Tests indicate that the models have a difficult time generalizing to SNRs lower than what is provided in training data, performing roughly 10 - 20% worse than when those SNRs are included in the training data. If intermediate SNRs are removed from the training data the models can generalize and perform similarly to tests where, intermediate noise levels are included in the training data. When testing data is generated with different parameters to training data performance is underwhelming, with a peak performance of 22.0% at SNR = -6 dB. The last tests done use telecom signals as additive noise instead of Gaussian noise. These tests are performed when the LPI and telecom signals appear at different frequencies. The models preform well on such cases with a peak performance of 80.3% at an intermidiate noise level.</p><p>This study also contribute with a different, and more realistic, way of generating data than what is prevalent in literature as well as a network that performs well without the need for signal preprocessing. Without preprocessing a peak performance of 64.9% was achieved at SNR = -6 dB. It is customary to generate data such that each sample always includes the start of its signals period which increases performance by around 20% across all tests. In a real application however it is not certain that the start of a received signal can be determined.</p>
----------------------------------------------------------------------
In diva2:1431620 
abstract is: 
<p>Since the 2008 financial crisis, the interest for the subject area of modelling non-maturity deposits has been growing quickly. The area has been widely analysed from the perspective of a traditional bank where customers foremost have transactional and salary deposits. However, in recent year the Swedish banking sector has become more digitized. This has opened up opportunities for more niche banking actors to establish themselves on the market. Therefore, this study aims to examine how the theories developed and previously used in modelling liquidity volumes at traditional banks can be used at a niche bank focused on savings and investments. In this study the topics covered are short-rate modelling using Vasicek's model, liquidity volume modelling using SARIMA and SARIMAX modelling as well as liquidity risk modelling using an approach developed by Kalkbrener and Willing. When modelling the liquidity volumes the data set was divided depending on account and customer type into six groups, for four out of these the models had lower in and out of set prediction errors using SARIMA models for only two of the six models were there improvements made to the in and out of set prediction error using SARIMAX models. Finally, the resulting minimization of liquidity volume forecasting 5 years in the future gave reasonable and satisfactory results.</p>

corrected abstract:
<p>Since the 2008 financial crisis the interest for the subject area of modelling non-maturity deposits has been growing quickly. The area has been widely analysed from the perspective of a traditional bank where customers foremost have transactional and salary deposits. However, in recent year the Swedish banking sector has become more digitized. This has opened up opportunities for more niche banking actors to establish themselves on the market.</p><p>Therefore, this study aims to examine how the theories developed and previously used in modelling liquidity volumes at traditional banks can be used at a niche bank focused on savings and investments. In this study the topics covered are short-rate modeling using Vasicek's model, liquidity volume modelling using SARIMA and SARIMAX modelling as well as liquidity risk modelling using an approach developed by Kalkbrener and Willing. When modelling the liquidity volumes the data set was divided depending on account and customer type into six groups, for four out of these the models had lower in and out of set prediction errors using SARIMA models for only two of the six model was there improvements made to the in and out of set prediction error using SARIMAX models.</p><p>Finally, the resulting minimization of liquidity volume forecasting 5 years in the future gave reasonable and satisfactory results.</p>

Note that someone had corrected the grammar from the original - the above matches the abstract in the thesis.
----------------------------------------------------------------------
In diva2:1423684 
abstract is: 
<p>In order to better characterise pressure-gradient turbulent boundary layers (TBLs), and to advance the theory of these flows, typically canonical conditions are required. This is achieved by keeping the Clauser pressure-gradient parameter, <em>β</em>, constant along the streamwise direction, see for instance Mellor and Gibson [9] and Bobke et al. [1] It is, however, very challenging to obtain the correct boundary conditions when performing numerical simulations or wind-tunnel experiments so that a give mean pressure gradient is kept constant along the edge of a turbulent boundary layer.</p><p>This study tackles this issue through numerically optimising the shape of the upper boundary of a 2D channel flow to obtain a target constant-<em>β</em> distribution at the TBL over the bottom wall. The shape of the upper boundary of the channel is parameterised and the parameters are optimised through Bayesian optimisation based on Gaussian process regressiong (GPR). To simulate the turbulent flow within the channel at a low computational cost, steady RANS (Reynolds-Averaged Navier-Stokes) simulations are conducted, which show a good agreement with DNS (Direct Numerical Simulation) and LES (Large-Eddy Simulation) benchmark data. The simulations are performed using the open-source finite-volume-based software OpenFoam.</p><p>Through the optimisation process, constant-<em>β</em> distributions are achieved after few iterations for several target values of <em>β, </em>corrensponding to ZPG (zero-pressure-gradient) and APG (adverse-pressure-gradient) TBLs. The results of constant-<em>β</em> distributions show very good agreement with reference data. The impact of parameterisation of the upper wall on the performance of the optimisation and the accuracy of the results is also studied. This study can be a very powerful tool to set constant-<em>β </em>distributions in experiments.</p>

corrected abstract:
<p>In order to better characterise pressure-gradient turbulent boundary layers (TBLs), and to advance the theory of these flows, typically canonical conditions are required. This is achieved by keeping the Clauser pressure-gradient parameter, <em>β</em>, constant along the streamwise direction, see for instance Mellor and Gibson [9] and Bobke et al. [1] It is, however, very challenging to obtain the correct boundary conditions when performing numerical simulations or wind-tunnel experiments so that a given mean pressure gradient is kept constant along the edge of a turbulent boundary layer.</p><p>This study tackles this issue through numerically optimising the shape of the upper boundary of a 2D channel flow to obtain a target constant-<em>β</em> distribution at the TBL over the bottom wall. The shape of the upper boundary of the channel is parameterised and the parameters are optimised through Bayesian optimisation based on Gaussian process regression (GPR). To simulate the turbulent flow within the channel at a low computational cost, steady RANS (Reynolds-Averaged Navier-Stokes) simulations are conducted, which show a good agreement with DNS (Direct Numerical Simulation) and LES (Large-Eddy Simulation) benchmark data. The simulations are performed using the open-source finite-volume-based software OpenFoam.</p><p>Through the optimisation process, constant-<em>β</em> distributions are achieved after few iterations for several target values of <em>β</em>, corresponding to ZPG (zero-pressure-gradient) and APG (adverse-pressure-gradient) TBLs. The results of constant-<em>β</em> distributions show very good agreement with reference data. The impact of parameterisation of the upper wall on the performance of the optimisation and the accuracy of the results is also studied. This study can be a very powerful tool to set constant-<em>β </em> distributions in experiments.</p>
----------------------------------------------------------------------
In diva2:1388640 
abstract is: 
<p>Most diseases have different heterogeneous effects on patients. Broadly, one may conclude what manifested symptoms correspond to which diagnosis, but usually there is more than one disease progression pattern. Because there is more than one pattern, and because each pattern may require a bespoke (and personalised) therapeutic intervention, time-series clustering is one option by which disease subpopulations can be identified. Such patient sub-typing is difficult due to information heterogeneity, information sparsity (few longitudinal observations) and complex temporal governing disease dynamics. To deal with these problems, and seeking to gain a robust description of them, we introduce a generative clustering model by way of a mixture of hidden Markov models. Our model deals with non-ergodic temporal dynamics, has variable state cardinality for the mixtures components and initialises the mixture in a more structured way than current methods. With the task of disease progression modelling in mind, we also take a broader perspective on parameter learning in finite mixture models (FFM). In many mixture models, obtaining optimal or near-optimal parameters is difficult with current learning methods, where the most common approach is to employ monotone learning algorithms e.g. the conventional expectation-maximisation algorithm. While effective, the success of any monotone algorithm is crucially dependant on good parameter initialisation. A common approach is to repeat the learning procedure multiple times starting from different points in the parameter space or to employ model specific initialisation schemes e.g. K-means initialisation for Gaussian mixture models. For other types of mixture models the path to good initialisation parameters is often unclear and may require a solution specific not only model, but also the data. To this end, we propose a general heuristic learning algorithm that utilises Boltzmann exploration to assign each observation to a specific base distribution within the mixture model, which we call Boltzmann exploration expectationmaximisation (BEEM). With BEEM, hard assignments allow straight forward parameter learning for each base distribution by conditioning only on its assigned observations. Consequently it can be applied to mixtures of any base distribution where single component parameter learning is tractable. The stochastic learning procedure is able to escape local optima and explores the parameter space, thus mitigates sensitivity to parameter initialisation. We show competitive performance on a number of synthetic benchmark cases as well as on real-world datasets. Finally we employ BEEM for the disease progression sub-typing task and contrast it to a task specific initialisation procedure on synthetic data as well as on a real progression modelling task, where we identify clinical phenotypes in Parkinson’s disease</p>

corrected abstract:
<p>Most diseases have different heterogeneous effects on patients. Broadly, one may conclude what manifested symptoms correspond to which diagnosis, but usually there is more than one disease progression pattern. Because there is more than one pattern, and because each pattern may require a bespoke (and personalised) therapeutic intervention, time-series clustering is one option by which disease subpopulations can be identified. Such patient sub-typing is difficult due to information heterogeneity, information sparsity (few longitudinal observations) and complex temporal governing disease dynamics. To deal with these problems, and seeking to gain a robust description of them, we introduce a generative clustering model by way of a mixture of hidden Markov models. Our model deals with non-ergodic temporal dynamics, has variable state cardinality for the mixtures components and initialises the mixture in a more structured way than current methods.</p><p>With the task of disease progression modelling in mind, we also take a broader perspective on parameter learning in finite mixture models (FFM). In many mixture models, obtaining optimal or near-optimal parameters is difficult with current learning methods, where the most common approach is to employ monotone learning algorithms e.g. the conventional expectation-maximisation algorithm. While effective, the success of any monotone algorithm is crucially dependant on good parameter initialisation. A common approach is to repeat the learning procedure multiple times starting from different points in the parameter space or to employ model specific initialisation schemes e.g. 𝐾-means initialisation for Gaussian mixture models. For other types of mixture models the path to good initialisation parameters is often unclear and may require a solution specific not only model, but also the data.</p><p>To this end, we propose a general heuristic learning algorithm that utilises Boltzmann exploration to assign each observation to a specific base distribution within the mixture model, which we call Boltzmann exploration expectation maximisation (BEEM). With BEEM, hard assignments allow straight forward parameter learning for each base distribution by conditioning only on its assigned observations. Consequently it can be applied to mixtures of any base distribution where single component parameter learning is tractable. The stochastic learning procedure is able to escape local optima and explores the parameter space, thus mitigates sensitivity to parameter initialisation. We show competitive performance on a number of synthetic benchmark cases as well as on real-world datasets. Finally we employ BEEM for the disease progression sub-typing task and contrast it to a task specific initialisation procedure on synthetic data as well as on a real progression modelling task, where we identify clinical phenotypes in Parkinson’s disease.</p>
----------------------------------------------------------------------
In diva2:1359771 - missing space in title:
"Nonlinear Attitude Control ofa Generic Aircraft"
==>
"Nonlinear Attitude Control of a Generic Aircraft"

abstract is: 
<p>Determining suitable controllers for the process of evaluating dynamic per-formance of multiple versions of an aircraft’s aerodynamical, geometric and propulsive properties in its conceptual stage is an expensive task.In this report a proposition is made to utilize a generalized feedback lin-earizing controller that o˙ers the aircraft designer valuable insight into the manoeuvre performance of their aircraft. This is carried out by first estab-lishing fundamental requirements for a controller capable of treating a generic airframe, and formulating the resulting control laws.It is shown in this report, that with a suÿciently simple aerodynamic and propulsive model explicit feedback linearization is possible with satisfactory performance and robustness. Whereas it would be necessary to implement INDI if explicit inverse mappings are not obtainable. Which in turn would introduce additional tuning parameters.Robustness verification is performed in two stages, firstly by introducing a high model uncertainty within the flight control system and showing, via simulation, that the control system successfully performs desired multi-axial manoeuvres whilst managing to maintain the induced side slip below 0.1◦. Secondly by disturbing the aircraft with a discrete side slip. Critical side slip disturbance angle was found to be considerably larger than that for regular aircraft entailing that the used case study may be somewhat over dimensioned with respect to yaw control authority.</p>

corrected abstract:
<p>Determining suitable controllers for the process of evaluating dynamic performance of multiple versions of an aircraft’s aerodynamical, geometric and propulsive properties in its conceptual stage is an expensive task.</p><p>In this report a proposition is made to utilize a generalized feedback linearizing controller that offers the aircraft designer valuable insight into the manoeuvre performance of their aircraft. This is carried out by first establishing fundamental requirements for a controller capable of treating a generic airframe, and formulating the resulting control laws.</p><p>It is shown in this report, that with a sufficiently simple aerodynamic and propulsive model explicit feedback linearization is possible with satisfactory performance and robustness. Whereas it would be necessary to implement INDI if explicit inverse mappings are not obtainable. Which in turn would introduce additional tuning parameters.</p><p>Robustness verification is performed in two stages, firstly by introducing a high model uncertainty within the flight control system and showing, via simulation, that the control system successfully performs desired multi-axial manoeuvres whilst managing to maintain the induced side slip below 0.1º. Secondly by disturbing the aircraft with a discrete side slip. Critical side slip disturbance angle was found to be considerably larger than that for regular aircraft entailing that the used case study may be somewhat over dimensioned with respect to yaw control authority.</p>
----------------------------------------------------------------------
In diva2:1350190 
abstract is: 
<p>The objective of this thesis has been to investigate the effect a liquid shim has on the bearing strength of a composite bolted joint. The shim is necessary to close gaps that occur during the assembly of the joints, preventing the structural parts from being fastened correctly. The shim however increases the load eccentricity of the joint and will have a negative effect on the joint strength, but the significance of this weakening is not well understood.This thesis primarily focuses on a parametric finite element study on the effect the liquid shim has on the bearing of both a homogenised carbon fibre/epoxy model and a fully detailed laminate model based upon the same material. Parameters studied were the plate and shim thicknesses, lateral support, number of fasteners, bolt pre-tension and bolt diameter and the relative strength decreases were documented.A literature study was also conducted to consider previous results concering the strength change due to the inclusion of a shim. It was found that the results show a large spread dependent on material system, geometry and assumptions regarding numerical behaviour. The finite element simulation was compared with the results from these studies, showing fairly good agreement.The analysis conducted has shown that there is a major increase in stress and strain on the bearing surface because of the shim. Moreover, it is shown that this strength reduction is dependent on both model and parameters studied, which necessitates experimental testing in order to verify which is the more applicable for future methodology.</p>

corrected abstract:
<p>The objective of this thesis has been to investigate the effect a liquid shim has on the bearing strength of a composite bolted joint. The shim is necessary to close gaps that occur during the assembly of the joints, preventing the structural parts from being fastened correctly. The shim however increases the load eccentricity of the joint and will have a negative effect on the joint strength, but the significance of this weakening is not well understood.</p><p>This thesis primarily focuses on a parametric finite element study on the effect the liquid shim has on the bearing of both a homogenised carbon fibre/epoxy model and a fully detailed laminate model based upon the same material. Parameters studied were the plate and shim thicknesses, lateral support, number of fasteners, bolt pre-tension and bolt diameter and the relative strength decreases were documented.</p><p>A literature study was also conducted to consider previous results concering the strength change due to the inclusion of a shim. It was found that the results show a large spread dependent on material system, geometry and assumptions regarding numerical behaviour. The finite element simulation was compared with the results from these studies, showing fairly good agreement.</p><p>The analysis conducted has shown that there is a major increase in stress and strain on the bearing surface because of the shim. Moreover, it is shown that this strength reduction is dependent on both model and parameters studied, which necessitates experimental testing in order to verify which is the more applicable for future methodology.</p>
----------------------------------------------------------------------
In diva2:1348582 
abstract is: 
<p>In radiation therapy treatment planning, recent works have used machine learning based on historically delivered plans to automate the process of producing clinically acceptable plans. Compared to traditional approaches such as repeated weighted-sum optimization or multicriteria optimization (MCO), automated planning methods have, in general, the benefits of low computational times and minimal user interaction, but on the other hand lack the flexibility associated with general-purpose frameworks such as MCO. Machine learning approaches can be especially sensitive to deviations in their dose prediction due to certain properties of the optimization functions usually used for dose mimicking and, moreover, suffer from the fact that there exists no general causality between prediction accuracy and optimized plan quality.In this thesis, we present a means of unifying ideas from machine learning planning methods with the well-established MCO framework. More precisely, given prior knowledge in the form of either a previously optimized plan or a set of historically delivered clinical plans, we are able to automatically generate Pareto optimal plans spanning a dose region corresponding to plans which are achievable as well as clinically acceptable. For the former case, this is achieved by introducing dose--volume constraints; for the latter case, this is achieved by fitting a weighted-data Gaussian mixture model on pre-defined dose statistics using the expectation--maximization algorithm, modifying it with exponential tilting and using specially developed optimization functions to take into account prediction uncertainties.Numerical results for conceptual demonstration are obtained for a prostate cancer case with treatment delivered by a volumetric-modulated arc therapy technique, where it is shown that the methods developed in the thesis are successful in automatically generating Pareto optimal plans of satisfactory quality and diversity, while excluding clinically irrelevant dose regions. For the case of using historical plans as prior knowledge, the computational times are significantly shorter than those typical of conventional MCO.</p>

corrected abstract:
<p>In radiation therapy treatment planning, recent works have used machine learning based on historically delivered plans to automate the process of producing clinically acceptable plans. Compared to traditional approaches such as repeated weighted-sum optimization or multicriteria optimization (MCO), automated planning methods have, in general, the benefits of low computational times and minimal user interaction, but on the other hand lack the flexibility associated with general-purpose frameworks such as MCO. Machine learning approaches can be especially sensitive to deviations in their dose prediction due to certain properties of the optimization functions usually used for dose mimicking and, moreover, suffer from the fact that there exists no general causality between prediction accuracy and optimized plan quality.</p><p>In this thesis, we present a means of unifying ideas from machine learning planning methods with the well-established MCO framework. More precisely, given prior knowledge in the form of either a previously optimized plan or a set of historically delivered clinical plans, we are able to automatically generate Pareto optimal plans spanning a dose region corresponding to plans which are achievable as well as clinically acceptable. For the former case, this is achieved by introducing dose-volume constraints; for the latter case, this is achieved by fitting a weighted-data Gaussian mixture model on pre-defined dose statistics using the expectation-maximization algorithm, modifying it with exponential tilting and using specially developed optimization functions to take into account prediction uncertainties.</p><p>Numerical results for conceptual demonstration are obtained for a prostate cancer case with treatment delivered by a volumetric-modulated arc therapy technique, where it is shown that the methods developed in the thesis are successful in automatically generating Pareto optimal plans of satisfactory quality and diversity, while excluding clinically irrelevant dose regions. For the case of using historical plans as prior knowledge, the computational times are significantly shorter than those typical of conventional MCO.</p>
----------------------------------------------------------------------
In diva2:1348300 
abstract is: 
<p>In this thesis will the question of how to construct implied volatility surfaces in a robust and arbitrage free way be investigated. To be able to know if the solutions are arbitrage free was an initial investigation about arbitrage in volatility surfaces made. From this investigation where two comprehensive theorems found. These theorems came from Roper in \cite{Roper2010}. Based on these where then two applicable arbitrage tests created. These tests came to be very important tools in the remaining thesis.The most reasonable classes of models for modeling the implied volatility surface where then investigated. It was concluded that the classes that seemed to have the best potential where the stochastic volatility models and the parametric representation models. The choice between these two classes where concluded to be based on a trade-off between simplicity and quality of the result. If it where possible to make the parametric representation models improve its result the best applicable choice would be that class. For the remaining thesis was it therefore decided to investigate this class. The parametric representation model that was chosen to be investigated where the SVI parametrization family since it seemed to have the most potential outside of its already strong foundation.The SVI parametrization family is diveded into 3 parametrizations, the raw SVI parametrization, the SSVI parametrization and the eSSVI parametrization. It was concluded that the raw SVI parametrization even though it gives very good market fits, was not robust enough to be chosen. This ment that the raw SVI parametrization would in most cases generate arbitrage in its surfaces. The SSVI model was concluded to be a very strong model compared to the raw SVI, since it was able to generate completely arbitrage free solutions with good enough results. The eSSVI is an extended parametrization of the SSVI with purpose to improve its short maturity results. It was concluded to give small improvements but with the trade of making the optimization procedure harder. It was therefore concluded that the SSVI parametrization might be the better application.To try to improve the results of the SSVI parametrization was a complementary procedure developed which got named the calibrated SSVI method. This method compared to the eSSVI parametrization would not change the parametrization but instead focusing on calibrating the initial fit that the SSVI generated. This method would heavily improve the initial fit of the SSVI surface but was less robust since it generated harder cases for the interpolation and extrapolation.</p>

corrected abstract:
<p>In this thesis will the question of how to construct implied volatility surfaces in a robust and arbitrage free way be investigated.</p><p>To be able to know if the solutions are arbitrage free was an initial investigation about arbitrage in volatility surfaces made. From this investigation where two comprehensive theorems found. These theorems came from Roper in [14]. Based on these where then two applicable arbitrage tests created. These tests came to be very important tools in the remaining thesis. The most reasonable classes of models for modeling the implied volatility surface where then investigated. It was concluded that the classes that seemed to have the best potential where the stochastic volatility models and the parametric representation models. The choice between these two classes where concluded to be based on a trade-off between simplicity and quality of the result. If it where possible to make the parametric representation models improve its result the best applicable choice would be that class. For the remaining thesis was it therefore decided to investigate this class.</p><p>The parametric representation model that was chosen to be investigated where the SVI parametrization family since it seemed to have the most potential outside of its already strong foundation.</p><p>The SVI parametrization family is diveded into 3 parametrizations, the raw SVI parametrization, the SSVI parametrization and the eSSVI parametrization.</p><p>It was concluded that the raw SVI parametrization even though it gives very good market fits, was not robust enough to be chosen. This ment that the raw SVI parametrization would in most cases generate arbitrage in its surfaces.</p><p>The SSVI model was concluded to be a very strong model compared to the raw SVI, since it was able to generate completely arbitrage free solutions with good enough results.</p><p>The eSSVI is an extended parametrization of the SSVI with purpose to improve its short maturity results. It was concluded to give small improvements but with the trade of making the optimization procedure harder. It was therefore concluded that the SSVI parametrization might be the better application.</p><p>To try to improve the results of the SSVI parametrization was a complementary procedure developed which got named the calibrated SSVI method. This method compared to the eSSVI parametrization would not change the parametrization but instead focusing on calibrating the initial fit that the SSVI generated. This method would heavily improve the initial fit of the SSVI surface but was less robust since it generated harder cases for the interpolation and extrapolation.</p>
----------------------------------------------------------------------
In diva2:1333977 - correct as is
----------------------------------------------------------------------
In diva2:1324110  - correct as is
----------------------------------------------------------------------
In diva2:1319851 
abstract is: 
<p>In this thesis, cluster analysis was applied to data comprising of customer spending habits at a retail chain in order to perform customer segmentation. The method used was a two-step cluster procedure in which the first step consisted of feature engineering, a square root transformation of the data in order to handle big spenders in the data set and finally principal component analysis in order to reduce the dimensionality of the data set. This was done to reduce the effects of high dimensionality. The second step consisted of applying clustering algorithms to the transformed data. The methods used were K-means clustering, Gaussian mixture models in the MCLUST family, t-distributed mixture models in the tEIGEN family and non-negative matrix factorization (NMF). For the NMF clustering a slightly different data pre-processing step was taken, specifically no PCA was performed. Clustering partitions were compared on the basis of the Silhouette index, Davies-Bouldin index and subject matter knowledge, which revealed that K-means clustering with K = 3 produces the most reasonable clusters. This algorithm was able to separate the customer into different segments depending on how many purchases they made overall and in these clusters some minor differences in spending habits are also evident. In other words there is some support for the claim that the customer segments have some variation in their spending habits.</p>

corrected abstract:
<p>In this thesis, cluster analysis was applied to data comprising of customer spending habits at a retail chain in order to perform customer segmentation. The method used was a two-step cluster procedure in which the first step consisted of feature engineering, a square root transformation of the data in order to handle big spenders in the data set and finally principal component analysis in order to reduce the dimensionality of the data set. This was done to reduce the effects of high dimensionality. The second step consisted of applying clustering algorithms to the transformed data. The methods used were 𝐾-means clustering, Gaussian mixture models in the <em>MCLUST</e> family, 𝑡-distributed mixture models in the tEIGEN family and non-negative matrix factorization (NMF). For the NMF clustering a slightly different data pre-processing step was taken, specifically no PCA was performed. Clustering partitions were compared on the basis of the Silhouette index, Davies-Bouldin index and subject matter knowledge, which revealed that 𝐾-means clustering with 𝐾 = 3 produces the most reasonable clusters. This algorithm was able to separate the customer into different segments depending on how many purchases they made overall and in these clusters some minor differences in spending habits are also evident. In other words there is some support for the claim that the customer segments have some variation in their spending habits.</p>
----------------------------------------------------------------------
In diva2:1290430 
abstract is: 
<p>The high strength/stiffness-to-weight ratio that composite materials exhibit has led to the utilization of composites as alternative to traditional materials in weight-critical applications. However, the highly anisotropic nature of composites renders the strength prediction under complex loading challenging. To efficiently predict the failure of composite structures especially in cases where out-of-plane stresses are dominant, the modeling of damage onset and propagation plays an essential role in accurate strength predictions.Firstly, in this Thesis work the analysis of a composite L-profile, which is loaded such that significant out-of-plane stresses are generated in the curved region, is conducted. However, the inherent heterogeneity at the micro/meso scale is not modeled for the stress analysis.Secondly, in this project the target was to accurately predict the initiation of failure at the ply level, modal based Puck’s matrix failure criteria have been implemented to the failure analysis. Maximum stress failure criteria were however retained to check the possible fiber-based failure which is not directly captured with in Puck’s failure criterion.Thirdly, Cohesive Zone Material Model has also been employed to model the growth of interlaminar damage (delamination). The delamination study is based on the Inter Fibre Fracture crack initiation and doesn’t include other causes like edge effects, voids, manufacturing defects etc.Finally, the attempt to validate the analysis results with the available test results was made. Further development of the existing model and several tests are required to be carried out for material characterization and complete validation of the developed damage model for composite structure.</p>

corrected abstract:
<p>The high strength/stiffness-to-weight ratio that composite materials exhibit has led to the utilization of composites as alternative to traditional materials in weight-critical applications. However, the highly anisotropic nature of composites renders the strength prediction under complex loading challenging. To efficiently predict the failure of composite structures especially in cases where out-of-plane stresses are dominant, the modeling of damage onset and propagation plays an essential role in accurate strength predictions.</p><p>Firstly, in this Thesis work the analysis of a composite L-profile, which is loaded such that significant out-of-plane stresses are generated in the curved region, is conducted. However, the inherent heterogeneity at the micro/meso scale is not modeled for the stress analysis.</p><p>Secondly, in this project the target was to accurately predict the initiation of failure at the ply level, modal based Puck’s matrix failure criteria have been implemented to the failure analysis. Maximum stress failure criteria were however retained to check the possible fiber-based failure which is not directly captured with in Puck’s failure criterion.</p><p>Thirdly, Cohesive Zone Material Model has also been employed to model the growth of interlaminar damage (delamination). The delamination study is based on the Inter Fibre Fracture crack initiation and doesn’t include other causes like edge effects, voids, manufacturing defects etc.</p><p>Finally, the attempt to validate the analysis results with the available test results was made. Further development of the existing model and several tests are required to be carried out for material characterization and complete validation of the developed damage model for composite structure.</p>
----------------------------------------------------------------------
In diva2:1249827 
abstract is: 
<p>To meet the need of lightweight chassis in the near future, a technological step of introducing anisotropic materials like Carbon Fibre Reinforced Plastics (CFRP) in structural parts of cars is a possible way ahead. Though there are commercially available tools to find suitability of Fibre Reinforced Plastics (FRPs) and their orientations, they depend on numerical optimization and complexity increases with the size of the model. Nevertheless, the user has a very limited control of intermediate steps. To understand the type of material system that can be used in different regions for a lightweight chassis, especially during the initial concept phase, a more simplified, yet reliable tool is desirable.The thesis aims to provide a framework for determining fibre orientations according to the most-ideal loading path to achieve maximum advantage from FRP-materials. This has been achieved by developing algorithms to find best-fit material orientations analytically, which uses principal stresses and their orientations in a finite element originating from multiple load cases. This thesis takes inspiration from the Durst criteria (2008) which upon implementation provides information on how individual elements must be modelled in a component subjected to multiple load cases. This analysis pre-evaluates the potential of FRP-suitable parts. Few modifications have been made to the existing formulations by the authors which have been explained in relevant sections.The study has been extended to develop additional MATLAB subroutines which finds the type of laminate design (uni-directional, bi-axial or quasi-isotropic) that is suitable for individual elements.Several test cases have been run to check the validity of the developed algorithm. Finally, the algorithm has been implemented on a Body-In-White subjected to two load cases. The thesis gives an idea of how to divide the structure into sub-components along with the local fibre directions based on the fibre orientations and an appropriate laminate design based on classical laminate theory.</p>

corrected abstract:
<p>To meet the need of lightweight chassis in the near future, a technological step of introducing anisotropic materials like Carbon Fibre Reinforced Plastics (CFRP) in structural parts of cars is a possible way ahead. Though there are commercially available tools to find suitability of Fibre Reinforced Plastics (FRPs) and their orientations, they depend on numerical optimization and complexity increases with the size of the model. Nevertheless, the user has a very limited control of intermediate steps. To understand the type of material system that can be used in different regions for a lightweight chassis, especially during the initial concept phase, a more simplified, yet reliable tool is desirable.</p><p>The thesis aims to provide a framework for determining fibre orientations according to the most-ideal loading path to achieve maximum advantage from FRP-materials. This has been achieved by developing algorithms to find best-fit material orientations analytically, which uses principal stresses and their orientations in a finite element originating from multiple load cases. This thesis takes inspiration from the Durst criteria (2008) which upon implementation provides information on how individual elements must be modelled in a component subjected to multiple load cases. This analysis pre-evaluates the potential of FRP-suitable parts. Few modifications have been made to the existing formulations by the authors which have been explained in relevant sections.</p><p>The study has been extended to develop additional MATLAB subroutines which finds the type of laminate design (uni-directional, bi-axial or quasi-isotropic) that is suitable for individual elements.</p><p>Several test cases have been run to check the validity of the developed algorithm. Finally, the algorithm has been implemented on a Body-In-White subjected to two load cases. The thesis gives an idea of how to divide the structure into sub-components along with the local fibre directions based on the fibre orientations and an appropriate laminate design based on classical laminate theory.</p>
----------------------------------------------------------------------
In diva2:1247567 
abstract is: 
<p>The main goal with the master thesis was to design and manufacture a frame and hull to a large AUV which KTH will use in research. The general aim for the Maribot LoLo (Long Endurance, Long Range AUV) is to act as an experimental platform and tool for a wide range of research activities, where all four capabilities within the SMaRC area (Autonomy, Perception, Endurance and Communication) can be tested, improved and demonstrated.The frame was designed and shall be built in aluminum profiles from Bosch Rexroth with custom made brackets, manufactured in a corrosion resistant steal. When the frame was designed, the associated load case and beam elementary case was implemented. No mechanical calculations have been executed since the frame is planned to be tested against the above mentioned load case.The hull was manufactured in glass fiber with a laminate built from 0/90 and ±45 fabrics. The laminates total weight per square meter was 4800 g/m2. The hull was manufactured by performing vacuum infusion onto a female-mould. The method used and the result is presented in appendix 1. The total cost for manufacturing the frame and hull was about 129 000 SEK, where the largest expenses were due to the manufacturing cost of the male-mould, and the labor cost when manufacturing the hull. Manufacturing a new glass fiber hull would in the future cost approximately 16 000 SEK, given it’s manufactured by students at KTH and in the same female-mould. Manufacturing the hull in carbon fiber would cost approximately 60 000 SEK.The future work required before the AUV is seaworthy are; assemble the frame, cut the hull to its correct dimensions, design and manufacture the attachment solutions between the hull and frame, and to test the frame against the applied loads. Future optimization work would be applying laminate theory and extract the mechanical properties of the hull. Combine the laminate properties with beam theory in order to estimate the deflection of the laminate when evacuating the water. Mechanical calculations on the frame could as well lead to optimizing the weight of the frame, where hopefully smaller dimensions on the profiles could be used.</p>

corrected abstract:
<p>The main goal with the master thesis was to design and manufacture a frame and hull to a large AUV which KTH will use in research. The general aim for the Maribot LoLo (Long Endurance, Long Range AUV) is to act as an experimental platform and tool for a wide range of research activities, where all four capabilities within the SMaRC area (Autonomy, Perception, Endurance and Communication) can be tested, improved and demonstrated.</p><p>The frame was designed and shall be built in aluminum profiles from Bosch Rexroth with custom made brackets, manufactured in a corrosion resistant steal. When the frame was designed, the associated load case and beam elementary case was implemented. No mechanical calculations have been executed since the frame is planned to be tested against the above mentioned load case.</p><p>The hull was manufactured in glass fiber with a laminate built from 0/90 and ±45 fabrics. The laminates total weight per square meter was 4800 g/m<sup>2</sup>. The hull was manufactured by performing vacuum infusion onto a female-mould. The method used and the result is presented in appendix 1. The total cost for manufacturing the frame and hull was about 129 000 SEK, where the largest expenses were due to the manufacturing cost of the male-mould, and the labor cost when manufacturing the hull. Manufacturing a new glass fiber hull would in the future cost approximately 16 000 SEK, given it’s manufactured by students at KTH and in the same female-mould. Manufacturing the hull in carbon fiber would cost approximately 60 000 SEK.</p><p>The future work required before the AUV is seaworthy are; assemble the frame, cut the hull to its correct dimensions, design and manufacture the attachment solutions between the hull and frame, and to test the frame against the applied loads. Future optimization work would be applying laminate theory and extract the mechanical properties of the hull. Combine the laminate properties with beam theory in order to estimate the deflection of the laminate when evacuating the water. Mechanical calculations on the frame could as well lead to optimizing the weight of the frame, where hopefully smaller dimensions on the profiles could be used.</p>
----------------------------------------------------------------------
In diva2:1247301 
abstract is: 
<p>The intention of this thesis is to develop, evaluate new concepts and look over the current design for a container sized barge module. By request of Group Ocean, a cost and weight reduction is the main improvement criteria along with keeping the strength of the module.Five concepts are developed, analyzed and discussed with the supervisor at Group Ocean, where three are decided to be presented here. The other two are left out, since they are considered way too expensive without giving a satisfying result. The three concepts that are developed throughout this thesis are; changing to high strength steel, changing to sandwich panels and increasing stiffeners with smaller dimensions.A structural optimization is made in the software MATLAB to find out the best dimension to use for the sandwich panels. To determine the local stresses, the finite element method is used in Inventor Professional. It is also where the design and CAD modules are built in, so for simplifications it is used for FEA (Finite Element Analysis) as well. To reduce the amount of elements and nodes, shell elements and other structural constraints are used in the FEA. All the concepts are modelled with the same structural constraints so a practical comparison study can be made.The final designs resulted in a total weight reduction up to 40% with a material cost reduction of 12%. Based on what type of material is chosen, the material cost reduction range is between 3-12% and the weight reduction range is between 13-40%.</p>

corrected abstract:
<p>The intention of this thesis is to develop, evaluate new concepts and look over the current design for a container sized barge module. By request of Group Ocean, a cost and weight reduction is the main improvement criteria along with keeping the strength of the module. Five concepts are developed, analyzed and discussed with the supervisor at Group Ocean, where three are decided to be presented here. The other two are left out, since they are considered way too expensive without giving a satisfying result. The three concepts that are developed throughout this thesis are; changing to high strength steel, changing to sandwich panels and increasing stiffeners with smaller dimensions.</p><p>A structural optimization is made in the software MATLAB to find out the best dimension to use for the sandwich panels. To determine the local stresses, the finite element method is used in Inventor Professional. It is also where the design and CAD modules are built in, so for simplifications it is used for FEA (Finite Element Analysis) as well. To reduce the amount of elements and nodes, shell elements and other structural constraints are used in the FEA. All the concepts are modelled with the same structural constraints so a practical comparison study can be made.</p><p>The final designs resulted in a total weight reduction up to 40% with a material cost reduction of 12%. Based on what type of material is chosen, the material cost reduction range is between 3-12% and the weight reduction range is between 13-40%.</p>
----------------------------------------------------------------------
In diva2:1229031 
abstract is: 
<p>Electric vehicles are a very interesting area in the automotive industry. Electric vehicles do not cause any tailpipe emissions, unlike conventional vehicles (diesel and gasoline vehicles with internal combustion engine) that emits large amounts of carbon dioxide equivalents through the tailpipe. In this study a comparative life cycle analysis is made through an environmental perspective between conventional vehicle and electric vehicle, where the carbon dioxide emissions are in focus. The life cycle is categorized in three parts; manufacturing, use and recycling.Definitions and assumptions were made to make the analysis possible. The driving distance was defined to 180 000 km under the lifespan of the vehicles. When calculating manufacturing emissions, a limitation was made that electric vehicles and conventional vehicles in the same size emits the same amount of carbon dioxide. Because of this the only difference in the manufacturing phase between the electric vehicles and the conventional vehicles was the emission from manufacturing the battery for the electric vehicles. Another limitation of the study is that no calculations were made on how much carbon dioxide that are emitted when producing the gasoline/diesel fuel for the conventional vehicles.The question stated is: How large is the difference in carbon dioxide emissions between an electric vehicle and a conventional vehicle from a lifecycle perspective?To answer the question a literature study was made based on former studies. To get a good representation of the lifecycle emissions caused by the different car types an analysis was made for six different cars, three electric vehicles and three conventional vehicles. It was found that in Sweden (with an electric mix of 50 grams C02/kWh) a Tesla model S 100D with a 100kWh Battery resulted in 9 metric tons C02. While a BMW 750i resulted in 53 metric tons. The BMW i3 with a battery capacity of 33kWh resulted in 6 metric tons and a Mitsubishi Mirage resulted in 29 metric tons.When calculations were based on the Chinese electricity mix (1000g CO2/kWh) the results were different. The Tesla model S 100D resulted in 60 metric tons, BMW i3 in 24 metric tons (the BMW 750i and the Mitsubishi Mirage resulted in the same amount). The lifecycle emissions were directly associated to the electricity mix of the country. When charging the battery of the electric vehicle and producing the battery in countries with “bad” electricity mix, the results clearly showed that a large amount of carbon dioxide was produced. Whilst in countries with “good” electricity mix (like Sweden) the electric vehicle was by far the better alternative compared to the conventional vehicle.</p>
----------------------------------------------------------------------


corrected abstract:
<p>Electric vehicles are a very interesting area in the automotive industry. Electric vehicles do not cause any tailpipe emissions, unlike conventional vehicles (diesel and gasoline vehicles with internal combustion engine) that emits large amounts of carbon dioxide equivalents through the tailpipe. In this study a comparative life cycle analysis is made through an environmental perspective between conventional vehicle and electric vehicle, where the carbon dioxide emissions are in focus. The life cycle is categorized in three parts; manufacturing, use and recycling. Definitions and assumptions were made to make the analysis possible. The driving distance was defined to 180 000 km under the lifespan of the vehicles. When calculating manufacturing emissions, a limitation was made that electric vehicles and conventional vehicles in the same size emits the same amount of carbon dioxide. Because of this the only difference in the manufacturing phase between the electric vehicles and the conventional vehicles was the emission from manufacturing the battery for the electric vehicles. Another limitation of the study is that no calculations were made on how much carbon dioxide that are emitted when producing the gasoline/diesel fuel for the conventional vehicles. The question stated is: How large is the difference in carbon dioxide emissions between an electric vehicle and a conventional vehicle from a lifecycle perspective? To answer the question a literature study was made based on former studies. To get a good representation of the lifecycle emissions caused by the different car types an analysis was made for six different cars, three electric vehicles and three conventional vehicles. It was found that in Sweden (with an electric mix of 50 grams CO<sub>2</sub>/kWh) a Tesla model S 100D with a 100kWh Battery resulted in 9 metric tons C02. While a BMW 750i resulted in 53 metric tons. The BMW i3 with a battery capacity of 33kWh resulted in 6 metric tons and a Mitsubishi Mirage resulted in 29 metric tons. When calculations were based on the Chinese electricity mix (1000g CO<sub>2</sub>/kWh) the results were different. The Tesla model S 100D resulted in 60 metric tons, BMW i3 in 24 metric tons (the BMW 750i and the Mitsubishi Mirage resulted in the same amount). The lifecycle emissions were directly associated to the electricity mix of the country. When charging the battery of the electric vehicle and producing the battery in countries with “bad” electricity mix, the results clearly showed that a large amount of carbon dioxide was produced. Whilst in countries with “good” electricity mix (like Sweden) the electric vehicle was by far the better alternative compared to the conventional vehicle.</p>
----------------------------------------------------------------------
In diva2:1189558 
abstract is: 
<p>There is a great need in the industry for measurement methods to characterize acoustic sources in ducts. One way to obtain a complete description of a source is to measure 2-port data, comprising of the source scattering matrix and the source vector. The resulting model can then be used to predict the source properties, e.g., radiated sound power, in the plane wave range for all installation conditions. Methods to measure the two-port model have been developed over the last few decades and can today be efficiently used for industrial purposes. The present thesis presents the whole process of designing, building, and validating a 2-port rig to measure high speed small fans, as well as an example of how to use the data to predict the noise emission of a product. All rig elements have been designed after a literature review and an analysis of the physical principles governing the behavior of the rig. Guidelines on microphone spacing, loudspeaker mounting, rig terminations and overall rig dimensions are given. The theory behind the measurement method of the active two-port in a duct is presented. </p><p>Additionally, a number of different post-processing methods are evaluated with respect to the properties of the experimental setup used i.e. the number of available microphones, the magnitude of the reflection coefficient at the rig terminations and the type of test object measured. The standard method that is most widely used nowadays is shown to become singular when the reflection coefficients at the rig terminations are high. A new post-processing method is suggested, and tested against the standard one. It is shown to behave better in highly reflective cases.</p>

corrected abstract:
<p>There is a great need in the industry for measurement methods to characterize acoustic sources in ducts. One way to obtain a complete description of a source is to measure 2-port data, comprising of the source scattering matrix and the source vector. The resulting model can then be used to predict the source properties, e.g., radiated sound power, in the plane wave range for all installation conditions. Methods to measure the two-port model have been developed over the last few decades and can today be efficiently used for industrial purposes. The present thesis presents the whole process of designing, building, and validating a 2-port rig to measure high speed small fans, as well as an example of how to use the data to predict the noise emission of a product.</p><p>All rig elements have been designed after a literature review and an analysis of the physical principles governing the behavior of the rig. Guidelines on microphone spacing, loudspeaker mounting, rig terminations and overall rig dimensions are given. The theory behind the measurement method of the active two-port in a duct is presented. Additionally, a number of different post-processing methods are evaluated with respect to the properties of the experimental setup used i.e. the number of available microphones, the magnitude of the reflection coefficient at the rig terminations and the type of test object measured. The standard method that is most widely used nowadays is shown to become singular when the reflection coefficients at the rig terminations are high. A new post-processing method is suggested, and tested against the standard one. It is shown to behave better in highly reflective cases.</p>
----------------------------------------------------------------------
In diva2:1184058 
abstract is: 
<p>The conceptual design of a modular AUV hull in fiber composite material has been determined. The goal has been to minimize the hull weight. Matrix and fiber materials have been investigated to find a composite combination that reduces the hull weight whilst being resistant to changes in the mechanical properties caused by submersion in water and the operational temperatures. A composite of a PEEK thermoplastic matrix with high strength carbon fibers is picked as the most suitable material option and used for hull calculations. Different composite part manufacturing processes are investigated to find a suitable method for the geometry and material that can produce high quality hull modules. Part quality factors such as fiber waviness and out of roundness in shape affects the collapse pressure of the hull. Filament winding adapted for a thermoplastic composite with in situ consolidation is picked as the most suitable manufacturing option.The structural designs considered for the hull are a fiber composite single skin construction (ring-stiffeners possible) and a sandwich construction, these would be manufactured as shorter modules that can be joined together to form the hull. The minimum hull thickness required for a single skin hull operating at 300 meters depth, considering material compression failure and buckling failure of the structure, is calculated for the PEEK/carbon fiber composite material. Buckling is the dimensioning failure mode of a thin walled cylinder with the AUV hull dimensions at the intended operational depth. The lay-up of the composite affects the thickness required so the lay-up is optimized to minimize the hull weight. For the cylindrical modules under hydrostatic pressure a [90/0/90] lay-up minimizes the thickness required and is the recommended lay-up. For comparison of hull weight with the existing AUV the minimum thickness required for a single skin hull in aluminium 7075 considering material compression failure and buckling failure of the structure is also calculated.From the analytical buckling analysis of a simple cylinder hull without joints the minimum thickness is determined as 11.82 mm for the composite hull and 15.23 mm for the aluminium hull, both values with a safety factor of 1.3 for the collapse pressure equating to 3.9 MPa. The single skin composite hull weight becomes 153 kg and the aluminium hull weight becomes 343 kg for these thicknesses. If the added stiffness of the structure from the joints would be taken into consideration it is expected that the thickness could be decreased further, but the relative weight difference between the composite and aluminium hull is expected to remain similar. From the finite element linear buckling analysis of the composite hull with thickness 11.82 mm the buckling pressure is determined as 3.39 MPa and for the aluminium hull with thickness 15.23 mm it is determined as 4.42 MPa.For a sandwich hull the minimum core thickness (using a weak core approximation and quasi-isotropic faces) is calculated as 19.96 mm, with safety factor 1.3 for the collapse depth and factor 1.1 for material failure of the faces. The weight for this sandwich hull with a carbon foam core becomes 72 kg. Based on a heat generation of 3 kW maximum during AUV operation heat transfer calculations through the thickness of the single skin composite hull give the maximum hull thickness as 50 mm before the AUV will overheat. The maximum 3 thickness of a regular PVC foam core sandwich hull is 4 mm and for a carbon foam core it is 21 mm before the AUV will overheat, making a sandwich with a carbon foam core a possible structural design choice but with some complicating factors compared to the single skin composite hull.</p>

corrected abstract:
<p>The conceptual design of a modular AUV hull in fiber composite material has been determined. The goal has been to minimize the hull weight. Matrix and fiber materials have been investigated to find a composite combination that reduces the hull weight whilst being resistant to changes in the mechanical properties caused by submersion in water and the operational temperatures. A composite of a PEEK thermoplastic matrix with high strength carbon fibers is picked as the most suitable material option and used for hull calculations. Different composite part manufacturing processes are investigated to find a suitable method for the geometry and material that can produce high quality hull modules. Part quality factors such as fiber waviness and out of roundness in shape affects the collapse pressure of the hull. Filament winding adapted for a thermoplastic composite with in situ consolidation is picked as the most suitable manufacturing option.</p><p>The structural designs considered for the hull are a fiber composite single skin construction (ring-stiffeners possible) and a sandwich construction, these would be manufactured as shorter modules that can be joined together to form the hull. The minimum hull thickness required for a single skin hull operating at 300 meters depth, considering material compression failure and buckling failure of the structure, is calculated for the PEEK/carbon fiber composite material. Buckling is the dimensioning failure mode of a thin walled cylinder with the AUV hull dimensions at the intended operational depth. The lay-up of the composite affects the thickness required so the lay-up is optimized to minimize the hull weight. For the cylindrical modules under hydrostatic pressure a [90º/0º/90º] lay-up minimizes the thickness required and is the recommended lay-up. For comparison of hull weight with the existing AUV the minimum thickness required for a single skin hull in aluminium 7075 considering material compression failure and buckling failure of the structure is also calculated.</p><p>From the analytical buckling analysis of a simple cylinder hull without joints the minimum thickness is determined as 11.82 mm for the composite hull and 15.23 mm for the aluminium hull, both values with a safety factor of 1.3 for the collapse pressure equating to 3.9 MPa. The single skin composite hull weight becomes 153 kg and the aluminium hull weight becomes 343 kg for these thicknesses. If the added stiffness of the structure from the joints would be taken into consideration it is expected that the thickness could be decreased further, but the relative weight difference between the composite and aluminium hull is expected to remain similar. From the finite element linear buckling analysis of the composite hull with thickness 11.82 mm the buckling pressure is determined as 3.39 MPa and for the aluminium hull with thickness 15.23 mm it is determined as 4.42 MPa.</p><p>For a sandwich hull the minimum core thickness (using a weak core approximation and quasi-isotropic faces) is calculated as 19.96 mm, with safety factor 1.3 for the collapse depth and factor 1.1 for material failure of the faces. The weight for this sandwich hull with a carbon foam core becomes 72 kg. Based on a heat generation of 3 kW maximum during AUV operation heat transfer calculations through the thickness of the single skin composite hull give the maximum hull thickness as 50 mm before the AUV will overheat. The maximum thickness of a regular PVC foam core sandwich hull is 4 mm and for a carbon foam core it is 21 mm before the AUV will overheat, making a sandwich with a carbon foam core a possible structural design choice but with some complicating factors compared to the single skin composite hull.</p>
----------------------------------------------------------------------
In diva2:1183380 
abstract is: 
<p>This paper investigates the potential to use adhesive bonding in high speed aluminum crafts with the purpose of replacing welding. By using adhesives, the higher strength of the aluminum prior to welding could be utilized thus allowing for thinner plating, in turn enabling lighter hulls. Avoiding welding would also eliminate the heat distortions that occurs today. These cost valuable production time and thereby increases the cost of the finished craft. The research question for this paper is formulated as:Can weight and/or costs be reduced by replacing welding with adhesive bonding?To answer the question a case study has been performed at Dockstavarvet where four different joints on the Interceptor Craft 16.5M have been redesigned using adhesive bonding instead of welding. A number of different adhesive types have been considered and Second generation acrylics, silicone, silane modified polymer, polyurethane and epoxy adhesives were evaluated in the text resulting in the conclusion that an epoxy is the most suitable adhesive. For this study the adhesive DP490 by 3M has been used to design the joints against Lloyds and DNV-GL rules as well as maximum shear stress in the adhesive. For each joint three different designs have been developed using FEA, allowing 20, 10 and 3 MPa of shear stress in the adhesive. FE simulations were also used to see the effects on the effective flange caused by the elasticity of the adhesive. This gave the result that with adhesives stiffer than 1 GPa the effective flange is fully utilized. To estimate the potential time savings two sample panels were manufactured, one welded and the other adhesively bonded.The result of the analysis is that in the best case, equivalent to the strength of the adhesive during the first years, around 15 % weight can be saved in the hull. However, considering long term effects such as cyclic and environmental fatigue the weight is instead increased compared to welded joints. It is also concluded that production time is saved by using adhesives and that this cost reduction outweighs the increased cost of the adhesive.</p>

corrected abstract:
<p>This paper investigates the potential to use adhesive bonding in high speed aluminum crafts with the purpose of replacing welding. By using adhesives, the higher strength of the aluminum prior to welding could be utilized thus allowing for thinner plating, in turn enabling lighter hulls. Avoiding welding would also eliminate the heat distortions that occurs today. These cost valuable production time and thereby increases the cost of the finished craft. The research question for this paper is formulated as: Can weight and/or costs be reduced by replacing welding with adhesive bonding? To answer the question a case study has been performed at Dockstavarvet where four different joints on the Interceptor Craft 16.5M have been redesigned using adhesive bonding instead of welding. A number of different adhesive types have been considered and Second generation acrylics, silicone, silane modified polymer, polyurethane and epoxy adhesives were evaluated in the text resulting in the conclusion that an epoxy is the most suitable adhesive. For this study the adhesive DP490 by 3M has been used to design the joints against Lloyds and DNV-GL rules as well as maximum shear stress in the adhesive. For each joint three different designs have been developed using FEA, allowing 20, 10 and 3 MPa of shear stress in the adhesive. FE simulations were also used to see the effects on the effective flange caused by the elasticity of the adhesive. This gave the result that with adhesives stiffer than 1 GPa the effective flange is fully utilized. To estimate the potential time savings two sample panels were manufactured, one welded and the other adhesively bonded. The result of the analysis is that in the best case, equivalent to the strength of the adhesive during the first years, around 15 % weight can be saved in the hull. However, considering long term effects such as cyclic and environmental fatigue the weight is instead increased compared to welded joints. It is also concluded that production time is saved by using adhesives and that this cost reduction outweighs the increased cost of the adhesive.</p>
----------------------------------------------------------------------
In diva2:1183366 
abstract is: 
<p>Nowadays, circular economy is becoming more relevant in society. In the context of the automotive industry, we no longer simply work on emissions emitted during the vehicle use phase but rather on the environmental impacts induced during all phases of the vehicle's life cycle (manufacturing, logistics, use, maintenance and end of life). For this purpose, many automakers, including the Group PSA, use life cycle assessment (LCA) to determine these environmental impacts. Also, the economy of sharing is gradually established and follows innovative uses of the car. New mobility systems emerge and compete with the classical system of sales of vehicles. These new uses of the automobile mainly take the form of car-sharing. In the future, it will become essential to evaluate these services from an environmental point of view.Some studies of the use of car-sharing already demonstrate important consequences such as reductions in the number of vehicles and in the number of kilometers traveled but also an increase in the use of other means of transport. However, to my knowledge, there is no LCA-based method to quantify the environmental benefit of the use of a car-sharing service in relation to the use of vehicles for exclusive use by the owner but also which would eco-design these services and the vehicles intended for these services.As part of this six-month project, a LCA approach was implemented to a PSA B2C (business-to-consumers) car-sharing service called “Emov” with a fleet of 500 Citroën C-Zero electric vehicles. The goal was to compare the use of Emov in Madrid, Spain with the urban use of a private Internal Combustion Engine (ICE) vehicle and a battery electric vehicle for one user characterized by its frequency, its average time and its average distance of use over a defined period. Thanks to a modeling of the service on the LCA software Gabi and by controlling over the input parameters related to the Emov service and the parameters related to the user's use of the service (variable parameters), it was therefore possible to show the influence of these parameters on the final results. Furthermore, it was possible to show also in which scenario it was more environmentally beneficial to use the service rather than a private vehicle. For the study, six impact indicators were chosen: the potentials for global warming, photochemical oxidation, air acidification, water eutrophication, resource depletion and primary energy demand.Using Emov’s big data to inform the service parameters and then varying the service user's usage parameters, it was possible to conclude that whatever the user's urban mobility needs, it is more beneficial to use the service than a private ICE vehicle for five of the six impact indicators. Only the acidification potential indicator (SO2 equivalent) is worse when using the service, which can be explained by the manufacture of the batteries of the Emov vehicles.</p>

corrected abstract:
<p>Nowadays, circular economy is becoming more relevant in society. In the context of the automotive industry, we no longer simply work on emissions emitted during the vehicle use phase but rather on the environmental impacts induced during all phases of the vehicle's life cycle (manufacturing, logistics, use, maintenance and end of life). For this purpose, many automakers, including the Group PSA, use life cycle assessment (LCA) to determine these environmental impacts. Also, the economy of sharing is gradually established and follows innovative uses of the car. New mobility systems emerge and compete with the classical system of sales of vehicles. These new uses of the automobile mainly take the form of car-sharing. In the future, it will become essential to evaluate these services from an environmental point of view.</p><p>Some studies of the use of car-sharing already demonstrate important consequences such as reductions in the number of vehicles and in the number of kilometers traveled but also an increase in the use of other means of transport. However, to my knowledge, there is no LCA-based method to quantify the environmental benefit of the use of a car-sharing service in relation to the use of vehicles for exclusive use by the owner but also which would eco-design these services and the vehicles intended for these services.</p><p>As part of this six-month project, a LCA approach was implemented to a PSA B2C (business-to-consumers) car-sharing service called “Emov” with a fleet of 500 Citroën C-Zero electric vehicles. The goal was to compare the use of Emov in Madrid, Spain with the urban use of a private Internal Combustion Engine (ICE) vehicle and a battery electric vehicle for one user characterized by its frequency, its average time and its average distance of use over a defined period. Thanks to a modeling of the service on the LCA software Gabi and by controlling over the input parameters related to the Emov service and the parameters related to the user's use of the service (variable parameters), it was therefore possible to show the influence of these parameters on the final results. Furthermore, it was possible to show also in which scenario it was more environmentally beneficial to use the service rather than a private vehicle. For the study, six impact indicators were chosen: the potentials for global warming, photochemical oxidation, air acidification, water eutrophication, resource depletion and primary energy demand.</p><p>Using Emov’s big data to inform the service parameters and then varying the service user's usage parameters, it was possible to conclude that whatever the user's urban mobility needs, it is more beneficial to use the service than a private ICE vehicle for five of the six impact indicators. Only the acidification potential indicator (SO2 equivalent) is worse when using the service, which can be explained by the manufacture of the batteries of the Emov vehicles.</p>
----------------------------------------------------------------------
In diva2:1183350 
abstract is: 
<p>In the following thesis the overset method, also called chimera or overlapping meshes, is discussed and applied toa formula race car, in order to calculate its aerodynamic map. The proposed method would allow reducing set-uptime through automation and avoided re-meshing process. a A theoretical background is presented before thediscussion of the way this kind of approach has been set-up in Star-CCM+. Results are obtained and discussedfor various car positions. Further investigations are finally suggested to further assess the viability of the method.</p>

corrected abstract:
<p>In the following thesis the overset method, also called chimera or overlapping meshes, is discussed and applied to a formula race car, in order to calculate its aerodynamic map. The proposed method would allow reducing set-up time through automation and avoided re-meshing process. a A theoretical background is presented before the discussion of the way this kind of approach has been set-up in Star-CCM+. Results are obtained and discussed for various car positions. Further investigations are finally suggested to further assess the viability of the method.</p>
----------------------------------------------------------------------
In diva2:1154527 
abstract is: 
<p>To access and operate in space, a wide range of propulsion systems has been developed, from high-thrust chemical propulsion to low-thrust electrical propulsion, and new kind of systems are considered, such as solar sails and nuclear propulsion. Recently, interest in hybrid rocket engines has been renewed due to their attractive features (safe, cheap, flexible) and they are now investigated and developed by research laboratories such as ONERA.This master’s thesis work is in line with their development at ONERA and aims at finding a methodology to study numerically the liquid oxidizer injection using a Lagrangian solver for the liquid phase. For this reason, it first introduces a model for liquid atomiser developed for aeronautical applications, the FIMUR model, and then focuses on its application to a hybrid rocket engine configuration.The FIMUR model and the Sparte solver have proven to work fine with high mass flow rates on coarse grids. The rocket engine simulations have pointed out the need of an initialisation of the flow field. The methodology study has proven that starting with a reduced liquid mass flow rate is preferable to a simulation with a reduced relaxation between the coupled solvers. The former could not be brought to conclusion due to lack of time but gives an encouraging path to further investigate.</p>

corrected abstract:
<p>To access and operate in space, a wide range of propulsion systems has been developed, from high-thrust chemical propulsion to low-thrust electrical propulsion, and new kind of systems are considered, such as solar sails and nuclear propulsion. Recently, interest in hybrid rocket engines has been renewed due to their attractive features (safe, cheap, flexible) and they are now investigated and developed by research laboratories such as ONERA.</p><p>This master’s thesis work is in line with their development at ONERA and aims at finding a methodology to study numerically the liquid oxidizer injection using a Lagrangian solver for the liquid phase. For this reason, it first introduces a model for liquid atomiser developed for aeronautical applications, the FIMUR model, and then focuses on its application to a hybrid rocket engine configuration.</p><p>The FIMUR model and the Sparte solver have proven to work fine with high mass flow rates on coarse grids. The rocket engine simulations have pointed out the need of an initialisation of the flow field. The methodology study has proven that starting with a reduced liquid mass flow rate is preferable to a simulation with a reduced relaxation between the coupled solvers. The former could not be brought to conclusion due to lack of time but gives an encouraging path to further investigate.</p>
----------------------------------------------------------------------
In diva2:1130092 
abstract is: 
<p>The purpose of this report is to examine the combination of an Extreme Learning Machine (ELM) with the Kernel Method</p><p>. Kernels lies at the core of Support Vector Machines success in classifying non-linearly separable datasets. The hypothesis is that by combining ELM with a kernel we will utilize features in the ELM-space otherwise unused. The report is intended as a proof of concept for the idea of using kernel methods in an ELM setting. This will be done by running the new algorithm against five image datasets for a classification accuracy and time complexity analysis. Results show that our extended ELM algorithm, which we have named</p><p>Extreme Kernel Machine (EKM), improve classification accuracy for some datasets compared to the regularised ELM, in the best scenarios around three percentage points. We found that the choice of kernel type and parameter values had great effect on the classification performance. The implementation of the kernel does however add computational complexity, but where that is not a concern EKM does have an advantage. This tradeoff might give EKM a place between other neural networks and regular ELMs.</p>

corrected abstract:
<p>The purpose of this report is to examine the combination of an <em>Extreme Learning Machine</em> (ELM) with the <em>Kernel Method</em>. Kernels lies at the core of Support Vector Machines success in classifying non-linearly separable datasets. The hypothesis is that by combining ELM with a kernel we will utilize features in the ELM-space otherwise unused. The report is intended as a proof of concept for the idea of using kernel methods in an ELM setting. This will be done by running the new algorithm against five image datasets for a classification accuracy and time complexity analysis.</p><p>Results show that our extended ELM algorithm, which we have named <em>Extreme Kernel Machine</em> (EKM), improve classification accuracy for some datasets compared to the regularised ELM, in the best scenarios around three percentage points. We found that the choice of kernel type and parameter values had great effect on the classification performance. The implementation of the kernel does however add computational complexity, but where that is not a concern EKM does have an advantage. This tradeoff might give EKM a place between other neural networks and regular ELMs.</p>
----------------------------------------------------------------------
In diva2:1120406 
abstract is: 
<p>The developments of algebraic set theory have given its models of settheory good closure properties under certain algebraic operations on thecategories which constitute the models. However, there does not yet seemto exist an established notion of morphism between such models. In thispaper, we develop a suggestion for such a notion by drawing on inspirationfrom the logical properties of the morphisms naturally arising from forcingin material set theory.</p>

corrected abstract:
<p>The developments of algebraic set theory have given its models of set theory good closure properties under certain algebraic operations on the categories which constitute the models. However, there does not yet seem to exist an established notion of morphism between such models. In this paper, we develop a suggestion for such a notion by drawing on inspiration from the logical properties of the morphisms naturally arising from forcing in material set theory.</p>
----------------------------------------------------------------------
In diva2:1098024 
abstract is: 
<p>Optimization problems have been immuned to any attempt of combination with machine learning until a decade ago but it is now an active research field. This thesis has studied the potential implementation of a machine learning heuristic to improve the resolution of the optimization scheduling problems based on a Constraint Programming solver. Some scheduling problems, known as <em>N P </em>-hard problems, suffer from large computational cost (large number of jobs to schedule) and consequent human effort (well-suited heuristics need to be derived). Moreover industrial scheduling problems obviously evolves over time but a lot of features and the basic structure remain the same. Hence they have potential in the implementation a supervised-learning-based heuristic.</p><p>First part of the study was to model a given benchmark of instances and im- plement some famous heuristics (as earliest due date, combined with the largest duration) in order to solve the benchmark.  Based on the none-optimality of returned solutions, primaries instances were choosen to implement our method. The second part represents the procedure which has been set up to design a supervised-learning-based heuristic. An instance generator was first  built to map the potential industrial evolutions of the instances. It returned secondaries instances representing the learning database. Then a CP-well-suited node ex- traction scheme was set up to collect relevant information from the resolution of the search tree. It will  collect data from nodes of the search tree given a proper criteria. These nodes are next projected onto a constant-dimensional space which described the system, the underlying subtree and the impact of the affectations. Upon these features and designed target values statistical mod- els are implemented. A linear and a gradient  boosting regressions have been implemented, calibrated and tuned upon the data. Last was to integrate the supervised-learning model into an heuristic framework. This has been done via a soft propagation module to try  the instantiation of all the children of the considered node and apply the given module upon them. The selection decision rule was based upon a reconstructed score. Third part was to test the procedure implemented. New secondaries instances were generated and supervised- learning-based heuristic tested against the earliest due date one.</p><p>The procedure was tested upon two different instances. The integrated heuristic returned positive results for both instances. For the first one (10 jobs to schedule) a gain in the first solution found (resp. the number of backtracks) of 18% (resp. 13% were realized. For the second instance (90 jobs to schedule) a gain in the first solution found of at least 16%. The results come to validate the procedure implemented and the methodology used.</p>

corrected abstract:
<p>Optimization problems have been immuned to any attempt of combination with machine learning until a decade ago but it is now an active research field. This thesis has studied the potential implementation of a machine learning heuristic to improve the resolution of the optimization scheduling problems based on a Constraint Programming solver. Some scheduling problems, known as 𝑁𝑃-hard problems, suffer from large computational cost (large number of jobs to schedule) and consequent human effort (well-suited heuristics need to be derived). Moreover industrial scheduling problems obviously evolves over time but a lot of features and the basic structure remain the same. Hence they have potential in the implementation a supervised-learning-based heuristic.</p><p>First part of the study was to model a given benchmark of instances and implement some famous heuristics (as earliest due date, combined with the largest duration) in order to solve the benchmark.  Based on the none-optimality of returned solutions, primaries instances were choosen to implement our method. The second part represents the procedure which has been set up to design a supervised-learning-based heuristic. An instance generator was first built to map the potential industrial evolutions of the instances. It returned secondaries instances representing the learning database. Then a CP-well-suited node extraction scheme was set up to collect relevant information from the resolution of the search tree. It will collect data from nodes of the search tree given a proper criteria. These nodes are next projected onto a constant-dimensional space which described the system, the underlying subtree and the impact of the affectations. Upon these features and designed target values statistical models are implemented. A linear and a gradient boosting regressions have been implemented, calibrated and tuned upon the data. Last was to integrate the supervised-learning model into an heuristic framework. This has been done via a soft propagation module to try the instantiation of all the children of the considered node and apply the given module upon them. The selection decision rule was based upon a reconstructed score. Third part was to test the procedure implemented. New secondaries instances were generated and supervised-learning-based heuristic tested against the earliest due date one.</p><p>The procedure was tested upon two different instances. The integrated heuristic returned positive results for both instances. For the first one (10 jobs to schedule) a gain in the first solution found (resp. the number of backtracks) of 18% (resp. 13% were realized. For the second instance (90 jobs to schedule) a gain in the first solution found of at least 16%. The results come to validate the procedure implemented and the methodology used.</p>
----------------------------------------------------------------------
In diva2:1083454 
abstract is: 
<p>ABB is moving towards more powerful and compact transformers and an efficient cooling is of paramount importance in order to avoid overheating.In this master thesis, transformers without guides for the oil flow have been analysed: they allow a faster and cheaper manufacturing process, but at the same time the absence of guides makes the cooling design more difficult.In order to be able to perform several parametric studies, a script with the Pointwise mesher has been developed for the automatic generation of the geometry and mesh of transformer windings. This has allowed to analyse transformer windings with a different shape, assessing the effect of a certain number of geometrical parameters on the cooling efficiency.The software ANSYS Fluent was used to perform 2D axisymmetric unsteady simulations on the company cluster and the simulation set up was validated thanks to comparison with experimental measurements in ABB in Vaasa (Finland), that showed an average relative error below 2%.A remarkable result of this study is the identification of a periodic pattern in the temperature of the windings from the inlet to the outlet of the transformer, with hot spot locations every 10-20 disc windings. This conclusion has also been confirmed by the experimental measurements performed in Vaasa on a test transformer. Furthermore, a periodic behaviour of the temperature of the windings and of the oil in time has also been identified.Finally, transformers with an additional cooling channel in the disc windings have been studied, revealing that an accurate design is needed when adding oil channels through the windings in order to avoid the formation of unexpected hot spots.</p>

corrected abstract:
<p>ABB is moving towards more powerful and compact transformers and an efficient cooling is of paramount importance in order to avoid overheating. In this master thesis, transformers without guides for the oil flow have been analysed: they allow a faster and cheaper manufacturing process, but at the same time the absence of guides makes the cooling design more difficult.</p><p>In order to be able to perform several parametric studies, a script with the Pointwise mesher has been developed for the automatic generation of the geometry and mesh of transformer windings. This has allowed to analyse transformer windings with a different shape, assessing the effect of a certain number of geometrical parameters on the cooling efficiency.</p><p>The software ANSYS Fluent was used to perform 2D axisymmetric unsteady simulations on the company cluster and the simulation set up was validated thanks to comparison with experimental measurements in ABB in Vaasa (Finland), that showed an average relative error below 2%.</p><p>A remarkable result of this study is the identification of a periodic pattern in the temperature of the windings from the inlet to the outlet of the transformer, with hot spot locations every 10-20 disc windings. This conclusion has also been confirmed by the experimental measurements performed in Vaasa on a test transformer. Furthermore, a periodic behaviour of the temperature of the windings and of the oil in time has also been identified.</p><p>Finally, transformers with an additional cooling channel in the disc windings have been studied, revealing that an accurate design is needed when adding oil channels through the windings in order to avoid the formation of unexpected hot spots.</p>
----------------------------------------------------------------------
In diva2:1082654 
abstract is: 
<p>This Master Thesis main purpose was to answer the question "Can you measure or calculate the velocity sound proﬁle while performing a bathrymetric survey in an oﬀshore environment like the North Sea, without any interaction nor modiﬁcation of the existing equipment?".This, since today underwater surveys are a complex and expensive operation to perform where you either are mapping the sea ﬂoor or on a searching mission for a sunken wrecks. To achieve this successfully, one has to ensure that the accuracy of the position for every discovery or created map over the sea ﬂoor is entirely correct. This is an identiﬁed problem since the bathrymetric device sends its position by sonar, which relies in the sound propagation velocity, which in return varies with the water density. In order to increase the accuracy one need to determine the water density for all depths, i.e measure the salinity and temperature between the towing ship and the device that travels close to the sea bed. This because of layers consisting of fresh water and saltwater that never entirely mixes with each other in the ocean.The outcome of this project is a manufactured conceptual design of an autonomous sensor carrier that has the ability to measure temperature to a theoretical maximum depth down to 150m. It ascends and descends autonomously with an propagation speed of 0.54m/s in a static condition along an existing tether line, connected to a bathrymetric device that follows the sea ﬂoor. The sensor carrier ascend and descends its motion with help of two connected drive wheels powered by an electric motor, combined with two hall sensors to to reverse its movement when reaching desired depth. It has the ability to store sampled data onto a removable SD card, with a theoretical maximum endurance of 6,2km and it can be handled by one single person.Unfortunately, the concept as a whole is not entirely successful, and must therefore be supplemented within some areas. The major occurrence is that the drive mechanism tender to slip along the tether when climbing in vertical direction with a risk of damaging the tether coating. Furthermore one needs to increase the operational depth rating. This to be able to utilize the sensor carrier at all depths in the North Sea and also the Baltic Sea. However, the project as whole has achieved a solid framework and platform ready to be developed further in a future second version.</p>

corrected abstract:
<p>This Master Thesis main purpose was to answer the question "<em>Can you measure or calculate the velocity sound profile while performing a bathrymetric survey in an offshore environment like the North Sea, without any interaction nor modification of the existing equipment?</e>".</p><p>This, since today underwater surveys are a complex and expensive operation to perform where you either are mapping the sea floor or on a searching mission for a sunken wrecks. To achieve this successfully, one has to ensure that the accuracy of the position for every discovery or created map over the sea floor is entirely correct. This is an identified problem since the bathrymetric device sends its position by sonar, which relies in the sound propagation velocity, which in return varies with the water density. In order to increase the accuracy one need to determine the water density for all depths, i.e measure the salinity and temperature between the towing ship and the device that travels close to the sea bed. This because of layers consisting of fresh water and saltwater that never entirely mixes with each other in the ocean.</p><p>The outcome of this project is a manufactured conceptual design of an autonomous sensor carrier that has the ability to measure temperature to a theoretical maximum depth down to 150m. It ascends and descends autonomously with an propagation speed of 0.54m/s in a static condition along an existing tether line, connected to a bathrymetric device that follows the sea floor. The sensor carrier ascend and descends its motion with help of two connected drive wheels powered by an electric motor, combined with two hall sensors to to reverse its movement when reaching desired depth. It has the ability to store sampled data onto a removable SD card, with a theoretical maximum endurance of 6,2km and it can be handled by one single person.</p><p>Unfortunately, the concept as a whole is not entirely successful, and must therefore be supplemented within some areas. The major occurrence is that the drive mechanism tender to slip along the tether when climbing in vertical direction with a risk of damaging the tether coating. Furthermore one needs to increase the operational depth rating. This to be able to utilize the sensor carrier at all depths in the North Sea and also the Baltic Sea. However, the project as whole has achieved a solid framework and platform ready to be developed further in a future second version.</p>
----------------------------------------------------------------------
In diva2:1078063 
abstract is: 
<p>The present paper describes the development of a finite element based tool at the launcher directorate of theFrench Space Agency (CNES) in order to support the ground study and testing of fairing separation systems.The main topic is the finite element modeling process and its validation. The considered finite element methodsand resolution algorithms are presented, with a focus on shell element formulations in the used finite elementpackages. The model results are compared with those obtained for a separation system with a pre-existing modeland a ground test. The developed model shows good agreement with the reference results and enables the CNESto fulfill its technical support role.</p>

corrected abstract:
<p>The present paper describes the development of a finite element based tool at the launcher directorate of the French Space Agency (CNES) in order to support the ground study and testing of fairing separation systems. The main topic is the finite element modeling process and its validation. The considered finite element methods and resolution algorithms are presented, with a focus on shell element formulations in the used finite element packages. The model results are compared with those obtained for a separation system with a pre-existing model and a ground test. The developed model shows good agreement with the reference results and enables the CNES to fulfill its technical support role.</p>
----------------------------------------------------------------------
In diva2:1072495 
abstract is: 
<p>Fitness tracking using machine learning algorithms is a new and widely unex-plored field. In most reports on fitness tracking, multiple accelerometers and other sensors are attached to the subject preforming exercises. This approach is not suitable for everyday usage. The last decade have seen a growing trend to- wards using smartphones for everyday applications. This offers a new possibility of collecting data for fitness tracking.This report investigates to what extent accelerometer and gyroscope data from a smartphone attached to an arm can be used to detect and separate push-ups from other workout exercises. A complete workout performed by two subjects have been recorded and analysed. A window based pre-processing technique was applied to extract features of the workout data. Support vector machines (SVM) and multi layer perceptrons (MLP) have been used to classify the data based on the feature extraction.Dips was the exercise easiest confused as push-ups. When a sample containing push-ups and dips was tested by a two layer perceptron trained on a variety of exercises not including dips, more false positives than true positives was acquired.A total of 87.5% correct classifications was obtained when the training data consisted of an entire workout by the first subject and the test data consisted of the same workout by the second subject. In the opposite case a total of 95.6% correct classifications were acquired.The results of this research support the idea that one smartphone attatched to the upper arm of a subject is sufficient to perform the distinction of push-ups and non push-ups.</p>

corrected abstract:
<p>Fitness tracking using machine learning algorithms is a new and widely unexplored field. In most reports on fitness tracking, multiple accelerometers and other sensors are attached to the subject preforming exercises. This approach is not suitable for everyday usage. The last decade have seen a growing trend towards using smartphones for everyday applications. This offers a new possibility of collecting data for fitness tracking.</p><p>This report investigates to what extent accelerometer and gyroscope data from a smartphone attached to an arm can be used to detect and separate push-ups from other workout exercises. A complete workout performed by two subjects have been recorded and analysed. A window based pre-processing technique was applied to extract features of the workout data. Support vector machines (SVM) and multi layer perceptrons (MLP) have been used to classify the data based on the feature extraction.</p><p>Dips was the exercise easiest confused as push-ups. When a sample containing push-ups and dips was tested by a two layer perceptron trained on a variety of exercises not including dips, more false positives than true positives was acquired.</p><p>A total of 87.5% correct classifications was obtained when the training data consisted of an entire workout by the first subject and the test data consisted of the same workout by the second subject. In the opposite case a total of 95.6% correct classifications were acquired.</p><p>The results of this research support the idea that one smartphone attatched to the upper arm of a subject is sufficient to perform the distinction of push-ups and non push-ups.</p>
----------------------------------------------------------------------
In diva2:1044602 
abstract is: 
<p>This thesis presents the work I have accomplished during my 6 months internship in Altran Research more precisely inthe Space Innovation Unit in Cannes, France. This period as trainee was also the conclusion of the double degree program I followed in Aerospace Engineering at KTH, the Royal Institute of Technology of Stockholm, Sweden. This report details every Space Safety related projects in which I have been involved. Every topic is related to the management of low earth orbit satellites disintegration during their atmospheric re-entry. Nowadays orbital pollution has pushed national space agencies to take the lead on space debris mitigation. There are currently more than twenty thousand objects of more than 10 cm constantly tracked from ground to avoid collision within-progress missions. This is implying expensive avoidance manoeuvres thus equipment and budget associated. Items shorter than 10 cm are even more numerous and they cannot be seen from ground so they are estimate by models. The debris population is threatening future missions and even launches if nothing is done to prevent/reduce the debrisformation. To avoid this catastrophic scenario, space agencies have developed and financed projects to prevent and reduce debris creation. In the meantime, risk on ground must be reduced to limit population injuries from falling object. Now satellites are designed/retro-designed to demise more, and in known ways, during uncontrolled re-entry. Software are also currently developed to simulate more precisely the complex aerothermal phenomenon of ablation duringatmospheric re-entry.</p>

corrected abstract:
<p>This thesis presents the work I have accomplished during my 6 months internship in Altran Research more precisely in the Space Innovation Unit in Cannes, France. This period as trainee was also the conclusion of the double degree program I followed in Aerospace Engineering at KTH, the Royal Institute of Technology of Stockholm, Sweden.</p><p>This report details every Space Safety related projects in which I have been involved. Every topic is related to the management of low earth orbit satellites disintegration during their atmospheric re-entry.</p><p>Nowadays orbital pollution has pushed national space agencies to take the lead on space debris mitigation. There are currently more than twenty thousand objects of more than 10 cm constantly tracked from ground to avoid collision with in-progress missions. This is implying expensive avoidance manoeuvres thus equipment and budget associated. Items shorter than 10 cm are even more numerous and they cannot be seen from ground so they are estimate by models. The debris population is threatening future missions and even launches if nothing is done to prevent/reduce the debris formation.</p><p>To avoid this catastrophic scenario, space agencies have developed and financed projects to prevent and reduce debris creation. In the meantime, risk on ground must be reduced to limit population injuries from falling object. Now satellites are designed/retro-designed to demise more, and in known ways, during uncontrolled re-entry. Software are also currently developed to simulate more precisely the complex aerothermal phenomenon of ablation during atmospheric re-entry.</p>
----------------------------------------------------------------------
In diva2:1040617 
abstract is: 
<p>Turbocharging the internal combustion engine is one of the most effective ways to reduce the fuel consumption and fulll the green house gas emissions requierements. Nevertheless, this techinque has some limitations that need to be addressed in order to improve the turbocharger performance. The range of use of the compressor is restricted by the surgeline at low massow rates and the choke line at high massow rates. The compressor map gives valuable information of the compressor stable operating points. However, due to the space constraints in an engine compartiment, complex pipes are needed to integrate all the components correctly leading to a dierence in the compressor preformance from measurments to in-situ congurations. Computational Fluid Dynamics is a powerful tool to predict compressor maps in a shorter time and less laborious way compared to experimental measurements and obtaining data in the whole domain.The compressor map of a turbocharger compressor was calculated using a steady-state RANS approach and the Moving Reference Frames technique to handle the rotating parts of the machine, validating the method with experimental data. The 　ow eld from near optimal e-ciency points to near surge was assessed identifying a strong swirling back　ow at odesign conditions responsible for the e-ciency and pressure ratio drop.The eect of a 90º bent pipe upstream the compressor inlet was studied. Two counter-rotating vortices were observed to form after the bend and vanishing under the eect of the wheel rotation in evey case. It was shown that the 　ow structures introduced by the bend can be benecialat near surge condition, mitigating the rotating back　ow and improving the e-ciency and pressure ratio of the compressor at this particular case.</p>

corrected abstract:
<p>Turbocharging the internal combustion engine is one of the most effective ways to reduce the fuel consumption and fulfill the green house gas emissions requierements. Nevertheless, this technique has some limitations that need to be addressed in order to improve the turbocharger performance. The range of use of the compressor is restricted by the surge line at low mass flow rates and the choke line at high mass flow rates. The compressor map gives valuable information of the compressor stable operating points. However, due to the space constraints in an engine compartiment, complex pipes are needed to integrate all the components correctly leading to a difference in the compressor preformance from measurments to in-situ configurations. Computational Fluid Dynamics is a powerful tool to predict compressor maps in a shorter time and less laborious way compared to experimental measurements and obtaining data in the whole domain.</p><p>The compressor map of a turbocharger compressor was calculated using a steady-state RANS approach and the Moving Reference Frames technique to handle the rotating parts of the machine, validating the method with experimental data. The flow field from near optimal effiiency points to near surge was assessed identifying a strong swirling backflow at off-design conditions responsible for the effiiency and pressure ratio drop. The effect of a 90º bent pipe upstream the compressor inlet was studied. Two counter-rotating vortices were observed to form after the bend and vanishing under the effect of the wheel rotation in evey case. It was shown that the flow structures introduced by the bend can be beneficial at near surge condition, mitigating the rotating backflow and improving the effiiency and pressure ratio of the compressor at this particular case.</p>
----------------------------------------------------------------------
In diva2:954433 - correct as is
----------------------------------------------------------------------
In diva2:942666  - correct as is
----------------------------------------------------------------------
In diva2:902524 
abstract is: 
<p>The use of electric propulsion is a watershed in the space field. Indeed, due to its eÿciency in term of mass consumption, the actors of the space industries see in this piece of technology a means to manufacture lighter satellites and to launch them at lower cost. To face with this new market, industries need to develop new tools to handle these satellites and their missions. This report will elaborate on the methods used to compute maneuvers for all-electric spacecraft.</p><p>One of the main phases during satellite operations is maneuvering to ensure on the one hand a correct configuration to achieve the mission and on the other hand the integrity of the satellite. The present work is focused on the computation of the orbital maneuvers during the early phase of the mission: orbit raising. Due to the characteristics of electric propulsion, an overall approach provided by the application of the optimal control theory is required to compute these maneuvers performed by low-thrust engines. This report will develop the use of an indirect method based on the Pontryagin minimum principle. Two types of problems related to the constraints during the space missions are presented. Because electric maneuvers are longer than chemical maneuvers, it is usually necessary to seek to minimize the duration of a maneuver. The second interesting performance is the remaining propellant mass to achieve the mission: therefore, the minimization of the mass consumption during the maneuver is the second performance considered in the report.</p><p>During the internship, a JAVA implementation of the resolution of these two problems has been done. The report will present the preliminary results as well as the encountered difficulties and some possible solutions.</p><p></p>

corrected abstract:
<p>The use of electric propulsion is a watershed in the space field. Indeed, due to its efficiency in term of mass consumption, the actors of the space industries see in this piece of technology a means to manufacture lighter satellites and to launch them at lower cost. To face with this new market, industries need to develop new tools to handle these satellites and their missions. This report will elaborate on the methods used to compute maneuvers for all-electric spacecraft.</p><p>One of the main phases during satellite operations is maneuvering to ensure on the one hand a correct configuration to achieve the mission and on the other hand the integrity of the satellite. The present work is focused on the computation of the orbital maneuvers during the early phase of the mission: orbit raising. Due to the characteristics of electric propulsion, an overall approach provided by the application of the optimal control theory is required to compute these maneuvers performed by low-thrust engines. This report will develop the use of an indirect method based on the Pontryagin minimum principle. Two types of problems related to the constraints during the space missions are presented. Because electric maneuvers are longer than chemical maneuvers, it is usually necessary to seek to minimize the duration of a maneuver. The second interesting performance is the remaining propellant mass to achieve the mission: therefore, the minimization of the mass consumption during the maneuver is the second performance considered in the report.</p><p>During the internship, a JAVA implementation of the resolution of these two problems has been done. The report will present the preliminary results as well as the encountered difficulties and some possible solutions.</p>
----------------------------------------------------------------------
In diva2:894068 
abstract is: 
<p>The aim with this thesis is to investigate if the Twin Fin concept can be a beneficial propulsion system for large cruise ships, about 300 m long.</p><p>The Twin Fin concept is a new propulsion system, launched in 2014 by Caterpillar Propulsion [1]. The concept is diesel-electric and has two fins, containing a gearbox and an electric motor, immersed in water [2]. Previous investigations have shown the concept to have several advantages compared to other propulsion systems . A seismic vessel, Polarcus, has been retrofitted with the Twin Fin concept and it has been proved to have both operating and cost benefits compared to its previous arrangement with azimuth thrusters [3].</p><p>Diesel-electric propulsion is common for cruise ships, which would make the Twin Fin concept a possible propulsion solution for them. It’s of interest to investigate if the concept can be as beneficial for large cruise ship as it has shown to be for other vessel types. To investigate this the whole concept is considered.</p><p>A cruise ship hull and fins are modeled with computer-aided design (CAD) using CAESES/FRIENDSHIP-Framework (CAESES/FFW), starting building up a procedure for customization of fin design into ship layout. Tracking of the operation of similar cruise ships is performed with automatic identification system (AIS) in order to create an operational profile for the model cruise ship. A propeller is designed for the model cruise ship, using a Caterpillar Propulsion in-house software. A conceptual drawing of the electric power plant is also created.</p><p>Computational fluid dynamics (CFD) simulations are performed on CAD model hull with and without fins, in order to find out how much resistance is added due to the presence of the fins. These CFD analyses are performed with the open source CFD toolbox OpenFOAM, using the volume of fluid (VOF) method for free surface modeling. Reynolds-averaged Navier-Stokes (RANS) is used for modeling turbulent flow, using the turbulence model SST together with wall functions. Also a coupling between RANS and the boundary element method (BEM) software PROCAL is used for an active propeller behind the ship, computing the effective wake fraction and thrust deduction.  k</p><p>Finally the Twin Fin concept is compared to other propulsion systems, conventional shafting and Azipod, finding its advantages and disadvantages.</p><p>The CFD simulations results in an added resistance of 18% in 20 knots due to the presence of the fins. A larger propeller can be fitted compared to the other propulsion systems, especially compared to the Azipod system, resulting in an increase of thrust by about 5.6% for the Twin Fin concept. The comparison between the systems shows that the Twin Fin concept have several advantages compared to the other systems, e.g. increased payload and increased reliability.</p><p>One main conclusions drawn from this investigation is that the added resistance is very dependent on hull form and it's important to customize the fin to hull form and operational profile. Another conclusion drawn is that the larger propeller can't fully compensate for the added resistance due to the fins. Since the Twin Fin concept have other advantages it could still be beneficial for cruise ship applications, especially if further optimizing the fin shape and position and by this lowering the added resistance.</p>

corrected abstract:
<p>The aim with this thesis is to investigate if the Twin Fin concept can be a beneficial propulsion system for large cruise ships, about 300 m long.</p><p>The Twin Fin concept is a new propulsion system, launched in 2014 by Caterpillar Propulsion [1]. The concept is diesel-electric and has two fins, containing a gearbox and an electric motor, immersed in water [2]. Previous investigations have shown the concept to have several advantages compared to other propulsion systems . A seismic vessel, Polarcus, has been retrofitted with the Twin Fin concept and it has been proved to have both operating and cost benefits compared to its previous arrangement with azimuth thrusters [3].</p><p>Diesel-electric propulsion is common for cruise ships, which would make the Twin Fin concept a possible propulsion solution for them. It’s of interest to investigate if the concept can be as beneficial for large cruise ship as it has shown to be for other vessel types. To investigate this the whole concept is considered.</p><p>A cruise ship hull and fins are modeled with computer-aided design (CAD) using CAESES/FRIENDSHIP-Framework (CAESES/FFW), starting building up a procedure for customization of fin design into ship layout. Tracking of the operation of similar cruise ships is performed with automatic identification system (AIS) in order to create an operational profile for the model cruise ship. A propeller is designed for the model cruise ship, using a Caterpillar Propulsion in-house software. A conceptual drawing of the electric power plant is also created.</p><p>Computational fluid dynamics (CFD) simulations are performed on CAD model hull with and without fins, in order to find out how much resistance is added due to the presence of the fins. These CFD analyses are performed with the open source CFD toolbox OpenFOAM, using the volume of fluid (VOF) method for free surface modeling. Reynolds-averaged Navier-Stokes (RANS) is used for modeling turbulent flow, using the turbulence model 𝑘-ω SST together with wall functions. Also a coupling between RANS and the boundary element method (BEM) software PROCAL is used for an active propeller behind the ship, computing the effective wake fraction and thrust deduction.</p><p>Finally the Twin Fin concept is compared to other propulsion systems, conventional shafting and Azipod, finding its advantages and disadvantages.</p><p>The CFD simulations results in an added resistance of 18% in 20 knots due to the presence of the fins. A larger propeller can be fitted compared to the other propulsion systems, especially compared to the Azipod system, resulting in an increase of thrust by about 5.6% for the Twin Fin concept. The comparison between the systems shows that the Twin Fin concept have several advantages compared to the other systems, e.g. increased payload and increased reliability.</p><p>One main conclusions drawn from this investigation is that the added resistance is very dependent on hull form and it's important to customize the fin to hull form and operational profile. Another conclusion drawn is that the larger propeller can't fully compensate for the added resistance due to the fins. Since the Twin Fin concept have other advantages it could still be beneficial for cruise ship applications, especially if further optimizing the fin shape and position and by this lowering the added resistance.</p>
----------------------------------------------------------------------
In diva2:883534   - correct as is
----------------------------------------------------------------------
In diva2:857660 
abstract is: 
<p>This master’s thesis project from the Department of Mathematics at KTH, Royal Institute of Technology, Stockholm, Sweden was carried out at Saab Dynamics, Linköping, Sweden. In this thesis, an octocopter was studied, which is a multirotor vehicle, a rotorcraft with more than two rotors. Multirotors have recently become very popular and have various interesting applications needing further research. Since the market for powerful credit-card-sized computers is continuously increasing, the development and research of multirotors is simplified.</p><p>The main purpose of this thesis project was to develop a complete open-sourced octocopter including control laws which should be used in future thesis projects at Saab Dynamics. Since the authors field is within Applied Mathematics, this thesis report has focused on the theoretical and analytical part of the project rather than the development process. Nevertheless, the report includes and gives a detailed overview of the complete octocopter and its final configuration. In addition, the report features system identifications which were carried out in order to estimate important properties of the vehicle.</p><p>In order to carry out mathematical analyses and propose control laws, a mathematical model of the vehicle was required. Since the vehicle moves in 6DoF (six-degrees of freedom), a suitable coordinate system handling these freedoms was needed. In this thesis, Euler angles and quaternions were used for representing attitude. The complete nonlinear model was derived using Newton’s second law of motion in a rotating reference frame. Additional effects such as aerodynamics, precession torques and motor dynamics were analyzed and modeled.</p><p>The main content of this report which can be divided into two part deals with the control of the octocopter. The first part investigates approaches for converting of the model control inputs, forces and torques, to corresponding motor commands, angular-rates. This issue is seldom covered in research articles proposing control laws for multirotors and requires special attention when developing multirotors. By minimizing the <em>L</em>2-norm, deriving control boundaries and using a priority algorithm which handles situations where the control demanded is greater than the momentary available control input, it was shown that the conversion between these properties was possible, even in critical situations.</p><p>The second part proposes control laws and approaches for controlling the vehicle in 6DoF. Three different control strategies have been proposed: pilot-based, attitude and position control. The pilot-based control is intended to be used by a pilot, controlling the in aviation standard roll, pitch and yaw-rate. The control method used is a full state feedback controller using a reduced observer, observing the motor dynamics. The attitude controller is an alteration of the pilot-based controller, controlling roll, pitch and yaw. Lastly, a position controller is derived, controlling the translational position of the vehicle, allowing for autonomous flying. The position controller uses a nonlinear Lyapunov based controller where the control inputs are converted to desired attitude reference inputs, send to the attitude controller.</p><p>The control laws were evaluated using a Simulink model where the complete nonlinear system was implemented. All derived control laws showed promising results and were able to accomplish desired behavior. The model featured a visualization of the vehicle in 6DoF running in real time and enabled for the use of a pc gaming controller, allowing for training and testing of e.g., the pilot-based controller. At the end, real flying data is presented and analyzed using the pilot-based controller. The report is finished of with a discussion, covering previously chapters of the thesis, proposing future interesting research and work.</p>

corrected abstract:
<p>This master’s thesis project from the Department of Mathematics at KTH, Royal Institute of Technology, Stockholm, Sweden was carried out at Saab Dynamics, Linköping, Sweden. In this thesis, an octocopter was studied, which is a multirotor vehicle, a rotorcraft with more than two rotors. Multirotors have recently become very popular and have various interesting applications needing further research. Since the market for powerful credit-card-sized computers is continuously increasing, the development and research of multirotors is simplified.</p><p>The main purpose of this thesis project was to develop a complete open-sourced octocopter including control laws which should be used in future thesis projects at Saab Dynamics. Since the authors field is within Applied Mathematics, this thesis report has focused on the theoretical and analytical part of the project rather than the development process. Nevertheless, the report includes and gives a detailed overview of the complete octocopter and its final configuration. In addition, the report features system identifications which were carried out in order to estimate important properties of the vehicle.</p><p>In order to carry out mathematical analyses and propose control laws, a mathematical model of the vehicle was required. Since the vehicle moves in 6DoF (six-degrees of freedom), a suitable coordinate system handling these freedoms was needed. In this thesis, Euler angles and quaternions were used for representing attitude. The complete nonlinear model was derived using Newton’s second law of motion in a rotating reference frame. Additional effects such as aerodynamics, precession torques and motor dynamics were analyzed and modeled.</p><p>The main content of this report which can be divided into two part deals with the control of the octocopter. The first part investigates approaches for converting of the model control inputs, forces and torques, to corresponding motor commands, angular-rates. This issue is seldom covered in research articles proposing control laws for multirotors and requires special attention when developing multirotors. By minimizing the <em>L</em>2-norm, deriving control boundaries and using a priority algorithm which handles situations where the control demanded is greater than the momentary available control input, it was shown that the conversion between these properties was possible, even in critical situations.</p><p>The second part proposes control laws and approaches for controlling the vehicle in 6DoF. Three different control strategies have been proposed: pilot-based, attitude and position control. The pilot-based control is intended to be used by a pilot, controlling the in aviation standard roll, pitch and yaw-rate. The control method used is a full state feedback controller using a reduced observer, observing the motor dynamics. The attitude controller is an alteration of the pilot-based controller, controlling roll, pitch and yaw. Lastly, a position controller is derived, controlling the translational position of the vehicle, allowing for autonomous flying. The position controller uses a nonlinear Lyapunov based controller where the control inputs are converted to desired attitude reference inputs, send to the attitude controller.</p><p>The control laws were evaluated using a Simulink model where the complete nonlinear system was implemented. All derived control laws showed promising results and were able to accomplish desired behavior. The model featured a visualization of the vehicle in 6DoF running in real time and enabled for the use of a pc gaming controller, allowing for training and testing of e.g., the pilot-based controller. At the end, real flying data is presented and analyzed using the pilot-based controller. The report is finished of with a discussion, covering previously chapters of the thesis, proposing future interesting research and work.</p>
----------------------------------------------------------------------
In diva2:845171   - correct as is
----------------------------------------------------------------------
In diva2:839875 
abstract is: 
<p>A component of a hydraulic shock absorber, a check valve, is analyzed using both numerical simulations as well as experimental testing. A uid-structure interaction (FSI) model is set up in ANSYSWorkbench and is validated through physical experiments - both steady state and transient. The uid eld is solved in ANSYS Fluent and the structural deformation is solved for in ANSYS Structural. The coupling is made using ANSYS System Coupling. The report covers the fundamentals of FSI analysis - methods of coupling uid and structure solution elds and methods for adapting the uid mesh to account for a changing geometry. A brief background on general moving/deforming mesh algorithms are presented but the emphasis lies on the methods available in ANSYS Fluent and how to apply these on the case of a shock absorber check valve. A moving/deforming mesh consisting of tetrahedral cells without ination layers on wall boundaries proves the most robust dynamic mesh setup. The exclusion of ination layers is shown to signicantly a ect the solution at low valve lift height. At full lift the exclusion of ination layers has no inuence on the solution. The check valve is 4-fold axisymmetric but is shown to exhibit asymmetrical displacement. This is due to an asymmetrical uid pressure distribution on the check valve. Steady state FSI simulations show satisfactory correlation to ow bench experiments at low ow rates. The opening pressure di erential of the check valve, determined by the spring preload, is accurately predicted by the FSI model. At flow rates above 10 l/min the di erential pressure is under predicted, due tosimplications to the computational domain. Transient simulations and experiments both show an oscillatory pressure di erentialacross the check valve as it opens, albeit with di erent frequencies.</p>

corrected abstract:
<p>A component of a hydraulic shock absorber, a check valve, is analyzed using both numerical simulations as well as experimental testing. A fluid-structure interaction (FSI) model is set up in ANSYS Workbench and is validated through physical experiments - both steady state and transient. The fluid field is solved in ANSYS Fluent and the structural deformation is solved for in ANSYS Structural. The coupling is made using ANSYS System Coupling.</p><p>The report covers the fundamentals of FSI analysis - methods of coupling fluid and structure solution fields and methods for adapting the fluid mesh to account for a changing geometry. A brief background on general moving/deforming mesh algorithms are presented but the emphasis lies on the methods available in ANSYS Fluent and how to apply these on the case of a shock absorber check valve.</p><p>A moving/deforming mesh consisting of tetrahedral cells without inflation layers on wall boundaries proves the most robust dynamic mesh setup. The exclusion of inflation layers is shown to significantly affect the solution at low valve lift height. At full lift the exclusion of inflation layers has no influence on the solution. The check valve is 4-fold axisymmetric but is shown to exhibit asymmetrical displacement. This is due to an asymmetrical fluid pressure distribution on the check valve.</p><p>Steady state FSI simulations show satisfactory correlation to flow bench experiments at low flow rates. The opening pressure differential of the check valve, determined by the spring preload, is accurately predicted by the FSI model. At flow rates above 10 l/min the differential pressure is under predicted, due to simplifications to the computational domain.</p><p>Transient simulations and experiments both show an oscillatory pressure differential across the check valve as it opens, albeit with different frequencies.</p>
----------------------------------------------------------------------
In diva2:827777 
abstract is: 
<p>Teleopti WFM Forecasts is a tool that can be used in order to predict future contact volumes in contact centers and staffing requirements, both in the short and the long term. This tool uses historical data of incoming contact volumes to perform a forecast on a given forecasting period. Today this tool uses a very simple algorithm which is not always very accurate. It also requires inputs from the customer in some of the steps, in order to generate the forecast. The task of this thesis is to improve this algorithm to get a more accurate forecast that can be generated automatically, without any input from the customer. Since Teleopti has more than 730 customers in more than 70 countries worldwide [3] the most challenging part of this project has been to find an algorithm that works for a lot of different historical data. Since different data contains different patterns there is not a single method that works best for all types of data. To investigate what method that is best to use for some specific data, and to perform a forecast according to this method, a step by step method was produced. A shortened version of this method is presented below.</p><ul><li><p><em> </em>Remove irrelevant data that differs too much from the latest data.</p></li><li><p><em> </em>Use the autocorrelation function to find out what seasonal variations that are present in the data.</p></li><li><p><em> </em>Estimate and remove the trend.</p></li><li><p><em> </em>Split the data, with the estimated trend removed, into two parts. Use the first part of the data to _t different models. Compare the different models with the other part of the data. The one that fits the second part best in least square sense is the one that is going to be used.</p></li><li><p><em> </em>Estimate the chosen model again, using all the data, and remove it from the full sample of data.</p></li><li><p><em> </em>Forecast the trend with Holts method.</p></li><li><p><em> </em>Combine the estimated trend with the estimated seasonal variations to perform the forecast.</p></li></ul><p>There are a lot of factors that affect the accuracy of the forecast generated by using this step by step method. By analysing a lot of data and the corresponding forecasts, the following three factors seem to have most impact on the forecasting result. First of all, if the data contains a lot of randomness it is difficult to forecast it, no matter how good the forecasting methods are. Also, if there are small volumes of historical data it will affect the forecasting result in a bad way, since estimating each seasonal variation requires a certain volume of data. And finally, if the trend tends to often change direction considerably in the data it is quite difficult to forecast it, since this means that it could probably change a lot in the future as well.</p><p>This step by step method has been tested on plenty of data from a lot of different contact centers in order to get it as good as possible for as many customers as possible. However, even though it has exhibited a good forecast of these data there is no guarantee that it will perform a good forecast for all possible data amongst Teleopti's customers. Hence, in the future, if this step by step method will be used by Teleopti, it will probably be updated continuously in order to satisfy as many customers as possible.</p>


corrected abstract:
<p>Teleopti WFM Forecasts is a tool that can be used in order to predict future contact volumes in contact centers and staffing requirements, both in the short and the long term. This tool uses historical data of incoming contact volumes to perform a forecast on a given forecasting period. Today this tool uses a very simple algorithm which is not always very accurate. It also requires inputs from the customer in some of the steps, in order to generate the forecast. The task of this thesis is to improve this algorithm to get a more accurate forecast that can be generated automatically, without any input from the customer. Since Teleopti has more than 730 customers in more than 70 countries worldwide [3] the most challenging part of this project has been to find an algorithm that works for a lot of different historical data. Since different data contains different patterns there is not a single method that works best for all types of data. To investigate what method that is best to use for some specific data, and to perform a forecast according to this method, a step by step method was produced. A shortened version of this method is presented below.</p><ul><li>Remove irrelevant data that differs too much from the latest data.</li><li>Use the autocorrelation function to find out what seasonal variations that are present in the data.</li><li>Estimate and remove the trend.</li><li>Split the data, with the estimated trend removed, into two parts. Use the first part of the data to fit different models. Compare the different models with the other part of the data. The one that fits the second part best in least square sense is the one that is going to be used.</li><li>Estimate the chosen model again, using all the data, and remove it from the full sample of data.</li><li>Forecast the trend with Holts method.</li><li>Combine the estimated trend with the estimated seasonal variations to perform the forecast.</li></ul><p>There are a lot of factors that affect the accuracy of the forecast generated by using this step by step method. By analysing a lot of data and the corresponding forecasts, the following three factors seem to have most impact on the forecasting result. First of all, if the data contains a lot of randomness it is difficult to forecast it, no matter how good the forecasting methods are. Also, if there are small volumes of historical data it will affect the forecasting result in a bad way, since estimating each seasonal variation requires a certain volume of data. And finally, if the trend tends to often change direction considerably in the data it is quite difficult to forecast it, since this means that it could probably change a lot in the future as well.</p><p>This step by step method has been tested on plenty of data from a lot of different contact centers in order to get it as good as possible for as many customers as possible. However, even though it has exhibited a good forecast of these data there is no guarantee that it will perform a good forecast for all possible data amongst Teleopti's customers. Hence, in the future, if this step by step method will be used by Teleopti, it will probably be updated continuously in order to satisfy as many customers as possible.</p>
----------------------------------------------------------------------
In diva2:827602    - correct as is
----------------------------------------------------------------------
In diva2:818756   - correct as is
----------------------------------------------------------------------
In diva2:814442 
abstract is: 
<p>The Liquid State Machine is a computational model introduced by Wolfgang Maass. Based on a short term memory model, it is a promising step in mimicking the adaptability and efficiency of the learning process found in neural cortices of living organisms. It has been successfully implemented using articial Spiking Neural Networks.</p><p>In this report we investigate the aforementioned adaptability and specically compare performance between two problems reusing essential parts of the network. Using publicly accessible tools and a simplistic approach we implement a Liquid State Machine for two static pattern recognition tasks: A basic XOR gate and classication of the Iris data set.</p><p>The fraction of correct answers of our Liquid State Machine reached a percentage of above 90% for both problems easily. A conclusion could not be drawn regarding the correlation of performance between the two problems. The inconclusiveness of the investigation was assessed to be caused mainly by the simplicity of both the benchmark problems, and possibly by the implementation of our Liquid State Machine.</p>

corrected abstract:
<p>The Liquid State Machine is a computational model introduced by Wolfgang Maass. Based on a short term memory model, it is a promising step in mimicking the adaptability and efficiency of the learning process found in neural cortices of living organisms. It has been successfully implemented using artificial Spiking Neural Networks.</p><p>In this report we investigate the aforementioned adaptability and specifically compare performance between two problems reusing essential parts of the network. Using publicly accessible tools and a simplistic approach we implement a Liquid State Machine for two static pattern recognition tasks: A basic XOR gate and classication of the Iris data set.</p><p>The fraction of correct answers of our Liquid State Machine reached a percentage of above 90% for both problems easily. A conclusion could not be drawn regarding the correlation of performance between the two problems. The inconclusiveness of the investigation was assessed to be caused mainly by the simplicity of both the benchmark problems, and possibly by the implementation of our Liquid State Machine.</p>
----------------------------------------------------------------------
In diva2:814299 
abstract is: 
<p>This report investigates two methods of finding the optimal control of aninverted pendulum with a quadratic cost functional.</p><p>In the first method a discretisation of a Hamiltonian system is taken as a symplectic Euler-scheme for Newton’s method which is used to find theoptimal control from an initial guess. According to the Pontryagin principle this gives the optimal control, since the solution to a Hamiltonian system gives the optimum to a control problem. The second method uses the matrix Riccati differential equation to find the optimal control for a linearised model of the pendulum.</p><p>The result was two programs that find the optimal control. The first method’s program demands clever initial guesses in order to converge. The linearised model’s solutions are only valid for a limited area, which turned out to be surprisingly large.</p>

corrected abstract:
<p>This report investigates two methods of finding the optimal control of an inverted pendulum with a quadratic cost functional.</p><p>In the first method a discretisation of a Hamiltonian system is taken as a symplectic Euler-scheme for Newton’s method which is used to find the optimal control from an initial guess. According to the Pontryagin principle this gives the optimal control, since the solution to a Hamiltonian system gives the optimum to a control problem. The second method uses the matrix Riccati differential equation to find the optimal control for a linearised model of the pendulum.</p><p>The result was two programs that find the optimal control. The first method’s program demands clever initial guesses in order to converge. The linearised model’s solutions are only valid for a limited area, which turned out to be surprisingly large.</p>
----------------------------------------------------------------------
In diva2:802021   - correct as is
----------------------------------------------------------------------
In diva2:784036 
abstract is: 
<p>Return collectors are predominant organs for rubber-tyred subways to operatesince they ensure both the track circuit shunt and the traction current return. Po- sitioned at the interface between the track and the rolling stock, they are subjected to the disruptions linked to the train movement and the track irregularities. One of the most critical steps is the crossing of a switch nose.This study aims at determining the collector position during this crossing by means of a quasi-static analysis of the system. Two approaches are investigated. The first one brings into play a rigid contact and geometrical angles. It enables to model the crossing until the contact with the crossing nose. The diving capability of the collector is also taken into account. The second one is a standard  approach of the contact. A slight penetration is considered, which allows to grasp the contact with  the crossing nose. The second advantage  is to prepare the ground for a complete dynamical analysis. Both approaches are then implemented on Matlab to solve the equations. Finally the study of the switch crossing in nominal conditionsand a parametric analysis are achieved for a specified switch.</p>

corrected abstract:
<p>Return collectors are predominant organs for rubber-tyred subways to operate since they ensure both the track circuit shunt and the traction current return. Positioned at the interface between the track and the rolling stock, they are subjected to the disruptions linked to the train movement and the track irregularities. One of the most critical steps is the crossing of a switch nose.</p><p>This study aims at determining the collector position during this crossing by means of a quasi-static analysis of the system. Two approaches are investigated. The first one brings into play a rigid contact and geometrical angles. It enables to model the crossing until the contact with the crossing nose. The diving capability of the collector is also taken into account. The second one is a standard approach of the contact. A slight penetration is considered, which allows to grasp the contact with the crossing nose. The second advantage is to prepare the ground for a complete dynamical analysis. Both approaches are then implemented on Matlab to solve the equations. Finally the study of the switch crossing in nominal conditions and a parametric analysis are achieved for a specified switch.</p>
----------------------------------------------------------------------
In diva2:755031 - missing space in title:
"Determination of a probabilistic model forflight path prediction"
==>
"Determination of a probabilistic model for flight path prediction"

abstract is: 
<p>Flightradar24.com is a website providing a flight tracking service that has a coverage spanning, a major part of the world. In some geographical areas though, the website is unable to get information from the airplanes. One such area is a large part of the Atlantic Ocean. When the website loses track of a plane, it keeps plotting it for about ten minutes keeping the latest givens peed and heading, before letting it disappear from view.</p><p>In this degree project, an attempt is made to improve the model used by the website, using statistical methods and theories of machine learning. Data for a large amount of flights is observed. The data used includes the speed of the airplanes, their positions, their altitude and their headings, as well as the time when the flight took place. There is also some basic information about the flight, such as airplane type and flight number. By finding connections and relations in this data, a probabilistic model is created. The model is then used to predict where a flight outside the coverage of Flightradar24.com is at any given time. Specifically, the model is used to predict the flight path of airplanes over the Atlantic Ocean.</p><p>Finally, the model is tested and found to give a more accurate prediction than the existing model for a number of flights.</p>

corrected abstract:
<p>Flightradar24.com is a website providing a flight tracking service that has a coverage spanning, a major part of the world. In some geographical areas though, the website is unable to get information from the airplanes. One such area is a large part of the Atlantic Ocean. When the website loses track of a plane, it keeps plotting it for about ten minutes keeping the latest given speed and heading, before letting it disappear from view.</p><p>In this degree project, an attempt is made to improve the model used by the website, using statistical methods and theories of machine learning. Data for a large amount of flights is observed. The data used includes the speed of the airplanes, their positions, their altitude and their headings, as well as the time when the flight took place. There is also some basic information about the flight, such as airplane type and flight number. By finding connections and relations in this data, a probabilistic model is created. The model is then used to predict where a flight outside the coverage of Flightradar24.com is at any given time. Specifically, the model is used to predict the flight path of airplanes over the Atlantic Ocean.</p><p>Finally, the model is tested and found to give a more accurate prediction than the existing model for a number of flights.</p>
----------------------------------------------------------------------
In diva2:754702 
abstract is: 
<p>In this thesis we analyze parameter optimization problems governed by linear ordinary differential equations (ODEs) and develop computationally efficient numerical methods for their solution. In addition, a series of noise-robust finite difference formulas are given for the estimation of the derivatives in the ODEs. The suggested methods have been employed to identify Gene Regulatory Networks (GRNs).</p><p>GRNs are responsible for the expression of thousands of genes in any given developmental process. Network inference deals with deciphering the complex interplay of genes in order to characterize the cellular state directly from experimental data. Even though a plethora of methods using diverse conceptual ideas has been developed, a reliable network reconstruction remains challenging. This is due to several reasons, including the huge number of possible topologies, high level of noise, and the complexity of gene regulation at different levels. A promising approach is dynamic modeling using differential equations. In this thesis we present such an approach to infer quantitative dynamic models from biological data which addresses inherent weaknesses in the current state-of-the-art methods for data-driven reconstruction of GRNs. The method is computationally cheap such that the size of the network (model complexity) is no longer a main concern with respect to the computational cost but due to data limitations; the challenge is a huge number of possible topologies. Therefore we embed a filtration step into the method to reduce the number of free parameters before simulating dynamical behavior. The latter is used to produce more information about the network’s structure.</p><p>We evaluate our method on simulated data, and study its performance with respect to data set size and levels of noise on a 1565-gene <em>E.coli </em>gene regulatory network. We show the computation time over various network sizes and estimate the order of computational complexity. Results on five networks in the benchmark collection DREAM4 Challenge are also presented. Results on five networks in the benchmark collection DREAM4 Challenge are also presented and show our method to outperform the current state of the art methods on synthetic data and allows the reconstruction of bio-physically accurate dynamic models from noisy data.</p>

corrected abstract:
<p>In this thesis we analyze parameter optimization problems governed by linear ordinary differential equations (ODEs) and develop computationally efficient numerical methods for their solution. In addition, a series of noise-robust finite difference formulas are given for the estimation of the derivatives in the ODEs. The suggested methods have been employed to identify Gene Regulatory Networks (GRNs).</p><p>GRNs are responsible for the expression of thousands of genes in any given developmental process. Network inference deals with deciphering the complex interplay of genes in order to characterize the cellular state directly from experimental data. Even though a plethora of methods using diverse conceptual ideas has been developed, a reliable network reconstruction remains challenging. This is due to several reasons, including the huge number of possible topologies, high level of noise, and the complexity of gene regulation at different levels. A promising approach is dynamic modeling using differential equations. In this thesis we present such an approach to infer quantitative dynamic models from biological data which addresses inherent weaknesses in the current state-of-the-art methods for data-driven reconstruction of GRNs. The method is computationally cheap such that the size of the network (model complexity) is no longer a main concern with respect to the computational cost but due to data limitations; the challenge is a huge number of possible topologies. Therefore we embed a filtration step into the method to reduce the number of free parameters before simulating dynamical behavior. The latter is used to produce more information about the network’s structure.</p><p>We evaluate our method on simulated data, and study its performance with respect to data set size and levels of noise on a 1565-gene <em>E.coli</em> gene regulatory network. We show the computation time over various network sizes and estimate the order of computational complexity. Results on five networks in the benchmark collection DREAM4 Challenge are also presented. Results on five networks in the benchmark collection DREAM4 Challenge are also presented and show our method to outperform the current state of the art methods on synthetic data and allows the reconstruction of bio-physically accurate dynamic models from noisy data.</p>

Note: Minor corrections to move the </em> to before the space.
----------------------------------------------------------------------
In diva2:752763 
abstract is: 
<p>Rear wheel steering of vehicles is a possible steering, which has been forgotten in vehicle design. Earlier works show only details concerning one vehicle type from this subject. In this work the author will first present a background to how different vehicles are steered and then the analysis of it.</p><p>The purpose with this paper is to from literature and interviews answer to which impact the placement of the engine and the conceptual differences have on the steering from different vessels. It will also answer which anticipated properties the steering will have if it´s not placed in a conventional way. Through simulation in MATLAB the paper will answer to which degree mass, velocity and length of axle influences steering behaviour of a rear wheal steered, RWS, car and how the steering response is different to a front wheal steered car, FWS.</p><p>The result from the literature and the interview shows that placement of engine and propeller near the rudder is very important for a ships steering capability. For vehicles it has most effect on the centrum of gravity, which is to prefer in middle of the vehicle,and for airplanes the ability to place cargo. The conceptual differences shows that a larger vehicle has more to gain, if it is equipped with four wheel steering, due to turning radius is a more critical aspect. For airplanes in super sonic speed it reveals that they need another set of wings. Ships need a smaller turning radius in harbours and all size of ships then use thrusters. The properties of a vessel with not conventional steering are most negative for cars and ships because of risk for oversteering and instability, while for airplanes its already in use in form of canard wings.</p><p>The simulations display that rear wheel steered vehicle should have a limit of velocity because of its behaviour in high speeds over 50 km/h. A larger mass has shown todepress the magnitude of lateral acceleration in wrong direction and a larger length of an axle is shown to depress the yaw-rate. The result also displays that a RWS vehicle has a delay of the lateral acceleration compared to a FWS vehicle with approximately 0,1-0,2 seconds depending on mass, velocity and length of axle. The test of response indicates that this delay contributes to the delay of the lateral movement of the vehicle the first second for speeds between 10-90 km/h, but after only 1,5 seconds further thelateral movement is twice the value for speeds 50-90km/h.</p><p>The conclusion is that vehicle doesn’t have RWS due to the delay in response in the beginning and the very larger lateral movement after only seconds later, which makes the vehicle hard to control.</p>

corrected abstract:
<p>Rear wheel steering of vehicles is a possible steering concept, which has seldom been used in vehicle design. Earlier works show only details concerning one vehicle type from this subject. In this work the author will first present a background to how different vehicles are steered and the analysis of it.</p><p>The purpose with this work is to from literature and interviews, answer which impact the placement of the engine and the conceptual differences have on the steering of different vessels. It will also answer which anticipated properties the steering will have if it´s not placed in a conventional way. Through simulation in MATLAB this work intend to answer to how mass, velocity and length of axle influences steering behaviour of a rear wheal steered, (RWS), car and how the steering response is different to a front wheal steered car, (FWS).</p><p>The result from the literature and the interviews shows that the placement of engine and propeller near the rudder is very important for a ships steering capability. For vehicles it has most effect on the centrum of gravity, which is to prefer in middle of the vehicle, and for airplanes the ability to place cargo. The conceptual differences shows that a larger vehicle has more to gain, if it is equipped with four wheel steering, due to that the turning radius is a more critical aspect. For airplanes in super sonic speed it reviles that they need another set of wings. Ships need a smaller turning radius in harbours and therefore all size of ships use thrusters. The properties of a vessel with not conventional steering are most negative for cars and ships because of risk for oversteering and instability, while for airplanes it is already in use in form of the canard wings.</p><p>The simulations display that rear wheel steered vehicle should have a limit of velocity because of its behaviour in high speeds over 50 km/h. A larger mass has shown to depress the magnitude of lateral acceleration in wrong direction and a larger length of an axle is shown to depress the yaw-rate motion. The result also displays that a RWS vehicle has a delay of the lateral acceleration compared to a FWS vehicle with approximately 0,1-0,2 s depending on mass, velocity and length of axle. The test of response indicates that this delay contributes to the delay of the lateral movement of the vehicle the first second for speeds between 10-90 km/h, but after only 1,5 s further the lateral movement is twice the value for speeds 50-90km/h.</p><p>The conclusion is that vehicles don’t have RWS due to the delay in the initial response and the larger lateral movement after only seconds later, which makes the vehicle difficult to control.</p>

Note: The abstract in DiVA has many changes in the wording from the thesis.
----------------------------------------------------------------------
In diva2:737028   - correct as is
----------------------------------------------------------------------
In diva2:725385 
abstract is: 
<p>This Bachelor’s thesis introduces a dynamic method for allocation of adjustable-rate mortgages which can be used to reduce a growing social problem. The Swedish household’s debt has steadily increased and the primary cause is mortgages loans, used to fund the purchase of a real property. Because of a period of rising home prices, Swedes are mortgaged more than ever [1]. If interest rates go up there is an increased risk of not managing the interest charge which is the greatest expense for many households.</p><p>However, there is an absence in the debate of the importance of efficient and safe mortgages. By means of the introduced dynamic method, this thesis optimizes the allocation of adjustable-rate mortgages, using both historical and future cases. In the forecasting case, expected interest charge and risk are being minimized. More specifically, the method consider three levels of risk which all values expected interest charge and risk differently. Further, the goal is to apply the dynamic method for decision support in reality and also commercialize as a business idea.</p><p>Modeling mortgage as a network flow is essential for the dynamic method. This way enables analysis of mortgages during a period of time which is necessary for determining an optimal allocation of adjustable-rate mortgages. The result from the historical case shows that shorter adjustment periods have been more favorable. Though, to an extent lower than expected – only during seven out of the last seventeen years. In addition, the results from the forecasting case indicate that it is advantageous to choose longer adjustment periods. Finally introduces a business model for the startup company, Looptime AB, of which business idea is toadminister mortgages for households, residents’ associations and non-financial companies.</p>

corrected abstract:
<p>This Bachelor’s thesis introduces a dynamic method for allocation of adjustable-rate mortgages which can be used to reduce a growing social problem. The Swedish household’s debt has steadily increased and the primary cause is mortgages loans, used to fund the purchase of a real property. Because of a period of rising home prices, Swedes are mortgaged more than ever [1]. If interest rates go up there is an increased risk of not managing the interest charge which is the greatest expense for many households.</p><p>However, there is an absence in the debate of the importance of efficient and safe mortgages. By means of the introduced dynamic method, this thesis optimizes the allocation of adjustable-rate mortgages, using both historical and future cases. In the forecasting case, expected interest charge and risk are being minimized. More specifically, the method consider three levels of risk which all values expected interest charge and risk differently. Further, the goal is to apply the dynamic method for decision support in reality and also commercialize as a business idea.</p><p>Modeling mortgage as a network flow is essential for the dynamic method. This way enables analysis of mortgages during a period of time which is necessary for determining an optimal allocation of adjustable-rate mortgages. The result from the historical case shows that shorter adjustment periods have been more favorable. Though, to an extent lower than expected – only during seven out of the last seventeen years. In addition, the results from the forecasting case indicate that it is advantageous to choose longer adjustment periods. Finally introduces a business model for the startup company, Looptime AB, of which business idea is to administer mortgages for households, residents’ associations and non-financial companies.</p>

Note: Only one merged word "toadminister" ==> "to administer"
----------------------------------------------------------------------
In diva2:723683   - correct as is
----------------------------------------------------------------------
In diva2:721093 
abstract is: 
<p>Graphene is a two-dimensional material, whose popularity has soared in both condensedmatter</p><p>physics and material science the past decade. Due to its unique properties,</p><p>graphene can be used in a vast array of new and interesting applications that could fundamentally</p><p>change the material industry. This report reviews the current research and</p><p>literature in order to trace the historical development of graphene. Then, in order to</p><p>better understand the material, the unique properties of graphene are explained and potential</p><p>applications are listed. From a theoretical physics perspective, the tight-binding</p><p>approximation is used to calculate the energy bands formed by the</p><p>-electrons. Then, the</p><p>electrons close to the Fermi energy are shown to satisfy the relativistic Dirac equation of</p><p>a massless fermion and additionally, the Landau energy levels of graphene are calculated.</p><p>Finally, the band gap structure of graphene is modelled and compared to that of silicon</p>

corrected abstract:
<p>Graphene is a two-dimensional material, whose popularity has soared in both condensed matter physics and material science the past decade. Due to its unique properties, graphene can be used in a vast array of new and interesting applications that could fundamentally change the material industry. This report reviews the current research and literature in order to trace the historical development of graphene. Then, in order to better understand the material, the unique properties of graphene are explained and potential applications are listed. From a theoretical physics perspective, the tight-binding approximation is used to calculate the energy bands formed by the π-electrons. Then, the electrons close to the Fermi energy are shown to satisfy the relativistic Dirac equation of a massless fermion and additionally, the Landau energy levels of graphene are calculated. Finally, the band gap structure of graphene is modelled and compared to that of silicon</p>
----------------------------------------------------------------------
In diva2:694343   - correct as is
----------------------------------------------------------------------
In diva2:664611   - correct as is
----------------------------------------------------------------------
In diva2:644428 - missing space in title:
"Distribution of Neutrino Mixing Parameters FromReal and Complex Random Neutrino Mass Matrices"
==>
"Distribution of Neutrino Mixing Parameters From Real and Complex Random Neutrino Mass Matrices"

abstract is: 
<p>The theory of neutrino oscillations is covered. Also, the theory of Dirac, Majorana, and</p><p>Dirac-Majorana neutrinos is presented. From this theory, and the model presented in [7],</p><p>a program is constructed which creates a suitable number of neutrino mass matrices</p><p>M</p><p>using pseudo-random numbers generated in MATLAB. These matrices are diagonalised,</p><p>and from the general notion of the Pontecorvo-Maki-Nakagawa-Sakata matrix</p><p>UPMNS,</p><p>statistical distributions of six mixing parameters are presented, namely;</p><p>12; 13; 23, the</p><p>CP-violating Dirac phase delta (</p><p>), and two Majorana CP-violating phases alpha one</p><p>and two (</p><p>1; 2). Furthermore, from the diagonal matrix resulting from the diagonalisation</p><p>of</p><p>M, masses mi, of the mass states i; i = 1; 2; 3, are taken. All data is presented</p><p>in histograms using 90 bins. This numerical approach to determining how the neutrino</p><p>mixing parameters are distributed will be discussed in section 2.10, and possible conclusions</p><p>regarding the statistical distributions of the mixing angles</p><p>12; 13, and 23, and the</p><p>masses</p><p>mi will be presented in chapter 3.</p>

corrected abstract:
<p>The theory of neutrino oscillations is covered. Also, the theory of Dirac, Majorana, and Dirac-Majorana neutrinos is presented. From this theory, and the model presented in [7], a program is constructed which creates a suitable number of neutrino mass matrices M<sub>&nu;</sub> using pseudo-random numbers generated in MATLAB. These matrices are diagonalised, and from the general notion of the Pontecorvo-Maki-Nakagawa-Sakata matrix U<sub>PMNS</sub>, statistical distributions of six mixing parameters are presented, namely; &theta;<sub>12</sub>; &theta;<sub>13</sub>; &theta;<sub>23</sub>, the CP-violating Dirac phase delta (&delta;), and two Majorana CP-violating phases alpha one and two ( &alpha;<sub>1</sub>, &alpha;<sub>2</sub>). Furthermore, from the diagonal matrix resulting from the diagonalisation of M<sub>&nu;</sub>, masses 𝑚<sub>𝑖</sub>, of the mass states &nu;<sub>i</sub>; 𝑖 = 1; 2; 3, are taken. All data is presented in histograms using 90 bins. This numerical approach to determining how the neutrino mixing parameters are distributed will be discussed in section 2.10, and possible conclusions regarding the statistical distributions of the mixing angles &theta;<sub>12</sub>, &theta;<sub>13</sub>, and &theta;<sub>23</sub>, and the masses 𝑚<sub>𝑖</sub> will be presented in chapter 3.</p>
----------------------------------------------------------------------
In diva2:634045 - missing space in title:
"Scalable Computation of Long-Range Potentialsfor Molecular Dynamics"
==>
"Scalable Computation of Long-Range Potentials for Molecular Dynamics"

abstract is: 
<p>To calculate long-range potentials in a molecular dynamics simulation, a naive approach using direct particle interactions needs a computational work of order O(<em>N</em>2). This is infeasible for larger simulations. In order to reduce this complexity and thus allow to increase the size of the simulation, several algorithms have been proposed in the last decades. This thesis first gives an overview over these algorithms and examines the advantages and disadvantages of these methods with respect to high performance computing, i.e., how well they are suited for a good scalability on a many-processor system. Two algorithms that seem well suited for this task, the Multilevel Summation Method and the Meshed Continuum Method, both of which are based on a hierarchy of multiple grids, are implemented and optimized for a massively parallel environment. The mathematical foundation as well as the implementation steps to improve the performance and scalability of the algorithms are explained in detail. Finally the algorithms were tested with up to 8192 processors at PDC. The results of these runs are presented together with an explanation of possible performance bottlenecks and a final comparison of both algorithms</p>

corrected abstract:
<p>To calculate long-range potentials in a molecular dynamics simulation, a naive approach using direct particle interactions needs a computational work of order 𝒪(𝑁<sup>2</sup>). This is infeasible for larger simulations. In order to reduce this complexity and thus allow to increase the size of the simulation, several algorithms have been proposed in the last decades. This thesis first gives an overview over these algorithms and examines the advantages and disadvantages of these methods with respect to high performance computing, i.e., how well they are suited for a good scalability on a many-processor system. Two algorithms that seem well suited for this task, the Multilevel Summation Method and the Meshed Continuum Method, both of which are based on a hierarchy of multiple grids, are implemented and optimized for a massively parallel environment. The mathematical foundation as well as the implementation steps to improve the performance and scalability of the algorithms are explained in detail. Finally the algorithms were tested with up to 8192 processors at PDC. The results of these runs are presented together with an explanation of possible performance bottlenecks and a final comparison of both algorithms.</p>
----------------------------------------------------------------------
In diva2:631612 - missing space in title:
"Pricing a basket option when volatility is capped using affinejump-diffusion models"
==>
"Pricing a basket option when volatility is capped using affine jump-diffusion models"

abstract is: 
<p>This thesis considers the price and characteristics of an exotic option called the Volatility-Cap-Target-Level(VCTL) option. The payoff function is a simple European option style but the underlying value is a dynamic portfolio which is comprised of two components: A risky asset and a non-risky asset. The non-risky asset is a bond and the risky asset can be a fund or an index related to any asset category such as equities, commodities, real estate, etc.</p><p>The main purpose of using a dynamic portfolio is to keep the realized volatility of the portfolio under control and preferably below a certain maximum level, denoted as the Volatility-Cap-Target-Level (VCTL). This is attained by a variable allocation between the risky asset and the non-risky asset during the maturity of the VCTL-option. The allocation is reviewed and if necessary adjusted every 15th day. Adjustment depends entirely upon the realized historical volatility of the risky asset.</p><p>Moreover, it is assumed that the risky asset is governed by a certain group of stochastic differential equations called affine jump-diffusion models. All models will be calibrated using out-of-the money European call options based on the Deutsche-Aktien-Index(DAX).</p><p>The numerical implementation of the portfolio diffusions and the use of Monte Carlo methods will result in different VCTL-option prices. Thus, to price a nonstandard product and to comply with good risk management, it is advocated that the financial institution use several research models such as the SVSJ- and the Seppmodel in addition to the Black-Scholes model.</p><p>Keywords: Exotic option, basket option, risk management, greeks, affine jumpdiffusions, the Black-Scholes model, the Heston model, Bates model with lognormal jumps, the Bates model with log-asymmetric double exponential jumps, the Stochastic-Volatility-Simultaneous-Jumps(SVSJ)-model, the Sepp-model.</p>

corrected abstract:
<p>This thesis considers the price and characteristics of an exotic option called the Volatility-Cap-Target-Level(VCTL) option. The payoff function is a simple European option style but the underlying value is a dynamic portfolio which is comprised of two components: A risky asset and a non-risky asset. The non-risky asset is a bond and the risky asset can be a fund or an index related to any asset category such as equities, commodities, real estate, etc.</p><p>The main purpose of using a dynamic portfolio is to keep the realized volatility of the portfolio under control and preferably below a certain maximum level, denoted as the Volatility-Cap-Target-Level (VCTL). This is attained by a variable allocation between the risky asset and the non-risky asset during the maturity of the VCTL-option. The allocation is reviewed and if necessary adjusted every 15th day. Adjustment depends entirely upon the realized historical volatility of the risky asset.</p><p>Moreover, it is assumed that the risky asset is governed by a certain group of stochastic differential equations called affine jump-diffusion models. All models will be calibrated using out-of-the money European call options based on the Deutsche-Aktien-Index(DAX).</p><p>The numerical implementation of the portfolio diffusions and the use of Monte Carlo methods will result in different VCTL-option prices. Thus, to price a nonstandard product and to comply with good risk management, it is advocated that the financial institution use several research models such as the SVSJ- and the Sepp-model in addition to the Black-Scholes model.</p>

Note: Missing "-" in "Spee-Model"
----------------------------------------------------------------------
In diva2:626254 - missing space in title:
"Non-local means denoising ofprojection images in cone beamcomputed tomography"
==>
"Non-local means denoising of projection images in cone beam computed tomography"

abstract is: 
<p>A new edge preserving denoising method is used to increase image quality in cone beam computed tomography. The reconstruction algorithm for cone beam computed tomography used by Elekta enhances high frequency image details, e.g. noise, and we propose that denoising is done on the projection images before reconstruction. The denoising method is shown to have a connection with computational statistics and some mathematical improvements to the method are considered. Comparisons are made with the state-of-theart method on both artificial and physical objects. The results show that the smoothness of the images is enhanced at the cost of blurring out image details. Some results show how the setting of the method parameters influence the trade off between smoothness and blurred image details in the images.</p>


corrected abstract:
<p>A new edge preserving denoising method is used to increase image quality in cone beam computed tomography. The reconstruction algorithm for cone beam computed tomography used by Elekta enhances high frequency image details, e.g. noise, and we propose that denoising is done on the projection images before reconstruction. The denoising method is shown to have a connection with computational statistics and some mathematical improvements to the method are considered. Comparisons are made with the state-of-the-art method on both artificial and physical objects. The results show that the smoothness of the images is enhanced at the cost of blurring out image details. Some results show how the setting of the method parameters influence the trade off between smoothness and blurred image details in the images.</p>
----------------------------------------------------------------------
In diva2:617156 - missing space and wrong version of L in title:
"L₁adaptive control of a generic fighter aircraft"
==>
"&Laplacetrf;₁ adaptive control of a generic fighter aircraft"

abstract is: 
<p>This master's thesis was performed at the section of Flight Control Systems at SAAB Aeronautics in Linköping as a part of my Master of Science in Aerospace Engineering at KTH, Stockholm. This report examines the use of L₁ adaptive control to stabilize the inner longitudinal and lateral loops of a generic fighter aircraft, in the event of failure of the system that measures current speed and altitude.</p><p>The philosophy of the L₁ adaptive controller is to decouple the adaptation from the control loop by using a state-predictor based adaptation scheme, still only compensating for the uncertainties within the bandwidth of the control channel by the use of low-pass filters.</p><p>The main goal of the project was to investigate in the tuning of the L₁ adaptive controller with respect to the nonlinear uncertainties related to the failure, and with a limited sampling rate of 60 Hz. The desired closed-loop dynamics for the statepredictor was designed by linearising the aircraft dynamics in a point in the middle of the flight envelope and placing the poles of the system with respect to flying qualities. The modified piecewise constant adaptation law was chosen as adaptation law, which achieves faster adaptation by increasing the sampling rate, yielding better performance at a given sample rate compared to the piecewise constant adaptation law [1]. All the states were transformed to discrete time in order to be implementable digitally.</p><p>Results have shown that augmenting a state-feedback controller with a L₁ adaptive controller increases robustness in the whole flying envelope, with good flying qualities. Problems were discovered in the low speed regions of the envelope, where the L₁ adaptive controller did not provide the desired performance. A switching scheme between two L1 adaptive controllers was examined. The switch between the controllers was done by knowing when the landing gear was up or down. The second state-predictor was designed with linearised dynamics in landing speed and altitude. The switching scheme was own in a simulator with a nonlinear generic fighter aircraft model with good results</p><p></p>

corrected abstract:
<p>This master's thesis was performed at the section of Flight Control Systems at SAAB Aeronautics in Linköping as a part of my Master of Science in Aerospace Engineering at KTH, Stockholm. This report examines the use of &Laplacetrf;₁ adaptive control to stabilize the inner longitudinal and lateral loops of a generic fighter aircraft, in the event of failure of the system that measures current speed and altitude.</p><p>The philosophy of the &Laplacetrf;₁ adaptive controller is to decouple the adaptation from the control loop by using a state-predictor based adaptation scheme, still only compensating for the uncertainties within the bandwidth of the control channel by the use of low-pass filters.</p><p>The main goal of the project was to investigate in the tuning of the &Laplacetrf;₁ adaptive controller with respect to the nonlinear uncertainties related to the failure, and with a limited sampling rate of 60 Hz. The desired closed-loop dynamics for the state predictor was designed by linearising the aircraft dynamics in a point in the middle of the flight envelope and placing the poles of the system with respect to flying qualities. The <em>modified</em> piecewise constant adaptation law was chosen as adaptation law, which achieves faster adaptation by increasing the sampling rate, yielding better performance at a given sample rate compared to the piecewise constant adaptation law [1]. All the states were transformed to discrete time in order to be implementable digitally.</p><p>Results have shown that augmenting a state-feedback controller with a &Laplacetrf;₁ adaptive controller increases robustness in the whole flying envelope, with good flying qualities. Problems were discovered in the low speed regions of the envelope, where the &Laplacetrf;₁ adaptive controller did not provide the desired performance. A switching scheme between two &Laplacetrf;₁ adaptive controllers was examined. The switch between the controllers was done by knowing when the landing gear was up or down. The second state-predictor was designed with linearised dynamics in landing speed and altitude. The switching scheme was flown in a simulator with a nonlinear generic fighter aircraft model with good results.</p>
----------------------------------------------------------------------
In diva2:608435 - missing space in title:
"CFD Analysis on the Main-Rotor  Blade ofa Scale Helicopter  Model using Overset Meshing"
==>
"CFD Analysis on the Main-Rotor  Blade of a Scale Helicopter  Model using Overset Meshing"

abstract is: 
<p>In this paper, an analysis in computational uid dynamics (CFD) is presented on a helicopter scale model with focus on the main-rotor blades.The helicopter model is encapsulated in a background region and the ow eld is solved using Star CCM+. A surface and volume mesh continuum was generated that contained approximately seven million polyhedral cells, where the Finite Volume Method (FVM) was chosen as a discretization technique.</p><p>Each blade was assigned to an overset region making it possible to rotate and add a cyclic pitch motion. Boundary information was exchanged between the overset and background mesh using a weighted interpolation method between cells.</p><p>An implicit unsteady ow solver, with an ideal gas and a SST (Mentar) K-Omega turbulence model were used. Hover and forward cases were examined. Forward ight cases were done by changing the rotor shaft angle of attacks and the collective pitch angle 0 at the helicopter freestream Mach number of M = 0:128, without the inclusion of a cyclic pitch motion. An additional ight case with cyclic pitch motion was examined at s = 0 and = 0.</p><p>Each simulation took roughly 48 hours with a total of 96 parallel cores to compute. Experimental data were taken from an existing NASA report for comparison of the results. Hover ight coincided well with the wind tunnel data. The forward ight cases (with no cyclic motion) produced lift matching the experimental data, but had diculties in producing a forward thrust. Moments in roll and pitch started to emerge. By adding a cyclic pitch successfully removed the pitch and roll moments. In conclusion this shows that applying overset meshes as a way to analyze the main-rotor blades using CFD does work. Adding a cyclic pitch motion at 0 = 5 and s = 0 successfully removed the roll and pitching moment from the results.</p>

corrected abstract:
<p>In this paper, an analysis in computational fluid dynamics (CFD) is presented on a helicopter scale model with focus on the main-rotor blades.</p><p>The helicopter model is encapsulated in a background region and the flow field is solved using Star CCM+. A surface and volume mesh continuum was generated that contained approximately seven million polyhedral cells, where the Finite Volume Method (FVM) was chosen as a discretization technique. Each blade was assigned to an overset region making it possible to rotate and add a cyclic pitch motion. Boundary information was exchanged between the overset and background mesh using a weighted interpolation method between cells.</p><p>An implicit unsteady flow solver, with an ideal gas and a SST (Mentar) K-Omega turbulence model were used. Hover and forward cases were examined. Forward flight cases were done by changing the rotor shaft angle of attack <em>&alpha;</em><sub>s</sub> and the collective pitch angle <em>&theta;</em><sub>0</sub> at the helicopter freestream Mach number of M = 0.128, without the inclusion of a cyclic pitch motion. An additional flight case with cyclic pitch motion was examined at <em>&alpha;</em><sub>s</sub> = 0&deg; and <em>&theta;</em>= 0&deg;.</p><p>Each simulation took roughly 48 hours with a total of 96 parallel cores to compute. Experimental data were taken from an existing NASA report for comparison of the results. Hover flight coincided well with the wind tunnel data. The forward flight cases (with no cyclic motion) produced lift matching the experimental data, but had difficulties in producing a forward thrust. Moments in roll and pitch started to emerge. By adding a cyclic pitch successfully removed the pitch and roll moments.</p><p>In conclusion this shows that applying overset meshes as a way to analyze the main-rotor blades using CFD does work. Adding a cyclic pitch motion at <em>&theta;</em><sub>0</sub> = 5&deg; and <em>&alpha;</em><sub>s</sub> = 0&deg; successfully removed the roll and pitching moment from the results.</p>
----------------------------------------------------------------------
In diva2:515500 
abstract is: 
<p>There are today large amounts of contaminated sediments in the Baltic Sea. These contaminants are the result of many years industrial activity where the contaminants have not been taken care of and just released out in the water. When ports want to dredge in their fairways and harbor areas the contaminated sediments need to be considered since they are a environmental risk. The development in this area has been slow and new techniques have just recently made its way into dredging operations in the Baltic Sea region. As a result of this there are no clear methods for implementing new techniques today. This study aimed to bring light to which factors are important when it comes to which technique is used and what would make the users want to invest in new techniques. In order to find a method for how implementation of new techniques for dredging and management of contaminated sediments this study has looked into which factors are of most importance when choosing technique. It has also aimed to answer which stakeholders have the biggest influence in the choice.This work has been done as a part of the Baltic Sea region project SMOCS together with the company Ecoloop. The result is based on an interview study with 3 ports in Sweden, 3 Swedish contractors, 4 Swedish government authorities as well as the port of Gdynia and port of Kokkola.This study has shown that the most important factors when it comes to which technique is used are the economical and environmental aspects as well as the relation between the two. It seems as though a common goal has not been set when it comes to dredging and how we could treat contaminated sediments. Ports and contractors do not share a common goal with the government authorities. The key stakeholders in the dredging process are the port, the contractors, the government authorities as well as the consultants. However Swedish ports only perform larger dredging operations when there is a need for it. When implementing a new technique focus should therefore be put on the other stakeholders who come in contact with dredging operations more frequently.</p>

corrected abstract:
<p>There are today large amounts of contaminated sediments in the Baltic Sea. These contaminants are the result of many years industrial activity where the contaminants have not been taken care of and just released out in the water. When ports want to dredge in their fairways and harbor areas the contaminated sediments need to be considered since they are a environmental risk. The development in this area has been slow and new techniques have just recently made its way into dredging operations in the Baltic Sea region. As a result of this there are no clear methods for implementing new techniques today. This study aimed to bring light to which factors are important when it comes to which technique is used and what would make the users want to invest in new techniques. In order to find a method for how implementation of new techniques for dredging and management of contaminated sediments this study has looked into which factors are of most importance when choosing technique. It has also aimed to answer which stakeholders have the biggest influence in the choice.</p><p>This work has been done as a part of the Baltic Sea region project SMOCS together with the company Ecoloop. The result is based on an interview study with 3 ports in Sweden, 3 Swedish contractors, 4 Swedish government authorities as well as the port of Gdynia and port of Kokkola.</p><p>This study has shown that the most important factors when it comes to which technique is used are the economical and environmental aspects as well as the relation between the two. It seems as though a common goal has not been set when it comes to dredging and how we could treat contaminated sediments. Ports and contractors do not share a common goal with the government authorities. The key stakeholders in the dredging process are the port, the contractors, the government authorities as well as the consultants. However Swedish ports only perform larger dredging operations when there is a need for it. When implementing a new technique focus should therefore be put on the other stakeholders who come in contact with dredging operations more frequently.</p>
----------------------------------------------------------------------
In diva2:460076 
abstract is: 
<p>The aim of this thesis project was to create a model of engine oil deterioration, suitable for implementation in the engine control unit. To this end, the model has to be sufficiently simple not to unnecessarily waste limited computing power and yet exact enough not to over- or underestimate the oil change interval too much. Too frequent oil changes mean an unnecessary extra cost for new oil and loss of availability while too long oil change intervals may damage the engine, at an even greater cost. The model was divided into four sub-models, one for each of the processes that cause oil deterioration, i.e. dilution by fuel, chemical degradation, soot contamination and dilution by water. The models created were based on publicized scientific studies of the various processes. In order to validate the models they were implemented in Matlab and compared with experiment data. Various modification attempts were made and the resulting models were evaluated based on their correspondence with measurements.</p>
<p>Dilution by fuel, dilution by water and soot contamination are all processes whose causes and dynamics are relatively well known. Oil degradation on the other hand is a complex process that depends on a large number of factors which makes it very complicated to model. This is especially problematic as oil degradation usually is limiting for oil change intervals.</p>
<p>Out of the four sub-models that were created, the model for fuel dilution gave the best correspondence with measurements. However data was only available for the part of the model handling evaporation and thus it is unknown how well the model for fuel influx performs. As the model is very simple there is reason to believe that the correspondence between the modelled influx and the actual influx is not very good. The model for oil degradation could not be validated using the available data and further experiments are necessary. However it was concluded from the data available that a model based only on oil temperature, as was first intended, will not be sufficient. The water dilution model was reasonably accurate in predicting average water concentrations, but missed short term variations completely. In order to fully validate the model, further experiments, where the various rates of condensation are measured, will be necessary. The model for oil-soot already available at Scania was considered sufficient and no further work has been done on this model. A short description of the model has been included for completeness.</p>

corrected abstract:
<p>The aim of this thesis project was to create a model of engine oil deterioration, suitable for implementation in the engine control unit. To this end, the model has to be sufficiently simple not to unnecessarily waste limited computing power and yet exact enough not to over- or underestimate the oil change interval too much. Too frequent oil changes mean an unnecessary extra cost for new oil and loss of availability while too long oil change intervals may damage the engine, at an even greater cost. The model was divided into four sub-models, one for each of the processes that cause oil deterioration, i.e. dilution by fuel, chemical degradation, soot contamination and dilution by water. The models created were based on publicized scientific studies of the various processes. In order to validate the models they were implemented in Matlab and compared with experiment data. Various modification attempts were made and the resulting models were evaluated based on their correspondence with measurements.</p><p>Dilution by fuel, dilution by water and soot contamination are all processes whose causes and dynamics are relatively well known. Oil degradation on the other hand is a complex process that depends on a large number of factors which makes it very complicated to model. This is especially problematic as oil degradation usually is limiting for oil change intervals.</p><p></p><p>Out of the four sub-models that were created, the model for fuel dilution gave the best correspondence with measurements. However data was only available for the part of the model handling evaporation and thus it is unknown how well the model for fuel influx performs. As the model is very simple there is reason to believe that the correspondence between the modelled influx and the actual influx is not very good.</p><p>The model for oil degradation could not be validated using the available data and further experiments are necessary. However it was concluded from the data available that a model based only on oil temperature, as was first intended, will not be sufficient.</p><p>The water dilution model was reasonably accurate in predicting average water concentrations, but missed short term variations completely. In order to fully validate the model, further experiments, where the various rates of condensation are measured, will be necessary.</p><p>The model for oil-soot already available at Scania was considered sufficient and no further work has been done on this model. A short description of the model has been included for completeness.</p>
----------------------------------------------------------------------
In diva2:458362 - redundant colon in title:
"Simulation Method Development of Ultra Thick Laminates:: with Cohesive Zone Method and Empirical Arcan Tests"
==>
"Simulation Method Development of Ultra Thick Laminates: with Cohesive Zone Method and Empirical Arcan Tests"

Abstract - correct as is
----------------------------------------------------------------------
In diva2:441553 
abstract is: 
<p>The global understanding that natural resources and non renewable energy sources are not inexhaustible has been growing lately together with the increase of conscientiousness on the consequences that our demanding way of life has on the environment. Global warming, ozone layer depletion, the greenhouse effect or the acid rain, are some of these consequences, which may reach catastrophic levels if nothing is done to emend the actual situation. Lately, society is beginning to see sustainability not only as a needed requirement but as a distinctive value which has to be pursued by the different areas of society involved and responsible for a sustainable development such as public administration and companies, engineers and researchers. As a fundamental part of society, infrastructures have utmost importance in sustainable development. Even more when it comes to rail transport infrastructure, given the important role of rail transport in the development of a sustainable society. That is why engineers should make an effort to use all the tools available to choose the best structural design, which not only meets structural requirements, but has also a good performance for the environment. To do so, engineers must focus on using renewable sources or energy and materials, increasing the life of the existing infrastructures, making them more durable. When it comes to railway bridges, it is preferable to reuse and adapt existing structures than tear them down to build new ones.</p>
<p>In this line, environmental assessment methodologies provide an incredibly valuable tool for help decision-makers and engineers to identify and select the best alternative design regarding environmental issues. Therefore, it is important to count on a common basis and established criteria together with a systematic methodology in order to obtain reliable results to compare alternatives and make the right decisions. However, nowadays, there exists very little guidance to perform this kind of analysis, and an extensive variety of databases and methodologies non standardized, which leads to uncertainties when it comes to evaluate and compare the obtained results.</p>
<p>This thesis means to be a good guide for engineers, when performing a Life Cycle Assessment of a railway bridge, and to become a useful tool to compare several alternatives to identify the best option relating the environmental burdens involved. With this purpose, in order to know the state of the art of LCA methodology, it has been studied a wide range of existing literature and previous studies performed to analyze bridges and building materials. Finally, it has been developed an own methodology based on all the research done before, and implemented in an Excel application program based on Visual Basic macros, which means to be easy to use with a simple user interface, and to provide reliable results. The application is useful for assessing, repair or improving existing bridges, where the amounts of materials and energy are known, but can also be helpful in the design phase to compare different alternatives. It also allows using different weighting methodologies according to several reference sources depending on the case of study.</p>
<p>The application is tested by carrying out a Life Cycle Assessment of a Spanish railway bridge located in the city center of Vitoria-Gasteiz, evaluating the different structures that conform the bridge system thorough all the stages of its life cycle identifying the most contributive parameters to the environmental impacts. The study was carried out over a 100 year time horizon. In the case of performing the LCA of this particular bridge, the contribution of the whole bridge is taken into consideration. When comparing two different bridges, the application has the option to compare them in the same basis, dividing by length and width of the bridge, which is a helpful tool if both bridges are not the same size. All stages of the life cycle were considered: the material stage, construction, the use and maintenance stage, and the end of life. The material stage includes the raw material extraction, production and distribution. The construction stage accounts the diesel, electricity and water consumption during construction activities. The use and maintenance stage covers the reparation and replacing operations. And the end of life covers several scenarios. In this case of study, in order not to interrupt the rail traffic, the bridge was constructed parallel to its final location, and then moved into the right place with hydraulic jacks. This leads to an important auxiliary structure with its own foundations, which has a significant contribution to the overall environmental impact. The scenario chosen for the end of life was based on similar actuation in other constructions in the proximities of the bridge, as the bridge is already in use. These assumptions were to recycle 70 % of the concrete and 90 % of the steel; all the wood used for formwork was disposed as landfill.</p>
<p>The results obtained, weighted according to the US Environmental Protection Agency, shows that the main contributor to the environmental impacts is the material phase, with the 64 % of the total weighted results with concrete and steel production as principal factors, followed by timber production. These processes account great amounts of CO 2emissions, which makes essential to focus on reducing the impact of the material processes by optimizing the processes but mainly by reusing materials from other constructions as much as it may be possible. The maintenance activities have some importance due to the frequency of the track replacement, assumed to be once every 25 years. While construction does not imply great burdens for the environment, the end of life causes the 33 % of the overall bridge impact. This is due to the timber formwork disposal as landfill and to a lesser extent because of the recycling of the steel. The timber disposal increases widely the eutrophication effect, and will be easy to be reused in further constructions. Regarding the different parts of the bridge structure, the auxiliary structure has an important contribution with the 61 % of the overall weighted impact. As it is a concrete bridge, both the substructure and superstructure has similar contribution. The substructure has a slightly higher impact with the 21 % and the superstructure the 15 %. Rail structure and transport have very little contribution.</p>

corrected abstract:
<p>The global understanding that natural resources and non renewable energy sources are not inexhaustible has been growing lately together with the increase of conscientiousness on the consequences that our demanding way of life has on the environment. Global warming, ozone layer depletion, the greenhouse effect or the acid rain, are some of these consequences, which may reach catastrophic levels if nothing is done to emend the actual situation. Lately, society is beginning to see sustainability not only as a needed requirement but as a distinctive value which has to be pursued by the different areas of society involved and responsible for a sustainable development such as public administration and companies, engineers and researchers. As a fundamental part of society, infrastructures have utmost importance in sustainable development. Even more when it comes to rail transport infrastructure, given the important role of rail transport in the development of a sustainable society. That is why engineers should make an effort to use all the tools available to choose the best structural design, which not only meets structural requirements, but has also a good performance for the environment. To do so, engineers must focus on using renewable sources or energy and materials, increasing the life of the existing infrastructures, making them more durable. When it comes to railway bridges, it is preferable to reuse and adapt existing structures than tear them down to build new ones.</p><p>In this line, environmental assessment methodologies provide an incredibly valuable tool for help decision-makers and engineers to identify and select the best alternative design regarding environmental issues. Therefore, it is important to count on a common basis and established criteria together with a systematic methodology in order to obtain reliable results to compare alternatives and make the right decisions. However, nowadays, there exists very little guidance to perform this kind of analysis, and an extensive variety of databases and methodologies non standardized, which leads to uncertainties when it comes to evaluate and compare the obtained results.</p><p>This thesis means to be a good guide for engineers, when performing a Life Cycle Assessment of a railway bridge, and to become a useful tool to compare several alternatives to identify the best option relating the environmental burdens involved. With this purpose, in order to know the state of the art of LCA methodology, it has been studied a wide range of existing literature and previous studies performed to analyze bridges and building materials. Finally, it has been developed an own methodology based on all the research done before, and implemented in an Excel application program based on Visual Basic macros, which means to be easy to use with a simple user interface, and to provide reliable results. The application is useful for assessing, repair or improving existing bridges, where the amounts of materials and energy are known, but can also be helpful in the design phase to compare different alternatives. It also allows using different weighting methodologies according to several reference sources depending on the case of study.</p><p>The application is tested by carrying out a Life Cycle Assessment of a Spanish railway bridge located in the city center of Vitoria-Gasteiz, evaluating the different structures that conform the bridge system thorough all the stages of its life cycle identifying the most contributive parameters to the environmental impacts. The study was carried out over a 100 year time horizon. In the case of performing the LCA of this particular bridge, the contribution of the whole bridge is taken into consideration. When comparing two different bridges, the application has the option to compare them in the same basis, dividing by length and width of the bridge, which is a helpful tool if both bridges are not the same size. All stages of the life cycle were considered: the material stage, construction, the use and maintenance stage, and the end of life. The material stage includes the raw material extraction, production and distribution. The construction stage accounts the diesel, electricity and water consumption during construction activities. The use and maintenance stage covers the reparation and replacing operations. And the end of life covers several scenarios. In this case of study, in order not to interrupt the rail traffic, the bridge was constructed parallel to its final location, and then moved into the right place with hydraulic jacks. This leads to an important auxiliary structure with its own foundations, which has a significant contribution to the overall environmental impact. The scenario chosen for the end of life was based on similar actuation in other constructions in the proximities of the bridge, as the bridge is already in use. These assumptions were to recycle 70 % of the concrete and 90 % of the steel; all the wood used for formwork was disposed as landfill.</p><p>The results obtained, weighted according to the US Environmental Protection Agency, shows that the main contributor to the environmental impacts is the material phase, with the 64 % of the total weighted results with concrete and steel production as principal factors, followed by timber production. These processes account great amounts of CO<sub>2</sub> emissions, which makes essential to focus on reducing the impact of the material processes by optimizing the processes but mainly by reusing materials from other constructions as much as it may be possible. The maintenance activities have some importance due to the frequency of the track replacement, assumed to be once every 25 years. While construction does not imply great burdens for the environment, the end of life causes the 33 % of the overall bridge impact. This is due to the timber formwork disposal as landfill and to a lesser extent because of the recycling of the steel. The timber disposal increases widely the eutrophication effect, and will be easy to be reused in further constructions. Regarding the different parts of the bridge structure, the auxiliary structure has an important contribution with the 61 % of the overall weighted impact. As it is a concrete bridge, both the substructure and superstructure has similar contribution. The substructure has a slightly higher impact with the 21 % and the superstructure the 15 %. Rail structure and transport have very little contribution.</p>
----------------------------------------------------------------------
In diva2:439914 
abstract is: 
<p>Polymer based porous materials largely exhibit viscoelastic properties which is a consequence of the viscoelastic nature of the constituent solid. If the constitutive relation for the constituent solid is known, then it is of interest to investigate how this constitutive relation of the solid at the microscale influences the macroscopic properties of the porous structure. In the present work porous structures are studied with the assumption that the constitutive solid is isotropic and that it also exhibits non-proportional damping characteristics. Non-proportional damping here refers to the dissimilarity in the frequency dependencies of the different complex elastic moduli of the constituent solid.Two different kinds of porous structures are investigated: pseudo random periodic microstructure and the other a pseudo random non-periodic microstructure. Both the structures are based on the implementation of Voronoi tessellations in 2D Euclidean space. A representative volume element(RVE) or a unit cell approach is adapted to analyse the properties of the porous structure. Periodic boundary conditions are implemented on the RVE in case of the periodic microstructure while a less elegant approach of using a large enough element size and measuring the stress strain fields at the interior boundaries is adapted for the non-periodic structure. A direct homogenisation technique based on the volume averaging of the micro stress and strain fields is used to estimate the macro level stress and strain fields. These macro fields are then used for determining the complex elastic moduli of the porous frame structures. Finally the results reveal that in spite of the assumption of non-proportional damping for the constituent solid, the porous frame structure exhibits proportional damping for structures of high porosity, thus possibly justifying the assumptions of proportional damping for a porous structure with sufficiently high porosity.</p>

corrected abstract:
<p>Polymer based porous materials largely exhibit viscoelastic properties which is a consequence of the viscoelastic nature of the constituent solid. If the constitutive relation for the constituent solid is known, then it is of interest to investigate how this constitutive relation of the solid at the micro scale influences the macroscopic properties of the porous structure. In the present work porous structures are studied with the assumption that the constitutive solid is isotropic and that it also exhibits non-proportional damping characteristics. Non-proportional damping here refers to the dissimilarity in the frequency dependencies of the different complex elastic moduli of the constituent solid.</p><p>Two different kinds of porous structures are investigated: pseudo random periodic microstructure and the other a pseudo random non-periodic microstructure. Both the structures are based on the implementation of Voronoi tessellations in 2D Euclidean space. A representative volume element (RVE) or a unit cell approach is adapted to analyse the properties of the porous structure. Periodic boundary conditions are implemented on the RVE in case of the periodic microstructure while a less elegant approach of using a large enough element size and measuring the stress strain fields at the interior boundaries is adapted for the non-periodic structure.</p><p>A direct homogenisation technique based on the volume averaging of the micro stress and strain fields is used to estimate the macro level stress and strain fields. These macro fields are then used for determining the complex elastic moduli of the porous frame structures. Finally the results reveal that in spite of the assumption of non-proportional damping for the constituent solid, the porous frame structure exhibits proportional damping for structures of high porosity, thus possibly justifying the assumptions of proportional damping for a porous structure with sufficiently high porosity.</p>
----------------------------------------------------------------------
In diva2:439838 
abstract is: 
<p>Turbochargers are now commonly used in modern automotive engines, which increase the density of air entering the engine to produce more power. This device not only greatly improves the degree of engine efficiency, but also reduces the pollutant emissions. However, one of the important issues which must be considered is the noise from turbochargers. This noise can radiate either after propagation through the intake and exhaust ducts or via induced vibrations in the turbocharger housing. Here the in duct sound will be studied and normally this cause problems mainly on the intake side where there is less silencing available. In particular noise from high frequency tones as well as surge noise phenomena can cause noise problems.The main aerodynamic noise from centrifugal compressors are blade tone noise, buzz-saw noise and flow separation (or surge) noise. For instance the tones occurring at blade passing frequencies can be a problem for large turbochargers,e.g., as found on trucks, where they lie in the range around 10 kHz. The most efficient way to achieve quieter machines is the reduction of the sources, therefore it is important to be able to measure and investigate the sound generation ofturbo chargers.In this thesis work the sound generated has been investigated using a dedicated turbocharger acoustic test rig. The main purpose is to investigate methods for estimating the in-duct acoustic power using a micro-phone array. Three differen tmodels are proposed to estimate the power in the propagating acoustic modes and to suppress the flow noise. In order to test the proposed methods experiments have been performed on an automotive compressor at four different operating conditions. The results are compared with the literature in order to identify the dominating sound generation mechanisms.</p>

corrected abstract:
<p>Turbochargers are now commonly used in modern automotive engines, which increase the density of air entering the engine to produce more power. This device not only greatly improves the degree of engine efficiency, but also reduces the pollutant emissions. However, one of the important issues which must be considered is the noise from turbochargers. This noise can radiate either after propagation through the intake and exhaust ducts or via induced vibrations in the turbocharger housing. Here the in duct sound will be studied and normally this cause problems mainly on the intake side where there is less silencing available. In particular noise from high frequency tones as well as surge noise phenomena can cause noise problems.</p><p>The main aerodynamic noise from centrifugal compressors are blade tone noise, buzz-saw noise and flow separation (or surge) noise. For instance the tones occurring at blade passing frequencies can be a problem for large turbochargers, e.g., as found on trucks, where they lie in the range around 10 kHz. The most efficient way to achieve quieter machines is the reduction of the sources, therefore it is important to be able to measure and investigate the sound generation of turbochargers.</p><p>In this thesis work the sound generated has been investigated using a dedicated turbocharger acoustic test rig. The main purpose is to investigate methods for estimating the in-duct acoustic power using a micro-phone array. Three different models are proposed to estimate the power in the propagating acoustic modes and to suppress the flow noise. In order to test the proposed methods experiments have been performed on an automotive compressor at four different operating conditions. The results are compared with the literature in order to identify the dominating sound generation mechanisms.</p>
----------------------------------------------------------------------
In diva2:371655 
abstract is: 
<p>Rail wear can result in extensive costs for the track owner if it is not predicted and preventedin an efficient way. To limit these costs, one measure is to predict rail wear through wear simulations. The purpose with this work is to perform simulations of successive rail wear on the Swedish light rail line Tvärbanan in Stockholm, by means of the track-vehicle dynamics software GENSYS in combination with a wear calculation program developed in MATLAB.</p>
<p>The simulation procedure is based on a methodology with a simulation set design, where the simulations to be performed are selected through a parametric study. The simulations include track-vehicle simulations, where the wheel-rail contact is modelled according to the Hertzian contact theory together with Kalker’s simplified theory (including the numerical algorithm FASTSIM). The results from the track-vehicle simulations serve as input to the wear calculations. When modelling rail wear Archard’s wear model has been used, including wear coefficients based on laboratory measurements. The measurements have been performed under dry conditions, so the wear coefficients under lubricated conditions (both natural and deliberate lubrication) are reduced by factors estimated by field observations. After the wear depth calculations the wear distribution is smoothed and the rail profile is updated. The simulation procedure continues with a new wear step as long as the desired tonnage is not attained.</p>
<p>Four curves of Tvärbanan with different curve radii, ranging from 85 to 410 m, have beenstudied in this work. On three of the curves the high rail is deliberately lubricated, whereas no lubrication has been applied in the widest curve. The vehicle operating the light rail line is an articulated tram with two motor end bogies and one intermediate trailer bogie. The line was opened in August 1999 and extended in one direction one year later. Rail profile measurements have been carried out by SL since March 2002. The traffic tonnage at the selected sites from the opening of the line to the last measurement occasion (September2004) is at most 8.9 mega gross ton per track.</p>
<p>The results of the rail wear prediction tool are evaluated by comparing worn-off area of the simulated rail profiles with that of the measured rail profiles. Simulated and measured resultsdo not agree too well, since the simulated rail wear is more extensive than the measured one, especially on the outer rail. However, the shapes of the simulated worn rail profiles are comparable to those of the measured rail profiles.</p>

corrected abstract:
<p>Rail wear can result in extensive costs for the track owner if it is not predicted and prevented in an efficient way. To limit these costs, one measure is to predict rail wear through wear simulations.</p><p>The purpose with this work is to perform simulations of successive rail wear on the Swedish light rail line Tvärbanan in Stockholm, by means of the track-vehicle dynamics software GENSYS in combination with a wear calculation program developed in MATLAB.</p><p>The simulation procedure is based on a methodology with a simulation set design, where the simulations to be performed are selected through a parametric study. The simulations include track-vehicle simulations, where the wheel-rail contact is modelled according to the Hertzian contact theory together with Kalker’s simplified theory (including the numerical algorithm FASTSIM). The results from the track-vehicle simulations serve as input to the wear calculations.</p><p>When modelling rail wear Archard’s wear model has been used, including wear coefficients based on laboratory measurements. The measurements have been performed under dry conditions, so the wear coefficients under lubricated conditions (both natural and deliberate lubrication) are reduced by factors estimated by field observations.</p><p>After the wear depth calculations the wear distribution is smoothed and the rail profile is updated. The simulation procedure continues with a new wear step as long as the desired tonnage is not attained.</p><p>Four curves of Tvärbanan with different curve radii, ranging from 85 to 410 m, have been studied in this work. On three of the curves the high rail is deliberately lubricated, whereas no lubrication has been applied in the widest curve. The vehicle operating the light rail line is an articulated tram with two motor end bogies and one intermediate trailer bogie.</p><p>The line was opened in August 1999 and extended in one direction one year later. Rail profile measurements have been carried out by SL since March 2002. The traffic tonnage at the selected sites from the opening of the line to the last measurement occasion (September 2004) is at most 8.9 mega gross ton per track.</p><p>The results of the rail wear prediction tool are evaluated by comparing worn-off area of the simulated rail profiles with that of the measured rail profiles. Simulated and measured results do not agree too well, since the simulated rail wear is more extensive than the measured one, especially on the outer rail. However, the shapes of the simulated worn rail profiles are comparable to those of the measured rail profiles.</p>
----------------------------------------------------------------------
In diva2:1904651 
abstract is: 
<p>The analysis of calcium signalling in various biological processes is crucial, as the calcium ion acts as a secondary messenger in signal transduction pathways, including regulating cell migrations and communications. Deviations in these calcium signals can lead to pathological conditions, making it a significant subject in wound healing and cancer therapy.This thesis project analyses how the cardiotonic steroid ouabain, via its interaction with the Na/K-ATPase pump, regulates calcium signalling and cell migration in Madin-Darby Canine Kidney II (MDCK II) cells expressing GCaMP6m. Utilising a combination of live cell experiments, fluorescence microscopy, and time-lapse imaging, this research investigates the dose- dependent effects of ouabain on intracellular calcium activities and migration dynamics using a range of concentrations: 1 nM, 10 nM, 100 nM, and 1000 nM. The advanced segmentation tool CellPose was employed for tracking individual cells and analysing their intracellular calcium concentrations, the Wound Healing Size tool for quantifying wound closure rates, and kymographs for getting an insight on cellular dynamics at the leading edge.This research demonstrated enhanced calcium signalling and migration at low concentrations of ouabain in both mini-colonies and scratch wounds, likely due to partial inhibition of Na/K-ATPase. In contrast, higher concentrations exhibited greater inhibitory effects, showcasing a dose- dependent effect of ouabain on calcium signalling. Despite the high variability observed, these results do highlight ouabain’s potential as a therapeutic agent in regulating cellular movements in pathological conditions.</p>

corrected abstract:
<p>The analysis of calcium signalling in various biological processes is crucial, as the calcium ion acts as a secondary messenger in signal transduction pathways, including regulating cell migrations and communications. Deviations in these calcium signals can lead to pathological conditions, making it a significant subject in wound healing and cancer therapy.</p><p>This thesis project analyses how the cardiotonic steroid ouabain, via its interaction with the Na/K-ATPase pump, regulates calcium signalling and cell migration in Madin-Darby Canine Kidney II (MDCK II) cells expressing GCaMP6m. Utilising a combination of live cell experiments, fluorescence microscopy, and time-lapse imaging, this research investigates the dose-dependent effects of ouabain on intracellular calcium activities and migration dynamics using a range of concentrations: 1 nM, 10 nM, 100 nM, and 1000 nM. The advanced segmentation tool CellPose was employed for tracking individual cells and analysing their intracellular calcium concentrations, the Wound Healing Size tool for quantifying wound closure rates, and kymographs for getting an insight on cellular dynamics at the leading edge.</p><p>This research demonstrated enhanced calcium signalling and migration at low concentrations of ouabain in both mini-colonies and scratch wounds, likely due to partial inhibition of Na/K-ATPase. In contrast, higher concentrations exhibited greater inhibitory effects, showcasing a dose-dependent effect of ouabain on calcium signalling. Despite the high variability observed, these results do highlight ouabain’s potential as a therapeutic agent in regulating cellular movements in pathological conditions.</p>
----------------------------------------------------------------------
In diva2:1900962   - correct as is
----------------------------------------------------------------------
In diva2:1900930 
abstract is: 
<p>Particulate matter (PM) emissions from air transportation impact the global environment and health. A sustainable aviation fuel (SAF) blend is one of the alternatives to reduce the emissions. However, there has been a challenge in performing experimental estimations and analyses of PM emissions of SAF blends with respect to conventional fuel at altitudes above 3000 ft. This thesis presents an implementation of integration of existing methods to simulate, calculate and analyse non-volatile particulate matter (nvPM) emissions. The focus is on EI values at each flight segment and total emissions, using SAF blends of 0%, 20%, 50% and 70% at altitude. The simulations are based on same fuel properties for both jet fuel and SAF. The greatest percentage reduction of nvPM emissions observed in descent phase, is 60.9% in EI mass and 52.3% in EI number. This is observed at 70% SAF blend. These analyses were conducted on the LEAP-1A35A engine using FDR dataset. Moreover, a case study of two aircraft, the A220-100 and A320neo, has been performed equipped with PW1524G and PW1127G-JM engines, respectively. This analysis based on Flightradar24 data allowed for a direct comparison of engine performance and EI value characteristics under similar flight conditions. The results from these two engines show a similar trend to the one using FDR. Comparatively, the PW1127G-JM showed the greatest potential for significant reduction in EI values through the use of SAF.</p>

corrected abstract:
<p>Particulate matter (PM) emissions from air transportation impact the global environment and health. A sustainable aviation fuel (SAF) blend is one of the alternatives to reduce the emissions. However, there has been a challenge in performing experimental estimations and analyses of PM emissions of SAF blends with respect to conventional fuel at altitudes above 3000 ft. This thesis presents an implementation of integration of existing methods to simulate, calculate and analyse non-volatile particulate matter (nvPM) emissions. The focus is on EI values at each flight segment and total emissions, using SAF blends of 0 %, 20 %, 50 % and 70 % at altitude. The simulations are based on same fuel properties for both jet fuel and SAF. The greatest percentage reduction of nvPM emissions observed in descent phase, is 60.9 % in EI mass and 52.3 % in EI number. This is observed at 70 % SAF blend. These analyses were conducted on the LEAP-1A35A engine using FDR dataset. Moreover, a case study of two aircraft, the A220-100 and A320neo, has been performed equipped with PW1524G and PW1127G-JM engines, respectively. This analysis based on Flightradar24 data allowed for a direct comparison of engine performance and EI value characteristics under similar flight conditions. The results from these two engines show a similar trend to the one using FDR. Comparatively, the PW1127G-JM showed the greatest potential for significant reduction in EI values through the use of SAF.</p>
----------------------------------------------------------------------
In diva2:1896066 
abstract is: 
<p>This thesis introduces a multi-fidelity approach to optimize ship hull design, leveraging both low-fidelity (LoFi) and high-fidelity (HiFi) viscous Reynolds-Averaged Navier-Stokes (RANS) simulations. This study primarily aims to devise a method that reduces both computational cost and time in CFD-based hull design optimization, particularly when viscous effects are significant and the Boundary Element Method (BEM), though less computationally intensive, falls short in accuracy and reliability. Traditional approaches in hull design optimization predominantly rely on either BEM or HiFi RANS simulations. While these methods are accurate, they are either limited in use cases or computationally expensive and time-consuming. This research proposes a framework where a Pareto optimization using a Multi-Objective Genetic Algorithm is conducted using a Multi-fidelity model which combines high-fidelity models and LoFi models in order to achieve accuracy at a reasonable cost. The validation of the optimization results is performed using a reference study, a Pareto optimization using the sea trial and tow tank validated marine CFD reference simulation setup by FINE™/Marine. This validation ensures the robustness and reliability of the simulation results, particularly in the absence of experimental data. Illustrated through case studies, the thesis explores the optimization of hydrofoils and multi-hull hulls. In such scenarios, a comprehensive analysis of the flow is required. Drawing upon existing literature, viscous flow base methods are identified as a more appropriate and accurate technique for modelling turbulent, free-surface flows which underscores the necessity of using CFD over BEM. Moreover, as mentioned above, the research integrates a multi-objective optimization framework (Pareto optimization), considering various performance criteria such as hydrodynamic resistance and sea-keeping qualities. This has been an important innovation for the field, and must be kept in this approach which uses Dakota, the optimization platform for this study. This thesis contributes to naval architecture by offering a pragmatic and efficient approach for hull design optimization, further enabling the development of rapid, cost-effective, and environmentally sustainable marine vessels.</p>

corrected abstract:
<p>This thesis introduces a multi-fidelity approach to optimize ship hull design, leveraging both low-fidelity (LoFi) and high-fidelity (HiFi) viscous Reynolds-Averaged Navier-Stokes (RANS) simulations. This study primarily aims to devise a method that reduces both computational cost and time in CFD-based hull design optimization, particularly when viscous effects are significant and the Boundary Element Method (BEM), though less computationally intensive, falls short in accuracy and reliability. Traditional approaches in hull design optimization predominantly rely on either BEM or HiFi RANS simulations. While these methods are accurate, they are either limited in use cases or computationally expensive and time-consuming. This research proposes a framework where a Pareto optimization using a Multi-Objective Genetic Algorithm is conducted using a Multi-fidelity model which combines high-fidelity models and LoFi models in order to achieve accuracy at a reasonable cost. The validation of the optimization results is performed using a reference study, a Pareto optimization using the sea trial and tow tank validated marine CFD reference simulation setup by FINE™/Marine. This validation ensures the robustness and reliability of the simulation results, particularly in the absence of experimental data. Illustrated through case studies, the thesis explores the optimization of hydrofoils and multi-hull hulls. In such scenarios, a comprehensive analysis of the flow is required. Drawing upon existing literature, viscous flow base methods are identified as a more appropriate and accurate technique for modeling turbulent, free-surface flows which underscores the necessity of using CFD over BEM. Moreover, as mentioned above, the research integrates a multi-objective optimization framework (Pareto optimization), considering various performance criteria such as hydrodynamic resistance and sea-keeping qualities. This has been an important innovation for the field, and must be kept in this approach which uses Dakota, the optimization platform for this study. This thesis contributes to naval architecture by offering a pragmatic and efficient approach for hull design optimization, further enabling the development of rapid, cost-effective, and environmentally sustainable marine vessels.</p>
----------------------------------------------------------------------
in diva2:1894641 
abstract is: 
<p>this thesis investigates the impact of various factors on airline ticket pricing, specifically analysing the influence of advance booking periods, seasonality, and time of booking on flight prices from sweden for the period 2019 to 2023. using multiple linear regression models, the study examines data for both short and long distance routes to destinations such as berlin, london, malaga, bangkok, new york, and addis abeba. findings indicate that booking flights well in advance, along with the time of booking and specific months, significantly affects ticket prices. seasonal trends particularly highlight the lower costs associated with travel during off-peak months and weekdays, providing actionable insights for online travel agencies (otas) to enhance pricing strategies and marketing approaches.the research demonstrates how otas can use these findings to implement dynamic pricing models and promotions tailored to consumer booking behaviours, potentially increasing sales and customer satisfaction. however, the study’s predictive power is limited by the exclusion of external factors such as economic conditions and geopolitical events, pointing to the need for further comprehensive analyses to integrate these variables. this thesis contributes valuable perspectives to the discourse on airline pricing strategies, suggesting pathways for future research to further clarify the complexities of the global airline market.</p>

corrected abstract:
<p>This thesis investigates the impact of various factors on airline ticket pricing, specifically analysing the influence of advance booking periods, seasonality, and time of booking on flight prices from Sweden for the period 2019 to 2023. Using multiple linear regression models, the study examines data for both short and long distance routes to destinations such as Berlin, London, Malaga, Bangkok, New York, and Addis Abeba. Findings indicate that booking flights well in advance, along with the time of booking and specific months, significantly affects ticket prices. Seasonal trends particularly highlight the lower costs associated with travel during off-peak months and weekdays, providing actionable insights for online travel agencies (OTAs) to enhance pricing strategies and marketing approaches.</p><p>The research demonstrates how OTAs can use these findings to implement dynamic pricing models and promotions tailored to consumer booking behaviours, potentially increasing sales and customer satisfaction. However, the study’s predictive power is limited by the exclusion of external factors such as economic conditions and geopolitical events, pointing to the need for further comprehensive analyses to integrate these variables. This thesis contributes valuable perspectives to the discourse on airline pricing strategies, suggesting pathways for future research to further clarify the complexities of the global airline market.</p>
----------------------------------------------------------------------
In diva2:1894640   - correct as is
----------------------------------------------------------------------
In diva2:1888179 
abstract is: 
<p>Since the Fukushima-Daiichi accident in 2011, mitigating the rapid reaction of zirconium alloyed cladding with steam during a loss of coolant accident at high temperatures has been a key research objective. Consequently, a new development branch dealing with so-called Accident Tolerant Fuels (ATF) has emerged. One approach within ATF is to apply a thin coating on the cladding, creating a protective layer. While chromium is poised to be used as a first-generation coating for pressurised water reactors, it proves inadequate in the more oxidising environments of boiling water reactors, where the formed oxide ends up dissolving. Based on autoclave tests, this master thesis focuses on designing chromium-niobium nitride (Cr,Nb)N using density functional theory to identify the optimal microstructure and to understand some of the underlying phenomena.Firstly, CrN and NbN are examined as separate materials in terms of bulk structures, the introduction of cationic and anionic vacancies, and cases of substitutional atoms. The main part of the thesis focuses on the microstructure of the coating, exploring whether a monolithic structure or a superlattice form is more favourable for (Cr,Nb)N. Changes in the structure depending on the number of superlattice layers were studied. Finally, to examine the oxidation effect, an oxygen atom was introduced into CrN and NbN cells.Even though it was found that, within the scope of this thesis, the most energetically favourable state for (Cr,Nb)N is to be in the form of a superlattice with 2 layers of CrN and 2 layers of NbN, there is still much to investigate.</p>

corrected abstract:
<p>Since the Fukushima-Daiichi accident in 2011, mitigating the rapid reaction of zirconium alloyed cladding with steam during a loss of coolant accident at high temperatures has been a key research objective. Consequently, a new development branch dealing with so-called Accident Tolerant Fuels (ATF) has emerged. One approach within ATF is to apply a thin coating on the cladding, creating a protective layer. While chromium is poised to be used as a first-generation coating for pressurised water reactors, it proves inadequate in the more oxidising environments of boiling water reactors, where the formed oxide ends up dissolving. Based on autoclave tests, this master thesis focuses on designing chromium-niobium nitride (Cr,Nb)N using density functional theory to identify the optimal microstructure and to understand some of the underlying phenomena.</p><p>Firstly, CrN and NbN are examined as separate materials in terms of bulk structures, the introduction of cationic and anionic vacancies, and cases of substitutional atoms. The main part of the thesis focuses on the microstructure of the coating, exploring whether a monolithic structure or a superlattice form is more favourable for (Cr,Nb)N. Changes in the structure depending on the number of superlattice layers were studied. Finally, to examine the oxidation effect, an oxygen atom was introduced into CrN and NbN cells.</p><p>Even though it was found that, within the scope of this thesis, the most energetically favourable state for (Cr,Nb)N is to be in the form of a superlattice with 2 layers of CrN and 2 layers of NbN, there is still much to investigate.</p>
----------------------------------------------------------------------
In diva2:1880454   - correct as is
----------------------------------------------------------------------
In diva2:1880389   - correct as is
----------------------------------------------------------------------
In diva2:1880296   - correct as is
----------------------------------------------------------------------
In diva2:1879604 
abstract is: 
<p>This report contributes to the ArtEmis project, a multidisciplinary initiative designed to enhance earthquake forecasting through sensor technology. By utilizing a sensor unit that measures radon levels—a potential precursor to seismic activity—the project seeks to develop predictive capabilities. Central to this report is the evaluation and enhancement of the acquisition electronics critical to sensor functionality. The study involves testing and monitoring of the initial set of sensor systems deployed across various locations in Southern Europe, with specific attention given to electronic components like the gateway and its connectivity options. The findings reveal that the D-Link DWR-932 router, originally used for network connectivity, is unsuitable due to poor performance. Consequently, this report recommends replacing the D-Link router with more reliable alternatives such as the SIM7600G-H modem and the Rut241 router, either individually or in combination, to enhance the system’s reliability and overall performance.</p>

corrected abstract:
<p>This report contributes to the ArtEmis project, a multidisciplinary initiative designed to enhance earthquake forecasting through sensor technology. By utilizing a sensor unit that measures radon levels&mdash;a potential precursor to seismic activity&mdash;the project seeks to develop predictive capabilities. Central to this report is the evaluation and enhancement of the acquisition electronics critical to sensor functionality. The study involves testing and monitoring of the initial set of sensor systems deployed across various locations in Southern Europe, with specific attention given to electronic components like the gateway and its connectivity options. The findings reveal that the D-Link DWR-932 router, originally used for network connectivity, is unsuitable due to poor performance. Consequently, this report recommends replacing the D-Link router with more reliable alternatives such as the SIM7600G-H modem and the Rut241 router, either individually or in combination, to enhance the system’s reliability and overall performance.</p>

Note: The only change is to replace the two hyphens with &mdash; - as the original uses long dash to separate the parenthetical.
----------------------------------------------------------------------
In diva2:1879381   - correct as is
----------------------------------------------------------------------
In diva2:1877792 
abstract is: 
<p>Pursuit-evasion problems comprise a set of pursuers that strive to catch oneor several evaders, often in a constrained environment. This thesis proposesand compares heuristic algorithms for pursuit-evasion problems wherein several double integrator agents pursue a single evader in a bounded subset of theEuclidean plane. Different methods for assigning surrounding target points tothe pursuers are tested numerically. In addition, a method which finds the timeoptimal strategy for pursuing a static target in an unconstrained setting is presented, and is then used to pursue the assigned, dynamic, target. Numericalresults show that the time optimal strategy for pursuing a static target translateswell to the dynamic problem.</p>

corrected abstract:
<p>Pursuit-evasion problems comprise a set of pursuers that strive to catch one or several evaders, often in a constrained environment. This thesis proposes and compares heuristic algorithms for pursuit-evasion problems wherein several double integrator agents pursue a single evader in a bounded subset of the Euclidean plane. Different methods for assigning surrounding target points to the pursuers are tested numerically. In addition, a method which finds the time optimal strategy for pursuing a static target in an unconstrained setting is presented, and is then used to pursue the assigned, dynamic, target. Numerical results show that the time optimal strategy for pursuing a static target translates well to the dynamic problem.</p>
----------------------------------------------------------------------
In diva2:1877661 
abstract is: 
<p>We present important results from Hilbert space and functional analysis for understanding the subject ofReproducing kernel Hilbert spaces. We then showcase the underlying theory and properties of Reproducingkernel Hilbert Spaces. Finally, we show how the theory of reproducing kernel Hilbert spaces is applicable inboth interpolation and machine learning.</p><p> </p>

corrected abstract:
<p>We present important results from Hilbert space and functional analysis for understanding the subject of Reproducing kernel Hilbert spaces. We then showcase the underlying theory and properties of Reproducing kernel Hilbert Spaces. Finally, we show how the theory of reproducing kernel Hilbert spaces is applicable in both interpolation and machine learning.</p>
----------------------------------------------------------------------
In diva2:1871609 
abstract is: 
<p>The primary contributor to greenhouse gases emissions in the European Union is the road transport sector, accounting for over 70% of total emissions. In response to this environmental concern, the EU is working on implementing strict regulations aimed at reducing emissions by 45% by the year 2030, compelling manufacturers of heavy-duty vehicles to produce emission free alternatives. While vehicle electrification is a promising solution,it introduces a new challenge: the need for lighter vehicles becomes more increasing with the incorporation of powerful yet heavy battery systems.</p><p>This work aims to study the possibility of substituting the steel bus frames with composite material frames with the dual objectives of reducing the overall weight of the bus and minimising its environmental footprint throughout the life cycle of the part. The most suitable composite materials, manufacturing processes and joining methods for the studied frames were investigated based on literature review and a list of requirements. Composite finite element analysis was performed to study the behaviour of the composite assembly when it is inserted into the steel bus model to simulate the real-life environment. </p><p>The analysis performed showed that composite frames can facilitate a mass reduction of over half of the steel frames weight. An unfailing structural behaviour of the laminate can be achieved using adhesive bonding as the joining method between the two distinct materials. At the same time, two significant needs were highlighted: the importance of laminate optimisation (in terms of different material properties, number of layers and orientations) both to reduce costs and to maximise the structural strength through effective and efficient material distribution and the importance of partitioning the composite structure in a cost beneficial way (when it comes to scrap levels and moulds complexity).</p>

corrected abstract:
<p>The primary contributor to greenhouse gases emissions in the European Union is the road transport sector, accounting for over 70% of total emissions. In response to this environmental concern, the EU is working on implementing strict regulations aimed at reducing emissions by 45% by the year 2030, compelling manufacturers of heavy-duty vehicles to produce emission free alternatives. While vehicle electrification is a promising solution, it introduces a new challenge: the need for lighter vehicles becomes more increasing with the incorporation of powerful yet heavy battery systems.</p><p>This work aims to study the possibility of substituting the steel bus frames with composite material frames with the dual objectives of reducing the overall weight of the bus and minimising its environmental footprint throughout the life cycle of the part. The most suitable composite materials, manufacturing processes and joining methods for the studied frames were investigated based on literature review and a list of requirements. Composite finite element analysis was performed to study the behaviour of the composite assembly when it is inserted into the steel bus model to simulate the real-life environment.</p><p>The analysis performed showed that composite frames can facilitate a mass reduction of over half of the steel frames weight. An unfailing structural behaviour of the laminate can be achieved using adhesive bonding as the joining method between the two distinct materials. At the same time, two significant needs were highlighted: the importance of laminate optimisation (in terms of different material properties, number of layers and orientations) both to reduce costs and to maximise the structural strength through effective and efficient material distribution and the importance of partitioning the composite structure in a cost beneficial way (when it comes to scrap levels and moulds complexity).</p>
----------------------------------------------------------------------
In diva2:1871579 
abstract is: 
<p>This paper presents conora, a robust trajectory optimization software utilizing orthogonal collocation methods for ascending rocket stages, targeting applications in early phases of mission design. The proposed methodology leverages orthogonal collocation techniques, preferred over the multitude of available options for their robustness to inaccuracies in the initial guess. This, together with low amount of available data about the ascent profile, often makes preliminary optimization considerably complex, extremely case-specific and, consequently, very time consuming. The software here implemented addresses the problem of maximizing the payload mass of a rocket by providing the required flexibility to adapt to any mission scenario disregarding of the celestial body, launch site, vehicle design and target orbit. Proper functionality is demonstrated by replicating existing missions, simplifying and reducing to the bare minimum the number of inputs. Ariane V ascending to GTO, Electron launch to SSO, ALTO mission to LEO, Apollo XI Lunar Module ascent and Starship take-off to LMO are the multifaceted mission scenarios selected to demonstrate the capabilities of conora, resulting in accurate injection into orbit and relatively close estimation of optimized payload masses. The obtained outcomes grow more valuable when considering the small amount of inputs provided, the simplicity of the utilized physical model and the strong assumptions considered. The whole software development process followed a V-model, from requirement definition, passing by the actual implementation, to thorough code testing of each conora’s module. 64 are the number of identified top level requirements, for a verification process elaborated via more than 270 tests, from unit to system level. The entire work was performed in the context of an internship at DLR, at the Institute of Space Systems in Bremen, Germany.</p>

corrected abstract:
<p>This paper presents <em>conora</em>, a robust trajectory optimization software utilizing orthogonal collocation methods for ascending rocket stages, targeting applications in early phases of mission design. The proposed methodology leverages orthogonal collocation techniques, preferred over the multitude of available options for their robustness to inaccuracies in the initial guess. This, together with low amount of available data about the ascent profile, often makes preliminary optimization considerably complex, extremely case-specific and, consequently, very time consuming. The software here implemented addresses the problem of maximizing the payload mass of a rocket by providing the required flexibility to adapt to any mission scenario disregarding of the celestial body, launch site, vehicle design and target orbit. Proper functionality is demonstrated by replicating existing missions, simplifying and reducing to the bare minimum the number of inputs. Ariane V ascending to GTO, Electron launch to SSO, ALTO mission to LEO, Apollo XI Lunar Module ascent and Starship take-off to LMO are the multifaceted mission scenarios selected to demonstrate the capabilities of <em>conora</em>, resulting in accurate injection into orbit and relatively close estimation of optimized payload masses. The obtained outcomes grow more valuable when considering the small amount of inputs provided, the simplicity of the utilized physical model and the strong assumptions considered. The whole software development process followed a V-model, from requirement definition, passing by the actual implementation, to thorough code testing of each <em>conora’s</em> module. 64 are the number of identified top level requirements, for a verification process elaborated via more than 270 tests, from unit to system level. The entire work was performed in the context of an internship at DLR, at the Institute of Space Systems in Bremen, Germany.</p>


Note: Only change - put software name in itlaics, i.e.,  <em>conora</em>
----------------------------------------------------------------------
In diva2:1871533 
abstract is: 
<p>In the future, international Lunar missions will include both surface and lava tunnel EVA explorations, together with the collection of soil composition and ground data. Some issues with the realization of these activities are the communication constraints between the astronauts and the ground base on the Moon’s surface (especially in the first phases of Lunar exploration, when the satellite communication network will be under-supplied), and the limited area that a classical ground analyzing scientific instrument can cover. A solution to those issues has already been explored by previous interns at Spaceship EAC, resulting in a set of hardware prototypes communicating via LoRa protocol in an Internet of Things network configuration. The main idea behind these devices is to act both as signal transmitters and receivers, bridging the astronaut’s EVA suit to the ground base, and as a broad sensor network, capable of collecting a large amount of ground data from a vast area (crucial, for example, because of the proven heterogeneity of lunar soil chemical composition between different zones).This thesis explores the feasibility of miniaturizing these prototypes and creating a new set of smaller units with enhanced functionalities and performance. The designed device is called CRUMB (Compact Radio Unit for Moon data Broadcasting), and each unit is capable of communicating to other CRUMBs via LoRa frequencies (868 MHz – standard for European applications) while answering to an IoT mesh protocol. Via an accelerometer sensor, the CRUMB units are also able to send gravimetry data to the rest of the mesh, and to send an emergency communication whenever a moonquake is detected. Moreover, CRUMB’s volume is 92% smaller than its predecessor’s, and it nominally operates with a low power consumption (less than 1 W).</p>

corrected abstract:
<p>In the future, international Lunar missions will include both surface and lava tunnel EVA explorations, together with the collection of soil composition and ground data. Some issues with the realization of these activities are the communication constraints between the astronauts and the ground base on the Moon’s surface (especially in the first phases of Lunar exploration, when the satellite communication network will be under-supplied), and the limited area that a classical ground analyzing scientific instrument can cover. A solution to those issues has already been explored by previous interns at Spaceship EAC, resulting in a set of hardware prototypes communicating via LoRa protocol in an Internet of Things network configuration. The main idea behind these devices is to act both as signal transmitters and receivers, bridging the astronaut’s EVA suit to the ground base, and as a broad sensor network, capable of collecting a large amount of ground data from a vast area (crucial, for example, because of the proven heterogeneity of lunar soil chemical composition between different zones).</p><p>This thesis explores the feasibility of miniaturizing these prototypes and creating a new set of smaller units with enhanced functionalities and performance. The designed device is called CRUMB (Compact Radio Unit for Moon data Broadcasting), and each unit is capable of communicating to other CRUMBs via LoRa frequencies (868 MHz – standard for European applications) while answering to an IoT mesh protocol. Via an accelerometer sensor, the CRUMB units are also able to send gravimetry data to the rest of the mesh, and to send an emergency communication whenever a moonquake is detected. Moreover, CRUMB’s volume is 92% smaller than its predecessor’s, and it nominally operates with a low power consumption (less than 1 W).</p>

Note: Only change is to insert separation into two paragraphs-
----------------------------------------------------------------------
In diva2:1871175 
abstract is: 
<p>G protein-coupled receptors (GPCR) are interesting drug targets. In this degree project the focus is on GPR183 and GPR61 which play a role in immune cell trafficking and metabolic regulation respectively. This project aims to test specific mutations from both blood cancer and high obesity samples, with the goal of seeing any different behaviors compared to wild type receptors. Behavior in this case pertains to differences in expression and activation, both constitutively and in the presence of ligands. The project employs a plethora of different physical instruments to obtain these results. The use of Bioluminescence Resonance Energy Transfer (BRET), Fluorescence Resonance Energy Transfer (FRET) and a combination of multiple different assays together form a robust way of measuring both expression and activation for different mutations. The main takeaways for GPR183 were that the mutation, L125P, found in Non-Hodgkin lymphoma samples, differ significantly in both expression and activation compared to wild type. Specifically at the native membrane domain. There were also results that speculated that this mutation might induce misfold, hindering its transportation to the membrane from the inner compartments of the cell.For GPR61, a majority of the mutations investigated showed lower expression compared to the wild type counterpart, and only a few saw a significant reduction in activation. Ligand experiments on GPR61 did not yield any increase in constitutive activity, prompting further assessment in the future. Overall, the project gained interesting results that prompts further investigation of GPR183 mutations, and signaling, specifically in the realm of G_α signaling. The project also narrows down the large magnitude of GPR61 mutations interesting for further research. This provides a solid foundation into GPCR signaling in said diseases.</p>

corrected abstract:
<p>G protein-coupled receptors (GPCR) are interesting drug targets. In this degree project the focus is on GPR183 and GPR61 which play a role in immune cell trafficking and metabolic regulation respectively.</p><p>This project aims to test specific mutations from both blood cancer and high obesity samples, with the goal of seeing any different behaviors compared to wild type receptors. Behavior in this case pertains to differences in expression and activation, both constitutively and in the presence of ligands.</p><p>The project employs a plethora of different physical instruments to obtain these results. The use of Bioluminescence Resonance Energy Transfer (BRET), Fluorescence Resonance Energy Transfer (FRET) and a combination of multiple different assays together form a robust way of measuring both expression and activation for different mutations.</p><p>The main takeaways for GPR183 were that the mutation, L125P, found in Non-Hodgkin lymphoma samples, differ significantly in both expression and activation compared to wild type. Specifically at the native membrane domain. There were also results that speculated that this mutation might induce misfold, hindering its transportation to the membrane from the inner compartments of the cell.</p><p>For GPR61, a majority of the mutations investigated showed lower expression compared to the wild type counterpart, and only a few saw a significant reduction in activation. Ligand experiments on GPR61 did not yield any increase in constitutive activity, prompting further assessment in the future.</p><p>Overall, the project gained interesting results that prompts further investigation of GPR183 mutations, and signaling, specifically in the realm of 𝐺<sub>&alpha;</sub> signaling. The project also narrows down the large magnitude of GPR61 mutations interesting for further research. This provides a solid foundation into GPCR signaling in said diseases.</p>
----------------------------------------------------------------------
In diva2:1859962 
abstract is: 
<p>Data-driven predictive maintenance has gained significant attention in recent years, with the goal of reducing unplanned downtime and maintenance costs in the transportation industry. This thesis was developed in collaboration with Scania, a prominent player in the sector, to research ways of combining different sources of operational data to improve predictions of component faults in commercial vehicles. </p><p>In particular, following a growing industry-wide interest for high-frequency data directly streamed from commercial vehicles, the thesis addresses the problem of using them in predictive models.</p><p>The purpose of this work is to present a framework combining commonly used low-frequency data in the form of aggregated read-outs, with these streaming signals. In order to test such approach, experiments were conducted on actual operational data linked to the NOx sensor, a critical component in the vehicle's exhaust system.</p><p>The study featured the development of separate predictive models for the two data types and the focus was on both their compatibility and their predictive performances. </p><p>For low-frequency aggregated data, a framework for fault prediction over time windows of variable lengths was developed. Both binary classification and survival analysis were considered for this purpose.</p><p>At the same time, real-time streaming signals were treated using Gaussian Hidden Markov Models (GHMM) and Long Short-Term Memory Recurrent Neural Networks (LSTM) for task of anomaly detection. In this case, the relation between fault and anomaly also had to be characterised, and this was done on the base of observed results.</p><p>Experiments on a test vehicle suggested that low frequency data showed early signs of a failure, while anomalous patterns in the signals were clearly visible in proximity of the breakdown event. Based on this, the combined approach featured running multiple time-window models on a same low frequency data read-out while an anomaly detection model runs in the background in an on-line fashion. </p><p>While the results are preliminary, mainly due to the limited access to the new streaming data, the framework proposed is promising. With further refinement and generalisation to more vehicles, this approach could be instrumental in developing a robust maintenance decision policy for various truck components.</p>

corrected abstract:
<p>Data-driven predictive maintenance has gained significant attention in recent years, with the goal of reducing unplanned downtime and maintenance costs in the transportation industry. This thesis was developed in collaboration with Scania, a prominent player in the sector, to research ways of combining different sources of operational data to improve predictions of component faults in commercial vehicles.</p><p>In particular, following a growing industry-wide interest for high-frequency data directly streamed from commercial vehicles, the thesis addresses the problem of using them in predictive models.</p><p>The purpose of this work is to present a framework combining commonly used low-frequency data in the form of aggregated read-outs, with these streaming signals. In order to test such approach, experiments were conducted on actual operational data linked to the NOx sensor, a critical component in the vehicle's exhaust system.</p><p>The study featured the development of separate predictive models for the two data types and the focus was on both their compatibility and their predictive performances.</p><p>For low-frequency aggregated data, a framework for fault prediction over time windows of variable lengths was developed. Both binary classification and survival analysis were considered for this purpose.</p><p>At the same time, real-time streaming signals were treated using Gaussian Hidden Markov Models (GHMM) and Long Short-Term Memory Recurrent Neural Networks (LSTM) for task of anomaly detection. In this case, the relation between fault and anomaly also had to be characterised, and this was done on the base of observed results.</p><p>Experiments on a test vehicle suggested that low frequency data showed early signs of a failure, while anomalous patterns in the signals were clearly visible in proximity of the breakdown event. Based on this, the combined approach featured running multiple time-window models on a same low frequency data read-out while an anomaly detection model runs in the background in an on-line fashion.</p><p>While the results are preliminary, mainly due to the limited access to the new streaming data, the framework proposed is promising. With further refinement and generalisation to more vehicles, this approach could be instrumental in developing a robust maintenance decision policy for various truck components.</p>


Note: Removed the unnecessary spaces at the end of paragraphs.
----------------------------------------------------------------------
In diva2:1852546   - correct as is
Note: no full text in DiVA
----------------------------------------------------------------------
In diva2:1852458 
Note: no full text in DiVA

abstract is: 
<p>Railway wheels have to be reprofiled or replaced if they do not fulfil certain demands and the costs for these actions can be considerable for the vehicle owner. The purpose of this work is to study wheel profiles at reprofiling through wheel wear simulations and reprofiling statistics.</p><p>The application at hand is the Rc4 locomotive used by the railway freight company Green Cargo. The locomotives travel the whole electrified Swedish railway network but a large part of the traffic is on the lines Malmö-Hallsberg and Luleå-Ockelbo. For these two lines the distributions of curve radius, track quality and lubrication have been analysed.</p><p>The simulation method is based on a load collective concept where the load collective is a discretization of actual conditions such as track design geometry, rail profiles, track irregularities, lubrication, vehicle speed and traction. The discretization results in a number of dynamic time-domain simulations to be performed in each wear step. The time-domain simulations are done with the MBS (Multi-Body-Simulation) tool Gensys. From these simulations the wear is calculated with a program developed in MATLAB, the wheel profile is updated and the next wear step is entered. In the vehicle-track simulations the wheel-rail contact is modelled with the Hertzian theory for the normal contact and Kalker’s simplified theory for the tangential forces. Archard’s wear equation together with wear coefficients have been used to calculate the material worn off from the wheel. The wear coefficients have been determined from dry conditions and to compensate for both natural and manmade lubrication two scaling factors are used.Reprofiling of wheels on the Rc4 locomotive is done in two workshops and wheel profiles and wheel lathe differs between these two workshops. At assembling of new wheels the wheels have the UIC/ORE S1002 profile with flange thickness 32.5 mm but in order to save wheel diameter at reprofiling two different profiles with smaller flange thickness are used. From a wheelset database used by Green Cargo for maintenance planning statistics such as distribution of reprofiling causes, seasonal variations of wheel wear and damages, running distances, life length of wheels and wear rates for different profiles have been extracted.</p><p>For evaluation purposes wheel profiles of three locomotives were measured before and after the first reprofiling. When comparing measured and simulated wheel profiles it can be concluded that the simulation overestimates flange wear somewhat but the shape of the flange is close to measurements. Wheel tread wear is generally well predicted but the shape differs more. The simulation results are also compared, with good agreement, to simple measurements of wheel wear scalars from the wheelset database. For some wheel profiles problems in the simulations with unrealistic cavities at the wheel tread remain to be solved.Through simulations it has been seen that with a slight change of one of the profiles used at reprofiling running distance before reprofiling due to thin flanges can be increased with 95 kkm.</p>

corrected abstract:
<p>Railway wheels have to be reprofiled or replaced if they do not fulfil certain demands and the costs for these actions can be considerable for the vehicle owner. The purpose of this work is to study wheel profiles at reprofiling through wheel wear simulations and reprofiling statistics.</p><p>The application at hand is the Rc4 locomotive used by the railway freight company Green Cargo. The locomotives travel the whole electrified Swedish railway network but a large part of the traffic is on the lines Malmö-Hallsberg and Luleå-Ockelbo. For these two lines the distributions of curve radius, track quality and lubrication have been analysed.</p><p>The simulation method is based on a load collective concept where the load collective is a discretization of actual conditions such as track design geometry, rail profiles, track irregularities, lubrication, vehicle speed and traction. The discretization results in a number of dynamic time-domain simulations to be performed in each wear step. The time-domain simulations are done with the MBS (Multi-Body-Simulation) tool Gensys. From these simulations the wear is calculated with a program developed in MATLAB, the wheel profile is updated and the next wear step is entered. In the vehicle-track simulations the wheel-rail contact is modelled with the Hertzian theory for the normal contact and Kalker’s simplified theory for the tangential forces. Archard’s wear equation together with wear coefficients have been used to calculate the material worn off from the wheel. The wear coefficients have been determined from dry conditions and to compensate for both natural and manmade lubrication two scaling factors are used. Reprofiling of wheels on the Rc4 locomotive is done in two workshops and wheel profiles and wheel lathe differs between these two workshops. At assembling of new wheels the wheels have the UIC/ORE S1002 profile with flange thickness 32.5 mm but in order to save wheel diameter at reprofiling two different profiles with smaller flange thickness are used. From a wheelset database used by Green Cargo for maintenance planning statistics such as distribution of reprofiling causes, seasonal variations of wheel wear and damages, running distances, life length of wheels and wear rates for different profiles have been extracted.</p><p>For evaluation purposes wheel profiles of three locomotives were measured before and after the first reprofiling. When comparing measured and simulated wheel profiles it can be concluded that the simulation overestimates flange wear somewhat but the shape of the flange is close to measurements. Wheel tread wear is generally well predicted but the shape differs more. The simulation results are also compared, with good agreement, to simple measurements of wheel wear scalars from the wheelset database. For some wheel profiles problems in the simulations with unrealistic cavities at the wheel tread remain to be solved. Through simulations it has been seen that with a slight change of one of the profiles used at reprofiling running distance before reprofiling due to thin flanges can be increased with 95 kkm.</p>
----------------------------------------------------------------------
In diva2:1849685 
abstract is: 
<p>Identifying and creating new molecules with specific desired properties is a central task in chemical discovery. A key consideration in this process is the synthesizability of proposed molecules, requiring computers to learn the intricate rules of chemistry. Synthesizability is a constrain that is historically assessed after identifying novel molecules. </p><p>This thesis builds on SynNet a model for synthesis planning, which involves finding strategies of pairing molecules and chemical reactions in a goal-directed manner. These models enhances the traditional approach to molecular discovery and SynNet is a data-driven, synthesizability-constrained generative model that models synthesis paths as a conditioned Markov decision process. This work advances the understanding of synthesizable molecular design and its limitations. </p><p>The main results of this work is the demonstration that systematic hyperparameter tuning increases the recovery rate to 58% and out-of-distribution datasets are a limiting factor. The analysis reveals that datasets with non-linear synthesis paths significantly impact the model’s performance, while data representation and neural network architecture play a secondary role. This finding challenges previous hypotheses that order-invariance of the first reactant would improve the recovery rate, highlighting the importance of high-quality datasets and questioning the problem formulation as a Markov decision process.</p><p>These results provide insights to advance models, algorithms, and datasets for synthesizability-constrained molecular design. By openly publishing theproject, this work facilitates further research in synthesizability-constrained molecular design and encourages the development of new applications in accelerated drug discovery.</p>

corrected abstract:
<p>Identifying and creating new molecules with specific desired properties is a central task in chemical discovery. A key consideration in this process is the synthesizability of proposed molecules, requiring computers to learn the intricate rules of chemistry. Synthesizability is a constrain that is historically assessed after identifying novel molecules. </p><p>This thesis builds on SynNet a model for synthesis planning, which involves finding strategies of pairing molecules and chemical reactions in a goal-directed manner. These models enhances the traditional approach to molecular discovery and SynNet is a data-driven, synthesizability-constrained generative model that models synthesis paths as a conditioned Markov decision process. This work advances the understanding of synthesizable molecular design and its limitations. </p><p>The main results of this work is the demonstration that systematic hyperparameter tuning increases the recovery rate to 58% and out-of-distribution datasets are a limiting factor. The analysis reveals that datasets with non-linear synthesis paths significantly impact the model’s performance, while data representation and neural network architecture play a secondary role. This finding challenges previous hypotheses that order-invariance of the first reactant would improve the recovery rate, highlighting the importance of high-quality datasets and questioning the problem formulation as a Markov decision process.</p><p>These results provide insights to advance models, algorithms, and datasets for synthesizability-constrained molecular design. By openly publishing the project, this work facilitates further research in synthesizability-constrained molecular design and encourages the development of new applications in accelerated drug discovery.</p>
----------------------------------------------------------------------
In diva2:1849180 
abstract is: 
<p>The emergence of large language models, such as BERT and GPT-3, has revolutionized natural language processing tasks. However, the development and deployment of these models pose challenges, including concerns about computational resources and environmental impact. This study aims to compare discriminative language models for text classification based on their performance and usage cost. We evaluate the models using a hierarchical multi-label text classification task and assess their performance using primarly F1-score. Additionally, we analyze the usage cost by calculating the Floating Point Operations (FLOPs) required for inference. We compare a baseline model, which consists of a classifier chain with logistic regression models, with fine-tuned discriminative language models, including BERT with two different sequence lengths and DistilBERT, a distilled version of BERT. Results show that the DistilBERT model performs optimally in terms of performance, achieving an F1-score of 0.56 averaged on all classification layers. The baseline model and BERT with a maximal sequence length of 128 achieve F1-scores of 0.51. However, the baseline model outperforms the transformers at the most specific classification level with an F1-score of 0.33. Regarding usage cost, the baseline model significantly requires fewer FLOPs compared to the transformers. Furthermore, restricting BERT to a maximum sequence length of 128 tokens instead of 512 sacrifices some performance but offers substantial gains in usage cost. The code and dataset are available on GitHub.</p>


corrected abstract:
<p>The emergence of large language models, such as BERT and GPT-3, has revolutionized natural language processing tasks. However, the development and deployment of these models pose challenges, including concerns about computational resources and environmental impact. This study aims to compare discriminative language models for text classification based on their performance and usage cost. We evaluate the models using a hierarchical multi-label text classification task and assess their performance using primarly F1-score. Additionally, we analyze the usage cost by calculating the Floating Point Operations (FLOPs) required for inference. We compare a baseline model, which consists of a classifier chain with logistic regression models, with fine-tuned discriminative language models, including BERT with two different sequence lengths and DistilBERT, a distilled version of BERT. Results show that the DistilBERT model performs optimally in terms of performance, achieving an F1-score of 0.56 averaged on all classification layers. The baseline model and BERT with a maximal sequence length of 128 achieve F1-scores of 0.51. However, the baseline model outperforms the transformers at the most specific classification level with an F1-score of 0.33. Regarding usage cost, the baseline model significantly requires fewer FLOPs compared to the transformers. Furthermore, restricting BERT to a maximum sequence length of 128 tokens instead of 512 sacrifices some performance but offers substantial gains in usage cost. The code and dataset are available on GitHub.<sup><a href="#fn1" id="ref1">1</a></sup></p>

<div id="footnotes">
    <ol>
        <li id="fn1">https://github.com/eengel7/comparison_NLP_classification_models<a href="#ref1" aria-label="Back to reference">↩</a></ĺi>
    </ol>
</div>
----------------------------------------------------------------------
In diva2:1849167 
abstract is: 
<p>This Master's Thesis project set out with the objective to propose a machine learning model for predicting insurance risk at the level of an individual coverage, and compare it towards the existing models used by the project provider Gjensidige Försäkring. Due to interpretability constraints, it was found that this problem can be translated into a standard tabular regression task, with well defined target distributions. However, it was early identified that the set of feasible models do not contain pure black box models such as XGBoost, LightGBM and CatBoost which are typical choices for tabular data regression. In the report, we explicitly formulate the interpretability constraints in sharp mathematical language. It is concluded that interpretability can be ensured by enforcing a particular structure on the Hilbert space across which we are looking for the model. </p><p>Using this formalism, we consider two different approaches for fitting high performing models that maintain interpretability, where we conclude that gradient boosted regression tree based Generalized Additive Models in general, and the Explainable Boosting Machine in particular, is a promising model candidate consisting of functions within the Hilbert space of interest. The other approach considered is the basis expansion approach, which is currently used at the project provider. We make the argument that the gradient boosted regression tree approach used by the Explainable Boosting Machine is a more suitable model type for an automated, data driven modelling approach which is likely to generalize well outside of the training set.</p><p>Finally, we perform an empirical study on three different internal datasets, where the Explainable Boosting Machine is compared towards the current production models. We find that the Explainable Boosting Machine systematically outperforms the current models on unseen test data. There are many potential ways to explain this, but the main hypothesis brought forward in the report is that the sequential model fitting procedure allowed by the regression tree approach allows us to effectively explore a larger portion of the Hilbert space which contains all permitted models in comparison to the basis expansion approach.</p>

corrected abstract:
<p>This Master's Thesis project set out with the objective to propose a machine learning model for predicting insurance risk at the level of an individual coverage, and compare it towards the existing models used by the project provider Gjensidige Försäkring. Due to interpretability constraints, it was found that this problem can be translated into a standard tabular regression task, with well defined target distributions. However, it was early identified that the set of feasible models do not contain pure black box models such as XGBoost, LightGBM and CatBoost which are typical choices for tabular data regression. In the report, we explicitly formulate the interpretability constraints in sharp mathematical language. It is concluded that interpretability can be ensured by enforcing a particular structure on the Hilbert space across which we are looking for the model.</p><p>Using this formalism, we consider two different approaches for fitting high performing models that maintain interpretability, where we conclude that gradient boosted regression tree based Generalized Additive Models in general, and the Explainable Boosting Machine developed by Nori et al. (2019) in particular, is a promising model candidate consisting of functions within the Hilbert space of interest. The other approach considered is the basis expansion approach, which is currently used at the project provider. We make the argument that the gradient boosted regression tree approach used by the Explainable Boosting Machine is a more suitable model type for an automated, data driven modelling approach which is likely to generalize well outside of the training set. These arguments are supported by previous work in Lou et al. (2013).</p><p>Finally, we perform an empirical study on three different internal datasets, where the Explainable Boosting Machine is compared towards the current production models. We find that the Explainable Boosting Machine systematically outperforms the current models on unseen test data. There are many potential ways to explain this, but the main hypothesis brought forward in the report is that the sequential model fitting procedure allowed by the regression tree approach allows us to effectively explore a larger portion of the Hilbert space which contains all permitted models in comparison to the basis expansion approach.</p>
----------------------------------------------------------------------
In diva2:1842169 
abstract is: 
<p>Proper modelling of the gravitational fields of irregularly shaped asteroids and comets is an essential yet challenging part of any spacecraft visit and flyby to these bodies. Accurate density representations provide crucial information for proximity missions, which rely heavily on it to design safe and efficient trajectories. This work explores using a spacecraft swarm to maximise the measured gravitational signal in a hypothetical mission around the comet 67P/Churyumov-Gerasimenko. Spacecraft trajectories are simultaneously computed and evaluated using a high-order numerical integrator and an evolutionary optimisation method to maximise overall signal return. The propagation is based on an open-source polyhedral gravity model using a detailed mesh of 67P/C-G and considers the comet’s sidereal rotation. We compare performance on various mission scenarios using one and four spacecraft. The results show that the swarm achieved an expected increase in coverage over a single spacecraft when considering a fixed mission duration. However, optimising for a single spacecraft results in a more effective trajectory. The impact of dimensionality is further studied by introducing an iterative local search strategy, resulting in a generally improved robustness for finding efficient solutions. Overall, this work serves as a testbed for designing a set of trajectories in particularly complex gravitational environments, balancing measured signals and risks in a swarm scenario.</p>

corrected abstract:
<p>Proper modelling of the gravitational fields of irregularly shaped asteroids and comets is an essential yet challenging part of any spacecraft visit and flyby to these bodies. Accurate density representations provide crucial information for proximity missions, which rely heavily on it to design safe and efficient trajectories. This work explores using a spacecraft swarm to maximise the measured gravitational signal in a hypothetical mission around the comet 67P/Churyumov-Gerasimenko. Spacecraft trajectories are simultaneously computed and evaluated using a high-order numerical integrator and an evolutionary optimisation method to maximise overall signal return. The propagation is based on an open-source polyhedral gravity model using a detailed mesh of 67P/C-G and considers the comet’s sidereal rotation. We compare performance on various mission scenarios using one and four spacecraft. The results show that the swarm achieved an expected increase in coverage over a single spacecraft when considering a fixed mission duration. However, optimising for a single spacecraft results in a more effective trajectory. The impact of dimensionality is further studied by introducing an iterative local search strategy, resulting in a generally improved robustness for finding efficient solutions. Overall, this work serves as a testbed for designing a set of trajectories in particularly complex gravitational environments, balancing measured signals and risks in a swarm scenario.</p>
<p>The codebase is publicly available at: <a href="https://github.com/rasmusmarak/TOSS">https://github.com/rasmusmarak/TOSS</a>.</p>
----------------------------------------------------------------------
In diva2:1838558   - correct as is
----------------------------------------------------------------------
In diva2:1830397   - correct as is
----------------------------------------------------------------------
In diva2:1816706   - correct as is
----------------------------------------------------------------------
In diva2:1816682   - correct as is
----------------------------------------------------------------------
In diva2:1814925   - correct as is
----------------------------------------------------------------------
In diva2:1814429 
Note: no full text in DiVA

abstract is: 
<p>This thesis investigates the cyclic plasticity and fatigue properties of stainless steel. Viscoplastic behavior of 304L and the use of additive manufactured 316L present significant potential and unique challenges for the nuclear power industry. Understanding of these material’s cyclic plasticity and fatigue behavior is crucial for ensuring reliability and safety.This study delves into the viscoplastic behavior of 304L stainless steel, specifically exploring the impact of preload through a single hold time on fatigue life. Surprising findings indicate a correlation between viscoplastic behavior and improved fatigue life under certain circumstances, suggesting the potential for enhancing component performance through preload considerations. The cyclic plasticity behavior of 304L was successfully modelled with a time-dependent viscoplastic regularization addition to the classic radial return-mapping algorithm using a nonlinear kinematic hardening model of the Armstron-Frederick type and nonlinear isotropic hardening of the Voce type.This study also explores how selective laser melting processes affect material properties generated from a monotonic tensile and cyclic plasticity test. Post-processing treatments, such as hot isostatic pressing and solution annealing, are performed on the tested material. Anisotropic material properties are observed, with yield and tensile strengths differing based on component orientation relative to the laser build direction. Understanding these variations is vital for ensuring structural integrity in nuclear power components. The cyclic plasticity behavior of AM 316L was successfully modelled with a time-independent radial return-mapping algorithm using the nonlinear kinematic and isotropic hardening previously discussed with a Hill yield criterion.By investigating these aspects, this research contributes to the development of comprehensive material models for time dependent plasticity and AM-produced components, facilitating their reliable and safe utilization in the nuclear power industry. Understanding these unique characteristics and challenges enables enhanced efficiency, cost reduction, and improved performance of nuclear power systems.</p>

corrected abstract:
<p>This thesis investigates the cyclic plasticity and fatigue properties of stainless steel. Viscoplastic behavior of 304L and the use of additive manufactured 316L present significant potential and unique challenges for the nuclear power industry. Understanding of these material’s cyclic plasticity and fatigue behavior is crucial for ensuring reliability and safety.</p><p>This study delves into the viscoplastic behavior of 304L stainless steel, specifically exploring the impact of preload through a single hold time on fatigue life. Surprising findings indicate a correlation between viscoplastic behavior and improved fatigue life under certain circumstances, suggesting the potential for enhancing component performance through preload considerations. The cyclic plasticity behavior of 304L was successfully modelled with a time-dependent viscoplastic regularization addition to the classic radial return-mapping algorithm using a nonlinear kinematic hardening model of the Armstron-Frederick type and nonlinear isotropic hardening of the Voce type.</p><p>This study also explores how selective laser melting processes affect material properties generated from a monotonic tensile and cyclic plasticity test. Post-processing treatments, such as hot isostatic pressing and solution annealing, are performed on the tested material. Anisotropic material properties are observed, with yield and tensile strengths differing based on component orientation relative to the laser build direction. Understanding these variations is vital for ensuring structural integrity in nuclear power components. The cyclic plasticity behavior of AM 316L was successfully modelled with a time-independent radial return-mapping algorithm using the nonlinear kinematic and isotropic hardening previously discussed with a Hill yield criterion.</p><p>By investigating these aspects, this research contributes to the development of comprehensive material models for time dependent plasticity and AM-produced components, facilitating their reliable and safe utilization in the nuclear power industry. Understanding these unique characteristics and challenges enables enhanced efficiency, cost reduction, and improved performance of nuclear power systems.</p>

Note: I interpreted terminal punctuation without a following space as end of a paragraph.
----------------------------------------------------------------------
In diva2:1812812 
Note: no full text in DiVA

abstract is: 
<p>The world's current energy use is dominated by fossil fuels such as oil, coal, and natural gas. According to the International Energy Agency (IEA), they account for approximately 60% of global energy consumption. The effects of burning fossil fuels on the environment and human health, such as air pollution, climate change, and the deterioration of natural ecosystems, are causing significant concern. Therefore, there is a high demand for seeking new sources of green energy or gaining waste energy from current sources. To that end, thermoelectric (TE) materials offer promising features because of their ability to convert heat energy to electrical energy or vice versa. In their suitable implementation, they can improve energy efficiency and decrease the production of greenhouse gas emissions in several applications, such as power generation, refrigeration and cooling, aerospace applications, transportation, medical applications, and waste heat recovery in general. Exploring environmentally sustainable and scalable synthesis methodologies is essential for the advancement and practical integration of thermoelectric materials into everyday applications. This research introduces a novel hydrothermal route that remarkably reduces the synthesis time to 5 minutes by utilizing a microwave-assisted heating method at 220 °C. This abbreviated duration is notably lower than commonly employed synthesis techniques in the literature, thereby saving time and energy. In addition, using water as a solvent and employing the low-temperature cold-press technique for compaction align with the principles of sustainability and green chemistry. At the end of the synthesis, an impressive 95% yield is obtained, and thereby, the innovative synthesis approach proves its sustainability and environmentally conscientious nature. Using synthesized Ag2Se powder, cold-pressed pellets and hybrid films are fabricated, followed by structural and thermoelectric characterization. A power factor of 2641 µW/mK2 and 1693 µW/mK2 have been achieved for bulk Ag2Se-R2 and Ag2Se-R1, respectively. The thermoelectric figure of merit (ZT) for bulk Ag2Se-R2 has reached an exceptional value of 1.56, which is found to be 0.72 for the pure phase. Ag2Se-R2/PMMA hybrid films exhibit a power factor of 130 µW/mK2, while Ag2Se-R1/PMMA hybrid films show a power factor near zero due to low electrical conductivity. All the properties are measured at room temperature. The findings highlight the great potential of this study to be integrated into large-scale applications such as thermoelectric generators or wearable electronic devices.</p>

corrected abstract:
<p>The world's current energy use is dominated by fossil fuels such as oil, coal, and natural gas. According to the International Energy Agency (IEA), they account for approximately 60% of global energy consumption. The effects of burning fossil fuels on the environment and human health, such as air pollution, climate change, and the deterioration of natural ecosystems, are causing significant concern. Therefore, there is a high demand for seeking new sources of green energy or gaining waste energy from current sources. To that end, thermoelectric (TE) materials offer promising features because of their ability to convert heat energy to electrical energy or vice versa. In their suitable implementation, they can improve energy efficiency and decrease the production of greenhouse gas emissions in several applications, such as power generation, refrigeration and cooling, aerospace applications, transportation, medical applications, and waste heat recovery in general. Exploring environmentally sustainable and scalable synthesis methodologies is essential for the advancement and practical integration of thermoelectric materials into everyday applications. This research introduces a novel hydrothermal route that remarkably reduces the synthesis time to 5 minutes by utilizing a microwave-assisted heating method at 220 °C. This abbreviated duration is notably lower than commonly employed synthesis techniques in the literature, thereby saving time and energy. In addition, using water as a solvent and employing the low-temperature cold-press technique for compaction align with the principles of sustainability and green chemistry. At the end of the synthesis, an impressive 95% yield is obtained, and thereby, the innovative synthesis approach proves its sustainability and environmentally conscientious nature. Using synthesized Ag<sub>2</sub>Se powder, cold-pressed pellets and hybrid films are fabricated, followed by structural and thermoelectric characterization. A power factor of 2641 µW/mK<sup>2</sup> and 1693 µW/mK<sup>2</sup> have been achieved for bulk Ag<sub>2</sub>Se-R2 and Ag<sub>2</sub>Se-R1, respectively. The thermoelectric figure of merit (ZT) for bulk Ag<sub>2</sub>Se-R2 has reached an exceptional value of 1.56, which is found to be 0.72 for the pure phase. Ag<sub>2</sub>Se-R2/PMMA hybrid films exhibit a power factor of 130 µW/mK<sup>2</sup>, while Ag<sub>2</sub>Se-R1/PMMA hybrid films show a power factor near zero due to low electrical conductivity. All the properties are measured at room temperature. The findings highlight the great potential of this study to be integrated into large-scale applications such as thermoelectric generators or wearable electronic devices.</p>

Note: The popwer factor has to have units of W/mK². See for example: https://www.sciencedirect.com/science/article/abs/pii/S1385894722002479
----------------------------------------------------------------------
In diva2:1807973   - correct as is
Note: no full text in DiVA
----------------------------------------------------------------------
In diva2:1807713   - correct as is
----------------------------------------------------------------------
In diva2:1800544   - correct as is
----------------------------------------------------------------------
In diva2:1800495 
abstract is: 
<p>Loss reserving in P&amp;amp;C insurance, is the common practice of</p><p>estimating the insurer’s liability from future claims it will have to</p><p>pay out on. In the recent years, it has been popular to explore the</p><p>options of forecasting this loss with the help of machine learning</p><p>methods. This is mainly attributed to the increase in computational</p><p>power, opening up opportunities for handling more complex computations</p><p>with large datasets. The main focus of this paper is to implement and</p><p>evaluate a recurrent neural network called the deeptriangle by Kuo for</p><p>modelling payments of individual reported but not settled claims. The</p><p>results are compared with the traditional Chain Ladder method and a</p><p>baseline model on a simulated dataset provided by Wüthrich’s simulation</p><p>machine.  The models were implemented in Python using Tensorflow’s</p><p>functional API. The results show that the recurrent neural network does</p><p>not outperform the Chain Ladder method on the given data. The recurrent</p><p>neural network is weak towards the sparse and chaotic nature of</p><p>individual claim payments and is unable to detect a stable sequential</p><p>pattern. Results also show that the neural network is prone to</p><p>overfitting, which can theoretically be compensated with larger dataset</p><p>but comes at a cost in terms of feasibility.</p>

corrected abstract:
<p>Loss reserving in P&amp;C insurance, is the common practice of estimating the insurer’s liability from future claims it will have to pay out on. In the recent years, it has been popular to explore the options of forecasting this loss with the help of machine learning methods. This is mainly attributed to the increase in computational power, opening up opportunities for handling more complex computations with large datasets. The main focus of this paper is to implement and evaluate a recurrent neural network called the deeptriangle by Kuo for modelling payments of individual reported but not settled claims. The results are compared with the traditional Chain Ladder method and a baseline model on a simulated dataset provided by Wüthrich’s simulation machine.  The models were implemented in Python using Tensorflow’s functional API. The results show that the recurrent neural network does not outperform the Chain Ladder method on the given data. The recurrent neural network is weak towards the sparse and chaotic nature of individual claim payments and is unable to detect a stable sequential pattern. Results also show that the neural network is prone to overfitting, which can theoretically be compensated with larger dataset but comes at a cost in terms of feasibility.</p>
----------------------------------------------------------------------
In diva2:1800318 
abstract is: 
<p>This study explores the performance of porous Paptic paper materials composed of a mixture of softwood and lyocell fibers. The investigation involves laboratory experiments and numerical simulations to analyse the impact of various parameters on the paper's characteristics. Tensile and hygroexpansion tests were conducted on sheets with different binder quantities and drying methods. VTT aided in the analysis of mechanical properties using tomography images. The objective was to determine the optimal binder content, to understand the behaviour of the paper under different drying conditions and to optimise the pulp mixture through numerical simulations.</p><p>Experimental tests involved producing paper sheets with varying binder amounts and different drying methods. Tensile tests were conducted to assess the elastic stiffness, strength, and strain at break. Constrained dried and freely dried papers were compared to evaluate the influence of drying conditions. Hygroexpansion tests were performed to examine the water storage behaviour of papers with added binders. Tomography measurements provided the density profile, which was replicated in the numerical sheets. A micro-mechanical model was employed for numerical simulations, representing each fiber as a beam. The model was calibrated using stress-strain data from VTT's tensile testing of the paper with the highest binder content. The influence of altering the amount and length of lyocell fibers was examined to optimise the pulp.</p><p>From the tensile tests, an optimal binder content was identified that yielded the highest elastic stiffness while considering the density increase caused by binders. Further additions of binders did not enhance elastic stiffness. However, no optimal value was found for strength and strain at break, as both parameters continued to increase with additional binders. Tensile tests comparing constrained dried and freely dried papers showed similar behaviour, suggesting inadequate constraint in the former. Hygroexpansion tests confirmed the similarities between the two drying methods and revealed that papers with added binders stored less water at a given humidity. Additionally, the drying-moistening cycling exhibited an unusual behaviour not observed in conventional paper, with irreversible expansion occurring during the first drying cycle.</p><p>Numerical simulations using a micro-mechanical model demonstrated that higher amounts of lyocell fibers improved performance, increasing strength and strain at break. However, varying fiber length did not yield significant improvements in these parameters, although stiffness showed a slight increase. While the literature suggests that the addition of long lyocell fibers decreases paper strength, this study found that when maintaining constant bulk, strength increased under the assumption that the bonding strength was unaffected by lyocell fibers. Furthermore, numerical simulations indicated that an even density profile throughout the paper thickness resulted in higher strength and strain at break. The tomography data revealed that the density profile is affected by the binder quantity. With the addition of binders, the thickness decreased even though the grammage increased. The density is high on the top and bottom surface of the papers which contain more binders while the density is lower in the middle. This difference in density is more pronounced with higher amounts of binders.</p>

corrected abstract:
<p>This study explores the performance of porous Paptic paper materials composed of a mixture of softwood and lyocell fibers. Paptic paper materials gain their unique qualities from the inclusion of regenerated cellulose fibers and the use of foam-forming technology. The resulting paper material outperforms conventional paper and has a smaller environmental impact than cotton. The paper materials also have a positively unique textile-like feel to it. The investigation involves laboratory experiments and numerical simulations to analyze the impact of various parameters on the paper's characteristics. Tensile and hygroexpansion tests were conducted on sheets with different binder quantities and drying methods. VTT aided in the analysis of mechanical properties using tomography images. The objective was to determine the optimal binder content, to find the effects that binders have on the density profile, to understand the behavior of the paper under different drying conditions, and to optimize the pulp mixture through numerical simulations.</p><p>Experimental tests involved producing paper sheets with varying binder amounts and different drying methods. Tensile tests were conducted to assess the elastic stiffness, strength, and strain at break. Constrained dried and freely dried papers were compared to evaluate the influence of drying conditions. Hygroexpansion tests were performed to examine the water storage behavior of papers with added binders. Tomography measurements provided the density profile, which was replicated in the numerical sheets. A micro-mechanical model was employed for numerical simulations, representing each fiber as a beam. The model was calibrated using stress-strain data from VTT's tensile testing of the paper with the highest binder content. The influence of altering the amount and length of lyocell fibers was examined to optimize the pulp.</p><p>From the tensile tests, an optimal binder content was identified that yielded the highest elastic stiffness while considering the density increase caused by binders. Further additions of binders did not enhance elastic stiffness. However, no optimal value was found for strength and strain at break, as both parameters continued to increase with additional binders. Tensile tests comparing constrained dried and freely dried papers showed similar behavior, suggesting inadequate constraint in the former. Hygroexpansion tests confirmed the similarities between the two drying methods and revealed that papers with added binders stored less water at a given humidity. Additionally, the drying-moistening cycling exhibited an unusual behavior not observed in conventional paper, with irreversible expansion occurring during the first drying cycle.</p><p>Numerical simulations using a micro-mechanical model demonstrated that higher amounts of lyocell fibers improved performance, increasing strength and strain at break. However, varying the fiber length did not yield significant improvements in these parameters, although stiffness showed a slight increase. While the literature suggests that the addition of long lyocell fibers decreases paper strength, this study found that when maintaining constant bulk, strength increased under the assumption that the bonding strength was unaffected by lyocell fibers. Furthermore, numerical simulations indicated that an even density profile throughout the paper thickness resulted in higher strength and strain at break. The tomography data revealed that the density profile is affected by the binder quantity. With the addition of binders, the thickness decreased even though the grammage increased. The density is high on the top and bottom surface of the papers, while the density is lower in the middle. This difference in density is more pronounced with higher amounts of binders.</p>

Note: There was different wording in the DIVA abstract that the thesis.
----------------------------------------------------------------------
In diva2:1800223 - text us not selectable in thesis - 
abstract is: 
<p>In the realm of neuroelectronics, the challenge lies in achieving finer observations of physiological processes to comprehend neuronal interactions and computations. This necessitates the development of more compliant and biomimetic interfaces for improved integration with biological tissues, enabling finer physiological process observations. Commonly used flat and static electrode interfaces contrast sharply with the dynamic, complex, and three dimensional (3D) extracellular matrix (ECM) in which cells reside. Introducing 3D patterns on electrode surfaces enhances cell-chip coupling, improving the signal recording. Moreover, inorganic electrodes are stiff and rigid, creating mechanical mismatches with softer biological tissues, and they fail to fully capture ionic conduction.This thesis addresses these challenges by focusing on designing and engineering a multi-layer dynamic and stimuli-responsive bioelectronic interface. The system combines light-responsive, deformable polymers like Poly(Disperse Red 1-methacrylate) (pDR1m) with conductive polymers such as Poly(3,4-ethylenedioxythiophene): poly(stirensulfonate) (PEDOT:PSS). pDR1m responds to light, exhibiting 3D surface topography deformation, while PEDOT:PSS facilitates electrical recording and stimulation of cells, offering mixed electronic and ionic conduction as well as good mechanical properties. The potential use of an intermediate Polydimethylsiloxane (PDMS) film to improve layer adhesion is also explored. The individual and multi-layer samples were first optimized for spin coating manufacturing, and then thoroughly characterized to investigate their thickness, morphology, optical and electrochemical properties. Patterning of pDR1m-based samples was carried out using laser scanning confocal microscopy and a Lloyd’s mirror interferometer.The pDR1m\PEDOT:PSS sample demonstrates promising morphological and conductive properties, and the presence of PEDOT:PSS does not alter the absorption spectra of pDR1m. The multi-layer approach also supports efficient inscription of 3D surface reliefs without damaging the conductive layer. In conclusion, this work successfully designs conductive and dynamic light-driven films, which showcase good potential for bioelectronics and neuroelectronic interfaces. These interfaces could lead to enhanced investigations into combined electromechanical stimulation on cells and provide a more biomimetic coupling with biological tissues.</p>

corrected abstract:
<p>In the realm of neuroelectronics, the challenge lies in achieving finer observations of physiological processes to comprehend neuronal interactions and computations. This necessitates the development of more compliant and biomimetic interfaces for improved integration with biological tissues, enabling finer physiological process observations. Commonly used flat and static electrode interfaces contrast sharply with the dynamic, complex, and three dimensional (3D) extracellular matrix (ECM) in which cells reside. Introducing 3D patterns on electrode surfaces enhances cell-chip coupling, improving the signal recording. Moreover, inorganic electrodes are stiff and rigid, creating mechanical mismatches with softer biological tissues, and they fail to fully capture ionic conduction.</p><p>This thesis addresses these challenges by focusing on designing and engineering a multi-layer dynamic and stimuli-responsive bioelectronic interface. The system combines light-responsive, deformable polymers like Poly(Disperse Red 1-methacrylate) (pDR1m) with conductive polymers such as Poly(3,4-ethylenedioxythiophene): poly(stirensulfonate) (PEDOT:PSS). pDR1m responds to light, exhibiting 3D surface topography deformation, while PEDOT:PSS facilitates electrical recording and stimulation of cells, offering mixed electronic and ionic conduction as well as good mechanical properties. The potential use of an intermediate Polydimethylsiloxane (PDMS) film to improve layer adhesion is also explored. The individual and multi-layer samples were first optimized for spin coating manufacturing, and then thoroughly characterized to investigate their thickness, morphology, optical and electrochemical properties. Patterning of pDR1m-based samples was carried out using laser scanning confocal microscopy and a Lloyd’s mirror interferometer.</p><p>The pDR1m\PEDOT:PSS sample demonstrates promising morphological and conductive properties, and the presence of PEDOT:PSS does not alter the absorption spectra of pDR1m. The multi-layer approach also supports efficient inscription of 3D surface reliefs without damaging the conductive layer. In conclusion, this work successfully designs conductive and dynamic light-driven films, which showcase good potential for bioelectronics and neuroelectronic interfaces. These interfaces could lead to enhanced investigations into combined electromechanical stimulation on cells and provide a more biomimetic coupling with biological tissues.</p>

Note: The only change was splitting into paragraphs as per the thesis.
----------------------------------------------------------------------
In diva2:1800171 
abstract is: 
<p>In recent years, financial trading has become more available. This has led to more market participants and more trades taking place each day. The increased activity also implies an increasing number of abusive trades. To detect the abusive trades, market surveillance systems are developed and used. In this thesis, two different methods were tested to detect these abusive trades on high-dimensional data. One was based on empirical quantiles, and the other was based on an unsupervised machine learning technique called isolation forest. The empirical quantile method uses empirical quantiles on dimensionally reduced data to determine if a datapoint is an outlier or not. Principal Component Analysis (PCA) is used to reduce the dimensionality of the data and handle the correlation between features.Isolation forest is a machine learning method that detects outliers by sorting each datapoint in a tree structure. If a datapoint is close to the root, it is more likely to be an outlier. Isolation forest have been proven to detect outliers in high-dimensional datasets successfully, but have not been tested before for market surveillance. The performance of both the quantile method and isolation forest was tested by using recall and run-time. </p><p>The conclusion was that the empirical quantile method did not detect outliers accurately when all dimensions of the data were used. The method most likely suffered from the curse of dimensionality and could not handle high dimensional data. However, the performance increased when the dimensionality was reduced. Isolation forest performed better than the empirical quantile method and detected 99% of all outliers by classifying 226 datapoints as outliers out of a dataset with 184 true outliers and 1882 datapoints.</p>

corrected abstract:
<p>In recent years, financial trading has become more available. This has led to more market participants and more trades taking place each day. The increased activity also implies an increasing number of abusive trades. To detect the abusive trades, market surveillance systems are developed and used. In this thesis, two different methods were tested to detect these abusive trades on high-dimensional data. One was based on empirical quantiles, and the other was based on an unsupervised machine learning technique called isolation forest. The empirical quantile method uses empirical quantiles on dimensionally reduced data to determine if a datapoint is an outlier or not. Principal Component Analysis (PCA) is used to reduce the dimensionality of the data and handle the correlation between features. Isolation forest is a machine learning method that detects outliers by sorting each datapoint in a tree structure. If a datapoint is close to the root, it is more likely to be an outlier. Isolation forest have been proven to detect outliers in high-dimensional datasets successfully, but have not been tested before for market surveillance. The performance of both the quantile method and isolation forest was tested by using recall and run-time.</p><p>The conclusion was that the empirical quantile method did not detect outliers accurately when all dimensions of the data were used. The method most likely suffered from the curse of dimensionality and could not handle high dimensional data. However, the performance increased when the dimensionality was reduced. Isolation forest performed better than the empirical quantile method and detected 99% of all outliers by classifying 226 datapoints as outliers out of a dataset with 184 true outliers and 1882 datapoints.</p>

Note: There was a space missing in "features.Isolation", it should be "features. Isolation"
----------------------------------------------------------------------
In diva2:550706 
abstract is: 
<p>The purpose of the present master thesis is to optimize the current fuel tank simulations procedure for the next generation of JAS 39 Gripen ghters developed by SAAB AB. The current simulation process involves three dierent steps performed in three dierent computer environments. While the procedure works reasonably well on the fuel tank models of the previous version of the aircraft, it is too slow for the new Gripen tank models and their high level of detail. An optimized version of the procedure is put forward, which allows for tank analysis and fuel system simulation within reasonable time frames. Suggestions are made for future improvements.</p>

w='ghters' val={'c': 'fighters', 's': 'diva2:550706', 'n': 'missing ligature'}

corrected abstract:
<p>The purpose of the present master thesis is to optimize the current fuel tank simulations procedure for the next generation of JAS 39 Gripen fighters developed by SAAB AB. The current simulation process involves three different steps performed in three different computer environments. While the procedure works reasonably well on the fuel tank models of the previous version of the aircraft, it is too slow for the new Gripen tank models and their high level of detail. An optimized version of the procedure is put forward, which allows for tank analysis and fuel system simulation within reasonable time frames. Suggestions are made for future improvements.</p>
----------------------------------------------------------------------
In diva2:1083070 - missing spaces and ligature in title:
"A CFD Study Of The Aerodynamic Eects OfPlatooning Trucks"
==>
"A CFD Study Of The Aerodynamic Effects Of Platooning Trucks"

abstract is: 
<p>In the present work the aerodynamic forces on trucks driving in so-called platoon are</p><p>investigated in a numerical fashion. Driving in platoon, or convoy, refers to in an orderly</p><p>manner driving in a line, one truck after the other, taking advantage of the unrecovered</p><p>_ow behind each truck. The phenomenon is called slipstreaming or drafting. The Compu-</p><p>tational Fluid Dynamics (CFD) software STAR-CCM+ is used to calculate the flow field</p><p>around a platoon consisting of two and three trucks at different distances, ranging from 5</p><p>to 70 m. Two numerical approaches are used, one is the Reynolds Averaged Navier Stokes</p><p>based (RANS) two-equation turbulence model k 􀀀 " realizable model with a two-layer</p><p>treatment. The second one is the Menter's Shear Stress Transport (SST) k 􀀀 ! Detached</p><p>Eddy Simulation (DES) model. The first one is time independent, so-called steady-state,</p><p>where platoons consisting of two and three trucks are used in the simulations. However,</p><p>the nature of the flow field around vehicles is inherently time-dependent, which makes</p><p>it difficult to receive a steady-state solution and thus, the reliability of the result is neg-</p><p>atively affected. The second model is time dependent and much more computationally</p><p>expensive, where only a platoon consisting of two trucks is simulated. Addition to this,</p><p>simulations with an isolated truck will be conducted in order to make a relative study for</p><p>both turbulence models. Since numerous of errors are introduced when approaching the</p><p>problem numerically, it is important to have a reference case to compare with, set under</p><p>the same conditions. Also, comparisons with other studies are done. A mesh independent</p><p>study is conducted with the function of investigating how the mesh density influences the</p><p>result, together with a mesh quality study, both helpful when assessing the credibility of</p><p>the results.</p><p>For the RANS approach, it is shown that for the 2-truck platoon, drag reductions</p><p>are the greatest at the closest distance, 5 m, with 26:9 and 28:1 % reductions in drag</p><p>for the leading and trailing vehicle, respectively, compared to the isolated case. There is</p><p>an increase in drag for both vehicles with increasing distance, however, the trend turns</p><p>around at 10 m for the trailing vehicle, where it also reaches its maximum, 5:5 % larger</p><p>drag than that of the reference case's. Then a reduction is seen for all distances greater</p><p>than 11 m. For the leading truck, the drag coeffcient CD is equal to the reference case's</p><p>around 18 m, with an overshoot of 2 􀀀 3 % afterward, which may be a result of numerical</p><p>errors. The same trend is seen for the 3-truck platoon, with largest reductions at the</p><p>closest distance 5 m, with the reductions 31:5 %, 48:5 % and 33:2 % for truck one, two</p><p>and three, respectively. At 10 m, there is also an abrupt increase in drag for the trailing</p><p>trucks, however, the drag never reaches over the drag of an isolated truck. An overshoot</p><p>is also seen for the first vehicle in the 3-truck platoon and it stops benefiting from platoon</p><p>driving around 22 m. It was found that at 10, thick low-velocity boundary layers were</p><p>formed on the leading trucks, which may be one of the reasons for the increase in drag.</p><p>For the time-dependent approach, the drag behavior is similar to the RANS cases for</p><p>the leading vehicle, but no overshoot is seen, instead the drag is always smaller than the</p><p>reference case's. The maximum reduction is also found at 5 m, with the value 31:7 %. A</p><p>completely different trend is found for the second vehicle, where the drag decreases with</p><p>increasing distance, where there is a minimum reduction at 5 m (4:0 %) and a maximum</p><p>reduction at the largest investigated distance 50 m (24:3 %). This kind of trend is also</p><p>seen for the RANS-based simulation in the interval 10 􀀀 50 m, but the reductions are not</p><p>as large. After 12 m, the trailing truck benefits the most.</p><p>It was found that the vortices and the time dependence of the flow field are important</p><p>features. The RANS-based model produced poor results in region of strong swirl and</p><p>therefore it is not a suitable model for the flows of this type. Also, based on the good</p><p>agreement with PowerFlow VLES (Very Large Eddy Simulation) simulations with the</p><p>DES ones even further puts great distrust on the RANS simulations. The k 􀀀 " realizable</p><p>model with a two-layer treatment has also shown deficiencies in predicting the downstream</p><p>effects (over predicts) and the size and intensity of recirculation areas (for instance, the</p><p>wake) as shown in the work of P.L. Davis, A.T. Rinehimer and M. Uddin, 20th Annual</p><p>Conference of the CFD Society of Canada.</p>

corrected abstract:
<p>In the present work the aerodynamic forces on trucks driving in so-called <em>platoon</em> are investigated in a numerical fashion. Driving in platoon, or convoy, refers to in an orderly manner driving in a line, one truck after the other, taking advantage of the unrecovered flow behind each truck. The phenomenon is called <em>slipstreaming</em> or <em>drafting</em>. The Computational Fluid Dynamics (CFD) software STAR-CCM+ is used to calculate the flow field around a platoon consisting of two and three trucks at different distances, ranging from 5 to 70 m. Two numerical approaches are used, one is the Reynolds-Averaged Navier-Stokes based (RANS) two-equation turbulence model 𝑘 &mdash; <em>ε</em> realizable model with a two-layer treatment. The second one is the Menter's Shear Stress Transport (SST) 𝑘 &mdash; <em>ω</em> Detached Eddy Simulation (DES) model. The first one is time independent, so-called steady-state, where platoons consisting of two and three trucks are used in the simulations. However, the nature of the flow field around vehicles is inherently time-dependent, which makes it difficult to receive a steady-state solution and thus, the reliability of the result is negatively affected. The second model is time dependent and much more computationally expensive, where only a platoon consisting of two trucks is simulated. Addition to this, simulations with an isolated truck will be conducted in order to make a relative study for both turbulence models. Since numerous of errors are introduced when approaching the problem numerically, it is important to have a reference case to compare with, set under the same conditions. Also, comparisons with other studies are done. A mesh independent study is conducted with the function of investigating how the mesh density influences the result, together with a mesh quality study, both helpful when assessing the credibility of the results.</p><p>For the RANS approach, it is shown that for the 2-truck platoon, drag reductions are the greatest at the closest distance, 5 m, with 26.9 and 28.1 % reductions in drag for the leading and trailing vehicle, respectively, compared to the isolated case. There is an increase in drag for both vehicles with increasing distance, however, the trend turns around at 10 m for the trailing vehicle, where it also reaches its maximum, 5.5 % larger drag than that of the reference case's. Then a reduction is seen for all distances greater than 11 m. For the leading truck, the drag coefficient C<sub>D</sub> is equal to the reference case's around 18 m, with an overshoot of 2 &mdash; 3 % afterward, which may be a result of numerical errors. The same trend is seen for the 3-truck platoon, with largest reductions at the closest distance 5 m, with the reductions 31.5 %, 48.5 % and 33.2 % for truck one, two and three, respectively. At 10 m, there is also an abrupt increase in drag for the trailing trucks, however, the drag never reaches over the drag of an isolated truck. An overshoot is also seen for the first vehicle in the 3-truck platoon and it stops benefiting from platoon driving around 22 m. It was found that at 10, thick low-velocity boundary layers were formed on the leading trucks, which may be one of the reasons for the increase in drag.</p><p>For the time-dependent approach, the drag behavior is similar to the RANS cases for the leading vehicle, but no overshoot is seen, instead the drag is always smaller than the reference case's. The maximum reduction is also found at 5 m, with the value 31.7 %. A completely different trend is found for the second vehicle, where the drag decreases with increasing distance, where there is a minimum reduction at 5 m (4.0 %) and a maximum reduction at the largest investigated distance 50 m (24.3 %). This kind of trend is also seen for the RANS-based simulation in the interval 10 &mdash; 50 m, but the reductions are not as large. After 12 m, the trailing truck benefits the most.</p><p>It was found that the vortices and the time dependence of the flow field are important features. The RANS-based model produced poor results in region of strong swirl and therefore it is not a suitable model for the flows of this type. Also, based on the good agreement with PowerFlow VLES (Very Large Eddy Simulation) simulations with the DES ones even further puts great distrust on the RANS simulations. The 𝑘 &mdash; <em>ε</em> realizable model with a two-layer treatment has also shown deficiencies in predicting the downstream effects (over predicts) and the size and intensity of recirculation areas (for instance, the wake) as shown in the work of P.L. Davis, A.T. Rinehimer and M. Uddin, 20<sup>th</sup> Annual Conference of the CFD Society of Canada.</p>
-----------------------------------------------------------------------
In diva2:1799827 
abstract is: 
<p>In response to the Great Financial Crisis of 2008, a handful of measures were taken to increase the resilience toward a similar disaster in the future. Global financial regulatory entities implemented several new directives with the intention to enhance global capital markets, leading to regulatory frameworks where financial participants (FPs) are regulated with own fund's requirements for market risks. This thesis intends to investigate two different methods presented in the framework Capital Requirements Regulation 3 (CRR 3), a framework stemming from the Basel Committee and implemented in EU legislation for determining the capital requirements for an FP. The first method, The Alternative Standardised Approach (A-SA), looks at categorical data, whereas the second method, The Alternative Internal Model Approach (A-IMA), uses the risk measure Expected Shortfall (ES) for determining the capital requirement and therefore requires the FP to estimate ES using a proprietary/internal model based on time series data. The proprietary model in this thesis uses a recurrent neural network (RNN) with several long short-term memory (LSTM) layers to predict the next day's ES using the previous 20 day's returns. The data consisted of categorical and time series data of a portfolio with the Nasdaq 100 companies as positions. This thesis concluds that A-IMA with an LSTM-network as the proprietary model, gives a lower capital requirement compared to A-SA but is less reliable in real-life applications due to its behaviour as a "black box" and is, thus, less compliant from a regulatory standpoint. The LSTM-model showed promising results for capturing the overall trend in the data, for example periods with high volatility, but underestimated the true ES.</p>

corrected abstract:
<p>In response to the Great Financial Crisis of 2008, a handful of measures were taken to increase the resilience toward a similar disaster in the future. Global financial regulatory entities implemented several new directives with the intention to enhance global capital markets, leading to regulatory frameworks where financial participants (FPs) are regulated with own fund's requirements for market risks. This thesis intends to investigate two different methods presented in the framework <em>Capital Requirements Regulation 3</em> (CRR 3), a framework stemming from the Basel Committee and implemented in EU legislation for determining the capital requirements for an FP. The first method, <em>The Alternative Standardised Approach</em> (A-SA), looks at categorical data, whereas the second method, <em>The Alternative Internal Model Approach</em> (A-IMA), uses the risk measure <em>Expected Shortfall</em> (ES) for determining the capital requirement and therefore requires the FP to estimate ES using a proprietary/internal model based on time series data. The proprietary model in this thesis uses a recurrent neural network (RNN) with several long short-term memory (LSTM) layers to predict the next day's ES using the previous 20 day's returns. The data consisted of categorical and time series data of a portfolio with the Nasdaq 100 companies as positions. This thesis concludes that A-IMA with an LSTM-network as the proprietary model, gives a lower capital requirement compared to A-SA but is less reliable in real-life applications due to its behaviour as a "black box" and is, thus, less compliant from a regulatory standpoint. The LSTM-model showed promising results for capturing the overall trend in the data, for example periods with high volatility, but underestimated the true ES.</p>
----------------------------------------------------------------------
In diva2:1792401 
abstract is: 
<p>In this study, four frequently used yield curve construction methods are evaulated on a set of metrics with the aim of determining which method is the most suitable for estimating yield curves from European zero rates. The included curve construction methods are Nelson-Siegel, Nelson-Siegel-Svensson, cubic spline interpolation and forward monotone convex spline interpolation. We let the methods construct yield curves on multiple sets of zero yields with different origins. It is found that while the interpolation methods show greater ability to adapt to variable market conditions as well as hedge arbitrary fixed income claims, they are outperformed by the parametric methods regarding the smoothness of the resulting yield curve as well as their sensitivity to noise and perturbations in the input rates. This apart from the Nelson-Siegel method's problem of capturing the behavior of underlying rates with a high curvature. The Nelson-Siegel-Svensson method did also exhibit instability issues when exposed to perturbations in the input rates. The Nelson-Siegel method and the forward monotone convex spline interpolation method emerge as most favorable in their respective categories. The ultimate selection between the two methods must however take the application at hand into consideration due to their fundamentally different characteristics.</p>


corrected abstract:
<p>In this study, four frequently used yield curve construction methods are evaluated on a set of metrics with the aim of determining which method is most suitable for estimating yield curves from European zero rates. The included curve construction methods are Nelson-Siegel, Nelson-Siegel-Svensson, cubic spline interpolation and forward monotone convex spline interpolation. We let the methods construct yield curves on multiple sets of zero yields with different origins. It is found that while the interpolation methods show greater ability to adapt to variable market conditions as well as hedge arbitrary fixed income claims, they are outperformed by the parametric methods regarding the smoothness of the resulting yield curve as well as their sensitivity to noise and perturbations in the input rates. This apart from the Nelson-Siegel method's problem of capturing the behavior of underlying rates with a high curvature. The Nelson-Siegel-Svensson method did also exhibit instability issues when exposed to perturbations in the input rates. The Nelson-Siegel method and the forward monotone convex spline interpolation method emerge as most favorable in their respective categories. The ultimate selection between the two methods must however take the application at hand into consideration due to their fundamentally different characteristics.</p>
----------------------------------------------------------------------
In diva2:1781254 
abstract is: 
<p>The capacitated vehicle routing problem (CVRP) is a well known combinatorial optimization problem with many practical applications. The purpose of solving the problem is to minimize the total cost for a fleet of vehicles, originating from a given depot, to travel to and serve a set of geographically dispersed customers with given demands. The most common real implementations of CVRP scenarios include delivery vehicles of various sorts. The aim of this paper is to present the problem and introduce various solution methods. An example problem will also be introduced and solved using computational methods. The paper will further discuss various properties of the solutions and the methods used to achieve them.</p><p> </p>

corrected abstract:
<p>The capacitated vehicle routing problem (CVRP) is a well known combinatorial optimization problem with many practical applications. The purpose of solving the problem is to minimize the total cost for a fleet of vehicles, originating from a given depot, to travel to and serve a set of geographically dispersed customers with given demands. The most common real implementations of CVRP scenarios include delivery vehicles of various sorts. The aim of this paper is to present the problem and introduce various solution methods. An example problem will also be introduced and solved using computational methods. The paper will further discuss various properties of the solutions and the methods used to achieve them.</p>

Note: Simply remove the unnecessary "<p> </p>" at the end.
----------------------------------------------------------------------
In diva2:1774356
Note: no full text in DiVA

abstract is: 
<p>The topic of this thesis is numerical simulation of low viscosity flow through porous media. It needs to be investigated if part of the carbon in the filter cassettes for a filter system is underutilized when it is time to change the filter media. If this is the case, this problem has to be solved while also keeping the pressure drop over the cassette chamber to a minimum. For this purpose, a redesigned cassette was provided. Due to the complex geometry and physics of the flow, the problem could not be solved analytically. </p><p>The low density and dynamic viscosity of air flowing through a porous medium at high Reynolds numbers meant that a numerical model had to be built and solved with computational fluid dynamics. This was done with the commercial software COMSOL. To establish the validity of the model, a test measurement was done on site where the original filter chamber with cassettes is used. </p><p>The results showed that the proposed redesign of the cassette had a lower pressure drop than the original design. This was confirmed by both the simulation and the infield measurement. Furthermore, the simulations showed that the redesigned cassette had a better flow distribution than the original design. However, there were some changes that had to be done to the filter chamber to improve the overall design of the system. This was done by changing the geometry of the cassette, which allowed the air to flow more evenly in the cassette. Another change was to attempt flow redirection, but this was decided not feasible as it would impact the modularity of the system.</p><p>The next step is further validation with experiments or onsite measurements to increase the validity of the model. When doing this, it is also desirable to attempt to use some sort of flow attenuating technique for the cassette inlet, e.g. honeycomb filter or perforated plate over the inlet. This is to further equalize the velocity distribution at the cassette inlet.</p><p> </p>

corrected abstract:
<p>The topic of this thesis is numerical simulation of low viscosity flow through porous media. It needs to be investigated if part of the carbon in the filter cassettes for a filter system is underutilized when it is time to change the filter media. If this is the case, this problem has to be solved while also keeping the pressure drop over the cassette chamber to a minimum. For this purpose, a redesigned cassette was provided. Due to the complex geometry and physics of the flow, the problem could not be solved analytically. </p><p>The low density and dynamic viscosity of air flowing through a porous medium at high Reynolds numbers meant that a numerical model had to be built and solved with computational fluid dynamics. This was done with the commercial software COMSOL. To establish the validity of the model, a test measurement was done on site where the original filter chamber with cassettes is used. </p><p>The results showed that the proposed redesign of the cassette had a lower pressure drop than the original design. This was confirmed by both the simulation and the infield measurement. Furthermore, the simulations showed that the redesigned cassette had a better flow distribution than the original design. However, there were some changes that had to be done to the filter chamber to improve the overall design of the system. This was done by changing the geometry of the cassette, which allowed the air to flow more evenly in the cassette. Another change was to attempt flow redirection, but this was decided not feasible as it would impact the modularity of the system.</p><p>The next step is further validation with experiments or onsite measurements to increase the validity of the model. When doing this, it is also desirable to attempt to use some sort of flow attenuating technique for the cassette inlet, e.g. honeycomb filter or perforated plate over the inlet. This is to further equalize the velocity distribution at the cassette inlet.</p>

Note: Simply remove the unnecessary "<p> </p>" at the end.
----------------------------------------------------------------------
In diva2:1771786 
abstract is: 
<p>Nuclear power plants generate electricity by means of splitting atoms. The basic safety requirements and objectives are to protect the people, society, and the environment from radiological accidents, limit harmful effects of ionizing radiation during operation and take all reasonable practical steps to prevent radiological accidents. Defense in depth is the concept of preventing and mitigate accidents with multiple layers of protection and is applied in nuclear power plants. Safety systems and safety criteria from regulatory authorities are put in place to ensure defense in depth and fulfill the safety requirements and objectives. The high-pressure injection system injects high concentrations of boron acid into the primary side of the plant, reducing reactivity and power. It has three lines connected to the cold leg of three out of four main coolant loops. Each line uses a piston pump to pump borated water from boron tanks into the primary side. The system is designed to suppress positive reactivity without a pressure drop on the primary side. For this work, the high-pressure injection system is activated at 107% nominal power, a condition for when SCRAM normally is activated. The amount of boron introduced to the system is decided by two main factors, the volumetric flow rate and the boron concentration. System codes for modelling and simulation of power plants have long been used for analysis of reactor dynamic behavior. The modelling and simulation software Apros has been developed for the purpose of modelling nuclear power plant systems. This thesis is conducted at Westinghouse Electric Sweden AB with the purpose of modelling the primary side of a VVER1000. The license, learning material and documentation were provided by the company. A sensitivity study of the boron concentration contra volumetric flow rate of the high-pressure injection system was performed to see if one factor had a larger effect than the other on the primary side. The sensibility study explored two scenarios where reactor trip is unavailable. One scenario where all the control rods are extracted and get stuck and another scenario where all the rods are fully withdrawn, increasing power, temperature and pressure, triggering the pressurizer pressure relief system. The analysis focused on the effects on power and reactor outlet pressure. Results showed that volumetric flow rate affects the system more than boron concentration. In particular, when volumetric flow rate increased to 8.3 m3 /h , the pressure relief system did not activate while it did for 7.3 m3 /h , suggesting that for a limited power increase rate and high enough volumetric flow rate, the high-pressure injection system dampen reactivity, and in extension, pressure enough to not activate the pressure relief system. For future work, the natural continuation of this work is to explore a larger range of boron concentrations and volumetric flow rates. Obtaining validation data and validating the model could yield results that are not purely theoretical.</p>

corrected abstract:
<p>Nuclear power plants generate electricity by means of splitting atoms. The basic safety requirements and objectives are to protect the people, society, and the environment from radiological accidents, limit harmful effects of ionizing radiation during operation and take all reasonable practical steps to prevent radiological accidents. Defense in depth is the concept of preventing and mitigate accidents with multiple layers of protection and is applied in nuclear power plants. Safety systems and safety criteria from regulatory authorities are put in place to ensure defense in depth and fulfill the safety requirements and objectives. The high-pressure injection system injects high concentrations of boron acid into the primary side of the plant, reducing reactivity and power. It has three lines connected to the cold leg of three out of four main coolant loops. Each line uses a piston pump to pump borated water from boron tanks into the primary side. The system is designed to suppress positive reactivity without a pressure drop on the primary side. For this work, the high-pressure injection system is activated at 107% nominal power, a condition for when SCRAM normally is activated. The amount of boron introduced to the system is decided by two main factors, the volumetric flow rate and the boron concentration. System codes for modelling and simulation of power plants have long been used for analysis of reactor dynamic behavior. The modelling and simulation software Apros has been developed for the purpose of modelling nuclear power plant systems. This thesis is conducted at Westinghouse Electric Sweden AB with the purpose of modelling the primary side of a VVER1000. The license, learning material and documentation were provided by the company. A sensitivity study of the boron concentration contra volumetric flow rate of the high-pressure injection system was performed to see if one factor had a larger effect than the other on the primary side. The sensibility study explored two scenarios where reactor trip is unavailable. One scenario where all the control rods are extracted and get stuck and another scenario where all the rods are fully withdrawn, increasing power, temperature and pressure, triggering the pressurizer pressure relief system. The analysis focused on the effects on power and reactor outlet pressure. Results showed that volumetric flow rate affects the system more than boron concentration. In particular, when volumetric flow rate increased to 8.3 m<sup>3</sup>/h , the pressure relief system did not activate while it did for 7.3 m<sup>3</sup>/h, suggesting that for a limited power increase rate and high enough volumetric flow rate, the high-pressure injection system dampen reactivity, and in extension, pressure enough to not activate the pressure relief system. For future work, the natural continuation of this work is to explore a larger range of boron concentrations and volumetric flow rates. Obtaining validation data and validating the model could yield results that are not purely theoretical.</p>
----------------------------------------------------------------------
In diva2:1768252 
abstract is: 
<p>Accurate measurement brain region volumes are important in studying brain plasticity, which brings insight into the fundamental mechanisms in animal, memory, cognitive, and behavior research. The traditional methods of brain volume measurements are ellipsoid or histology. In this study, micro-computed tomography (micro-CT) method was used to achieve more accurate results. However, manual segmentation of micro-CT images is time consuming, hard to reprodu-ce, and has the risk of human error. Automatic image segmentation is a faster method for obtaining the segmentations and has the potential to provide eciency, reliability, repeatability, and scalability. Different methods are tested and compared in this thesis.</p><p>In this project, 29 micro-CT scans of lizard heads were used and measurements of the volumes of 6 dierent brain regions was of interest. The lizard heads were semi-manually segmented into 6 regions and three open-source segmentation algorithms were compared, one atlas-based algorithm and two deep-learning-based algorithms. Dierent number of training data were quantitatively compared for deep-learning methods from all three orientations (sagittal, horizontal and coronal). Data augmentation was tested and compared, as well.</p><p>The comparison shows that the deep-learning algorithms provided more accurate results than the atlas-based algorithm. The results also demonstrated that in the sagittal plane, 5 manually segmented images for training are enough to provide resulting predictions with high accuracy (dice score 0.948). Image augmentation was shown to improve the accuracy of the segmentations but a unique dataset still plays an important role.</p><p>In conclusion, the results show that the manual segmentation work can be reduced drastically by using deep learning for image segmentation.</p>

corrected abstract:
<p>Accurate measurement brain region volumes are important in studying brain plasticity, which brings insight into the fundamental mechanisms in animal, memory, cognitive, and behavior research. The traditional methods of brain volume measurements are ellipsoid or histology. In this study, micro-computed tomography (micro-CT) method was used to achieve more accurate results. However, manual segmentation of micro-CT images is time consuming, hard to reprodu-ce, and has the risk of human error. Automatic image segmentation is a faster method for obtaining the segmentations and has the potential to provide efficiency, reliability, repeatability, and scalability. Different methods are tested and compared in this thesis.</p><p>In this project, 29 micro-CT scans of lizard heads were used and measurements of the volumes of 6 different brain regions was of interest. The lizard heads were semi-manually segmented into 6 regions and three open-source segmentation algorithms were compared, one atlas-based algorithm and two deep-learning-based algorithms. Dierent number of training data were quantitatively compared for deep-learning methods from all three orientations (sagittal, horizontal and coronal). Data augmentation was tested and compared, as well.</p><p>The comparison shows that the deep-learning algorithms provided more accurate results than the atlas-based algorithm. The results also demonstrated that in the sagittal plane, 5 manually segmented images for training are enough to provide resulting predictions with high accuracy (dice score 0.948). Image augmentation was shown to improve the accuracy of the segmentations but a unique dataset still plays an important role.</p><p>In conclusion, the results show that the manual segmentation work can be reduced drastically by using deep learning for image segmentation.</p>
----------------------------------------------------------------------
In diva2:1766567 
abstract is: 
<p>In structural biology, immense effort has been put into discovering functionally relevant atomic resolution protein structures. Still, most experimental, computational and machine learning-based methods alone struggle to capture all the functionally relevant states of many proteins without very involved and system-specific techniques. In this thesis, I propose a new broadly applicable method for determining an ensemble of functionally relevant protein structures. The method consists of (1) generating multiple protein structures from AlphaFold2 by stochastic subsampling of the multiple sequence alignment (MSA) depth, (2) screening these structures using small-angle X-ray scattering (SAXS) data and a structure validation scoring tool, (3) simulating the screened conformers using short molecular dynamics (MD) simulations and (4) refining the ensemble of simulated structures by reweighting it against SAXS data using a bayesian maximum entropy (BME) approach. I apply the method to the T-cell intracellular antigen-1 (TIA-1) protein and find that the generated ensemble is in good agreement with the SAXS data it is fitted to, in contrast to the original set of conformations from AF2. Additionally, the predicted radius of gyration is much more consistent with the experimental value than what is predicted from a 450 ns long MD simulation starting from a single structure. Finally, I cross-validate my findings against small-angle neutron scattering (SANS) data and find that the method-generated ensemble, although not in a perfect way, fits some of the SANS data much better than the ensemble from the long MD simulation. Since the method is fairly automatic, I argue that it could be used by non-experts in MD simulations and also in combination with more advanced methods for more accurate results. I also propose generalisations of the method by tuning it to different biological systems, by using other AI-based methods or a different type of experimental data.</p>

corrected abstract:
<p>In structural biology, immense effort has been put into discovering functionally relevant atomic resolution protein structures. Still, most experimental, computational and machine learning-based methods alone struggle to capture all the functionally relevant states of many proteins without very involved and system-specific techniques. In this thesis, I propose a new broadly applicable method for determining an ensemble of functionally relevant protein structures. The method consists of (1) generating multiple protein structures from AlphaFold2<sup>1</sup> (AF2) by stochastic subsampling of the multiple sequence alignment (MSA) depth, (2) screening these structures using small-angle X-ray scattering (SAXS) data and a structure validation scoring tool, (3) simulating the screened conformers using short molecular dynamics (MD) simulations and (4) refining the ensemble of simulated structures by reweighting it against SAXS data using a bayesian maximum entropy (BME) approach. I apply the method to the T-cell intracellular antigen-1 (TIA-1) protein and find that the generated ensemble is in good agreement with the SAXS data it is fitted to, in contrast to the original set of conformations from AF2. Additionally, the predicted radius of gyration is much more consistent with the experimental value than what is predicted from a 450 ns long MD simulation starting from a single structure. Finally, I cross-validate my findings against small-angle neutron scattering (SANS) data and find that the method-generated ensemble, although not in a perfect way, fits some of the SANS data much better than the ensemble from the long MD simulation. Since the method is fairly automatic, I argue that it could be used by non-experts in MD simulations and also in combination with more advanced methods for more accurate results. I also propose generalisations of the method by tuning it to different biological systems, by using other AI-based methods or a different type of experimental data.</p>


Note: The DiVA entry was missing the superscript for a reference and the string "(AF2)".
----------------------------------------------------------------------
In diva2:1761921   - correct as is
----------------------------------------------------------------------
In diva2:1761894   - correct as is
----------------------------------------------------------------------
In diva2:1761858 
abstract is: 
<p>Transportation via train is considered the most environmentally friendly way of traveling and is widely seen as the future of transportation. Canceled and delayed trains worsen customer satisfaction; thus, punctual trains are crucial for railway companies. One reason for canceled and delayed trains is the shortage of employees due to sickness or care of relatives, known as short-term absences. Therefore, it is important for railway companies to have reliable predictions of these. This thesis is in collaboration with SJ, the largest railway company in Sweden which offers trips all over Sweden and some other parts of northern Europe.</p><p>The thesis predicts short-term absences with data provided by SJ, by using the machine learning methods random forest and extreme gradient boosting (XGBoost). The aim is to investigate if SJ can use machine learning algorithms and statistical analysis in their absence predictions and if it can yield better results than their current absence prediction methodology. Furthermore, the thesis identifies which factors are most important for the predictions. In addition to this, quantile regression is implemented for both methods since overestimating absenteeism could be better for avoiding employee shortage. </p><p>Two different datasets are used for two different tasks; one regression task to predict the number of absent employees on each date and one classification task to predict the probability of an absent employee on a specific duty, and then adding the probabilities to achieve the total predicted number of absent employees on each date. Both task formulations yielded good absence prediction results. XGBoost resulted overall in lower errors than random forest, meaning it was a slightly better model to implement for this task. When comparing the results, the performance for the developed models was better than the current predictions at SJ, meaning machine learning models could benefit SJ's prediction work.</p>

corrected abstract:
<p>Transportation via train is considered the most environmentally friendly way of traveling and is widely seen as the future of transportation. Canceled and delayed trains worsen customer satisfaction; thus, punctual trains are crucial for railway companies. One reason for canceled and delayed trains is the shortage of employees due to sickness or care of relatives, known as short-term absences. Therefore, it is important for railway companies to have reliable predictions of these. This thesis is in collaboration with SJ, the largest railway company in Sweden which offers trips all over Sweden and some other parts of northern Europe.</p><p>The thesis predicts short-term absences with data provided by SJ, by using the machine learning methods random forest and extreme gradient boosting (XGBoost). The aim is to investigate if SJ can use machine learning algorithms and statistical analysis in their absence predictions and if it can yield better results than their current absence prediction methodology. Furthermore, the thesis identifies which factors are most important for the predictions. In addition to this, quantile regression is implemented for both methods since overestimating absenteeism could be better for avoiding employee shortage.</p><p>Two different datasets are used for two different tasks; one regression task to predict the number of absent employees on each date and one classification task to predict the probability of an absent employee on a specific duty, and then adding the probabilities to achieve the total predicted number of absent employees on each date. Both task formulations yielded good absence prediction results. XGBoost resulted overall in lower errors than random forest, meaning it was a slightly better model to implement for this task. When comparing the results, the performance for the developed models was better than the current predictions at SJ, meaning machine learning models could benefit SJ's prediction work.</p>

Note: The only change was the removal of an unnecessaary space at the end of the second paragraph.
----------------------------------------------------------------------
In diva2:620212 - missing space in title:
"A comparison between finite differenceand binomial methods for solvingAmerican single-stock options"
==>
"A comparison between finite difference and binomial methods for solving American single-stock options"

abstract is: 
<p>In this thesis, we compare four different finite-difference solvers with a binomial solver for pricing American options, with a special emphasis on achievable accuracy under computational time constraints. The three finite-difference solvers are: an operator splitting method suggested by S. Ikonen and J. Toivanen, a boundary projection method suggested by M. Brennan and E. Schwartz, projected successive overrelaxation and second order accurate operator splitting method known as Peaceman-Rachford. The binomial method is a modified variant employing an analytical final step as suggested by M. Broadie and J. Detemple. The model problem is an American put option, and we empirically examine the effects of the relevant numerical parameters on the quality of the solutions. For the finite-difference methods we utilize both a Crank-Nicolson discretization and a fully implicit second-order-in-time discretization.</p><p>We conclude that the operator splitting method suggested by S. Ikonen and J. Toivanen is the Alternating Direction Implicit algorithm known as the Douglas-Rachford algorithm. We also conclude that the accuracy of the Peaceman- Rachford algorithm degrades to first order for the American option problem.</p><p>Of the finite-difference methods tried, the Douglas-Rachford algorithm has the highest performance in terms of accuracy under computational time constraints. We conclude that it does, however, not outperform the modified binomial model</p>

corrected abstract:
<p>In this thesis, we compare four different finite-difference solvers with a binomial solver for pricing American options, with a special emphasis on achievable accuracy under computational time constraints. The three finite-difference solvers are: an operator splitting method suggested by S. Ikonen and J. Toivanen, a boundary projection method suggested by M. Brennan and E. Schwartz, projected successive overrelaxation and second order accurate operator splitting method known as Peaceman-Rachford. The binomial method is a modified variant employing an analytical final step as suggested by M. Broadie and J. Detemple. The model problem is an American put option, and we empirically examine the effects of the relevant numerical parameters on the quality of the solutions. For the finite-difference methods we utilize both a Crank-Nicolson discretization and a fully implicit second-order-in-time discretization.</p><p>We conclude that the operator splitting method suggested by S. Ikonen and J. Toivanen is the Alternating Direction Implicit algorithm known as the Douglas-Rachford algorithm. We also conclude that the accuracy of the Peaceman-Rachford algorithm degrades to first order for the American option problem.</p><p>Of the finite-difference methods tried, the Douglas-Rachford algorithm has the highest performance in terms of accuracy under computational time constraints. We conclude that it does, however, not outperform the modified binomial model.</p>

Note: The only chnage was to correct the name hyphenation (for "Peaceman- Rachford").
----------------------------------------------------------------------
In diva2:1057120 
abstract is: 
<p>After a period of high activity within the offshore sector in Norway, they now experience a decrease in activity. At the same time, the railway sector is experiencing high activity and new investments. This leads to a shift in the need of engineering expertise. A relevant area of expertise is the designand engineering processes related to safety. This is a study of such processes in the two industries, based on a comparison of the relevant industry standards, and supported by impressions collected in interviews. The purpose of this study is to investigate into the agility of transfer of safety engineers from the offshore industry and into the railway industry. The study shows that the principles of safety engineering are built on the same grounds. There is however a difference in management approach, and in the use of certain tools and methods. The offshore industry has a more developed quantitative approach, while the railway industry relies more solely on qualitative methods. The offshore industry seems to be more narrow and specialized in their approaches, while the railway industry rely on a broader concept of safety engineering. The study shows that safety engineers are not very dependent of deep technical knowledge, but they need to be able to control the processes in a manner that utilize the knowledge of other experts, to analyse the systems under consideration. Even if the technical skills might not be crucial, it seems evident that being able to communicate on the premises of the industry is vital. This seems like an area relevant for specific training, for new safety engineers entering the railway industry. Railway RAMS1 management has been implemented during the last decade, where quantitative methods are gradually being introduced. Safety engineering has historically been based on the experience of the engineers in the railway industry. The new approach, with an increased focus on reliability and availability, can be a good chance for offshore engineers to bring their expertise to use within a new field of engineering. Even if the overall concept of the industry applications is somewhat different, this study shows that the structure and working methods are similar enough for an agile transfer between the industries.</p>

corrected abstract:
<p>After a period of high activity within the offshore sector in Norway, they now experience a decrease in activity. At the same time, the railway sector is experiencing high activity and new investments. This leads to a shift in the need of engineering expertise. A relevant area of expertise is the design and engineering processes related to safety. This is a study of such processes in the two industries, based on a comparison of the relevant industry standards, and supported by impressions collected in interviews. The purpose of this study is to investigate into the agility of transfer of safety engineers from the offshore industry and into the railway industry. The study shows that the principles of safety engineering are built on the same grounds. There is however a difference in management approach, and in the use of certain tools and methods. The offshore industry has a more developed quantitative approach, while the railway industry relies more solely on qualitative methods. The offshore industry seems to be more narrow and specialized in their approaches, while the railway industry rely on a broader concept of safety engineering. The study shows that safety engineers are not very dependent of deep technical knowledge, but they need to be able to control the processes in a manner that utilize the knowledge of other experts, to analyse the systems under consideration. Even if the technical skills might not be crucial, it seems evident that being able to communicate on the premises of the industry is vital. This seems like an area relevant for specific training, for new safety engineers entering the railway industry. Railway RAMS<sup><a href="#fn1" id="ref1">1</a></sup> management has been implemented during the last decade, where quantitative methods are gradually being introduced. Safety engineering has historically been based on the experience of the engineers in the railway industry. The new approach, with an increased focus on reliability and availability, can be a good chance for offshore engineers to bring their expertise to use within a new field of engineering. Even if the overall concept of the industry applications is somewhat different, this study shows that the structure and working methods are similar enough for an agile transfer between the industries.</p>
<div id="footnotes">
    <ol>
        <li id="fn1">Reliability, Availability, Maintainability, Safety <a href="#ref1" aria-label="Back to reference">↩</a></ĺi>
    </ol>
</div>

----------------------------------------------------------------------
In diva2:729147 
abstract is: 
<p>In the area of pricing insurances, many statistical tools are used. The number</p><p>of tools that exist are overwhelming and it is dicult to know which one to choose.</p><p>In this thesis ve regression models are compared on how good they t the number</p><p>of reported claims in third party automobile insurance. The models considered</p><p>are OLS, Poisson, Negative Binomial and two Hurdle models. The Hurdle models</p><p>are based on Poisson regression and Negative Binomial regression respectively, but</p><p>with additional number of zeros. The AIC and BIC statistics are considered for all</p><p>the models and the predicted number of claims are calculated and compared to the</p><p>observed number of claims. Also, a hypothesis test for the null hypothesis that the</p><p>Hurdle models are not needed is performed. The OLS regression is not suitable for</p><p>this kind of data. This can be explained by the fact that the number of claims are</p><p>not normally distributed. This is the case because many policyholders never report</p><p>any claims and the data therefore includes an excess number of zeros. Also, the</p><p>number of claims can never be negative. The other four models are considerably</p><p>better and all of them t the data satisfactory. The one of them that performs</p><p>best in one test is inadequate in another. The Negative Binomial model is a bit</p><p>better than the other models, but the model choice is not obvious. The conclusion</p><p>is not that a specic model is preferable, but that one need to choose model critically.</p><p>Keywords: Regression Models, Insurance, Count Data, Regression Analysis, Hurdle</p><p>Regression</p>

corrected abstract:
<p>In the area of pricing insurances, many statistical tools are used. The number of tools that exist are overwhelming and it is difficult to know which one to choose. In this thesis five regression models are compared on how good they fit the number of reported claims in third party automobile insurance. The models considered are OLS, Poisson, Negative Binomial and two Hurdle models. The Hurdle models are based on Poisson regression and Negative Binomial regression respectively, but with additional number of zeros. The AIC and BIC statistics are considered for all the models and the predicted number of claims are calculated and compared to the observed number of claims. Also, a hypothesis test for the null hypothesis that the Hurdle models are not needed is performed. The OLS regression is not suitable for this kind of data. This can be explained by the fact that the number of claims are not normally distributed. This is the case because many policyholders never report any claims and the data therefore includes an excess number of zeros. Also, the number of claims can never be negative. The other four models are considerably better and all of them fit the data satisfactory. The one of them that performs best in one test is inadequate in another. The Negative Binomial model is a bit better than the other models, but the model choice is not obvious. The conclusion is not that a specific model is preferable, but that one need to choose model critically.</p>
----------------------------------------------------------------------
In diva2:1849663 
abstract is: 
<p>We introduce a new, accurate, stable, and divergence-free cut finite element discretization for the Stokes interface problem. The method is based on the Brezzi-Douglas-Marini-elements (\textbf{BDM}-elements). We provide analysis to demonstrate that the proposed scheme results in a pointwise divergence-free velocity field, and we prove consistency, continuity, coercivity, and an inf-sup result. </p><p>Additionally, we present three numerical experiments that support the theoretical results. We utilize the element pair $(\textbf{BDM}_1, Q_0)$, that is, \textbf{BDM}$_1$-elements for the velocity and piecewise constant polynomials for the pressure. These numerical experiments show that the method is robust and attains an optimal convergence order of two for the velocity and one for the pressure</p>

corrected abstract:
<p>We introduce a new, accurate, stable, and divergence-free cut finite element discretization for the Stokes interface problem. The method is based on the Brezzi-Douglas-Marini-elements ([<strong>BDM</strong>-elements). We provide analysis to demonstrate that the proposed scheme results in a pointwise divergence-free velocity field, and we prove consistency, continuity, coercivity, and an inf-sup result.</p><p>Additionally, we present three numerical experiments that support the theoretical results. We utilize the element pair (<strong>BDM</strong><sub>1</sub>, Q<sub>0</sub>), that is, [<strong>BDM</strong><sub>1</sub>-elements for the velocity and piecewise constant polynomials for the pressure. These numerical experiments show that the method is robust and attains an optimal convergence order of two for the velocity and one for the pressure.</p>
----------------------------------------------------------------------
In diva2:566653 
abstract is: 
<p>The recent development of CubeSat nano-satellites shows that it is an effective way to send a payload onto orbit, as it is a relatively inexpensive and quick access to space. If the small size of these satellites is their main advantage, it is also the principal source of problems when it comes to designing it. The control electronics, electric power system and the payload are limited in mass and have to t in a tiny ten-centimeter cube. The necessity of a compact deployable structure to hold the payload once the satellite reached its orbit is one of the principal subject of study for the design of a CubeSat. In the CubeSat program SWIM (Space Weather using Ion Spectrometers and Magnetometers) that KTH takes part in, the  deployablestructure developed consists of bi-stable semi-tubular booms made by a woven-composite fabric. Preliminary tests show that this structure is very compact and stable in the packaged con guration while being suciently long and stiin the deployed con guration. However little is known about the deployment phase, the physical model of the booms is very inaccurate in determining the deployment force and speed, because of the complexity of the material mechanics behind it. Modeling a woven composite material in a nite element analysis software is a dicult task due the structure of the material itself. The ber yarns interlace each other like in textile material, and they are impregnated in a soft matrix resin. Although in-plane properties of these materials can be calculated accurately using the classic lamination theory (CLT), the corresponding out-of-plane properties lack any accuracy for one-ply woven composites. Solutions are found through micromechanical approaches but these models are dicult to implement and are computationally expensive. The solution to this problem is to decline the CLT model of the material in two versions, each with a its own purpose. This paper presents rst a CLT model of the woven composite which aim is to predict in-plane properties accurately and giving a good estimation of the out-of-plane properties. The second version of the CLT model is developed with the aim of predicting accurately the amount of strain energy stored and the stable radius of the rolled-up con guration. The purpose of this version is to be used in deployment analysis. This paper also presents the main lines of a fully parameterized nite element model of the deployment analysis for future use.</p>

corrected abstract:
<p>The recent development of CubeSat nano-satellites shows that it is an effective way to send a payload onto orbit, as it is a relatively inexpensive and quick access to space. If the small size of these satellites is their main advantage, it is also the principal source of problems when it comes to designing it. The control electronics, electric power system and the payload are limited in mass and have to fit in a tiny ten-centimeter cube. The necessity of a compact deployable structure to hold the payload once the satellite reached its orbit is one of the principal subject of study for the design of a CubeSat. In the CubeSat program SWIM (Space Weather using Ion Spectrometers and Magnetometers) that KTH takes part in, the deployable structure developed consists of bi-stable semi-tubular booms made by a woven-composite fabric. Preliminary tests show that this structure is very compact and stable in the packaged configuration while being sufficiently long and stiff in the deployed configuration. However little is known about the deployment phase, the physical model of the booms is very inaccurate in determining the deployment force and speed, because of the complexity of the material mechanics behind it. Modeling a woven composite material in a finite element analysis software is a difficult task due the structure of the material itself. The fiber yarns interlace each other like in textile material, and they are impregnated in a soft matrix resin. Although in-plane properties of these materials can be calculated accurately using the classic lamination theory (CLT), the corresponding out-of-plane properties lack any accuracy for one-ply woven composites. Solutions are found through micromechanical approaches but these models are difficult to implement and are computationally expensive. The solution to this problem is to decline the CLT model of the material in two versions, each with a its own purpose. This paper presents first a CLT model of the woven composite which aim is to predict in-plane properties accurately and giving a good estimation of the out-of-plane properties. The second version of the CLT model is developed with the aim of predicting accurately the amount of strain energy stored and the stable radius of the rolled-up configuration. The purpose of this version is to be used in deployment analysis. This paper also presents the main lines of a fully parameterized finite element model of the deployment analysis for future use.</p>
----------------------------------------------------------------------
In diva2:877595 
abstract is: 
<p> </p><p>A recent trend in the world is that more and more countries and therefore their mil-itaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to ad-opt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense com-pany, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an e˙ective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare di˙erent solutions. The delta model includes reference values for relevant costs that can be a˙ected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>

corrected abstract:
<p>A recent trend in the world is that more and more countries and therefore their militaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to adopt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense company, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an effective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare different solutions. The delta model includes reference values for relevant costs that can be affected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>
----------------------------------------------------------------------
In diva2:571139 
abstract is: 
<p>In this paper heursitic algorithms are developed for the pursuit evasion problem in polygonal enviroments.</p><p>In this problem, continuous trajectories shall be constructed for a group of pursuers, searching for an</p><p>evader, in such a way that the evader is guaranteed to be seen at some time during the search. Three</p><p>fundamentaly dierent heuristic methods are considered: tabu search, genetic algorithms and greedy</p><p>methods. The result is three heuristic algorithms. Two algorithms are readily implemented in ANSI C,</p><p>yielding solutions of high quality compared to previous work. The report attains and evaluates statistics</p><p>on runtime of the algorithms. The algorithms are compared considering the quality and e-ciency for a</p><p>vast amount of randomly generated enviroments.</p><p>Key-words</p><p></p><p>: Pursuit and Evasion, Heuristic algorithms, tabu search, greedy methods, genetic algorithms.</p>

corrected abstract:
<p>In this paper heursitic algorithms are developed for the pursuit evasion problem in polygonal enviroments. In this problem, continuous trajectories shall be constructed for a group of pursuers, searching for an evader, in such a way that the evader is guaranteed to be seen at some time during the search. Three fundamentaly different heuristic methods are considered: tabu search, genetic algorithms and greedy methods. The result is three heuristic algorithms. Two algorithms are readily implemented in ANSI C, yielding solutions of high quality compared to previous work. The report attains and evaluates statistics on runtime of the algorithms. The algorithms are compared considering the quality and efficiency for a vast amount of randomly generated enviroments.</p>
----------------------------------------------------------------------
In diva2:1800177 
abstract is: 
<p>This paper uses a back-propagating neural network (BPN) to predict the price movements of major crypto currencies, leveraging technical factors as well as measurements of collective sentiment derived from the micro-blogging network Twitter. Our dataset consists of daily, hourly and minutely price levels for Bitcoin, Ether and Litecoin along with 8 popular technical indicators, as well as all tweets with the currencies' cash tags during respective time periods. Insprired by previous research which suggest that artificial neural networks are superior forecasting models in this setting, we were able to create a system generating automated investment decisions on a daily, hourly and minutely time basis. The study concluded that price trends are indeed predictable, with a correct prediction rate above 50% for all models, and corrensponding profitable trading strategies for all currencies on an hourly basis when neglecting trading fees, buy-sell spreads and order delays. The overall highest predictability is obtained on the hourly trading interval for Bitcoin, yielding an accuracy of 55.74% and a cumulative return of 175.1% between October 16, 2021 and December 31, 2021.</p>

corrected abstract:
<p>This paper uses a back-propagating neural network (BPN) to predict the price movements of major crypto currencies, leveraging technical factors as well as measurements of collective sentiment derived from the micro-blogging network Twitter. Our dataset consists of daily, hourly and minutely price levels for Bitcoin, Ether and Litecoin along with 8 popular technical indicators , as well as all tweets with the currencies' cash tags during respective time periods. Inspired by previous research which suggests that artificial neural networks are superior forecasting models in this setting, we were able to create a system generating automated investment decisions on a daily, hourly and minutely time basis. The study concluded that price trends are indeed predictable, with a correct prediction rate above 50% for all models, and corresponding profitable trading strategies for all currencies on an hourly basis when neglecting trading fees, buy-sell spreads and order delays. The overall highest predictability is obtained on the hourly trading interval for Bitcoin, yielding an accuracy of 55.74% and a cumulative return of 175.1% between October 16, 2021 and December 31, 2021.</p>
----------------------------------------------------------------------
In diva2:562089 
abstract is: 
<p>Through monetary policy, central banks aim to prevent societal costs associated with high</p><p>or unstable ination. Forecasts and several other tools are used to provide guidance to</p><p>this end, as outcomes of interest rate decisions are not fully predictable.</p><p>This report presents a statistical approach, viewing the development of the economy as</p><p>a Markov chain. The economy is thus represented by a nite number of states, composed</p><p>of ination and short-term variations in GDP. The Markov property is assumed to hold,</p><p>that is, the economy moves between states over an appropriately chosen time period and</p><p>the transition probabilities depend only on the initial state. Using the Markov Decision</p><p>Process (MDP) framework, the transition probabilities between such states are evaluated</p><p>using historical data, distinguished by the interest rate decision preceding the transition.</p><p>Completing the model, a cost of ination is de ned for each state as the deviation from</p><p>a set target. An optimal policy is then determined as a xed decision for each state,</p><p>minimizing the expected average cost incurred while using the model.</p><p>The model is evaluated on data from Sweden and the U.S., for periods 1994-2007 and</p><p>1954-2007 respectively. The results are assessed by the estimated transition probabilities</p><p>as well as by the optimal policy suggested. While the Swedish observations are concluded</p><p>to be too few in number to render valuable results, outcomes using the U.S. data agree</p><p>in several aspects with what would have been expected from macroeconomic theory. In</p><p>conclusion, the results suggest that the model might be applied to the problem, granted</p><p>sucient data is available for reliable transition probabilities to be estimated and that</p><p>this estimation can be performed in an unbiased way. Presently, this appears to be a</p><p>dicult task.</p>

corrected abstract:
<p>Through monetary policy, central banks aim to prevent societal costs associated with high or unstable inflation. Forecasts and several other tools are used to provide guidance to this end, as outcomes of interest rate decisions are not fully predictable.</p><p>This report presents a statistical approach, viewing the development of the economy as a Markov chain. The economy is thus represented by a finite number of states, composed of inflation and short-term variations in GDP. The Markov property is assumed to hold, that is, the economy moves between states over an appropriately chosen time period and the transition probabilities depend only on the initial state. Using the Markov Decision Process (MDP) framework, the transition probabilities between such states are evaluated using historical data, distinguished by the interest rate decision preceding the transition. Completing the model, a cost of inflation is defined for each state as the deviation from a set target. An optimal policy is then determined as a fixed decision for each state, minimizing the expected average cost incurred while using the model.</p><p>The model is evaluated on data from Sweden and the U.S., for periods 1994-2007 and 1954-2007 respectively. The results are assessed by the estimated transition probabilities as well as by the optimal policy suggested. While the Swedish observations are concluded to be too few in number to render valuable results, outcomes using the U.S. data agree in several aspects with what would have been expected from macroeconomic theory. In conclusion, the results suggest that the model might be applied to the problem, granted sufficient data is available for reliable transition probabilities to be estimated and that this estimation can be performed in an unbiased way. Presently, this appears to be a difficult task.</p>
----------------------------------------------------------------------
In diva2:559489 
abstract is: 
<p>The performance of an optimal-weighted portfolio strategy is evaluated when transaction costs are penalized compared to an equal-weighted portfolio strategy. The optimal allocation weights are found by maximizing a modified Sharpe ratio measure each trading day, where modified refers to the expected return of an asset in this context. The leverage of the investment is determined by a conditional expectation estimate of the number of portfolio assets of the next-coming day. A moving window is used to historically measure the transition probabilities of moving from one state to another within this stochastic count process and this is used as an input to the estimator. It is found that the most accurate estimate is the actual trading day’s number of portfolio assets and this is obtained when the size of the moving window is one. Increasing the penalty parameter on transaction costs of selling and buying assets between trading days lowers the aggregated transaction cost and increases the performance of the optimal-weighted portfolio considerably. The best portfolio performance is obtained when at least 50% of the capital is invested equally among the assets when maximizing the modified Sharpe ratio. The optimal-weighted and equal-weighted portfolios are constructed on a daily basis, where the allowed VaR0:05 is €300 000 for each portfolio. This sets the limit on the amount of capital allowed to be invested each trading day, and is determined by empirical VaR0:05 simulations of these two portfolios.</p>

corrected abstract:
<p>The performance of an optimal-weighted portfolio strategy is evaluated when transaction costs are penalized compared to an equal-weighted portfolio strategy. The optimal allocation weights are found by maximizing a modified Sharpe ratio measure each trading day, where modified refers to the expected return of an asset in this context. The leverage of the investment is determined by a conditional expectation estimate of the number of portfolio assets of the next-coming day. A moving window is used to historically measure the transition probabilities of moving from one state to another within this stochastic count process and this is used as an input to the estimator. It is found that the most accurate estimate is the actual trading day’s number of portfolio assets and this is obtained when the size of the moving window is one. Increasing the penalty parameter on transaction costs of selling and buying assets between trading days lowers the aggregated transaction cost and increases the performance of the optimal-weighted portfolio considerably. The best portfolio performance is obtained when at least 50% of the capital is invested equally among the assets when maximizing the modified Sharpe ratio. The optimal-weighted and equal-weighted portfolios are constructed on a daily basis, where the allowed VaR<sub>0.05</sub> is €300 000 for each portfolio. This sets the limit on the amount of capital allowed to be invested each trading day, and is determined by empirical VaR<sub>0.05</sub> simulations of these two portfolios.</p>
----------------------------------------------------------------------
In diva2:912828 
abstract is: 
<p>When the Gemini Telescopes were built 20 years ago, the control architecture for the high precision requirements was the same for all systems. It consisted of VME cards, Programmable Multi Axis Con-trollers (PMAC) with speciﬁc ampliﬁers and a central computer. It was the state of the art. Nevertheless such an infrastructure takes up a lot of space. It is also diﬃcult to maintain mainly because of obsoles-cence and the lack of support of the engineers who do not consider this system adapted to the control.The code, documentation and wiring were complex and not fully understood.</p><p>Thus, it led the Gemini Observatory to consider the acquisition of a new controller for one of the most critical unit: the Acquisition and Guidance (A&amp;G). To address these issues, I propose an alternative control system which will not only solve the current issues but also improve the performance.</p><p>The new generation of controller from Delta Tau is more eﬃcient, more reliable and with a high level of integration. The main concern of compatibility with the current motors and encoders of the Acquisition and Guidance has been solved by testing 27 motors out of the 29 present in the unit. Results were obtained using a test bench and mechanical systems built during the internship. A fully functional test bench has been delivered.</p><p>Furthermore, a new control scheme for the backlash compensation is proposed. It consumes half the energy of the current one, is nearly two times faster and without oscillations. This will reduce the frequency of maintenance and the reliability of the unit. A cross gantry control for the skew compensation of the Science fold leads to a smarter control of the diﬀerences between the motors to prevent the mirror from breaking. Finally, an identiﬁcation technique for the tilt mechanism provides more robustness and takes into account the ageing of the equipment.</p>

corrected abstract:
<p>When the Gemini Telescopes were built 20 years ago, the control architecture for the high precision requirements was the same for all systems. It consisted of VME cards, Programmable Multi Axis Controllers (PMAC) with specific amplifiers and a central computer. It was the state of the art. Nevertheless such an infrastructure takes up a lot of space. It is also difficult to maintain mainly because of obsolescence and the lack of support of the engineers who do not consider this system adapted to the control. The code, documentation and wiring were complex and not fully understood.</p><p>Thus, it led the Gemini Observatory to consider the acquisition of a new controller for one of the most critical unit: the Acquisition and Guidance (A&amp;G). To address these issues, I propose an alternative control system which will not only solve the current issues but also improve the performance.</p><p>The new generation of controller from Delta Tau is more efficient, more reliable and with a high level of integration. The main concern of compatibility with the current motors and encoders of the Acquisition and Guidance has been solved by testing 27 motors out of the 29 present in the unit. Results were obtained using a test bench and mechanical systems built during the internship. A fully functional test bench has been delivered.</p><p>Furthermore, a new control scheme for the backlash compensation is proposed. It consumes half the energy of the current one, is nearly two times faster and without oscillations. This will reduce the frequency of maintenance and the reliability of the unit. A cross gantry control for the skew compensation of the Science fold leads to a smarter control of the differences between the motors to prevent the mirror from breaking. Finally, an identification technique for the tilt mechanism provides more robustness and takes into account the ageing of the equipment.</p>
----------------------------------------------------------------------
In diva2:1568344 
abstract is: 
<p>This thesis investigates the nonlinear partial differential equation known as sine-Gordon and its special soliton solutions.Simpler analytical results are derived and more advanced methods and their results are discussed.Further, a finite difference scheme is derived, implemented and compared against a known energy conserving scheme of sine-Gordon in terms of stability, accuracy, convergence and computation time.The complete solvability of the equation enables comparison between numerical solutions and their analytical counterparts.No unified answer to which numerical scheme is best was determined as they both were shown to have pros and cons.</p>

corrected abstract:
<p>This thesis investigates the nonlinear partial differential equation known as sine-Gordon and its special soliton solutions. Simpler analytical results are derived and more advanced methods and their results are discussed. Further, a finite difference scheme is derived, implemented and compared against a known energy conserving scheme of sine-Gordon in terms of stability, accuracy, convergence and computation time. The complete solvability of the equation enables comparison between numerical solutions and their analytical counterparts. No unified answer to which numerical scheme is best was determined as they both were shown to have pros and cons.</p>
----------------------------------------------------------------------
In diva2:732062 
abstract is: 
<p> </p><p></p><p>This report will provide an overview of climate modeling from a mathematical perspective, particularly with respect to the use of partial differential equations. A visit to the Swedish Meterological and Hydrological Institute's Rossby Center for climate research in Norrkoping, Sweden, is at the foundation of our investigations. An introduction and a brief history section will be followed by a description of the Navier-Stokes equations, which are at</p><p>the heart of climate-related mathematics, as well as a survey of many of the</p><p>popular approximations and modeling techniques in use by climate researchers</p><p>today. Subsequently, a boundary value problem based on the one dimensional</p><p>compressible Euler equations will be discussed from an analytical as well as a</p><p>numerical point of view, especially with concern to the well-posedness of the</p><p>same.</p><p> </p>

corrected abstract:
<p>This report will provide an overview of climate modeling from a mathematical perspective, particularly with respect to the use of partial differential equations. A visit to the Swedish Meterological and Hydrological Institute's Rossby Center for climate research in Norrköping, Sweden, is at the foundation of our investigations. An introduction and a brief history section will be followed by a description of the Navier-Stokes equations, which are at the heart of climate-related mathematics, as well as a survey of many of the popular approximations and modeling techniques in use by climate researchers today. Subsequently, a boundary value problem based on the one dimensional compressible Euler equations will be discussed from an analytical as well as a numerical point of view, especially with concern to the well-posedness of the same.</p>
----------------------------------------------------------------------
In diva2:1709501 
abstract is: 
<p>This thesis develops and evaluates a physics-informed neural network (PINN) modelling framework for solving inverse problems in epidemiology. The PINN works by modifying the standard mean squared error loss function of the neural network, by adding a term penalizing deviations from a given compartmental model's system of ordinary differential equations. To find estimates for the unknown parameters in the compartmental model, such as the transmission rate, this compound loss function is then minimized with respect to both the neural network's inherent parameters and the unknown parameters in the compartmental model. The following question guided the study: Given time-series data consisting of the 7-day rolling average of the daily incidence of new infectious individuals, and a compartmental model for that data, can a PINN learn the corresponding time-dependent transmission rate parameter? The PINN framework was first validated on simulated (synthetic) epidemiological data, where the PINN was tasked o retrieve the unknown parameters in a given three-compartment SIR (Susceptible-Infectious-Recovered) model. It was then tested on real Covid-19 case data, and tasked to retrieve a time-dependent transmission rate parameter in an SEIR (Susceptible-Exposed-INfectious-Recovered) model. Two different approaches to learning a time-dependent transmission rate based were compared: one assumed a sigmoidal transmission rate with three unknown parameters (model IIa); the other allowed the transmission rate to be aprameterized by the neural netowrk, by adding it as an additional output node (Model IIb). The findings were that the PINN was able to reliably retrieve unknown constant parameters in an SIR model based on simulated data. However, it was also found that the PINN's parameter estimates can be sensitive to noise. Moreover, when learning a time-dependent transmission rate with Model IIb, an important finding was that the PINN would struggle to converge to the true transmission rate in regions of time when there were a relatively low number of total infections. Nevertheless, when employed on Covid-19 data from Stockholm county corresponding to the first wave, the PINN was still able to extract a time-dependent transmission rate in the given SEIR model, largely consistent with the 7-day rolling average of the incidence of new cases, without imposing any a priori assumptions on the shape of the transmission rate other than it should be positive.</p>

corrected abstract:
<p>This thesis develops and evaluates a physics-informed neural network (PINN) modelling framework for solving inverse problems in epidemiology. The PINN works by modifying the standard mean squared error loss function of the neural network, by adding a term penalizing deviations from a given compartmental model's system of ordinary differential equations. To find estimates for the unknown parameters in the compartmental model, such as the transmission rate, this compound loss function is then minimized with respect to both the neural network's inherent parameters and the unknown parameters in the compartmental model. The following question guided the study: <em>Given time-series data consisting of the 7-day rolling average of the daily incidence of new infectious individuals, and a compartmental model for that data, can a PINN learn the corresponding time-dependent transmission rate parameter?</em> The PINN framework was first validated on simulated (synthetic) epidemiological data, where the PINN was tasked to retrieve the unknown parameters in a given three-compartment SIR (Susceptible-Infectious-Recovered) model. It was then tested on real Covid-19 case data, and tasked to retrieve a time-dependent transmission rate parameter in an SEIR (Susceptible-Exposed-Infectious-Recovered) model. Two different approaches to learning a time-dependent transmission rate based were compared: one assumed a sigmoidal transmission rate with three unknown parameters (Model IIa); the other allowed the transmission rate to be parameterized by the neural network, by adding it as an additional output node (Model IIb). The findings were that the PINN was able to reliably retrieve unknown constant parameters in an SIR model based on simulated data. However, it was also found that the PÌNN's parameter estimates can be sensitive to noise. Moreover, when learning a time-dependent transmission rate with Model IIb, an important finding was that the PINN would struggle to converge to the true transmission rate in regions of time when there were a relatively low number of total infectives. Nevertheless, when employed on Covid-19 data from Stockholm county corresponding to the first wave, the PINN was still able to extract a time-dependent transmission rate in the given SEIR model, largely consistent with the 7-day rolling average of the incidence of new cases, without imposing any a priori assumptions on the shape of the transmission rate other than it should be positive.</p>


Note that "PÌNN's" is in the original thesis abstract.
----------------------------------------------------------------------
In diva2:555900 
abstract is: 
<p>Abstract</p><p>We propose an algorithm to price and analyze the performance of auto-callable structured _nancial products. The algorithm contains Monte-Carlo simulations in order to reproduce, as probable as possible, a future product. This model is then compared to other, previously presented models. The di_erent in-data parameters together with a time dependency study is then performed to evaluate what one might expect when investing in these products. Numerical results conclude that, the risks taken by the investor closely reect the potential return for each product. When constructing these products for the near future, one must closely evaluate the demand from the investors i.e. evaluate the level of risk that the investors are willing to take.</p>

corrected abstract:
<p>We propose an algorithm to price and analyze the performance of auto-callable structured financial products. The algorithm contains Monte-Carlo simulations in order to reproduce, as probable as possible, a future product. This model is then compared to other, previously presented models. The different in-data parameters together with a time dependency study is then performed to evaluate what one might expect when investing in these products. Numerical results conclude that, the risks taken by the investor closely reflect the potential return for each product. When constructing these products for the near future, one must closely evaluate the demand from the investors i.e. evaluate the level of risk that the investors are willing to take.</p>
----------------------------------------------------------------------
In diva2:760102 
abstract is: 
<p>This report is a study if a multi linear regression could be used to predict the cap hit of hockey forwards from the NHL. Data was collected during the 2010-2011, 2011-2012, and 2012-2013 seasons. The chosen variables were common hockey statistics and a few none hockey-related, like origin and age. The initial model was improved by removing insignicant covariates, detected by BIC-test and p-values. The final model consisted of 291 players and had an adjusted R<sup>2</sup>-value of 0,7820. Of the covariates, goals, assists and ice time had the biggest impact on a player's cap hit.</p>

corrected abstract:
<p>This report is a study if a multi linear regression could be used to predict the cap hit of hockey forwards from the NHL. Data was collected during the 2010-2011, 2011-2012, and 2012-2013 seasons. The chosen variables were common hockey statistics and a few none hockey-related, like origin and age. The initial model was improved by removing insignificant covariates, detected by BIC-test and p-values. The final model consisted of 291 players and had an adjusted R<sup>2</sup>-value of 0,7820. Of the covariates, goals, assists and ice time had the biggest impact on a player's cap hit.</p>
----------------------------------------------------------------------
in diva2:1833714 
abstract is: 
<p>this study focuses on initial public offerings (ipos), which are the process of making a company's shares available for public trading on a stock market. despite global uncertainties in recent years, there has been a high demand for company listings in the market. many ipos have experienced a positive trend in share prices on the first day of trading as a publicly traded company.the objective of this study is to develop a multiple linear regression model to analyze the impact of various parameters on the first day return of ipos. the generated model will be evaluated to create a reduced model with an optimal subset of variables. the study will specifically focus on ipos listed on some nordic marketplaces during the period 2017-2022.the results of the study suggest that the created models are not effective in capturing the variance of first day returns. the deficiency of the created models is likely due to both the complexity of the stock market and the difficulties of quantifying and capturing some of the factors impacting the initial performance of ipos. however, the reduced model performs slightly better and indicates that variables such as ngm sme, first north, presubscribed, and amountnewlyissued explain most of the variance in the response variable.</p>

corrected abstract:
<p>This study focuses on initial public offerings (IPOs), which are the process of making a company's shares available for public trading on a stock market. Despite global uncertainties in recent years, there has been a high demand for company listings in the market. Many IPOs have experienced a positive trend in share prices on the first day of trading as a publicly traded company.</p><p>The objective of this study is to develop a multiple linear regression model to analyze the impact of various parameters on the first day return of IPOs. The generated model will be evaluated to create a reduced model with an optimal subset of variables. The study will specifically focus on IPOs listed on some Nordic marketplaces during the period 2017-2022.</p><p>The results of the study suggest that the created models are not effective in capturing the variance of first day returns. The deficiency of the created models is likely due to both the complexity of the stock market and the difficulties of quantifying and capturing some of the factors impacting the initial performance of IPOs. However, the reduced model performs slightly better and indicates that variables such as NGM SME, First North, PreSubscribed, and AmountNewlyIssued explain most of the variance in the response variable.</p>
----------------------------------------------------------------------
In diva2:1359762 
abstract is: 
<p>When designing a building, sound is one of the problems to take into account. Vibrating machines, such as ventilation fans, water pumps and compressors, generate structure-borne sound. The structure-borne sound travels up the structure of the building and generates sound in adjacent rooms. To be able to predict the sound radiated in the adjacent rooms when designing a building, a semi-analytical model has been developed. Using the incident vibrations from the floor plate where the vibrating machine is standing, the transmission loss in the junction between the floor plates and the wall plate is calculated. This can bed one in every junction in the building, creating a system of multiple junctions. The sound radiation to the adjacent rooms is later approximated using the velocity of the plates.The model is verified with measurements in two case studies. This shows that the model has good potential in predicting the normal acceleration amplitudes in the relevant plates. The two case studies have different geometric properties and different sources. The comparison between the model and the measurement gives similar results. The model analyses the output of the bending waves since this is the wave type that radiates sound, but longitudinal waves are present in the model. With only two case studies it is too early to say that the model works for all systems, but it could be used as a fist approach. The model, right now, is restricted to isotropic, homogeneous material without losses. A parametric study shows that the transmission loss is dependent on the ratio between the thicknesses of the floor plate and the wall plate. The ratio should be as large as possible to get a high transmission loss, but depends on how the junction is structured.</p>
mc='plates.The' c='plates. The'

partal corrected: diva2:1359762: <p>When designing a building, sound is one of the problems to take into account. Vibrating machines, such as ventilation fans, water pumps and compressors, generate structure-borne sound. The structure-borne sound travels up the structure of the building and generates sound in adjacent rooms. To be able to predict the sound radiated in the adjacent rooms when designing a building, a semi-analytical model has been developed. Using the incident vibrations from the floor plate where the vibrating machine is standing, the transmission loss in the junction between the floor plates and the wall plate is calculated. This can bed one in every junction in the building, creating a system of multiple junctions. The sound radiation to the adjacent rooms is later approximated using the velocity of the plates. The model is verified with measurements in two case studies. This shows that the model has good potential in predicting the normal acceleration amplitudes in the relevant plates. The two case studies have different geometric properties and different sources. The comparison between the model and the measurement gives similar results. The model analyses the output of the bending waves since this is the wave type that radiates sound, but longitudinal waves are present in the model. With only two case studies it is too early to say that the model works for all systems, but it could be used as a fist approach. The model, right now, is restricted to isotropic, homogeneous material without losses. A parametric study shows that the transmission loss is dependent on the ratio between the thicknesses of the floor plate and the wall plate. The ratio should be as large as possible to get a high transmission loss, but depends on how the junction is structured.</p>
w='.The' val={'c': '. The', 's': ['diva2:915628', 'diva2:1776821', 'diva2:1527832', 'diva2:1188275', 'diva2:618564', 'diva2:912828', 'diva2:1568344', 'diva2:1833714', 'diva2:1359762', 'diva2:1083484', 'diva2:618592', 'diva2:1263422', 'diva2:1465518', 'diva2:1244654', 'diva2:1298377', 'diva2:1183380', 'diva2:1380198', 'diva2:412700', 'diva2:1880821', 'diva2:1698347', 'diva2:559083', 'diva2:1083457', 'diva2:1110767', 'diva2:1834483', 'diva2:459344', 'diva2:1698423', 'diva2:1247197', 'diva2:1299489', 'diva2:1078073', 'diva2:1465511', 'diva2:1613411', 'diva2:1183272', 'diva2:1244644', 'diva2:1298486', 'diva2:919311', 'diva2:1894641', 'diva2:854657', 'diva2:1216849', 'diva2:1293627', 'diva2:1263450', 'diva2:1800176', 'diva2:408836', 'diva2:1142969', 'diva2:503959', 'diva2:401129', 'diva2:938784', 'diva2:1110812', 'diva2:516084', 'diva2:1739365', 'diva2:405993', 'diva2:1701306', 'diva2:1142785', 'diva2:1120402', 'diva2:1136781', 'diva2:1040684', 'diva2:1681378', 'diva2:1083454', 'diva2:608435', 'diva2:1110830', 'diva2:1270437', 'diva2:1699779', 'diva2:1683766', 'diva2:1673571', 'diva2:1465540', 'diva2:1677530', 'diva2:1801976', 'diva2:1779369', 'diva2:1345189', 'diva2:1247301', 'diva2:1292396', 'diva2:405938', 'diva2:1184058', 'diva2:1900963', 'diva2:1527916', 'diva2:1739380', 'diva2:1357370', 'diva2:1816745', 'diva2:1380196', 'diva2:485806', 'diva2:1085448', 'diva2:1737092', 'diva2:1238555', 'diva2:919302', 'diva2:1817109', 'diva2:919848', 'diva2:1426161', 'diva2:1816888', 'diva2:585833', 'diva2:1509432', 'diva2:1440619', 'diva2:1888131', 'diva2:1613485', 'diva2:1800223', 'diva2:872213', 'diva2:1247567', 'diva2:1111560', 'diva2:1287120', 'diva2:1285512', 'diva2:618555', 'diva2:1817475', 'diva2:1673657', 'diva2:1033800', 'diva2:1852460', 'diva2:1806869', 'diva2:1862229', 'diva2:1876088', 'diva2:1741190', 'diva2:1127919', 'diva2:920005', 'diva2:1082654', 'diva2:1609997', 'diva2:1871461', 'diva2:1808437', 'diva2:1595616', 'diva2:1249827', 'diva2:1877759', 'diva2:1078086', 'diva2:1756983', 'diva2:1701469', 'diva2:1871601', 'diva2:515592', 'diva2:515581', 'diva2:515572', 'diva2:408831', 'diva2:1823869', 'diva2:891537', 'diva2:1698159', 'diva2:1592077', 'diva2:1644922', 'diva2:515459', 'diva2:1188262', 'diva2:783982', 'diva2:1357321', 'diva2:1857264', 'diva2:1881041', 'diva2:1764265', 'diva2:1894689', 'diva2:1729237', 'diva2:1757012', 'diva2:1334020', 'diva2:1188292', 'diva2:705805', 'diva2:1781270', 'diva2:1776549', 'diva2:401124', 'diva2:1040617', 'diva2:1110758', 'diva2:748438', 'diva2:1516123', 'diva2:618595', 'diva2:1781523', 'diva2:1901232', 'diva2:408837', 'diva2:408838', 'diva2:1719082', 'diva2:1674003', 'diva2:1816899', 'diva2:784019', 'diva2:1216861', 'diva2:1249325', 'diva2:783984', 'diva2:643818', 'diva2:919831', 'diva2:1188300', 'diva2:1287155', 'diva2:1587914', 'diva2:1880984', 'diva2:1817132', 'diva2:1528127', 'diva2:1350190', 'diva2:491817', 'diva2:515568', 'diva2:1803505', 'diva2:1142776', 'diva2:1120852', 'diva2:1464110', 'diva2:737929', 'diva2:1133508', 'diva2:1183313', 'diva2:1577149', 'diva2:492846', 'diva2:783994', 'diva2:1229031', 'diva2:1216812', 'diva2:408835', 'diva2:515507', 'diva2:515496', 'diva2:408834', 'diva2:891555', 'diva2:1184068', 'diva2:1781241', 'diva2:1335459', 'diva2:1820859', 'diva2:1761916', 'diva2:1237812', 'diva2:1745587', 'diva2:783990', 'diva2:642321', 'diva2:515487', 'diva2:408555', 'diva2:1249040', 'diva2:1646404', 'diva2:1240365', 'diva2:1285510', 'diva2:1321182', 'diva2:492864', 'diva2:1820981', 'diva2:1307667', 'diva2:1877617', 'diva2:1183391', 'diva2:1216784', 'diva2:513813', 'diva2:1527828', 'diva2:618588', 'diva2:1319886', 'diva2:1757049', 'diva2:1876262', 'diva2:1292412', 'diva2:1527803', 'diva2:1499819', 'diva2:1078070', 'diva2:1881338', 'diva2:1547559', 'diva2:618227', 'diva2:1465506', 'diva2:1795177', 'diva2:1782728', 'diva2:488441', 'diva2:1111205', 'diva2:1780195', 'diva2:1858209', 'diva2:1509441', 'diva2:439838', 'diva2:1114153', 'diva2:492776', '<p>1. Collision due to crossing or turning</p>', '<p>2. Vehicle and pedestrian collision</p>', '<p>3. Rear-end collision</p>', '<p>4. Single-vehicle accident</p>', '<p>5. Other collisions</p>', '<p>6. Head-on collision</p>', '<p>Statistics also show that of all fatally injured crash victims in urban trafficapproximately; one third is travelling by car; one third by motorcycle, moped or pedal-cycle;and one third are pedestrians. This means that unprotected road travelers correspond to twothirds of all fatal urban traffic accidents, a fact that has to be taken into account in future crashtesting of urban vehicles. With all the information gathered a total of four new crash testscenarios for light-weight urban vehicles have been presented:</p>', '<p>• Vehicle-to-vehicle side impact at 40km/h with a 1 300kg striking vehicle to evaluate theoccupant protection level of the light-weight vehicle.</p>', '<p>• Vehicle-to-motorcycle side impact at 40km/h with motorcycle rider protection evaluation.</p>', '<p>• Pedestrian protection assessment at 40km/h over the whole vehicle front and roof area.</p>', '<p>• Rigid barrier impact at 40km/h corresponding to an urban single vehicle accident with aroad side object or a collision with a heavier or similar sized vehicle.</p>', 'diva2:1154527', 'diva2:1880367', 'diva2:652385', 'diva2:618571', 'diva2:1860537', 'diva2:1087251', 'diva2:892115', 'diva2:1818051', 'diva2:1120393', 'diva2:1120459', 'diva2:1528067', 'diva2:1831429', 'diva2:1321152', 'diva2:1876753', 'diva2:1571214', 'diva2:1698135', 'diva2:1356933', 'diva2:1528136', 'diva2:1078069', 'diva2:1679305', 'diva2:784003', 'diva2:1083057', 'diva2:1528140', 'diva2:1707770', 'diva2:1640036', 'diva2:872195', 'diva2:1083220', 'diva2:1807973', 'diva2:1216708', 'diva2:1432665', 'diva2:1669916', 'diva2:1350191', 'diva2:1741184', 'diva2:401149', '<p> </p>', '<p>During the 2000s, the ship owners have become more and more concerned thattheir ships save fuel. Several projects have been undertaken to exploit the resourcesavailable on board today’s vessels to reduce fuel consumption. As a stepin this the Swedish Meteorological and Hydrological Institute (SMHI) today offera Weather Routing service to ships. By planning your route more effectivelymuch fuel can be saved.This thesis has been about developing a fuel prediction program (FPP) forhow much fuel a ship consumes in different sea conditions. The model takes intoaccount the ship’s loading condition, winds, wind waves and swell. Any othereffects are pooled in one term. This makes it possible to also consider how muchfuel the ship consumes on the various route options in the planning process.The model will also be a useful tool to retrospectively evaluate how a ship hasperformed in relation to the contract.On the ships in this report the prediction program was able to calculate thefuel consumption with an error of only 1% of the reported fuel consumption.This requires that the data about the vessel is accurate and up to date. If not,the model can still, with thoughtful assumptions, reach an error of less than10% of the reported consumption, which is better than the strategy that SMHIuses today.</p>', 'diva2:624028', 'diva2:1816751', 'diva2:1541213', 'diva2:1799888', 'diva2:1072495', 'diva2:813063', 'diva2:405988', 'diva2:1900961', 'diva2:1678903', 'diva2:1831431', 'diva2:802066', 'diva2:1740181', 'diva2:1081137', 'diva2:1465546', 'diva2:1739683', 'diva2:1673915', 'diva2:1572329', 'diva2:1229796', 'diva2:1880451', 'diva2:1142942', 'diva2:1120520', 'diva2:1142922', 'diva2:1120498', 'diva2:1120314', 'diva2:1900946', 'diva2:1527799', 'diva2:1900895', 'diva2:1813128', 'diva2:1334283', 'diva2:1643670', 'diva2:708360', 'diva2:1078063', 'diva2:1229785', 'diva2:13112', 'diva2:893822', 'diva2:1642607', 'diva2:1678462', 'diva2:1189528', 'diva2:662361', 'diva2:1142914', 'diva2:1120585', 'diva2:1089907', 'diva2:1871673', 'diva2:618335', 'diva2:1745694', 'diva2:1083783', 'diva2:1141679', 'diva2:558921', 'diva2:643823', 'diva2:1206952', 'diva2:1896429', 'diva2:1348300', 'diva2:1827845', 'diva2:605419', 'diva2:1229776', 'diva2:1718318', 'diva2:1774381', 'diva2:919666', 'diva2:1880788', 'diva2:744907', 'diva2:920001', 'diva2:492075', 'diva2:1285502', 'diva2:1899604', 'diva2:1528126', 'diva2:1595652', 'diva2:1356348', 'diva2:1287065', 'diva2:1247161', 'diva2:1244326', 'diva2:1514717', 'diva2:411684', 'diva2:515492', 'diva2:515494', 'diva2:408828', 'diva2:408832', 'diva2:529136', 'diva2:1189541', 'diva2:919807', 'diva2:1595559', 'diva2:1546808', 'diva2:893770', 'diva2:1499290', 'diva2:1830907', 'diva2:1120513', 'diva2:515504', 'diva2:1739353', 'diva2:1823810', 'diva2:1465539', 'diva2:503940', 'diva2:1357359', 'diva2:1078083', 'diva2:1879630', 'diva2:1120472', 'diva2:1228966']}

corrected abstract:
<p>When designing a building, sound is one of the problems to take into account. Vibrating machines, such as ventilation fans, water pumps and compressors, generate structure-borne sound. The structure-borne sound travels up the structure of the building and generates sound in adjacent rooms. To be able to predict the sound radiated in the adjacent rooms when designing a building, a semi-analytical model has been developed. Using the incident vibrations from the floor plate where the vibrating machine is standing, the transmission loss in the junction between the floor plates and the wall plate is calculated. This can bed one in every junction in the building, creating a system of multiple junctions. The sound radiation to the adjacent rooms is later approximated using the velocity of the plates. The model is verified with measurements in two case studies. This shows that the model has good potential in predicting the normal acceleration amplitudes in the relevant plates. The two case studies have different geometric properties and different sources. The comparison between the model and the measurement gives similar results. The model analyses the output of the bending waves since this is the wave type that radiates sound, but longitudinal waves are present in the model. With only two case studies it is too early to say that the model works for all systems, but it could be used as a fist approach. The model, right now, is restricted to isotropic, homogeneous material without losses. A parametric study shows that the transmission loss is dependent on the ratio between the thicknesses of the floor plate and the wall plate. The ratio should be as large as possible to get a high transmission loss, but depends on how the junction is structured.</p>
In diva2:1359762 
abstract is: 
<p>When designing a building, sound is one of the problems to take into account. Vibrating machines, such as ventilation fans, water pumps and compressors, generate structure-borne sound. The structure-borne sound travels up the structure of the building and generates sound in adjacent rooms. To be able to predict the sound radiated in the adjacent rooms when designing a building, a semi-analytical model has been developed. Using the incident vibrations from the floor plate where the vibrating machine is standing, the transmission loss in the junction between the floor plates and the wall plate is calculated. This can bed one in every junction in the building, creating a system of multiple junctions. The sound radiation to the adjacent rooms is later approximated using the velocity of the plates.The model is verified with measurements in two case studies. This shows that the model has good potential in predicting the normal acceleration amplitudes in the relevant plates. The two case studies have different geometric properties and different sources. The comparison between the model and the measurement gives similar results. The model analyses the output of the bending waves since this is the wave type that radiates sound, but longitudinal waves are present in the model. With only two case studies it is too early to say that the model works for all systems, but it could be used as a fist approach. The model, right now, is restricted to isotropic, homogeneous material without losses. A parametric study shows that the transmission loss is dependent on the ratio between the thicknesses of the floor plate and the wall plate. The ratio should be as large as possible to get a high transmission loss, but depends on how the junction is structured.</p>

corrected abstract:
<p>When designing a building, sound is one of the problems to take into account. Vibrating machines, such as ventilation fans, water pumps and compressors, generate structure-borne sound. The structure-borne sound travels up the structure of the building and generates sound in adjacent rooms. To be able to predict the sound radiated in the adjacent rooms when designing a building, a semi-analytical model has been developed. Using the incident vibrations from the floor plate where the vibrating machine is standing, the transmission loss in the junction between the floor plates and the wall plate is calculated. This can be done in every junction in the building, creating a system of multiple junctions. The sound radiation to the adjacent rooms is later approximated using the velocity of the plates.</p><p>The model is verified with measurements in two case studies. This shows that the model has good potential in predicting the normal acceleration amplitudes in the relevant plates. The two case studies have different geometric properties and different sources. The comparison between the model and the measurement gives similar results. The model analyses the output of the bending waves since this is the wave type that radiates sound, but longitudinal waves are present in the model. With only two case studies it is too early to say that the model works for all systems, but it could be used as a fist approach. The model, right now, is restricted to isotropic, homogeneous material without losses.</p><p>A parametric study shows that the transmission loss is dependent on the ratio between the thicknesses of the floor plate and the wall plate. The ratio should be as large as possible to get a high transmission loss, but depends on how the junction is structured.</p>
----------------------------------------------------------------------
In diva2:1640118 
abstract is: 
<p>A new vortex model for wind turbines was developed in order to evaluate the loads at the blades and other important characteristics of interest for the wind industry such as power and thrust coefficients. Nowadays, the calculation of these quantaties is done in a reliable and precise manner with LES simulation using actuator line or actuator disk models. However, LES simulations are computationally heavy and the model here developed aims at calculating the same quantities of interest in less time but still giving reliable and accurate results for any wind turbine model.</p><p>The idea of a vortex model for wind turbines was developed by Segalini &amp; Aöfredsson, J. Fluid Mech., vol. 725, pages 91-116, 2013, using vortex filaments to reproduce the vorticity on the blades and in the wake. Nevertheless, that model had some limitations, among which, the main one, was the impossibility to simluate wind turbines with varying circulation along the blade, something that is always present in reality.</p><p>With this thesis it is proposed a model based on the one of Segalini &amp; Alfredsson (2013) but with the introduction of a vortex sheet that allows to simulate a vorticity release from the wind turbine blades and hence wind turbines with varying circulation along the blades.</p><p>The model was validated against a LES simulation of the Tjaereborg wind turbine by Sarmast, KTH Royal Institute of Technology, 2014, that utilized an actuator line model. The results confirmed the improvement of the vortex model compared to the previous one of Segalini &amp; Alfredsson (2013) and gave consistent results regarding the flow field at the rotor plane and the loads on the blades.</p>

corrected abstract:
<p>A new vortex model for wind turbines was developed in order to evaluate the loads at the blades and other important characteristics of interest for the wind industry such as the power and thrust coefficients. Nowadays, the calculation of these quantities is done in a reliable and precise manner with LES simulation using actuator line or actuator disk models. However, LES simulations are computationally heavy and the model here developed aims at calculating the same quantities of interest in less time but still giving reliable and accurate results for any wind turbine model.</p><p>The idea of a vortex model for wind turbines was developed by Segalini &amp; Alfredsson, J. Fluid Mech., vol. 725, pages 91-116, 2013, using vortex filaments to reproduce the vorticity on the blades and in the wake. Nevertheless, that model had some limitations, among which, the main one, was the impossibility to simulate wind turbines with varying circulation along the blade, something that is always present in reality. With this thesis it is proposed a model based on the one of Segalini &amp; Alfredsson (2013) but with the introduction of a vortex sheet that allows to simulate a vorticity release from the wind turbine blades and hence wind turbines with varying circulation along the blades.</p><p>The model was validated against a LES simulation of the Tjaereborg wind turbine by Sarmast, KTH Royal Institute of Technology, 2014, that utilized an actuator line model. The results confirmed the improvement of the vortex model compared to the previous one of Segalini &amp; Alfredsson (2013) and gave consistent results regarding the flow field at the rotor plane and the loads on the blades.</p>
----------------------------------------------------------------------
In diva2:560204 
abstract is: 
<p>This Bachelor thesis has as its aim to investigate Josephson junctions by examining</p><p>their I-V characteristics and to visualize various phenomena associated with Josephson</p><p>junctions. Primarily, everything considered in this thesis uses a semi-classical model</p><p>where quantum eects have been excluded. The thesis focuses on 1D arrays of Josephson</p><p>junctions fed with direct current with the goal to study what occurs when many junctions</p><p>are placed in a row. First a single Josephson junction is simulated and we observe a</p><p>hysteresis in the I-V characteristics that is examined. Secondly a 1-D array is considered</p><p>and the coupled system of dierential equations that follows is solved numerically, and</p><p>from this data the properties of Josephson junctions are deduced. We discovered that this</p><p>array generally has similar properties to a single Josephson junction. We also sought to</p><p>replicate results obtained experimentally in the lab by David Haviland and his co-workers</p><p>at KTH, but by simulations, to see how good our models are. We have produced data</p><p>agreeing partly with the experimental results and we have had no directly contradicting</p><p>outcomes from our simulations. An explicit relation between the number of Josephson</p><p>junctions and the feature of the I-V characteristics was derived and the validity of our</p><p>model was strengthened by the consistency of our results.</p>

corrected abstract:
<p>This Bachelor thesis has as its aim to investigate Josephson junctions by examining their I-V characteristics and to visualize various phenomena associated with Josephson junctions. Primarily, everything considered in this thesis uses a semi-classical model where quantum effects have been excluded. The thesis focuses on 1D arrays of Josephson junctions fed with direct current with the goal to study what occurs when many junctions are placed in a row. First a single Josephson junction is simulated and we observe a hysteresis in the I-V characteristics that is examined. Secondly a 1-D array is considered and the coupled system of differential equations that follows is solved numerically, and from this data the properties of Josephson junctions are deduced. We discovered that this array generally has similar properties to a single Josephson junction. We also sought to replicate results obtained experimentally in the lab by David Haviland and his co-workers at KTH, but by simulations, to see how good our models are. We have produced data agreeing partly with the experimental results and we have had no directly contradicting outcomes from our simulations. An explicit relation between the number of Josephson junctions and the feature of the I-V characteristics was derived and the validity of our model was strengthened by the consistency of our results.</p>
----------------------------------------------------------------------
In diva2:1282827   - correct as is
----------------------------------------------------------------------
In diva2:650284 
abstract is: 
<p>In this report we will present the basic concepts and results of the theory of dynamical billiards</p><p>which idealizes the concept of a volumeless ball re　ecting against the inside of a billiard table</p><p>without friction.This motion will continue indenitely and it is of interest to study its behaviour.</p><p>We will show that the study of a billiard system can be reduced to the study of an associated</p><p>map called the billiard map dened on a cylindrical phase space. Using this formalism the</p><p>specic systems where the billiard table is given by a circle, right iscoceles triangle and ellipse</p><p>will be studied in some detail along with the existence of peridic points through Birkho's</p><p>famous theorem and some more novel results such as an instance of Benford's law regarding</p><p>the distribution of rst digits in real-life data. We will also dene the concept of a caustic</p><p>and investigate their existence and non-existence which will lead us to the concept of circle</p><p>homeomorphisms and will also provide the opportunity to illustrate the systems with some</p><p>simulations and yield some more informal and practical insight into the behaviour of these</p><p>systems.</p>

corrected abstract:
<p>In this report we will present the basic concepts and results of the theory of dynamical billiards which idealizes the concept of a volumeless ball reflecting against the inside of a billiard table without friction. This motion will continue indefinitely and it is of interest to study its behaviour.</p><p>We will show that the study of a billiard system can be reduced to the study of an associated map called the billiard map defined on a cylindrical phase space. Using this formalism the specific systems where the billiard table is given by a circle, right iscoceles triangle and ellipse will be studied in some detail along with the existence of peridic points through Birkhoff's famous theorem and some more novel results such as an instance of Benford's law regarding the distribution of first digits in real-life data. We will also define the concept of a caustic and investigate their existence and non-existence which will lead us to the concept of circle homeomorphisms and will also provide the opportunity to illustrate the systems with some simulations and yield some more informal and practical insight into the behaviour of these systems.</p>
----------------------------------------------------------------------
In diva2:572275 
abstract is: 
<p>A consensus problem is best described as a situation where multiple agents need to come</p><p>to an agreement based on information gathered by all agents. However, each agent only</p><p>has information from a few other agents. Applications for this could, for instance, be any</p><p>kind of situation where you have autonomous agents that need to cooperate. This could be</p><p>agreeing upon a rendezvous time or maintaining a formation.</p><p>In this thesis a specic kind of consensus problem is considered and expanded upon,</p><p>namely a target pursuit problem where a number of agents want to approach a target and</p><p>assume a formation around it. In our case the agents will only get information from their</p><p>next neighbour with respect to the angle around the target, which makes it a cyclic pursuit.</p><p>We will show that if a directed communication graph, representing the network, contains</p><p>a rooted directed spanning tree the agents will reach consensus. We also construct a</p><p>simulation of an introductory problem, examine what makes it work and verify convergence</p><p>and collision avoidance. The problem will be modied by making the agents converge to an</p><p>arbitrary plane instead of the xy-plane as presented in the original setup. Basic concepts of</p><p>matrix and graph theory and zero-order hold sampling will be explained.</p>

corrected abstract:
<p>A consensus problem is best described as a situation where multiple agents need to come to an agreement based on information gathered by all agents. However, each agent only has information from a few other agents. Applications for this could, for instance, be any kind of situation where you have autonomous agents that need to cooperate. This could be agreeing upon a rendezvous time or maintaining a formation.</p><p>In this thesis a specific kind of consensus problem is considered and expanded upon, namely a target pursuit problem where a number of agents want to approach a target and assume a formation around it. In our case the agents will only get information from their next neighbour with respect to the angle around the target, which makes it a cyclic pursuit.</p><p>We will show that if a directed communication graph, representing the network, contains a rooted directed spanning tree the agents will reach consensus. We also construct a simulation of an introductory problem, examine what makes it work and verify convergence and collision avoidance. The problem will be modified by making the agents converge to an arbitrary plane instead of the xy-plane as presented in the original setup. Basic concepts of matrix and graph theory and zero-order hold sampling will be explained.</p>
----------------------------------------------------------------------
In diva2:1194281 
abstract is: 
<p>Bayesian optimization is a well known class of derivative-free optimization algorithms mainly used for expensive black-box objective functions. Despite their eﬃciency, they suﬀer from a lack of rigorous convergence criterion which makes them more prone to be used as modeling tools rather than optimizing tools. This master thesis proposes, analyzes, and tests a globally convergent framework (that is to say the convergence to a stationary point regardless the initial sample) for Bayesian optimization algorithms. The framework design intends to preserve the global search characteristics for minimum while being rigorously monitored to converge.</p>

corrected abstract:
<p>Bayesian optimization is a well known class of derivative-free optimization algorithms mainly used for expensive black-box objective functions. Despite their efficiency, they suffer from a lack of rigorous convergence criterion which makes them more prone to be used as modeling tools rather than optimizing tools. This master thesis proposes, analyzes, and tests a globally convergent framework (that is to say the convergence to a stationary point regardless the initial sample) for Bayesian optimization algorithms. The framework design intends to preserve the global search characteristics for minimum while being rigorously monitored to converge.</p>
----------------------------------------------------------------------
In diva2:1334411 
abstract is: 
<p>A methodology for increasing the success rate in debt collection by matching individual call center agents with optimal debtors is developed. This methodology, called the trade algorithm, consists of the following steps. The trade algorithm first identifies groups of debtors for which agent performance varies. Based on these diﬀerences in performance, agents are put into clusters. An optimal call allocation for the clusters is then decided. Two methods to estimate the performance of an optimal call allocation are suggested. These methods are combined with Monte Carlo cross-validation and an alternative time-consistent validation procedure. Tests of significance are applied to the results and the eﬀect size is estimated.</p><p>The trade algorithm is applied to a dataset from the credit management services company Intrum and is shown to enhance performance.</p>

corrected abstract:
<p>A methodology for increasing the success rate in debt collection by matching individual call center agents with optimal debtors is developed. This methodology, called the trade algorithm, consists of the following steps. The trade algorithm first identifies groups of debtors for which agent performance varies. Based on these differences in performance, agents are put into clusters. An optimal call allocation for the clusters is then decided. Two methods to estimate the performance of an optimal call allocation are suggested. These methods are combined with Monte Carlo cross-validation and an alternative time-consistent validation procedure. Tests of significance are applied to the results and the effect size is estimated.</p><p>The trade algorithm is applied to a dataset from the credit management services company Intrum and is shown to enhance performance.</p>
----------------------------------------------------------------------
In diva2:1731898 
abstract is: 
<p>We study the interconnections between the spin Benjamin-Ono (sBO) and half-wave maps (HWM) equations, a pair of nonlinear partial integro-differential equations that have recently been found to permit multi-soliton solutions, where the time evolution of the constituent solitons can be described in terms of the well-known, completely integrable, spin Calogero-Moser (sCM) system.</p><p>By considering a symmetry transformation of the sCM dynamics we are led to introduce a scale parameter into the sBO equation, yielding what we call the rescaled sBO (rsBO) equation, which has both the sBO and HWM equations as special cases. Together with the addition of a new constant background term in the multi-soliton ansatz for the sBO equation, this allows us to formulate a theorem for the rsBO equation that unifies and generalizes previously known soliton theorems for the sBO and HWM equations. The theorem offers a new perspective on these equations; we use it to show the emergence of HWM dynamics in a certain background-dominated limit of the sBO equation, and to suggest a generalization of the HWM equation.</p><p>Along the way we discuss basic properties of the new multi-soliton solutions, and how to construct them. We spend some time proving that indeed all previously known multi-soliton solutions of the HWM equation are given by the new theorem, and not just a subset. We discuss, and state a conjecture about, possible physical interpretations of the sBO equation. Finally, we apply the same ideas to the spin non-chiral intermediate long-wave (sncILW) and non-chiral intermediate Heisenberg ferromagnet (ncIHF) equations, find that they are related in the same way as the sBO and HWM equations, and formulate a unified theorem for their multi-soliton solutions.</p><p>For ease of exposition we keep the discussion to hermitian solutions of the sBO and sncILW equations and $\bb R^3$-valued solutions of the HWM and ncIHF equations, though readers familiar with the subject will have no problem generalizing to the non-hermitian and $\bb C^3$-valued cases.</p>

corrected abstract:
<p>We study the interconnections between the spin Benjamin-Ono (sBO) and half-wave maps (HWM) equations, a pair of nonlinear partial integro-differential equations that have recently been found to permit multi-soliton solutions, where the time evolution of the constituent solitons can be described in terms of the well-known, completely integrable, spin Calogero-Moser (sCM) system.</p><p>By considering a symmetry transformation of the sCM dynamics we are led to introduce a scale parameter into the sBO equation, yielding what we call the rescaled sBO (rsBO) equation, which has both the sBO and HWM equations as special cases. Together with the addition of a new constant background term in the multi-soliton ansatz for the sBO equation, this allows us to formulate a theorem for the rsBO equation that unifies and generalizes previously known soliton theorems for the sBO and HWM equations. The theorem offers a new perspective on these equations; we use it to show the emergence of HWM dynamics in a certain background-dominated limit of the sBO equation, and to suggest a generalization of the HWM equation.</p><p>Along the way we discuss basic properties of the new multi-soliton solutions, and how to construct them. We spend some time proving that indeed all previously known multi-soliton solutions of the HWM equation are given by the new theorem, and not just a subset. We discuss, and state a conjecture about, possible physical interpretations of the sBO equation. Finally, we apply the same ideas to the spin non-chiral intermediate long-wave (sncILW) and non-chiral intermediate Heisenberg ferromagnet (ncIHF) equations, find that they are related in the same way as the sBO and HWM equations, and formulate a unified theorem for their multi-soliton solutions.</p><p>For ease of exposition we keep the discussion to hermitian solutions of the sBO and sncILW equations and ℝ<sup>3</sup>-valued solutions of the HWM and ncIHF equations, though readers familiar with the subject will have no problem generalizing to the non-hermitian and ℂ<sup>3</sup>-valued cases.</p>
----------------------------------------------------------------------
In diva2:556492 
abstract is: 
<p>A GPU version of the pressure projection solver using OpenCL is implemented. Then it has been compared with CPU version which is accelerated with OpenMP. The GPU version shows a sensible reduction in time despite using a simple algorithm in the kernel. The nal code is plugged into a commercial uid simulator software. Dierent kinds of algorithms and data transfer methods have been investigated. Overlapping the computation and communication showed a more than 3 times speed-up versus the serial communication-computation pattern. Finally we exploit methods for partitioning data and writing kernels to use many of the bene ts of computation on a heterogeneous system. We ran all the simulations on a machine with an Intel core i7-2600 cpu and 16 GB main memory coupled with a GeForce GTX 560 Ti graphic processing unit on a windows OS.</p>

corrected abstract:
<p>A GPU version of the pressure projection solver using OpenCL is implemented. Then it has been compared with CPU version which is accelerated with OpenMP. The GPU version shows a sensible reduction in time despite using a simple algorithm in the kernel. The final code is plugged into a commercial fluid simulator software.</p><p>Different kinds of algorithms and data transfer methods have been investigated. Overlapping the computation and communication showed a more than 3 times speed-up versus the serial communication-computation pattern. Finally we exploit methods for partitioning data and writing kernels to use many of the benefits of computation on a heterogeneous system.</p><p>We ran all the simulations on a machine with an Intel core i7-2600 cpu and 16 GB main memory coupled with a GeForce GTX 560 Ti graphic processing unit on a windows OS.</p>
----------------------------------------------------------------------
In diva2:676723 
abstract is: 
<p>Acoustic comfort in train compartments is discussed in relation to speech intelligibility. Passive acoustic design solutions were developed, resulting in two screen prototypes that use absorption and reflection to lower the level of speech intelligibility. A method of evaluating these prototype's eect on speech intelligibility was developed based on Speech Intelligibility Index (SII) measurements. Measurements were carried out in a train compartment mockup that resembled a 3 m section of a train coach. A computer ray-tracing model of the measurement setup was created in the ODEON ray-tracing software and verifed against the mockup.</p><p>Simulations were carried out and compared to the measurement results and the applicability of the ray-tracing model discussed. The measurements and subsequent evaluation according to the method developed here were able to predict the prototype's efect on speech intelligibility and are found to be applicable to other future setups.</p>

corrected abstract:
<p>Acoustic comfort in train compartments is discussed in relation to speech intelligibility. Passive acoustic design solutions were developed, resulting in two screen prototypes that use absorption and reflection to lower the level of speech intelligibility. A method of evaluating these prototype's effect on speech intelligibility was developed based on Speech Intelligibility Index (SII) measurements. Measurements were carried out in a train compartment mockup that resembled a 3 m section of a train coach. A computer ray-tracing model of the measurement setup was created in the ODEON ray-tracing software and verified against the mockup. Simulations were carried out and compared to the measurement results and the applicability of the ray-tracing model discussed. The measurements and subsequent evaluation according to the method developed here were able to predict the prototype's effect on speech intelligibility and are found to be applicable to other future setups.</p>
----------------------------------------------------------------------
In diva2:1779337 
abstract is: 
<p>The WHO has set a sound level threshold of 55 dB, beyond which noise is considered harmful to humans when exposed for an extended period of time. It is, however, estimated by the European Environmental Agency that over 110 million people in the EU are regularly exposed to levels of traffic noise above 55 dB. This work will investigate one of the constituent parts of the devices that deal with reducing noise emissions from road vehicles, specifically exhaust noise. Exhaust after-treatment systems deals, not only with filtering exhaust particulates but also with sound filtration. Their acoustic transmission properties are evermore of interest as noise pollution standards become more stringent. Catalytic converters are found in these after-treatment devices to filter NOx, CO, and HC and is used to treat exhaust gas from essentially all internal combustion engines. As exhaust particle emission standards also become more stringent, manufacturers may find themselves using more catalyst substrates in their after-treatment devices, meaning that their acoustic performance is put even further into the forefront.</p><p>This project aims to model the acoustic two-ports of catalytic converters and perform model validation through transmission loss measurements. Since the catalytic converter channels are of a size where the boundary layer occupy a large proportion of the channel area, viscous effects will start to significantly affect the wave propagation through these channels. To account for this, the Kirchhoff solution for capillaries is used as the basis of the model and its validity for non-circular pipes also become of interest. Since the catalytic converters being investigated have square channels as well as channels formed from corrugation, a model of their equivalent hydraulic radius is put forward. The proposed sinusoidal approximation of corrugation is especially noteworthy since this catalyst geometry has seldomely been treated in previous works.</p><p>Measurement data for model validation has been acquired with a transmission loss rig using an over-determined plane wave decomposition from three microphone measurements. Source-switching has been used to retrieve linearly independent measurements in order to solve the two-port scattering matrix. It is found that the Kirchhoff model and the proposed hydraulic radius model yield transmission loss predictions that are accurate to measurements. For catalysts with square channels, this result validates previous works that have attempted to model transmission loss through catalytic converters. Additionally, it can be concluded that the newly proposed sinusoidal model for corrugation may be used to accurately model catalysts whose channels are formed with waves between concentric liners. Finally, this result should, in the future, also be validated with the addition of a mean flow through the catalyst in order to, more closely, replicate its operating conditions.</p>

corrected abstract:
<p>The WHO has set a sound level threshold of 55 dB, beyond which noise is considered harmful to humans when exposed for an extended period of time. It is, however, estimated by the European Environmental Agency that over 110 million people in the EU are regularly exposed to levels of traffic noise above 55 dB. This work will investigate one of the constituent parts of the devices that deal with reducing noise emissions from road vehicles, specifically exhaust noise. Exhaust after-treatment systems deals, not only with filtering exhaust particulates but also with sound filtration. Their acoustic transmission properties are evermore of interest as noise pollution standards become more stringent. Catalytic converters are found in these after-treatment devices to filter NO<sub>x</sub>, CO, and HC and is used to treat exhaust gas from essentially all internal combustion engines. As exhaust particle emission standards also become more stringent, manufacturers may find themselves using more catalyst substrates in their after-treatment devices, meaning that their acoustic performance is put even further into the forefront.</p><p>This project aims to model the acoustic two-ports of catalytic converters and perform model validation through transmission loss measurements. Since the catalytic converter channels are of a size where the boundary layer occupy a large proportion of the channel area, viscous effects will start to significantly affect the wave propagation through these channels. To account for this, the Kirchhoff solution for capillaries is used as the basis of the model and its validity for non-circular pipes also become of interest. Since the catalytic converters being investigated have square channels as well as channels formed from corrugation, a model of their equivalent hydraulic radius is put forward. The proposed sinusoidal approximation of corrugation is especially noteworthy since this catalyst geometry has seldomely been treated in previous works.</p><p>Measurement data for model validation has been acquired with a transmission loss rig using an over-determined plane wave decomposition from three microphone measurements. Source-switching has been used to retrieve linearly independent measurements in order to solve the two-port scattering matrix. It is found that the Kirchhoff model and the proposed hydraulic radius model yield transmission loss predictions that are accurate to measurements. For catalysts with square channels, this result validates previous works that have attempted to model transmission loss through catalytic converters. Additionally, it can be concluded that the newly proposed sinusoidal model for corrugation may be used to accurately model catalysts whose channels are formed with waves between concentric liners. Finally, this result should, in the future, also be validated with the addition of a mean flow through the catalyst in order to, more closely, replicate its operating conditions.</p>
----------------------------------------------------------------------
In diva2:1670746 
abstract is: 
<p>Tyre wear is a vital problem in vehicles, especially in articulated vehicles because of their heavier axle loads. Tyre wear can not only do harm to vehicle dynamics but also cause tyre particle emissions. Therefore, solutions that can minimise tyre wear are where this thesis work focuses.</p><p>The suspension design is one of the main factors affecting tyre wear. This thesis considers different kinds of semi-active (such as ADD, SH-2 and GH-2) and active suspensions (\(H_\infty\) control) and compares them with a passive suspension with regards to tyre wear. Driving comfort and road holding are considered as well.</p><p>The simulation starts by running a IPG/TruckMaker model on a route introducing a certain road roughness profile, and outputs lateral slip angles, which together with vehicle parameters and route data are input into a Simulink model. Then, based on the static vertical load on each axles of the articulated vehicle, three different quarter car models corresponding to the axles with various suspension systems are built in Simulink. Simulation results show that the active suspension (\(H_\infty\) control approach) works best in minimising tyre wear, reducing tyre wear by 1-10\% compared to the passive suspension when the vehicle is driving on road profile Class C (country road). Meanwhile, several control strategies of \(H_\infty\) control are applied in order to result in overall good vehicle performance considering comfort and road holding. Semi-active suspensions also work well in reducing tyre wear, but the amount of tyre wear reduction is lower than that of the \(H_\infty\) controllers.</p>

corrected abstract:
<p>Tyre wear is a vital problem in vehicles, especially in articulated vehicles because of their heavier axle loads. Tyre wear can not only do harm to vehicle dynamics but also cause tyre particle emissions. Therefore, solutions that can minimise tyre wear are where this thesis work focuses.</p><p>The suspension design is one of the main factors affecting tyre wear. This thesis considers different kinds of semi-active (such as ADD, SH-2 and GH-2) and active suspensions (H<sub>&infin;</sub> control) and compares them with a passive suspension with regards to tyre wear. Driving comfort and road holding are considered as well.</p><p>The simulation starts by running a IPG/TruckMaker model on a route introducing a certain road roughness profile, and outputs lateral slip angles, which together with vehicle parameters and route data are input into a Simulink model. Then, based on the static vertical load on each axles of the articulated vehicle, three different quarter car models corresponding to the axles with various suspension systems are built in Simulink. Simulation results show that the active suspension (H<sub>&infin;</sub> control approach) works best in minimising tyre wear, reducing tyre wear by 1-10% compared to the passive suspension when the vehicle is driving on road profile Class C (country road). Meanwhile, several control strategies of (H<sub>&infin;</sub>\ control are applied in order to result in overall good vehicle performance considering comfort and road holding. Semi-active suspensions also work well in reducing tyre wear, but the amount of tyre wear reduction is lower than that of the H<sub>&infin;</sub>\ controllers.</p>
----------------------------------------------------------------------
In diva2:1728303 
abstract is: 
<p>In the context of building a framework for active flow control of turbulent boundary layers in wings, a set of large-eddy simulation (LES) are implemented in OpenFOAM. The flow around a NACA4412 wing profile is simulated at 5° angle of attack and Re_c = 400˙000. Validation of the uncontrolled flow results is performed with respect to the dataset generated by Vinuesa et al. (2018) at the same aerodynamic configuration. Afterwards, two different flow control strategies are analyzed over the suction side (SS) of the wing to yield skin friction drag reduction and an overall improvement of the aerodynamic efficiency. The region subject to the actuation spans 0.25 x_ss/c to 0.:86 x_ss/c, where c is the chord length of the wing. In the current setup, uniform blowing (BLW) and suction (SCT) control schemes show close agreement with the trends presented by Atzori (2021). Indeed, BLW decreases the viscous drag, but increases its pressure contribution and penalizes the lift, thus lowering the global efficiency of the wing, while SCT has an opposite effect. Thus, these methods behave similarly to pressure gradients (PGs) conditions, as BLW enhances the APG, whereas SCT damps it. The streamwise travelling waves strategy is then assessed for three set-ups characterized by different phase speeds. A consistent skin friction drag reduction and efficiency improvement are observed for two cases, while milder benefits are recorded even when drag increase was expected. Trends which have already been reported in the literature by Quadrio et al. (2009) and Skote (2014) are identified, i.e. the effects of this actuation to be mainly enclosed in the viscous sub-layer and the gross amount of drag reduction to be dependent on the wave relative speed; however, it is believed that the PGs conditions over the SS of the wing significantly alters the outcomes of the chosen parameters. Eventually, Reynolds averaged Navier-Stokes (RANS) simulations are performed to assess their accuracy with respect to the generated LES set-up, in the effort to enable a multi-fidelity approach for future works.</p><p> </p>

corrected abstract:
<p>In the context of building a framework for active flow control of turbulent boundary layers in wings, a set of large-eddy simulation (LES) are implemented in OpenFOAM. The flow around a NACA4412 wing profile is simulated at 5° angle of attack and <em>Re<sub>c</sub></em> = 400 000. Validation of the uncontrolled flow results is performed with respect to the dataset generated by Vinuesa <em>et al.</em> (2018) at the same aerodynamic configuration. Afterwards, two different flow control strategies are analyzed over the suction side (SS) of the wing to yield skin friction drag reduction and an overall improvement of the aerodynamic efficiency. The region subject to the actuation spans 0.25 x<sub>ss</sub>/c to 0.86 x<sub>ss</sub>/c, where c is the chord length of the wing. In the current setup, uniform blowing (BLW) and suction (SCT) control schemes show close agreement with the trends presented by Atzori (2021). Indeed, BLW decreases the viscous drag, but increases its pressure contribution and penalizes the lift, thus lowering the global efficiency of the wing, while SCT has an opposite effect. Thus, these methods behave similarly to pressure gradients (PGs) conditions, as BLW enhances the APG, whereas SCT damps it. The streamwise travelling waves strategy is then assessed for three set-ups characterized by different phase speeds. A consistent skin friction drag reduction and efficiency improvement are observed for two cases, while milder benefits are recorded even when drag increase was expected. Trends which have already been reported in the literature by Quadrio <em>et al.</em> (2009) and Skote (2014) are identified, i.e. the effects of this actuation to be mainly enclosed in the viscous sub-layer and the gross amount of drag reduction to be dependent on the wave relative speed; however, it is believed that the PGs conditions over the SS of the wing significantly alters the outcomes of the chosen parameters. Eventually, Reynolds averaged Navier-Stokes (RANS) simulations are performed to assess their accuracy with respect to the generated LES set-up, in the effort to enable a multi-fidelity approach for future works.</p>
----------------------------------------------------------------------
In diva2:1440097 
abstract is: 
<p>Actuator saturation is a well studied subject regarding control theory. However, little research exist regarding aircraft behavior during actuator saturation. This paper aims to identify flight mechanical parameters that can be useful when analyzing actuator saturation. The studied aircraft is an unstable delta-canard aircraft. By varying the aircraft’s center-of-gravity and applying a square wave input in pitch, saturated actuators have been found and investigated closer using moment coeÿcients as well as other flight mechanical parameters. The studied flight mechanical parameters has proven to be highly relevant when analyzing actuator saturation, and a simple connection between saturated actuators and moment coeÿcients has been found. One can for example look for sudden changes in the moment coeÿcients during saturated actuators in order to find potentially dangerous flight cases. In addition, the studied parameters can be used for robustness analysis, but needs to be further investigated. Lastly, the studied pitch square wave input shows no risk of aircraft departure with saturated elevons during flight, provided non-saturated canards, and that the free-stream velocity is high enough to be flyable.</p>

corrected abstract:
<p>Actuator saturation is a well studied subject regarding control theory. However, little research exist regarding aircraft behavior during actuator saturation. This paper aims to identify flight mechanical parameters that can be useful when analyzing actuator saturation. The studied aircraft is an unstable delta-canard aircraft. By varying the aircraft’s center-of-gravity and applying a square wave input in pitch, saturated actuators have been found and investigated closer using moment coefficients as well as other flight mechanical parameters. The studied flight mechanical parameters has proven to be highly relevant when analyzing actuator saturation, and a simple connection between saturated actuators and moment coefficients has been found. One can for example look for sudden changes in the moment coefficients during saturated actuators in order to find potentially dangerous flight cases. In addition, the studied parameters can be used for robustness analysis, but needs to be further investigated. Lastly, the studied pitch square wave input shows no risk of aircraft departure with saturated elevons during flight, provided non-saturated canards, and that the free-stream velocity is high enough to be flyable.</p>
----------------------------------------------------------------------
In diva2:1287144 
abstract is: 
<p>the FE analyses. Traditionally all parts are modeled with isotropic base material. Analyses are made on a part of the nozzle which includes both a butt weld and metal deposition and which is an interface to another part causing loads that has to be sustained by the weld and the MD. As a small part of this thesis was also a fatigue study made to a spot weld test specimen.</p><p>In order to strengthen the nozzle to prevent structural damage, an outer layer is added to the already existing metal cone by material deposition, MD, or additive manufacturing. During the manufacturing process the material will indicate some degree of anisotropic properties.</p><p>The key purpose of this thesis was to analyze how this anisotropic behaviour might affect the structural stiffener connected to this anisotropic material when exposed to a load at the end of the stiffener. Further analysis due to fatigue was also done to parts of the structure. The procedure was done by building a model and setting up the different anisotropic properties with help of a finite element program, Ansys. The material properties regarding the anisotropy of the material was changed and compared in order to see how it affected stresses and strains in the anisotropic material and it‘s surrounding materials. Further analysis was made to the properties of the weld such as the yield limit.</p><p>The result would indicate that for loadings that did not generate plastic deformations, hence elastic deformations, there were no significant difference forthe different trial values of the yield ratios. However, the differences became parent when studying large plastic deformations. Variation of the Young’s modulus would show some differences in the monitored properties for both elastic and plastic deformations. Studies of degrading the welds yield limit would show no diffrences when elastic deformations were present, but would have a big impact when large plastic deformations were present. The J-values variations for the spotweld would indicate huge differences depending on the yield limits for the spotweld and base material.</p>

corrected abstract:
<p>This thesis investigates how structural margins are affected by including ansiotropic material as well as weld material in the FE analyses. Traditionally all parts are modeled with isotropic base material. Analyses are made on a part of the nozzle which includes both a butt weld and metal deposition and which is an interface to another part causing loads that has to be sustained by the weld and the MD. As a small part of this thesis was also a fatigue study made to a spot weld test specimen.</p><p>In order to strengthen the nozzle to prevent structural damage, an outer layer is added to the already existing metal cone by material deposition, MD, or additive manufacturing. During the manufacturing process the material will indicate some degree of anisotropic properties.</p><p>The key purpose of this thesis was to analyze how this anisotropic behaviour might affect the structural stiffener connected to this anisotropic material when exposed to a load at the end of the stiffener. Further analysis due to fatigue was also done to parts of the structure.</p><p>The procedure was done by building a model and setting up the different anisotropic properties with help of a finite element program, Ansys. The material properties regarding the anisotropy of the material was changed and compared in order to see how it affected stresses and strains in the anisotropic material and it's surrounding materials. Further analysis was made to the properties of the weld such as the yield limit.</p><p>The result would indicate that for loadings that did not generate plastic deformations, hence elastic deformations, there were no significant difference for the different trial values of the yield ratios. However, the differences became parent when studying large plastic deformations. Variation of the Young’s modulus would show some differences in the monitored properties for both elastic and plastic deformations. Studies of degrading the welds yield limit would show no diffrences when elastic deformations were present, but would have a big impact when large plastic deformations were present. The J-values variations for the spotweld would indicate huge differences depending on the yield limits for the spotweld and base material.</p>
----------------------------------------------------------------------
In diva2:1380198 
abstract is: 
<p>The introduction of autonomous vehicles comes with many benefits related to safety and quality-of-life, the implementation of which is a challenging task for engineers to solve; one part of this aforementioned task is the requirement for precise positioning.Firstly, this thesis work investigated the contemporary fields of science rel-evant for the task of position estimation. The paper then dissertated the theory, implementation and the experimental evaluation of three di˙erent methods of estimating position. The individual properties was then examined for the dif-ferent vehicle filters and the optimal method of estimating position was then determined with regards to its specified use case, the autonomous truck appli-cation.The methods chosen for evaluation were the Unscented Kalman Filter (UKF), the Controller Output Observer (COO) and the Washout filter method. These methods were evaluated using physical experiments carried out on roads and the result of which showed that the Controller Output Observer (COO) and the Washout filter methods shared significant disadvantages compared to the supe-rior Unscented Kalman Filter (UKF). Based on the experimental results a new filter constellation were proposed whereby two modified position evaluating filters are connected in series.</p>

corrected abstract:
<p>The introduction of autonomous vehicles comes with many benefits related to safety and quality-of-life, the implementation of which is a challenging task for engineers to solve; one part of this aforementioned task is the requirement for precise positioning.</p><p>Firstly, this thesis work investigated the contemporary fields of science relevant for the task of position estimation. The paper then dissertated the theory, implementation and the experimental evaluation of three different methods of estimating position. The individual properties was then examined for the different vehicle filters and the optimal method of estimating position was then determined with regards to its specified use case, the autonomous truck application.</p><p>The methods chosen for evaluation were the Unscented Kalman Filter (UKF), the Controller Output Observer (COO) and the Washout filter method. These methods were evaluated using physical experiments carried out on roads and the result of which showed that the Controller Output Observer (COO) and the Washout filter methods shared significant disadvantages compared to the superior Unscented Kalman Filter (UKF). Based on the experimental results a new filter constellation were proposed whereby two modified position evaluating filters are connected in series.</p>
----------------------------------------------------------------------
In diva2:1057237 
abstract is: 
<p>Computational analysis of experimental aircraft prior to test ights can be a valuable tool to estimate ight characteristics and determine areas of elevated caution. It can also provide feedback to software and model developers as to the accuracy of models used when the aircraft is ultimately own. This paper describes the aerodynamic analysis and characterisation of an experimental tilt-wing aircraft with a unique design. The paper covers what analysis is performed as well as results of these aircraft characterisations. Through this analysis a database le is created for use with NASA Design and Analysis of Rotorcraft (NDARC) tool.</p>

corrected abstract:
<p>Computational analysis of experimental aircraft prior to test flights can be a valuable tool to estimate flight characteristics and determine areas of elevated caution. It can also provide feedback to software and model developers as to the accuracy of models used when the aircraft is ultimately flown. This paper describes the aerodynamic analysis and characterisation of an experimental tilt-wing aircraft with a unique design. The paper covers what analysis is performed as well as results of these aircraft characterisations. Through this analysis a database file is created for use with NASA Design and Analysis of Rotorcraft (NDARC) tool.</p>
----------------------------------------------------------------------
In diva2:737287 
abstract is: 
<p>This thesis describes the process of developing the aerodynamics package of</p><p>a Formula Student race car with computational 　uid dynamics. It investigates</p><p>the eects of aerodynamics on the vehicle's behaviour and performance with</p><p>regard to the Formula Student competition format. The methods used during</p><p>the development are evaluated and put into context by investigating the</p><p>correlation between a wind-tunnel experiment of a wing in ground proximity,</p><p>and its simulated counterpart. The aerodynamics package consists of an undertray,</p><p>a front wing and a rear wing and the report details the stages involved</p><p>in optimising these components to achieve the desired results.</p>

corrected abstract:
<p>This thesis describes the process of developing the aerodynamics package of a Formula Student race car with computational fluid dynamics. It investigates the effects of aerodynamics on the vehicle's behaviour and performance with regard to the Formula Student competition format. The methods used during the development are evaluated and put into context by investigating the correlation between a wind-tunnel experiment of a wing in ground proximity, and its simulated counterpart. The aerodynamics package consists of an undertray, a front wing and a rear wing and the report details the stages involved in optimising these components to achieve the desired results.</p>
----------------------------------------------------------------------
In diva2:559083 
abstract is: 
<p>An aerodynamic propeller model, which can contribute to the prediction of structural loads experienced by aircraft in different flight maneuvers is presented.The model is based on Blade Element Momentum theory and is able to predict the unsymmetrical and frequency-dependent forces and moments induced by the propeller on the airplane structure at steady and unsteady inflow-conditions.In order to validate the model, a comparison with experimental results was performed and it can be seen that the model is in agreement with the experimental data providing that the aerodynamic data used for the calculations has good accuracy.</p>

corrected abstract:
<p>An aerodynamic propeller model, which can contribute to the prediction of structural loads experienced by aircraft in different flight maneuvers is presented.</p><p>The model is based on Blade Element Momentum theory and is able to predict the unsymmetrical and frequency-dependent forces and moments induced by the propeller on the airplane structure at steady and unsteady inflow-conditions.</p><p>In order to validate the model, a comparison with experimental results was performed and it can be seen that the model is in agreement with the experimental data providing that the aerodynamic data used for the calculations has good accuracy.</p>
----------------------------------------------------------------------
In diva2:1083457 
abstract is: 
<p>The thesis aims to provide an evaluation on the Volvo 1/5th scaled wind tunnel regarding its potentials and capabilities in aerodynamic study. The flow quality in the test section was evaluated. The experiments were performed included measurements of airspeed stability, tunnel-wall boundary layer profile and horizontal buoyancy. A numerical model was developed to predict the boundary layer thickness on the test floor. Repeatability tests were also conducted to establish the appropriate operating regime.A correlation study between the 1/5th scaled wind tunnel (MWT) and full scale wind tunnel (PVT) was performed using steady force and unsteady pressure measurements. The Volvo Aero 2020 concept car was selected to be the test model.The Reynolds effect and the tunnel-wall boundary layer interference were identified in the steady force measurements. Unsteady near-wake phenomena such as wake pumping and wake flapping were discussed in the unsteady base pressure measurements.</p>

corrected abstract:
<p>The thesis aims to provide an evaluation on the Volvo 1/5<sup>th</sup> scaled wind tunnel regarding its potentials and capabilities in aerodynamic study. The flow quality in the test section was evaluated. The experiments were performed included measurements of airspeed stability, tunnel-wall boundary layer profile and horizontal buoyancy. A numerical model was developed to predict the boundary layer thickness on the test floor. Repeatability tests were also conducted to establish the appropriate operating regime.</p><p>A correlation study between the 1/5<sup>th</sup> scaled wind tunnel (MWT) and full scale wind tunnel (PVT) was performed using steady force and unsteady pressure measurements. The Volvo Aero 2020 concept car was selected to be the test model.</p><p>The Reynolds effect and the tunnel-wall boundary layer interference were identified in the steady force measurements. Unsteady near-wake phenomena such as wake pumping and wake flapping were discussed in the unsteady base pressure measurements.</p>
----------------------------------------------------------------------
In diva2:419447 
abstract is: 
<p>The unsteady ow around an aerofoil placed in a uniform ow stream with an angle of attack is investigated, under the assumption of inviscid, incompressible, two-dimensional flow. In particular, a function of the velocity jump over the wake is achieved, where this function depends on the horizontal displacement and time. The aerofoil geometry is represented by two arbitrary functions, one for the upper and one for the lower side of the aerofoil. These functions are dependent on time, hence the aerofoil can perform oscillating movement, which is the case when subjected to utter. The governing equations for the ow are the Euler equations. By assuming thin aerofoil, small angle of attack and that the perturbation of the wake is small, the problem is linearised. It is shown that the linearised Euler equations can be rewritten as the Cauchy-Riemann equations, and an analytic function exists where its real part is the horizontal velocity component and its imaginary part is the vertical velocity component with opposite sign. The ow eld is then investigated in the complex plane by making an appropriate branch cut removing all discontinuities, and with restrictions on the analytic function such that the kinematic and boundary conditions are satis ed. By using Cauchy's integral formula an expression for the anti-symmetric part of the analytic function is achieved. A general expression for the velocity jump over the wake is obtained, which is applied to the speci c case of harmonic oscillations for a symmetric aerofoil. In the end three types of utter is investigated; twisting oscillations around the centre of stiness, vertical oscillation, and aileron flutter.</p>

corrected abstract:
<p>The unsteady flow around an aerofoil placed in a uniform flow stream with an angle of attack is investigated, under the assumption of inviscid, incompressible, two-dimensional flow. In particular, a function of the velocity jump over the wake is achieved, where this function depends on the horizontal displacement and time. The aerofoil geometry is represented by two arbitrary functions, one for the upper and one for the lower side of the aerofoil. These functions are dependent on time, hence the aerofoil can perform oscillating movement, which is the case when subjected to flutter.</p><p>The governing equations for the flow are the Euler equations. By assuming thin aerofoil, small angle of attack and that the perturbation of the wake is small, the problem is linearised. It is shown that the linearised Euler equations can be rewritten as the Cauchy-Riemann equations, and an analytic function exists where its real part is the horizontal velocity component and its imaginary part is the vertical velocity component with opposite sign.</p><p>The flow field is then investigated in the complex plane by making an appropriate branch cut removing all discontinuities, and with restrictions on the analytic function such that the kinematic and boundary conditions are satisfied. By using Cauchy's integral formula an expression for the anti-symmetric part of the analytic function is achieved. A general expression for the velocity jump over the wake is obtained, which is applied to the specific case of harmonic oscillations for a symmetric aerofoil. In the end three types of flutter is investigated; twisting oscillations around the centre of stiffness, vertical oscillation, and aileron flutter.</p>
----------------------------------------------------------------------
In diva2:1670995 
abstract is: 
<p>This report presents the first steps of development aiming towards making, the open-source aeroelastic code, GEBTAero flight dynamics capable. The implementation was done partly in the Fortran code and part in the GEBTAero Python API with the objective of reusing as much of the existing code as possible with as little substantial architecture modification. The added capacities include the widening of the purview of the software to take into account beam assembly arranged in a plane-like structure, a trim function for the steady level flight was also implemented and the twelve degree of freedom flight mechanics system of equations was introduced in the algorithm. In this short time, unfortunately, few tests were performed fully but important foundation work giving preliminary results was carried out. This includes the verification of the structural modes simulation as well as several bug and inacuracy fixes.</p>

corrected abstract:
<p>This report presents the first steps of development aiming towards making, the open source aeroelastic code, GEBTAero flight dynamics capable. The implementation was done partly in the Fortran code and partly in the GEBTAero Python API with the objective of reusing as much of the existing code as possible with as little substantial architecture modification. The added capacities include the widening of the purview of the software to take into account beam assembly arranged in a plane-like structure, a trim function for steady level flight was also implemented and the twelve degree of freedom flight mechanics system of equations was introduced in the algorithm. In this short time, unfortunately, few tests were performed fully but important foundation work giving preliminary results was carried out. This includes the verification of the structural modes simulation as well as several bug and inacuracy fixes.</p>
----------------------------------------------------------------------
In diva2:1795583 
abstract is: 
<p>The task of training AIs for imperfect-information games has long been difficult. However, recently the algorithm ReBeL, a general framework for self-play reinforcement learning, has been shown to excel at heads-up no-limit Texas hold 'em, among other imperfect-information games. In this report the ability to adapt ReBeL to a downscaled version of the strategy wargame \say{Game of the Generals} is explored. It is shown that an implementation of ReBeL that uses no domain-specific knowledge is able to beat all benchmark bots, which indicates that ReBeL can be a useful framework when training AIs for imperfect-information wargames.</p>

corrected abstract:
<p>The task of training AIs for imperfect-information games has long been difficult. However, recently the algorithm ReBeL, a general framework for self-play reinforcement learning, has been shown to excel at heads-up no-limit Texas hold ’em, among other imperfect-information games. In this report the ability to adapt ReBeL to a downscaled version of the strategy wargame &OpenCurlyDoubleQuote;Game of the Generals&CloseCurlyDoubleQuote; is explored. It is shown that an implementation of ReBeL that uses no domain-specific knowledge is able to beat all benchmark bots, which indicates that ReBeL can be a useful framework when training AIs for imperfect-information wargames.</p>
----------------------------------------------------------------------
In diva2:459344 
abstract is: 
<p>The Tahitian company ‘Electricité de Tahiti’operates two out of eight power groups named G7P and G8P equipped with SCR units to limit the pollution of these groups.The real impact of these units is to be assessed. By using an atmospheric dispersion model called AERMOD,it is shown that the results are very similar with and without the SCR units, as the heights of the chimneys and the wind profile of the valley induce a good dispersion of the fumes for these two groups.</p>


corrected abstract:
<p>The Tahitian company ‘Electricité de Tahiti’ operates two out of eight power groups named G7P and G8P equipped with SCR units to limit the pollution of these groups. The real impact of these units is to be assessed. By using an atmospheric dispersion model called AERMOD, it is shown that the results are very similar with and without the SCR units, as the heights of the chimneys and the wind profile of the valley induce a good dispersion of the fumes for these two groups.</p>
----------------------------------------------------------------------
In diva2:1247197 
abstract is: 
<p>The aim of this internship is to improve the test process, especially the data exchange and treatment, as well as the production traceability, of an aeronautical Diesel engine. New criteria are devised to enhance the verifications of the engine’s performance.The documentation used throughout the process is globally reworked. New documents are also created, to simplify and codify the process. The updates to forms, instructions and specifications are included. Search for data in the Production Records is simplified by sorting the documents composing them, both in paperand digital formats. This paper also presents the work accomplished with the test subcontractor.</p>

corrected abstract:
<p>The aim of this internship is to improve the test process, especially the data exchange and treatment, as well as the production traceability, of an aeronautical Diesel engine. New criteria are devised to enhance the verifications of the engine’s performance. The documentation used throughout the process is globally reworked. New documents are also created, to simplify and codify the process. The updates to forms, instructions and specifications are included. Search for data in the Production Records is simplified by sorting the documents composing them, both in paper and digital formats. This paper also presents the work accomplished with the test subcontractor.</p>
----------------------------------------------------------------------
In diva2:1871588 
abstract is: 
<p>Sustainable aviation fuels are one of the proposals that the aviation industry is adopting to reduce its impact on the environment. These are obtained by applying chemical processes to biological and non-biological resources, and their main objective is to replace conventional (fossil-derived) aviation fuel in their entirety. There are currently seven approved production pathways and two co-processing processes. Understanding the real impact of these fuels on the amount of emissions produced by an aircraft along a trajectory is essential to further develop sustainable proposals and regulations.</p><p>So to understand how the use of these fuels affects total emissions, the model developed by Boeing to estimate aircraft emissions has been adapted to sustainable aviation fuels using the Lower Heating Value as a proposal throughout this project. The methodology has been applied to a real flight between Stockholm Airport and Bordeaux Airport. Four different scenarios, characterized by varying fuel blends, have been studied: exclusive use of kerosene, 10% sustainable fuel blend, an equal 50% blend of each fuel, and sustainable fuel only. The analysis has been performed for five different types of non-conventional fuels; Shell FT-SPK, Sasol FT-SPK, UOP HEFA-SPK, Coconut HEFA-SPK and Hevo ATJ-SPK.</p><p>At large, two trends have been detected in the effect of these fuels on total emissions; some types of SAF have increased CO and HC emissions and reduced NOx emissions compared to kerosene, and others whose behavior has been the reverse. In addition, the percentage of fuel used has an impact on total emissions. It can be concluded that no non-conventional fuel type among those studied has been found to produce a reduction in all types of emissions, however, given that their life cycle is circular, they do contribute to making the aviation sector more sustainable by reducing CO2. The results can help to better understand the impact of this type of fuel, as well as provide valuable information for decision-making in the implementation of sustainable strategies in the aviation industry.</p>


corrected abstract:
<p>Sustainable aviation fuels are one of the proposals that the aviation industry is adopting to reduce its impact on the environment. These are obtained by applying chemical processes to biological and non-biological resources, and their main objective is to replace conventional (fossil-derived) aviation fuel in their entirety. There are currently seven approved production pathways and two co-processing processes. Understanding the real impact of these fuels on the amount of emissions produced by an aircraft along a trajectory is essential to further develop sustainable proposals and regulations.</p><p>So to understand how the use of these fuels affects total emissions, the model developed by Boeing to estimate aircraft emissions has been adapted to sustainable aviation fuels using the Lower Heating Value as a proposal throughout this project. The methodology has been applied to a real flight between Stockholm Airport and Bordeaux Airport. Four different scenarios, characterized by varying fuel blends, have been studied: exclusive use of kerosene, 10% sustainable fuel blend, an equal 50% blend of each fuel, and sustainable fuel only. The analysis has been performed for five different types of non-conventional fuels; Shell FT-SPK, Sasol FT-SPK, UOP HEFA-SPK, Coconut HEFA-SPK and Hevo ATJ-SPK.</p><p>At large, two trends have been detected in the effect of these fuels on total emissions; some types of SAF have increased CO and HC emissions and reduced NOx emissions compared to kerosene, and others whose behavior has been the reverse. In addition, the percentage of fuel used has an impact on total emissions. It can be concluded that no non-conventional fuel type among those studied has been found to produce a reduction in all types of emissions, however, given that their life cycle is circular, they do contribute to making the aviation sector more sustainable by reducing CO<sub>2</sub>. The results can help to better understand the impact of this type of fuel, as well as provide valuable information for decision-making in the implementation of sustainable strategies in the aviation industry.</p>
----------------------------------------------------------------------
In diva2:839835 
abstract is: 
<p>During the end of 2010 and beginning of 2011 the med tech company Bioservo requested an investigation into the feasibility of a new design of foot-drop aid. Foot-drop is a relatively common disorder where the patient partially or completely lacks the ability to dorsiflex, flex the foot upwards towards the shin, one or both feet. Bioservo wished to find what requirements are placed upon such an aid, to find out whether it is possible to construct an aid in accordance with their design solutions and finally to get an indication of the market interest for the proposed aid.</p><p>This thesis will present the results of the pre-study performed, the prototype that was constructed and the resulting conclusions that were made.</p><p>The proposed aid was to have active control and use Bioservos patented Soft Extra Muscles system (SEM) to convey power from actuator to foot. The SEM-system consists of thin wires that are led along an existing body part and functioning in the same way as the bodys own tendons. The system was developed for use in Bioservos currently available aid, a grip force enhancing glove. The new food-drop aid was also to be usable with or without the need for a shoe, something no other active foot-drop aid had yet managed.</p><p>The pre-study showed several different solutions for both active and passive foot-drop aids, but nothing quite like what Bioservo had in mind. The market for foot-drop aids in Sweden offer few options for active aids, but interest was shown from both orthopedics and patients to broaden the market by introduction of new aids such as the one proposed.</p><p>The prototype that was constructed showed promising results for the mechanical solution, but also shone a light on the need of a powerful actuator and sufficient information of the aids immediate surroundings. Given those needs being fulfilled, Bioservos proposed aid should be well suited for further development as well as public and medical market introduction.</p>

corrected abstract:
<p>During the end of 2010 and beginning of 2011 the med tech company Bioservo requested an investigation into the feasibility of a new design of foot-drop aid. Foot-drop is a relatively common disorder where the patient partially or completely lacks the ability to dorsiflex, flex the foot upwards towards the shin, one or both feet. Bioservo wished to find what requirements are placed upon such an aid, to find out whether it is possible to construct an aid in accordance with their design solutions and finally to get an indication of the market interest for the proposed aid.</p><p>This thesis will present the results of the pre-study performed, the prototype that was constructed and the resulting conclusions that were made.</p><p>The proposed aid was to have active control and use Bioservos patented Soft Extra Muscles system (SEM) to convey power from actuator to foot. The SEM-system consists of thin wires that are led along an existing body part and functioning in the same way as the bodys own tendons. The system was developed for use in Bioservos currently available aid, a grip force enhancing glove. The new food-drop aid was also to be usable with or without the need for a shoe, something no other active foot-drop aid had yet managed.</p><p>The pre-study showed several different solutions for both active and passive foot-drop aids, but nothing quite like what Bioservo had in mind.</p><p>The market for foot-drop aids in Sweden offer few options for active aids, but interest was shown from both orthopedics and patients to broaden the market by introduction of new aids such as the one proposed.</p><p>The prototype that was constructed showed promising results for the mechanical solution, but also shone a light on the need of a powerful actuator and sufficient information of the aids immediate surroundings. Given those needs being fulfilled, Bioservos proposed aid should be well suited for further development as well as public and medical market introduction.</p>
----------------------------------------------------------------------
In diva2:1299489 -- missing subscript and text in title:
"An alternative future for shipping – the way there: Risks and benefits of energy efficiency measures and alternative fuels for CO2 reduction in"
==>
"An alternative future for shipping – the way there: Risks and benefits of energy efficiency measures and alternative fuels for CO<sub>2</sub> reduction in container ships"

abstract is: 
<p>Shipping is the world’s largest mode of transportation, considering mass moved a distance: it is the most e˙effective way to carry large volumes far. In order for the shipping industry to keep its position and develop even further, efforts are made to increase efficiency and reduce the environmental footprint from the industry. More efficient ships, reduced fuel consumption, use of alternative fuels and exhaust gas treatment are some of the choices to reduce shipping’s environmental footprint and achieve the sustainability goal established by EU and enforced by the International Maritime Organization.Throughout the thesis, en evaluation of 18 energy efficiency measures and 4 alternative fuels is performed. Energy efficiency measures reduce a ship’s fuel consumption and alternative fuels substitutes fossil fuels with higher content of environmentally harmful content. The measures and fuels, covered in the study, are evaluated for nine representative container ship´s. Data from year 2016 are used for the nine container ships. The current procedure followed for new investments is analyzed for all measures and fuels for each ship, focused on the financial study of each measure and fuel. The results are then included in a risk and benefit analysis that introduces external aspects, not included in the traditional financial evaluation, that include: those that influence the ship and the ship’s environment and those affected by the ship’s operations.The main goal is to evaluate the possibilities to reduce emissions by considering these aspects and involve more stakeholders in the investment of measures and fuels for shipping to keep its position as the most efficient mode of transportation.</p>

corrected abstract:
<p>Shipping is the world’s largest mode of transportation, considering mass moved a distance: it is the most effective way to carry large volumes far. In order for the shipping industry to keep its position and develop even further, efforts are made to increase efficiency and reduce the environmental footprint from the industry. More efficient ships, reduced fuel consumption, use of alternative fuels and exhaust gas treatment are some of the choices to reduce shipping’s environmental footprint and achieve the sustainability goal established by EU and enforced by the International Maritime Organization.</p><p>Throughout the thesis, en evaluation of 18 energy efficiency measures and 4 alternative fuels is performed. Energy efficiency measures reduce a ship’s fuel consumption and alternative fuels substitutes fossil fuels with higher content of environmentally harmful content. The measures and fuels, covered in the study, are evaluated for nine representative container ships’. Data from year 2016 are used for the nine container ships. The current procedure followed for new investments is analyzed for all measures and fuels for each ship, focused on the financial study of each measure and fuel. The results are then included in a risk and benefit analysis that introduces external aspects, not included in the traditional financial evaluation, that include: those that influence the ship and the ship’s environment and those affected by the ship’s operations.</p><p>The main goal is to evaluate the possibilities to reduce emissions by considering these aspects and involve more stakeholders in the investment of measures and fuels for shipping to keep its position as the most efficient mode of transportation.</p>
----------------------------------------------------------------------
In diva2:624027   - correct as is
----------------------------------------------------------------------
In diva2:1040726 
abstract is: 
<p>The jet printing of solder paste from a uid dynamics perspective involves viscosity change due to varying shear rate and eventual break o of the ejected solder paste droplet from the uid in the printer head. The ability to model the jetting process in a simulation package is important as it can be used as a tool for future development of the jetting device. The jetting process is modelled as a two phase (air - solder paste) ow with interface tracking performed using phase eld method and temporal stepping based on a second-order Backward Di erence Formula with relaxed tolerences. This thesis investigates the droplet morphology, volume and speed predictions for three di erent piston actuation modes and solder paste viscosity denitions given by the Carreau- Yasuda model. A Darcy condition with the porosity parameter is calibrated equal to unity such that the droplet speed is within the realistic range of 20 m/s - 30 m/s. The simulations are compared against previous simulation results from IBOFlow, performed within a collaboration between Mycronic AB and Fraunhofer-Chalmers Centre. As the Carreau models cannot capture the dependence of the uid viscosity of ow history, an indirect structure based viscosity model is used to compare the thixotopic behaviour. The expressions for the parameters of the structure based viscosity model are derived based on an analytical model which assumes that shear rate is constant. Experimental data for constant shear rate is curve tted on a Carreau model and an initial estimate of the parameters are obtained. The parameters are then adjusted to match experimental thixotopic behaviour. This method can be used to obtain parameter values for structure based viscosity models for uids with no previous data. Once the solder paste is ejected through the nozzle and the piston retracts, the uid undergoes stretching. Studying lament stretching during jetting is dicult as it can be driven by both droplet and piston motion. The data from an extensional rheometer is analyzed to study the lament stretching phenomenon for solder pastes. An analytical model for the critical aspect ratio is derived for a Newtonian uid lament undergoing a pure extension and modelled as a cylinder whose radius is decreases with time. The exponential decrease of the lament radius predicted by the analytical model is found to reproduce the experimental observations very well. The lament radius calculated based on the lament height from the experiments and analytical model shows that the model captures the stretching process, but the formation of beads usually seen in suspensions is not accounted for.</p>

corrected abstract:
<p>The jet printing of solder paste from a fluid dynamics perspective involves viscosity change due to varying shear rate and eventual break off of the ejected solder paste droplet from the fluid in the printer head. The ability to model the jetting process in a simulation package is important as it can be used as a tool for future development of the jetting device. The jetting process is modelled as a two phase (air - solder paste) flow with interface tracking performed using phase field method and temporal stepping based on a second-order Backward Difference Formula with relaxed tolerences. This thesis investigates the droplet morphology, volume and speed predictions for three different piston actuation modes and solder paste viscosity definitions given by the Carreau-Yasuda model. A Darcy condition with the porosity parameter Φ is calibrated equal to unity such that the droplet speed is within the realistic range of 20 m/s - 30 m/s. The simulations are compared against previous simulation results from IBOFlow, performed within a collaboration between Mycronic AB and Fraunhofer-Chalmers Centre.</p><p>As the Carreau models cannot capture the dependence of the fluid viscosity of flow history, an indirect structure based viscosity model is used to compare the thixotopic behaviour. The expressions for the parameters of the structure based viscosity model are derived based on an analytical model which assumes that shear rate is constant. Experimental data for constant shear rate is curve fitted on a Carreau model and an initial estimate of the parameters are obtained. The parameters are then adjusted to match experimental thixotopic behaviour. This method can be used to obtain parameter values for structure based viscosity models for fluids with no previous data.</p><p>Once the solder paste is ejected through the nozzle and the piston retracts, the fluid undergoes stretching. Studying filament stretching during jetting is difficult as it can be driven by both droplet and piston motion. The data from an extensional rheometer is analyzed to study the filament stretching phenomenon for solder pastes. An analytical model for the critical aspect ratio is derived for a Newtonian fluid filament undergoing a pure extension and modelled as a cylinder whose radius is decreases with time. The exponential decrease of the filament radius predicted by the analytical model is found to reproduce the experimental observations very well. The filament radius calculated based on the filament height from the experiments and analytical model shows that the model captures the stretching process, but the formation of beads usually seen in suspensions is not accounted for.</p>
----------------------------------------------------------------------
In diva2:1761955 
abstract is: 
<p>Empirical evidence indicates that the volatility in asset prices is not constant, but varies over time. However, many simple models for asset pricing rest on an assumption of constancy. In this thesis we analyse the zero-coupon bond price under a two-factor Vasicek model, where both the short rate and its volatility follow Ornstein-Uhlenbeck processes. Yield curves based on the two-factor model are then compared to those obtained from the standard Vasicek model with constant volatility. The simulated yield curves from the two-factor model exhibit "humps" that can be observed in the market, but which cannot be obtained from the standard model.</p>

corrected abstract:
<p>Empirical evidence indicates that the volatility in asset prices is not constant, but varies over time. However, many simple models for asset pricing rest on an assumption of constancy. In this thesis we analyse zero-coupon bond prices under a two-factor Vasicek model, where both the short rate and its volatility follow Ornstein-Uhlenbeck processes. Yield curves based on the two-factor model are then compared to those obtained from the standard Vasicek model with constant volatility. The simulated yield curves from the two-factor model exhibit "humps" that can be observed in the market, but which cannot be obtained from the standard model.</p>
----------------------------------------------------------------------
In diva2:1285782 
abstract is: 
<p>Previous studies have shown that many drivers are unable to provide the right amount of steering torque when facing an imminent collision with an upcoming obstacle. In some cases, drivers under-react i.e., they provide too low steering inputs and thus collide with the obstacle in front; in other cases, drivers might apply a higher steering input than necessary, potentially resulting in the vehicle leaving the road or losing stability. The EMA function is an active safety feature which has the sole objective of providing steering torque interference when performing such a manoeuvre. The motivation for the thesis work is to overcome some limitations of the existing MA function which does not incorporate the ability to differentiate driver reactions. In this thesis, an Evasive Manoeuvre Assist (EMA) function is designed to adapt to both types of the drivers, by an optimised steering torque overlay. The existing current EMA function is always amplifying the driver steering inputs using a feed-forward controller. The focus of this thesis work is to identify and dene a proper steering sequence reference model for closed-loop feedback control design. A simple single-point preview model is designed first to calculate the reference steering angle. A few test scenarios are set-up using the IPG CarMaker<sup>TM</sup>simulation tool. The reference model is then tuned with respect to the amplitude and frequency by batch simulations to obtain the optimal steering prole. A feedback controller is then designed using this reference model. The controller is implemented in a real-time environment, using a Volvo rapid-prototype test vehicle. Preliminary variation tests have shown that the developed controller can enhance both an over-reacting and under-reacting driver's performance during an evasive manoeuvre, by applying assistance/resistance EPAS torque timely. The designed EMA function is shown to accommodate different driver reactions and provide intuitive torque interference. As opposed to the earlier notion that the EMA function only assists the driver with an additional steering wheel torque, it was shown that the optimal steering torque overlay might be in the form of assistance or resistance.</p>


corrected abstract:
<p>Previous studies have shown that many drivers are unable to provide the right amount of steering torque when facing an imminent collision with an upcoming obstacle. In some cases, drivers under-react i.e, they provide too low steering inputs and thus collide with the obstacle in front; in other cases, drivers might apply a higher steering input than necessary, potentially resulting in the vehicle leaving the road or losing stability. The EMA function is an active safety feature which has the sole objective of providing steering torque interference when performing such a manoeuvre. The motivation for the thesis work is to overcome some limitations of the existing EMA function which does not incorporate the ability to differentiate driver reactions.</p><p>In this thesis, an Evasive Manoeuvre Assist (EMA) function is designed to adapt to both types of the drivers, by an optimised steering torque overlay. The existing current EMA function is always amplifying the driver steering inputs using a feed-forward controller. The focus of this thesis work is to identify and define a proper steering sequence reference model for closed-loop feedback control design. A simple single-point preview model is designed first to calculate the reference steering angle. A few test scenarios are set-up using the IPG CarMaker™ simulation tool. The reference model is then tuned with respect to the amplitude and frequency by batch simulations to obtain the optimal steering profile. A feedback controller is then designed using this reference model. The controller is implemented in a real-time environment, using a Volvo rapid-prototype test vehicle.</p><p>Preliminary verification tests have shown that the developed controller can enhance both an over-reacting and under-reacting driver's performance during an evasive manoeuvre, by applying assistance/resistance EPAS torque timely. The designed EMA function is shown to accommodate different driver reactions and provide intuitive torque interference. As opposed to the earlier notion that the EMA function only assists the driver with an additional steering wheel torque, it was shown that the optimal steering torque overlay might be in the form of assistance or resistance.</p>
----------------------------------------------------------------------
In diva2:1547583 
abstract is: 
<p>A major pre-requisite for mass manufacturing composite parts for the automobile industry is shorter lead times. Compression molding is a popular manufacturing method used today for manufacturing of composite parts and in combination with fast curing prepreg material, can lead to shorter production time. This however requires knowledge about the form-ability and processabilty of the fast curing prepregs used. Interply slip is one of the most complicated deformation mechanisms that occur during forming and is dependent on several factors like temperature, pressure, fiber orientation, fiber/resin type among others. The aim of this thesis was to experimentally investigate interply slip at conditions close to that of compression molding in fast curing unidirectional and plain weave prepreg material.</p>

corrected abstract:
<p>A major pre-requisite for mass manufacturing composite parts for the automobile industry is shorter lead times. Compression molding is a popular manufacturing method used today for manufacturing of composite parts and in combination with fast curing prepreg material, can lead to shorter production time. This however requires knowledge about the formability and processability of the fast curing prepregs used. Interply slip is one of the most complicated deformation mechanisms that occur during forming and is dependent on several factors like temperature, pressure, fiber orientation, fiber/resin type among others. The aim of this thesis was to experimentally investigate interply slip at conditions close to that of compression molding in fast curing unidirectional and plain weave prepreg material.</p>

Note: I added the missing "i" to processability.
----------------------------------------------------------------------
In diva2:839881 
abstract is: 
<p>In this thesis, fundamental properties of the turbulent ow above di erent wind-farm models were determined by means of wind-tunnel measurements. The assessed wind farms consisted in two staggered congurations, and two inline congurations, where two di erent streamwise spacings were evaluated. The experiment was focused on dense wind farms: the spacing in the spanwise direction was xed to approximately 3d (where d indicates the rotor diameters) for every case, and two streamwise spacings were used: 2.5d and 5d. Freelyrotating turbines were used to perform this experiment. The wind-turbine models had a diameter of 45 mm and a height of 85 mm from the ground to the top tip. The wind-farm models were placed one at a time inside the test section of the KTH NT2011 wind tunnel, where the inow was completely at, i.e there was no simulated atmospheric boundary layer. X-wire anemometry was the measurement technique to measure the streamwise and wall-normal velocities above the wind farms. From the results, it could be observed that close to the turbines, the streamwise mean velocity had variations in the spanwise direction for inline and staggered congurations, even deep downstream on the wind farms. Horizontal averages were applied to the data to account for the inhomogeneity of the properties above the wind farms. A scaling behaviour was found on the ow above all the wind farms studied: in order to have the streamwise mean velocity prole, all that was needed was the boundary layer thickness, the free-stream velocity, and the streamwise velocity close to the top tip of the turbines. Other scaling behaviours were found for the Reynolds stresses. The dispersive stresses above di erent wind farms did not scale with the friction velocity, since it was seen that these stresses are highly dependent on the wind farm layout. Therefore, it was concluded that Reynolds stresses and dispersive stresses cannot be compared to each other because they come from di erent scales: the small scales and the large velocity scales, respectively. An equation to estimate the friction velocity above wind farms was derived, requiring measurements of the angular velocity of the turbines, the free-stream velocity, and the mean velocities close to the tip of the turbines. Finally, it was found that the angular velocity of the turbines was 25 % higher for the staggered arrangement, when comparing it with an inline wind farm.</p>

corrected abstract:
<p>In this thesis, fundamental properties of the turbulent flow above different wind-farm models were determined by means of wind-tunnel measurements. The assessed wind farms consisted in two staggered configurations, and two inline configurations, where two different streamwise spacings were evaluated. The experiment was focused on dense wind farms: the spacing in the spanwise direction was fixed to approximately 3d (where d indicates the rotor diameters) for every case, and two streamwise spacings were used: 2.5d and 5d. Freely-rotating turbines were used to perform this experiment. The wind-turbine models had a diameter of 45 mm and a height of 85 mm from the ground to the top tip. The wind-farm models were placed one at a time inside the test section of the KTH NT2011 wind tunnel, where the inflow was completely flat, i.e there was no simulated atmospheric boundary layer. X-wire anemometry was the measurement technique to measure the streamwise and wall-normal velocities above the wind farms. From the results, it could be observed that close to the turbines, the streamwise mean velocity had variations in the spanwise direction for inline and staggered configurations, even deep downstream on the wind farms. Horizontal averages were applied to the data to account for the inhomogeneity of the properties above the wind farms. A scaling behaviour was found on the flow above all the wind farms studied: in order to have the streamwise mean velocity profile, all that was needed was the boundary layer thickness, the free-stream velocity, and the streamwise velocity close to the top tip of the turbines. Other scaling behaviours were found for the Reynolds stresses. The dispersive stresses above different wind farms did not scale with the friction velocity, since it was seen that these stresses are highly dependent on the wind farm layout. Therefore, it was concluded that Reynolds stresses and dispersive stresses cannot be compared to each other because they come from different scales: the small scales and the large velocity scales, respectively. An equation to estimate the friction velocity above wind farms was derived, requiring measurements of the angular velocity of the turbines, the free-stream velocity, and the mean velocities close to the tip of the turbines. Finally, it was found that the angular velocity of the turbines was 25 % higher for the staggered arrangement, when comparing it with an inline wind farm.</p>
----------------------------------------------------------------------
In diva2:1169943 
abstract is: 
<p>Topological data analysis has been shown to provide novel insight in many natural sciences. To our knowledge, the area is however relatively unstudied on financial data. This thesis explores the use of topological data analysis on one dimensional financial time series. Takens embedding theorem is used to transform a one dimensional time series to an $m$-dimensional point cloud, where $m$ is the embedding dimension. The point cloud of the time series represents the states of the dynamical system of the one dimensional time series. To see how the topology of the states differs in different partitions of the time series, sliding window technique is used. The point cloud of the partitions is then reduced to three dimensions by PCA to allow for computationally feasible persistent homology calculation. Synthetic examples are shown to illustrate the process. Lastly, persistence landscapes are used to allow for statistical analysis of the topological features. The topological properties of financial data are compared with quantum noise data to see if the properties differ from noise. Complexity calculations are performed on both datasets to further investigate the differences between high-frequency FX data and noise. The results suggest that high-frequency FX data differs from the quantum noise data and that there might be some property other than mutual information of financial data which topological data analysis uncovers.</p>

corrected abstract:
<p>Topological data analysis has been shown to provide novel insight in many natural sciences. To our knowledge, the area is however relatively unstudied on financial data. This thesis explores the use of topological data analysis on one dimensional financial time series. Takens embedding theorem is used to transform a one dimensional time series to an 𝑚-dimensional point cloud, where 𝑚 is the embedding dimension. The point cloud of the time series represents the states of the dynamical system of the one dimensional time series. To see how the topology of the states differs in different partitions of the time series, sliding window technique is used. The point cloud of the partitions is then reduced to three dimensions by PCA to allow for computationally feasible persistent homology calculation. Synthetic examples are shown to illustrate the process. Lastly, persistence landscapes are used to allow for statistical analysis of the topological features. The topological properties of financial data are compared with quantum noise data to see if the properties differ from noise. Complexity calculations are performed on both datasets to further investigate the differences between high-frequency FX data and noise. The results suggest that high-frequency FX data differs from the quantum noise data and that there might be some property other than mutual information of financial data which topological data analysis uncovers.</p>
----------------------------------------------------------------------
In diva2:1613411 
abstract is: 
<p>As commercial lunar rovers are being developed and planned to fly from next year, in the context of a global momentum for lunar exploration, the mindset of system design is shifting to a product-oriented approach (as opposed to traditionally single mission-designed system). This deeply affects the system engineering discipline, which is also evolving through the development of more integrated, model-centric methodologies such as Model-Based System Engineering (MBSE).</p><p>This Master Thesis combines 2 research questions:- How to adapt systems engineering processes and tools to a commercially-driven / product-oriented approach?- How to leverage new developments (e.g MBSE) within the system engineering discipline to support the rover design transition to a product-oriented philosophy?</p><p>These research questions are investigated through this Master Thesis, carried out as a 6-month internship at ispace Europe (Luxembourg), a global lunar exploration company developing landers, rovers and data tools. The Master Thesis is applied to ispace’s Exploration Rover currently under development within the Polar Ice Explorer (PIE) mission with support from the Luxembourg Government.The goal of this Master Thesis is to develop an integrated toolchain (set of tools) for efficiently designing rover products (Exploration Rover), including platform configuration for a given mission concept and set of payloads, system sizing and mission analysis.</p><p>The chosen methodology can be summarized as:1. Adapting PIE models to a generic, parametric/configurable toolset that can be used for mission/platform analysis and optimization2. Defining the Exploration Rover toolchain requirements &amp; architecture, and selecting its environment (trade-off including MBSE solutions)3. Building the Exploration Rover toolchain, integrating models inside the defined architecture</p><p>By maturing existing models, leveraging new software functionalities (in this case Valispace) and MBSE practises along with adding new parametric models for quick feasibility studies and integrating all models together, it was successfully shown that this integrated toolchain can support rover products definition, performing frequent and insightful design iterations, analysis and trade-offs. Not only does the toolchain comply with the product-approach but also successfully supports the Polar Ice Explorer (PIE) mission, by directly contributing to the system engineering activities and models of the Phase B.</p><p>Therefore, the Master Thesis proved to be a successful demonstrator for developing more product-driven rovers, by leveraging new practices within the system engineering discipline.</p>

corrected abstract:
<p>As commercial lunar rovers are being developed and planned to fly from next year, in the context of a global momentum for lunar exploration, the mindset of system design is shifting to a product-oriented approach (as opposed to traditionally single mission-designed system). This deeply affects the system engineering discipline, which is also evolving through the development of more integrated, model-centric methodologies such as Model-Based System Engineering (MBSE).</p><p>This Master Thesis combines 2 research questions:<ul><li>How to adapt systems engineering processes and tools to a commercially-driven / product-oriented approach?</li><li>How to leverage new developments (e.g MBSE) within the system engineering discipline to support the rover design transition to a product-oriented philosophy?</li></ul></p><p>These research questions are investigated through this Master Thesis, carried out as a 6-month internship at ispace Europe (Luxembourg), a global lunar exploration company developing landers, rovers and data tools. The Master Thesis is applied to ispace’s Exploration Rover currently under development within the Polar Ice Explorer (PIE) mission with support from the Luxembourg Government. The goal of this Master Thesis is to develop an <strong>integrated</strong> toolchain (set of tools) for <strong>efficiently designing rover products</strong> (Exploration Rover), including platform configuration for a given mission concept and set of payloads, system sizing and mission analysis.</p><p>The chosen methodology can be summarized as:<ol><li>Adapting PIE models to a generic, parametric/configurable toolset that can be used for mission/platform analysis and optimization</li><li>Defining the Exploration Rover toolchain requirements &amp; architecture, and selecting its environment (trade-off including MBSE solutions)</li><li>Building the Exploration Rover toolchain, integrating models inside the defined architecture</li></ol></p><p>By maturing existing models, leveraging new software functionalities (in this case Valispace) and MBSE practises along with adding new parametric models for quick feasibility studies and integrating all models together, it was successfully shown that this integrated toolchain can support rover products definition, performing frequent and insightful design iterations, analysis and trade-offs. Not only does the toolchain comply with the product-approach but also successfully supports the Polar Ice Explorer (PIE) mission, by directly contributing to the system engineering activities and models of the Phase B.</p><p>Therefore, the Master Thesis proved to be a successful demonstrator for developing more product-driven rovers, by leveraging new practices within the system engineering discipline.</p>
----------------------------------------------------------------------
In diva2:752775 
abstract is: 
<p>This paper exhibits the intertwinement between the prime numbers and the zeros of the Riemann zeta function, drawing upon existing literature by Davenport, Ahlfors, et al.</p><p>We begin with the meromorphic continuation of the Riemann zeta function ζ and the gamma function Γ . We then derive a functional equation that relates these functions and formulate the Riemann hypothesis.</p><p>We move on to the topic of nite-ordered functions and their Hadamard products. We show that the xi function ξ is of finite order, whence we obtain many useful properties. We then use these properties to and a zero-free region for ζ in the critical strip. We also determine the vertical distribution of the non-trivial zeros.</p><p>We finally use Perron's formula to derive von Mangoldt's explicit formula, which is an approximation of the Chebyshevfunction ψ . Using this approximation, we prove the prime number theorem and conclude with an implication ofthe Riemann hypothesis.</p>

corrected abstract:
<p>This paper exhibits the intertwinement between the prime numbers and the zeros of the Riemann zeta function, drawing upon existing literature by Davenport, Ahlfors, et al.</p><p>We begin with the meromorphic continuation of the Riemann zeta function ζ and the gamma function Γ. We then derive a functional equation that relates these functions and formulate the Riemann hypothesis.</p><p>We move on to the topic of finite-ordered functions and their Hadamard products. We show that the xi function ξ is of finite order, whence we obtain many useful properties. We then use these properties to find a zero-free region for ζ in the critical strip. We also determine the vertical distribution of the non-trivial zeros.</p><p>We finally use Perron's formula to derive von Mangoldt's explicit formula, which is an approximation of the Chebyshev function <em>ψ</em>. Using this approximation, we prove the prime number theorem and conclude with an implication of the Riemann hypothesis.</p>
----------------------------------------------------------------------
In diva2:1183272 
abstract is: 
<p>With the advent of private commercial suborbital spaceﬂight, a new demo-graphic of untrained individuals will begin to travel to space. These individuals are exposed to high levels of G-forces, resulting in medical considerations which are not a normal factor with high performance ﬁghter pilots or astronauts.The acceleration proﬁles of the Virgin Galactic and Blue Origin spacecraft were obtained from publicly available data. Video analysis was performed on footage of spacecraft test launches and human centrifuge tests to obtain individual data sets. These data sets were used to develop the acceleration proﬁles for both spacecraft. Based on the spacecraft’s acceleration proﬁles and peak G-forces, medical conditions were investigated and considered to identify potential risks that may aﬀect the passengers, particularly the elderly.</p>


corrected abstract:
<p>With the advent of private commercial suborbital spaceflight, a new demographic of untrained individuals will begin to travel to space. These individuals are exposed to high levels of G-forces, resulting in medical considerations which are not a normal factor with high performance fighter pilots or astronauts.</p><p>The acceleration profiles of the Virgin Galactic and Blue Origin spacecraft were obtained from publicly available data. Video analysis was performed on footage of spacecraft test launches and human centrifuge tests to obtain individual data sets. These data sets were used to develop the acceleration profiles for both spacecraft.</p><p>Based on the spacecraft’s acceleration profiles and peak G-forces, medical conditions were investigated and considered to identify potential risks that may affect the passengers, particularly the elderly.</p>
----------------------------------------------------------------------
In diva2:562098 
abstract is: 
<p>During the last few years, there has been a lot of debate regarding the price of</p><p>electricity on the Swedish market. According to a recent survey from Sifo, a</p><p>non-bias governmental institute for consumer research and testing, the price</p><p>of electricity is what worries Swedish households the most. An investigation</p><p>of the eciency of the Nordic power grids inuence on the Swedish spot</p><p>price is therefore both relevant and valuable. Three quarters of all electricity</p><p>produced in the Nordic region is traded at the Nord Pool Spot power market.</p><p>This survey examines how much of the variation of the swedish spot price</p><p>can be descried by the variation of the nordic system price and how much is</p><p>caused because of ineciencies in the power grid. Primarily, linear regression</p><p>with adjustments for endogeneity and heteroskedasticity has been used in order</p><p>to analyze data obtained mainly from Nord Pool Spot and Vattenfall AB.</p><p>The results show that the variation of the system price can account for all but</p><p>about 40% of the variation in the Swedish spot price. Thus, it is reasonable to</p><p>believe that these 40% are caused by limitations and local imbalance between</p><p>production and consumption in the Nordic price areas. Our conclusion from</p><p>the results is that it is in place with improvements of the Nordic power grid.</p><p>The alternative would be to reallocate the power production to match the</p><p>consumption. However, the latter is neither economically nor environmentally</p><p>sustainable.</p>

corrected abstract:
<p>During the last few years, there has been a lot of debate regarding the price of electricity on the Swedish market. According to a recent survey from Sifo, a non-bias governmental institute for consumer research and testing, the price of electricity is what worries Swedish households the most. An investigation of the efficiency of the Nordic power grids influence on the Swedish spot price is therefore both relevant and valuable. Three quarters of all electricity produced in the Nordic region is traded at the Nord Pool Spot power market.</p><p>This survey examines how much of the variation of the swedish spot price can be descried by the variation of the nordic system price and how much is caused because of inefficiencies in the power grid. Primarily, linear regression with adjustments for endogeneity and heteroskedasticity has been used in order to analyze data obtained mainly from Nord Pool Spot and Vattenfall AB.</p><p>The results show that the variation of the system price can account for all but about 40% of the variation in the Swedish spot price. Thus, it is reasonable to believe that these 40% are caused by limitations and local imbalance between production and consumption in the Nordic price areas. Our conclusion from the results is that it is in place with improvements of the Nordic power grid. The alternative would be to reallocate the power production to match the consumption. However, the latter is neither economically nor environmentally sustainable.</p>
----------------------------------------------------------------------
In diva2:1211821 
abstract is: 
<p>This thesis in applied mathematics and industrial economics investigates different factors affecting the fuel consumption in passenger cars. This is done by performing a multiple linear regression using the software R. Further, an analysis of the car industry is done covering the internal and external factors affecting the car manufacturers, in order to present an optimal strategy regarding RnD for car manufacturers having relatively low revenues. This analysis is based on Porter's five forces framework and a PEST-analysis. The data used for the regression has been gathered by the U.S. Environmental Protection Agency (EPA) and consists of 1245 passenger cars which was then processed in order to apply the regression. The results of the regression analysis performed indicated that it is possible to explain approximately 80% of the fuel consumption in passenger cars. More precisely, the number of cylinders, type of transmission and cylinder deactivation-system had the biggest impact on the fuel consumption. Furthermore the economic analysis of the industry revealed highly influencing external factors such as regulations and changing consumer preferences together with competition from other actors within the industry being the biggest threat. The main conclusion from this thesis is that car manufacturers having relatively small revenues should implement cylinder deactivation systems and abandon the turbo-downsizing trend within the industry. Furthermore manual gearboxes should be abandoned in order to reduce the fuel consumption and manufacturing costs while increase the comfort for the consumer.</p>

corrected abstract:
<p>This thesis in applied mathematics and industrial economics investigates different factors affecting the fuel consumption in passenger cars. This is done by performing a multiple linear regression using the software R. Further, an analysis of the car industry is done covering the internal and external factors affecting the car manufacturers, in order to present an optimal strategy regarding RnD for car manufacturers having relatively low revenues. This analysis is based on Porter's five forces framework and a PEST-analysis.</p><p>The data used for the regression has been gathered by the U.S. Environmental Protection Agency (EPA) and consists of 1245 passenger cars which was then processed in order to apply the regression. The results of the regression analysis performed indicated that it is possible to explain approximately 80% of the fuel consumption in passenger cars. More precisely, the number of cylinders, type of transmission and cylinder deactivation-system had the biggest impact on the fuel consumption. Furthermore the economic analysis of the industry revealed highly influencing external factors such as regulations and changing consumer preferences together with competition from other actors within the industry being the biggest threat.</p><p>The main conclusion from this thesis is that car manufacturers having relatively small revenues should implement cylinder deactivation systems and abandon the turbo-downsizing trend within the industry. Furthermore manual gearboxes should be abandoned in order to reduce the fuel consumption and manufacturing costs while increase the comfort for the consumer.</p>
----------------------------------------------------------------------
In diva2:1655678   - correct as is

w='degress' val={'c': 'degrees', 's': 'diva2:1655678', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:787494
Note: no full text in DiVA

abstract is: 
<p>Since cable shifting is a new sector for Scania, they want to have methods for analysis</p><p>and evaluation of such a system, but also possibilities of making comparisons</p><p>between dierent gear shifting devices regardless of the type of system. For that purpose</p><p>several measures and plots been have developed in order to work as tools for</p><p>evaluation and comparison of gear shift devices. The more important among those</p><p>are:meanboxplotslength, Tp-play, system rigidity, and wire buckling degree if a cable</p><p>shifting device is to be analyzed.</p><p>The report explains how to proceed when doing an analysis with FEM, but also</p><p>with rough estimations using simple formulas for calculations of the backlash in the</p><p>cable and the eciency. Input to those formulae are the total bending angle, the</p><p>friction coecient and the play between the core and the liner. Three dierent gear</p><p>shift devices have been investigated with those methods whereas two where with</p><p>cable and one was a linkage. Analysis indicates that the amount of increased play</p><p>in the system due to wear is about 36 %.</p><p>One diculty with transporting push loads in cables is that the core buckles</p><p>inside the liner at straight parts. The wire buckling degree has been developed in</p><p>order make comparison between dierent cable layings. Therefor is it recommended</p><p>to avoid straight parties in order to keep the buckling at a low level.</p><p>Regarding the gear shift housing for the cable - a design proposal has been made</p><p>that propose that a push force on the lever should give a push force in the cable.</p>

corrected abstract:
<p>Since cable shifting is a new sector for Scania, they want to have methods for analysis and evaluation of such a system, but also possibilities of making comparisons between different gear shifting devices regardless of the type of system. For that purpose several measures and plots been have developed in order to work as tools for evaluation and comparison of gear shift devices. The more important among those are: mean boxplots length, Tp-play, system rigidity, and wire buckling degree if a cable shifting device is to be analyzed. The report explains how to proceed when doing an analysis with FEM, but also with rough estimations using simple formulas for calculations of the backlash in the cable and the efficiency. Input to those formulae are the total bending angle, the friction coefficient and the play between the core and the liner. Three different gear shift devices have been investigated with those methods whereas two where with cable and one was a linkage. Analysis indicates that the amount of increased play in the system due to wear is about 36 %. One difficulty with transporting push loads in cables is that the core buckles inside the liner at straight parts. The wire buckling degree has been developed in order make comparison between different cable layings. Therefor is it recommended to avoid straight parties in order to keep the buckling at a low level. Regarding the gear shift housing for the cable - a design proposal has been made that propose that a push force on the lever should give a push force in the cable.</p>
----------------------------------------------------------------------
In diva2:651091 
abstract is: 
<p>The aim of this report is to examine which factors in general</p><p>that influence the price of a malt whisky and in particular</p><p>the price effect of the brand. Data was gathered from Systembolaget</p><p>AB’s assortment and was manipulated to fit the</p><p>needs of this project’s needs. The variables that were analysed</p><p>were the age and alcohol content of malt whisky, its</p><p>brand, the country of origin and, if it was a Scotch malt</p><p>whisky, the Scottish region of origin.</p><p>A theoretical model was developed and three different</p><p>main models were specified to part brand, country of origin</p><p>and region. To be able to conclude which independant</p><p>variables that were to be included in the final models the</p><p>Akaike Information Criterion corrected (AICc) for every</p><p>subset of the main model was calculated. The models with</p><p>low AICc score and highly significant independant variables</p><p>were chosen as the final models. These models were then</p><p>evaluated using multiple regression analysis to quantify the</p><p>variables’ price influence.</p><p>The analysis showed that some of the brands and countries</p><p>of origin had significant influence on the price and</p><p>there could be seen some significant influence on the price</p><p>in the Scottish whisky regions model as well. All models</p><p>gave an acceptable result for the purpose of the report.</p>


corrected abstract:
<p>The aim of this report is to examine which factors in general that influence the price of a malt whisky and in particular the price effect of the brand. Data was gathered from Systembolaget AB’s assortment and was manipulated to fit the needs of this project’s needs. The variables that were analysed were the age and alcohol content of malt whisky, its brand, the country of origin and, if it was a Scotch malt whisky, the Scottish region of origin.</p><p>A theoretical model was developed and three different main models were specified to part brand, country of origin and region. To be able to conclude which independant variables that were to be included in the final models the Akaike Information Criterion corrected (AICc) for every subset of the main model was calculated. The models with low AICc score and highly significant independant variables were chosen as the final models. These models were then evaluated using multiple regression analysis to quantify the variables’ price influence.</p><p>The analysis showed that some of the brands and countries of origin had significant influence on the price and there could be seen some significant influence on the price in the Scottish whisky regions model as well. All models gave an acceptable result for the purpose of the report.</p>
----------------------------------------------------------------------
In diva2:560956 
abstract is: 
<p>This thesis conducts a study on the stability of steady states in the</p><p>glycolysis of in silico models of Saccharomyces cerevisiae. Such un-</p><p>controlled models could reach unstable steady states that are unlikely</p><p>to occur in vivo. Little work has previously been done to examine</p><p>stability of such models.</p><p>The glycolysis is modeled as a system of nonlinear dierential equa-</p><p>tions. This is done by using rate equations describing the rate of change</p><p>in concentration of each metabolite involved in glycolysis. By lineariz-</p><p>ing this system around dierent equilibria and calculating the eigen-</p><p>values of the associated jacobian matrices the stability of the steady</p><p>states can be determined. Additionally perturbation analysis adds fur-</p><p>ther insight into the stability of the steady state.</p><p>Given the large range of possible initial conditions which result in</p><p>dierent steady states, a physiologically feasible one, as well as the</p><p>environment around it, is chosen to be the subject of this study. A</p><p>steady state is stable if all the eigenvalues of the Jacobian matrix are</p><p>negative. The model created by Teusink et al, and expanded upon by</p><p>Pritchard et, for the glycolysis in S.cerevisiae is used as the primary</p><p>model of the study.</p><p>The steady state does not have strictly negative eigenvalues: Two</p><p>of them are very close to zero, with one positive, within error tolerance</p><p>of our numerical methods. This means that linear analysis cannot</p><p>determine whether the steady state is stable. The whole nonlinear</p><p>system has to be considered. After performing perturbation analysis</p><p>we conclude that the steady state is most likely stable in the Lyapunov</p><p>sense.</p>

corrected abstract:
<p>This thesis conducts a study on the stability of steady states in the glycolysis of in silico models of Saccharomyces cerevisiae. Such uncontrolled models could reach unstable steady states that are unlikely to occur in vivo. Little work has previously been done to examine stability of such models.</p><p>The glycolysis is modeled as a system of nonlinear differential equations. This is done by using rate equations describing the rate of change in concentration of each metabolite involved in glycolysis. By linearizing this system around different equilibria and calculating the eigenvalues of the associated jacobian matrices the stability of the steady states can be determined. Additionally perturbation analysis adds further insight into the stability of the steady state.</p><p>Given the large range of possible initial conditions which result in different steady states, a physiologically feasible one, as well as the environment around it, is chosen to be the subject of this study. A steady state is stable if all the eigenvalues of the Jacobian matrix are negative. The model created by Teusink et al, and expanded upon by Pritchard et, for the glycolysis in S.cerevisiae is used as the primary model of the study.</p><p>The steady state does not have strictly negative eigenvalues: Two of them are very close to zero, with one positive, within error tolerance of our numerical methods. This means that linear analysis cannot determine whether the steady state is stable. The whole nonlinear system has to be considered. After performing perturbation analysis we conclude that the steady state is most likely stable in the Lyapunov sense.</p>
----------------------------------------------------------------------
In diva2:1298486 
abstract is: 
<p>Humanity has always wanted to explore the world we live in and answer different questions about our universe. After the International Space Station will end its service one possible next step could be a Moon Outpost: a convenient location for research, astronaut training and technological development that would enable long-duration space. This location can be inside one of the presumed lava tubes that should be present under the surface but would first need to be inspected, possibly by machine capable of capturing and relaying a map to a team on Earth.In this report the past and future Moon base missions will be summarized considering feasible outpost scenarios from the space companies or agencies. and their prospected manned budget. Potential mission profiles, objectives, requirements and constrains of the BATonomous Moon cave Explorer (BAT-ME) mission will be discussed and defined. Vehicle and mission concept will be addressed, comparing and presenting possible propulsion or locomotion approaches inside the lava tube.The Inkonova “Batonomous™” system is capable of providing Simultaneous Localization And Mapping (SLAM), relay the created maps, with the possibility to easily integrate the system on any kind of vehicle that would function in a real-life scenario.Although the system is not fully developed, it will be assessed from a technical perspective, and proper changes for a viable system transition for the space-Moon environment will be devised. The transition of the system from the Batonomous™ state to the BAT-ME required state will be presented from the requirement, hardware, software, electrical and operational point of view.The mission will be devised into operational phases, with key goals in mind. Two different vehicles will be presented and designed on a high engineering level. A risk analysis and management system will be made to understand the possible negative outcomes of different parts failure on the mission outcome.</p>

corrected abstract:
<p>Humanity has always wanted to explore the world we live in and answer different questions about our universe. After the International Space Station will end its service one possible next step could be a Moon Outpost: a convenient location for research, astronaut training and technological development that would enable long-duration space. This location can be inside one of the presumed lava tubes that should be present under the surface but would first need to be inspected, possibly by machine capable of capturing and relaying a map to a team on Earth.</p><p>In this report the past and future Moon base missions will be summarized considering feasible outpost scenarios from the space companies or agencies. and their prospected manned budget. Potential mission profiles, objectives, requirements and constrains of the BATonomous Moon cave Explorer (BAT-ME) mission will be discussed and defined. Vehicle and mission concept will be addressed, comparing and presenting possible propulsion or locomotion approaches inside the lava tube.</p><p>The Inkonova “Batonomous™” system is capable of providing Simultaneous Localization And Mapping (SLAM), relay the created maps, with the possibility to easily integrate the system on any kind of vehicle that would function in a real-life scenario.</p><p>Although the system is not fully developed, it will be assessed from a technical perspective, and proper changes for a viable system transition for the space-Moon environment will be devised. The transition of the system from the Batonomous™ state to the BAT-ME required state will be presented from the requirement, hardware, software, electrical and operational point of view.</p><p>The mission will be devised into operational phases, with key goals in mind. Two different vehicles will be presented and designed on a high engineering level. A risk analysis and management system will be made to understand the possible negative outcomes of different parts failure on the mission outcome.</p>
----------------------------------------------------------------------
In diva2:736615 
abstract is: 
<p>In this thesis, an attempt will be made to nd a model that predicts apartment prices</p><p>in inner city Stockholm. Lately, apartment prices have steadily increased and it has</p><p>become rather popular to invest money in apartments due to the high potential of value</p><p>increase. Of course not all apartments will increase in value, and we hope to nd the</p><p>factors that determine the value of an apartment that is about to be sold. To nd these</p><p>factors, we will combine the approaches of regression and time series analysis, creating</p><p>a linear regression model and a time series model, and then taking the mean of their</p><p>predictions.</p><p>Statistics of apartment sales during the time period of August 2012 to February 2014 will</p><p>be used as data, and when nding the models explicitly, large data handling softwares</p><p>such as SPSS, Excel and Matlab will be used. The thesis consists of an introduction</p><p>to the general theory behind the two approaches, followed by our specic case. Lastly,</p><p>an attempt will be made to draw some general conclusions from our ndings, and the</p><p>accuracy of our model will be tested by predicting the selling price of a few apartments</p><p>that have been or are about to be sold and comparing the predictions with the actual</p><p>price.</p>

corrected abstract:
<p>In this thesis, an attempt will be made to find a model that predicts apartment prices in inner city Stockholm. Lately, apartment prices have steadily increased and it has become rather popular to invest money in apartments due to the high potential of value increase. Of course not all apartments will increase in value, and we hope to find the factors that determine the value of an apartment that is about to be sold. To find these factors, we will combine the approaches of regression and time series analysis, creating a linear regression model and a time series model, and then taking the mean of their predictions.</p><p>Statistics of apartment sales during the time period of August 2012 to February 2014 will be used as data, and when finding the models explicitly, large data handling softwares such as SPSS, Excel and Matlab will be used. The thesis consists of an introduction to the general theory behind the two approaches, followed by our specific case. Lastly, an attempt will be made to draw some general conclusions from our findings, and the accuracy of our model will be tested by predicting the selling price of a few apartments that have been or are about to be sold and comparing the predictions with the actual price.</p>
----------------------------------------------------------------------
In diva2:919311 
abstract is: 
<p>When environmental laws are constricted and downsizing of engines has become the reality of the vehicle industry, there needs to be a solution for the rise in torsion vibrations in the drivetrain. These increased levels of torsion vibrations are mostly due to excitations from the firing pulses, which in turn have become increased due to higher cylinder pressures. One of the solutions for further dampening the system is to add a centrifugal pendulum absorber to the flywheel, and predicting the behaviour of such a device has become imperative.The intent of this thesis is to create a model that will accurately emulate the effectiveness and functionality of a centrifugal pendulum absorber, so that it can be used in simulations to predict vehicle behaviour with its addition. To validate the model, a comparison is made between simulated results, using the model created in Adams/Car and Matlab, and road measurements conducted using a prototype acquired by the industry.The results from the simulations show that, with existing theory on the subject and software provided by Scania, an accurate model can be created. The reduction of torsion vibrations is evident, and the model’s behaviour correlates to that of the prototype.Future work on the subject requires a larger insight into pendulums tuned to multiple orders, and an extension of the model geometry would be advantageous.</p>

corrected abstract:
<p>When environmental laws are constricted and downsizing of engines has become the reality of the vehicle industry, there needs to be a solution for the rise in torsion vibrations in the drivetrain. These increased levels of torsion vibrations are mostly due to excitations from the firing pulses, which in turn have become increased due to higher cylinder pressures. One of the solutions for further dampening the system is to add a centrifugal pendulum absorber to the flywheel, and predicting the behaviour of such a device has become imperative.</p><p>The intent of this thesis is to create a model that will accurately emulate the effectiveness and functionality of a centrifugal pendulum absorber, so that it can be used in simulations to predict vehicle behaviour with its addition. To validate the model, a comparison is made between simulated results, using the model created in Adams/Car and Matlab, and road measurements conducted using a prototype acquired by the industry.</p><p>The results from the simulations show that, with existing theory on the subject and software provided by Scania, an accurate model can be created. The reduction of torsion vibrations is evident, and the model’s behaviour correlates to that of the prototype.</p><p>Future work on the subject requires a larger insight into pendulums tuned to multiple orders, and an extension of the model geometry would be advantageous.</p>
----------------------------------------------------------------------
In diva2:1673690 
abstract is: 
<p>Emissions related to aviation have during the last decades become an important topic of discussion. Besides carbon dioxide ($CO_2$) which is the major pollutant from air travel, other gas emissions such as hydrocarbons ($HC$), nitrogen oxides ($NO_x$), carbon monoxide ($CO$) and sulfur oxides ($SO_x$) also need to be investigated. The work within this field has traditionally been challenged by the fact that aircraft emission calculations often required engine proprietary data which usually is difficult to obtain. However, in recent years other methods have been developed and this report investigates one such method, namely The Boeing Fuel Flow Method 2. The analysis is carried out on an ATR 72-500 turboprop aircraft flying at $13000$ feet from Visby to Bromma, Sweden. The method uses continuous data of fuel flow and altitude together with estimated emission index data at sea level for the specific engine to estimate the amount of emissions emitted during the flight. From this it was possible to determine the levels of $HC$,$NO_x$,$CO$, $CO_2$ and $SO_x$ emitted during the different stages of flight. There was a clear trend that $HC$ and $CO$ emissions were the highest at low fuel flow levels, i.e. at low power, while $NO_x$ increased with increasing fuel flow. Emission levels of $CO_2$ and $SO_x$ were found to be proportional to fuel flow. In addition, two alternative trajectories at $10000$ and $24000$ feet were studied. When comparing the $10000$ feet route with the original $13000$ feet route the the level of $NO_x$, $CO_2$ and $SO_x$ were unaffected while $HC$ and $CO$ decreased as the period of high fuel flow were shortened. In the $24000$ feet route the levels of $HC$, $CO$ and $SO_x$ were unaffected while the level of $CO_2$ and $NO_x$ decreased. This decrease can be explained by the lowered fuel flow rate as air resistance is significantly lower at $24000$ feet compared to $13000$ feet.</p>

corrected abstract:
<p>Emissions related to aviation have during the last decades become an important topic of discussion. Besides carbon dioxide (<em>CO<sub>2</sub></em>) which is the major pollutant from air travel, other gas emissions such as hydrocarbons (<em>HC</em>), nitrogen oxides (<em>NO<sub>x</sub></em>), carbon monoxide (<em>CO</em>) and sulfur oxides (<em>SO<sub>x</sub></em>) also need to be investigated. The work within this field has traditionally been challenged by the fact that aircraft emission calculations often required engine proprietary data which usually is difficult to obtain. However, in recent years other methods have been developed and this report investigates one such method, namely The Boeing Fuel Flow Method 2. The analysis is carried out on an ATR 72-500 turboprop aircraft flying at 𝟣𝟥𝟢𝟢𝟢 feet from Visby to Bromma, Sweden. The method uses continuous data of fuel flow and altitude together with estimated emission index data at sea level for the specific engine to estimate the amount of emissions emitted during the flight. From this it was possible to determine the levels of <em>HC</em>, <em>NO<sub>x</sub></em>, <em>CO</em>, <em>CO<sub>2</sub></em> and <em>SO<sub>x</sub></em> emitted during the different stages of flight. There was a clear trend that <em>HC</em> and <em>CO</em> emissions were the highest at low fuel flow levels, i.e. at low power, while <em>NO<sub>x</sub></em> increased with increasing fuel flow. Emission levels of <em>CO<sub>2</sub></em> and <em>SO<sub>x</sub></em> were found to be proportional to fuel flow. In addition, two alternative trajectories at 𝟣𝟢𝟢𝟢𝟢 and 𝟤𝟦𝟢𝟢𝟢 feet were studied. When comparing the 𝟣𝟢𝟢𝟢𝟢 feet route with the original 𝟣𝟥𝟢𝟢𝟢 feet route the the level of <em>NO<sub>x</sub></em>, <em>CO<sub>2</sub></em> and <em>SO<sub>x</sub></em> were unaffected while <em>HC</em> and <em>CO</em> decreased as the period of high fuel flow were shortened. In the 𝟤𝟦𝟢𝟢𝟢 feet route the levels of <em>HC</em>, <em>CO</em> and <em>SO<sub>x</sub></em> were unaffected while the level of <em>CO<sub>2</sub></em> and <em>NO<sub>x</sub></em> decreased. This decrease can be explained by the lowered fuel flow rate as air resistance is significantly lower at 𝟤𝟦𝟢𝟢𝟢 feet compared to 𝟣𝟥𝟢𝟢𝟢 feet.</p>

Note: The digits have been set using Mathematical Sans-Serif Digits.
----------------------------------------------------------------------
In diva2:1017468 
abstract is: 
<p>Head injuries such as concussions is a major problem in full contact sports like ice hockey, these injuries usually leads to rehabilitation and absence from sports as result. This risk in full contact sport is something athletes in these sports must expect, however to gain a better understanding of these injuries and reduce the number concussions would be a great progress.</p><p>This master thesis will investigate through video analysis in motion tracking software Skillspector velocities in moment of the body check. These velocities will form the basis of the LS-Dyna simulation to conduct when concussion may occur.</p><p>The video footage was mostly from NHL but some others are present all collected from YouTube. No special selection was made other than HD quality and some statement where concussions had occurred, a small reference group of non-concussed players was also collected to compare some results.</p><p>In total 31 concussions and 7 non concussion cases were evaluated. The average of these hits resulted in 6.87m/s for the attacking player, 4.46 for the concussed player, linear acceleration 657 m/s2, rotational acceleration 6883 rad/s2 and a HIC value of 731. For the players not receiving a concussion the average values were 5.83 m/s for the attacker, 3.53m/s for the targeted players resulting in linear acceleration of 385 m/s2, rotational acceleration 5474 rad/s2 and a HIC of 309.</p>


corrected abstract:
<p>Head injuries such as concussions is a major problem in full contact sports like ice hockey, these injuries usually leads to rehabilitation and absence from sports as result. This risk in full contact sport is something athletes in these sports must expect, however to gain a better understanding of these injuries and reduce the number concussions would be a great progress.</p><p>This master thesis will investigate through video analysis in motion tracking software Skillspector velocities in moment of the body check. These velocities will form the basis of the LS-Dyna simulation to conduct when concussion may occur.</p><p>The video footage was mostly from NHL but some others are present all collected from YouTube. No special selection was made other than HD quality and some statement where concussions had occurred, a small reference group of non-concussed players was also collected to compare some results.</p><p>In total 31 concussions and 7 non concussion cases were evaluated. The average of these hits resulted in 6.87m/s for the attacking player, 4.46 for the concussed player, linear acceleration 657 m/s<sup>2</sup>, rotational acceleration 6883 rad/s<sup>2</sup> and a HIC value of 731. For the players not receiving a concussion the average values were 5.83 m/s for the attacker, 3.53m/s for the targeted players resulting in linear acceleration of 385 m/s<sup>2</sup>, rotational acceleration 5474 rad/s<sup>2</sup> and a HIC of 309.</p>
----------------------------------------------------------------------
In diva2:1033230 
abstract is: 
<p>The Formula Student team participates in a competition every year where the car they build gets tested and rated according to strict rules and regulations. The more empirical testings are done in order to validate the data used to design various parts of the car, the higher the score in the competition. Until now the students have only had the opportunity to test the cars performance in general and not been able to go into detail with the forces acting on links and other parts during specic load scenarios.</p><p>This project is aimed at discerning the most suitable intruments to empirically measure the forces acting upon the suspension system as well as learning to use the ADAMS software in order to calculate the said forces on the suspension links of a Formula Student</p><p>Team car. The collected data from the empirical tests will be used to validate the team's Matlab code and ADAMS model, as well as to decide if the links included in the suspension system are designed properly. Due to some technical difficulties, the empirical testing was not perfomed in time to be included in the report. However, a full vehicle assembly was created using ADAMS model and the Matlab code was validated.</p>

corrected abstract:
<p>The Formula Student team participates in a competition every year where the car they build gets tested and rated according to strict rules and regulations. The more empirical testings are done in order to validate the data used to design various parts of the car, the higher the score in the competition. Until now the students have only had the opportunity to test the cars performance in general and not been able to go into detail with the forces acting on links and other parts during specific load scenarios.</p><p>This project is aimed at discerning the most suitable intruments to empirically measure the forces acting upon the suspension system as well as learning to use the ADAMS software in order to calculate the said forces on the suspension links of a Formula Student Team car. The collected data from the empirical tests will be used to validate the team's Matlab code and ADAMS model, as well as to decide if the links included in the suspension system are designed properly. Due to some technical difficulties, the empirical testing was not perfomed in time to be included in the report. However, a full vehicle assembly was created using ADAMS model and the Matlab code was validated.</p>

Note the spelling errors are in the original.
----------------------------------------------------------------------
In diva2:1450567 
abstract is: 
<p>This bachelor thesis examined the relationship between performance measures and prize money earnings on the PGA Tour. Using regression analysis and data from seasons 2004 through 2019 retrieved from the PGA Tour website this thesis examined if prize money could be predicted. Starting with 102 covariates, comprehensibly covering all aspects of the game, the model was reduced to 13 with Driving Distance being most prominent, favouring simplicity resulting in an R2Adjusted of 0.6918. The final model was discussed in regard to relevance, reliability and usability.</p><p>This thesis further analysed how the entry of ShotLink, the technology responsible for the vast statistical database surrounding the PGA Tour, have affected golf in general and the PGA Tour in particular. Analysis regarding how ShotLink affected golf on different levels, both for players as well as other stakeholders, where conducted. These show developments on multiple levels; on how statistics are used, golf related technologies, broadcasts, betting market, and both amateur and PGA Tour playing golf players. The analysis of the latter, using statistics from the PGA Tour website, showed a significant improvement in scoring average since ShotLinks inception.</p>

corrected abstract:
<p>This bachelor thesis examined the relationship between performance measures and prize money earnings on the PGA Tour. Using regression analysis and data from seasons 2004 through 2019 retrieved from the PGA Tour website this thesis examined if prize money could be predicted. Starting with 102 covariates, comprehensibly covering all aspects of the game, the model was reduced to 13 with <em>Driving Distance</em> being most prominent, favoring simplicity resulting in an <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>2</sup><sub>Adj</sub></span></span></em> of 0.6918. The final model was discussed in regards to relevance, reliability and usability.</p><p>This thesis further analyzed how the entry of ShotLink, the technology responsible for the vast statistical database surrounding the PGA Tour, have affected golf in general and the PGA Tour in particular. Analysis regarding how ShotLink affected golf on different levels, both for players as well as other stakeholders, where conducted. These show developments on multiple levels; on how statistics are used, golf related technologies, broadcasts, betting market, and both amateur and PGA Tour playing golf players. The analysis of the latter, using statistics from the PGA Tour website, showed a significant improvement in scoring average since ShotLinks inception.</p>
----------------------------------------------------------------------
In diva2:1114463 
abstract is: 
<p>This thesis investigates which factors that affect the salary for basketball players in the NBA and if the salary cap has achieved its purpose. The data for this project was collected from basketball-reference.com and consisted of performance measures from season 2015/2016 and salaries from the beginning of the season 2016/2017.</p><p>The study was performed by using multiple linear regression analysis in the software R and the data was handled in Excel. The results from the regression indicates that position point guard, if the player has played in D-league or not,Age, Offensive rebounds, Assists, Steals, Two point attempts, Three point attempts, Free throw attempts, Field goal percentage, Usage percentage and Defensive rating are the main factors that affect the salary. The performance measures that had the greatest were two and three point attempts. The regression model achieved an explanatory level of 57:4%. In complementary to analyze if the salary cap has achieved its purpose, a literature analysis was used and showed that the salary cap systems in North America are neither accurately designed nor do they satisfy the intentions of what they were set to achieve.</p>

corrected abstract:
<p>This thesis investigates which factors that affect the salary for basketball players in the NBA and if the salary cap has achieved its purpose. The data for this project was collected from basketball-reference.com and consisted of performance measures from season 2015/2016 and salaries from the beginning of the season 2016/2017.</p><p>The study was performed by using multiple linear regression analysis in the software R and the data was handled in Excel. The results from the regression indicates that position point guard, if the player has played in <em>D-league or not</em>, <em>Age</em>, <em>Offensive rebounds</em>, <em>Assists</em>, <em>Steals</em>, <em>Two point attempts</em>, <em>Three point attempts</em>, <em>Free throw attempts</em>, <em>Field goal percentage</em>, <em>Usage percentage</em> and <em>Defensive rating</em> are the main factors that affect the salary. The performance measures that had the greatest were <em>two</em> and <em>three</em> point attempts. The regression model achieved an explanatory level of 57.4%. In complementary to analyze if the salary cap has achieved its purpose, a literature analysis was used and showed that the salary cap systems in North America are neither accurately designed nor do they satisfy the intentions of what they were set to achieve.</p>
----------------------------------------------------------------------
In diva2:608463 - missing spaces in title:
"Analysis of simultaneous measurement of acousticpressure in the far-field and density gradient in thenear-field in a cold jet"
==>
"Analysis of simultaneous measurement of acoustic pressure in the far-field and density gradient in the near-field in a cold jet"

abstract is: 
<p>The purpose of the investigation presented here is to study the degree of correlation between the density gradient in the near eld and the aeroacoustic sound generation in the far eld of a cold jet. The variations of the density gradient in the ow are visualized and  recorded by the Background Oriented Schlieren (BOS) technique. In this method high power LEDs are used as illumination source. Simultaneously the pressure uctuations in the acoustic far-eld are recorded by a set of microphones. Measurements on a round nozzle and a chevron nozzle are conducted and the obtained results are compared. Large structures emitting noise are localized, being in the case of the chevron nozzle further upstream due to the higher level of entrainment of the jet with the surrounded ambient uid. In addition, it is demonstrated that the noise produced by these structures is radiated mainly downstream, having the highest correlations with the pressure uctuations recorded with the microphone located at an angle Ø= 26X respect to the jet-axis.</p>

corrected abstract:
<p>The purpose of the investigation presented here is to study the degree of correlation between the density gradient in the near field and the aeroacoustic sound generation in the far field of a cold jet. The variations of the density gradient in the flow are visualized and recorded by the Background Oriented Schlieren (BOS) technique. In this method high power LEDs are used as illumination source. Simultaneously the pressure fluctuations in the acoustic far-field are recorded by a set of microphones. Measurements on a round nozzle and a chevron nozzle are conducted and the obtained results are compared. Large structures emitting noise are localized, being in the case of the chevron nozzle further upstream due to the higher level of entrainment of the jet with the surrounded ambient fluid. In addition, it is demonstrated that the noise produced by these structures is radiated mainly downstream, having the highest correlations with the pressure fluctuations recorded with the microphone located at an angle <em>&theta;</em> = 26º respect to the jet-axis.</p>
----------------------------------------------------------------------
In diva2:575536  - there should not be a period at the end of the title
Note: The document appears to be scanned - there are no fonts.

abstract is: 
<p>Operational development is a frequently used term within organizations nowadays. The term refers to organizations that continuously strive to achieve better results by increasing the efficiency, improve the safety, expanding the customer base etc.</p><p>This study investigates what is determining if an organization is successful in operational development and what is needed in the organization on order to improve the performance in an efficient way. It also investigates how operational development should be conducted in companies consisting of a parent company and a number of subsidiaries in order to be successful. The main question of the study is whether there are any synergies in common strategies for operational development at subsidiaries or if operational development is more efficient if it is conducted separately. The study also investigates whether subsidiaries can take advantages of each others experiences of operational development by working more closely toghether with it. This is investigated through examining how learning occurs within and between organizations.</p><p>The study´s aim is also to identify which factors are essential for organizations in order to succeed with development of their operation, according to theories on operational development, different quality systems and a case study at The Company."The Association`s" recommendations on gow operational development should be performed by organizations in the type of business in which The Company operates are also examined. The recommendations are examined in order to identify crucial key factors and organisational attributes for operational development.</p><p>The project has been executed in cooperation with the Company and its five direct subsidiaries. As a basis for this investigation a number of qualitative interviews, a workshop and a litterature review have been performed.</p><p>The results of the investigation argue in favour for the existence of synergies in subsidiaries, working in common with operational development. Cooperation may even stimulate the work with operational development in respective subsidiary.</p><p>One possible strategy to work in common with operational development is to establish an operational development network where the organizations have the opportunity to discuss and exchange experiences. The litterature also shows clearly that there are great opportunities for subsidiaries to learn from each other as long as the willingness to share knowledge is present.</p><p>The study also indicates that it exist at least eight key factors that affect how successful the work with operational development is in an organization. These key factors are;  management commitment, employee involvement, communication,customer orientation, process orientation, continuous improvements, cultural changes and decisions based on facts.</p>

corrected abstract:
<p>Operational development is a frequently used term within organizations nowadays. The term refers to organizations that continuously strive to achieve better results by increasing the efficiency, improve the safety, expanding the customer base etc.</p><p>This study investigates what is determining if an organization is successful in operational development and what is needed in the organization on order to improve the performance in an efficient way. It also investigates how operational development should be conducted in companies consisting of a parent company and a number of subsidiaries in order to be successful. The main question of the study is whether there are any synergies in common strategies for operational development at subsidiaries or if operational development is more efficient if it is conducted separately. The study also investigates whether subsidiaries can take advantages of each others experiences of operational development by working more closely together with it. This is investigated through examining how learning occurs within and between organizations.</p><p>The study´s aim is also to identify which factors are essential for organizations in order to succeed with development of their operation, according to theories on operational development, different quality systems and a case study at The Company."The Association`s" recommendations on how operational development should be performed by organizations in the type of business in which The Company operates are also examined. The recommendations are examined in order to identify crucial key factors and organisational attributes for operational development.</p><p>The project has been executed in cooperation with the Company and its five direct subsidiaries. As a basis for this investigation a number of qualitative interviews, a workshop and a litterature review have been performed.</p><p>The results of the investigation argue in favour for the existence of synergies in subsidiaries, working in common with operational development. Cooperation may even stimulate the work with operational development in respective subsidiary.</p><p>One possible strategy to work in common with operational development is to establish an operational development network where the organizations have the opportunity to discuss and exchange experiences. The litterature also shows clearly that there are great opportunities for subsidiaries to learn from each other as long as the willingness to share knowledge is present.</p><p>The study also indicates that it exist at least eight key factors that affect how successful the work with operational development is in an organization. These key factors are;  management commitment, employee involvement, communication, customer orientation, process orientation, continuous improvements, cultural changes and decisions based on facts.</p>
----------------------------------------------------------------------
In diva2:1525187 
abstract is: 
<p>Flush-mounted cavity hot-wire probes have been around since two decades, but have typically not been applied as often compared to the traditional wall hot-wires mounted several wire diameters above the surface. While the latter suffer from heat conduction from the hot wire to the substrate in particular when used in air flows, the former is belived to significantly enhance the frequency response of the sensor. The recent work using a cavity hotwire by Gubian <em>et al.</em> (2019) came to the surprising conclusion that the magnitute of the fluctuating wall-shear stress <em>τ+w,rms </em>reaches an asymptotic value of 0.44 beyond the friction Reynolds number <em>Re τ </em>∼ 600. In an effort to explain this result, which is at odds with the majority of the literature, the present work combines direct numerical simulations (DNS) of a turbulent channel flow with a cavity modelled using the immersed boundary method, as well as an experimental replication of the study of Gubian et al. in a turbulent boundary layer to explain how the contradicting results could have been obtained. It is shown that the measurements of the mentioned study can be replicated qualitatively as a result of measurement problems. We will present why cavity hot-wire probes should neither be used for quantitative nor qualitative measurements of wall-bounded flows, and that several experimental short-comings can interact to sometimes falsely yield seemingly correct results.</p>

corrected abstract:
<p>Flush-mounted cavity hot-wire probes have been around since two decades, but have typically not been applied as often compared to the traditional wall hot-wires mounted several wire diameters above the surface. While the latter suffer from heat conduction from the hot wire to the substrate in particular when used in air flows, the former is believed to significantly enhance the frequency response of the sensor. The recent work using a cavity hotwire by Gubian <em>et al.</em> (2019) came to the surprising conclusion that the magnitude of the fluctuating wall-shear stress <em>τ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>+</sup><sub>w,rms</sub></span></span></em> reaches an asymptotic value of 0.44 beyond the friction Reynolds number <em>Re<sub>τ</sub></em> ∼600. In an effort to explain this result, which is at odds with the majority of the literature, the present work combines direct numerical simulations (DNS) of a turbulent channel flow with a cavity modelled using the immersed boundary method, as well as an experimental replication of the study of Gubian et al. in a turbulent boundary layer to explain how the contradicting results could have been obtained. It is shown that the measurements of the mentioned study can be replicated qualitatively as a result of measurement problems. We will present why cavity hot-wire probes should neither be used for quantitative nor qualitative measurements of wall-bounded flows, and that several experimental short-comings can interact to sometimes falsely yield seemingly correct results.</p>
----------------------------------------------------------------------
In diva2:812822   - correct as is

Note: Spelling errors in the original
w='numericaly' val={'c': 'numerically', 's': 'diva2:812822', 'n': 'error in original'}
w='preposses' val={'c': 'prepossess', 's': 'diva2:812822', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1341567 
Note: no full text in DiVA

abstract is: 
<p>An essential element to any online application is good service quality at the customers end. An effective way to ensure this is to predict the service quality using statistical learning. This project aims to use two different Machine Learning models, Random Forest and Linear Regression, to predict real-time measurements of service quality, on a Video on Demand and Key-Value store service. The focus is on their performances but the study also looks into what kind of conclusions about the system one can draw from analyzing these models. The method was to investigate the most important features to find the optimal relationship between computational time and accuracy. The study shows that Random Forest in most cases outperforms Linear regression when it comes to accuracy of the predictions. Our Random Forest implementation with the sci kit learn library for Python, was able to predict the video frame rate, for random and cyclic server load patterns, with an accuracy of 90.7-91.5%. The model was also able to predict average response times of a Key-Value store service, with an accuracy of 98-99.2%. This study present a general method of how to use feature reduction to find which parts of the system that correlates to the service quality,regardless of service application, machine learning algorithm or load pattern. Our results show that only a handful of selected features of the system needs to be observed to obtain an adequate prediction.</p>

corrected abstract:
<p>An essential element to any online application is good service quality at the customers end. An effective way to ensure this is to predict the service quality using statistical learning. This project aims to use two different Machine Learning models, Random Forest and Linear Regression, to predict real-time measurements of service quality, on a Video on Demand and Key-Value store service. The focus is on their performances but the study also looks into what kind of conclusions about the system one can draw from analyzing these models. The method was to investigate the most important features to find the optimal relationship between computational time and accuracy. The study shows that Random Forest in most cases outperforms Linear regression when it comes to accuracy of the predictions. Our Random Forest implementation with the scikit-learn library for Python, was able to predict the video frame rate, for random and cyclic server load patterns, with an accuracy of 90.7-91.5%. The model was also able to predict average response times of a Key-Value store service, with an accuracy of 98-99.2%. This study present a general method of how to use feature reduction to find which parts of the system that correlates to the service quality, regardless of service application, machine learning algorithm or load pattern. Our results show that only a handful of selected features of the system needs to be observed to obtain an adequate prediction.</p>
----------------------------------------------------------------------
In diva2:435252 
abstract is: 
<p>Computational Fluid Dynamics (CFD) is increasingly being used in nuclear reactor safety analysis as a tool that enables safety related physical phenomena occurring in the reactor coolant system to be described in more detail and accuracy. Validation is a necessary step in improving predictive capability of a computationa code or coupled computational codes. Validation refers to the assessment of model accuracy incorporating any uncertainties (aleatory and epistemic) that may be of importance. The uncertainties must be identi ed, quanti ed and if possible, reduced.</p>
<p>In the rst part of this thesis, a discussion on the development of an approach and experimental facility for the validation of coupled Computational Fluid Dynamics codes and System Thermal Hydraulics (STH) codes is given. The validation of a coupled code requires experiments which feature signi cant two-way feedbacks between the component (CFD sub-domain) and the system (STH sub-domain).</p>
<p>Results of CFD analysis that are used in the development of a exible design of the TALL-3D experimental facility are presented. The facility consists of a lead-bismuth eutectic (LBE) thermal-hydraulic loop operating in forced and natural circulation regimes with a heated pool-type 3D test section.</p>
<p>Transient analysis of the mixing and strati cation phenomena in the 3D test section under forced and natural circulation conditions in the loop show that the test section outlet temperature deviates from that predicted by analytical solution (which the 1D STH solution essentially is). Also an experimental validation test matrix according to the key physical phenomena of interest in the new experimental facility is developed.</p>
<p>In the second part of the thesis we consider the risk related to steam generator tube leakage or rupture (SGTL/R) in a pool-type design of lead-cooled reactor (LFR). We demonstrate that there is a possibility that small steam bubbles leaking from the SGT will be dragged by the turbulent coolant ow into the core region. Voiding of the core might cause threats of reactivity insertion accident or local damage (burnout) of fuel rod cladding.</p>
<p>Trajectories of the bubbles are determined by the bubble size and turbulent ow eld of lead coolant. The main objective of such study is to quantify likelihood of steam bubble transport to the core region in case of SGT leakage in the primary coolant system of the ELSY (European Lead-cooled</p>
<p>SYstem) design. Coolant ow eld and bubble motion are simulated by CFD code Star-CCM+. First, we discuss drag correlations for a steam bubble moving in liquid lead. Thereafter the steady state liquid lead ow eld in the primary system is modeled according to the ELSY design parameters of nominal full power operation. Finally, the consequences of SGT leakage are modeled by injecting bubbles in the steam generator region.</p>
<p>An assessment of the probability that bubbles can reach the core region and also accumulate in the primary system, is performed. The most dangerous leakage positions in the SG and bubble sizes are identi ed. Possible design solutions for prevention of core voiding in case of SGTL/R are discussed.</p>
<p> </p>



corrected abstract:
<p>Computational Fluid Dynamics (CFD) is increasingly being used in nuclear reactor safety analysis as a tool that enables safety related physical phenomena occurring in the reactor coolant system to be described in more detail and accuracy. Validation is a necessary step in improving predictive capability of a computational code or coupled computational codes. Validation refers to the assessment of model accuracy incorporating any uncertainties (aleatory and epistemic) that may be of importance. The uncertainties must be identified, quantified and if possible, reduced.</p>
<p>In the first part of this thesis, a discussion on the development of an approach and experimental facility for the validation of coupled Computational Fluid Dynamics codes and System Thermal Hydraulics (STH) codes is given. The validation of a coupled code requires experiments which feature significant two-way feedbacks between the component (CFD sub-domain) and the system (STH sub-domain). Results of CFD analysis that are used in the development of a flexible design of the TALL-3D experimental facility are presented. The facility consists of a lead-bismuth eutectic (LBE) thermal-hydraulic loop operating in forced and natural circulation regimes with a heated pool-type 3D test section. Transient analysis of the mixing and stratification phenomena in the 3D test section under forced and natural circulation conditions in the loop show that the test section outlet temperature deviates from that predicted by analytical solution (which the 1D STH solution essentially is). Also an experimental validation test matrix according to the key physical phenomena of interest in the new experimental facility is developed.</p>
<p>In the second part of the thesis we consider the risk related to steam generator tube leakage or rupture (SGTL/R) in a pool-type design of lead-cooled reactor (LFR). We demonstrate that there is a possibility that small steam bubbles leaking from the SGT will be dragged by the turbulent coolant flow into the core region. Voiding of the core might cause threats of reactivity insertion accident or local damage (burnout) of fuel rod cladding. Trajectories of the bubbles are determined by the bubble size and turbulent flow field of lead coolant. The main objective of such study is to quantify likelihood of steam bubble transport to the core region in case of SGT leakage in the primary coolant system of the ELSY (European Lead-cooled SYstem) design. Coolant flow field and bubble motion are simulated by CFD code Star-CCM+. First, we discuss drag correlations for a steam bubble moving in liquid lead. Thereafter the steady state liquid lead flow field in the primary system is modeled according to the ELSY design parameters of nominal full power operation. Finally, the consequences of SGT leakage are modeled by injecting bubbles in the steam generator region. An assessment of the probability that bubbles can reach the core region and also accumulate in the primary system, is performed. The most dangerous leakage positions in the SG and bubble sizes are identified. Possible design solutions for prevention of core voiding in case of SGTL/R are discussed.</p>
----------------------------------------------------------------------
In diva2:1362789   - correct as is

Note spelling error in original
w='centrury' val={'c': 'century', 's': 'diva2:1362789', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:957659 
abstract is: 
<p>This thesis aims at implementing and evaluating the performance of multivariate Expected Shortfall models on high frequency foreign exchange data. The implementation is conducted with a unique portfolio consisting of five foreign exchange rates; EUR/SEK, EUR/NOK, EUR/USD, USD/SEK and USD/NOK. High frequency is in this context defined as observations with time intervals from second by second up to minute by minute. The thesis consists of three main parts. In the first part, the exchange rates are modelled individually with time series models for returns and realized volatility. In the second part, the dependence between the exchange rates is modelled with copulas. In the third part, Expected Shortfall is calculated, the risk contribution of each exchange rate is derived and the models are backtested.</p><p>The results of the thesis indicate that three of the five final models can be rejected at a 5% significance level if the risk is measured by Expected Shortfall (ES0:05). The two models that cannot be rejected are based on the Clayton and Student’s t copulas, the only two copulas with heavy left tails. The rejected models are based on the Gaussian, Gumbel-Hougaard and Frank copulas. The fact that some of the copula models are rejected emphasizes the importance of choosing an appropriate dependence structure. The risk contribution calculations show that the risk contributions are highest from EUR/NOK and USD/NOK, and that EUR/USD has the lowest risk contribution and even decreases the portfolio risk in some cases. Regarding the underlying models, it is concluded that for the data used in this thesis, the final combined time series and copula models perform quite well, given that the purpose is to measure the risk. However, the most important parts to capture seem to be the fluctuations in the volatilities as well as the tail dependencies between the exchange rates. Thus, the predictions of the return mean values play a less significant role, even though they still improve the results and are necessary in order to proceed with other parts of the modelling. As future research, we first and foremost recommend including the liquidity aspect in the models.</p>

corrected abstract:
<p>This thesis aims at implementing and evaluating the performance of multivariate Expected Shortfall models on high frequency foreign exchange data. The implementation is conducted with a unique portfolio consisting of five foreign exchange rates; EUR/SEK, EUR/NOK, EUR/USD, USD/SEK and USD/NOK. High frequency is in this context defined as observations with time intervals from second by second up to minute by minute. The thesis consists of three main parts. In the first part, the exchange rates are modelled individually with time series models for returns and realized volatility. In the second part, the dependence between the exchange rates is modelled with copulas. In the third part, Expected Shortfall is calculated, the risk contribution of each exchange rate is derived and the models are backtested.</p><p>The results of the thesis indicate that three of the five final models can be rejected at a 5% significance level if the risk is measured by Expected Shortfall (ES<sub>0.05</sub>). The two models that cannot be rejected are based on the Clayton and Student’s t copulas, the only two copulas with heavy left tails. The rejected models are based on the Gaussian, Gumbel-Hougaard and Frank copulas. The fact that some of the copula models are rejected emphasizes the importance of choosing an appropriate dependence structure. The risk contribution calculations show that the risk contributions are highest from EUR/NOK and USD/NOK, and that EUR/USD has the lowest risk contribution and even decreases the portfolio risk in some cases. Regarding the underlying models, it is concluded that for the data used in this thesis, the final combined time series and copula models perform quite well, given that the purpose is to measure the risk. However, the most important parts to capture seem to be the fluctuations in the volatilities as well as the tail dependencies between the exchange rates. Thus, the predictions of the return mean values play a less significant role, even though they still improve the results and are necessary in order to proceed with other parts of the modelling. As future research, we first and foremost recommend including the liquidity aspect in the models.</p>


Note: The only change was to insert and fix the subscript.
----------------------------------------------------------------------
In diva2:1729765 - missing subscripting in title:
"Approaches to Structural Characterization of a Heteromeric GABA(A)R"
==>
"Approaches to Structural Characterization of a Heteromeric GABA<sub>A<subR"

abstract is: 
<p>Structural biology has become an important part of researching various diseases and drug development. In this thesis, I provide details on how I worked with approaches to structural characterization of a heteromeric GABA(A)R. These pentameric ligand gated ion channels take part in regulating inhibition of action potentials in nerve cells by allowing the passage of Cl- ions when bound by gamma-aminobutyric acid (GABA). They are formed by the assembly of five subunits which can be of various different types, denoted by greek letters and a number. Much is still unknown about how GABA and several other ligands bind to these ion channels and how that impacts function. Obtaining a structure of these proteins can aid in closing those knowledge gaps. It is reasonable to screen the proteins you have before you study their structures by Cryo-EM in order to get the best result, a methodology for which is described here. I have followed this methodology to screen two heteromeric GABA$_A$R that we wish to determine the structure of, alpha 5 beta 3 and rho 1 gamma 2. Neither of the combinations of genes we used to express these proteins proved to produce the desired fully assembled heteromeric protein. In the case of alpha 5 beta 3, we only witnessed building blocks, with no fully assembled channels. In rho 1 gamma 2, we instead only witnessed fully formed homomers of the rho 1 subunit. These findings then exclude the gene constructs used from further structural study, and the methodology described will inform the next steps to be taken.</p>

corrected abstract:
<p>Structural biology has become an important part of researching various diseases and drug development. In this thesis, I provide details on how I worked with approaches to structural characterization of a heteromeric GABA<sub>A<sub>R. These pentameric ligand gated ion channels take part in regulating inhibition of action potentials in nerve cells by allowing the passage of Cl<sup>-</sup> ions when bound by γ-aminobutyric acid (GABA). They are formed by the assembly of five subunits which can be of various different types, denoted by greek letters and a number. Much is still unknown about how GABA and several other ligands bind to these ion channels and how that impacts function. Obtaining a structure of these proteins can aid in closing those knowledge gaps. It is reasonable to screen the proteins you have before you study their structures by Cryo-EM in order to get the best result, a methodology for which is described here. I have followed this methodology to screen two heteromeric GABA<sub>A<sub>R that we wish to determine the structure of, α5β3 and ρ1γ2. Neither of the combinations of genes we used to express these proteins proved to produce the desired fully assembled heteromeric protein. In the case of α5β3, we only witnessed building blocks, with no fully assembled channels. In ρ1γ2, we instead only witnessed fully formed homomers of the ρ1 subunit. These findings then exclude the gene constructs used from further structural study, and the methodology described will inform the next steps to be taken.</p>
----------------------------------------------------------------------
In diva2:550494 
abstract is: 
<p>With increasing presence and interest of shipping activities in the Arctic, the risk for an oil spill also increases. The activities are coupled to a growth of Arctic tourism (cruise vessels), exploitation of oil and gas resources as well as possibilities for merchant ships to sail the routes of Northwest and Northeast passages.</p><p>The Arctic offers an impressive environment with high potential for tourism and offshore activities, however the Arctic is also highly vulnerable. Thus, higher demand of awareness of the risks as well as the possibilities and opportunities to take care of an oil spill and reduce the consequences are needed. Initially the report gives a background to the subject of Arctic oil spill which is followed by a review of Arctic oil spill response.</p><p>The processes involved and oil spill countermeasures that are used or have shown potential in Arctic conditions are handled. To increase awareness a decision support tool which aims to cover preparedness, response and performance of an Arctic oil spill response operations is developted and presented. In the model structure, a wide range of input and sub-models are included to be able to cover the whole operation and different sub-areas that are identified.</p><p>Finally a further developed part of the decision support tool is presented concerning the window of opportunity which review the response methods. The model, which is based on a Bayesian Network approach, provides the user with estimations of response method potentials as function of time. The model output are easy and clear to interpret for contingency planning as well as for operational use.</p>

corrected abstract:
<p>With increasing presence and interest of shipping activities in the Arctic, the risk for an oil spill also increases. The activities are coupled to a growth of Arctic tourism (cruise vessels), exploitation of oil and gas resources as well as possibilities for merchant ships to sail the routes of Northwest and Northeast passages.</p><p>The Arctic offers an impressive environment with high potential for tourism and offshore activities, however the Arctic is also highly vulnerable. Thus, higher demand of awareness of the risks as well as the possibilities and opportunities to take care of an oil spill and reduce the consequences are needed. Initially the report gives a background to the subject of Arctic oil spill which is followed by a review of Arctic oil spill response. The processes involved and oil spill countermeasures that are used or have shown potential in Arctic conditions are handled.</p><p>To increase awareness a decision support tool which aims to cover preparedness, response and performance of an Arctic oil spill response operations is developted and presented. In the model structure, a wide range of input and sub-models are included to be able to cover the whole operation and different sub-areas that are identified.</p><p>Finally a further developed part of the decision support tool is presented concerning the window of opportunity which review the response methods. The model, which is based on a Bayesian Network approach, provides the user with estimations of response method potentials as function of time. The model output are easy and clear to interpret for contingency planning as well as for operational use.</p>

Note - spelling error in the original:
w='developted' val={'c': 'developed', 's': 'diva2:550494', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1478959 
abstract is: 
<p>Globally, approximately 1.2 million people die each year due to traffic accidents. Upper extremity injuries account for 18% to 25% of all car accident injuries. In order to be able to analyze these crash-related injuries, Human body models(HBMs) are used as a complement to FE simulations. An example of a HBM is the THUMS SAFER that is based on a 50 percentile American male. The aim of this study was to improve the upper extremity of the THUMS SAFER with respect to Autoliv's requirements to better predict fractures. In addition, this was validated against the Forman experiment(Forman, et al., The journal of trauma and acute care surgery, vol. 77, 2014) where human cadavers of the upper extremity were axially impacted to replicate a car collision. This was done by generating the upper extremity geometry with segmentation of medical images of a right human hand in combination with the complete STL-geometry of the forearm from the Piper project. The STL-geometry of the segmented human hand and Piper forearm was integrated and a complete STL-geometry of the upper extremity was obtained. Based on the complete STL-geometry, the FE-arm HEX 4.0 was built with modelling of bones, ligaments, soft tissue and skin with corresponding material choice in accordance with Autoliv's requirements.</p><p>The model HEX 4.0 was improved considering an increased mesh density from an average of 94% to 98%. HEX 4.0 was also validated against the data from the Forman experiment for experiments 5, 6 and 15. It showed a good correlation with the acceleration curves between the simulated and experimental values for the three experiments. The reaction force in the elbow was compared for experiment 15, where the simulated value 5.7 kN divided by a factor of 1.4 from 4 kN for the experiment. Furthermore, the fi rst principal strains that occurred in HEX 4.0 were analysed by 17 ms were the highest acceleration was achieved for experiments 5 and 6. Both experiments were shown to be close to the failure threshold of bones. However, the highest value e5=9.8E-03 occurred in the radius for experiment 5, while e6=9.3E-03 in a ligament for experiment 6. In addition, the failure threshold for experiment 15 exceeded 5 ms in lunate, schapoid and triquetrum. This indication of fractures is in good agreement with the experimental results where the corresponding bones resulted in fractures in experiment 15. HEX 4.0 was an improved upper extremity of the THUMS SAFER considering an increased mesh density. It is also capable of indicating fractures and corresponding positions in the form of analyzes of occurring stresses and strains. Nevertheless, improvements and further validation of HEX 4.0 has been proposed in the future work section.</p>


corrected abstract:
<p>Globally, approximately 1.2 million people die each year due to traffic accidents. Upper extremity injuries account for 18% to 25% of all car accident injuries. In order to be able to analyze these crash-related injuries, Human body models(HBMs) are used as a complement to FE simulations. An example of a HBM is the THUMS SAFER that is based on a 50 percentile American male. The aim of this study was to improve the upper extremity of the THUMS SAFER with respect to Autoliv's requirements to better predict fractures. In addition, this was validated against the Forman experiment(Forman, et al., The journal of trauma and acute care surgery, vol. 77, 2014) where human cadavers of the upper extremity were axially impacted to replicate a car collision. This was done by generating the upper extremity geometry with segmentation of medical images of a right human hand in combination with the complete STL-geometry of the forearm from the Piper project. The STL-geometry of the segmented human hand and Piper forearm was integrated and a complete STL-geometry of the upper extremity was obtained. Based on the complete STL-geometry, the FE-arm HEX 4.0 was built with modelling of bones, ligaments, soft tissue and skin with corresponding material choice in accordance with Autoliv's requirements.</p><p>The model HEX 4.0 was improved considering an increased mesh density from an average of 94% to 98%. HEX 4.0 was also validated against the data from the Forman experiment for experiments 5, 6 and 15. It showed a good correlation with the acceleration curves between the simulated and experimental values for the three experiments. The reaction force in the elbow was compared for experiment 15, where the simulated value 5.7 kN divided by a factor of 1.4 from 4 kN for the experiment. Furthermore, the first principal strains that occurred in HEX 4.0 were analysed by 17 ms were the highest acceleration was achieved for experiments 5 and 6. Both experiments were shown to be close to the failure threshold of bones. However, the highest value <em>&epsilon;</em><sub>5</sub>=9.8E-03 occurred in the radius for experiment 5, while <em>&epsilon;</em><sub>6</sub>=9.3E-03 in a ligament for experiment 6. In addition, the failure threshold for experiment 15 exceeded 5 ms in lunate, schapoid and triquetrum. This indication of fractures is in good agreement with the experimental results where the corresponding bones resulted in fractures in experiment 15. HEX 4.0 was an improved upper extremity of the THUMS SAFER considering an increased mesh density. It is also capable of indicating fractures and corresponding positions in the form of analyzes of occurring stresses and strains. Nevertheless, improvements and further validation of HEX 4.0 has been proposed in the future work section.</p>

Note spelling error in original:
w='schapoid' val={'c': 'scaphoid', 's': 'diva2:1478959', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1380216 
abstract is: 
<p>New Cities expand rapidly and the increase of population is resulting in longer and more frequent travelling distances. The demand of increasing the vehicles’ efficiency, safety and comfort is highly prioritized. Therefore, the demand of au-tonomous vehicles is increasing, since many studies indicate of reducing road acci-dent by eliminating the human errors. This thesis is mainly about a study of Con-volution neural network and how it is used for the purpose of object identification. Also, a customized model has been created used for objects identification found in the Swedish traffic, which are cars, people, traffic lights and Swedish traffic signs. The choice of the network has been decided based on specific criteria, which are; the network should not be large and is still sufficient enough in terms of accuracy and processing speed. In this thesis, based on the results analysis, the criteria are partly fulfilled and in the discussion section, the drawbacks are presented and what should be done before the model can be implemented on an embedded system.</p>

corrected abstract:
<p>New Cities expand rapidly and the increase of population is resulting in longer and more frequent travelling distances. The demand of increasing the vehicles’ efficiency, safety and comfort is highly prioritized. Therefore, the demand of autonomous vehicles is increasing, since many studies indicate of reducing road accident by eliminating the human errors. This thesis is mainly about a study of Convolution neural network and how it is used for the purpose of object identification.</p><p>Also, a customized model has been created used for objects identification found in the Swedish traffic, which are cars, people, traffic lights and Swedish traffic signs. The choice of the network has been decided based on specific criteria, which are; the network should not be large and is still sufficient enough in terms of accuracy and processing speed. In this thesis, based on the results analysis, the criteria are partly fulfilled and in the discussion section, the drawbacks are presented and what should be done before the model can be implemented on an embedded system.</p>
----------------------------------------------------------------------
In diva2:517594 
abstract is: 
<p>Abstract</p><p> </p><p>An Asian option is a path-depending exotic option, which means that either the settlement price or the strike of the option is formed by some aggregation of underlying asset prices during the option lifetime. This thesis will focus on European style Arithmetic Asian options where the settlement price at maturity is formed by the arithmetic average price of the last seven days of the underlying asset.</p><p>For this type of option it does not exist any closed form analytical formula for calculating the theoretical option value. There exist closed form approximation formulas for valuing this kind of option. One such, used in this thesis, approximate the value of an Arithmetic Asian option by conditioning the valuation on the geometric mean price. To evaluate the accuracy in this approximation and to see if it is possible to use the well known Black-Scholes formula for valuing Asian options, this thesis examines the bias between Monte-Carlo simulation pricing and these closed form approximate pricings. The bias examination is done for several different volatility schemes.</p><p>In general the Asian approximation formula works very well for valuing Asian options. For volatility scenarios where there is a drastic volatility shift and the period with higher volatility is before the average period of the option, the Asian approximation formula will underestimate the option value. These underestimates are very significant for OTM options, decreases for ATM options and are small, although significant, for ITM options.</p><p>The Black-Scholes formula will in general overestimate the Asian option value. This is expected since the Black-Scholes formula applies to standard European options which only, implicitly, considers the underlying asset price at maturity of the option as settlement price. This price is in average higher than the Asian option settlement price when the underlying asset price has a positive drift. However, for some volatility scenarios where there is a drastic volatility shift and the period with higher volatility is before the average period of the option, even the Black-Scholes formula will underestimate the option value. As for the Asian approximation formula, these over-and underestimates are very large for OTM options and decreases for ATM and ITM options.</p><p> </p>


corrected abstract:
<p>An Asian option is a path-depending exotic option, which means that either the settlement price or the strike of the option is formed by some aggregation of underlying asset prices during the option lifetime. This thesis will focus on European style Arithmetic Asian options where the settlement price at maturity is formed by the arithmetic average price of the last seven days of the underlying asset.</p><p>For this type of option it does not exist any closed form analytical formula for calculating the theoretical option value. There exist closed form approximation formulas for valuing this kind of option. One such, used in this thesis, approximate the value of an Arithmetic Asian option by conditioning the valuation on the geometric mean price. To evaluate the accuracy in this approximation and to see if it is possible to use the well known Black-Scholes formula for valuing Asian options, this thesis examines the bias between Monte-Carlo simulation pricing and these closed form approximate pricings. The bias examination is done for several different volatility schemes.</p><p>In general the Asian approximation formula works very well for valuing Asian options. For volatility scenarios where there is a drastic volatility shift and the period with higher volatility is before the average period of the option, the Asian approximation formula will underestimate the option value. These underestimates are very significant for OTM options, decreases for ATM options and are small, although significant, for ITM options.</p><p>The Black-Scholes formula will in general overestimate the Asian option value. This is expected since the Black-Scholes formula applies to standard European options which only, implicitly, considers the underlying asset price at maturity of the option as settlement price. This price is in average higher than the Asian option settlement price when the underlying asset price has a positive drift. However, for some volatility scenarios where there is a drastic volatility shift and the period with higher volatility is before the average period of the option, even the Black-Scholes formula will underestimate the option value. As for the Asian approximation formula, these over-and underestimates are very large for OTM options and decreases for ATM and ITM options.</p>
----------------------------------------------------------------------
In diva2:1040629   - correct as is

Note spelling error in originaL:
w='thier' val={'c': 'their', 's': 'diva2:1040629', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1211174 
abstract is: 
<p>There is no single commonly adapted model that explains the influence that various monetary policy instruments carry for the economy. During 2011-2017, the Swedish inflation rate has remained below the 2 percent target which has led the Riksbank to take measures aimed at stimulating the inflation. As of May 2018, the repo rate has experienced a number of decreases and is now at 􀀀0:50% which represents an unprecedentedly low level. With the inflation rate remaining below the target whilst the housing market has experienced substantial growth and recent decline, the question arises regarding what impact the repo rate exerts on various macroeconomic measures. In this paper, a statistical time series analysis is conducted using a Vector Autoregression model and the impulse responses are studied. A model of 7 economic variables is constructed to specially study the effect of the repo rate on employment and housing prices. Results demonstrate that rational expectations exist in the economy. Furthermore, results show that the repo rate influences factors affected by inflation rapidly, exerting maximum influence during the first year after the shock. On the other hand, real variables based on quantitative measures that are adjusted for inflation experience the greatest influence of the repo rate after a delay of 6 to 7 quarters. Employment experiences the greatest negative response to a repo rate shock after 7 quarters, with a magnitude of 0.317 standard deviations per standard deviation in the repo rate shock. Housing prices experience the greatest negative response to a repo rate shock after 4 quarters, with a magnitude of 0.209 standard deviations per standard deviation in the repo rate shock.</p>

corrected abstract:
<p>There is no single commonly adapted model that explains the influence that various monetary policy instruments carry for the economy. During 2011-2017, the Swedish inflation rate has remained below the 2 percent target which has led the Riksbank to take measures aimed at stimulating the inflation. As of May 2018, the repo rate has experienced a number of decreases and is now at −0.50% which represents an unprecedentedly low level. With the inflation rate remaining below the target whilst the housing market has experienced substantial growth and recent decline, the question arises regarding what impact the repo rate exerts on various macroeconomic measures. In this paper, a statistical time series analysis is conducted using a Vector Autoregression model and the impulse responses are studied. A model of 7 economic variables is constructed to specifically study the effect of the repo rate on employment and housing prices. Results demonstrate that rational expectations exist in the economy. Furthermore, results show that the repo rate influences factors affected by inflation rapidly, exerting maximum influence during the first year after the shock. On the other hand, real variables based on quantitative measures that are adjusted for inflation experience the greatest influence of the repo rate after a delay of 6 to 7 quarters.</p><p>Employment experiences the greatest negative response to a repo rate shock after 7 quarters, with a magnitude of 0.317 standard deviations per standard deviation in the repo rate shock. Housing prices experience the greatest negative response to a repo rate shock after 4 quarters, with a magnitude of 0.209 standard deviations per standard deviation in the repo rate shock.</p>
----------------------------------------------------------------------
In diva2:1351779 
abstract is: 
<p>The work conducted in this project proposes a method to determine the location of fracture initiation for non-load carrying fillet welds based on continuous geometry measurements. Measurements were carried out on welded specimens using a laser line triangluator together with the weld quality evaluation software qWeld from Winteria®. The 119 specimens produced were after scanning fatigue tested until failure. The fracture surfaces have been investigated in order to find the most probable location(s) of fracture initiation. This data was then used to fit the proposed model parameters used to analytically predict the point(s) of fracture initiation. Local weld geometry measurements were extracted from the predicted fracture initiation location(s) in an effort to analyse the correlation between local weld geometry and fatigue life. This showed that fatigue life and leg length were positively correlated and that strong correlations exists between the individual geometrical parameters. New specimens were produced to test the fitted data of the proposed model, this data was however inconclusive as the new specimens had a high ratio of weld defects.</p>

corrected abstract:
<p>The work conducted in this project proposes a method to determine the location of fracture initiation for non-load carrying fillet welds based on continuous geometry measurements. Measurements were carried out on welded specimens using a laser line triangulator together with the weld quality evaluation software qWeld from Winteria®. The 119 specimens produced were after scanning fatigue tested until failure. The fracture surfaces have been investigated in order to find the most probable location(s) of fracture initiation. This data was then used to fit the proposed model parameters used to analytically predict the point(s) of fracture initiation. Local weld geometry measurements were extracted from the predicted fracture initiation location(s) in an effort to analyse the correlation between local weld geometry and fatigue life. This showed that fatigue life and leg length were positively correlated and that strong correlations exists between the individual geometrical parameters. New specimens were produced to test the fitted data of the proposed model, this data was however inconclusive as the new specimens had a high ratio of weld defects.</p>

Only change: correction of spelling error:
w='triangluator' val={'c': 'triangulator', 's': 'diva2:1351779', 'n': 'no full text'}
----------------------------------------------------------------------
In diva2:562498 
abstract is: 
<p>The eXtended Finite Element Method is a partition of unity based method, particularly suitable for modelling crack propagation phenomena, without knowing a priori the crack path. Its numerical implementation is mostly achieved with stand-alone codes. The implementation of the eXtended Finite Element Method in commercial FEA softwares is still limited, and the most famous one including such capabilities is Abaqus TM. However, due to its relatively recent intro-duction, XFEM technique in Abaqus has been proved to provide trustable results only in few simple benchmark problems involving linear elastic material models.In this work, we present an assessment of the applicability of the eXtendend Finite Element Method in Abaqus, to deal with fracture mechanics problems of rubber-like materials. Results are provided for both Neo-Hookean and Arruda-Boyce material models, under plane strain conditions. In the rst part of this work, a static analysis for the pure Mode-I and for a 45o mixed-Mode load condition, whose objective has been to evaluate the ability of the XFEM technique in Abaqus, to correctly model the stress and displacement elds around a crack tip, has been performed. Outcomes from XFEM analysis with coarse meshes have been compared with the analogous ones obtained with highly re ned standard FEM discretizations. Noteworthy, despite the remarkable level of accuracy in analyzing the displacement eld at the crack tip, concerning the stress eld, the adoption of the XFEM provides no bene ts, if compared to the standard FEM formulation. The only remarkable advantage is the possibility to discretize the model without the mesh con-forming the crack geometry. Furthermore, the dynamic process of crack propagation has been analyzed by means of the XFEM. A 45o mixed-Mode and a 30o mixed-Mode load condition are analyzed. In particular, three fundamental aspects of the crack propagation phenomenon have been investigated, i.e. the instant at which a pre-existing crack starts to propagate within the body under the applied boundary conditions, the crack propagation direction and the predicted crack propagation speeds. According to the obtained results, the most inuent parameters are thought to be the elements size at the crack tip hand the applied displacement ratev. Severe diculties have been faced to attain convergence. Some reasonable motivations of the unsatisfactory convergence behaviour are proposed.</p>

corrected abstract:
<p>The eXtended Finite Element Method is a partition of unity based method, particularly suitable for modelling crack propagation phenomena, without knowing a priori the crack path. Its numerical implementation is mostly achieved with stand-alone codes.</p><p>The implementation of the eXtended Finite Element Method in commercial FEA softwares is still limited, and the most famous one including such capabilities is Abaqus™. However, due to its relatively recent introduction, XFEM technique in Abaqus has been proved to provide trustable results only in few simple benchmark problems involving linear elastic material models.</p><p>In this work, we present an assessment of the applicability of the eXtendend Finite Element Method in Abaqus, to deal with fracture mechanics problems of rubber-like materials. Results are provided for both Neo-Hookean and Arruda-Boyce material models, under plane strain conditions.</p><p>In the first part of this work, a static analysis for the pure Mode-I and for a 45º mixed-Mode load condition, whose objective has been to evaluate the ability of the XFEM technique in Abaqus, to correctly model the stress and displacement fields around a crack tip, has been performed. Outcomes from XFEM analysis with coarse meshes have been compared with the analogous ones obtained with highly refined standard FEM discretizations.</p><p>Noteworthy, despite the remarkable level of accuracy in analyzing the displacement field at the crack tip, concerning the stress field, the adoption of the XFEM provides no benefits, if compared to the standard FEM formulation. The only remarkable advantage is the possibility to discretize the model without the mesh conforming the crack geometry.</p><p>Furthermore, the dynamic process of crack propagation has been analyzed by means of the XFEM. A 45º mixed-Mode and a 30º mixed-Mode load condition are analyzed. In particular, three fundamental aspects of the crack propagation phenomenon have been investigated, i.e. the instant at which a pre-existing crack starts to propagate within the body under the applied boundary conditions, the crack propagation direction and the predicted crack propagation speeds.</p><p>According to the obtained results, the most influent parameters are thought to be the elements size at the crack tip ℎ and the applied displacement rate &#x1D708;. Severe difficulties have been faced to attain convergence. Some reasonable motivations of the unsatisfactory convergence behaviour are proposed.</p>


Note the spelling error "eXtendend" is in the original.
----------------------------------------------------------------------
In diva2:1571111 
abstract is: 
<p>Shaped Charges (SC) are explosives which are set up to concentrate the energy of an explosive to deform a thin metal cover, so called liner into either a jet of particles or a solid projectile, the latter one is refered to an Explosively Formed Projectile or EFP. The purpose of this project is to map out the effect of different asymmetries in different parts of the EFP. Several asymmetries are investigated such as cavities in the explosive, offset positions of the point of detonation as well as an array of errors concerning the liner. Using ANSYS Space Claim for modelling and IMPETUS for simulating the combustion of the explosive and deformation of the liner. </p><p>By analyzing the velocity of the projectile in both the direction it is pointed in and in the direction of the asymmetry a mapping of how potential production errors effect its performance. The following is some of the most interesting results acquired: </p><p>There is close to no difference having the detonation point further in, there is however a crucial difference in performance for the remaining asymmetries. Rust gives an asymmetrically deformed projectile, although rust on the inside perimeter of the liner gives better results than having the rust on the outside perimeter it still gives a close to useless projectile because of its form. Skewed detonation seems to give approximately 1 m/s velocity per 0.5mm of distance from the central axis. Although the simulations run here only gave a miss of around a decimeter the rotational velocity of the projectiles is what is most worrying. When it comes to cavities in the HE there is a clear pattern of the effects getting worse the closer to the liner the bubbles are. The Offset detonation shows most rotation.</p><p> </p>


corrected abstract:
<p>Shaped Charges (SC) are explosives which are set up to concentrate the energy of an explosive to deform a thin metal cover, so called liner into either a jet of particles or a solid projectile, the latter one is refered to an Explosively Formed Projectile or EFP. The purpose of this project is to map out the effect of different asymmetries in different parts of the EFP. Several asymmetries are investigated such as cavities in the explosive, offset positions of the point of detonation as well as an array of errors concerning the liner. Using ANSYS Space Claim for modelling and IMPETUS for simulating the combustion of the explosive and deformation of the liner.</p><p>By analyzing the velocity of the projectile in both the direction it is pointed in and in the direction of the asymmetry a mapping of how potential production errors effect its performance. The following is some of the most interesting results acquired:</p><p>There is close to no difference having the detonation point further in, there is however a crucial difference in performance for the remaining asymmetries. Rust gives an asymmetrically deformed projectile, although rust on the inside perimeter of the liner gives better results than having the rust on the outside perimeter it still gives a close to useless projectile because of its form. Skewed detonation seems to give approximately 1 m/s velocity per 0.5mm of distance from the central axis. Although the simulations run here only gave a miss of around a decimeter the rotational velocity of the projectiles is what is most worrying. When it comes to cavities in the HE there is a clear pattern of the effects getting worse the closer to the liner the bubbles are. The Offset detonation shows most rotation.</p

Note: Only changes were to remove some unnecessary spaces and the unnecessarty empty paragraph.
Note spelling error in original:
w='refered' val={'c': 'referred', 's': 'diva2:1571111', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1573058 
abstract is: 
<p>This thesis concerns the effects of a select few asymmetries on a jet</p><p>forming shaped charge. A Shaped charge(SC) is a formed explosive with</p><p>metal liner that collapses under the detonation. The resulting</p><p>shock­ compression results in a jet of metal being shot forward in up</p><p>10km/s. The asymmetries that were modelled are bubbles in the LX­14 high</p><p>explosive, rust on the copper liner and off­ center detonation. To</p><p>simulate the SC the software IMPETUS afea is used. By replacing the</p><p>geometry with SPH particles instead of a mesh, a simulation with broad</p><p>boundaries is possible which helps in the simulation of explosives. The</p><p>results showed that the off­ center detonation caused the biggest</p><p>deviation in the jet. The rust caused fragmentation in the jet but a</p><p>smaller deviation and the bubbles caused very little damage to the jet</p><p>formation.</p>

corrected abstract:
<p>This thesis concerns the effects of a select few asymmetries on a jet forming shaped charge. A Shaped charge(SC) is a formed explosive with metal liner that collapses under the detonation. The resulting shock­compression results in a jet of metal being shot forward in up 10km/s. The asymmetries that were modelled are bubbles in the LX­14 high explosive, rust on the copper liner and off­center detonation. To simulate the SC the software IMPETUS afea is used. By replacing the geometry with SPH particles instead of a mesh, a simulation with broad boundaries is possible which helps in the simulation of explosives. The results showed that the off­center detonation caused the biggest deviation in the jet. The rust caused fragmentation in the jet but a smaller deviation and the bubbles caused very little damage to the jet formation.</p>
----------------------------------------------------------------------
In diva2:612188 
abstract is: 
<p>This paper deals with the study of the robustness concerning the attitude control of the spacecraft TARANIS regarding disturbances on its sun acquisition sensors. Two kinds of disturbances have been studied: the masking of the sunlight by the dierent spacecraft devices as well as the sunlight reexion on their surface. This study has been performed by doing rst the sensor and observer modeling on a simulator specially designed for the study from the whole spacecraft simulator. Then the modeling of the disturbances has been achieved depending on the characteristics of the sources in terms of size, positioning, roughness and light reection. Finally a set of simulations of the acquisition and survival mode has been executed in order to evaluate the impact of the disturbances on its convergence time. The study shows that the algorithm designed to calculate the spacecraft attitude from the solar sensors data set is robust concerning these disturbances with the actual design of the satellite, but also shows limits concerning the size and positioning of its devices.</p>

corrected abstract:
<p>This paper deals with the study of the robustness concerning the attitude control of the spacecraft TARANIS regarding disturbances on its sun acquisition sensors. Two kinds of disturbances have been studied: the masking of the sunlight by the different spacecraft devices as well as the sunlight reflexion on their surface. This study has been performed by doing first the sensor and observer modeling on a simulator specially designed for the study from the whole spacecraft simulator. Then the modeling of the disturbances has been achieved depending on the characteristics of the sources in terms of size, positioning, roughness and light reflection. Finally a set of simulations of the acquisition and survival mode has been executed in order to evaluate the impact of the disturbances on its convergence time. The study shows that the algorithm designed to calculate the spacecraft attitude from the solar sensors data set is robust concerning these disturbances with the actual design of the satellite, but also shows limits concerning the size and positioning of its devices.</p>
----------------------------------------------------------------------
In diva2:1081973 
abstract is: 
<p>The ADCS concept in MIST reects the limitations of the CubeSat in terms of space, power and onboard computer computational capability. The control is constrained to the use of only magnetic torquers and the determination to magnetometers and Sun sensors in spite of the the under-actuation and underdetermination during eclipses. Usually small satellites with a similar ADCS and demanding requirements fail, therefore MIST would be a design reference for this kind of concept in the case it succeeds.</p><p>The objectives of this thesis work are the feasibility assessment of the concept to meet the nominal requirements in MIST and the consideration of alternatives. Firstly, the importance of gravitational stabilization and di erent congurations for the inertial properties are analyzed based on the linear stability regions for nadir pointing spacecraft. Besides, extended stability regions are derived for the case when a momentum wheel is used to consider alternative options for passive stabilization in terms of the inertial properties. Then a controller based on the Asymptotic Periodic Linear Quadratic Regulation (AP LQR) theory, the currently most extended and e ective for pure magnetic control in small satellites, is assessed. Also a Liner Quadratic Regulator design by means of numerical optimization methods, which has not been used in any real mission, is considered and its performances compared with the AP LQR. Regarding attitude determination a Linear Kalman Filter is designed using the AP LQR theory. Finally, a robustness analysis is conducted via Monte Carlo simulations for those control and determination strategies.</p>
mc='underdetermination' c='under determination'

corrected abstract:
<p>The ADCS concept in MIST reflects the limitations of the CubeSat in terms of space, power and onboard computer computational capability. The control is constrained to the use of only magnetic torquers and the determination to magnetometers and Sun sensors in spite of the the under-actuation and under-determination during eclipses. Usually small satellites with a similar ADCS and demanding requirements fail, therefore MIST would be a design reference for this kind of concept in the case it succeeds.</p><p>The objectives of this thesis work are the feasibility assessment of the concept to meet the nominal requirements in MIST and the consideration of alternatives. Firstly, the importance of gravitational stabilization and different configurations for the inertial properties are analyzed based on the linear stability regions for nadir pointing spacecraft. Besides, extended stability regions are derived for the case when a momentum wheel is used to consider alternative options for passive stabilization in terms of the inertial properties. Then a controller based on the Asymptotic Periodic Linear Quadratic Regulation (AP LQR) theory, the currently most extended and effective for pure magnetic control in small satellites, is assessed. Also a Liner Quadratic Regulator design by means of numerical optimization methods, which has not been used in any real mission, is considered and its performances compared with the AP LQR. Regarding attitude determination a Linear Kalman Filter is designed using the AP LQR theory. Finally, a robustness analysis is conducted via Monte Carlo simulations for those control and determination strategies.</p>
----------------------------------------------------------------------
In diva2:1220116   - correct as is

Note - spelling error in orginal:
w='electroencefalography' val={'c': 'electroencephalography', 's': 'diva2:1220116', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1356998   - correct as is
----------------------------------------------------------------------
In diva2:815687 
abstract is: 
<p>The accuracy of depth measurements in shallow parts of the Stockholm archipelago nautical charts is relatively low at present. In order to improve it, an autonomous depth measuring craft equipped with three downward facing sonar sensors is being developed at KTH.</p><p>In this report, a navigation algorithm that does not require any prior knowledge of the target environment is proposed to be implemented in the craft. It consists of a predifined sweeping pattern, a contour following that only relies on sensor data, and a way of identifying and scanning missed partitions. Performance and the impact of sensor inaccuracies in addition to environmental factors, such as wind and waves, are investigated through simulations.</p><p>Results from simulations show that the proposed navigation algorithm is able to complete scans of complex environments. However, the uncertainty in sensor readings is proven to have a large inuence on performance.</p><p>Finally, further improvements to the algorithm are proposed and the realism of the simulation is discussed.</p>

corrected abstract:
<p>The accuracy of depth measurements in shallow parts of the Stockholm archipelago nautical charts is relatively low at present. In order to improve it, an autonomous depth measuring craft equipped with three downward facing sonar sensors is being developed at KTH.</p><p>In this report, a navigation algorithm that does not require any prior knowledge of the target environment is proposed to be implemented in the craft. It consists of a predefined sweeping pattern, a contour following that only relies on sensor data, and a way of identifying and scanning missed partitions. Performance and the impact of sensor inaccuracies in addition to environmental factors, such as wind and waves, are investigated through simulations.</p><p>Results from simulations show that the proposed navigation algorithm is able to complete scans of complex environments. However, the uncertainty in sensor readings is proven to have a large influence on performance.</p><p>Finally, further improvements to the algorithm are proposed and the realism of the simulation is discussed.</p>
----------------------------------------------------------------------
In diva2:760081 
abstract is: 
<p>The interest in high-energy astrophysics and to venture beyond the standard model has driven theoretical and experimental physicists around the world to collaborate and buildthe necessary instrumentation to test out all the different theories in particle physics. Energies up to 14 <em>TeV</em> have been thoroughly investigated so far by colliding protons in the Large Hadron Collider (LHC). Cosmic ray maps showed us the existence of even higher energies coming from outer space. What is the nature of such particles, and why do they accelerate to as much as 10<sup>20</sup> <em>eV</em>? The study of Ultra High-Energy Cosmic Rays (UHECR) is a new and active research area where physicists hope to find the sources of such high-energy particles and confirm already existing theories.</p><p>This thesis discusses the detections of UHECR, and more specifically the space approach. The ongoing project JEM-EUSO is planned to be launched in 2018 by the Japanese heavy liftrocket H2B, and conveyed to the International Space Station (ISS), where it is thought to gather more data than what already have been obtained all the previous years from the earth’s surface.</p><p>The detection of UHECR occurs through the "atmospheric detector", where an incoming particle makes a shower of secondary particles in the atmosphere called Extensive Air Shower (EAS) upon contact. This shower of particles yields both flourenscence light and Cherenkov light. The problem at hand is to figure out the amount and distribution of the photons created, and how many of these photons actually will reach the detectors of the JEM-EUSO up on the ISS.</p>

corrected abstract:
<p>The interest in high-energy astrophysics and to venture beyond the standard model has driven theoretical and experimental physicists around the world to collaborate and build the necessary instrumentation to test out all the different theories in particle physics. Energies up to 14 <em>TeV</em> have been thoroughly investigated so far by colliding protons in the Large Hadron Collider (LHC). Cosmic ray maps showed us the existence of even higher energies coming from outer space. What is the nature of such particles, and why do they accelerate to as much as 10<sup>20</sup> <em>eV</em>? The study of Ultra High-Energy Cosmic Rays (UHECR) is a new and active research area where physicists hope to find the sources of such high-energy particles and confirm already existing theories.</p><p>This thesis discusses the detections of UHECR, and more specifically the space approach. The ongoing project JEM-EUSO is planned to be launched in 2018 by the Japanese heavy liftrocket H2B, and conveyed to the International Space Station (ISS), where it is thought to gather more data than what already have been obtained all the previous years from the earth’s surface.</p><p>The detection of UHECR occurs through the "atmospheric detector", where an incoming particle makes a shower of secondary particles in the atmosphere called Extensive Air Shower (EAS) upon contact. This shower of particles yields both flourenscence light and Cherenkov light. The problem at hand is to figure out the amount and distribution of the photons created, and how many of these photons actually will reach the detectors of the JEM-EUSO up on the ISS.</p>

Note spelling error in original:
w='flourenscence' val={'c': 'fluorescence', 's': 'diva2:760081', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1859981 
abstract is: 
<p>Radars operating at sea encounter a common phenomenon known as sea clutter, characterized by undesired reflections originating from the sea surface. This phenomenon can significantly impair the radar’s capacity to detect small, slow-moving targets. Therefore, it is crucial to gain a comprehensive understanding of the statistical attributes that describes the sea clutter. This comprehension is pivotal for the development of efficient signal processing strategies.</p><p>The core of this work revolves around the imperative requirement for accurate statistical models to characterize sea clutter. Within this context, this work particularly explores the application of Field’s model. Field’s model describes the sea clutter process using three stochastic differential equations that form the dynamical process of the complex reflectivity of the sea surface. One equation describes the radar cross section, which is given by a Cox-Ingersoll-Ross process, parameterized by the parameters A and α. The other equations describe the speckle process, which is a complex Ornstein-Uhlenbeck process parameterized by B. The aim of this thesis is to explore the possibilities in estimating the parameters A, α and B in Field’s model through the application of Bayesian inference.</p><p>To achieve this objective, Metropolis-Hastings and Sequential Monte Carlo methods are employed. The clutter data, represented by the complex reflectivity, is synthetically generated by using the Euler-Maruyma and Milstein schemes. Three algorithms are designed for estimating the sea clutter parameters. Two algorithms require 300 seconds of data and are based on the approach suggested by Clement Roussel in his PhD thesis [1]. Specifically, these algorithms employ the Metropolis-Hastings method for estimating A, α and B, respectively. As input data to the algorithms, estimators of the Cox-Ingersoll-Ross process and the real part of the Ornstein-Uhlenbeck process are utilized. In contrast, the last algorithm describes an approach that employs only 3 seconds of data. This algorithm is a Metropolis-Hastings method that incorporates a particle filter for approximation of likelihoods.</p><p>For evaluation of the algorithms, two distinct sets of parameters are considered, leading to varying characteristics of the complex reflectivity. The two algorithms that require 300 seconds of data are ex- ecuted ten times for each parameter set. Evidently, the algorithm designed for estimating B generates values that closely aligns with the true values while the algorithm designed for estimating A and α does not yield as satisfactory results. Due to time constraints and the computational demands of the simulations, the last algorithm, requiring 3 seconds of data, is executed only twice for each parameter set. Remarkably, this algorithm generates estimates that agree with the true values, indicating strong performance. Nonetheless, additional simulations are required to conclusively confirm its robustness.</p><p>To conclude, it is possible to estimate sea clutter parameters within Field’s model by using the applied methods of Bayesian inference. However, it is important to analyze the applicability of these methods for a large quantity of diverse clutter data. Moreover, their computational demands pose challenges in real-world applications. Future research should address the need for more computation- ally efficient methods to overcome this challenge.</p>

corrected abstract:
<p>Radars operating at sea encounter a common phenomenon known as sea clutter, characterized by undesired reflections originating from the sea surface. This phenomenon can significantly impair the radar’s capacity to detect small, slow-moving targets. Therefore, it is crucial to gain a comprehensive understanding of the statistical attributes that describes the sea clutter. This comprehension is pivotal for the development of efficient signal processing strategies.</p><p>The core of this work revolves around the imperative requirement for accurate statistical models to characterize sea clutter. Within this context, this work particularly explores the application of Field’s model. Field’s model describes the sea clutter process using three stochastic differential equations that form the dynamical process of the complex reflectivity of the sea surface. One equation describes the radar cross section, which is given by a Cox-Ingersoll-Ross process, parameterized by the parameters 𝒜 and α. The other equations describe the speckle process, which is a complex Ornstein-Uhlenbeck process parameterized by ℬ. The aim of this thesis is to explore the possibilities in estimating the parameters 𝒜, α and ℬ in Field’s model through the application of Bayesian inference.</p><p>To achieve this objective, Metropolis-Hastings and Sequential Monte Carlo methods are employed. The clutter data, represented by the complex reflectivity, is synthetically generated by using the Euler-Maruyma and Milstein schemes. Three algorithms are designed for estimating the sea clutter parameters. Two algorithms require 300 seconds of data and are based on the approach suggested by Clement Roussel in his PhD thesis [1]. Specifically, these algorithms employ the Metropolis-Hastings method for estimating 𝒜, α and ℬ, respectively. As input data to the algorithms, estimators of the Cox-Ingersoll-Ross process and the real part of the Ornstein-Uhlenbeck process are utilized. In contrast, the last algorithm describes an approach that employs only 3 seconds of data. This algorithm is a Metropolis-Hastings method that incorporates a particle filter for approximation of likelihoods.</p><p>For evaluation of the algorithms, two distinct sets of parameters are considered, leading to varying characteristics of the complex reflectivity. The two algorithms that require 300 seconds of data are executed ten times for each parameter set. Evidently, the algorithm designed for estimating ℬ generates values that closely aligns with the true values while the algorithm designed for estimating 𝒜 and α does not yield as satisfactory results. Due to time constraints and the computational demands of the simulations, the last algorithm, requiring 3 seconds of data, is executed only twice for each parameter set. Remarkably, this algorithm generates estimates that agree with the true values, indicating strong performance. Nonetheless, additional simulations are required to conclusively confirm its robustness.</p><p>To conclude, it is possible to estimate sea clutter parameters within Field’s model by using the applied methods of Bayesian inference. However, it is important to analyze the applicability of these methods for a large quantity of diverse clutter data. Moreover, their computational demands pose challenges in real-world applications. Future research should address the need for more computationally efficient methods to overcome this challenge.</p>
----------------------------------------------------------------------
In diva2:516084 
abstract is: 
<p>A user-centered design process is used in this thesis with the goal to evaluate different methods that is used to create a prototype for a new resource management tool for Valtech AB, a Swedish IT consulting company. During the thesis we have tried to sort out how good these methods are for identifying who the user and their needs are and analyzed which design principles that are important for a system with high usability. The company is at the moment using a resource management tool that does not fulfill the needs of either the company or the end-user. Methods from human-computer interaction, user-centered design process and agile software development is used in this thesis to evaluate how these methods affect our view of the users’ needs with a resource management tool. We have also studied how these methods themselves and different parts of the design of the tool affect its usability.The result after the different surveys has shown that the needs of thea resource management device have changed since it was implemented. The size of the company is one factor that affects how the manning process is done. As the company continues to grow the user needs better support from the resource management tool for it to be useful in the company’s processes. The users prioritize that the new resource management tool is simple to use, has a simple design, is fast to use and that it gives a good overview. There for it is important that the new tool doesn’t become too complicated.The thesis’s main focus is on the different methods that we have been using, their result and comparisons between these. The content in this report is there for mostly theoretical.</p>

corrected abstract:
<p>A user-centered design process is used in this thesis with the goal to evaluate different methods that is used to create a prototype for a new resource management tool for Valtech AB, a Swedish IT consulting company. During the thesis we have tried to sort out how good these methods are for identifying who the user and their needs are and analyzed which design principles that are important for a system with high usability. The company is at the moment using a resource management tool that does not fulfill the needs of either the company or the end-user. Methods from human-computer interaction, user-centered design process and agile software development is used in this thesis to evaluate how these methods affect our view of the users’ needs with a resource management tool. We have also studied how these methods themselves and different parts of the design of the tool affect its usability.</p><p>The result after the different surveys has shown that the needs of the <span style="text-decoration: line-through;">a</span> resource management device have changed since it was implemented. The size of the company is one factor that affects how the manning process is done. As the company continues to grow the user needs better support from the resource management tool for it to be useful in the company’s processes. The users prioritize that the new resource management tool is simple to use, has a simple design, is fast to use and that it gives a good overview. There for it is important that the new tool doesn’t become too complicated.</p><p>The thesis’s main focus is on the different methods that we have been using, their result and comparisons between these. The content in this report is there for mostly theoretical.</p>

Note: The marked up difference manscript with corrections is present as the full text. I did not use the red color nor the underline for replacement text, but did add the strikthrough for "a". A similar corection is needed for the Swedish version of the abstract.
----------------------------------------------------------------------
In diva2:643754 
abstract is: 
<p>Bell's theorem is an important milestone in the development of quantum mechanics.</p><p>It draws a line between concepts such as realism and locality on the one hand and</p><p>quantum mechanics on the other. Experimentally veried violations of Bell inequalities</p><p>can yield important information about what assumptions can be made about physical</p><p>reality. Experiments have been conducted for over 40 years, but have left loopholes open</p><p>in their executions. It is of great interest to close these loopholes, and in order to do</p><p>so, further experiments are needed. The objective of this project is to investigate the</p><p>underlying, previous work in the eld and to formulate a proposal for a new experiment.</p><p>To do this, the derivations of the original Bell inequality and the CHSH inequality are</p><p>presented along with previously conducted experiments. Also presented is the EPR</p><p>paradox from which sprung the original Bell inequality.</p>

corrected abstract:
<p>Bell's theorem is an important milestone in the development of quantum mechanics. It draws a line between concepts such as realism and locality on the one hand and quantum mechanics on the other. Experimentally verified violations of Bell inequalities can yield important information about what assumptions can be made about physical reality. Experiments have been conducted for over 40 years, but have left loopholes open in their executions. It is of great interest to close these loopholes, and in order to do so, further experiments are needed. The objective of this project is to investigate the underlying, previous work in the field and to formulate a proposal for a new experiment. To do this, the derivations of the original Bell inequality and the CHSH inequality are presented along with previously conducted experiments. Also presented is the EPR paradox from which sprung the original Bell inequality.</p>
----------------------------------------------------------------------
In diva2:650621 
abstract is: 
<p>A natural question in general relativity is whether there exist singularities, like the Big Bang and</p><p>black holes, in the universe. Albert Einstein did not in the beginning believe that singularities</p><p>in general relativity are generic ([2], [3]). He claimed that the existence of singularities is due</p><p>to symmetry assumptions. The symmetry assumptions are usually spatial isotropy and spatial</p><p>homogeneity. Spatial isotropy means intuitively that, for a xed time, universe looks the same at</p><p>all points and in all spatial directions.</p><p>In the present paper, we will show the following: If we solve Einstein's vacuum equations with a</p><p>certain type of initial data, called the Bianchi type I, the resulting space-time will either be the</p><p>Minkowski space or an anisotropic space-time equipped with a so called Kasner metric. We show</p><p>that, in the anisotropic case, the space-time will contain a certain singularity: the Big Bang.</p><p>We distinguish between two dierent classes of a Kasner metrics; the Flat Kasner metric and</p><p>the Non-at Kasner metric. In the case of a Flat Kasner metric, we show that it is possible to</p><p>isometrically embed the entire space-time into Minkowski space. In the case of the Non-at Kasner</p><p>metric, the space-time is not extendible and the gravity goes to innity approaching the time of</p><p>the Big Bang.</p><p>In addition we show, using any Kasner metric, that the universe expands proportional to the</p><p>time passed since the Big Bang. This happens even though some directions will shrink or not</p><p>change.</p><p>The conclusion is: We have found two natural classes of anisotropic space-times, that include</p><p>a Big Bang and expand. These results supports the idea that singularities are generic, i.e. are not</p><p>due to the assumptions of symmetry of the universe.</p><p>1</p>

corrected abstract:
<p>a natural question in general relativity is whether there exist singularities, like the big bang and black holes, in the universe. albert einstein did not in the beginning believe that singularities in general relativity are generic ([2], [3]). he claimed that the existence of singularities is due to symmetry assumptions. the symmetry assumptions are usually spatial isotropy and spatial homogeneity. spatial isotropy means intuitively that, for a fixed time, universe looks the same at all points and in all spatial directions.</p><p>in the present paper, we will show the following: if we solve einstein's vacuum equations with a certain type of initial data, called the bianchi type i, the resulting space-time will either be the minkowski space or an anisotropic space-time equipped with a so called kasner metric. we show that, in the anisotropic case, the space-time will contain a certain singularity: the big bang.</p><p>we distinguish between two different classes of a kasner metrics; the flat kasner metric and the non-flat kasner metric. in the case of a flat kasner metric, we show that it is possible to isometrically embed the entire space-time into minkowski space. in the case of the non-flat kasner metric, the space-time is not extendible and the gravity goes to infinity approaching the time of the big bang.</p><p>in addition we show, using any kasner metric, that the universe expands proportional to the time passed since the big bang. this happens even though some directions will shrink or not change.</p><p>The conclusion is: We have found two natural classes of anisotropic space-times, that include a Big Bang and expand. These results supports the idea that singularities are generic, i.e. are not due to the assumptions of symmetry of the universe.</p>
----------------------------------------------------------------------
In diva2:854842 
abstract is: 
<p>This thesis is about three combinatorial concepts and their relations:</p><p>One concept is the k-Shi arrangement (also called extended Shi-arrangement), which is the set of all hyperplanes in R^n of the form x_i-x_j=-k+1,-k+2,...,k for 0&lt;i&lt;j&lt;n+1.</p><p>The second concept is a k-parking function, that is a sequence (x_1,x_2,...,x_n) of positive integers that, when rearranged from smallest to largest, satisfies x_i&lt; 2+k(i-1).</p><p>In 1996, Pak and Stanley gave a bijection from the regions of the n-dimensional k-Shi arrangement to the k-parking functions of length n, but they could not describe the inverse.</p><p>Athanasiadis and Linusson found a different bijection in 1999, where they were able to specify explicitly both directions.</p><p>A new approach was given by Beck et al. (2015) who gave a bijection from the 1-parking functions, respectively the regions of the 1-Shi-Arrangement to a subset of the class of mixed graphs (i.e. graphs that could have directed as well as undirected edges) which they called parking graphs.</p><p> </p><p>In this thesis we define k-parking graphs and use them to extend Beck's bijections to k-Shi arrangements and k-parking functions.</p><p>This gives an explicit description of the inverse of the Pak-Stanley bijection.</p>

corrected abstract:
<p>This thesis is about three combinatorial concepts and their relations: One concept is the <em>𝑘-Shi arrangement</em> (also called extended Shi-arrangement), which is the set of all hyperplanes in ℝ<sup>𝑛</sup> of the form 𝑥<sub>𝑖</sub>-𝑥<sub>𝑗</sub>=-𝑘+1,-𝑘+2,...,𝑘 for 1&le;𝑖&lt;𝑗&le;𝑛. The second concept is a <em>𝑘-parking function</em>, that is a sequence (𝑥<sub>1</sub>,𝑥<sub>2</sub>,...,𝑥<sub>𝑛</sub>) &isin; ℤ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>𝑛</sup><sub>&gt;0</sub></span></span> that, when rearranged from smallest to largest, satisfies 𝑥<sub>𝑖</sub>&lt; 1+𝑘(𝑖-1). In 1996, Pak and Stanley gave a bijection from the regions of the 𝑛-dimensional 𝑘-Shi arrangement to the 𝑘-parking functions of length 𝑛, but they could not describe the inverse. Athanasiadis and Linusson found a different bijection in 1999, where they were able to specify explicitly both directions. A new approach was given by Beck et al. (2015) who gave a bijection from the 1-parking functions, respectively the regions of the 1-Shi-Arrangement to a subset of the class of mixed graphs (i.e. graphs that could have directed as well as undirected edges) which they called <em>parking graphs</em>.</p><p>In this thesis we define 𝑘-parking graphs and use them to extend Beck's bijections to 𝑘-Shi arrangements and 𝑘-parking functions. This gives an explicit description of the inverse of the Pak-Stanley bijection.</p>
----------------------------------------------------------------------
In diva2:814324 
abstract is: 
<p>Physical activity has always been part of human nature. The human anatomy minimizes energy consumption and enables endurance and strength. Our bodiesare constructed for a life in motion, optimized for our survival. In recent times, the art of survival depends less on our physical ability to move. A more sedentary lifestyle has become the result of the changes made in society over the past decades and the amount of natural physical activity has become less. To compensate for the absence of activity, forced physical activity, training, has evolved in popularity. With a growing resistance training and fitness trend, focus lies on forcing our bodies into work. But, when attempting to replace natural exercise with forced movement and added weights, technique could suddenly become an important factor. This study was aimed to focus on the mechanical aspects of training, looking at squats and lunges to compare how dierent techniques affect the resulting muscle forces.</p><p>By tracking reflective body markers and measuring forces with force platforms, data for the dffierent exercises was obtained. Using software specialized within the musculoskeletal field enabled analysis of the data and computation of the muscle forces. In this study, focus lay on the muscle forces in the quadriceps muscle group and the sartorius muscle.</p><p>The computed forces showed no direct force dierence when letting the knees and feet point in the same direction, regardless of internal or external rotation of the knees in relation to the hips and feet. The largest force dierences occurred in exercises with little stability and when balance was lost.</p>

corrected abstract:
<p>Physical activity has always been part of human nature. The human anatomy minimizes energy consumption and enables endurance and strength. Our bodies are constructed for a life in motion, optimized for our survival. In recent times, the art of survival depends less on our physical ability to move. A more sedentary lifestyle has become the result of the changes made in society over the past decades and the amount of natural physical activity has become less. To compensate for the absence of activity, forced physical activity, training, has evolved in popularity. With a growing resistance training and fitness trend, focus lies on forcing our bodies into work. But, when attempting to replace natural exercise with forced movement and added weights, technique could suddenly become an important factor. This study was aimed to focus on the mechanical aspects of training, looking at squats and lunges to compare how different techniques affect the resulting muscle forces.</p><p>By tracking reflective body markers and measuring forces with force platforms, data for the different exercises was obtained. Using software specialized within the musculoskeletal field enabled analysis of the data and computation of the muscle forces. In this study, focus lay on the muscle forces in the quadriceps muscle group and the sartorius muscle.</p><p>The computed forces showed no direct force difference when letting the knees and feet point in the same direction, regardless of internal or external rotation of the knees in relation to the hips and feet. The largest force differences occurred in exercises with little stability and when balance was lost.</p>
----------------------------------------------------------------------
In diva2:1145332 
abstract is: 
<p>Brake squeal is a major problem for the vehicle industry due to recla-mations and quality perception of vehicles, as well as the unpredictability of its occurrence. This report investigates the physical reasons behind a multi-tonal brake squeal generated in a motorcycle disc brake. It also covers why previous remedies to eliminate the noise has had an e↵ect, as well as trying to ﬁnd a stable cost-e↵ective solution to the problem. The brake components are studied using numerical simulations as well as experimental analysis, the accuracy of which is investigated by analyzing a simple beam and compar-ing the results to analytical calculations. By performing a FEM-simulation of the di↵erent subsystems, the multi-tonal sound was tracked down to the ABS tone wheel, e.a. the sensory ring that is used for wheel speed mea-surements. By simulating the modes with altered tone wheel thickness, the modal pattern remains the same albeit reduced in frequency. A character-istic dimension between the tone wheels mounting points where found, as the frequency where these bending modes between the mounting points oc-curred closely correlated with the frequency content of the measured noise. A thinner tone wheel was then manufactured and ﬁeld tested, these test results shows a reduction in frequency for the multi-tonal sound, thus the indications from the simulation is conﬁrmed. Furthermore, an investigation in to brake disc properties is carried out in order to explain the reason why discs with altered geometry lacks the squealing problems of the original disc. This is done by a simulation of the sound propagation eﬃciency of the discs out-of-plane surfaces as well as an experimental study of the insertion loss from the friction surface between the brake disc and the brake pads to the tone wheel. The report ends with suggestions of design changes that can help combat the current issues, as well as problems of the same nature that might arise in future brake designs</p>

corrected abstract:
<p>Brake squeal is a major problem for the vehicle industry due to reclamations and quality perception of vehicles, as well as the unpredictability of its occurrence. This report investigates the physical reasons behind a multi-tonal brake squeal generated in a motorcycle disc brake. It also covers why previous remedies to eliminate the noise has had an e↵ect, as well as trying to find a stable cost-e↵ective solution to the problem. The brake components are studied using numerical simulations as well as experimental analysis, the accuracy of which is investigated by analyzing a simple beam and comparing the results to analytical calculations. By performing a FEM-simulation of the di↵erent subsystems, the multi-tonal sound was tracked down to the ABS tone wheel, e. a. the sensory ring that is used for wheel speed measurements. By simulating the modes with altered tone wheel thickness, the modal pattern remains the same albeit reduced in frequency. A characteristic dimension between the tone wheels mounting points where found, as the frequency where these bending modes between the mounting points occurred closely correlated with the frequency content of the measured noise. A thinner tone wheel was then manufactured and field tested, these test results shows a reduction in frequency for the multi-tonal sound, thus the indications from the simulation is confirmed. Furthermore, an investigation in to brake disc properties is carried out in order to explain the reason why discs with altered geometry lacks the squealing problems of the original disc. This is done by a simulation of the sound propagation efficiency of the discs out-of-plane surfaces as well as an experimental study of the insertion loss from the friction surface between the brake disc and the brake pads to the tone wheel. The report ends with suggestions of design changes that can help combat the current issues, as well as problems of the same nature that might arise in future brake designs.</p>
----------------------------------------------------------------------
In diva2:1290072 
abstract is: 
<p>This paper focuses on how to improve strategic asset allocation in practice. Strategic asset allocation is perhaps the most fundamental issue in portfolio management and it has been thoroughly discussed in previous research. We take our starting point in the traditional work of Markowitz within portfolio optimization. We provide a new solution of how to perform portfolio optimization in practice, or more specifically how to estimate the covariance matrix, which is needed to perform conventional portfolio optimization. Many researchers within this field have noted that the return distribution of financial assets seems to vary over time, so called regime switching, which makes it dicult to estimate the covariance matrix. We solve this problem by using a Bayesian approach for developing a Markov chain Monte Carlo algorithm that detects break points in the return distribution of financial assets, thus enabling us to improve the estimation of the covariance matrix. We find that there are two break points during the time period studied and that the main difference between the periods are that the volatility was substantially higher for all assets during the period that corresponds to the financial crisis, whereas correlations were less affected. By evaluating the performance of the algorithm we find that the algorithm can increase the Sharpe ratio of a portfolio, thus that our algorithm can improve strategic asset allocation over time.</p>

corrected abstract:
<p>This paper focuses on how to improve strategic asset allocation in practice. Strategic asset allocation is perhaps the most fundamental issue in portfolio management and it has been thoroughly discussed in previous research. We take our starting point in the traditional work of Markowitz within portfolio optimization. We provide a new solution of how to perform portfolio optimization in practice, or more specifically how to estimate the covariance matrix, which is needed to perform conventional portfolio optimization. Many researchers within this field have noted that the return distribution of financial assets seems to vary over time, so called regime switching, which makes it difficult to estimate the covariance matrix. We solve this problem by using a Bayesian approach for developing a Markov chain Monte Carlo algorithm that detects break points in the return distribution of financial assets, thus enabling us to improve the estimation of the covariance matrix. We find that there are two break points during the time period studied and that the main difference between the periods are that the volatility was substantially higher for all assets during the period that corresponds to the financial crisis, whereas correlations were less affected. By evaluating the performance of the algorithm we find that the algorithm can increase the Sharpe ratio of a portfolio, thus that our algorithm can improve strategic asset allocation over time.</p>
----------------------------------------------------------------------
In diva2:693601 
abstract is: 
<p>In vehicle control practice, there are some variables, such as lateral tire force, body slip angle and yaw rate, that cannot or is hard to be measured directly and accurately. Vehicle model, like the bicycle model, offers an alternative way to get them indirectly, however due to the widely existent simplification and inaccuracy of vehicle models, there are always biases and errors in prediction from them. When developing advanced vehicle control functions, it is necessary and significant to know these variables in relatively high precision. Kalman filter offers a choice to estimate these variables accurately with measurable variables and with vehicle model together. In this thesis, estimation models based on Extended Kalman Filter (EKF) and Uncented Kalman Filter (UKF) are built separately to evaluate the lateral tire force, body slip angel and yaw rate of two typical passenger vehicles. Matlab toolbox EKF/UKF developed by Simo Särkkä, et al. is used to implement the estimation models. By comparing their principle, algorithm and results, the better one for vehicle state estimation will be chosen and justified.</p><p>The thesis is organized in the following 4 parts:</p><p>First, EKF and UKF are studied from their theory and features.</p><p>Second, vehicle model used for prediction in Kalman filter is build and justified.</p><p>Third, algorithms of EKF and UKF for this specific case are analysed. EKF and UKF are</p><p>then implemented based on the algorithms with the help of Matlab toolbox EKF/UKF.</p><p>Finally, comparisons between EKF and UKF are presented and discussed.</p>

corrected abstract:
<p>In vehicle control practice, there are some variables, such as lateral tire force, body slip angle and yaw rate, that cannot or is hard to be measured directly and accurately. Vehicle model, like the bicycle model, offers an alternative way to get them indirectly, however due to the widely existent simplification and inaccuracy of vehicle models, there are always biases and errors in prediction from them. When developing advanced vehicle control functions, it is necessary and significant to know these variables in relatively high precision. Kalman filter offers a choice to estimate these variables accurately with measurable variables and with vehicle model together. In this thesis, estimation models based on Extended Kalman Filter (EKF) and Uncented Kalman Filter (UKF) are built separately to evaluate the lateral tire force, body slip angel and yaw rate of two typical passenger vehicles. Matlab toolbox EKF/UKF developed by Simo Särkkä, et al. is used to implement the estimation models. By comparing their principle, algorithm and results, the better one for vehicle state estimation will be chosen and justified.</p><p>The thesis is organized in the following 4 parts:</p><p>First, EKF and UKF are studied from their theory and features.</p><p>Second, vehicle model used for prediction in Kalman filter is build and justified.</p><p>Third, algorithms of EKF and UKF for this specific case are analysed. EKF and UKF are then implemented based on the algorithms with the help of Matlab toolbox EKF/UKF.</p><p>Finally, comparisons between EKF and UKF are presented and discussed.</p>

Note spelling error in the original:
w='Uncented' val={'c': 'Unscented', 's': 'diva2:693601', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1880206 
abstract is: 
<p>This thesis explores the development of a recommendation system for finan- cial instruments, specifically tailored for StockRepublic, a software company based in Stockholm, Sweden. StockRepublic offers a social platform for finan- cial institutions, enabling users to share and follow investment portfolios. The primary aim of this research is to devise a method for recommending new fi- nancial instruments to users based on their existing portfolios. The approach involves constructing a mathematical measure of similarity between instruments and developing an algorithm that leverages this similarity to suggest relevant additions to a user’s portfolio. Experimental results indicate that while the pro- posed similarity measure and algorithm function correctly, the overall similarity between instruments in typical portfolios is relatively low, resulting in poor end to end performance. This highlights the challenge of defining effective similarity metrics in the diverse field of data science.</p>

corrected abstract:
<p>This thesis explores the development of a recommendation system for financial instruments, specifically tailored for StockRepublic, a software company based in Stockholm, Sweden. StockRepublic offers a social platform for financial institutions, enabling users to share and follow investment portfolios. The primary aim of this research is to devise a method for recommending new financial instruments to users based on their existing portfolios. The approach involves constructing a mathematical measure of similarity between instruments and developing an algorithm that leverages this similarity to suggest relevant additions to a user’s portfolio. Experimental results indicate that while the proposed similarity measure and algorithm function correctly, the overall similarity between instruments in typical portfolios is relatively low, resulting in poor end to end performance. This highlights the challenge of defining effective similarity metrics in the diverse field of data science.</p>
----------------------------------------------------------------------
In diva2:1878911 
abstract is: 
<p>To ensure future proof security against quantum computers, quantum key distribution (QKD) is at the forefront of nascent technology. The Quantum Nano Photonics lab at AlbaNova University Center and Ericsson lab in Kista is connected by a telecom fiber, which is used to send entangled photons in different polarization states as part of a QKD setup. In the current setup, two lasers are used as reference when stabilizing the polarization of the light, since linearly polarized light can be measured in two different bases. This project has explored the possibility of reducing the number of lasers from two to one, by using a fast motor that rotates a half-waveplate, which in turn switches the basis of the polarization of the laser.</p><p>A stepper motor has been used for this purpose, driven by a stepper motor driver, a CNC shield, as well as an Arduino UNO microcontroller. To be able to place the motor in an optical setup, a mounting bracket and waveplate adapter was designed and manufactured. A half-waveplate was utilized to change the polarization of the incoming light. When the aforementioned parts had been composed, an optical setup including the motor was constructed, where the motor's speed and repeatability was tested. The speed of both a $360^\circ$ and a $45^\circ$ rotation was tested. To test the repeatability, the motor was rotated $45^\circ$ back and forth $1000$ times, and the resulting angles were measured to see if the motor missed any steps.</p><p>The speed of $45^\circ$ rotations was on average $24.3$ ms, and for $360^\circ$ rotations it was on average $179$ ms. During the $1000$ rotations of $45^\circ,$ the azimuth angle of the state of polarization of the light had not varied with more than 0.33$^\circ$ for one of the positions, and $0.37^\circ$ for the other, which is within the margin of error of the polarimeter.</p><p>It was noticed that the motor sometimes unpredictably takes a few steps when initialized, which is a problem that needs fixing to be able to implement the motor in the QKD setup. Apart from that, the results were satisfactory, with a speed and repeatability that is acceptable for use in the QKD setup. Since the torque of the motor is limiting when it comes to speed, building a waveplate adapter with lower moment of inertia could be for future projects to explore.</p>

corrected abstract:
<p>To ensure future proof security against quantum computers, quantum key distribution (QKD) is at the forefront of nascent technology. The Quantum Nano Photonics lab at AlbaNova University Center and Ericsson lab in Kista is connected by a telecom fiber, which is used to send entangled photons in different polarization states as part of a QKD setup. In the current setup, two lasers are used as reference when stabilizing the polarization of the light, since linearly polarized light can be measured in two different bases. This project has explored the possibility of reducing the number of lasers from two to one, by using a fast motor that rotates a half-waveplate, which in turn switches the basis of the polarization of the laser.</p><p>A stepper motor has been used for this purpose, driven by a stepper motor driver, a CNC shield, as well as an Arduino UNO microcontroller. To be able to place the motor in an optical setup, a mounting bracket and waveplate adapter was designed and manufactured. A half-waveplate was utilized to change the polarization of the incoming light. When the aforementioned parts had been composed, an optical setup including the motor was constructed, where the motor's speed and repeatability was tested. The speed of both a 36𝟢º and a 𝟦𝟧º rotation was tested. To test the repeatability, the motor was rotated 𝟦𝟧º back and forth 𝟣𝟢𝟢𝟢 times, and the resulting angles were measured to see if the motor missed any steps.</p><p>The speed of 𝟦𝟧º rotations was on average 𝟤𝟦.𝟥 ms, and for 𝟥𝟨𝟢º rotations it was on average 𝟣𝟩𝟫 ms. During the 𝟣𝟢𝟢𝟢 rotations of 𝟦𝟧º the azimuth angle of the state of polarization of the light had not varied with more than 0.𝟥𝟥º for one of the positions, and 0.𝟥𝟩º for the other, which is within the margin of error of the polarimeter.</p><p>It was noticed that the motor sometimes unpredictably takes a few steps when initialized, which is a problem that needs fixing to be able to implement the motor in the QKD setup. Apart from that, the results were satisfactory, with a speed and repeatability that is acceptable for use in the QKD setup. Since the torque of the motor is limiting when it comes to speed, building a waveplate adapter with lower moment of inertia could be for future projects to explore.</p>


Note: Since the various digits were within LaTeX math expressions, I've converted all of the digits to Mathematical Sans-Serif Digit.
----------------------------------------------------------------------
In diva2:1571205 
abstract is: 
<p>Understanding biological systems oftentimes requires mapping the behaviors of biomolecules on small scales such as single proteins or nucleic acids. Fluorescence Correlation Spectroscopy (FCS) is a group of particularly noninvasive and sensitive fluorescence-­based techniques that can be used for this purpose. In this project an FCS-­apparatus has been built and consequently used to test the boundaries of a newly developed variant of FCS called FRET­FCS (Förster Resonance Energy Transfer). Unlike regular FCS, this technique employs fluorescence resonance energy transfer (FRET) in addition to fluorescence.</p><p>In practice, testing the boundaries means adding increasing amounts of FRET­ DNA to a mix of red­ respectively green marked DNA and determine the lower limit for which FRET­-DNA can be detected. A functioning FCS-­setup was built. When FCS­ and FCCS (Fluorescence Cross­ Correlation Spectroscopy)­ measurements were conducted, the desired curves for the intensity functions were generated. The result of the sensitivity analysis was that the lower limit of detectable FRET­-DNA was approximately 1.3% of a sample containing FRET­-DNA and single­ marked DNA.</p>

corrected abstract:
<p>Understanding biological systems oftentimes requires mapping the behaviors of biomolecules on small scales such as single proteins or nucleic acids. Fluorescence Correlation Spectroscopy (FCS) is a group of particularly noninvasive and sensitive fluorescence-based techniques that can be used for this purpose. In this project an FCS-apparatus has been built and consequently used to test the boundaries of a newly developed variant of FCS called FRET-FCS (Förster Resonance Energy Transfer). Unlike regular FCS, this technique employs fluorescence resonance energy transfer (FRET) in addition to fluorescence.</p><p>In practice, testing the boundaries means adding increasing amounts of FRET-DNA to a mix of red- respectively green marked DNA and determine the lower limit for which FRET-DNA can be detected. A function ing FCS-setup was built. When FCS- and FCCS (Fluorescence Cross-Correlation Spectroscopy)-measurements were conducted, the desired curves for the intensity functions were generated.</p><p>The result of the sensitivity analysis was that the lower limit of detectable FRET-DNA was approximately 1.3% of a sample containing FRET-DNA and single-marked DNA.</p>
----------------------------------------------------------------------
In diva2:1566308 
abstract is: 
<p>Since the completion of the the Human genome project in 2003, the evident complexity of our genome and its regulation has only grown. The idea that having sequenced the human genome would solve this mystery was quickly discarded. With the decreasing costs of DNA sequencing, a plethora of new methods have evolved to further understand the role of non-coding regions of our genome, which makes up 98% its length. Genetic variations in these regions are therefore abundant in the human population, but their e ects are hard to characterize. Many non-coding variants have been linked to complex diseases such as cancer predisposition. This thesis aims to investigate the potential e ects of non-coding variants on drug toxicity, that is, how severe the adverse e ects of a drug are to the treated patients. More specifically it will study the effects of two cancer drugs, Gemcitabine and Carboplatin, on a set of 96 patients with lung cancer. To do this we use spatial data acquired by the promoter-targeting method HiCap as well as expression data obtained from blood cell lines. Using the variants obtained through whole genome sequencing of the patients, a supervised learning approach was attempted to predict the final toxicity experienced by the patients. The large number of variants present among the comparably few patients resulted in poor accuracy. The conclusion was drawn that the resolution of HiCap is too low compared to the density of variants in the non-coding regions. Additional data, such as transcription factor Chip-Seq data, and transcription factor motifs are needed to locate potentially contributing variants within the interactions.</p>

corrected abstract:
<p>Since the completion of the the Human genome project in 2003, the evident complexity of our genome and its regulation has only grown. The idea that having sequenced the human genome would solve this mystery was quickly discarded. With the decreasing costs of DNA sequencing, a plethora of new methods have evolved to further understand the role of non-coding regions of our genome, which makes up 98% its length. Genetic variations in these regions are therefore abundant in the human population, but their effects are hard to characterize. Many non-coding variants have been linked to complex diseases such as cancer predisposition.</p><p>This thesis aims to investigate the potential effects of non-coding variants on drug toxicity, that is, how severe the adverse effects of a drug are to the treated patients. More specifically it will study the effects of two cancer drugs, Gemcitabine and Carboplatin, on a set of 96 patients with lung cancer. To do this we use spatial data acquired by the promoter-targeting method HiCap as well as expression data obtained from blood cell lines.</p><p>Using the variants obtained through whole genome sequencing of the patients, a supervised learning approach was attempted to predict the final toxicity experienced by the patients. The large number of variants present among the comparably few patients resulted in poor accuracy. The conclusion was drawn that the resolution of HiCap is too low compared to the density of variants in the non-coding regions. Additional data, such as transcription factor Chip-Seq data, and transcription factor motifs are needed to locate potentially contributing variants within the interactions.</p>
----------------------------------------------------------------------
In diva2:1341345 - error in title:
"Bärplan till 505-jolleHydrofoilHydrofoil"
==>
"Bärplan till 505-jolle: Grundläggande framtagning av koncept"

abstract is: 
<p>The aim of this bachelor’s thesis was to study the feasibility of using hydrofoils on a 505 sailing dinghy. This was done by designing a concept solution of a hydrofoil system. The system type chosen to be evaluated was a bi-T-foil system. The basic design criterias used as the framework for this project was firstly that the wing must be able to provide sufficient lift. Secondly the system must be able to be regulated for a range of operating conditions and finally the system must be able to withstand the load specified by operating conditions. To accomplish this a pilot study of previous similar applications of this type of system was made. This study resulted in two different wing profiles chosen to be compared to each other. This comparison was made using mathematical models for lift force and drag. Based on this model the size and take-off speeds for both wing profiles was determined. From the size and takeoff speed the most suitable profile was chosen. This resulted in a NACA-63-412 profile for both rudder and centerboard wing. The size of the centerboard wing was a chord length of 300 mm and a span of 1678 mm. The size of the rudder wing was a chord length of 200 mm and a span of 580 mm. This resulted in a takeoff speed of 6.2 knots. When the wing profile and size was determined a mechanical flight height-regulating system was designed. This was done based on a desired flight height at cruising speed and the rotational torque produced by the wing. This resulted in a controll system able to regulate the flight height within the speed interval 6,2-23 kntos. To determine the durability and strength of the system a basic FE-analysis was made with a pre determined material AL6061-T6, a light and strong aluminium alloy. This showed that the system built from this material would hold for the loads it was to be subjected to with a great safety margin. Although it is not studied in this project weather this type of system is the most suitable type for this application and the fact that this project could have been done with a much moore detailed design process to study the effects within areas overlooked or simplified. It can be concluded from this project that it is feasible to equip a 505-dinghy with this type of hydrofoil system.</p><p> </p>

corrected abstract:
<p>The aim of this bachelor’s thesis was to study the feasibility of using hydrofoils on a 505 sailing dinghy. This was done by designing a concept solution of a hydrofoil system. The system type chosen to be evaluated was a bi-T-foil system. The basic design criterias used as the framework for this project was firstly that the wing must be able to provide sufficient lift. Secondly the system must be able to be regulated for a range of operating conditions and finally the system must be able to withstand the load specified by operating conditions.</p><p>To accomplish this a pilot study of previous similar applications of this type of system was made. This study resulted in two different wing profiles chosen to be compared to each other. This comparison was made using mathematical models for lift force and drag. Based on this model the size and take-off speeds for both wing profiles was determined. From the size and takeoff speed the most suitable profile was chosen. This resulted in a NACA-63-412 profile for both rudder and centerboard wing. The size of the centerboard wing was a chord length of 300 mm and a span of 1678 mm. The size of the rudder wing was a chord length of 200 mm and a span of 580 mm. This resulted in a takeoff speed of 6.2 knots.</p><p>When the wing profile and size was determined a mechanical flight height-regulating system was designed. This was done based on a desired flight height at cruising speed and the rotational torque produced by the wing. This resulted in a controll system able to regulate the flight height within the speed interval 6,2-23 kntos.</p><p>To determine the durability and strength of the system a basic FE-analysis was made with a pre determined material AL6061-T6, a light and strong aluminium alloy. This showed that the system built from this material would hold for the loads it was to be subjected to with a great safety margin.</p><p>Although it is not studied in this project weather this type of system is the most suitable type for this application and the fact that this project could have been done with a much moore detailed design process to study the effects within areas overlooked or simplified. It can be concluded from this project that it is feasible to equip a 505-dinghy with this type of hydrofoil system.</p>

Note misspellings in orignal:
w='controll' val={'c': 'controll', 's': 'diva2:1341345', 'n': 'error in original'}
w='criterias' val={'c': 'criteria', 's': 'diva2:1341345', 'n': 'error in original'}
w='kntos' val={'c': 'kntos', 's': 'diva2:1341345', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:550016 
abstract is: 
<p>Cross country skiing is considered to be one of the most demanding sports in terms of endurance. Therefore the skiers are attractive subjects for physiological and biomechanical research whose interest has increased a lot during the 21 stcentury. The results are used to improve the mechanical knowledge about the body and to improve the capacity and technique for the competitors.</p><p>The aim with this study is to implement a method for mechanical energy calculation in cross country skiing. This is based on data from 15 skiers using the double poling technique, where the potential, rotational and translational energies are calculated.</p><p>The measurements are made in a lab using a treadmill with stepwise increased velocity. The system used is Vicon MX where the skiers wear re- ective markers, whose positions is calculated from data from infra-red light cameras. The positions of the joint centres are calculated used as input data to the program. Joint centres and marker data divide the body into segments where the energies of each segment are calculated and possible to sum up for the whole body.</p><p>The results are examples of obtainable data from the model. It is possible to compare chosen subjects' total mechanical energy but also the energies and segments separately. The results can be used to analyse the dierent techniques to improve the capacity of the competitors.</p>

corrected abstract:
<p>Cross country skiing is considered to be one of the most demanding sports in terms of endurance. Therefore the skiers are attractive subjects for physiological and biomechanical research whose interest has increased a lot during the 21<sup>st</sup> century. The results are used to improve the mechanical knowledge about the body and to improve the capacity and technique for the competitors.</p><p>The aim with this study is to implement a method for mechanical energy calculation in cross country skiing. This is based on data from 15 skiers using the double poling technique, where the potential, rotational and translational energies are calculated.</p><p>The measurements are made in a lab using a treadmill with stepwise increased velocity. The system used is Vicon MX where the skiers wear reflective markers, whose positions is calculated from data from infra-red light cameras. The positions of the joint centres are calculated used as input data to the program. Joint centres and marker data divide the body into segments where the energies of each segment are calculated and possible to sum up for the whole body.</p><p>The results are examples of obtainable data from the model. It is possible to compare chosen subjects' total mechanical energy but also the energies and segments separately. The results can be used to analyse the different techniques to improve the capacity of the competitors.</p>
----------------------------------------------------------------------
In diva2:1319706   - correct as is

Note - speeling error in original:
w='interdepencies' val={'c': 'interdependencies', 's': 'diva2:1319706', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1577999 
abstract is: 
<p>In this study, we investigate the feasibility of Feebly Interacting Massive particles (FIMP) as possible candidates to constitute the observed Dark Matter abundance in the universe. FIMPs are particles that couple very feebly with known particles in the Standard Model (SM). As such, they do not attain thermal equilibrium with the baryon abundant environment in the early universe before Nucleosynthesis. In contrast to a freeze-out mechanism common for Weakly Interacting Massive Particles (WIMP) as DM candidates, FIMPs are produced by the so-called freeze-in mechanism that we will describe in this study. The purpose of this study is to investigate how the Coleman Weinberg (C-W) mechanism affects the FIMP [freeze-in] mechanism. We specifically consider a minimal extension of SM in which an Electroweak Singlet-scalar ($S$) couples only to the Higgs-boson ($H$); This is called the Higgs-portal mechanism. We study the C-W effective potentials for the Higgs and Dark-scalar singlet and their implications on FIMP mechanism.Using these, we focus on the High-temperature production of the DM with just the $HH\mapsto SS$ to compute the reaction rates, comparing Bose-Einstein statistics ($\Gamma_{HH\mapsto S S}^{B-E}$) to Maxwell-Boltzmann statistics $\Gamma_{HH\mapsto S S}^{M-B}$. We employ only $\Gamma_{HH\mapsto S S}^{B-E}$ to compute DM relic abundance ($Y$) at several Dark-scalar masses ($m_S$) as a function of coupling $k$, establishing that Higgs-Dark scalar coupling $k$ $\mapsto$ $k_{DM}$ corresponding to actual DM abundance lies in between ${10}^{-8.7}$ and ${10}^{-8}$, i.e. ${10}^{-8.7}&lt;k_{DM}&lt;="" div=""&gt;</p>

corrected abstract:
<p>In this study, we investigate the feasibility of Feebly Interacting Massive particles (FIMP) as possible candidates to constitute the observed Dark Matter abundance in the universe. FIMPs are particles that couple very feebly with known particles in the Standard Model (SM). As such, they do not attain thermal equilibrium with the baryon abundant environment in the early universe before Nucleosynthesis. In contrast to a freeze-out mechanism common for Weakly Interacting Massive Particles (WIMP) as DM candidates, FIMPs are produced by the so-called freeze-in mechanism that we will describe in this study. The purpose of this study is to investigate how the Coleman Weinberg (C-W) mechanism affects the FIMP [freeze-in] mechanism. We specifically consider a minimal extension of SM in which an Electroweak Singlet-scalar (𝑆) couples only to the Higgs-boson (𝐻); This is called the Higgs-portal mechanism. We study the C-W effective potentials for the Higgs and Dark-scalar singlet and their implications on FIMP mechanism. Using these, we focus on the High-temperature production of the DM with just the 𝐻&RightTeeArrow;S to compute the reaction rates, comparing Bose-Einstein statistics (<em>&Gamma;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>B-E</sup><sub>𝐻𝐻&RightTeeArrow;𝑆𝑆</sub></span></span></em>) to Maxwell-Boltzmann statistics <em>&Gamma;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>M-B</sup><sub>𝐻𝐻&RightTeeArrow;𝑆𝑆</sub></span></span></em>. We employ only <em>&Gamma;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>B-E</sup><sub>𝐻𝐻&RightTeeArrow;𝑆𝑆</sub></span></span></em> to compute DM relic abundance (𝑌) at several Dark-scalar masses (𝑚<sub>𝑆</sub>) as a function of coupling 𝑘, establishing that Higgs-Dark scalar coupling 𝑘 &RightTeeArrow; 𝑘<sub>𝐷𝑀</sub> corresponding to actual DM abundance lies in between 𝟣𝟢<sup>-𝟪.𝟩</sup> and 𝟣𝟢<sup>-𝟪</sup>, i.e. 𝟣𝟢<sup>-𝟪.𝟩</sup>&lt;𝑘<sub>𝐷𝑀</sub>&lt;𝟣𝟢<sup>-𝟪</sup>.  We find that the singlet-scalar candidate for the FIMP DM freezes­in at much higher temperatures in comparison to earlier studies.</p>
----------------------------------------------------------------------
In diva2:1498680 
abstract is: 
<p>The fast pyrolysis plant at RISE – ETC, Piteå produces carbon rich chars in bulk from various sources of biomass as feedstock. These in-house manufactured carbon rich chars were upgraded via pyrolysis as well as chemical activation using KOH to enhance their potential as an electrode material for supercapacitors. Commercial activated charcoal (Merck) was also studied and used as a yardstick for comparing performance of our materials. Investigations using EDX show enrichment in carbon content and very low amounts of impurities in the materials prepared from wood char after specific treatments for upgrading. Two-electrode coin cell apparatus with an aqueous electrolyte was used to determine the electrochemical performance of these materials. Wood char after KOH activation shows a high specific capacitance of ~105 Fg-1 at 2 Ag-1 in galvanostatic charge discharge measurements which outperformed activated charcoal used in this study (~68 Fg-1 at 2 Ag-1). This material was tested in a wide range of conditions (current density ranging from 0.1 Ag-1 to 10 Ag-1) and showed specific capacitance from ~90 Fg-1 (for 10 Ag-1) up to ~118 Fg-1 (for 0.1 Ag-1). Fatigue testing for &gt;20000 cycles showed a remarkably high retention (&gt;96%) of capacitance. Currently, most commercial supercapacitors use activated carbon materials prepared from coconut shells as the active electrode material which are not native to Sweden. In this study, we upgrade wood chars produced at RISE – ETC from biomass sources obtained locally (Sweden and Scandinavia) and demonstrate their applicability as supercapacitor electrode materials.</p><p></p>


corrected abstract:
<p>The fast pyrolysis plant at RISE – ETC, Piteå produces carbon rich chars in bulk from various sources of biomass as feedstock. These in-house manufactured carbon rich chars were upgraded via pyrolysis as well as chemical activation using KOH to enhance their potential as an electrode material for supercapacitors. Commercial activated charcoal (Merck) was also studied and used as a yardstick for comparing performance of our materials. Investigations using EDX show enrichment in carbon content and very low amounts of impurities in the materials prepared from wood char after specific treatments for upgrading. Two-electrode coin cell apparatus with an aqueous electrolyte was used to determine the electrochemical performance of these materials. Wood char after KOH activation shows a high specific capacitance of ~105 Fg<sup>-1</sup> at 2 Ag<sup>-1</sup> in galvanostatic charge discharge measurements which outperformed activated charcoal used in this study (~68 Fg<sup>-1</sup> at 2 Ag<sup>-1</sup>). This material was tested in a wide range of conditions (current density ranging from 0.1 Ag<sup>-1</sup> to 10 Ag<sup>-1</sup>) and showed specific capacitance from ~90 Fg<sup>-1</sup> (for 10 Ag<sup>-1</sup>) up to ~118 Fg<sup>-1</sup> (for 0.1 Ag<sup>-1</sup>). Fatigue testing for &gt;20000 cycles showed a remarkably high retention (&gt;96%) of capacitance. Currently, most commercial supercapacitors use activated carbon materials prepared from coconut shells as the active electrode material which are not native to Sweden. In this study, we upgrade wood chars produced at RISE – ETC from biomass sources obtained locally (Sweden and Scandinavia) and demonstrate their applicability as supercapacitor electrode materials.</p>
----------------------------------------------------------------------
In diva2:1518588   - correct as is

Note spelling error in original:
w='cardinalty' val={'c': 'cardinality', 's': 'diva2:1518588', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:933300 
abstract is: 
<p>We show that functions which are analytic on the open unit disc and fulfil the Hölder condition of order r  there, where r lies in the interval (0,1), are operator Hölder of order r on the set of all linear contractions on a Hilbert space. Further, it is known that analytic Lipschitz functions on the unit disc need not be operator Lipschitz. We show that under a certain additional integral condition, these functions are operator Lipschitz. The two results are shown by tools from operator theory including the Spectral theorem and dilations of contractions.</p><p>We also solve a problem related to theory of dilations which was arisen on a mathematical question- and answer site. More specificaly we show that, for a certain operator-valued polynomial, the von Neumann inequality is false.</p>
w='specificaly' val={'c': 'specifically', 's': 'diva2:933300', 'n': 'missing ligature'}

corrected abstract:
<p>For a positive number α, we denote by Λ<sub>α</sub> the space of all functions 𝑓 : ⅅ →  ℂ which are analytic in the open unit disc ⅅ ⊂ ℂ and fulfil the Hölder condition of order α on ⅅ. For α &isin; (0, 1), we show that if 𝑓 &isin; Λ<sub>α</sub>, then 𝑓 is an operator Hölder function of order α on the set of all linear contractions on a Hilbert space. Further, it is known that if 𝑓 &isin; Λ<sub>1</sub>, i.e that 𝑓 is an analytic Lipschitz function on ⅅ, then 𝑓 need not be operator Lipschitz. We show that, if we add the property<br>
<img style="display: block; margin-left: auto; margin-right: auto;" src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Csup%5Climits_%7Bt%20%5Cin%20%5Cmathbb%7BR%7D%20%7D%20%7B%5Cdisplaystyle%5Ciint%5Climits_%5Cmathbb%7BD%7D%7D%20%5Cfrac%7B1-%7C%20z%7C%5E%7B2%7D%7D%7B%7C%20e%5E%7Bit%7D%20-%20z%7C%5E%7B2%7D%7D%7C%20f%5E%7B%5Cprime%5Cprime%7D%28z%29%7C%20dA%28z%29%20%3C%20%5Cinfty" alt="LaTeX: \sup\limits_{t \in \mathbb{R} } {\displaystyle\iint\limits_\mathbb{D}} \frac{1-\vert z\vert&sup2;}{\vert e^{it} - z\vert&sup2;}\vert f^{\prime\prime}(z)\vert dA(z) &lt; \infty" /><br>
where 𝑧 = x+𝑖𝑦 and dA = 𝑑x𝑑𝑦, then 𝑓 is operator Lipschitz. The two results are shown by tools from operator theory including the Spectral theorem and dilations of contractions.</p><p>We also solve a problem related to theory of dilations which was arisen on a mathematical question- and answer site. We show that if 𝑋, 𝑌, 𝑍 are pairwise commutative operators on a Hilbert space with
<ol><li>||𝑍|| ≤ 1,</li>
<li>for any any z &isin; ℂ with ||z||,</li>
</ol>
<p style="text-align: center;">||𝑋 + 𝑧𝑌 || &le; 1,</p>
<p>then ||𝑋 + 𝑍𝑌 || ≤ 1 is false in general.</p>
----------------------------------------------------------------------
In diva2:891912 
abstract is: 
<p>Extensive research has been done on the aerodynamics of supersonic aircrafts, especially in the military and commercial airplanes’ field. Regarding supersonic business jets (SSBJs), two major problems have been addressed in past investigations: reducing the sonic boom and decreasing the NOx emissions. This report focuses on a di˙erent aspect, the controls and stability of this type of aircrafts. This field has not been addressed thoroughly by the di˙erent companies and universities investigating SSBJs, as most of the existing concepts are preliminary designs that have not been developed extensively. With this report I try to put my two cents in analyzing the longitudinal stability and control surfaces of three di˙erent SSBJs designs.</p>

corrected abstract:
<p>Extensive research has been done on the aerodynamics of supersonic aircrafts, especially in the military and commercial airplanes’ field. Regarding supersonic business jets (SSBJs), two major problems have been addressed in past investigations: reducing the sonic boom and decreasing the NOx emissions. This report focuses on a different aspect, the controls and stability of this type of aircrafts. This field has not been addressed thoroughly by the different companies and universities investigating SSBJs, as most of the existing concepts are preliminary designs that have not been developed extensively. With this report I try to put my two cents in analyzing the longitudinal stability and control surfaces of three different SSBJs designs.</p>
----------------------------------------------------------------------
In diva2:1040684 
abstract is: 
<p>This project is concerned with CFD simulations of jets issued from elliptical nozzles. The investigated jet flow in this project is turbulent flow emanating from microscopic nozzles into a combustion chamber. Jet flows are very common in engineering, medical and environmental applications and are for instance used in fuel injection systems, spray painting and drying. Jet flow devices are also very common in applications such as cutting, hydraulic drilling, cooling and heating. A better understanding of the flow phenomenons in jet flows are required in order to make these devices function and perform in a more efficient way. The performance of diesel engines is strongly affected by the fuel spray, atomization and in turn the mixing process. This depends ultimately on the dimensions and geometry of the nozzle. The purpose of this project was therefore to investigate different elliptical nozzle geometries which also was compared to a conventional circular nozzle. Three dimensional simulations have been performed to investigate flow quantities in the turbulent Reynold’s Averaged Navier Stokes and Large Eddy Simulation models in a single phase flow. Simulation of a two-phase flow with the Large Eddy Simulation model was also performed to investigate the inception and development of cavitation. The Volume of Fluid approach was used to describe the twophase flow and Rayleigh-Plesset equation to solve bubble dynamics.The mathematical models regarding those in single phase flow have been solved in the CFD software ANSYS FLUENT, while those in two-phase flow have been solved in the open source C++ toolbox OpenFOAM 2.0.0.</p>

corrected abstract:
<p>This project is concerned with CFD simulations of jets issued from elliptical nozzles. The investigated jet flow in this project is turbulent flow emanating from microscopic nozzles into a combustion chamber. Jet flows are very common in engineering, medical and environmental applications and are for instance used in fuel injection systems, spray painting and drying. Jet flow devices are also very common in applications such as cutting, hydraulic drilling, cooling and heating. A better understanding of the flow phenomenons in jet flows are required in order to make these devices function and perform in a more efficient way.</p><p>The performance of diesel engines is strongly affected by the fuel spray, atomization and in turn the mixing process. This depends ultimately on the dimensions and geometry of the nozzle. The purpose of this project was therefore to investigate different elliptical nozzle geometries which also was compared to a conventional circular nozzle.</p><p>Three dimensional simulations have been performed to investigate flow quantities in the turbulent Reynold’s Averaged Navier Stokes and Large Eddy Simulation models in a single phase flow. Simulation of a two-phase flow with the Large Eddy Simulation model was also performed to investigate the inception and development of cavitation. The Volume of Fluid approach was used to describe the twophase flow and Rayleigh-Plesset equation to solve bubble dynamics.</p><p>The mathematical models regarding those in single phase flow have been solved in the CFD software ANSYS FLUENT, while those in two-phase flow have been solved in the open source C++ toolbox OpenFOAM 2.0.0.</p>


Note grammar error in original:
w='phenomenons' val={'c': 'phenomena', 's': ['diva2:1879381', 'diva2:1040684', 'diva2:1440770', 'diva2:839851']}
----------------------------------------------------------------------
In diva2:805319 
abstract is: 
<p>The  purpose of this computational  fluid dynamics investigation  was to consider a method for boundary  layer control using air injection  on a  modern  transonic airfoil  similar  to those found on most commercial airliners.   Common  industry  software titles  such as Pointwise  for mesh generation,  and ANSYS Fluent for the CFD computation  were implemented.   Coordinates  for  an experimental  transonic airfoil  which  had been used previously in wind tunnel  testing were acquired and used in this analysis. A full alpha sweep, from 0 degrees to stall, was performed on the standard three-element configuration at wind tunnel testing conditions.   ISA  conditions at  sea level  was assumed for  all  simulations. These simulations mirrored  previously published wind tunnel results. A series simulations  were then  performed  on the  standard  configuration with  retracted  leading edge slat and four different  injection  points.  It was  found that  although  the  flight  performance  could be matched  at lower angles of attack,  the <em>CLmax  </em>and lift  at the stall angle of the standard configuration could not be achieved.</p><p> </p>
w='CLmax' val={'c': 'C<sub>Lmax</sub>', 's': 'diva2:805319', 'n': 'correct in original'}

corrected abstract:
<p>The purpose of this computational fluid dynamics investigation was to consider a method for boundary layer control using air injection on a modern transonic airfoil similar to those found on most commercial airliners. Common industry software titles such as Pointwise for mesh generation, and ANSYS Fluent for the CFD computation were implemented. Coordinates for an experimental transonic airfoil which had been used previously in wind tunnel testing were acquired and used in this analysis. A full alpha sweep, from 0 degrees to stall, was performed on the standard three-element configuration at wind tunnel testing conditions. ISA conditions at sea level was assumed for all simulations. These simulations mirrored previously published wind tunnel results. A series simulations were then performed on the standard configuration with retracted leading edge slat and four different injection points. It was found that although the flight performance could be matched at lower angles of attack, the <em>C<sub>Lmax</sub></em> and lift at the stall angle of the standard configuration could not be achieved.</p>
----------------------------------------------------------------------
In diva2:1458913 
abstract is: 
<p>This report is the result of a joint collaboration between the two Swedish companies AFRY and Scania AB. The need of CFD analyses for internal flow purposes are continuously increasing in companies like Scania. However the use of CFD software is limited to license costs. The objective and idea of this thesis is to esablish whether OpenFOAM, an open-source CFD software, could be a suitable alternative to the currently used CFD commercial software StarCCM+ when performing total pressure drops analysis on air intake systems. Furthermore, a second objective is to develop a standard method to analyze this class of systems using OpenFOAM. Air intake systems play a significant role in the efficiency of the engine of a bus or a truck, being responsible to drive clean fresh air to the engine with the minimum amount of pressure drops.</p><p>After an illustrative introduction of the topic and theory principles, the following technical analysis focuses first on a CFD study of the simple Buice and Eaton 2D diffuser, to primarily benchmark OpenFOAM, StarCCM+ and the experimental data in a turbulence models and a mesh independence analyses. For both software the turbulence model RNG <em>k − ε </em>fails almost entirely to detect the separation and wide re-circulation bubble formed in the diffuser, while <em>k − </em><em>ω </em>SST, being not much more computationally expensive, gives very accurate results. For these reasons, even the velocity profiles, pressure coefficient and skin friction coefficient plots analyzed with RNG <em>k − ε</em> turbulence model show larger discrepancy compared to the experimental data. <em> </em></p><p>Secondly, a total pressure drop CFD study of the Scania air intake system follows. Performing multiple simulations for a range of different mass flow rates, both OpenFOAM and StarCCM+ show very good agreement with the experimental data. This time, due to the lack of big separation zones in the system, <em>k − </em><em>ω </em>SST and RNG <em>k − ε </em>perform quite similarly, the therefore <em>k − ε </em>relative inexpensiveness in this more complex and heavy system makes the latter the most convenient turbulence model for this kind of study.</p><p>Looking at different parameters such as user-friendliness, time required, software costs and versatility, OpenFOAM proves itself to be a convenient, accurate and valid alternative to its expensive commercial counterpart. Finally, following these conclusions a user-guide method to analyze not only the single but the entire class of air intake systems for Scania internal purposes is developed and reported.</p>


corrected abstract:
<p>This report is the result of a joint collaboration between the two Swedish companies AFRY and Scania AB. The need of CFD analyses for internal flow purposes are continuously increasing in companies like Scania. However the use of CFD software is limited to license costs. The objective and idea of this thesis is to establish whether OpenFOAM, an open-source CFD software, could be a suitable alternative to the currently used CFD commercial software StarCCM+ when performing total pressure drops analysis on air intake systems. Furthermore, a second objective is to develop a standard method to analyze this class of systems using OpenFOAM. Air intake systems play a significant role in the efficiency of the engine of a bus or a truck, being responsible to drive clean fresh air to the engine with the minimum amount of pressure drops.</p><p>After an illustrative introduction of the topic and theory principles, the following technical analysis focuses first on a CFD study of the simple Buice and Eaton 2D diffuser, to primarily benchmark OpenFOAM, StarCCM+ and the experimental data in a turbulence models and a mesh independence analyses. For both software the turbulence model RNG <em>k − ε</em> fails almost entirely to detect the separation and wide re-circulation bubble formed in the diffuser, while <em>k − ω</em> SST, being not much more computationally expensive, gives very accurate results. For these reasons, even the velocity profiles, pressure coefficient and skin friction coefficient plots analyzed with RNG <em>k − ε</em> turbulence model show larger discrepancy compared to the experimental data.</p><p>Secondly, a total pressure drop CFD study of the Scania air intake system follows. Performing multiple simulations for a range of different mass flow rates, both OpenFOAM and StarCCM+ show very good agreement with the experimental data. This time, due to the lack of big separation zones in the system, <em>k − ω</em> SST and RNG <em>k − ε</em> perform quite similarly, and therefore <em>k − ε</em> relative inexpensiveness in this more complex and heavy system makes the latter the most convenient turbulence model for this kind of study.</p><p>Looking at different parameters such as user-friendliness, time required, software costs and versatility, OpenFOAM proves itself to be a convenient, accurate and valid alternative to its expensive commercial counterpart. Finally, following these conclusions a user-guide method to analyze not only the single but the entire class of air intake systems for Scania internal purposes is developed and reported.</p>
----------------------------------------------------------------------
In diva2:1594613 
abstract is: 
<p>Industries (food, beverage, petrochemical, etc.) normally use various gravitational separation echniques in their processes. Such separation processes often suffer from the deposition of undesirable material on the active surfaces of the process equipment, e.g. a high-speed separator or decanter, causing a slew of problems with the process or product quality. To restore operational efficiencies, additional cleaning steps using both water and chemicals are required, making the process more expensive and less environmentally friendly. Other than operating time and concentration of the process fluid there are several factors such as surface nature, surface roughness, type of material, surface charge, etc which influence the fouling deposition of surfaces. Fouling on the surfaces can grow following different mechanisms. The goal of this research work is to learn more about the nature of foulant interactions with stainless steel surfaces and eventually design some antifouling methodology. It is too difficult to study foulingfor all kinds of solutions and industries, so we tried to investigate the organic deposition in dairy and brewery industries by using lab-scale synthesized milk and beer solutions, For quantitative and statistical examination of these characteristics, several experimental approaches (FTIR, percent weight change, surface roughness, surface energy) were used. It was confirmed that fouling grows on the surfaces in a non-linear fashion irrespective of the time and concentration of the solution. The fouling of surfaces can be improved by producing more hydrophilic surfaces or by reducing surface roughness. Steric hindrance, electrostatic charge, and water barrier or hydration layer theories can be used to modify the surface nature and hence the fouling deposition. For antifouling purposes, PMMA (organic) and tungsten oxide (inorganic) coatings were employed. The PMMA was deposited using a dip-coating technique using (6%,10%, and 12%) PMMA solution, and the tungsten oxide coating was carried out by using a standard two electrode electrochemical system under different voltage (3.5V and 4.5V) and time (5min, 10 min, and 20 min) conditions. The coatings were characterized by using different techniques and their antifouling effects were studied in model milk and model beer solutions</p>

corrected abstract:
<p>Industries (food, beverage, petrochemical, etc.) normally use various gravitational separation techniques in their processes. Such separation processes often suffer from the deposition of undesirable material on the active surfaces of the process equipment, e.g. a high-speed separator or decanter, causing a slew of problems with the process or product quality. To restore operational efficiencies, additional cleaning steps using both water and chemicals are required, making the process more expensive and less environmentally friendly. Other than operating time and concentration of the process fluid there are several factors such as surface nature, surface roughness, type of material, surface charge, etc which influence the fouling deposition of surfaces. Fouling on the surfaces can grow following different mechanisms. The goal of this research work is to learn more about the nature of foulant interactions with stainless steel surfaces and eventually design some antifouling methodology. It is too difficult to study fouling for all kinds of solutions and industries, so we tried to investigate the organic deposition in dairy and brewery industries by using lab-scale synthesized milk and beer solutions, For quantitative and statistical examination of these characteristics, several experimental approaches (FTIR, percent weight change, surface roughness, surface energy) were used. It was confirmed that fouling grows on the surfaces in a non-linear fashion irrespective of the time and concentration of the solution. The fouling of surfaces can be improved by producing more hydrophilic surfaces or by reducing surface roughness. Steric hindrance, electrostatic charge, and water barrier or hydration layer theories can be used to modify the surface nature and hence the fouling deposition. For antifouling purposes, PMMA (organic) and tungsten oxide (inorganic) coatings were employed. The PMMA was deposited using a dip-coating technique using (6%,10%, and 12%) PMMA solution, and the tungsten oxide coating was carried out by using a standard two-electrode electrochemical system under different voltage (3.5V and 4.5V) and time (5min, 10 min, and 20 min) conditions. The coatings were characterized by using different techniques and their antifouling effects were studied in model milk and model beer solutions</p>

Note the original abstract lacks termina punctuation at the end of the paragraph.
----------------------------------------------------------------------
In diva2:1214238 
abstract is: 
<p>The aim of this thesis is to apply deep learning on medical images in order to build an image recognition algorithm. The medical images used for this purpose are CT scans of lung tissue, which is a three dimensional image of a patients lungs. The purpose is to design an image recognition algorithm that is able to differentiate between tumors and normal tissue in lungs. The algorithm is based on artificial neural networks and therefore the ability of a convolutional neural network (CNN) to predict a tumor is studied. Two different architectures are designed in this thesis, which are a three and six layer CNN. In addition, different hyper-parameters and optimizers are compared in order to find suitable settings.</p><p>This thesis concludes that no significant difference exists between the results of the two architectures. The architecture with three layers is faster to train and therefore 100 trainings with same settings are completed with this architecture in order to get statistics of the trainings. The mean accuracy for the test set is 91:1% and the standard-deviation for the test set is 2:39%. The mean sensitivity is 89:7% and the mean specificity is 92:4%.</p>

corrected abstract:
<p>The aim of this thesis is to apply deep learning on medical images in order to build an image recognition algorithm. The medical images used for this purpose are CT scans of lung tissue, which is a three dimensional image of a patients lungs. The purpose is to design an image recognition algorithm that is able to differentiate between tumors and normal tissue in lungs. The algorithm is based on artificial neural networks and therefore the ability of a convolutional neural network (CNN) to predict a tumor is studied. Two different architectures are designed in this thesis, which are a three and six layer CNN. In addition, different hyper-parameters and optimizers are compared in order to find suitable settings.</p><p>This thesis concludes that no significant difference exists between the results of the two architectures. The architecture with three layers is faster to train and therefore 100 trainings with same settings are completed with this architecture in order to get statistics of the trainings. The mean accuracy for the test set is 91.1% and the standard-deviation for the test set is 2.39%. The mean sensitivity is 89.7% and the mean specificity is 92.4%.</p>
----------------------------------------------------------------------
In diva2:550297 
abstract is: 
<p>Because of increased computing power it is necessary for modern passwords to be very long and complex, this makes them hard to remember. Research show that it might be easier for people to remember visual passwords instead of textual ones. The goal of this project was to find a safe graphical password scheme which does not require any modification on the server side. A proposed solution is called the Abagram which is a system that transforms patterns on a grid into textual passwords. The main idea behind the scheme is to assign each cell in the grid a letter or a symbol. The users select some cells by passing their finger over them. The password becomes the letters of the cells in the order in which they are passed. The thesis consists of a study of the combinatorics of user-selected patterns, a theoretical security analysis of the Abagram, an analysis of a user study constructed for Android smartphones and methods for evaluating the strenght of a given pattern. The Abagram does show promise, an average pattern from the study suggest a password space with entropy of about 68 bits which is comparable with a random 10 digit password. The Abagram might be especially useful when used with a smartphone but there are still some usability and implementation aspects which must be analysed further</p>

corrected abstract:
<p>Because of increased computing power it is necessary for modern passwords to be very long and complex, this makes them hard to remember. Research show that it might be easier for people to remember visual passwords instead of textual ones. The goal of this project was to find a safe graphical password scheme which does not require any modification on the server side. A proposed solution is called the Abagram which is a system that transforms patterns on a grid into textual passwords. The main idea behind the scheme is to assign each cell in the grid a letter or a symbol. The users select some cells by passing their finger over them. The password becomes the letters of the cells in the order in which they are passed. The thesis consists of a study of the combinatorics of user-selected patterns, a theoretical security analysis of the Abagram, an analysis of a user study constructed for Android smartphones and methods for evaluating the strength of a given pattern. The Abagram does show promise, an average pattern from the study suggest a password space with entropy of about 68 bits which is comparable with a random 10 digit password. The Abagram might be especially useful when used with a smartphone but there are still some usability and implementation aspects which must be analysed further.</p>
----------------------------------------------------------------------
In diva2:561774 
abstract is: 
<p>Fragments from explosive device have been and still are a great threat, but has now changed</p><p>into terrorist attacks involving IED (Improvised Explosive Devise) rather than hostile forces.</p><p>This master thesis will show the effects on concrete plates (50 mm depth) impacted by</p><p>different projectiles, which should replicate fragments hitting buildings. The projectiles used</p><p>were the 8 mm sphere, the 6 mm sphere, the 8 mm cylinder (RCC) and the FSP (fragment</p><p>simulating projectile). The thesis was made at request from FOI (Swedish Research Defiance</p><p>Agency) at Grindsjön and it contains both experiments made there and numerical simulations.</p><p>The experiments were conducted at Grindsjön, the projectile were fired in velocities between</p><p>800 m/s and 1600 m/s. The 8 mm and RCC obtains higher penetration depth at lower velocity,</p><p>while the FSP, due to its soft material, will need much higher velocity. Full penetration was</p><p>obtained at 1510 m/s for the 6 mm sphere, 1310 m/s for the 8 mm sphere and 950 m/s for the</p><p>RCC. The simulations were made in LS-DYNA using a meshfree solver (SPH) and the results</p><p>shows that the RCC creates a bigger initial elastic wave, which will make the concrete block</p><p>crack more, but it will also make the projectile lose more kinetic energy resulting in lower</p><p>penetration depth in the concrete. The spherical projectiles have higher penetration depth, but</p><p>it gives smaller elastic waves resulting in less cracking of the concrete. The simulations</p><p>overestimate the penetration depth for the non-flat projectiles while giving good agreements</p><p>for the flat projectiles, also the damage pattern are consistent with the experiments. An actual</p><p>fragment from a grenade was obtained and simulated showing that all of the projectile except</p><p>the RCC shows good agreements and therefore the RCC should not be used in simulating</p><p>fragments.</p>

corrected abstract:
<p>Fragments from explosive device have been and still are a great threat, but has now changed into terrorist attacks involving IED (Improvised Explosive Devise) rather than hostile forces. This master thesis will show the effects on concrete plates (50 mm depth) impacted by different projectiles, which should replicate fragments hitting buildings. The projectiles used were the 8 mm sphere, the 6 mm sphere, the 8 mm cylinder (RCC) and the FSP (fragment simulating projectile). The thesis was made at request from FOI (Swedish Research Defiance Agency) at Grindsjön and it contains both experiments made there and numerical simulations. The experiments were conducted at Grindsjön, the projectile were fired in velocities between 800 m/s and 1600 m/s. The 8 mm and RCC obtains higher penetration depth at lower velocity, while the FSP, due to its soft material, will need much higher velocity. Full penetration was obtained at 1510 m/s for the 6 mm sphere, 1310 m/s for the 8 mm sphere and 950 m/s for the RCC. The simulations were made in LS-DYNA using a meshfree solver (SPH) and the results shows that the RCC creates a bigger initial elastic wave, which will make the concrete block crack more, but it will also make the projectile lose more kinetic energy resulting in lower penetration depth in the concrete. The spherical projectiles have higher penetration depth, but it gives smaller elastic waves resulting in less cracking of the concrete. The simulations overestimate the penetration depth for the non-flat projectiles while giving good agreements for the flat projectiles, also the damage pattern are consistent with the experiments. An actual fragment from a grenade was obtained and simulated showing that all of the projectile except the RCC shows good agreements and therefore the RCC should not be used in simulating fragments.</p>
----------------------------------------------------------------------
In diva2:706783 
abstract is: 
<p>Commonly, when calculating ship responses one uses idealized wave spectra to represent the sea. In the idealized model, the sea is frequently assumed to consist of swell and windwaves, which are usually represented by idealized 1D wave spectra, and the directionality of wind-waves is accounted for by multiplication with a standard spreading function. In operational response predictions these idealized spectra are typically generated by extracted parameters from real directional 2D wave spectra obtained from a weather forecast, i.e. spectra that reflects the sea state conditions for the particular place and time. It is generally not known in a statistical sense how large the errors become when idealized wave spectra are used to represent 2D wave spectra, especially not regarding the directionality. The objective with the study is hence to assess the errors that arise when adopting this simplification.</p><p>The analysis compares three ship types that cover different combinations of hull form, load condition and operational conditions: a 153m RORO ship, a 219 m PCTC and a 240m bulk carrier. Chosen response parameters are roll motion, vertical acceleration and wave added resistance, which were calculated in 12240 sea states, for 10 speeds and 36 courses for each ship. The sea states are forecast 2D spectra from the North Atlantic 25th of September 2012. Transfer functions were generated from the hull geometry and realistic load conditions at speeds 2-20 knots. For each sea state-speed-course combination, responses were calculated for 2D wave spectra and corresponding generalized spectra. The error is taken as the difference in response between results obtained with 2D and idealized spectra, using 2D-results as reference. Several statistical measures were used to represent the errors for one sea state with only one number, and among them the root-mean-square error (RMSE) and the worst possible error (WPE) are regarded most relevant.</p><p>The results show that the relative error decreases with increasing share of wind waves and decreasing share of swell. Multi-directionality of wind waves causes large errors only for small waves, and it is concluded that for higher sea states (for which the wind waves are predominant) the Bretschneider representation with spreading function leads to small relative errors. Absolute errors are considered the only relevant for investigating the effect of the error on seakeeping calculations. In general, the RMS acceleration levels are in the order of percentages of one <em>g </em>for all ships. For the bulker, WPE and RMSE for wave added resistance was found to be 8.3% and 3.8% of the total calm-water hull resistance in general, and almost 50% in worst case. The roll angle bias could reach up to 15. Also, the effect of ship speed was investigated, and it shows that the error increases in general with higher speed. It is concluded that it is necessary to use 2D spectra in order to avoid large errors, and to keep performance predictions correct on average.</p>

corrected abstract:
<p>Commonly, when calculating ship responses one uses idealized wave spectra to represent the sea. In the idealized model, the sea is frequently assumed to consist of swell and wind-waves, which are usually represented by idealized 1D wave spectra, and the directionality of wind-waves is accounted for by multiplication with a standard spreading function. In operational response predictions these idealized spectra are typically generated by extracted parameters from real directional 2D wave spectra obtained from a weather forecast, i.e. spectra that reflects the sea state conditions for the particular place and time. It is generally not known in a statistical sense how large the errors become when idealized wave spectra are used to represent 2D wave spectra, especially not regarding the directionality. The objective with the study is hence to assess the errors that arise when adopting this simplification.</p><p>The analysis compares three ship types that cover different combinations of hull form, load condition and operational conditions: a 153m RORO ship, a 219 m PCTC and a 240m bulk carrier. Chosen response parameters are roll motion, vertical acceleration and wave added resistance, which were calculated in 12240 sea states, for 10 speeds and 36 courses for each ship. The sea states are forecast 2D spectra from the North Atlantic 25th of September 2012. Transfer functions were generated from the hull geometry and realistic load conditions at speeds 2-20 knots. For each sea state-speed-course combination, responses were calculated for 2D wave spectra and corresponding generalized spectra. The error is taken as the difference in response between results obtained with 2D and idealized spectra, using 2D-results as reference. Several statistical measures were used to represent the errors for one sea state with only one number, and among them the root-mean-square error (RMSE) and the worst possible error (WPE) are regarded most relevant.</p><p>The results show that the relative error decreases with increasing share of wind waves and decreasing share of swell. Multi-directionality of wind waves causes large errors only for small waves, and it is concluded that for higher sea states (for which the wind waves are predominant) the Bretschneider representation with spreading function leads to small relative errors. Absolute errors are considered the only relevant for investigating the effect of the error on seakeeping calculations. In general, the RMS acceleration levels are in the order of percentages of one 𝑔 for all ships. For the bulker, WPE and RMSE for wave added resistance was found to be 8.3% and 3.8% of the total calm-water hull resistance in general, and almost 50% in worst case. The roll angle bias could reach up to 15º. Also, the effect of ship speed was investigated, and it shows that the error increases in general with higher speed. It is concluded that it is necessary to use 2D spectra in order to avoid large errors, and to keep performance predictions correct on average.</p>
----------------------------------------------------------------------
In diva2:1673571 
abstract is: 
<p>Supply chain management is an important aspect of economics and logistics for companies and governments alike. As supply chains are required to enable profits and be eﬀicient, while still being able to withstand disruptions, they are a matter of optimization and consideration toachieve a desirable performance. Using a Python-based model, a simulation has been performed on two serial supply chains. The chains represented a global and a domestic supply chain respectively, mainly differing in delivery time and their associated costs. Comparisons were made using four different test scenarios, where delivery times differed and the chains were analyzed in their ability to be both cost-eﬀicient and maintain stable inventory levels.The simulations resulted in the domestic chain being better suited for cases when uncertainties were large and the global chain being more cost eﬀicient when everything runs smoothly. Similar effects can be seen in reality.</p>

corrected abstract:
<p>Supply chain management is an important aspect of economics and logistics for companies and governments alike. As supply chains are required to enable profits and be efficient, while still being able to withstand disruptions, they are a matter of optimization and consideration to achieve a desirable performance.</p><p>Using a Python-based model, a simulation has been performed on two serial supply chains. The chains represented a global and a domestic supply chain respectively, mainly differing in delivery time and their associated costs. Comparisons were made using four different test scenarios, where delivery times differed and the chains were analyzed in their ability to be both cost-efficient and maintain stable inventory levels.</p><p>The simulations resulted in the domestic chain being better suited for cases when uncertainties were large and the global chain being more cost efficient when everything runs smoothly. Similar effects can be seen in reality.</p>
----------------------------------------------------------------------
In diva2:1801976 
abstract is: 
<p>This project deals with the CFD modelling of a free surface flow. The aim is to develop and validate a fast and accurate numerical model for stratified two-phase flows. Volume of Fluid (VOF) multiphase model is employed. The purpose is to use the developed numerical model for the design of an element within a compact nuclear reactor.Unsteady Reynolds Averaged Navier-Stokes (RANS) simulations are conducted. Two free surface test cases are simulated to verify and ensure robustness of the model: a dam break and a vertical cylindrical obstacle set in a channel. From there, an optimization is performed in order to find the best compromise between accuracy and rapidity with the solver. The proper set of parameter models is found by carrying out extensive sensitivity studies and compare the solutions with available measurements.The obtained numerical results show a reasonable good agreement with the experimental data for the dam-break. Significant time savings are achieved thanks to the implemented optimization process while maintaining accuracy. The optimized model is then applied to the second test case and comparisons with experimental measurements are carried out. The same physical behavior of the flow as in experiments is captured with the simulations. The differences found between the simulation data and experiments are partly due to the difficulty to monitor experimentally with a high accuracy the highly non uniform regions within the flow.</p>

corrected abstract:
<p>This project deals with the CFD modelling of a free surface flow. The aim is to develop and validate a fast and accurate numerical model for stratified two-phase flows. Volume of Fluid (VOF) multiphase model is employed. The purpose is to use the developed numerical model for the design of an element within a compact nuclear reactor.</p><p>Unsteady Reynolds Averaged Navier-Stokes (RANS) simulations are conducted. Two free surface test cases are simulated to verify and ensure robustness of the model: a dam break and a vertical cylindrical obstacle set in a channel. From there, an optimization is performed in order to find the best compromise between accuracy and rapidity with the solver. The proper set of parameter models is found by carrying out extensive sensitivity studies and compare the solutions with available measurements.</p><p>The obtained numerical results show a reasonable good agreement with the experimental data for the dam-break. Significant time savings are achieved thanks to the implemented optimization process while maintaining accuracy. The optimized model is then applied to the second test case and comparisons with experimental measurements are carried out. The same physical behavior of the flow as in experiments is captured with the simulations. The differences found between the simulation data and experiments are partly due to the difficulty to monitor experimentally with a high accuracy the highly non uniform regions within the flow.</p>
----------------------------------------------------------------------
In diva2:560213 
abstract is: 
<p>Motility or spontaneous motion of eukaryotic cells, such as white blood cells, has been</p><p>extensively studied in the recent literature. A mechanism has been established based</p><p>on polymerization of actin filaments that pushes the cell wall forwards. However, many</p><p>features of this phenomenon remain incompletely understood and more insights from</p><p>modeling is desirable. We study the problem of understanding the origin and magnitude</p><p>of the velocity achieved by the moving cells, and compare it with existing experimental</p><p>results. We have developed and simulated a simplified model based on the relevant</p><p>features of eukaryotic protrusion, formulating main elements required to describe the</p><p>cellular motility. The main simplification is the isolation of a few actin filaments, whereas</p><p>other similar models have previously been built on more complicated cases of polymer</p><p>ensembles. The strength of the simplified model is that it clarifies the actual e</p><p>ffective</p><p>elements of cellular protrusion. A computer program simulates the growth of an actin</p><p>polymer behind a cellular membrane and delivers the protrusion speed of the eukaryotic.</p><p>We also construct a real time 3D graphical representation of the movement process.</p><p>The results obtained are in reasonable agreement with experimental results for the cell</p><p>velocity. The agreement is actually improved compared to previous studies of more</p><p>complicated models, indicating that our simplified model indeed seems to work very</p><p>well. Moreover, the detailed graphical representation highlights the process in greater</p><p>detail than has previously been achieved.</p>

corrected abstract:
<p>Motility or spontaneous motion of eukaryotic cells, such as white blood cells, has been extensively studied in the recent literature. A mechanism has been established based on polymerization of actin filaments that pushes the cell wall forwards. However, many features of this phenomenon remain incompletely understood and more insights from modeling is desirable. We study the problem of understanding the origin and magnitude of the velocity achieved by the moving cells, and compare it with existing experimental results. We have developed and simulated a simplified model based on the relevant features of eukaryotic protrusion, formulating main elements required to describe the cellular motility. The main simplification is the isolation of a few actin filaments, whereas other similar models have previously been built on more complicated cases of polymer ensembles. The strength of the simplified model is that it clarifies the actual effective elements of cellular protrusion. A computer program simulates the growth of an actin polymer behind a cellular membrane and delivers the protrusion speed of the eukaryotic. We also construct a real time 3D graphical representation of the movement process. The results obtained are in reasonable agreement with experimental results for the cell velocity. The agreement is actually improved compared to previous studies of more complicated models, indicating that our simplified model indeed seems to work very well. Moreover, the detailed graphical representation highlights the process in greater detail than has previously been achieved.</p>
----------------------------------------------------------------------
In diva2:1057203 
abstract is: 
<p>In 2010 about 25% of the Swedish coastal areas had been surveyed with modern bathymetry methods, the rest was surveyed with older methods in the 1800s. By 2015 this portion has increased to 48% and the Swedish Maritime Administration's goal is to have surveyed 75% by 2020. The remaining 25% consists mostly of shallow water with a depth of less than 10 meters. These areas are di-cult to measure with conventional methods; surveying ships equipped with a multi-beam sonar. Of these areas the Swedish Maritime Administration intend to survey 15% by 2022. This is only possible if newer technologies are made available for surveying shallower waters. This thesis is a part of a project developing an autonomous vessel suited for bathymetric surveying in shallow waters. The thesis focuses on the proof of concept as well as the development and testing of a prototype vessel and its systems. The benet of an autonomous vessel like this is that it is supposed to be cheaper and easier to operate than a smaller ship with sonars operated by a crew.</p>

corrected abstract:
<p>In 2010 about 25% of the Swedish coastal areas had been surveyed with modern bathymetry methods, the rest was surveyed with older methods in the 1800s. By 2015 this portion has increased to 48% and the Swedish Maritime Administration's goal is to have surveyed 75% by 2020. The remaining 25% consists mostly of shallow water with a depth of less than 10 meters. These areas are difficult to measure with conventional methods; surveying ships equipped with a multi-beam sonar. Of these areas the Swedish Maritime Administration intend to survey 15% by 2022. This is only possible if newer technologies are made available for surveying shallower waters.</p><p>This thesis is a part of a project developing an autonomous vessel suited for bathymetric surveying in shallow waters. The thesis focuses on the proof of concept as well as the development and testing of a prototype vessel and its systems. The benefit of an autonomous vessel like this is that it is supposed to be cheaper and easier to operate than a smaller ship with sonars operated by a crew.</p>
----------------------------------------------------------------------
In diva2:1127924 - it is likely that the title is missing a space:
"Conception of a tooloptimizing the sizing of anelastomeric oil level retainer"
==>
"Conception of a tool optimizing the sizing of an elastomeric oil level retainer"

Note: no full text in DiVA

abstract is: 
<p>afran Transmission Systems, subsidiary of Safran Group, is the world’s leading supplier of power transmissions for commercial aircrafts with over 100 seats. The products they develop require lubrication in order to ensure their good functioning. Several devices exist to this end such as metallic gaskets which guarantee the tightness of the housing containing the components whereas the aim of an elastomeric retainer is to maintain a sufficient level of oil for the lubrication of a shaft and the associated splines for instance. For such products as those developed by Safran Transmission Systems, tests must of course be performed in order to demonstrate their functioning and lead to their certification and afterwards, their use in aeronautical industry. In August 2015, a phenomenon named fretting corrosion affecting the splines of a shaft was identified. After some research of the root cause, it was found that an elastomeric retainer was not designed efficiently and could not ensure its function, namely maintain a sufficient oil level for the lubrication of the component affected by fretting corrosion. The problem was solved with a new retainer’s design. The use of a tool predicting the behavior of such a retainer could have avoided these problems which justifies the subject of the M.Sc.-thesis project. This paper consequently presents the solutions adopted in order to develop such a tool. For confidentiality reasons, this report will not include any data (concerning dimensions or results amongst others).</p>

corrected abstract:
<p>Safran Transmission Systems, subsidiary of Safran Group, is the world’s leading supplier of power transmissions for commercial aircrafts with over 100 seats. The products they develop require lubrication in order to ensure their good functioning. Several devices exist to this end such as metallic gaskets which guarantee the tightness of the housing containing the components whereas the aim of an elastomeric retainer is to maintain a sufficient level of oil for the lubrication of a shaft and the associated splines for instance. For such products as those developed by Safran Transmission Systems, tests must of course be performed in order to demonstrate their functioning and lead to their certification and afterwards, their use in aeronautical industry. In August 2015, a phenomenon named fretting corrosion affecting the splines of a shaft was identified. After some research of the root cause, it was found that an elastomeric retainer was not designed efficiently and could not ensure its function, namely maintain a sufficient oil level for the lubrication of the component affected by fretting corrosion. The problem was solved with a new retainer’s design. The use of a tool predicting the behavior of such a retainer could have avoided these problems which justifies the subject of the M.Sc. - thesis project. This paper consequently presents the solutions adopted in order to develop such a tool. For confidentiality reasons, this report will not include any data (concerning dimensions or results amongst others).</p>
----------------------------------------------------------------------
In diva2:1130068 
abstract is: 
<p>The mission of this project is to conceptually design a transport aircraft. A typical mission for a transport aircraft is to deliver supplies to countries in need of help. The given requirement is that the aircraft should be able to travel from a place within EU to a place along the equator in Africa, deliver the supplies, and return (to the takeoff location) without refueling. The operational mission will be to provide people in need with supplies such as food, water and tents. The aircraft will be able to carry necessities that will be able to provide 5000 persons during a week. Since a landing runway is not available at the destination, the payload will be airdropped in parachutes.</p><p>First off, the desired requirements are defined, they are either already given or estimated. An analysis of´the mission and the desired performance of the aircraft is made by creating a mission profile. With the help of this a weight estimation is done, most importantly the takeoff weight of the aircraft is estimated. With the takeoff weight known and by the help of the desired performance requirements, a constraint diagram is made. By a constraint analysis the optimal wing loading and thrust-to-weight ratio is found. This makes it possible to choose an appropriate engine and to design the wings so that they are customized for the desired mission. Other parts of the aircraft such as the tail and fuselage are designed, and the center-of gravity of the aircraft is found. Throughout the project, different aerodynamic parameters are changed in order to optimize the aircraft and its performance to make it as adapted as possible to the desired mission.</p>

corrected abstract:
<p>The mission of this project is to conceptually design a transport aircraft. A typical mission for a transport aircraft is to deliver supplies to countries in need of help. The given requirement is that the aircraft should be able to travel from a place within EU to a place along the equator in Africa, deliver the supplies, and return (to the takeoff location) without refueling. The operational mission will be to provide people in need with supplies such as food, water and tents. The aircraft will be able to carry necessities that will be able to provide 5000 persons during a week. Since a landing runway is not available at the destination, the payload will be airdropped in parachutes.</p><p>First off, the desired requirements are defined, they are either already given or estimated. An analysis of the mission and the desired performance of the aircraft is made by creating a mission profile. With the help of this a weight estimation is done, most importantly the takeoff weight of the aircraft is estimated. With the takeoff weight known and by the help of the desired performance requirements, a constraint diagram is made. By a constraint analysis the optimal wing loading and thrust-to-weight ratio is found. This makes it possible to choose an appropriate engine and to design the wings so that they are customized for the desired mission. Other parts of the aircraft such as the tail and fuselage are designed, and the center-of gravity of the aircraft is found. Throughout the project, different aerodynamic parameters are changed in order to optimize the aircraft and its performance to make it as adapted as possible to the desired mission.</p>


Note the only change was:
w='of´the' val={'c': 'of the', 's': 'diva2:1130068', 'n': 'correct in original; error occurs with a linebreak'}
----------------------------------------------------------------------
In diva2:787512 
abstract is: 
<p>This thesis is an initial conceptual study of an unmanned surface vehicle (USV) for the</p><p>Swedish Navy. The purpose is to highlight challenges and possibilities connected to</p><p>unmanning a vessel and to suggest one specific concept.</p><p>Generating one concept meant weighing characteristics of different hull types against one</p><p>another to derive a valuable compromise. The generated concept includes a feature of</p><p>longitudinal bulkheads separating an inner, dry volume from outer ballast volumes. The</p><p>latter fill with seawater at low velocities, causing the hull to partly submerge which in turn</p><p>decreases signature and increases ballistic protection within the semi-submerged speed</p><p>range. The!concept!measures 6.2 by 2.3 meters, weigh, approximately 1800 kg and may</p><p>reach a calm water top speed of 44 knots. Investigating needs and potentials of an</p><p>unmanned vessel within the Swedish Navy resulted in a set of requirements, including the</p><p>possibility to arm the USV. This study investigates feasibility of carrying the SAAB Trackfire</p><p>remote weapon platform aboard.</p><p>Development of the initial USV concept was focused on aspects of naval architecture,</p><p>including the making of a!general arrangement, evaluating materials, a structural design,</p><p>stability and power need analysis. Although unmanned vessels are likely to be subject to</p><p>lessened structural safety factors as opposed to manned, scantling determination is based</p><p>on DNV rules for classification of High Speed, Light Craft and Naval Surface Craft.</p><p> </p>

corrected abstract:
<p>This thesis is an initial conceptual study of an unmanned surface vehicle (USV) for the Swedish Navy. The purpose is to highlight challenges and possibilities connected to unmanning a vessel and to suggest one specific concept.</p><p>Generating one concept meant weighing characteristics of different hull types against one another to derive a valuable compromise. The generated concept includes a feature of longitudinal bulkheads separating an inner, dry volume from outer ballast volumes. The latter fill with seawater at low velocities, causing the hull to partly submerge which in turn decreases signature and increases ballistic protection within the semi-submerged speed range. The concept measures 6.2 by 2.3 meters, weighs approximately 1800 kg and may reach a calm water top speed of 44 knots. Investigating needs and potentials of an unmanned vessel within the Swedish Navy resulted in a set of requirements, including the possibility to arm the USV. This study investigates feasibility of carrying the SAAB Trackfire remote weapon platform aboard.</p><p>Development of the initial USV concept was focused on aspects of naval architecture, including the making of a general arrangement, evaluating materials, a structural design, stability and power need analysis. Although unmanned vessels are likely to be subject to lessened structural safety factors as opposed to manned, scantling determination is based on DNV rules for classification of High Speed, Light Craft and Naval Surface Craft.</p>
----------------------------------------------------------------------
In diva2:1900963 
Note: no full text in DiVA
abstract is: 
<p>The problem of climate change requires action to avert its impacts as much as possible. Among other measures, this has led to regulations promoting emission-free transport in the European Union. The current solution to achieve these regulatory goals is the electrification of the drive trains of all road vehicles, including commercial vehicles. As these vehicle concepts transition from experimental small-scale productions to high-volume series models, a critical analysis of the most suitable and ecient technology for all components of the drive train is essential. This includes electric machines. As Voith Turbo produces drive systems for electric buses and is soon entering the market for electric trucks, a dedicated analysis is necessary for such a high- investment technology roadmap.For the most common current technological solution, the use of rare- earth permanent magnets is necessary. Since these materials lead to high dependencies on a few countries, a technical alternative is crucial to ensure supply chain stability. The technology for an electric machine without rare earths already exists on the market for passenger cars. Within this framework, this project aims to compare these two types of machine topologies and their suitability for the application in commercial vehicles within the scope of Voith Turbo. Therefore, both theoretical and experimental analyses are being conducted. The theoretical comparison focuses on the technical characteristics arising from the machine design with and without permanent magnets and their respective performance properties. In the experimental analysis, a simulation model has been built to run the two machines in a realistic environment, and their eciencies have been investigated. A brief cost comparison has been conducted supplementary.The investigated scenarios have shown that in this simplified environment, without any changes to current drive train solutions (such as gearboxes with multiple gears), the use of the alternative machine without rare earths is technologically and economically suitable for vehicles expected to travel long distances and carry heavy loads, such as trucks or interurban buses. On the other hand, the permanent magnet machine is characterized by higher eciency in applications involving city buses with frequent stops.</p>

corrected abstract:
<p>The problem of climate change requires action to avert its impacts as much as possible. Among other measures, this has led to regulations promoting emission-free transport in the European Union. The current solution to achieve these regulatory goals is the electrification of the drive trains of all road vehicles, including commercial vehicles. As these vehicle concepts transition from experimental small-scale productions to high-volume series models, a critical analysis of the most suitable and ecient technology for all components of the drive train is essential. This includes electric machines. As Voith Turbo produces drive systems for electric buses and is soon entering the market for electric trucks, a dedicated analysis is necessary for such a high- investment technology roadmap.</p><p>For the most common current technological solution, the use of rare- earth permanent magnets is necessary. Since these materials lead to high dependencies on a few countries, a technical alternative is crucial to ensure supply chain stability. The technology for an electric machine without rare earths already exists on the market for passenger cars. Within this framework, this project aims to compare these two types of machine topologies and their suitability for the application in commercial vehicles within the scope of Voith Turbo. Therefore, both theoretical and experimental analyses are being conducted. The theoretical comparison focuses on the technical characteristics arising from the machine design with and without permanent magnets and their respective performance properties. In the experimental analysis, a simulation model has been built to run the two machines in a realistic environment, and their efficiencies have been investigated. A brief cost comparison has been conducted supplementary.</p><p>The investigated scenarios have shown that in this simplified environment, without any changes to current drive train solutions (such as gearboxes with multiple gears), the use of the alternative machine without rare earths is technologically and economically suitable for vehicles expected to travel long distances and carry heavy loads, such as trucks or interurban buses. On the other hand, the permanent magnet machine is characterized by higher efficiency in applications involving city buses with frequent stops.</p>

Note: Inferred paragraph breaks where terminal punction was not followed by a space.
----------------------------------------------------------------------
In diva2:1739380 
abstract is: 
<p>Swarm Control Theory studies how multi-agent systems interact to solve tasks cooperatively. In this thesis it was studied how graph theory and the Dynamic Average Consensus Algorithm can be used to control swarming sunshade satellites near L1 for global temperature control, and if it would be suitable for the purpose.The Consensus Algorithm was chosen due to the desirable traits, aggregation, pattern formation, and high scalability. To limit the amount of data handled by each agent and handle limitations on communications between agents due to large distances, the problem was modeled with a dynamically changing interaction topology, and to limit the speed of the controller a consensus algorithm with input constraints was considered.The stability of the first part of the controller was proven via Lyapunov stability criterion. The performance of the collision avoidance controller was tested through simulations with random initial states which showed that it can avoid collisions while successfully reaching the desired formation. The final acquired control law for the consensus algorithm consists of two terms: one term that ensures the convergence to the desired location of each agent, and one term that ensures no collisions between two agents take place.</p>

corrected abstract:
<p>Swarm Control Theory studies how multi-agent systems interact to solve tasks cooperatively. In this thesis it was studied how graph theory and the Dynamic Average Consensus Algorithm can be used to control swarming sunshade satellites near L<sub>1</sub> for global temperature control, and if it would be suitable for the purpose. The Consensus Algorithm was chosen due to the desirable traits, aggregation, pattern formation, and high scalability. To limit the amount of data handled by each agent and handle limitations on communications between agents due to large distances, the problem was modelled with a dynamically changing interaction topology, and to limit the speed of the controller a consensus algorithm with input constraints was considered. The stability of the first part of the controller was proven via Lyapunov stability criterion. The performance of the collision avoidance controller was tested through simulations with random initial states which showed that it can avoid collisions while successfully reaching the desired formation. The final acquired control law for the consensus algorithm consists of two terms: one term that ensures the convergence to the desired location of each agent, and one term that ensures no collisions between two agents take place.</p>
----------------------------------------------------------------------
In diva2:572052 - no need for period at the end of the title
abstract is: 
<p>An interesting eld of mathematics is the study of swarming and ocking.</p><p>By using graph theory, one can describe a system of agents that transfer</p><p>information between each other. With the help of certain algorithms it</p><p>is possible to update the agent's information in order to reach consensus</p><p>between the agents. If the information relates to the position, the velocity,</p><p>or the acceleration of each agent, a behaviour similar to that of animals</p><p>ocks or insect swarms is observed. Several other applications also exist,</p><p>for example in systems of multiple robots when no central coordination is</p><p>possible or simply not desired.</p><p>In this paper dierent algorithms used to change the agent's information</p><p>state will be studied and researched in order to determine the requirements</p><p>under which the entire set of agents achieve consensus. First the</p><p>case where agents receive information from a non-changing set of agents</p><p>will be studied. Specically a particular algorithm, where each agent's information</p><p>is determined by a linear function depending on the information</p><p>state of all other agents from which information is received, will be considered.</p><p>A requirement for this particular algorithm to reach consensus is</p><p>that every agent both receives information and also sends information to</p><p>every other agent, directly or indirectly through other agents. If all information</p><p>transfers are weighed equally, the consensus achieved will be the</p><p>average of all initial information states. Consensus can also be reached</p><p>under looser conditions where there exists an agent that sends information</p><p>to every other agent, directly or indirectly.</p><p>The changes of the system's behaviour when one uses dierent consensus</p><p>algorithms will be discussed, and computer simulations of these will</p><p>be provided. An interesting case is where the information (often referring</p><p>to location, velocity or acceleration) is received only from agents within a</p><p>given distance and thus the information is received from dierent agents</p><p>at dierent times. This results in nonlinear algorithms and mostly simulations</p><p>and interpretations will be given. An observation is that whether</p><p>consensus is achieved or not depends partially on the initial information</p><p>states of the agents and the maximum distance for information transfer.</p>

corrected abstract:
<p>An interesting field of mathematics is the study of swarming and flocking. By using graph theory, one can describe a system of agents that transfer information between each other. With the help of certain algorithms it is possible to update the agent's information in order to reach consensus between the agents. If the information relates to the position, the velocity, or the acceleration of each agent, a behaviour similar to that of animals flocks or insect swarms is observed. Several other applications also exist, for example in systems of multiple robots when no central coordination is possible or simply not desired.</p><p>In this paper different algorithms used to change the agent's information state will be studied and researched in order to determine the requirements under which the entire set of agents achieve consensus. First the case where agents receive information from a non-changing set of agents will be studied. Specifically a particular algorithm, where each agent's information is determined by a linear function depending on the information state of all other agents from which information is received, will be considered. A requirement for this particular algorithm to reach consensus is that every agent both receives information and also sends information to every other agent, directly or indirectly through other agents. If all information transfers are weighed equally, the consensus achieved will be the average of all initial information states. Consensus can also be reached under looser conditions where there exists an agent that sends information to every other agent, directly or indirectly.</p><p>The changes of the system's behaviour when one uses different consensus algorithms will be discussed, and computer simulations of these will be provided. An interesting case is where the information (often referring to location, velocity or acceleration) is received only from agents within a given distance and thus the information is received from different agents at different times. This results in nonlinear algorithms and mostly simulations and interpretations will be given. An observation is that whether consensus is achieved or not depends partially on the initial information states of the agents and the maximum distance for information transfer.</p>
----------------------------------------------------------------------
In diva2:1683863 
abstract is: 
<p>This work entails studying unfitted finite element discretizations for convection-diffusion equations in domains that evolve in time. In particular, these partial differential equations model the evolution of the concentration of soluble surfactants in bulk-interface domains. The work in this thesis docuses on developing numerical methods which conserve the modeled physical quantities. In this work, we propose cut finite element discretizations based on the Discontinuous Galerkin framework which are both locally and globally conservative. Local conservation is achieved on so-called macro elements, and we investigate macro element partitioning of the mesh for both stationary and time-dependent domains. Additionally, we develop globally conservative methods for time-dependent problems. We analyze the proposed methods by studying the convergence of the L2-error with respect to mesh size, condition numbers of the associated linear system matrices, and the conservation error. In numerical experiments for time-dependent problems, we show that the proposed methods have optimal convergence and that the developed macro element stabilization for time-dependent problems leads to increased accuracy while retaining stable condition numbers. Moreover, the measured conservation errors verify the global conservation of the proposed methods.</p>

corrected abstract:
<p>This work entails studying unfitted finite element discretizations for convection-diffusion equations in domains that evolve in time. In particular, these partial differential equations model the evolution of the concentration of soluble surfactants in bulk-interface domains. The work in this thesis focuses on developing numerical methods which conserve the modeled physical quantities. In this work, we propose cut finite element discretizations based on the Discontinuous Galerkin framework which are both locally and globally conservative. Local conservation is achieved on so-called macro elements, and we investigate macro element partitioning of the mesh for both stationary and time-dependent domains. Additionally, we develop globally conservative methods for time-dependent problems. We analyze the proposed methods by studying the convergence of the 𝐿<sup>2</sup>-error with respect to mesh size, condition numbers of the associated linear system matrices, and the conservation error. In numerical experiments for time-dependent problems, we show that the proposed methods have optimal convergence and that the developed macro element stabilization for time-dependent problems leads to increased accuracy while retaining stable condition numbers. Moreover, the measured conservation errors verify the global conservation of the proposed methods.</p>
----------------------------------------------------------------------
In diva2:1319781 
abstract is: 
<p>This thesis intends to construct and compare multiple Residential Price Property Indices (RPPI) with the aim to express the price development of houses in Stockholm county from January 2013 to September 2018. The index method used is the hedonic time dummy variable method. Different methods of imputation of missing data will be applied and new variables will be derived from the available data in order to develop various regression models. Observations judged as not part of the index's target population will be excluded to improve the quality of the training data. The indices will be computed by fitting the final model with OLS regression (as a benchmark), Huber regression, Tukey regression, Ridge regression as well as least-angle regression. Lastly, the obtained indices will be assessed by analyzing different measures of performance when included in \textit{Booli}'s valuation engine. The main result of this thesis is that a specific regression model is produced and that it is concluded that Huber regression slightly outperforms the other methods.</p><p> </p>
mc='\\textit{Booli}' c='\\textit{ Booli}'

partal corrected: diva2:1319781: <p>This thesis intends to construct and compare multiple Residential Price Property Indices (RPPI) with the aim to express the price development of houses in Stockholm county from January 2013 to September 2018. The index method used is the hedonic time dummy variable method. Different methods of imputation of missing data will be applied and new variables will be derived from the available data in order to develop various regression models. Observations judged as not part of the index's target population will be excluded to improve the quality of the training data. The indices will be computed by fitting the final model with OLS regression (as a benchmark), Huber regression, Tukey regression, Ridge regression as well as least-angle regression. Lastly, the obtained indices will be assessed by analyzing different measures of performance when included in \textit{ Booli}'s valuation engine. The main result of this thesis is that a specific regression model is produced and that it is concluded that Huber regression slightly outperforms the other methods.</p><p> </p>
w='Booli}' val={'c': 'Booli</em>', 's': 'diva2:1319781'}

corrected abstract:
<p>This thesis intends to construct and compare multiple Residential Price Property Indices (RPPI) with the aim to express the price development of houses in Stockholm county from January 2013 to September 2018. The index method used is the hedonic time dummy variable method. Different methods of imputation of missing data will be applied and new variables will be derived from the available data in order to develop various regression models. Observations judged as not part of the index's target population will be excluded to improve the quality of the training data. The indices will be computed by fitting the final model with OLS regression (as a benchmark), Huber regression, Tukey regression, Ridge regression as well as least-angle regression. Lastly, the obtained indices will be assessed by analyzing different measures of performance when included in <em>Booli</em>'s valuation engine. The main result of this thesis is that a specific regression model is produced and that it is concluded that Huber regression slightly outperforms the other methods.</p>
----------------------------------------------------------------------
In diva2:408821 - title is missing a space:
"Construction and testing of a cosmic raydetector for the House of Science"
==>
"Construction and testing of a cosmic ray detector for the House of Science"

abstract is: 
<p> </p><p> </p><p> </p><p>This Master’s Thesis discusses the construction and possible use of a cosmic raymuon detector at the House of Science. A scintillatorbased detector has been developed from the detector used in the Stockholm Educational Air Shower Array (SEASA) project. The SEASA detectors were placed on school roofs inside car ski boxes and were therefore visible to students. The new detector is possible for students to handle by themselves. The new detector consists of three detector plates that are placed on top of each other separated by 33 cm. The school programme developed for students in the upper secondary school focuses on encouraging students to learn how they could work with data analysis and how a scientific model is developed and changed with time. The main method used is developed by Millar et al. (1999) which studies if a laboratory exercise is e</p><p>ffective or not. The task tested in this thesis is shown to be effective on level 1, but can hopefully be effective on level 2 with some changes. It is also shown possible for the students to contribute to the meaning of the task.</p><p> </p><p> </p>

corrected abstract:
<p>This Master’s Thesis discusses the construction and possible use of a cosmic ray muon detector at the House of Science. A scintillatorbased detector has been developed from the detector used in the Stockholm Educational Air Shower Array (SEASA) project. The SEASA detectors were placed on school roofs inside car ski boxes and were therefore visible to students. The new detector is possible for students to handle by themselves. The new detector consists of three detector plates that are placed on top of each other separated by 33 cm.</p><p>The school programme developed for students in the upper secondary school focuses on encouraging students to learn how they could work with data analysis and how a scientific model is developed and changed with time. The main method used is developed by Millar et al. (1999) which studies if a laboratory exercise is effective or not. The task tested in this thesis is shown to be effective on level 1, but can hopefully be effective on level 2 with some changes. It is also shown possible for the students to contribute to the meaning of the task.</p>
----------------------------------------------------------------------
In diva2:814465 
Note: no full text in DiVA
abstract is: 
<p>Construction of an optimal beam is a typical inverse problem. Similar problems arise in many fields of science when we cannot solve the problem from its cause. They are important since they give information about parameters that cannot be observed directly.</p><p>The problem for this project was to find the width of a beam with constant height that minimizes the stored bending energy of a deected beam. With the Lagrange multiplier method, mathematical models of the Euler-Bernoulli beam were set up for the constrained optimization problem. It was solved with two numerical methods, steepest descent method and Newton's method which were implemented in Matlab.</p><p>The results of our calculations with a uniform load show that the optimal beam with rectangular cross section is symmetric, at its widest in the middle and thinnest at the ends.</p>

corrected abstract:
<p>Construction of an optimal beam is a typical inverse problem. Similar problems arise in many fields of science when we cannot solve the problem from its cause. They are important since they give information about parameters that cannot be observed directly.</p><p>The problem for this project was to find the width of a beam with constant height that minimizes the stored bending energy of a deflected beam. With the Lagrange multiplier method, mathematical models of the Euler-Bernoulli beam were set up for the constrained optimization problem. It was solved with two numerical methods, steepest descent method and Newton's method which were implemented in Matlab.</p><p>The results of our calculations with a uniform load show that the optimal beam with rectangular cross section is symmetric, at its widest in the middle and thinnest at the ends.</p>
----------------------------------------------------------------------
In diva2:1183319 
abstract is: 
<p>A vessel operating in the real world has to overcome wind, waves and ocean currents. The result of all the above is a motion of 6 degrees of freedom (DOF). Typically, for the maneuvering phase, the Newton-Euler equations are used to derive the equation of motion of the rigid body and the maneuvering theory to model the external forces and moments acting on a vessel. The main topic in this Master Thesis is to assess the maneuvering behavior of a specific container vessel through a 4DOF model. The purpose behind this study is to investigate the differences between the expected maneuvering behavior of the vessel and the operational one. To accomplish that, raw data from the vessel’s sea trials were used and a time domain simulation model created with the sway-roll yaw movements coupled and surge decoupled. The Son and No moto maneuvering model served as the base for the motion equations. The maneuvering coefficients (MC) were firstly estimated by semi-empirical formulas using the vessel particulars. The model was validated using the Esso Osaka sea trials data. The validation was limited to maneuvering parameters such as advance, tactical diameter, yaw overshoot angle etc. The final model was used on the sea trials data of the container vessel taking into consideration the wind forces through the Blender mann wind model. Moreover, correction factors for swallow water effects were used on the MC in order to provide a better accuracy and also to allow comparison between the operational data and the simulated ones since the sea trials depth could not be considered as deep waters. Finally, a system identification procedure was perfomed in order to investigate the possibility of identifying the exact MC values of a vessel. The results were encouraging. The simulation follows the patterns of the raw data relative accurately. In addition, the swallow water corrections provided enough evidence of the different behavior of the vessel depending on the depth under keel. From the SI side, a list of issues were encountered like parameter drift, multicollinearity and cost function prone to local minimum. A series of different procedures and algorithm proposed to overcome those difficulties and the results were promising.</p>

corrected abstract:
<p>A vessel operating in the real world has to overcome wind, waves and ocean currents. The result of all the above is a motion of 6 degrees of freedom (DOF). Typically, for the maneuvering phase, the Newton-Euler equations are used to derive the equation of motion of the rigid body and the maneuvering theory to model the external forces and moments acting on a vessel.</p><p>The main topic in this Master Thesis is to assess the maneuvering behavior of a specific container vessel through a 4DOF model. The purpose behind this study is to investigate the differences between the expected maneuvering behavior of the vessel and the operational one. To accomplish that, raw data from the vessel’s sea trials were used and a time domain simulation model created with the sway-roll-yaw movements coupled and surge decoupled.</p><p>The Son and Nomoto maneuvering model served as the base for the motion equations. The maneuvering coefficients (MC) were firstly estimated by semi-empirical formulas using the vessel particulars. The model was validated using the Esso Osaka sea trials data. The validation was limited to maneuvering parameters such as advance, tactical diameter, yaw overshoot angle etc. The final model was used on the sea trials data of the container vessel taking into consideration the wind forces through the Blendermann wind model.</p><p>Moreover, correction factors for swallow water effects were used on the MC in order to provide a better accuracy and also to allow comparison between the operational data and the simulated ones since the sea trials depth could not be considered as deep waters.</p><p>Finally, a system identification procedure was perfomed in order to investigate the possibility of identifying the exact MC values of a vessel.</p><p>The results were encouraging. The simulation follows the patterns of the raw data relative accurately. In addition, the swallow water corrections provided enough evidence of the different behavior of the vessel depending on the depth under keel. From the SI side, a list of issues were encountered like parameter drift, multicollinearity and cost function prone to local minimum. A series of different procedures and algorithm proposed to overcome those difficulties and the results were promising.</p>
----------------------------------------------------------------------
In diva2:485806 
abstract is: 
<p>During the irradiation in a fuel assembly of a Pressurized Water Reactor (PWR), the thermal diffusion processes are not the only active mechanisms: the effects of irradiation are significant, involving variations of the diffusion coefficients as well as creation and evolution of point defects and cavities in the material.In this document we present the work we have done on MOGADOR, a numerical fission gas behaviour model for nuclear fuel under irradiation in a PWR focusing on the modelling of the behaviour of irradiation defects, fission gases and as-fabricated pores. This work has been accomplished within the frame of a six-month internship, from September 2006 to March 2007 at the LSC laboratory, located at the Cadarache research centre.The first task was to optimize the model and to improve its convergence in a simplified case, which was a necessary condition to go further with the study of a complete case. Then we started to tackle the physical qualification of MOGADOR, that is to say verify the behaviour of some physical quantities and their dependencies to some parameters. We present also a short review of numerical methods commonly used for solving ordinary differential equations.</p>

corrected abstract:
<p>During the irradiation in a fuel assembly of a Pressurized Water Reactor (PWR), the thermal diffusion processes are not the only active mechanisms: the effects of irradiation are significant, involving variations of the diffusion coefficients as well as creation and evolution of point defects and cavities in the material.</p><p>In this document we present the work we have done on MOGADOR, a numerical fission gas behaviour model for nuclear fuel under irradiation in a PWR focusing on the modelling of the behaviour of irradiation defects, fission gases and as-fabricated pores. This work has been accomplished within the frame of a six-month internship, from September 2006 to March 2007 at the LSC laboratory, located at the Cadarache research centre.</p><p>The first task was to optimize the model and to improve its convergence in a simplified case, which was a necessary condition to go further with the study of a complete case. Then we started to tackle the physical qualification of MOGADOR, that is to say verify the behaviour of some physical quantities and their dependencies to some parameters. We present also a short review of numerical methods commonly used for solving ordinary differential equations.</p>
----------------------------------------------------------------------
In diva2:1719737 
abstract is: 
<p>This project evaluates the opportunity to convert a three-cylinder automobile piston engine (the Tiny Friendly Giant) to an aircraft engine from an environmental and practical point of view. The problem of increased emissions from aviation calls for technical and socioeconomic solutions, which is the reason why this report is written. The main goals are to choose the best fuel for the piston engine in aviation, as well as to study emissions, engine cooling and practical challenges with conversion. The structure resembles a feasibility study where the problem is solved using literature in a trade study, together with emission estimations using The Greenhouse gases, Regulated Emissions, and Energy use in Technologies Model framework and Boeing Fuel Flow Method 2. An estimation for engine cooling is done using a semiemprical method from Lycoming, showing air cooling can be sufficient for the Tiny Friendly Giant in aviation. The results furthermore show that none of the alternative automobile fuels are appropriate for use in aviation and that alternative pathways to jet fuel are more suited for high altitude. The conclusion is thus that the engine should be converted to jet fuel compatibility. To avoid large turning moment fluctuations, two-stroke can be applied. Conversion and use of the engine in aviation is not considered to be feasible because of practical limitations - instead the study concludes designing a new engine from scratch is easier and most likely quicker. The study shows that reducing carbon dioxide emissions also lead to reductions in water and sulfur- and nitrous oxides. However, the same mitigation strategy leads to increase in carbon monoxide and hydrocarbons. In general, the conclusion is that alternative fuels can significantly reduce aircraft emissions.</p>

corrected abstract:
<p>This project evaluates the opportunity to convert a three-cylinder automobile piston engine (the Tiny Friendly Giant) to an aircraft engine from an environmental and practical point of view. The problem of increased emissions from aviation calls for technical and socioeconomic solutions, which is the reason why this report is written. The main goals are to choose the best fuel for the piston engine in aviation, as well as to study emissions, engine cooling and practical challenges with conversion. The structure resembles a feasibility study where the problem is solved using literature in a trade study, together with emission estimations using The Greenhouse gases, Regulated Emissions, and Energy use in Technologies Model framework and Boeing Fuel Flow Method 2. An estimation for engine cooling is done using a semiemprical method from Lycoming, showing air cooling can be sufficient for the Tiny Friendly Giant in aviation. The results furthermore show that none of the alternative automobile fuels are appropriate for use in aviation and that alternative pathways to jet fuel are more suited for high altitude. The conclusion is thus that the engine should be converted to jet fuel compatibility. To avoid large turning moment fluctuations, two-stroke can be applied. Conversion and use of the engine in aviation is not considered to be feasible because of practical limitations - instead the study concludes designing a new engine from scratch is easier and most likely quicker. The study shows that reducing carbon dioxide emissions also lead to reductions in water and sulfur- and nitrous oxides. However, the same mitigation strategy leads to increase in carbon monoxide and hydrocarbons. In general, the conclusion is that alternative fuels can significantly reduce aircraft emissions.</p>
----------------------------------------------------------------------
In diva2:1773163 - missing spaces in title:
"Convolution-compacted visiontransformers forprediction of localwall heat flux atmultiple Prandtlnumbers in turbulentchannel flow"
==>
"Convolution-compacted vision transformers for prediction of local wall heat flux at multiple Prandtl numbers in turbulent channel flow"

abstract is: 
<p>Predicting wall heat flux accurately in wall-bounded turbulent flows is critical for a variety of engineering applications, including thermal management systems and energy-efficient designs. Traditional methods, which rely on expensive numerical simulations, are hampered by increasing complexity and extremly high computation cost. Recent advances in deep neural networks (DNNs), however, offer an effective solution by predicting wall heat flux using non-intrusive measurements derived from off-wall quantities. This study introduces a novel approach, the convolution-compacted vision transformer (ViT), which integrates convolutional neural networks (CNNs) and ViT to predict instantaneous fields of wall heat flux accurately based on off-wall quantities including velocity components at three directions and temperature. Our method is applied to an existing database of wall-bounded turbulent flows obtained from direct numerical simulations (DNS). We first conduct an ablation study to examine the effects of incorporating convolution-based modules into ViT architectures and report on the impact of different modules. Subsequently, we utilize fully-convolutional neural networks (FCNs) with various architectures to identify the distinctions between FCN models and the convolution-compacted ViT. Our optimized ViT model surpasses the FCN models in terms of instantaneous field predictions, learning turbulence statistics, and accurately capturing energy spectra. Finally, we undertake a sensitivity analysis using a gradient map to enhance the understanding of the nonlinear relationship established by DNN models, thus augmenting the interpretability of these models.</p>

corrected abstract:
<p>Predicting wall heat flux accurately in wall-bounded turbulent flows is critical for a variety of engineering applications, including thermal management systems and energy-efficient designs. Traditional methods, which rely on expensive numerical simulations, are hampered by increasing complexity and extremly high computation cost. Recent advances in deep neural networks (DNNs), however, offer an effective solution by predicting wall heat flux using non-intrusive measurements derived from off-wall quantities. This study introduces a novel approach, the convolution-compacted vision transformer (ViT), which integrates convolutional neural networks (CNNs) and ViT to predict instantaneous fields of wall heat flux accurately based on off-wall quantities including velocity components at three directions and temperature. Our method is applied to an existing database of wall-bounded turbulent flows obtained from direct numerical simulations (DNS). We first conduct an ablation study to examine the effects of incorporating convolution-based modules into ViT architectures and report on the impact of different modules. Subsequently, we utilize fully-convolutional neural networks (FCNs) with various architectures to identify the distinctions between FCN models and the convolution-compacted ViT. Our optimized ViT model surpasses the FCN models in terms of instantaneous field predictions, learning turbulence statistics, and accurately capturing energy spectra. Finally, we undertake a sensitivity analysis using a gradient map to enhance the understanding of the nonlinear relationship established by DNN models, thus augmenting the interpretability of these models.</p>


Note spelling error in original:
w='extremly' val={'c': 'extremely', 's': 'diva2:1773163', 'n': 'correct in original'}
----------------------------------------------------------------------
In diva2:1800343 
abstract is: 
<p>In this thesis, we investigate the advantages of using high-dimensional copula modeling to understand the riskiness of portfolio investments and to more realistically estimate future portfolio values. Our approach involves benchmarking some pre-determined fitted copulas to the 0.05-quantile, the Tail Conditional Expectation, and the probability of negative returns for each portfolio. We find that the two R-Vine copula models used in this study provide good estimations of the distribution of portfolio values for the 1-month time frame, the shortest we consider in this thesis, most probably due to their flexibility and ability to represet a diverse array of dependence structures. However, for longer time frames (1 year or more), the Clayton copula appears to be a more suitable model. It aligns more closely with market behaviour due to its capacity of capturing lower tail dependence. In conclusion, we argue that by employing the right copula model, in our case the Clayton copula, we obtain a more realistic view on the distribution of the future portfolio values.</p>

corrected abstract:
<p>In this thesis, we investigate the advantages of using high-dimensional copula modeling to understand the riskiness of portfolio investments and to more realistically estimate future portfolio values. Our approach involves benchmarking some pre-determined fitted copulas to the 0.05-quantile, the Tail Conditional Expectation, and the probability of negative returns for each portfolio. We find that the two R-Vine copula models used in this study provide good estimations of the distribution of portfolio values for the 1-month time frame, the shortest we consider in this thesis, most probably due to their flexibility and ability to represent a diverse array of dependence structures. However, for longer time frames (1 year or more), the Clayton copula appears to be a more suitable model. It aligns more closely with market behavior due to its capacity of capturing lower tail dependence. In conclusion, we argue that by employing the right copula model, in our case the Clayton copula, we obtain a more realistic view on the distribution of the future portfolio values.</p>
----------------------------------------------------------------------
In diva2:1085448 
abstract is: 
<p>In this thesis, literature is reviewed for theory regarding elliptical copulas (Gaussian, Student’s t, and Grouped t) and methods for calibrating parametric copulas to sets of observations. Theory regarding model diagnostics is also summarized in the thesis. Historical data of equity indices and government bond rates from several geo-graphical regions along with U.S. corporate bond indices are used as proxies of the most significant stochastic variables in the investment portfolio of If P&amp;C. These historical observations are transformed into pseudo-uniform observations, pseudo-observations, using parametric and non-parametric univariate models. The parametric models are fitted using both maximum likelihood and least squares of the quantile function. Ellip-tical copulas are then calibrated to the pseudo-observations using the well known methods Inference Function for Margins (IFM) and Semi-Parametric (SP) as well as compositions of these methods and a non-parametric estimator of Kendall’s tau.The goodness-of-fit of the calibrated multivariate models is assessed in aspect of general dependence, tail dependence, mean squared error as well as by using universal measures such as Akaike and Bayesian Informa-tion Criterion, AIC and BIC. The mean squared error is computed both using the empirical joint distribution and the empirical Kendall distribution function. General dependence is measured using the scale-invariant measures Kendall’s tau, Spearman’s rho, and Blomqvist’s beta, while tail dependence is assessed using Krup-skii’s tail-weighted measures of dependence (see [16]). Monte Carlo simulation is used to estimate these mea-sures for copulas where analytical calculation is not feasible.Gaussian copulas scored lower than Student’s t and Grouped t copulas in every test conducted. However, not all test produced conclusive results. Further, the obtained values of the tail-weighted measures of depen-dence imply a systematically lower tail dependence of Gaussian copulas compared to historical observations.</p>

corrected abstract:
<p>In this thesis, literature is reviewed for theory regarding elliptical copulas (Gaussian, Student’s 𝑡, and Grouped 𝑡) and methods for calibrating parametric copulas to sets of observations. Theory regarding model diagnostics is also summarized in the thesis. Historical data of equity indices and government bond rates from several geographical regions along with U.S. corporate bond indices are used as proxies of the most significant stochastic variables in the investment portfolio of If P&amp;C. These historical observations are transformed into pseudo-uniform observations, pseudo-observations, using parametric and non-parametric univariate models. The parametric models are fitted using both maximum likelihood and least squares of the quantile function. Elliptical copulas are then calibrated to the pseudo-observations using the well known methods <em>Inference Function for Margins</em> (IFM) and <em>Semi-Parametric</em> (SP) as well as compositions of these methods and a non-parametric estimator of Kendall’s tau.</p><p>The goodness-of-fit of the calibrated multivariate models is assessed in aspect of general dependence, tail dependence, mean squared error as well as by using universal measures such as Akaike and Bayesian Information Criterion, <em>AIC</em> and <em>BIC</em>. The mean squared error is computed both using the empirical joint distribution and the empirical Kendall distribution function. General dependence is measured using the scale-invariant measures Kendall’s tau, Spearman’s rho, and Blomqvist’s beta, while tail dependence is assessed using Krupskii’s tail-weighted measures of dependence (see [16]). Monte Carlo simulation is used to estimate these measures for copulas where analytical calculation is not feasible.</p><p>Gaussian copulas scored lower than Student’s 𝑡 and Grouped 𝑡 copulas in every test conducted. However, not all test produced conclusive results. Further, the obtained values of the tail-weighted measures of dependence imply a systematically lower tail dependence of Gaussian copulas compared to historical observations.</p>
----------------------------------------------------------------------
In diva2:1238555 
abstract is: 
<p>Is the execution of computer programs really deterministic? Will a perturbation cause the program to break or give an incorrect output?In this project, the goal is to add perturbations to programs in C, and see if we can get the same result as the program executed without said perturbations.The algorithm takes source code and transforms it into XML. The code is transformed in order to identify nodes to perturb. When the nodes are identified we add a perturbation to some node and transform the XML code back to source code. When we have the perturbed code we execute the code and compare it to the code executed without any perturbations.We have, using cPerturb, tested several nodes in five diﬀer- ent programs and identified a subset of these which yields the same result as the unperturbed programs.</p>

corrected abstract:
<p>Is the execution of computer programs really deterministic? Will a perturbation cause the program to break or give an incorrect output?</p><p>In this project, the goal is to add perturbations to programs in C, and see if we can get the same result as the program executed without said perturbations.</p><p>The algorithm takes source code and transforms it into XML. The code is transformed in order to identify nodes to perturb. When the nodes are identified we add a perturbation to some node and transform the XML code back to source code. When we have the perturbed code we execute the code and compare it to the code executed without any perturbations.</p><p>We have, using cPerturb, tested several nodes in five different programs and identified a subset of these which yields the same result as the unperturbed programs.</p>
----------------------------------------------------------------------
In diva2:228384 
abstract is: 
<p>In living cells, calcium ions (Ca2+) play an important role as an intracellular second messenger. It mediates the regulation of cellular processes such as gene expression, initiation of vesicle fusion in synapses, is used in muscle contraction and is believed to play a fundamental role in synaptic plasticity as a molecular substrate for learning. The Ca2+ signals are created by the fact that the concentration of Ca2+ in the cytosol is four orders of magnitude lower than in the extracellular fluid as well as in cytoplasmic compartments such as the endoplasmic reticulum (ER). This enables fast increments in the cytosol concentration, which is regulated back to normal concentration by different mechanisms. In this project, the connection between Ca2+ signals of different cells was analysed using different correlation techniques: cross-correlation of continuous signals and digitalised signals. Therefore a software tool was developed in MATLAB, which takes Ca2+ recordings from time-lapse fluorescence microscopy as input and calculates the pair wise correlation for all cells. The software was tested by using previous data from experiments with embryonic stem cells from mouse (mES) and human (hES) as well as data from recordings done as part of the project. The study shows that the mathematical method of cross-correlation can successfully be applied to quantitative and qualititative analysis of Ca2+ signals. Furthermore, there exist strongly correlated cells in colonies of mES cells and hES cells. We suggest the synchronisation is achieved by physical coupling implicating a decrease of correlation as the distance increases for strong correlations. In addition, the lag used by the cross-correlation function (an effective phase shift) decreases as the correlation coefficient increases and increases as the intercellular distance increases for high correlation coefficients. Interestingly, the number of cells included in small scale clusters of strongly correlated cells is significantly larger for the differentiating mES cells than for the proliferating mÉS cells. In a broader perspective, the developed software might be usd in for instance analysis of cellular electrical activity and shows the relevance of applying methods from the exact sciences to biology.</p>

corrected abstract:
<p>In living cells, calcium ions (Ca<sup>2+</sup>) play an important role as an intracellular second messenger. It mediates the regulation of cellular processes such as gene expression, initiation of vesicle fusion in synapses, is used in muscle contraction and is believed to play a fundamental role in synaptic plasticity as a molecular substrate for learning. The Ca<sup>2+</sup> signals are created by the fact that the concentration of Ca<sup>2+</sup> in the cytosol is four orders of magnitude lower than in the extracellular fluid as well as in cytoplasmic compartments such as the endoplasmic reticulum (ER). This enables fast increments in the cytosol concentration, which is regulated back to normal concentration by different mechanisms. In this project, the connection between Ca<sup>2+</sup> signals of different cells was analysed using different correlation techniques: cross-correlation of continuous signals and digitalised signals. Therefore a software tool was developed in MATLAB, which takes Ca<sup>2+</sup> recordings from time-lapse fluorescence microscopy as input and calculates the pair wise correlation for all cells. The software was tested by using previous data from experiments with embryonic stem cells from mouse (mES) and human (hES) as well as data from recordings done as part of the project. The study shows that the mathematical method of cross-correlation can successfully be applied to quantitative and qualitative analysis of Ca<sup>2+</sup> signals. Furthermore, there exist strongly correlated cells in colonies of mES cells and hES cells. We suggest the synchronisation is achieved by physical coupling implicating a decrease of correlation as the distance increases for strong correlations. In addition, the lag used by the cross-correlation function (an effective phase shift) decreases as the correlation coefficient increases and increases as the intercellular distance increases for high correlation coefficients. Interestingly, the number of cells included in small scale clusters of strongly correlated cells is significantly larger for differentiating mES cells than for proliferating mES cells. In a broader perspective, the developed software might be used in for instance analysis of cellular electrical activity and shows the relevance of applying methods from the exact sciences to biology.</p>
----------------------------------------------------------------------
In diva2:642343 
abstract is: 
<p>Natural killer cells constitute part of the innate immune system, defending</p><p>against cancer tumours and infections. Ongoing research has shown a diering</p><p>e-ciency to kill target cells among individual cells in natural killer cell</p><p>populations, and new tools allow for in-depth studies of large cell numbers over</p><p>an extended period of time.</p><p>In this thesis, the killing e-ciency of natural killer cells is correlated with</p><p>their migration behaviour. Migratory properties are found to be of either of</p><p>two essentially dierent forms, being active or inactive, and killing e-ciency</p><p>is demonstrated to not be strongly related to migration behaviour. Further,</p><p>natural killer cell populations are shown to exhibit additional heterogeneity as</p><p>cells inducing fast death of target cells are shown to dier in migration compared</p><p>to cells inducing slow death. Lastly, cells showing exhaustion in cytotoxicity</p><p>during the assay are demonstrated to also experience migratory exhaustion.</p>

corrected abstract:
<p>Natural killer cells constitute part of the innate immune system, defending against cancer tumours and infections. Ongoing research has shown a differing efficiency to kill target cells among individual cells in natural killer cell populations, and new tools allow for in-depth studies of large cell numbers over an extended period of time.</p><p>In this thesis, the killing efficiency of natural killer cells is correlated with their migration behaviour. Migratory properties are found to be of either of two essentially different forms, being active or inactive, and killing efficiency is demonstrated to not be strongly related to migration behaviour. Further, natural killer cell populations are shown to exhibit additional heterogeneity as cells inducing fast death of target cells are shown to differ in migration compared to cells inducing slow death. Lastly, cells showing exhaustion in cytotoxicity during the assay are demonstrated to also experience migratory exhaustion.</p>
----------------------------------------------------------------------
In diva2:1359787 missing hyphen in title_
"Creation and Validation of Early Stage Conceptual Design Methodology for Blended WingBody Aircraft"
==>
"Creation and Validation of Early Stage Conceptual Design Methodology for Blended Wing-Body Aircraft"

abstract is: 
<p>The current design paradigm for developing tube-and-wing style aircraft has been well documented in literature. This research attempts to develop and val-idate a similar design methodology to what is presently utilized for tube-and-wing based aircraft, but has so far not been successfully implemented for the blended wing-body. This construction has no clear distinction between the lift generating surfaces and the cargo carrying structure. The methodology that was developed included the concatenation and validation of low-fidelity, low speed and low complexity aerodynamic models in order to allow for quick and simple analysis of a large number of possible geometries. This enables the user eÿciently determine the most promising candidate geometries for further study and/or development. Known issues with the low velocity and low com-plexity aerodynamic models include the absence of shock wave modelling, an important part in determining the aerodynamic performance of a lift generat-ing surface. The result of this work is the creation and documentation of a procedure for early-stage design of a blended wing-body airframe. However, due to convergence issues with the high-fidelity CFD solver, the methodology could not been validated for transonic flow. It can thus be only considered valid for flow velocities for which the Prandtl-Glauert correction is valid.</p>

corrected abstract:
<p>The current design paradigm for developing tube-and-wing style aircraft has been well documented in literature. This research attempts to develop and validate a similar design methodology to what is presently utilized for tube-and-wing based aircraft, but has so far not been successfully implemented for the blended wing-body. This construction has no clear distinction between the lift generating surfaces and the cargo carrying structure. The methodology that was developed included the concatenation and validation of low-fidelity, low speed and low complexity aerodynamic models in order to allow for quick and simple analysis of a large number of possible geometries. This enables the user efficiently determine the most promising candidate geometries for further study and/or development. Known issues with the low velocity and low complexity aerodynamic models include the absence of shock wave modelling, an important part in determining the aerodynamic performance of a lift generating surface. The result of this work is the creation and documentation of a procedure for early-stage design of a blended wing-body airframe. However, due to convergence issues with the high-fidelity CFD solver, the methodology could not been validated for transonic flow. It can thus be only considered valid for flow velocities for which the Prandtl-Glauert correction is valid.</p>
----------------------------------------------------------------------
in diva2:939534 
abstract is: 
<p>credit risk management is a significant fragment in financial institutions' security precautions against the downside of their investments. a major quandary within the subject of credit risk is the modeling of simultaneous defaults. globalization causes economises to be affected by innumerous external factors and companies to become interdependent, which in turn enlarges the complexity of establishing reliable mathematical models. the precarious situation is exacerbated by the fact that managers often suffer from the lack of data. the default correlations are most often calibrated by either using financial and/or market information. however, there exists circumstances where these types of data are inaccessible or unreliable. the problem of scarce data also induces diculties in the estimation of default probabilities. the frequency of insolvencies and changes in credit ratings are usually updated on an annual basis and historical information covers 20-25 years at best. from a mathematical perspective, this is considered as a small sample and standard statistical models are inferior in such situations.</p><p>the first part of this thesis specifies the so-called entropy model which estimates the impact of macroeconomic fluctuations on the probability of defaults, and aims to outperform standard statistical models for small samples. the second part specifies the cimdo, a framework for modeling correlated defaults without financial and market data. the last part submits a risk analysis framework for calculating the uncertainty in the simulated losses.</p><p>it is shown that the entropy model will reduce the variance of the regression coefficients but increase its bias compared to the ols and maximum likelihood. furthermore there is a significant difference between the student's t cimdo and the t-copula. the former appear to reduce the model uncertainty, however not to such extent that evident conclusions were carried out.</p>

corrected abstract:
<p>Credit risk management is a significant fragment in financial institutions' security precautions against the downside of their investments. A major quandary within the subject of credit risk is the modeling of simultaneous defaults. Globalization causes economises to be affected by in numerous external factors and companies to become interdependent, which in turn enlarges the complexity of establishing reliable mathematical models. The precarious situation is exacerbated by the fact that managers often suffer from the lack of data. The default correlations are most often calibrated by either using financial and/or market information. However, there exists circumstances where these types of data are inaccessible or unreliable. The problem of scarce data also induces difficulties in the estimation of default probabilities. The frequency of insolvencies and changes in credit ratings are usually updated on an annual basis and historical information covers 20-25 years at best. From a mathematical perspective, this is considered as a small sample and standard statistical models are inferior in such situations.</p><p>The first part of this thesis specifies the so-called entropy model which estimates the impact of macroeconomic fluctuations on the probability of defaults, and aims to outperform standard statistical models for small samples. The second part specifies the CIMDO, a framework for modeling correlated defaults without financial and market data. The last part submits a risk analysis framework for calculating the uncertainty in the simulated losses.</p><p>It is shown that the entropy model will reduce the variance of the regression coefficients but increase its bias compared to the OLS and Maximum Likelihood. Furthermore there is a significant difference between the student's 𝑡 CIMDO and the 𝑡-Copula. The former appear to reduce the model uncertainty, however not to such extent that evident conclusions were carried out.</p>
----------------------------------------------------------------------
In diva2:281929   - correct as is
----------------------------------------------------------------------
In diva2:919848 
abstract is: 
<p>The high specific stiffness and strength of composites makes it advantageous for load carrying structures in the automotive industry. By successfully be able to numerically simulate the crush behaviour of composites, structure with high specific energy absorption can be implemented in the automotive industry. The purpose of this thesis is to verify the predictive capabilities of a crush model developed at SICOMP.</p><p>Initially currently available material models are investigated. Puck’s criterion is deeper studied. An improvement of the criterion is suggested and the model is updated to be able to output fracture angles in Abaqus.The material model developed by SICOMP is a three-dimensional physically based damage model where failure initiation is estimated with proven failure criteria and damage growth is combined with friction to account for the right energy absorption.</p><p>The crush damage model has been implemented in Abaqus/Explicit as a VUMAT subroutine. Numerical predictions are compared with experimental results. Specimens with different fibre layups and crash triggers are tested.</p>

corrected abstract:
<p>The high specific stiffness and strength of composites makes it advantageous for load carrying structures in the automotive industry. By successfully be able to numerically simulate the crush behaviour of composites, structure with high specific energy absorption can be implemented in the automotive industry. The purpose of this thesis is to verify the predictive capabilities of a crush model developed at SICOMP.</p><p>Initially currently available material models are investigated. Puck’s criterion is deeper studied. An improvement of the criterion is suggested and the model is updated to be able to output fracture angles in Abaqus.</p><p>The material model developed by SICOMP is a three-dimensional physically based damage model where failure initiation is estimated with proven failure criteria and damage growth is combined with friction to account for the right energy absorption.</p><p>The crush damage model has been implemented in Abaqus/Explicit as a VUMAT subroutine. Numerical predictions are compared with experimental results. Specimens with different fibre layups and crash triggers are tested.</p>

Note: Only change is the introduction of one missing "</p><p>".
----------------------------------------------------------------------
In diva2:871850 
Note: no full text in DiVA

abstract is: 
<p>Experiment and simulations of a wedge indenter into Body Centered Cubic (BCC) single crystal were performed. The indenter with included angle of 90 degrees was applied 200 _m in depth. The indentation was performed under plane strain conditions, resulting in seven in-plane slip system sets. The midsection of the single crystal was exposed by wire Electrical Discharge Machine (EDM) and the in-plane lattice rotation was calculated by measuring the crystallographic orientation with electron backscatter diffraction (EBSD). Lower bounds of Geometrically Necessary Dislocation (GND) densities were also calculated and plotted. Numerical simulations were performed in order to determine the plastic properties of tantalum and to understand the internal crystallographic interaction of a BCC single crystal. The inuence of latent to self hardening ratio between slip system sets in the same slip family and latent to self hardening between slip system sets in different slip families have been investigated. For Face Center Cubic (FCC) material latent to self hardening ratio between slip system sets in the same slip family has not been taken into account, since FCC material only consist of one slip family. However, materials with BCC crystal structure have two slip families and therefore, both ratios have to be taken into consideration. The simulations were performed with and without self to latent hardening between sets in different slip families. The comparison between the experimental lattice rotation and the lattice rotation given by the simulations are overall very good for both cases. In order to determine which case that describes the experimental behavior best additional experiments have to be performed.</p>

corrected abstract:
<p>Experiment and simulations of a wedge indenter into Body Centered Cubic (BCC) single crystal were performed. The indenter with included angle of 90 degrees was applied 200 &mu;m in depth. The indentation was performed under plane strain conditions, resulting in seven in-plane slip system sets. The midsection of the single crystal was exposed by wire Electrical Discharge Machine (EDM) and the in-plane lattice rotation was calculated by measuring the crystallographic orientation with electron backscatter diffraction (EBSD). Lower bounds of Geometrically Necessary Dislocation (GND) densities were also calculated and plotted. Numerical simulations were performed in order to determine the plastic properties of tantalum and to understand the internal crystallographic interaction of a BCC single crystal. The influence of latent to self hardening ratio between slip system sets in the same slip family and latent to self hardening between slip system sets in different slip families have been investigated. For Face Center Cubic (FCC) material latent to self hardening ratio between slip system sets in the same slip family has not been taken into account, since FCC material only consist of one slip family. However, materials with BCC crystal structure have two slip families and therefore, both ratios have to be taken into consideration. The simulations were performed with and without self to latent hardening between sets in different slip families. The comparison between the experimental lattice rotation and the lattice rotation given by the simulations are overall very good for both cases. In order to determine which case that describes the experimental behavior best additional experiments have to be performed.</p>
----------------------------------------------------------------------
In diva2:571089 
abstract is: 
<p>Curve fitting is used in a variety of fields, especially in physics, mathematics and economics.</p><p>The method is often used to smooth noisy data and for doing path planning. In this bachelor</p><p>thesis calculus of variations will be used to derive a formula for finding an optimal curve to fit a</p><p>set of data points. We evaluate a cost function (defined on the set of all curves</p><p></p><p>f on the interval</p><p>[</p><p></p><p>a; b]) given by F(f) =</p><p>R</p><p></p><p>b</p><p>a</p><p></p><p>(f00(x))2dx +</p><p>P</p><p></p><p>n</p><p>i</p><p></p><p>=1(f(xi) 􀀀 yi)2. The integral term represents the</p><p>smoothness of the curve, the interpolation error is given by the summation term and</p><p></p><p>&gt; 0 is</p><p>defined as the interpolation parameter. An ideal curve minimizes the interpolation error and</p><p>is relatively smooth. This is problematic since a smooth function generally has a large interpolation</p><p>error when doing curve fitting, and therefore the interpolation parameter</p><p></p><p>is needed</p><p>to decide how much consideration should be given to each attribute. For the cost function</p><p></p><p>F</p><p>a larger value of</p><p></p><p>decreases the interpolation error of the curve. The analytical calculations</p><p>performed made it possible to construct a</p><p></p><p>Matlab program, that could be used to solve the</p><p>minimization problem. In the result part some examples are presented for different values of</p><p></p><p>.</p><p>The conclusion is that a larger value of the interpolation parameter</p><p></p><p>is generally needed when</p><p>using more data points and if the points are closely placed on the x-axis. Further on, a method</p><p>called Ordinary Cross Validation (OCV) is evaluated to find an optimal value of</p><p></p><p>. This method</p><p>gave good results, except for the case when the points could almost be fitted with a straight line.</p>

corrected abstract:
<p>Curve fitting is used in a variety of fields, especially in physics, mathematics and economics. The method is often used to smooth noisy data and for doing path planning. In this bachelor thesis calculus of variations will be used to derive a formula for finding an optimal curve to fit a set of data points. We evaluate a cost function (defined on the set of all curves 𝑓 on the interval [𝑎, 𝑏]) given by 𝐹(𝑓) = &int;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup style="position: relative; top: -0.5rem; left: 0.05rem;">𝑏</sup> <sub style="position: relative; bottom: -0.3rem; left: -0.1rem;">𝑎</sub></span></span>(𝑓&Prime;(𝑥))<sup>2</sup> d𝑥 + λ &sum;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup style="position: relative; top: -0.5rem; left: 0.05rem;">n</sup><sub style="position: relative; bottom: -0.5rem; left: 0.05rem;">𝑖=1</sub></span></span>(𝑓(𝑥<sub>𝑖</sub>) - 𝑦<sub>𝑖</sub>)<sup>2</sup>. The integral term represents the smoothness of the curve, the interpolation error is given by the summation term and λ &gt; 0 is defined as the interpolation parameter. An ideal curve minimizes the interpolation error and is relatively smooth. This is problematic since a smooth function generally has a large interpolation error when doing curve fitting, and therefore the interpolation parameter λ is needed to decide how much consideration should be given to each attribute. For the cost function 𝐹 a larger value of λ decreases the interpolation error of the curve. The analytical calculations performed made it possible to construct a M<span style="font-variant: small-caps;">atlab</span> program, that could be used to solve the minimization problem. In the result part some examples are presented for different values of λ.  The conclusion is that a larger value of the interpolation parameter λ is generally needed when using more data points and if the points are closely placed on the x-axis. Further on, a method called Ordinary Cross Validation (OCV) is evaluated to find an optimal value of λ. This method gave good results, except for the case when the points could almost be fitted with a straight line.</p>
----------------------------------------------------------------------
In diva2:1426161 
abstract is: 
<p>This past decade, the majority of services have been digitalized and data more and more available, easy to store and to process in order to understand customers behaviors. In order to be leaders in their proper industries, subscription-based businesses must focus on their Customer Relationship Management and in particular churn management, that is understanding customers cancelling their subscription. In this thesis, churn analysis is performed on real life data from a Software as a Service (SaaS) company selling an advanced cloud-based business phone system, Aircall. This use case has the particularity that the available dataset gathers customers data on a monthly basis and has a very imbalanced distribution of the target: a large majority of customers do not churn. Therefore, several methods are tried in order to diminish the impact of the imbalance while remaining as close as possible to the real world and the temporal framework. These methods include oversampling and undersampling (SMOTE and Tomek's link) and time series cross-validation. Then logistic regression and random forest models are used with an aim to both predict and explain churn.The non-linear method performed better than logistic regression, suggesting the limitation of linear models for our use case. Moreover, mixing oversampling with undersampling gives better performances in terms of precision/recall trade-off. Time series cross-validation also happens to be an efficient method to improve performance of the model. Overall, the resulting model is more useful to explain churn than to predict it. It highlighted some features majorly influencing churn, mostly related to product usage.</p>

corrected abstract:
<p>This past decade, the majority of services have been digitalized and data more and more available, easy to store and to process in order to understand customers behaviors. In order to be leaders in their proper industries, subscription-based businesses must focus on their Customer Relationship Management and in particular churn management, that is understanding customers cancelling their subscription. In this thesis, churn analysis is performed on real life data from a Software as a Service (SaaS) company selling an advanced cloud-based business phone system, Aircall. This use case has the particularity that the available dataset gathers customers data on a monthly basis and has a very imbalanced distribution of the target: a large majority of customers do not churn. Therefore, several methods are tried in order to diminish the impact of the imbalance while remaining as close as possible to the real world and the temporal framework. These methods include oversampling and undersampling (SMOTE and Tomek's link) and time series cross-validation. Then logistic regression and random forest models are used with an aim to both predict and explain churn. The non-linear method performed better than logistic regression, suggesting the limitation of linear models for our use case. Moreover, mixing oversampling with undersampling gives better performances in terms of precision/recall trade-off. Time series cross-validation also happens to be an efficient method to improve performance of the model. Overall, the resulting model is more useful to explain churn than to predict it. It highlighted some features majorly influencing churn, mostly related to product usage.</p>
----------------------------------------------------------------------
In diva2:1328904 - missing space in title:
"Customer Churn Prediction for PC Games: Probability of churn predicted for big-spenders usingsupervised machine learning"
==>
"Customer Churn Prediction for PC Games: Probability of churn predicted for big-spenders using supervised machine learning"

abstract is: 
<p>Paradox Interactive is a Swedish video game developer and publisher which has players all around the world. Paradox’s largest platform in terms of amount of players and revenue is the PC. The goal of this thesis was to make a churn predic-tion model to predict the probability of players churning in order to know which players to focus on in retention campaigns. Since the purpose of churn prediction is to minimize loss due to customers churning the focus was on big-spenders (whales) in Paradox PC games.</p><p>In order to define which players are big-spenders the spending for players over a 12 month rolling period (from 2016-01-01 until 2018-12-31) was investigated. The players spending more than the 95th-percentile of the total spending for each pe-riod were defined as whales. Defining when a whale has churned, i.e. stopped being a big-spender in Paradox PC games, was done by looking at how many days had passed since the players bought something. A whale has churned if he has not bought anything for the past 28 days.</p><p>When data had been collected about the whales the data set was prepared for a number of di˙erent supervised machine learning methods. Logistic Regression, L1 Regularized Logistic Regression, Decision Tree and Random Forest were the meth-ods tested. Random Forest performed best in terms of AUC, with AUC = 0.7162. The conclusion is that it seems to be possible to predict the probability of churning for Paradox whales. It might be possible to improve the model further by investi-gating more data and fine tuning the definition of churn.</p>


corrected abstract:
<p>Paradox Interactive is a Swedish video game developer and publisher which has players all around the world. Paradox’s largest platform in terms of amount of players and revenue is the PC. The goal of this thesis was to make a churn prediction model to predict the probability of players churning in order to know which players to focus on in retention campaigns. Since the purpose of churn prediction is to minimize loss due to customers churning the focus was on big-spenders (whales) in Paradox PC games.</p><p>In order to define which players are big-spenders the spending for players over a 12 month rolling period (from 2016-01-01 until 2018-12-31) was investigated. The players spending more than the 95th-percentile of the total spending for each period were defined as whales. Defining when a whale has churned, i.e. stopped being a big-spender in Paradox PC games, was done by looking at how many days had passed since the players bought something. A whale has churned if he has not bought anything for the past 28 days.</p><p>When data had been collected about the whales the data set was prepared for a number of different supervised machine learning methods. Logistic Regression, L1 Regularized Logistic Regression, Decision Tree and Random Forest were the methods tested. Random Forest performed best in terms of AUC, with <em>AUC</em> = 0.7162. The conclusion is that it seems to be possible to predict the probability of churning for Paradox whales. It might be possible to improve the model further by investigating more data and fine tuning the definition of churn.</p>
----------------------------------------------------------------------
In diva2:720978 
abstract is: 
<p>This report focuses on the evolution of dark matter particles in a simplied, homogeneous</p><p>and isotropic model of the Universe. The purpose is to analyze theoretical predictions and</p><p>recent experimental measurements to be able to draw conclusions about the properties</p><p>of the dark matter particles. The inexperienced reader is introduced to the subject and</p><p>thorough derivations of the formulas relevant to the analysis are made. To analyze the</p><p>evolution of dark matter, the Boltzmann equation is applied to a freeze-out model. Both</p><p>analytical and numerical approaches will be taken and discrepancies between those are</p><p>investigated. Qualitative eects of the particle cross section and mass are studied and</p><p>constraints on the parameters are set using experimental data. Finally, assumptions are</p><p>discussed and suggestions for further research are made.</p>
w='eect' val={'c': 'effect', 's': ['diva2:555900', 'diva2:560204', 'diva2:676723', 'diva2:737287', 'diva2:1078078', 'diva2:450612', 'diva2:612188', 'diva2:1081973', 'diva2:1110812', 'diva2:1110830', 'diva2:814465', 'diva2:720978', 'diva2:1111560', 'diva2:558033', 'diva2:720261', 'diva2:1040617', 'diva2:571303', 'diva2:783984', 'diva2:796765', 'diva2:1528145', 'diva2:1109484', 'diva2:526641', 'diva2:561900', 'diva2:561907', 'diva2:1465506', 'diva2:488441', 'diva2:562186', 'diva2:740784', 'diva2:412867', 'diva2:1130148', 'diva2:612201', 'diva2:1334273', 'diva2:405436', 'diva2:1189528', 'diva2:618581', 'diva2:571402', 'diva2:1163145', 'diva2:560966', 'diva2:1078083'], 'n': 'missing ligature'}

corrected abstract:
<p>This report focuses on the evolution of dark matter particles in a simplified, homogeneous and isotropic model of the Universe. The purpose is to analyze theoretical predictions and recent experimental measurements to be able to draw conclusions about the properties of the dark matter particles. The inexperienced reader is introduced to the subject and thorough derivations of the formulas relevant to the analysis are made. To analyze the evolution of dark matter, the Boltzmann equation is applied to a freeze-out model. Both analytical and numerical approaches will be taken and discrepancies between those are investigated. Qualitative effects of the particle cross section and mass are studied and constraints on the parameters are set using experimental data. Finally, assumptions are discussed and suggestions for further research are made.</p>
----------------------------------------------------------------------
In diva2:1640127 
abstract is: 
<p>This work consists of two parts: the analysis of the operating data collected at Lillgrund between December 2007 and March 2018, and the implementation and evaluation of three wake models. Operational and meteorological data from the Lillgrund wind farm have been analyzed to assess the aerodynamic efficiency and general performance of the farm. Power output, wake losses and array efficiency have been calculated in order to further characterize phenomena related to clustering of wind turbine and garther a database for future model validations. The results of the analysis quantify the intensity of the wake effect at Lillgrund and the power losses due to both wake effect and unavailability of turbines. Moreover, it is found that the array efficiency did not vary through the years. Additionally, effects of atmospheric stratification are evaluated by showing diurnal and seasonal variations of the array efficiency: results are opposite of what was expected, with the wind farm being more efficient at night and during winter.</p><p>Furthermore, three wake models have been implemented and validated using the developed database: the Jensen model, the Ainslie model and the Gaussian model developed by Bastankhah &amp; Porté-Agel (2014). The objective of the wake effect and power output that is reachable at a very low computational cost for offshore cases. By comparing the results with those of prior works, it can be stated that there is not a substantial diffference in the accuracy of wake models and CFD simulations for the case of Lillgrund.</p>

corrected abstract:
<p>This work consists of two parts: the analysis of the operating data collected at Lillgrund between December 2007 and March 2018, and the implementation and evaluation of three wake models.</p><p>Operational and meteorological data from the Lillgrund wind farm have been analyzed to assess the aerodynamic efficiency and general performance of the farm. Power output, wake losses and array efficiency have been calculated in order to further characterize phenomena related to clustering of wind turbine and gather a database for future model validations. The results of the analysis quantify the intensity of the wake effect at Lillgrund and the power losses due to both wake effect and unavailability of turbines. Moreover, it is found that the array efficiency did not vary through the years. Additionally, effects of atmospheric stratification are evaluated by showing diurnal and seasonal variations of the array efficiency: results are opposite of what was expected, with the wind farm being more efficient at night and during winter.</p><p>Furthermore, three wake models have been implemented and validated using the developed database: the Jensen model, the Ainslie model and the Gaussian model developed by Bastankhah &amp; Porté-Agel (2014). The objective of the wake modelling is to assess the accuracy in the estimation of both  wake effect and power output that is reachable at a very low computational cost for offshore cases. By comparing the results with those of prior works, it can be stated that there is not a substantial difference in the accuracy of wake models and CFD simulations for the case of Lillgrund.</p>
----------------------------------------------------------------------
In diva2:1776595 
abstract is: 
<p>This text provides an overview of problems in the field of data assimilation. We explore the possibility of recreating unknown data by continuously inserting known data into certain dynamical systems, under certain regularity assumptions. Additionally, we discuss an alternative statistical approach to data assimilation and investigate the utilization of the Ensemble Kalman Filter for assimilating data into dynamical models. A key challenge in numerical weather prediction is incorporating convective precipitation into an idealized setting for numerical computations. To answer this question we examine the modified rotating shallow water equations, a nonlinear coupled system of partial differential equations and further assess if this primitive model accurately mimics phenomena observed in operational numerical weather prediction models. Numerical experiments conducted using a Deterministic Ensemble Kalman Filter algorithm support its applicability for convective-scale data assimilation. Furthermore, we analyze the frequency spectrum of numerical forecasts using the Wavelet transform. Our frequency analysis suggests that, under certain experimental settings, there are similarities in the initialization of operational models, which can aid in understanding the problem of intialization of numerical weather prediction models.</p>


corrected abstract:
<p>This text provides an overview of problems in the field of data assimilation. We explore the possibility of recreating unknown data by continuously inserting known data into certain dynamical systems, under certain regularity assumptions. Additionally, we discuss an alternative statistical approach to data assimilation and investigate the utilization of the Ensemble Kalman Filter for assimilating data into dynamical models. A key challenge in numerical weather prediction is incorporating convective precipitation into an idealized setting for numerical computations. To answer this question we examine the modified rotating shallow water equations, a nonlinear coupled system of partial differential equations and further assess if this primitive model accurately mimics phenomena observed in operational numerical weather prediction models. Numerical experiments conducted using a Deterministic Ensemble Kalman Filter algorithm support its applicability for convective-scale data assimilation. Furthermore, we analyze the frequency spectrum of numerical forecasts using the Wavelet transform. Our frequency analysis suggests that, under certain experimental settings, it might be possible to find similarities in the initialization of operational models, which can aid in understanding the problem of intialization of numerical weather prediction models.</p>

Note spelling error in original:
w='intialization' val={'c': 'initialization', 's': 'diva2:1776595', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1130082 -- missing space in title:
"Deciphering conversationparticipants' uncertainty: A study using non-verbal cues"
==>
"Deciphering conversation participants' uncertainty: A study using non-verbal cues"

abstract is: 
<p>The eld of human emotion expression has become increasingly important for engineers due to the potential of developing computors and robots that can interact naturally with humans. This study investigates uncertainty is manifested in facial expressions. We formulated the binary denition "A conversation participant is uncertain when they feel they do not understand what the counterpart is trying to communicate or that they do not know what to say". With regards to this denition, a corpus consisting of videos where people were playing a spot the dierence-game and participants could only communicate verbally was marked with uncertainty labels. Facial features were extracted using the computor vision software OpenFace. These features, along with the uncertainty markings were then used to train a neural network. The trained model was then used to predict whether a participant is uncertain or not at a given moment. To evaluate the model, it was tested on previously unseen data with an even distribution of certain and uncertain data points as well the actual data. For both of the cases, the best prediction model managed to perform only better than if the model would employ majority class prediction.</p>

corrected abstract:
<p>The field of human emotion expression has become increasingly important for engineers due to the potential of developing computors and robots that can interact naturally with humans. This study investigates uncertainty is manifested in facial expressions. We formulated the binary definition ”A conversation participant is <em>uncertain</em> when they feel they do not understand what the counterpart is trying to communicate or that they do not know what to say”. With regards to this definition, a corpus consisting of videos where people were playing a spot the difference-game and participants could only communicate verbally was marked with uncertainty labels. Facial features were extracted using the computor vision software OpenFace. These features, along with the uncertainty markings were then used to train a neural network. The trained model was then used to predict whether a participant is uncertain or not at a given moment. To evaluate the model, it was tested on previously unseen data with an even distribution of certain and uncertain data points as well the actual data. For both of the cases, the best prediction model managed to perform only better than if the model would employ majority class prediction.</p>
----------------------------------------------------------------------
In diva2:585833 
abstract is: 
<p>The purpose of this thesis is to develop a mathematical approach and associated software implementation for deconvolution of two-dimensional Transmission Electron Microscope (TEM) images. The focus is on TEM images of weakly scattering amorphous biological specimens that mainly produce phase contrast. The deconvolution is to remove the distortions introduced by the TEM detector that are modeled by the Modulation Transfer Function (MTF). The report tests deconvolution of the TEM detector MTF by Wiener _ltering and Tikhonov regularization on a range of simulated TEM images with varying degree of noise.The performance of the two deconvolution methods are quanti_ed by means of Figure of Merits (FOMs) and comparison in-between methods is based on statistical analysis of the FOMs.</p>

corrected abstract:
<p>The purpose of this thesis is to develop a mathematical approach and associated software implementation for deconvolution of two-dimensional Transmission Electron Microscope (TEM) images. The focus is on TEM images of weakly scattering amorphous biological specimens that mainly produce phase contrast. The deconvolution is to remove the distortions introduced by the TEM detector that are modeled by the Modulation Transfer Function (MTF). The report tests deconvolution of the TEM detector MTF by Wiener filtering and Tikhonov regularization on a range of simulated TEM images with varying degree of noise. The performance of the two deconvolution methods are quantified by means of Figure of Merits (FOMs) and comparison in-between methods is based on statistical analysis of the FOMs.</p>
----------------------------------------------------------------------
In diva2:1431145 
abstract is: 
<p>The purpose of this work is to ﬁnd out if an artiﬁcial neural network can be useful purpose of this work is to ﬁnd out if an artiﬁcial neural network can be useful in order to detect rail squats with the existing Quiet Track Measurement System (QTMS). Squats are surface-initiated rail defects which arise due to rolling contact fatigue. The monitoring system, installed on seven trains running on the green line in the Stockholm underground, aims to improve the maintenance process. The early detection and surveillance of defects helps to extend the service life of the tracks and reduce operating costs. An artiﬁcial neural network is used to analyse the the continuously recorded measurements, which consist of vertical bogie acceleration and surrounding noise, each sampled with a frequency of 22 kHz.In particular, the power spectral density as input for multi-layer Fully-connected Neural Network (FNN) has proven to be promising for accurate squat predictions. The supervised learning was carried out according to the one-vs-all principle, i.e. squats versus all other events. A two-hidden-layer FNN has ﬁnally been chosen to complement the QTMS. The usage of the full available frequency range from almost DC up to 11kHz, but minimum 7 kHz, allows good prediction with only low false prediction rates. When concatenating all six measurement channels to a single classiﬁer input, an accuracy of over 96% for the squat class and up to 99.98% can in total be achieved. The chosen network type also showed high stability despite quite strong parameter variations and a massive under-representation of squat observations in the measurement data.However, since limited maintenance information about actual squats is available for labelling and testing, more evaluation is needed. The correct identiﬁcation of mis-labelled squats indicates the high potentials of artiﬁcial neural networks.</p>

corrected abstract:
<p>The purpose of this work is to find out if an artificial neural network can be useful in order to detect rail squats with the existing Quiet Track Measurement System (QTMS). Squats are surface-initiated rail defects which arise due to rolling contact fatigue. The monitoring system, installed on seven trains running on the green line in the Stockholm underground, aims to improve the maintenance process. The early detection and surveillance of defects helps to extend the service life of the tracks and reduce operating costs. An artificial neural network is used to analyse the the continuously recorded measurements, which consist of vertical bogie acceleration and surrounding noise, each sampled with a frequency of 22 kHz.</p><p>In particular, the power spectral density as input for multi-layer Fully-connected Neural Network (FNN) has proven to be promising for accurate squat predictions. The supervised learning was carried out according to the one-vs-all principle, i.e. squats versus all other events. A two-hidden-layer FNN has finally been chosen to complement the QTMS. The usage of the full available frequency range from almost DC up to 11kHz, but minimum 7 kHz, allows good prediction with only low false prediction rates. When concatenating all six measurement channels to a single classifier input, an accuracy of over 96% for the squat class and up to 99.98% can in total be achieved. The chosen network type also showed high stability despite quite strong parameter variations and a massive under-representation of squat observations in the measurement data.</p><p>However, since limited maintenance information about actual squats is available for labelling and testing, more evaluation is needed. The correct identification of mis-labelled squats indicates the high potentials of artificial neural networks.</p>
----------------------------------------------------------------------
In diva2:1215657 
abstract is: 
<p>Deep neural networks trained with spatio-temporal evolution of a dynamical system may be regarded as an empirical alternative to conventional models using differential equations. In this thesis, such deep learning models are constructed for the problem of turbulent shear flow. However, as a first step, this modeling is restricted to a simplified low-dimensional representation of turbulence physics. The training datasets for the neural networks are obtained from a 9-dimensional model using Fourier modes proposed by Moehlis, Faisst, and Eckhardt [29] for sinusoidal shear flow. These modes were appropriately chosen to capture the turbulent structures in the near-wall region. The time series of the amplitudes of these modes fully describe the evolution of flow. Trained deep learning models are employed to predict these time series based on a short input seed. Two fundamentally different neural network architectures, namely multilayer perceptrons (MLP) and long short-term memory (LSTM) networks are quantitatively compared in this work. The assessment of these architectures is based on (i) the goodness of fit of their predictions to that of the 9-dimensional model, (ii) the ability of the predictions to capture the near-wall turbulence structures, and (iii) the statistical consistency of the predictions with the test data. LSTMs are observed to make predictions with an error that is around 4 orders of magnitude lower than that of the MLP. Furthermore, the flow fields constructed from the LSTM predictions are remarkably accurate in their statistical behavior. In particular, deviations of 0:45 % and 2:49 % between the true data and the LSTM predictions were obtained for the mean flow and the streamwise velocity fluctuations, respectively.<strong></strong></p>

corrected abstract:
<p>Deep neural networks trained with spatio-temporal evolution of a dynamical system may be regarded as an empirical alternative to conventional models using differential equations. In this thesis, such deep learning models are constructed for the problem of turbulent shear flow. However, as a first step, this modeling is restricted to a simplified low-dimensional representation of turbulence physics. The training datasets for the neural networks are obtained from a 9-dimensional model using Fourier modes proposed by Moehlis, Faisst, and Eckhardt [29] for sinusoidal shear flow. These modes were appropriately chosen to capture the turbulent structures in the near-wall region. The time series of the amplitudes of these modes fully describe the evolution of flow. Trained deep learning models are employed to predict these time series based on a short input seed. Two fundamentally different neural network architectures, namely multilayer perceptrons (MLP) and long short-term memory (LSTM) networks are quantitatively compared in this work. The assessment of these architectures is based on (i) the goodness of fit of their predictions to that of the 9-dimensional model, (ii) the ability of the predictions to capture the near-wall turbulence structures, and (iii) the statistical consistency of the predictions with the test data. LSTMs are observed to make predictions with an error that is around 𝟦 orders of magnitude lower than that of the MLP. Furthermore, the flow fields constructed from the LSTM predictions are remarkably accurate in their statistical behavior. In particular, deviations of 0.45% and 2.49% between the true data and the LSTM predictions were obtained for the mean flow and the streamwise velocity fluctuations, respectively.</p>
----------------------------------------------------------------------
In diva2:1525411 
abstract is: 
<p>Segmentation of magnetic resonance images is an important part of planning radiotherapy treat-ments for patients with brain tumours but due to the number of images contained within a scan and the level of detail required, manual segmentation is a time consuming task. Convolutional neural networks have been proposed as tools for automated segmentation and shown promising results. However, the data sets used for training these deep learning models are often imbalanced and contain data that does not contribute to the performance of the model. By carefully selecting which data to train on, there is potential to both speed up the training and increase the network’s ability to detect tumours.</p><p>This thesis implements the method of importance sampling for training a convolutional neural network for patch-based segmentation of three dimensional multimodal magnetic resonance images of the brain and compares it with the standard way of sampling in terms of network performance and training time. Training is done for two different patch sizes. Features of the most frequently sampled volumes are also analysed.</p><p>Importance sampling is found to speed up training in terms of number of epochs and also yield models with improved performance. Analysis of the sampling trends indicate that when patches are large, small tumours are somewhat frequently trained on, however more investigation is needed to confirm what features may influence the sampling frequency of a patch.</p>

corrected abstract:
<p>Segmentation of magnetic resonance images is an important part of planning radiotherapy treatments for patients with brain tumours but due to the number of images contained within a scan and the level of detail required, manual segmentation is a time consuming task. Convolutional neural networks have been proposed as tools for automated segmentation and shown promising results. However, the data sets used for training these deep learning models are often imbalanced and contain data that does not contribute to the performance of the model. By carefully selecting which data to train on, there is potential to both speed up the training and increase the network’s ability to detect tumours.</p><p>This thesis implements the method of importance sampling for training a convolutional neural network for patch-based segmentation of three dimensional multimodal magnetic resonance images of the brain and compares it with the standard way of sampling in terms of network performance and training time. Training is done for two different patch sizes. Features of the most frequently sampled volumes are also analysed.</p><p>Importance sampling is found to speed up training in terms of number of epochs and also yield models with improved performance. Analysis of the sampling trends indicate that when patches are large, small tumours are somewhat frequently trained on, however more investigation is needed to confirm what features may influence the sampling frequency of a patch.</p>

Note: only change was:
w='treat-ments' val={'c': 'treatments', 's': 'diva2:1525411', 'n': 'unnecessary hyphen'}
----------------------------------------------------------------------
In diva2:1816913 
abstract is: 
<p>Cone Beam Computed Tomography is a technology to visualize the 3D interior anatomy of a patient. It is important for image-guided radiation therapy in cancer treatment. During a scan, iterative methods are often used for the image reconstruction step. A key challenge is the ill-posedness of the resulting inversion problem, causing the images to become noisy. To combat this, regularizers can be introduced, which help stabilize the problem. This thesis focuses on Adversarial Convex Regularization that with deep learning regularize the scans according to a target image quality. It can be interpreted in a Bayesian setting by letting the regularizer be the prior, approximating the likelihood with the measurement error, and obtaining the patient image through the maximum-a-posteriori estimate. Adversarial Convex Regularization has previously shown promising results in regular Computed Tomography, and this study aims to investigate its potential in Cone Beam Computed Tomography. </p><p>Three different learned regularization methods have been developed, all based on Convolutional Neural Network architectures. One model is based on three-dimensional convolutional layers, while the remaining two rely on 2D layers. These two are in a later stage crafted to be applicable to 3D reconstruction by either stacking a 2D model or by averaging 2D models trained in three orthogonal planes. All neural networks are trained on simulated male pelvis data provided by Elekta. The 3D convolutional neural network model has proven to be heavily memory-consuming, while not performing better than current reconstruction methods with respect to image quality. The two architectures based on merging multiple 2D neural network gradients for 3D reconstruction are novel contributions that avoid memory issues. These two models outperform current methods in terms of multiple image quality metrics, such as Peak Signal-to-Noise Ratio and Structural Similarity Index Measure, and they also generalize well for real Cone Beam Computed Tomography data. Additionally, the architecture based on a weighted average of 2D neural networks is able to capture spatial interactions to a larger extent and is adjustable to favor the plane that best shows the field of interest, a possibly desirable feature in medical practice.</p>

corrected abstract:
<p>Cone Beam Computed Tomography is a technology to visualize the 3D interior anatomy of a patient. It is important for image-guided radiation therapy in cancer treatment. During a scan, iterative methods are often used for the image reconstruction step. A key challenge is the ill-posedness of the resulting inversion problem, causing the images to become noisy. To combat this, regularizers can be introduced, which help stabilize the problem. This thesis focuses on Adversarial Convex Regularization that with deep learning regularize the scans according to a target image quality. It can be interpreted in a Bayesian setting by letting the regularizer be the prior, approximating the likelihood with the measurement error, and obtaining the patient image through the maximum-a-posteriori estimate. Adversarial Convex Regularization has previously shown promising results in regular Computed Tomography, and this study aims to investigate its potential in Cone Beam Computed Tomography.</p><p>Three different learned regularization methods have been developed, all based on Convolutional Neural Network architectures. One model is based on three-dimensional convolutional layers, while the remaining two rely on 2D layers. These two are in a later stage crafted to be applicable to 3D reconstruction by either stacking a 2D model or by averaging 2D models trained in three orthogonal planes. All neural networks are trained on simulated male pelvis data provided by Elekta. The 3D convolutional neural network model has proven to be heavily memory-consuming, while not performing better than current reconstruction methods with respect to image quality. The two architectures based on merging multiple 2D neural network gradients for 3D reconstruction are novel contributions that avoid memory issues. These two models outperform current methods in terms of multiple image quality metrics, such as Peak Signal-to-Noise Ratio and Structural Similarity Index Measure, and they also generalize well for real Cone Beam Computed Tomography data. Additionally, the architecture based on a weighted average of 2D neural networks is able to capture spatial interactions to a larger extent and is adjustable to favor the plane that best shows the field of interest, a possibly desirable feature in medical practice.</p>

Note missing italics error in originaL:
w='maximum-a-posteriori' val={'c': 'maximum <em>a posteriori</em>', 's': 'diva2:1816913', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1849139 
abstract is: 
<p>This thesis evaluates the use of a Deep Reinforcement Learning (DRL) approach to portfolio management on the Swedish stock market. The idea is to construct a portfolio that is adjusted daily using the DRL algorithm Proximal policy optimization (PPO) with a multi perceptron neural network. The input to the neural network was historical data in the form of open, high, and low price data. The portfolio is evaluated by its performance against the OMX Stockholm 30 index (OMXS30). Furthermore, three different approaches for optimization are going to be studied, in that three different reward functions are going to be used. These functions are Sharp ratio, cumulative reward (Daily return) and Value at risk reward (which is a daily return with a value at risk penalty). The historival data that is going to be used is from the period 2010-01-01 to 2015-12-31 and the DRL approach is then tested on two different time periods which represents different marked conditions, 2016-01-01 to 2018-12-31 and 2019-01-01 to 2021-12-31. The results show that in the first test period all three methods (corresponding to the three different reward functions) outperform the OMXS30 benchmark in returns and sharp ratio, while in the second test period none of the methods outperform the OMXS30 index.</p>

corrected abstract:
<p>This thesis evaluates the use of a Deep Reinforcement Learning (DRL) approach to portfolio management on the Swedish stock market. The idea is to construct a portfolio that is adjusted daily using the DRL algorithm Proximal policy optimization (PPO) with a multi perceptron neural network. The input to the neural network was historical data in the form of open, high, and low price data. The portfolio is evaluated by its performance against the OMX Stockholm 30 index (OMXS30).</p><p>Furthermore, three different approaches for optimization are going to be studied, in that three different reward functions are going to be used. These functions are Sharp ratio, cumulative reward (Daily return) and Value at risk reward (which is a daily return with a value at risk penalty).</p><p>The historical data that is going to be used is from the period 2010-01-01 to 2015-12-31 and the DRL approach is then tested on two different time periods which represents different market conditions, 2016-01-01 to 2018-12-31 and 2019-01-01 to 2021-12-31.</p><p>The results show that in the first test period all three methods (corresponding to the three different reward functions) outperform the OMXS30 benchmark in returns and sharp ratio, while in the second test period none of the methods outperform the OMXS30 index.</p>
----------------------------------------------------------------------
In diva2:1645397 
abstract is: 
<p>Photogrammetry is a great technology to check and track deformations on anysurface. This information is critical in many tranportation industries such as aeronautics and the automotive industry. In this thesis report photogrammetry measurements were done using 2 Aicon cameras providing 3D targets coordinates. The goal of this thesis was to ensure the best environment and positioning of the cameras to gather a good amount and quality of data. First positioning of the cameras is optimized by generating a map of the best projected areas that give the best coverage possible of the object. Then all the post processing was done with python using various mathematical methods to extract as much information as possible from the data recorded. First, data transformation to the correct coordinate system was performed using a sparse point-to-plane ICP algorithm. Then missing data was recovered by interpolating it over the time range. Finally raw deflections measurement were extracted and post processed to get essential information such as rigid body motion and static deformation. After an introduction about what is photogrammetry and why it is of high importance in transportation industries, this thesis tackles all the relative challenges that any photogrammetry specialist may face when dealing with such a technology.</p>

corrected abstract:
<p>Photogrammetry is a great technology to check and track deformations on any surface. This information is critical in many tranportation industries such as aeronautics and the automotive industry. In this thesis report photogrammetry measurements were done using 2 Aicon cameras providing 3D targets coordinates. The goal of this thesis was to ensure the best environment and positioning of the cameras to gather a good amount and quality of data. First positioning of the cameras is optimized by generating a map of the best projected areas that give the best coverage possible of the object. Then all the post processing was done with python using various mathematical methods to extract as much information as possible from the data recorded. First, data transformation to the correct coordinate system was performed using a sparse point-to-plane ICP algorithm. Then missing data was recovered by interpolating it over the time range. Finally raw deflections measurement were extracted and post processed to get essential information such as rigid body motion and static deformation. After an introduction about what is photogrammetry and why it is of high importance in transportation industries, this thesis tackles all the relative challenges that any photogrammetry specialist may face when dealing with such a technology.</p>
----------------------------------------------------------------------
In diva2:1038820 - error in title:
"Degree Project in Engineering Physics, First Level Department of Mathematics KTH Royal Institute of Technology"
==>
"Elliptic curves over finite fields and the rationals: Determining the underlying group structure"

abstract is: 
<p>The study of elliptic curves is an important part of the elds of</p><p>algebraic geometry and number theory, with many applications in areas</p><p>such as cryptography. While much of the groundwork has already</p><p>been laid out, the results often times fall short of giving an easily digestible</p><p>overview of the subject as a whole. The aim of this paper is</p><p>to condense a number of high-level results into a much more readily</p><p>accessible version that is better suited for a reader encountering elliptic</p><p>curves for the rst time. Additionally, the paper provides a toolkit</p><p>for identifying elliptic curve groups, detailing steps to take in order to</p><p>determine the group behind a given elliptic curve.</p>

corrected abstract:
<p>The study of elliptic curves is an important part of the fields of algebraic geometry and number theory, with many applications in areas such as cryptography. While much of the groundwork has already been laid out, the results often times fall short of giving an easily digestible overview of the subject as a whole. The aim of this paper is to condense a number of high-level results into a much more readily accessible version that is better suited for a reader encountering elliptic curves for the first time. Additionally, the paper provides a toolkit for identifying elliptic curve groups, detailing steps to take in order to determine the group behind a given elliptic curve.</p>
----------------------------------------------------------------------
In diva2:718397 
abstract is: 
<p>The work aims to study different methods suitable for de-icing and ice prevention of vehicle headlamps and tail lamps, especially LED-lights. Furthermore, the work aims to investigate the scale of the problem with insufficient or lack of de-icing on automotive lamps depending on the region and the environment the vehicle operates in.</p><p>The problem with insufficient de-icing in automotive lamps was investigated by observations, tests of various lamps and a driver survey. Deicing methods were identified through a litera-ture review. The methods were studied in detail, and some were also evaluated by tests. The tests were narrowed down to temperature measurements and de-icing measurements. The latter were performed using a test method especially developed for the task. The collected data was used to evaluate whether actions are needed to be taken and to form recommen-dations for future developments.</p><p>The number one priority should be to improve the tail lamps de-icing ability. Headlamps can also be improved but there is no imminent need. Insufficient de-icing of headlamps and tail lamps can potentially be a problem in all areas subjected to cold winter climate. Tail lamps should be fitted with electrical heating in order to improve the de-icing ability. They should be positioned so that snow and ice does not stack on top of them. The de-icing time of tail lamps should be less than 10 min. Truck drivers needs to be better in scraping their headlamps and tail lamps.</p><p>The time needed for de-icing Scania’s H7 headlamps is 20 min for halogen version and 35 min for xenon version at -18 °C. This should be compared to the BMW LED-headlamp which needs 65 min to complete de-icing. LED-headlamps are probably limited to a de-icing time of approximately 60 min unless additional heat is added to the headlamp lens. The fastest and most efficient way to de-ice the headlamps is to use hot washer fluid. Electrically heated lenses are also effective but the de-icing process is slower. The proposed test method is a simple and effective way to compare and evaluate headlamps and tail lamps without knowing internal airflows and light sources. The way the ice layer is created on the device under test is unique to this method. The created ice layer is extremely uniform and the results are easy to evaluate.</p><p></p>

corrected abstract:
<p>The work aims to study different methods suitable for de-icing and ice prevention of vehicle headlamps and tail lamps, especially LED-lights. Furthermore, the work aims to investigate the scale of the problem with insufficient or lack of de-icing on automotive lamps depending on the region and the environment the vehicle operates in.</p><p>The problem with insufficient de-icing in automotive lamps was investigated by observations, tests of various lamps and a driver survey. Deicing methods were identified through a literature review. The methods were studied in detail, and some were also evaluated by tests. The tests were narrowed down to temperature measurements and de-icing measurements. The latter were performed using a test method especially developed for the task. The collected data was used to evaluate whether actions are needed to be taken and to form recommendations for future developments.</p><p>The number one priority should be to improve the tail lamps de-icing ability. Headlamps can also be improved but there is no imminent need. Insufficient de-icing of headlamps and tail lamps can potentially be a problem in all areas subjected to cold winter climate. Tail lamps should be fitted with electrical heating in order to improve the de-icing ability. They should be positioned so that snow and ice does not stack on top of them. The de-icing time of tail lamps should be less than 10 min. Truck drivers needs to be better in scraping their headlamps and tail lamps.</p><p>The time needed for de-icing Scania’s H7 headlamps is 20 min for halogen version and 35 min for xenon version at -18 °C. This should be compared to the BMW LED-headlamp which needs 65 min to complete de-icing. LED-headlamps are probably limited to a de-icing time of approximately 60 min unless additional heat is added to the headlamp lens. The fastest and most efficient way to de-ice the headlamps is to use hot washer fluid. Electrically heated lenses are also effective but the de-icing process is slower. The proposed test method is a simple and effective way to compare and evaluate headlamps and tail lamps without knowing internal airflows and light sources. The way the ice layer is created on the device under test is unique to this method. The created ice layer is extremely uniform and the results are easy to evaluate.</p>
----------------------------------------------------------------------
In diva2:1440619 
abstract is: 
<p>Once a year an event is being held at the fluid physics lab at KTH. One of the main purposes of this event is to awaken an interest in the visiting students from other universities, by demonstrating fluid physics phenomena. Showing the transition from laminar to turbulent flow in an open water channel is one of the demonstrations. This will give the students a feel for the theoretical Reynolds number. The existing water table used for this purpose was lacking in some areas. The purpose of this project is to construct, design and test a rig for openwater channel flow which can be used for the event in the coming years.The character of the flow in an open channel depends on the velocity of the fluid. When the velocity get to a critical level, the flow shifts from a laminar to a turbulent one. It is however difficult to see water flows with the naked eye which makes it difficult to demonstrate this phenomena. There are ways to solve this, that is whatis called flow visualisation. In this project a visualisation method using small hydrogen bubbles in the flow as tracer particles is investigated. The channel design and the visualisation method are investigated experimentally and documented and also compared to existing theory within these areas. The results are deemed satisfactory and the method is documented for further work.</p>

corrected abstract:
<p>Once a year an event is being held at the fluid physics lab at KTH. One of the main purposes of this event is to awaken an interest in the visiting students from other universities, by demonstrating fluid physics phenomena. Showing the transition from laminar to turbulent flow in an open water channel is one of the demonstrations. This will give the students a feel for the theoretical Reynolds number. The existing water table used for this purpose was lacking in some areas. The purpose of this project is to construct, design and test a rig for open water channel flow which can be used for the event in the coming years.</p><p>The character of the flow in an open channel depends on the velocity of the fluid. When the velocity get to a critical level, the flow shifts from a laminar to a turbulent one. It is however difficult to see water flows with the naked eye which makes it difficult to demonstrate this phenomena. There are ways to solve this, that is what is called flow visualisation. In this project a visualisation method using small hydrogen bubbles in the flow as tracer particles is investigated.</p><p>The channel design and the visualisation method are investigated experimentally and documented and also compared to existing theory within these areas. The results are deemed satisfactory and the method is documented for further work.</p>
----------------------------------------------------------------------
In diva2:435555 
abstract is: 
<p>The so-called three-band Hubbard model is generally believed to be a good model for fermions on a CuO 2 lattice and to contain key features allowing to understand high-temperature superconductivity in the cuprates. A simpler and more popular model is the one-band Hubbard model. In this thesis,an extended one-band Hubbard model is derived from the three-band model.</p>
<p>First, some mathematical background is given, as well as an introduction to Hubbard-like models. Then, to derive the extended one-band Hubbard model, the CuO 2 lattice is divided into clusters of one Cu and two O sites, and a variant of the Feshbach method is used to replace each such cluster by a single lattice site. To nd a suitable one-band model, all Hamiltonian matrix elements are matched between the two models. Finally, dierent sets of three-band parameters are considered, and it is studied how this aects the parameters in the extended one-band Hubbard model.</p>
<p></p>
mc='thesis,an' c='thesis, an'

partal corrected: diva2:435555: <p>The so-called three-band Hubbard model is generally believed to be a good model for fermions on a CuO 2 lattice and to contain key features allowing to understand high-temperature superconductivity in the cuprates. A simpler and more popular model is the one-band Hubbard model. In this thesis, an extended one-band Hubbard model is derived from the three-band model.</p>
<p>First, some mathematical background is given, as well as an introduction to Hubbard-like models. Then, to derive the extended one-band Hubbard model, the CuO 2 lattice is divided into clusters of one Cu and two O sites, and a variant of the Feshbach method is used to replace each such cluster by a single lattice site. To nd a suitable one-band model, all Hamiltonian matrix elements are matched between the two models. Finally, dierent sets of three-band parameters are considered, and it is studied how this aects the parameters in the extended one-band Hubbard model.</p>
<p></p>
w='dierent' val={'c': 'different', 's': ['diva2:550706', 'diva2:618564', 'diva2:571139', 'diva2:1083484', 'diva2:787494', 'diva2:560956', 'diva2:610360', 'diva2:612188', 'diva2:1110812', 'diva2:650621', 'diva2:814324', 'diva2:550016', 'diva2:1110830', 'diva2:572052', 'diva2:642343', 'diva2:435555', 'diva2:651260', 'diva2:550688', 'diva2:558033', 'diva2:720261', 'diva2:891472', 'diva2:853494', 'diva2:740383', 'diva2:783984', 'diva2:796765', 'diva2:550437', 'diva2:1109484', 'diva2:561900', 'diva2:561907', 'diva2:1231306', 'diva2:1183391:', 'diva2:488441', 'diva2:501400', 'diva2:561554', 'diva2:1140141', 'diva2:667759', 'diva2:1130148', 'diva2:787495', 'diva2:558593', 'diva2:562861', 'diva2:1334273', 'diva2:1350191', 'diva2:692295', 'diva2:813063', 'diva2:719811', 'diva2:571135', 'diva2:1120599', 'diva2:661737', 'diva2:1182890', 'diva2:1130095', 'diva2:430778', 'diva2:618600', 'diva2:561939', 'diva2:1141679', 'diva2:618581', 'diva2:1163145', 'diva2:560966', 'diva2:458876', 'diva2:1189541', 'diva2:560862', 'diva2:1768252'], 'n': 'missing ligature'}
w='aects' val={'c': 'affects', 's': ['diva2:1078078', 'diva2:435555', 'diva2:740383', 'diva2:501400', 'diva2:787495', 'diva2:503940']}

corrected abstract:
<p>The so-called three-band Hubbard model is generally believed to be a good model for fermions on a CuO 2 lattice and to contain key features allowing to understand high-temperature superconductivity in the cuprates. A simpler and more popular model is the one-band Hubbard model. In this thesis, an extended one-band Hubbard model is derived from the three-band model.</p>
<p>First, some mathematical background is given, as well as an introduction to Hubbard-like models. Then, to derive the extended one-band Hubbard model, the CuO 2 lattice is divided into clusters of one Cu and two O sites, and a variant of the Feshbach method is used to replace each such cluster by a single lattice site. To nd a suitable one-band model, all Hamiltonian matrix elements are matched between the two models. Finally, different sets of three-band parameters are considered, and it is studied how this affects the parameters in the extended one-band Hubbard model.</p>
<p></p>
In diva2:435555 
abstract is: 
<p>The so-called three-band Hubbard model is generally believed to be a good model for fermions on a CuO 2 lattice and to contain key features allowing to understand high-temperature superconductivity in the cuprates. A simpler and more popular model is the one-band Hubbard model. In this thesis,an extended one-band Hubbard model is derived from the three-band model.</p>
<p>First, some mathematical background is given, as well as an introduction to Hubbard-like models. Then, to derive the extended one-band Hubbard model, the CuO 2 lattice is divided into clusters of one Cu and two O sites, and a variant of the Feshbach method is used to replace each such cluster by a single lattice site. To nd a suitable one-band model, all Hamiltonian matrix elements are matched between the two models. Finally, dierent sets of three-band parameters are considered, and it is studied how this aects the parameters in the extended one-band Hubbard model.</p>
<p></p>

corrected abstract:
<p>The so-called three-band Hubbard model is generally believed to be a good model for fermions on a CuO<sub>2</sub> lattice and to contain key features allowing to understand high-temperature superconductivity in the cuprates. A simpler and more popular model is the one-band Hubbard model. In this thesis, an extended one-band Hubbard model is derived from the three-band model.</p>
<p>First, some mathematical background is given, as well as an introduction to Hubbard-like models. Then, to derive the extended one-band Hubbard model, the CuO<sub>2</sub> lattice is divided into clusters of one Cu and two O sites, and a variant of the Feshbach method is used to replace each such cluster by a single lattice site. To find a suitable one-band model, all Hamiltonian matrix elements are matched between the two models. Finally, different sets of three-band parameters are considered, and it is studied how this affects the parameters in the extended one-band Hubbard model.</p>
----------------------------------------------------------------------
In diva2:1380334 
abstract is: 
<p>A Scania S-series front chassis module feasibility study is carried out to investigate the potential gains and losses by changing to a composite material system. The existing front chassis module comprises multiple steel, sheet metal and plastic components. The design space is fixed by the location of adjacent components in the current design. A new methodology is put together on the basis of "The GAP Methodology: A new way to design composite structures" by F. Neveu et al. (2019)[1] for facilitating the complex nature of multivariable composite structures design. By applying the methodology a set of hand sketches based on vari-ous geometry classes and applicable manufacturing processes can be created for a technical screening, where one concept is brought forward for detailed analysis. The concept design is refined by the use of the surface modeller tool in CATIA V5 and a structural analysis is undertaken using the finite element method software for composites ANSYS ACP. The composite laminate layup is designed by using aerospace design rules as guidelines for the given material system. The proposed design solution satisfies the design requirements and improves the benchmark Scania Chassis module by lowering the amount of components with about 30%, has a recommended metal to composite joining method, reduces the mass by around 40% (53.5% excluded the suggested joining method) and has a safety factor to material failure strains. The feasibility study demonstrates that the proposed methodology and design of the new composite chassis component is plausible using a simplified analysis.</p>

corrected abstract:
<p>A Scania S-series front chassis module feasibility study is carried out to investigate the potential gains and losses by changing to a composite material system. The existing front chassis module comprises multiple steel, sheet metal and plastic components. The design space is fixed by the location of adjacent components in the current design.</p><p>A new methodology is put together on the basis of "The GAP Methodology: A new way to design composite structures" by F. Neveu et al. (2019)[1] for facilitating the complex nature of multi-variable composite structures design. By applying the methodology a set of hand sketches based on various geometry classes and applicable manufacturing processes can be created for a technical screening, where one concept is brought forward for detailed analysis. The concept design is refined by the use of the surface modeller tool in CATIA V5 and a structural analysis is undertaken using the finite element method software for composites ANSYS ACP. The composite laminate lay-up is designed by using aerospace design rules as guidelines for the given material system.</p><p>The proposed design solution satisfies the design requirements and improves the benchmark Scania Chassis module by lowering the amount of components with about 30%, has a recommended metal to composite joining method, reduces the mass by around 40% (53.5% excluded the suggested joining method) and has a safety factor to material failure strains. The feasibility study demonstrates that the proposed methodology and design of the new composite chassis component is plausible using a simplified analysis.</p>
----------------------------------------------------------------------
In diva2:813229   - correct as is

Note:
w='swopped' val={'c': 'swapped', 's': 'diva2:813229', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1287120 
abstract is: 
<p>The mast is a key element in the behavior of a sail boat. It impacts the overall dynamic when sailing and gives the lifting shape to the sail generating the driving force. The objective of this thesis is to design a new mast that can equal or surpass the behavior of a competing mast, most widespread and appreciated mast by the sailors today.The focus of this study was set on the design of the composite layup for a given mast shape. The mast was modeled in a Computer Assisted Design software. Thanks to this numerical model, the composite layup was optimized manually so the mast would reach the deformation specifications given by the sailors and validate navigation, competition and manufacturing constraints.To specify design criteria, the competing mast was retro-engineered. A first set of mast designs were explored and one was chosen according to the criteria. This led to the fine-tuning of the layup and manufacturing of a first mast that was tested by the sailors. The feedback from the sailors was encouraging regarding previous design tentative, but the mast was actually too flexible with respect to the deformation specifications. A second mast was designed very close to the sailors’ target to confirm the feedback given on the first mast.The results of the two masts showed an improvement of the behavior, but not sufficient. The CAD model predicts quite well the deformations out of manufacturing and the masts can now be designed directly to the sailors’ specifications. The validation cases are not sufficient to predict the mast’s behavior. Hence, the optimization will be automated to generate and test numerically a high quantity of masts. The focus will then be set on dynamic and structural-aerodynamic co-simulation to improve the predictability of the behavior.</p>

corrected abstract:
<p>The mast is a key element in the behavior of a sail boat. It impacts the overall dynamic when sailing and gives the lifting shape to the sail generating the driving force. The objective of this thesis is to design a new mast that can equal or surpass the behavior of a competing mast, most widespread and appreciated mast by the sailors today.</p><p>The focus of this study was set on the design of the composite layup for a given mast shape. The mast was modeled in a Computer Assisted Design software. Thanks to this numerical model, the composite layup was optimized manually so the mast would reach the deformation specifications given by the sailors and validate navigation, competition and manufacturing constraints.</p><p>To specify design criteria, the competing mast was retro-engineered. A first set of mast designs were explored and one was chosen according to the criteria. This led to the fine-tuning of the layup and manufacturing of a first mast that was tested by the sailors. The feedback from the sailors was encouraging regarding previous design tentative, but the mast was actually too flexible with respect to the deformation specifications. A second mast was designed very close to the sailors’ target to confirm the feedback given on the first mast.</p><p>The results of the two masts showed an improvement of the behavior, but not sufficient. The CAD model predicts quite well the deformations out of manufacturing and the masts can now be designed directly to the sailors’ specifications. The validation cases are not sufficient to predict the mast’s behavior. Hence, the optimization will be automated to generate and test numerically a high quantity of masts. The focus will then be set on dynamic and structural-aerodynamic co-simulation to improve the predictability of the behavior.</p>

Note: I assmmmmed that terminal punction without a space after it is the start of a new paragraph.
----------------------------------------------------------------------
In diva2:1900878 
abstract is: 
<p>Today, the Swedish Armed Forces are using mainly two sonobuoys targeting objects in the water, Sonobuoy 225B and Sonobuoy 225C. The products are obsolete and in need of modernisation. The purpose of the project was to design, develop and test concepts related to the next generation sonobuoy, where the whole process was performed in close collaboration with EmbeddedArt as well as two master students from Integrated Product Design. The authors’ work focused on hydroacoustic optimisation of the two most acoustically crucial subsystems, being the housing attaching the hydrophones to the acoustic array as well as the casing protecting the element from impacts. The other group focused on the overall design of the buoy, involving the body itself and folding mechanism.</p><p>Two main goals were defined to achieve the project’s purpose. First, to construct a prototype of the sonobuoy, focused on material choices, design, and construction of critical subsystems taking a hydroacoustic point of view, and CAD modelling to finalise the prototype. Second, experiments were conducted to investigate acoustic windows of materials in water, acoustic shadowing and sound propagation through the prototype, as well as an acoustic comparison between the existing casing and our casing.</p><p>Knowing the sound needed to be absorbed between the hydrophone and housing, a material with high sound absorption and an impedance mismatch from water’s impedance, representing how easy it is for the sound wave to propagate through the material, was required. This resulted in three different material choices, being natural rubber, cellular EPDM rubber and silicone. Furthermore, the casing needed to be rigid and robust while still ensuring an eﬀicient transmission of sound waves, which resulted in PPA, PC and ABS+PC.</p><p>The first experiment investigated the acoustic properties of different materials underwater, resulted in PC as the choice of material for the casing due to least impact on the stable peak of voltage amplitude compared to its corresponding measurement with no material. Furthermore, cellular EPDM rubber was chosen as adapter between housing and hydrophone revealing the biggest mismatch in voltage amplitude from water.</p><p>The prototype was CAD-modelled and built in order to perform the second and third experiments, investigating the acoustic shadowing and sound propagation through the prototype, as well as an acoustic comparison between the existing casing and our designed casing. The experiments showed that the casing did not reduce the incoming signal significantly. However, the central hub did reduce signals by around 5 dB. The work laid a solid foundation for further research and development within the field, suggesting that conducting experiments at lower frequencies and performing a Finite Element Method analysis of the design would provide deeper insights into the development of the next generation sonobuoy.</p>

corrected abstract:
<p>Today, the Swedish Armed Forces are using mainly two sonobuoys targeting objects in the water, Sonobuoy 225B and Sonobuoy 225C. The products are obsolete and in need of modernisation. The purpose of the project was to design, develop and test concepts related to the next generation sonobuoy, where the whole process was performed in close collaboration with EmbeddedArt as well as two master students from Integrated Product Design. The authors’ work focused on hydroacoustic optimisation of the two most acoustically crucial subsystems, being the housing attaching the hydrophones to the acoustic array as well as the casing protecting the element from impacts. The other group focused on the overall design of the buoy, involving the body itself and folding mechanism.</p><p>Two main goals were defined to achieve the project’s purpose. First, to construct a prototype of the sonobuoy, focused on material choices, design, and construction of critical subsystems taking a hydroacoustic point of view, and CAD modelling to finalise the prototype. Second, experiments were conducted to investigate acoustic windows of materials in water, acoustic shadowing and sound propagation through the prototype, as well as an acoustic comparison between the existing casing and our casing.</p><p>Knowing the sound needed to be absorbed between the hydrophone and housing, a material with high sound absorption and an impedance mismatch from water’s impedance, representing how easy it is for the sound wave to propagate through the material, was required. This resulted in three different material choices, being natural rubber, cellular EPDM rubber and silicone. Furthermore, the casing needed to be rigid and robust while still ensuring an efficient transmission of sound waves, which resulted in PPA, PC and ABS+PC.</p><p>The first experiment investigated the acoustic properties of different materials underwater, resulted in PC as the choice of material for the casing due to least impact on the stable peak of voltage amplitude compared to its corresponding measurement with no material. Furthermore, cellular EPDM rubber was chosen as adapter between housing and hydrophone revealing the biggest mismatch in voltage amplitude from water.</p><p>The prototype was CAD-modelled and built in order to perform the second and third experiments, investigating the acoustic shadowing and sound propagation through the prototype, as well as an acoustic comparison between the existing casing and our designed casing. The experiments showed that the casing did not reduce the incoming signal significantly. However, the central hub did reduce signals by around 5 dB. The work laid a solid foundation for further research and development within the field, suggesting that conducting experiments at lower frequencies and performing a Finite Element Method analysis of the design would provide deeper insights into the development of the next generation sonobuoy.</p>

Note only one change:
w='eﬀicient' val={'c': 'efficient', 's': ['diva2:1832739', 'diva2:1900878', 'diva2:1673571'], 'n': 'ligature'}
----------------------------------------------------------------------
In diva2:1817476   - correct as is

Note: Spelling error in original:
w='PUGH' val={'c': 'Pugh', 's': 'diva2:1817476', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1701166 
Note: no full text in DiVA

abstract is: 
<p>Surface plasmon resonance (SPR) based sensor generally has high sensitivity, good stability, and rapid detection speed. The technology has been widely used in chemistry, biotechnology, and many other testing fields. With finite-element method (FEM), we numerically investigate in this thesis project performances of a new type of SPR sensors. Two detailed models, both based on the Kretschmann configuration, are investigated. In the first design, a thin continuous WS2 layer is added on top of gold film to increase the sensitivity; and in the second design, the gold film is patterned with an array of holes, which are filled with WS2 material. Through 2D full-wave electromagnetic simulation, sensitivity, full width at half maximum (FWHM), and figure of merit (FOM) for both models are obtained. The first design is found to have the narrowest FWHM but lower sensitivity. The second design is fould to have higher sensitivity but larger FWHM. Corresponding 3D model has also been built to verify the second design.</p>


corrected abstract:
<p>Surface plasmon resonance (SPR) based sensor generally has high sensitivity, good stability, and rapid detection speed. The technology has been widely used in chemistry, biotechnology, and many other testing fields. With finite-element method (FEM), we numerically investigate in this thesis project performances of a new type of SPR sensors. Two detailed models, both based on the Kretschmann configuration, are investigated. In the first design, a thin continuous WS2 layer is added on top of gold film to increase the sensitivity; and in the second design, the gold film is patterned with an array of holes, which are filled with WS2 material. Through 2D full-wave electromagnetic simulation, sensitivity, full width at half maximum (FWHM), and figure of merit (FOM) for both models are obtained. The first design is found to have the narrowest FWHM but lower sensitivity. The second design is found to have higher sensitivity but larger FWHM. Corresponding 3D model has also been built to verify the second design.</p>
----------------------------------------------------------------------
In diva2:1787791   - correct as is

Note: Error in original:
w='deigned' val={'c': 'designed', 's': 'diva2:1787791', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:872192   - correct as is

Note error in original:
w='responsability' val={'c': 'responsibility', 's': 'diva2:872192', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1679061 
abstract is: 
<p>The analytical modelling of complex turbulent airflow remains one of the great unsolved mysteries of physics, but in this paper two widely used Reynolds Averaged Navier-Stokes models (k-$\epsilon$ and k-$\omega$ SST) are compared while designing a heat exchanger for the KTH Formula Student electric race car. CAD software was used to design lattices for the heat exchanger core and theorise about how to increase heat transfer while also taking into account the utilisation of metal additive manufacturing. The models were then analysed using Computational Fluid Dynamics to determine their characteristics as well as the effects of the two turbulence models. </p><p>It was found that the first iteration of the second design performed best in terms of pressure drop and generating turbulent kinetic energy closely followed by the second iteration of the second design and the second iteration of the first design. When comparing the turbulence models the results indicated agreement with their theoretical foundations. The first model overestimating turbulent kinetic energy relative to the second, which picked up more detail of near-wall turbulence thanks to better boundary layer formulation. Future work includes improving the simulation setup, correlating the results with wind tunnel testing and further evaluating more complex designs.</p>

corrected abstract:
<p>The analytical modelling of complex turbulent airflow remains one of the great unsolved mysteries of physics, but in this paper two widely used Reynolds Averaged Navier-Stokes models (𝑘-<em<&epsilon;</em> and 𝑘-<em>&omega;</em> SST) are compared while designing a heat exchanger for the KTH Formula Student electric race car. CAD software was used to design lattices for the heat exchanger core and theorise about how to increase heat transfer while also taking into account the utilisation of metal additive manufacturing. The models were then analysed using Computational Fluid Dynamics to determine their characteristics as well as the effects of the two turbulence models.</p><p>It was found that the first iteration of the second design performed best in terms of pressure drop and generating turbulent kinetic energy closely followed by the second iteration of the second design and the second iteration of the first design. When comparing the turbulence models the results indicated agreement with their theoretical foundations. The first model overestimating turbulent kinetic energy relative to the second, which picked up more detail of near-wall turbulence thanks to better boundary layer formulation. Future work includes improving the simulation setup, correlating the results with wind tunnel testing and further evaluating more complex designs.</p>
----------------------------------------------------------------------
In diva2:1229744 
abstract is: 
<p>The project consists of characterization of the sound from and simulations of a silencer for a centrifugal fan. The sound characterization consisted of 1/3 octaveband sound power meassurements and narrowband meassurements for both inlet and outlet sound. The measurements were conducted in a reverberation chamber. The sound power from the outlet was 5 dB to 10 dB larger than from the inlet, with the largest difference between 80 Hz and 400 Hz. In the simulations the performance of a silencer based on the so called Cremer impedance were investigated. Simulations show up to 20 dB dampening at low frequencies, with rapidly decreasing performance at higher frequencies.</p>

corrected abstract:
<p>The project consists of characterization of the sound from and simulations of a silencer for a centrifugal fan. The sound characterization consisted of 1/3 octave band sound power measurements and narrowband measurements for both inlet and outlet sound. The measurements were conducted in a reverberation chamber. The sound power from the outlet was 5 dB to 10 dB larger than from the inlet, with the largest difference between 80 Hz and 400 Hz. In the simulations the performance of a silencer based on the so called Cremer impedance were investigated. Simulations show up to 20 dB dampening at low frequencies, with rapidly decreasing performance at higher frequencies.</p>
----------------------------------------------------------------------
In diva2:1230653 
Note: no full text in DiVA

abstract is: 
<p>The aim of this project is to design and build a demonstration apparatus consisting of alevitating globe that demonstrates how the use of the magnetic force and the principleof a feedback loop can create a semi-stable state. The fundamental components of thesystem is a metallic shell, coil, light emitting diode(LED) and photodiode. The main partof the report is focusing on the design of the coil and the electric circuit that controlsthe current through the coil.</p>

corrected abstract:
<p>The aim of this project is to design and build a demonstration apparatus consisting of a levitating globe that demonstrates how the use of the magnetic force and the principle of a feedback loop can create a semi-stable state. The fundamental components of thesystem is a metallic shell, coil, light emitting diode(LED) and photodiode. The main partof the report is focusing on the design of the coil and the electric circuit that controls the current through the coil.</p>
----------------------------------------------------------------------
In diva2:896301 
abstract is: 
<p>This thesis proposes a novel anomaly detection algorithm for detect-ing anomalies in high-dimensional, multimodal, real-valued time se-ries data. The approach, requiring no domain knowledge, is based on Stochastic Recurrent Networks (STORNs), a universal distribution approximator for sequential data leveraging the power of Recurrent Neural Networks (RNNs) and Variational Auto-Encoders (VAEs).</p><p>The detection algorithm is evaluated on real robot time series data in order to prove that the method robustly detects anomalies off- and on-line.</p>

corrected abstract:
<p>This thesis proposes a novel anomaly detection algorithm for detecting anomalies in high-dimensional, multimodal, real-valued time series data. The approach, requiring no domain knowledge, is based on Stochastic Recurrent Networks (STORNs), a universal distribution approximator for sequential data leveraging the power of Recurrent Neural Networks (RNNs) and Variational Auto-Encoders (VAEs).</p><p>The detection algorithm is evaluated on real robot time series data in order to prove that the method robustly detects anomalies off- and on-line.</p>
----------------------------------------------------------------------
In diva2:1033800 
abstract is: 
<p>Abstract—Uranium compounds are used in many nuclear</p><p>power plants, and the neutron-induced fission reactions produce</p><p>many decay products. Some of these are of interest from an</p><p>efficiency or safety point of view. For this reason, it is important to</p><p>quantitatively and qualitatively know these nuclides.The objective</p><p>of this study is to experimentally determine these properties</p><p>through neutron activation and gamma ray spectroscopy. Using</p><p>a germanium semiconductor detector, gamma rays emitted from</p><p>fission daughter isotopes produced from neutron irradiation of</p><p>a uranium sample are measured. The resulting spectra are</p><p>analyzed, proving the existence of fission daughter isotopes in</p><p>the specimen. The data also provides experimental estimations</p><p>of initial half-life of these isotopes.</p>

corrected abstract:
<p>Uranium compounds are used in many nuclear power plants, and the neutron-induced fission reactions produce many decay products. Some of these are of interest from an efficiency or safety point of view. For this reason, it is important to quantitatively and qualitatively know these nuclides. The objective of this study is to experimentally determine these properties through neutron activation and gamma ray spectroscopy. Using a germanium semiconductor detector, gamma rays emitted from fission daughter isotopes produced from neutron irradiation of a uranium sample are measured. The resulting spectra are analyzed, proving the existence of fission daughter isotopes in the specimen. The data also provides experimental estimations of initial half-life of these isotopes.</p>
----------------------------------------------------------------------
In diva2:852504   - correct as is

Note: spelling error in original: 
w='positon' val={'c': 'position', 's': 'diva2:852504', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:651680 
abstract is: 
<p>The micro Seyfert galaxy NGC-4395 hosts a supermassive black hole at its center that</p><p>actively accrete matter and thus radiates very strongly in the X-ray range. Denoted</p><p>the active galactic nucleus this thesis focuses on determining its spin. The spin parameter</p><p>corresponds to the angular momentum of the black hole and the properties of the</p><p>accretion disk surrounding the black hole.</p><p>Our spectrum was obtained from satellite telescope XMM-Newton and was analysed</p><p>in the range of</p><p>3 to 10 keV using the X-ray spectral fitting software XSPEC with the</p><p>standard models provided. A good fit was found, and therefore a good description of the</p><p>parameters describing the black hole and its disk. These start values were used for the</p><p>far more advance model</p><p>KERRCONV*REFLIONX which contains the spin.</p><p>Specially noteworthy is the broad and narrow Iron</p><p>Kemission lines normally occurring</p><p>at</p><p>6:4 keV whose form tells which different relativistic or gravitational effects that</p><p>affects the emissivity. In our spectrum both the broad and narrow Fe emission lines were</p><p>prominent as shown by the significant improvement in</p><p>2. Furthermore the reflected</p><p>component of our fit completely dominated our spectra and a majority of the emission</p><p>originates from the innermost regions of the disk, probably caused by strong gravitational</p><p>light bending revealing a closely positioned corona. The high value obtained on the spin</p><p>parameter seems to be consistent with the relatively low mass and possibly implies a</p><p>coherent accretion during the black holes formation.</p>

corrected abstract:
<p>The micro Seyfert galaxy NGC-4395 hosts a supermassive black hole at its center that actively accrete matter and thus radiates very strongly in the X-ray range. Denoted the active galactic nucleus this thesis focuses on determining its spin. The spin parameter corresponds to the angular momentum of the black hole and the properties of the accretion disk surrounding the black hole.</p><p>Our spectrum was obtained from satellite telescope XMM-Newton and was analysed in the range of 3 to 10 keV using the X-ray spectral fitting software XSPEC with the standard models provided. A good fit was found, and therefore a good description of the parameters describing the black hole and its disk. These start values were used for the far more advance model <span style="font-variant: small-caps;">KERRCONV*REFLIONX</span> which contains the spin.</p><p>Specially noteworthy is the broad and narrow Iron K<sub>α</sub> emission lines normally occurring at 6.4 keV whose form tells which different relativistic or gravitational effects that affects the emissivity. In our spectrum both the broad and narrow Fe emission lines were prominent as shown by the significant improvement in <em>&chi;</em><sup>2</sup>. Furthermore the reflected component of our fit completely dominated our spectra and a majority of the emission originates from the innermost regions of the disk, probably caused by strong gravitational light bending revealing a closely positioned corona. The high value obtained on the spin parameter seems to be consistent with the relatively low mass and possibly implies a coherent accretion during the black holes formation.</p>
----------------------------------------------------------------------
In diva2:1894671 
abstract is: 
<p>Challenging competition in the aviation industry requires flexibility in airline prognosis models. At SAS Ground Handling, the staff needs are predicted using a complex and time-consuming optimisation system which generates a value of full-time employees (FTE). To understand the driving forces behind the FTE-value a multiple linear regression model is developed. The model is built on simulations where 9-45 flights in a week are removed. Using six variables derived from flight schedules, we are able to explain 89 % of the variance in FTE outcomes. The model combines several factors such as aircraft type, turn-around times, origins and destinations. We find that propellers are found to be statiscally significant. Further, short turn-arounds (&lt;1 hr) and long turn-arounds (&gt;2 hrs) help predicting FTE. Our results show that flights originating from Sweden and European charter flights have a significant effect. Yet, the most considerable variable is intercontinental flights which stands for the most decisive coefficient. The results are valuable when interpreting and validating optimisation outcomes and provide important insights when analysing the effects of changes in flight schedules.</p>

corrected abstract:
<p>Challenging competition in the aviation industry requires flexibility in airline prognosis models. At SAS Ground Handling, the staff needs are predicted using a complex and time-consuming optimisation system which generates a value of full-time employees (FTE). To understand the driving forces behind the FTE-value a multiple linear regression model is developed. The model is built on simulations where 9-45 flights in a week are removed. Using six variables derived from flight schedules, we are able to explain 89 % of the variance in FTE outcomes. The model combines several factors such as aircraft type, turn-around times, origins and destinations. We find that propellers are found to be statiscally significant. Further, short turn-arounds (&lt;1 hr) and long turn-arounds (&gt;2 hrs) help predicting FTE. Our results show that flights originating from Sweden and European charter flights have a significant effect. Yet, the most considerable variable is intercontinental flights which stands for the most decisive coefficient. The results are valuable when interpreting and validating optimisation outcomes and provide important insights when analysing the effects of changes in flight schedules.</p>

Note - speeling error:
w='statiscally' val={'c': 'statistically', 's': 'diva2:1894671', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:893813 
abstract is: 
<p>Since the last ten to 15 years, railway operators have to face new challenges which are an increased demand on comfort and high safety requirements, increased energy costs and the high competitive market situation. German Aerospace Center, DLR is designing a novel concept of an ultra-high-speed train with distributed propulsion system—the Next Generation Train (NGT). It consists of two Power units and eight powered intermediate cars. The intermediate cars are characterised by powered single-wheel single running gears supporting the low-floor concept within the whole train, re-sulting in both effective usage of the inner space but less space for the run-ning gears itself. Additionally, the traditional guiding system with wheel-sets cannot be applied.  The first goal of this Master’s thesis is the analysis of the state of the art for unconventional running and driving gears to get an overview of possi-ble and operating technologies. Based on this research and previous spec-ifications for the train, a technical specification for the running gear was created. This led to the development of a number of different concepts un-der consideration of the available space and an optimised arrangement of relevant components. After a conclusion and evaluation of these concepts, a CAD implementation of the relevant structural elements of the selected variant is the final goal of this thesis. Additionally to the technical specifi-cations, the focus lies on lightweight design to support the stringent weight concept of the NGT. However, the accessibility for assembling and mainte-nance is considered, too. The result is a CAD model of only one of doubtless more than a dozen possible variants. Hence, possible changes at specific parts are addressed; advantages and disadvantages were discussed. Finally, the concept was evaluated on the basis of the underlying technical specification.</p>

corrected abstract:
<p>Since the last ten to 15 years, railway operators have to face new challenges which are an increased demand on comfort and high safety requirements, increased energy costs and the high competitive market situation. German Aerospace Center, DLR is designing a novel concept of an ultra-high-speed train with distributed propulsion system&mdash;the Next Generation Train (NGT). It consists of two Power units and eight powered intermediate cars. The intermediate cars are characterised by powered single-wheel single running gears supporting the low-floor concept within the whole train, resulting in both effective usage of the inner space but less space for the running gears itself. Additionally, the traditional guiding system with wheel-sets cannot be applied.</p><p>The first goal of this Master’s thesis is the analysis of the state of the art for unconventional running and driving gears to get an overview of possible and operating technologies. Based on this research and previous specifications for the train, a technical specification for the running gear was created. This led to the development of a number of different concepts under consideration of the available space and an optimised arrangement of relevant components. After a conclusion and evaluation of these concepts, a CAD implementation of the relevant structural elements of the selected variant is the final goal of this thesis. Additionally to the technical specifications, the focus lies on lightweight design to support the stringent weight concept of the NGT. However, the accessibility for assembling and maintenance is considered, too.</p><p>The result is a CAD model of only one of doubtless more than a dozen possible variants. Hence, possible changes at specific parts are addressed; advantages and disadvantages were discussed. Finally, the concept was evaluated on the basis of the underlying technical specification.</p>
----------------------------------------------------------------------
In diva2:1852460 
abstract is: 
<p>The main goal of this thesis is to build a mathematical model of a Gblss-y freight wagon with the Powell Duffryn TF25SA running gear which corresponds reasonably to reality. The model is verified with dynamic vehicle reactions measured on track.</p><p>The TF25SA running gear allows a maximum speed of 160 km/h with an axle load of 20 tons. The running gear is tested in Sweden according to UIC 518 and used on Gblss-y freight wagons by Green Cargo in overnight postal transport.</p><p>Necessary information and data on the running gear have been provided by Powell Duffryn Rail.</p><p>Necessary data on the measurements have been provided by TrainTech Engineering Sweden AB.</p><p>Data on the track irregularities have been provided by the Swedish National Rail Administration, Banverket.</p><p>The present vehicle model is build in the simulation software GENSYS as a rigid multibody model including the carbody. Also the track is modelled.</p><p>In this thesis studies have been carried out to examine the vehicle’s comfort characteristics and its behaviour on different tracks.The model give reasonable results concerning vertical and lateral dynamics. However the model does not agree that well with the measurements concerning ride comfort characteristics in the lateral direction.</p>

corrected abstract:
<p>The main goal of this thesis is to build a mathematical model of a Gblss-y freight wagon with the Powell Duffryn TF25SA running gear which corresponds reasonably to reality. The model is verified with dynamic vehicle reactions measured on track.</p><p>The TF25SA running gear allows a maximum speed of 160 km/h with an axle load of 20 tons. The running gear is tested in Sweden according to UIC 518 and used on Gblss-y freight wagons by Green Cargo in overnight postal transport.</p><p>Necessary information and data on the running gear have been provided by Powell Duffryn Rail.</p><p>Necessary data on the measurements have been provided by TrainTech Engineering Sweden AB.</p><p>Data on the track irregularities have been provided by the Swedish National Rail Administration, Banverket.</p><p>The present vehicle model is build in the simulation software GENSYS as a rigid multibody model including the carbody. Also the track is modelled.</p><p>In this thesis studies have been carried out to examine the vehicle’s comfort characteristics and its behaviour on different tracks.</p><p>The model give reasonable results concerning vertical and lateral dynamics. However the model does not agree that well with the measurements concerning ride comfort characteristics in the lateral direction.</p>

Note the only change was to add a paragraph break following the terminal punctuation without a following space.
----------------------------------------------------------------------
In diva2:508575 
abstract is: 
<p>The profession of staircase builder is a hard job. Technically complex, it requires solid skills and certain know-how to get good quality results and to be able to compete commercially with low-cost precast stair manufacturers. Towards the expansion of that discount market, the trade of traditional construction has evolved a lot over the past few years. For sake of optimizing Cost-Quality-Delivery criterions, Choquet Co., the company I have worked with has practiced continuous improvement for more than a decade, making strategic decisions to find the best concept for stairs construction. The present study returns to all the history of concrete staircase building, the improvements made or to make to ease the realizations of those constructions, to get high finish grade and accelerate the proceedings. In fact, despite the expansion of our technologies and of our control on materials, this trade remains hard to handle and is often realized by unskilled masons and in an archaic way. This can be explain culturally by the fact that the building trade is in France a family tradition, developed on very old uses that people of this world tend to keep alive. The main part of the study is about the conception of innovative mechanisms aiming at the realization of formworks for straight and winder stairs. A particular attention has been dedicated to mechanical issues, choice of materials and technological solutions to face problems which include robustness, fatigue, weight, reliability and fouling matters. Starting the conception procedure by ideas, drafts and inventive drives, we then produced set of specifications to segment and delimit the requirements of our relative products. Afterwards, we could start the design by realizing some easy models from which we could evaluate the future lacks and so the fields in need of more accurate studies. FEM analyses have been performed on the key parts of the mechanism, so that we could refine some geometrical and material parameters. In addition, a presentation will be made individually on all the set of tools thought to be parts of the equipments of the perfect staircase builder. From moulds to the workshop truck, it is a real concept of a profession that is still not recognized in a world that is pretty hard to invest but that could bring a real change in the way to build in the future.</p>

corrected abstract:
<p>The profession of staircase builder is a hard job. Technically complex, it requires solid skills and certain know-how to get good quality results and to be able to compete commercially with low-cost precast stair manufacturers. Towards the expansion of that discount market, the trade of traditional construction has evolved a lot over the past few years. For sake of optimizing Cost-Quality-Delivery criterions, Choquet Co., the company I have worked with has practiced continuous improvement for more than a decade, making strategic decisions to find the best concept for stairs construction.</p><p>The present study returns to all the history of concrete staircase building, the improvements made or to make to ease the realizations of those constructions, to get high finish grade and accelerate the proceedings. In fact, despite the expansion of our technologies and of our control on materials, this trade remains hard to handle and is often realized by unskilled masons and in an archaic way. This can be explain culturally by the fact that the building trade is in France a family tradition, developed on very old uses that people of this world tend to keep alive.</p><p>The main part of the study is about the conception of innovative mechanisms aiming at the realization of formworks for straight and winder stairs. A particular attention has been dedicated to mechanical issues, choice of materials and technological solutions to face problems which include robustness, fatigue, weight, reliability and fouling matters. Starting the conception procedure by ideas, drafts and inventive drives, we then produced set of specifications to segment and delimit the requirements of our relative products. Afterwards, we could start the design by realizing some easy models from which we could evaluate the future lacks and so the fields in need of more accurate studies. FEM analyses have been performed on the key parts of the mechanism, so that we could refine some geometrical and material parameters.</p><p>In addition, a presentation will be made individually on all the set of tools thought to be parts of the equipments of the perfect staircase builder. From moulds to the workshop truck, it is a real concept of a profession that is still not recognized in a world that is pretty hard to invest but that could bring a real change in the way to build in the future.</p>

Note - the only change was to break into paragraphs to match the original.
Note - error in original:
w='criterions' val={'c': 'criteria', 's': ['diva2:508575', 'diva2:408819']}
----------------------------------------------------------------------
In diva2:1579573 
abstract is: 
<p>X-­ray nanotomography is an imaging technique used to study three­-dimensional structures at submicrometer length scale. The samples to be studied should be only 10s of micrometers in diameter, and ideally cylindrical. Focused ion beam milling is the most common technique used to prepare samples for nanotomography experiments, and the current technique used at the NanoMAX beamline of the MAX IV synchrotron facility. It is a time­-consuming process as preparing one sample can take hours. With the aim of offering a faster, alternative sample preparation method, a CNC milling setup was developed, and is presented here. The CNC setup is based on two spindles placed on precision linear stages - one for the sample and one for the milling tool. The sample is rotated while being trimmed gently by the milling tool, resulting in a small sample cylinder. A Python script generating G­-code commands controls the procedure. The setup was used to trim copper samples down to 18.5 micrometers. Further work is needed to optimize milling parameters in order to reach similar diameters for other sample materials. The developed setup offers a time-­efficient, repeatable and low­-cost sample preparation method for X-­ray nanotomography.</p>

corrected abstract:
<p>X-ray nanotomography is an imaging technique used to study three-dimensional structures at submicrometer length scale. The samples to be studied should be only 10s of micrometers in diameter, and ideally cylindrical. Focused ion beam milling is the most common technique used to prepare samples for nanotomography experiments, and the current technique used at the NanoMAX beamline of the MAX IV synchrotron facility. It is a time-consuming process as preparing one sample can take hours. With the aim of offering a faster, alternative sample preparation method, a CNC milling setup was developed, and is presented here. The CNC setup is based on two spindles placed on precision linear stages - one for the sample and one for the milling tool. The sample is rotated while being trimmed gently by the milling tool, resulting in a small sample cylinder. A Python script generating G-code commands controls the procedure. The setup was used to trim copper samples down to 18.5 micrometers. Further work is needed to optimize milling parameters in order to reach similar diameters for other sample materials. The developed setup offers a time-efficient, repeatable and low-cost sample preparation method for X-ray nanotomography.</p>
----------------------------------------------------------------------
In diva2:1806869 
abstract is: 
<p>Cell culture is a fundamental procedure in many laboratories and precedes much research performed under the microscope. Despite the significance of this procedural stage, the monitoring of cells throughout growth is impossible due to the absence of equipment and methodological approaches. This thesis presents a low-cost, power-effective and versatile microscope with small enough dimensions to operate inside an incubator. Besides image acquisition, the microscope comprises other functions such as a data pipeline, implemented to save the images on the user’s computer via a server whilst also offering storage of the images on an integrated micro SD-card. Furthermore, a machine learning algorithm with a human-in-the-loop approach has been trained to segment the acquired images for cell proliferation and cell apoptosis tracking, and yielded promising results with an accuracy of 94%. For comparison, conventional segmentation techniques using operations such as the watershed function were deployed.The microscope described is versatile in operation as it offers the user to utilise one or more functions, depending on the purpose of the imaging.</p>

corrected abstract:
<p>Cell culture is a fundamental procedure in many laboratories and precedes much research performed under the microscope. Despite the significance of this procedural stage, the monitoring of cells throughout growth is impossible due to the absence of equipment and methodological approaches. This thesis presents a low-cost, power-effective and versatile microscope with small enough dimensions to operate inside an incubator. Besides image acquisition, the microscope comprises other functions such as a data pipeline, implemented to save the images on the user’s computer via a server whilst also offering storage of the images on an integrated micro SD-card. Furthermore, a machine learning algorithm with a human-in-the-loop approach has been trained to segment the acquired images for cell proliferation and cell apoptosis tracking, and yielded promising results with an accuracy of 94%. For comparison, conventional segmentation techniques using operations such as the watershed function were deployed.</p><p>The microscope described is versatile in operation as it offers the user to utilise one or more functions, depending on the purpose of the imaging.</p>

Note: Only change was to add the missing paragraph break.
----------------------------------------------------------------------
In diva2:1903490 
Note: no full text in DiVA

abstract is: 
<p>The structure of the tires used on mine vehicles is made to absorb the deforma- tion caused by the indentation that appears when rolling on rocks. This defor- mation is absorbed thanks to the mechanical properties of rubber-penetrated steel cables present in one of the external layers of the tire structure. The kinematics of these rubber-penetrated cables at the scale of the interaction of the wires of the cable with the rubber has not been investigated so far. This master thesis intends to develop a method to observe the kinematics at the wires-rubber interaction scale thanks to in-situ (inside a tomograph) tensile and indentation experiments. The 3D images provided by the tomograph are analysed via Digital Volume Correlation (DVC) in order to extract the dis- placement field of a rubber-penetrated cable between different loading steps of the in-situ experiments. Two protocols for the in-situ tensile and indenta- tion test as well as an indentation device compatible with the in-situ experi- mentation were developed and tested. The tests were performed on samples composed of one rubber-penetrated cable each and two types of cables were considered in this study. The presence of the indentation device didn't reduce the quality of the tomographic images despite an insufficient indentation of the cable. The images obtained by the tomograph were analysed using a fi- nite element base DVC method developped at the Laboratoire de Mécanique de Paris-Saclay (LMPS) called Correli 3.0. The DVC algorithm converged to- ward a reliable displacement field on the rubber outside of the cable and on the wires for the two types of cables and the two types of test (tensile and indenta- tion tests). The result is more discussable on the rubber that has penetrated the cable because of the presence of artefacts that appear in the 3D reconstruction process of the 3D images. The expulsion of the rubber out of the center of the cable and torsion was observed for the tensile tests. The axial deformation of the cables was extracted from the DVC computed displacement field for both cables and compared to reference data.</p>

corrected abstract:
<p>The structure of the tires used on mine vehicles is made to absorb the deforma- tion caused by the indentation that appears when rolling on rocks. This defor- mation is absorbed thanks to the mechanical properties of rubber-penetrated steel cables present in one of the external layers of the tire structure. The kinematics of these rubber-penetrated cables at the scale of the interaction of the wires of the cable with the rubber has not been investigated so far. This master thesis intends to develop a method to observe the kinematics at the wires-rubber interaction scale thanks to in-situ (inside a tomograph) tensile and indentation experiments. The 3D images provided by the tomograph are analysed via Digital Volume Correlation (DVC) in order to extract the dis- placement field of a rubber-penetrated cable between different loading steps of the in-situ experiments. Two protocols for the in-situ tensile and indenta- tion test as well as an indentation device compatible with the in-situ experi- mentation were developed and tested. The tests were performed on samples composed of one rubber-penetrated cable each and two types of cables were considered in this study. The presence of the indentation device didn't reduce the quality of the tomographic images despite an insufficient indentation of the cable. The images obtained by the tomograph were analysed using a fi- nite element base DVC method developed at the Laboratoire de Mécanique de Paris-Saclay (LMPS) called Correli 3.0. The DVC algorithm converged to- ward a reliable displacement field on the rubber outside of the cable and on the wires for the two types of cables and the two types of test (tensile and indenta- tion tests). The result is more discussable on the rubber that has penetrated the cable because of the presence of artefacts that appear in the 3D reconstruction process of the 3D images. The expulsion of the rubber out of the center of the cable and torsion was observed for the tensile tests. The axial deformation of the cables was extracted from the DVC computed displacement field for both cables and compared to reference data.</p>

Only change was to correct the spelling as per below:
w='developped' val={'c': 'developed', 's': 'diva2:1903490', 'n': 'no full text'}
----------------------------------------------------------------------
In diva2:1127919 
abstract is: 
<p>The SEAL Carrier is a marine hybrid craft with capacity to travel both on the surface and completely submerged with main objective to transport a group of divers. The craft is developed for transporting a unit of combat divers in and out the of area of operation where the high speed at surface and low signature when submerged are key features. In order to reduce the workload for the pilots during longer transports in submerged mode the manufacturer James Fisher Defence (JFD) has instigated two master theses with the aim to develop an automated depth control.This thesis concerns the ÿrst part in this project and describes the development of a hydrodynamic model of the craft. The main purpose of the model is to re°ect the behavior of the craft with focus on depth maneuvers so it can be used as a tool for the development of such a controller.Initially the fundamental theory that this mathematical model is established from is presented and explained. From how positions and motion of an underwater craft is expressed with well established methods the six equa-tions of motion necessary to fully describe these are derived. For estimating the external forces acting on the craft a semi-empirical approach is presented in order to obtain expressions and estimations of these. With these results a platform for the model have been established in the programming environment MATLAB/SIMULINK. With this platform as basis a procedure for manipulating the external forces in order to achieve a better repre-sentation of the real craft is presented.The developed model experiences some di˝culties to capture the motion of the real craft during maneuvers in depth. The cause for this is considered to be because of the methods used for expressing the hydrodynamic forces developed by the hull. The function of the model to serve as a tool for the development of a depth controller is still to be veriÿed and tests are planned. The developed model has a modular design that enables simple modiÿcations of the implemented theory as well as structural modiÿcations for future development and improvements.</p>

corrected abstract:
<p>The SEAL Carrier is a marine hybrid craft with capacity to travel both on the surface and completely submerged with main objective to transport a group of divers. The craft is developed for transporting a unit of combat divers in and out the of area of operation where the high speed at surface and low signature when submerged are key features. In order to reduce the workload for the pilots during longer transports in submerged mode the manufacturer James Fisher Defence (JFD) has instigated two master theses with the aim to develop an automated depth control.</p><p>This thesis concerns the first part in this project and describes the development of a hydrodynamic model of the craft. The main purpose of the model is to reflect the behavior of the craft with focus on depth maneuvers so it can be used as a tool for the development of such a controller.</p><p>Initially the fundamental theory that this mathematical model is established from is presented and explained. From how positions and motion of an underwater craft is expressed with well established methods the six equations of motion necessary to fully describe these are derived. For estimating the external forces acting on the craft a semi-empirical approach is presented in order to obtain expressions and estimations of these. With these results a platform for the model have been established in the programming environment MATLAB/SIMULINK. With this platform as basis a procedure for manipulating the external forces in order to achieve a better representation of the real craft is presented.</p><p>The developed model experiences some difficulties to capture the motion of the real craft during maneuvers in depth. The cause for this is considered to be because of the methods used for expressing the hydrodynamic forces developed by the hull. The function of the model to serve as a tool for the development of a depth controller is still to be verified and tests are planned. The developed model has a modular design that enables simple modifications of the implemented theory as well as structural modifications for future development and improvements.</p>
----------------------------------------------------------------------
In diva2:1795524 
abstract is: 
<p>Noise pollution is a growing concern due to its harmfulness to human health. Heavy vehicles powered by internal combustion engines stands for a major part of the environmental noise, why noise reduction is an increasing priority in enginve development. Within this study, an optimization problem is posed in order to minimize acoustic output without impairing the engine's overall performance.</p><p>In our quest to diversify out noise reduction strategies, innovative ways of investigating this complex subject are essential. Here, we use simulations to investigate the possibility to reduce noise by component settings, as well as methods available to achieve that.</p><p>Regarding the methods, the results indicates that a built in optimization tool within the simulation software used works well, despite the high complexity of the problem. A significant noise reduction is achieved when adjusting the settings of two of the parameters studied.</p><p>This is a first attempt to tackle noise reduction in internal combustion engines by component settings. From the promising results, further improvements are expected as the simulation methods are refined and more components can be investigated accurately.</p>

corrected abstract:
<p>Noise pollution is a growing concern due to its harmfulness to human health. Heavy vehicles powered by internal combustion engines stands for a major part of the environmental noise, why noise reduction is an increasing priority in engine development. Within this study, an optimization problem is posed in order to minimize acoustic output without impairing the engine's overall performance.</p><p>In our quest to diversify out noise reduction strategies, innovative ways of investigating this complex subject are essential. Here, we use simulations to investigate the possibility to reduce noise by component settings, as well as methods available to achieve that.</p><p>Regarding the methods, the results indicates that a built in optimization tool within the simulation software used works well, despite the high complexity of the problem. A significant noise reduction is achieved when adjusting the settings of two of the parameters studied.</p><p>This is a first attempt to tackle noise reduction in internal combustion engines by component settings. From the promising results, further improvements are expected as the simulation methods are refined and more components can be investigated accurately.</p>

Note: The only change was to correct the misspelling as per below:
w='enginve' val={'c': 'enginve', 's': 'diva2:1795524', 'n': 'correct in original'}
----------------------------------------------------------------------
In diva2:1380104 
abstract is: 
<p>In the last few years, with the development of sensor and actuator technology along with increased computation power available on-board of vehicles, the automotive industry is em-ploying more and more mechatronic systems for Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD). Driver Assistance Systems are being used to increase safety (eg. Electronic stability program, lane keep assist etc.), reduce drive fatigue (eg. Electronic power steering) and of increasing vehicle performance and handling (eg. torque vectoring).‚is thesis explores one such driver assistance system, the Active Rear Wheel steering (ARWS) system, which is capable of increase the stability and handling of the vehicle at high speeds and reduce driver fatigue at very low speeds (such as parking manoeuvre). ‚e thesis starts by discussing the history and present state of art of ARWS systems and the control algorithms used for it. ‚en, e‡ort is put in to develop tests and objective metrics to evaluate the per-formance of the system compared to a passive vehicle. ‚ese metrics are of importance in situations where subjective driver feedback is either not available at all (such as computer simulations) or when data is needed to back up the driver feedback (inexperienced drivers).‚ese objective metrics can help the design engineer to evaluate and even predict vehicle’s performance during the design and tuning phase.‚e thesis then deÿnes how the ARWS system should beneÿt the handling of the vehicle along with certain undesired behaviour that may arise due to ARWS and should be avoided.‚is was done based upon feedback from experienced drivers and engineers along with inputs from various literature.‚e Sliding Mode Control (SMC) algorithm is chosen for the control of ARWS system due to it relative simplicity and robust performance. ‚e SMC theory is presented and then the con-troller is developed along with the reference model. ‚e controller is then put in a So›ware-in-Loop environment with IPG CarMaker and put through various test scenarios for tuning and evaluation purposes. ‚e results suggest that the system improves the vehicle’s handling.‚e last part of the thesis looks into the steering feel and its objectiÿcation along with how the AWRS system in…uences the steering feel compared to that of a passive vehicle.</p>

corrected abstract:
<p>In the last few years, with the development of sensor and actuator technology along with increased computation power available on-board of vehicles, the automotive industry is employing more and more mechatronic systems for Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD). Driver Assistance Systems are being used to increase safety (eg. Electronic stability program, lane keep assist etc.), reduce drive fatigue (eg. Electronic power steering) and of increasing vehicle performance and handling (eg. torque vectoring).</p><p>This thesis explores one such driver assistance system, the Active Rear Wheel steering (ARWS) system, which is capable of increase the stability and handling of the vehicle at high speeds and reduce driver fatigue at very low speeds (such as parking manoeuvre). The thesis starts by discussing the history and present state of art of ARWS systems and the control algorithms used for it. Then, effort is put in to develop tests and objective metrics to evaluate the performance of the system compared to a passive vehicle. These metrics are of importance in situations where subjective driver feedback is either not available at all (such as computer simulations) or when data is needed to back up the driver feedback (inexperienced drivers). These objective metrics can help the design engineer to evaluate and even predict vehicle’s performance during the design and tuning phase.</p><p>The thesis then defines how the ARWS system should benefit the handling of the vehicle along with certain undesired behaviour that may arise due to ARWS and should be avoided. This was done based upon feedback from experienced drivers and engineers along with inputs from various literature.</p><p>The Sliding Mode Control (SMC) algorithm is chosen for the control of ARWS system due to it relative simplicity and robust performance. The SMC theory is presented and then the controller is developed along with the reference model. The controller is then put in a Software-in-Loop environment with IPG CarMaker and put through various test scenarios for tuning and evaluation purposes. The results suggest that the system improves the vehicle’s handling.</p><p>The last part of the thesis looks into the steering feel and its objectification along with how the AWRS system influences the steering feel compared to that of a passive vehicle.</p>
----------------------------------------------------------------------
In diva2:721892 
abstract is: 
<p>Diffusion-limited aggregation (DLA) is a model for computer simulation of particle aggregation.</p><p>It is known to generate aggregates with a characteristic appearance with</p><p>many branches spreading in all directions and a specic fractal dimension. The aggregates</p><p>resemble many created by processes in nature such as crystallization, 　uid 　ow and</p><p>growth of bacteria colonies. In this thesis the DLA model and a few variations of it are</p><p>investigated to illuminate its 　exibility. The variations investigated are a noise-reduced</p><p>model, introducing surface tension and increasing the 　ux of particles. The fractal dimension</p><p>is calculated in each case and compared to the results of similar experiments,</p><p>both simulated and real.</p>

corrected abstract:
<p>Diffusion-limited aggregation (DLA) is a model for computer simulation of particle aggregation. It is known to generate aggregates with a characteristic appearance with many branches spreading in all directions and a specific fractal dimension. The aggregates resemble many created by processes in nature such as crystallization, fluid flow and growth of bacteria colonies. In this thesis the DLA model and a few variations of it are investigated to illuminate its flexibility. The variations investigated are a noise-reduced model, introducing surface tension and increasing the flux of particles. The fractal dimension is calculated in each case and compared to the results of similar experiments, both simulated and real.</p>
----------------------------------------------------------------------
In diva2:1218549 
abstract is: 
<p>This project is an analysis of how a planet can be found in space with the aid of mathematics. This is based on the fact that in the 19th century two mathematicians John C. Adams and Urbain Le Verrier both independent of each other found Neptune, the 8th planet in the solar system, by calculating its location based on discrepancies between theoretical and observed longitudes. We recreate Adams’ problem and solve it with numerical analysis to see how one could improve this method of finding a planet using mathematics. We created a model of the solar system using Runge-Kutta 4 (RK4) to solve ODE’s explaining how the planets affect each other. We then created an inverse problem where we pretended that Neptune did not exist and tried to find its position and data using Gauss-Newton’s algorithm.</p><p>Our method gives a better result than those of Adams, although we use a better start guess for the position of Neptune than he did. The important parameter to find is at what direction to look for the planet, also called the longitude angle. Both Adams and us get close to the correct longitude—Adams’ being 2:5_ off and us within 1_. This is especially interesting since without getting this parameter correct they would never have found the planet at that time.</p>

corrected abstract:
<p>This project is an analysis of how a planet can be found in space with the aid of mathematics. This is based on the fact that in the 19<sup>th</sup> century two mathematicians John C. Adams and Urbain Le Verrier both independent of each other found Neptune, the 8<sup>th</sup> planet in the solar system, by calculating its location based on discrepancies between theoretical and observed longitudes.</p><p>We recreate Adams’ problem and solve it with numerical analysis to see how one could improve this method of finding a planet using mathematics. We created a model of the solar system using Runge-Kutta 4 (RK4) to solve ODE’s explaining how the planets affect each other. We then created an inverse problem where we pretended that Neptune did not exist and tried to find its position and data using Gauss-Newton’s algorithm.</p><p>Our method gives a better result than those of Adams, although we use a better start guess for the position of Neptune than he did. The important parameter to find is at what direction to look for the planet, also called the longitude angle. Both Adams and us get close to the correct longitude &mdash; Adams’ being 2.5º off and us within 1º. This is especially interesting since without getting this parameter correct they would never have found the planet at that time.</p>
----------------------------------------------------------------------
In diva2:729112 
abstract is: 
<p>Domineerings is the classic combinatorial game in which players take turns placing</p><p>1 x 2 or 2 x 1 dominoes to cover a board of squares, losing when there is no space</p><p>available. In this paper, we study Domineering under the additional rule that the</p><p>board must be a</p><p>Young tableau at all times. We analyze some simple positions of</p><p>the game, solve the game for a specic family of boards, and present an interesting</p><p>conjecture.</p>

corrected abstract:
<p><em>Domineering</em> is the classic combinatorial game in which players take turns placing 1 × 2 or 2 × 1 dominoes to cover a board of squares, losing when there is no space available. In this paper, we study Domineering under the additional rule that the board must be a <em>Young tableau</em> at all times. We analyze some simple positions of the game, solve the game for a specific family of boards, and present an interesting conjecture.</p>
----------------------------------------------------------------------
In diva2:1595616 
abstract is: 
<p>The door closing action is a recurrent situation when using a vehicle, and its sound is therefore a common sensation, which would elicit pleasant feelings. Sensory pleasantness is an important aspect in terms of customer’s perspective, and it can be a contributing factor when deciding to buy or not a specific vehicle. The first contact between a prospective customer and the automobile usually happens in car salons or at the car retailer. The initial impression of the vehicle might be sight-based, and the door may commonly be the first physical contact. Depending on the car brand and type, doors differ in terms of mass, structure, dimension. Furthermore, there are differences regarding the latching system and the door sealing structure, in terms of material and construction. The closing sound produced when slamming the door is related to all these parameters. Auditory pleasantness can be described by characteristics of the sound that are described through psychoacoustics. Loudness, sharpness, roughness, and tonality are important auditory parameters to objectively describe this complex sensation. The aim of car doors would be to generate an enthusiastic, low-pitched, and saturated sound, which would elicit feelings of solidity, robustness, and security. On the other side, a metallic, high-pitched, fragmented sound could be a source of annoyance and produce feelings of insecurity and cheap vehicle.The present work aims to provide a broad picture on the mechanics and acoustics of door closing for automobiles. In specific, the closing sound was evaluated in relation to the door gaskets and their sealing performance over time. The sealing performance was analyzed in energy and force terms. The door closing motion was studied as a quasi-static problem, as well as a dynamic problem, where the former is related to the latching capability of the door, the latter is connected to the slamming action. The measurement results include the sealing performance trend from fresh to aged gaskets. From these measurements, the rubber non-linear behaviour could then be evaluated from a sound quality perspective. The acoustic analysis revealed inconsistencies of the psychoacoustic parameters in the description of the hearing sensations. Spectral analysis was also implemented to capture the door closing phenomenon, and the Wavelet transform emerged as the method with the highest resolution in the description of the sound wave progression.Several measurements were performed in order to assess all the established points, and methods were implemented for the sealing stiffness analysis and the acoustic analysis. The severe transiency of the door closing event was put in evidence. The stiffness analysis method showed also potential in helping to adjust the end of line tuning of the vehicle. Finally, benchmarking was included in the project, which enabled comparisons with competitor cars.</p>

corrected abstract:
<p>The door closing action is a recurrent situation when using a vehicle, and its sound is therefore a common sensation, which would elicit pleasant feelings. Sensory pleasantness is an important aspect in terms of customer’s perspective, and it can be a contributing factor when deciding to buy or not a specific vehicle. The first contact between a prospective customer and the automobile usually happens in car salons or at the car retailer. The initial impression of the vehicle might be sight-based, and the door may commonly be the first physical contact. Depending on the car brand and type, doors differ in terms of mass, structure, dimension. Furthermore, there are differences regarding the latching system and the door sealing structure, in terms of material and construction. The closing sound produced when slamming the door is related to all these parameters.</p><p>Auditory pleasantness can be described by characteristics of the sound that are described through psychoacoustics. Loudness, sharpness, roughness, and tonality are important auditory parameters to objectively describe this complex sensation. The aim of car doors would be to generate an enthusiastic, low-pitched, and saturated sound, which would elicit feelings of solidity, robustness, and security. On the other side, a metallic, high-pitched, fragmented sound could be a source of annoyance and produce feelings of insecurity and cheap vehicle.</p><p>The present work aims to provide a broad picture on the mechanics and acoustics of door closing for automobiles. In specific, the closing sound was evaluated in relation to the door gaskets and their sealing performance over time. The sealing performance was analyzed in energy and force terms. The door closing motion was studied as a quasi-static problem, as well as a dynamic problem, where the former is related to the latching capability of the door, the latter is connected to the slamming action. The measurement results include the sealing performance trend from fresh to aged gaskets. From these measurements, the rubber non-linear behaviour could then be evaluated from a sound quality perspective. The acoustic analysis revealed inconsistencies of the psychoacoustic parameters in the description of the hearing sensations. Spectral analysis was also implemented to capture the door closing phenomenon, and the Wavelet transform emerged as the method with the highest resolution in the description of the sound wave progression.</p><p>Several measurements were performed in order to assess all the established points, and methods were implemented for the sealing stiffness analysis and the acoustic analysis. The severe transiency of the door closing event was put in evidence. The stiffness analysis method showed also potential in helping to adjust the end of line tuning of the vehicle. Finally, benchmarking was included in the project, which enabled comparisons with competitor cars.</p>
----------------------------------------------------------------------
In diva2:1852464 
Note: no full text in DiVA

abstract is: 
<p>Swedish rail-freight operations today have a fundamental structure with a locomotive fleet that is practically twofold. Electric locomotives are used mostly for main line service at night and diesel locomotives for shunting and branch line service mainly at daytime. A large part of the locomotive fleet is hence inactive at all times which generates more tied-up capital. With a locomotive capable of running on all lines, this problem can be reduced significantly.</p><p>A systems study was therefore made of a dual-mode locomotive, i.e. a locomotive operating on electric catenary systems as well as diesel mode.</p><p>With a well-balanced performance along with high adhesive mass such a solution is believed to make a more profitable operation from many perspectives. Futher, the environmental benefits of a dual-mode locomotive compared to a diesel locomotive are clear (especially when comparing with the most common Swedish diesel loco, the class T44). Large reductions in terms of emissions and energy consumption can be achieved. Running time and hauling capacity in the electric mode of the dual-mode locomotive are comparable with more potent 4-axled locomotives used by Swedish and European operators. Careful dimensioning also makes a dual-mode locomotive very cost-effective. The purchase price is estimated to about the same as a modern diesel locomotive with a 1 500 - 2 500 kW engine in the order of 2/3 a modern European electric locomotive. This relation is due to the usage of standardised components that can be used due to the careful dimensioning.</p><p>The present study was carried out by studying a number of real cases apropriate for dualmode locomotives. At present these operations are either operated by diesel locomotives running all the way (including a part under electric catenary) or several locomotive types (electric and diesel). The results showed that a dual-mode locomotive has several benefits, foremost in terms of environmental aspects where the emissions are extensively reduced foremost owing to lower energy consumption which is a result of the change from diesel operation to electric. Benefits are also due to better efficiency and exhaust emission control.</p><p>Operations that today demands several loco types, electric and diesel, dual-mode locomotives will in many duties reduce the overall running time as a result of the rationalisation. Another positive effect of dual-mode locomotives can be higher rate of usage of locomotives where the benefits are adopted.</p><p>Other aspects of the study was an analysis of actual train weights showing that operators typically run with just about half of the desired train weight (the desired is often the maximum allowed for best profit). Further, the running times were simulated for the dualmode locomotive (in electric mode) along with the most common locomotives in Sweden and also a modern electric European locomotive. These simulations showed that a dualmode locomotive with approximately 3 200 kW rated power (in electric-mode), 250 kN starting tractive effort and 90 tonnes of adhesive mass had highly satisfactory running times (comparable with the European and Swedish electric locomotives) on lines with varied topographies.</p><p>The diesel engine in the dual-mode locomotive will preferably be smaller than in an ordinary main-line diesel locomotive. The reasons is that the need for diesel power is lower on e.g. sidings where the dual-mode loco will mostly run in diesel mode at fairly low speed.</p><p>It should be pointed out that the introduction of dual-mode locomotives does not exclude freight operations with ordinary electric or diesel locomotives. For heavy main-line operations over longer distances, in particular on lines with long and steep gradients, highpowered conventional locomotives would still be the most advantageous alternative.</p>

corrected abstract:
<p>Swedish rail-freight operations today have a fundamental structure with a locomotive fleet that is practically twofold. Electric locomotives are used mostly for main line service at night and diesel locomotives for shunting and branch line service mainly at daytime. A large part of the locomotive fleet is hence inactive at all times which generates more tied-up capital. With a locomotive capable of running on all lines, this problem can be reduced significantly.</p><p>A systems study was therefore made of a dual-mode locomotive, i.e. a locomotive operating on electric catenary systems as well as diesel mode.</p><p>With a well-balanced performance along with high adhesive mass such a solution is believed to make a more profitable operation from many perspectives. Further, the environmental benefits of a dual-mode locomotive compared to a diesel locomotive are clear (especially when comparing with the most common Swedish diesel locomotive, the class T44). Large reductions in terms of emissions and energy consumption can be achieved. Running time and hauling capacity in the electric mode of the dual-mode locomotive are comparable with more potent 4-axled locomotives used by Swedish and European operators. Careful dimensioning also makes a dual-mode locomotive very cost-effective. The purchase price is estimated to about the same as a modern diesel locomotive with a 1 500 - 2 500 kW engine in the order of 2/3 a modern European electric locomotive. This relation is due to the usage of standardised components that can be used due to the careful dimensioning.</p><p>The present study was carried out by studying a number of real cases appropriate for dual-mode locomotives. At present these operations are either operated by diesel locomotives running all the way (including a part under electric catenary) or several locomotive types (electric and diesel). The results showed that a dual-mode locomotive has several benefits, foremost in terms of environmental aspects where the emissions are extensively reduced foremost owing to lower energy consumption which is a result of the change from diesel operation to electric. Benefits are also due to better efficiency and exhaust emission control.</p><p>Operations that today demands several locomotive types, electric and diesel, dual-mode locomotives will in many duties reduce the overall running time as a result of the rationalisation. Another positive effect of dual-mode locomotives can be higher rate of usage of locomotives where the benefits are adopted.</p><p>Other aspects of the study was an analysis of actual train weights showing that operators typically run with just about half of the desired train weight (the desired is often the maximum allowed for best profit). Further, the running times were simulated for the dual-mode locomotive (in electric mode) along with the most common locomotive in Sweden and also a modern electric European locomotive. These simulations showed that a dual-mode locomotive with approximately 3 200 kW rated power (in electric-mode), 250 kN starting tractive effort and 90 tonnes of adhesive mass had highly satisfactory running times (comparable with the European and Swedish electric locomotives) on lines with varied topographies.</p><p>The diesel engine in the dual-mode locomotive will preferably be smaller than in an ordinary main-line diesel locomotive. The reasons is that the need for diesel power is lower on e.g. sidings where the dual-mode locomotive will mostly run in diesel mode at fairly low speed.</p><p>It should be pointed out that the introduction of dual-mode locomotives does not exclude freight operations with ordinary electric or diesel locomotives. For heavy main-line operations over longer distances, in particular on lines with long and steep gradients, highpowered conventional locomotives would still be the most advantageous alternative.</p>
----------------------------------------------------------------------
In diva2:441466 
abstract is: 
<p>With the development of high-speed railroads the dynamic behaviour of railroad bridges is increasingly important to explore. Deeper knowledge about the influence of different factors and what should be included in a model is essential if the designer shall be able to make reliable estimates of responses in existing and new structures. One factor is the soil-structure interaction (SSI), describing how the foundation of the bridge and the soil properties affect the behavior of the bridge under dynamic loading.</p>
<p>In this thesis, the influence of including SSI in a model of a portal frame railway bridge is studied, and an analysis procedure in the frequency domain for models with frequency-dependent boundary conditions is described. A 3D finite element model of an e isting bridge has been built up, based on the theory of linear elasticity. The model has been given three different types of boundary conditions: clamped, static stiffness and frequency-dependent stiffness from SSI. Results from simulated train passages, with a train set consisting of two wagons, were compared for the different boundary conditions. The models have also been compared with measurement data from the bridge, which has given indications about which model describes reality in the best way.</p>
<p>The results show that the model in which SSI is included by frequency dependent boundary conditions is in slightly better agreement with measurement data than the clamped model and the model with static stiffness. The model gives a slightly better damping of the free vibrations and the natural frequencies correspond better with experimental data. The difference in maximum acceleration from a train passage is very small between the different models, even if it is found that the clamped model generally has lower accelerations and hence is non-conservative. It appears that the train speed affects the maximum acceleration, the size of the free vibrations and the natural frequencies that are present in the free vibrations in the models.</p>
<p>Further studies are suggested where it is emphasized that an analysis with longer trains, which give resonance phenomena, should be made to see how the different eigenfrequencies in the models affect the accelerations at different speeds. It is also noted that more measurements would be needed in order to draw more general conclusions about the degree of correspondence between the measurements and the models, and to calibrate the parameters of the model against measurement data.</p>

corrected abstract:
<p>With the development of high-speed railroads the dynamic behavior of railroad bridges is increasingly important to explore. Deeper knowledge about the influence of different factors and what should be included in a model is essential if the designer shall be able to make reliable estimates of responses in existing and new structures. One factor is the soil-structure interaction (SSI), describing how the foundation of the bridge and the soil properties affect the behavior of the bridge under dynamic loading.</p>
<p>In this thesis, the influence of including SSI in a model of a portal frame railway bridge is studied, and an analysis procedure in the frequency domain for models with frequency-dependent boundary conditions is described. A 3D finite element model of an existing bridge has been built up, based on the theory of linear elasticity. The model has been given three different types of boundary conditions: clamped, static stiffness and frequency-dependent stiffness from SSI. Results from simulated train passages, with a train set consisting of two wagons, were compared for the different boundary conditions. The models have also been compared with measurement data from the bridge, which has given indications about which model describes reality in the best way.</p>
<p>The results show that the model in which SSI is included by frequency dependent boundary conditions is in slightly better agreement with measurement data than the clamped model and the model with static stiffness. The model gives a slightly better damping of the free vibrations and the natural frequencies correspond better with experimental data. The difference in maximum acceleration from a train passage is very small between the different models, even if it is found that the clamped model generally has lower accelerations and hence is non-conservative. It appears that the train speed affects the maximum acceleration, the size of the free vibrations and the natural frequencies that are present in the free vibrations in the models.</p>
<p>Further studies are suggested where it is emphasized that an analysis with longer trains, which give resonance phenomena, should be made to see how the different eigenfrequencies in the models affect the accelerations at different speeds. It is also noted that more measurements would be needed in order to draw more general conclusions about the degree of correspondence between the measurements and the models, and to calibrate the parameters of the model against measurement data.</p>
----------------------------------------------------------------------
In diva2:1751424 
abstract is: 
<p>In this paper we study a dynamical system given by a variant of the classic skew-shifts on the torus $S^1\times S^1$. Our map is defined based on $T(x,y) = (x+\omega,x+f(y))$, where $\omega$ is Diophatine, $f$ is an orientation-preserving circle diffeomorphism. We show that for our specific type of $f$, there exist 2 maps from the torus to itself whose graphs are $T$-invariant. Moreover, one of the graphs attracts (Lebesgue) almost every point in the torus. The results are robust under a small $C^1$ perturbation on the second coordinate.</p>

corrected abstract:
<p>In this paper we study a dynamical system given by a variant of the classic skew-shifts on the torus 𝑆<sup>1</sup> × 𝑆<sup>1</sup>. Our map is defined based on 𝑇(𝑥,𝑦) = (𝑥 + &#x1D714;,𝑥 + 𝑓(𝑦)</em>, where &#x1D714; is Diophatine, 𝑓 is an orientation-preserving circle diffeomorphism. We show that for our specific type of 𝑓, there exist 2 maps from the torus to itself whose graphs are 𝑇-invariant. Moreover, one of the graphs attracts (Lebesgue) almost every point in the torus. The results are robust under a small 𝘊<sup>1</sup> perturbation on the second coordinate.</p>
----------------------------------------------------------------------
In diva2:653258 -missing space in title:
"Economic Analysis of Hydrogen Production from Wastewater and Wood forMunicipal Bus System"
==>
"Economic Analysis of Hydrogen Production from Wastewater and Wood for Municipal Bus System"

Note: no full text in DiVA

abstract is: 
<p>The levelized cost of hydrogen for municipal fuel cell buses has been determined using the DOE H2A model for steam methane reforming (SMR), molten carbonate fuel cell reforming (MCFC), and wood gasification using wastewater biogas and willow wood chips as energy feedstocks. 300 kg H2/day was chosen as the design capacity.</p><p>Greenhouse gas emissions were calculated for each for the three processes and compared to diesel bus emissions in order to assess environmental impact. The levelized cost for SMR, MCFC, and gasification is $5.12, $8.59, and $10.62, respectively. SMR provided the lowest sensitivity to feedstock price, and lowest levelized cost at various scales, with competitive cost to diesel on a cost/km basis. All three technologies provide a reduction in total greenhouse gases compared to diesel bus emissions, with MCFC providing the largest reduction. These results provide preliminary evidence that small scale distributed hydrogen production for public transportation can be relatively cost-effective and have minimal environmental impact.</p>

corrected abstract:
<p>The levelized cost of hydrogen for municipal fuel cell buses has been determined using the DOE H2A model for steam methane reforming (SMR), molten carbonate fuel cell reforming (MCFC), and wood gasification using wastewater biogas and willow wood chips as energy feedstocks. 300 kg H<sub>2</sub>/day was chosen as the design capacity.</p><p>Greenhouse gas emissions were calculated for each for the three processes and compared to diesel bus emissions in order to assess environmental impact. The levelized cost for SMR, MCFC, and gasification is $5.12, $8.59, and $10.62, respectively. SMR provided the lowest sensitivity to feedstock price, and lowest levelized cost at various scales, with competitive cost to diesel on a cost/km basis. All three technologies provide a reduction in total greenhouse gases compared to diesel bus emissions, with MCFC providing the largest reduction. These results provide preliminary evidence that small scale distributed hydrogen production for public transportation can be relatively cost-effective and have minimal environmental impact.</p>

Note: Only change:
w='H2/day' val={'c': 'H<sub>2</sub>/day', 's': 'diva2:653258', 'n': 'no full text'}
----------------------------------------------------------------------
In diva2:1210790 - the full text points to https://kth.diva-portal.org/smash/get/diva2:1210790/FULLTEXT02.pdf which is a thesis titled: "Developing an Advanced Internal Ratings-Based Model by Applying Machine Learning" - its DiVA, id is diva2:1431648

Here we will focus on the abstract that is in DiVA for 'diva2:1210790'

abstract is: 
<p>This thesis within Industrial Economics and Applied Mathematics investigates the relationship between economic development and subjective well-being. The Easterlin Paradox, originally stated by Richard Easterlin in 1974, is reassessed by utilizing cross-sectional and time series data. A simple regression model is applied, using average happiness within a country as dependent variable, and gross national product per capita as regressor. In addition, an extensive study of previous research is conducted, focusing on reliability of data and earlier methodologies. The Easterlin Paradox is confirmed to still be valid when analyzing the United States over the time period 1972{2016, and 140 of the countries across the world 2012.</p>
w='1972{2016' val={'c': '1972-2016', 's': 'diva2:1210790'}

corrected abstract:
<p>This thesis within Industrial Economics and Applied Mathematics investigates the relationship between economic development and subjective well-being. The Easterlin Paradox, originally stated by Richard Easterlin in 1974, is reassessed by utilizing cross-sectional and time series data. A simple regression model is applied, using average happiness within a country as dependent variable, and gross national product per capita as regressor. In addition, an extensive study of previous research is conducted, focusing on reliability of data and earlier methodologies. The Easterlin Paradox is confirmed to still be valid when analyzing the United States over the time period 1972&dash;2016, and 140 of the countries across the world 2012.</p>

Note: As the dash is is a range of years - it should be an actual dash, hence &dash;
----------------------------------------------------------------------
In diva2:1811853 
abstract is: 
<p>Design Space Exploration (DSE) is the exploration of a space of possible designs with the goal of finding some optimal design according to some constraints and criteria. Within embedded systems design, automated DSE in particular can allow the system designer to efficiently find good solutions in highly complex design spaces. One particular tool for performing automated DSE is IDeSyDe which uses Constraint Programming (CP) and constraint optimization for modelling and optimization.</p><p>The constraint models of DSE often include some real-valued parameters, but optimized CP-solvers typically require integer arguments. This makes it necessary to discretize the problem in order to make the approach useful in practice, effectively limiting the size of the search space significantly. The effects of this discretization procedure on the quality of the solutions have not previously been well studied. An investigation into how this kind of discretization affects the approximate solutions could make the approach more rigorous, and possibly also uncover exploitable details that could facilitate the development of even more efficient algorithms.</p><p>This project presents a convergence proof based in CP and Multiresolutional analysis (MRA), including a practically useful error bound for solutions obtained with different discretizations. In particular, the mapping and scheduling of Syncronous Data Flow (SDF) models for streaming applications onto tile-based multiple processor system-on-chip platforms with a common time-division multiplexing bus interconnect is studied. The theoretical results are also verified using IDeSyDe for a few different configurations of applications and platforms. It can be seen that the experiments behave as predicted, with first order convergence in total error and adherence to the bound.</p>

corrected abstract:
<p><em>Design Space Exploration</em> (DSE) is the exploration of a space of possible designs with the goal of finding some optimal design according to some constraints and criteria. Within embedded systems design, automated DSE in particular can allow the system designer to efficiently find good solutions in highly complex design spaces. One particular tool for performing automated DSE is <em>IDeSyDe</em> which uses <em>Constraint Programming</em> (CP) and constraint optimization for modelling and optimization.</p><p>The constraint models of DSE often include some real-valued parameters, but optimized CP-solvers typically require integer arguments. This makes it necessary to discretize the problem in order to make the approach useful in practice, effectively limiting the size of the search space significantly. The effects of this discretization procedure on the quality of the solutions have not previously been well studied. An investigation into how this kind of discretization affects the approximate solutions could make the approach more rigorous, and possibly also uncover exploitable details that could facilitate the development of even more efficient algorithms.</p><p>This project presents a convergence proof based in CP and <em>Multiresolutional analysis</em> (MRA), including a practically useful error bound for solutions obtained with different discretizations. In particular, the mapping and scheduling of <em>Syncronous Data Flow</em> (SDF) models for streaming applications onto tile-based multiple processor system-on-chip platforms with a common time-division multiplexing bus interconnect is studied. The theoretical results are also verified using <em>IDeSyDe</em> for a few different configurations of applications and platforms. It can be seen that the experiments behave as predicted, with first order convergence in total error and adherence to the bound.</p>


Note - spelling error:
w='Syncronous' val={'c': 'Synchronous', 's': 'diva2:1811853', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:787497 - missing spacea in title:
"Effects of vortex induced vibrationson cylinders in tandem configurationat critical Reynold numbers"
==>
"Effects of vortex induced vibrations on cylinders in tandem configuration at critical Reynold numbers"

abstract is: 
<p>Vortex-induced vibrations (VIV) and wake interference on circular cylinders have</p><p>reached a lot of research attention over the years. However, most research is done</p><p>for relativly low Reynold numbers (sub-critical) and does not correspond to real</p><p>world chimney situations. A real chimney in the south of Sweden has experienced</p><p>some large vibrations and data gathered from this chimney shows a correlation</p><p>between these motions and wind directions. Results from 2D CFD simulations used</p><p>to simulate VIV on a circular cylinder when laying in the wake of another larger, and</p><p>fix, cylinder at critical Reynold numbers are presented in this thesis. Simulations</p><p>are done for three cases; one single cylinder, for comparisons, and two simluations</p><p>for cylinders in tandem arrangement, at 35 and 70 m respectively. Results show</p><p>that the cylinder when in tandem arrangement experience much larger vibrations</p><p>than the single cylinder. For velocities above 10 m/s the tandem arrangement can</p><p>lead to resonance on the cylinder, which is not occuring in the case with a single</p><p>cylinder. This thesis shows that even at distances of 70 m, wake interference can</p><p>have a large effect on VIV and more advanced models and simulations would be</p><p>useful for better predictions of the behaivor of chimneys.</p>

corrected abstract:
<p>Vortex-induced vibrations (VIV) and wake interference on circular cylinders have reached a lot of research attention over the years. However, most research is done for relativly low Reynold numbers (sub-critical) and does not correspond to real world chimney situations. A real chimney in the south of Sweden has experienced some large vibrations and data gathered from this chimney shows a correlation between these motions and wind directions. Results from 2D CFD simulations used to simulate VIV on a circular cylinder when laying in the wake of another larger, and fix, cylinder at critical Reynold numbers are presented in this thesis. Simulations are done for three cases; one single cylinder, for comparisons, and two simluations for cylinders in tandem arrangement, at 35 and 70 m respectively. Results show that the cylinder when in tandem arrangement experience much larger vibrations than the single cylinder. For velocities above 10 m/s the tandem arrangement can lead to resonance on the cylinder, which is not occuring in the case with a single cylinder. This thesis shows that even at distances of 70 m, wake interference can have a large effect on VIV and more advanced models and simulations would be useful for better predictions of the behaivor of chimneys.</p>

Note: Spelling error in original:
w='behaivor' val={'c': 'behavior', 's': 'diva2:787497', 'n': 'error in original'}
w='occuring' val={'c': 'occurring', 's': ['diva2:787497', 'diva2:753825'], 'n': 'error in original'}
w='simluations' val={'c': 'simulations', 's': 'diva2:787497', 'n': 'error in original'}
w='relativly' val={'c': 'relatively', 's': 'diva2:787497', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:839173 
abstract is: 
<p>Rental pricing in Sweden have endured criticism the latest decades due to its non-concrete structure. As an answer to this, in year 2003, Malmö introduced a new model for valuing rental prices for apartments.</p><p>The model was named <em>Malmömodellen </em>and consists of 180 parameters valuing apartment standard and location. The parameter-value combined with type of apartment and size defines the guidelines for allowed pricing by landlords.</p><p>The model has well defined the basis for rents but also highlighted were extra revenue can be generated through investments in existing holdings - by so called <em>Standardhöjning</em>.</p><p>The Swedish real estate sector is characterized by longterm management and has not yet recognized the full potential in the model. Companies are applying simplified models, which are easier to overlook but not always optimal in terms of profit maximization.</p><p>Investments in apartmens is conditioned by the tenants acceptence. Based on statistical data regarding earlier offering of standardhöjning, the willingness to pay in relationship to type and size of apartments has been analyzed and modelled. The result shows that the willingness to pay is higher in larger apartments which implies that revenue is being lost by applying a simplified model.</p><p>The report shows that for every property there is an optimal pricing strategy which is a combination of the guidelines in Malmömodellen and a simplified model. The price should consist of a floating part proportional to the size of the apartment and a fixed part. The optimal pricing stragy depends on the distribution of apartments within the property and the investment cost.</p><p>The model is further criticized due to its transparency disfavouring necessary investments with low returns. This especially penalize socially vurnable areas were the willingness to pay is low. The discussion takes into consideration the pros and cons regarding the economic effects of a regulated price cap.</p>

corrected abstract:
<p>Rental pricing in Sweden have endured criticism the latest decades due to its non-concrete structure. As an answer to this, in year 2003, Malmö introduced a new model for valuing rental prices for apartments.</p><p>The model was named <em lang="sv">Malmömodellen</em> and consists of 180 parameters valuing apartment standard and location. The parameter-value combined with type of apartment and size defines the guidelines for allowed pricing by landlords.</p><p>The model has well defined the basis for rents but also highlighted were extra revenue can be generated through investments in existing holdings - by so called <em lang="sv">Standardhöjning</em>.</p><p>The Swedish real estate sector is characterized by longterm management and has not yet recognized the full potential in the model. Companies are applying simplified models, which are easier to overlook but not always optimal in terms of profit maximization.</p><p>Investments in apartmens is conditioned by the tenants acceptence. Based on statistical data regarding earlier offering of standardhöjning, the willingness to pay in relationship to type and size of apartments has been analyzed and modelled. The result shows that the willingness to pay is higher in larger apartments which implies that revenue is being lost by applying a simplified model.</p><p>The report shows that for every property there is an optimal pricing strategy which is a combination of the guidelines in <span lang="sv">Malmömodellen</span> and a simplified model. The price should consist of a floating part proportional to the size of the apartment and a fixed part. The optimal pricing stragy depends on the distribution of apartments within the property and the investment cost.</p><p>The model is further criticized due to its transperency disfavouring necessary investments with low returns. This especially penalize socially vurnable areas were the willingness to pay is low. The discussion takes into consideration the pros and cons regarding the economic effects of a regulated price cap.</p>

Note spelling errors:
w='acceptence' val={'c': 'acceptance', 's': 'diva2:839173', 'n': 'error in original'}
w='apartmens' val={'c': 'apartmens', 's': 'diva2:839173', 'n': 'error in original'}
w='stragy' val={'c': 'strategy', 's': 'diva2:839173', 'n': 'error in original'}
w='vurnable' val={'c': 'vulnerable', 's': 'diva2:839173', 'n': 'error in original'}
also "transperency"
----------------------------------------------------------------------
In diva2:558597 - no need for period at end of title
abstract is: 
<p>Isosurface visualization is an important problem in computer graphics, with applications in elds</p><p>such as medical imaging, uid dynamics and chemistry. A very natural application for isosurface</p><p>visualization is found in scienti c visualization software, where ecient visualization of implicit</p><p>functions is of great importance. This paper proposes an ecient algorithm for visualizing threedimensional</p><p>isosurfaces of implicit functions in real time, with topology guarantees, subject</p><p>to constraints, and optimally with regard to some error measure. An implementation of the</p><p>algorithm in the open source interactive geometry software GeoGebra is discussed and analyzed.</p>

corrected abstract:
<p>Isosurface visualization is an important problem in computer graphics, with applications in fields such as medical imaging, fluid dynamics and chemistry. A very natural application for isosurface visualization is found in scientific visualization software, where efficient visualization of implicit functions is of great importance. This paper proposes an efficient algorithm for visualizing three-dimensional isosurfaces of implicit functions in real time, with topology guarantees, subject to constraints, and optimally with regard to some error measure. An implementation of the algorithm in the open source interactive geometry software GeoGebra is discussed and analyzed.</p>
----------------------------------------------------------------------
In diva2:1359848 
abstract is: 
<p>With steadily increasing computational power, computational fluid dynamics (CFD) can be applied to unsteady problems such as seakeeping simulations. Therefore, a good balance between accuracy and computational speed is required. This thesis investigates the application of CFD to seakeeping performance predictions and aims to propose a best-practice procedure for efficient seakeeping simulations.</p><p>The widely used KVLCC2 research vessel serves as a test case for this thesis and FINEŠ/Marine software package is used for CFD computations. In order to validate the simulations, results are compared to recent experimental data from SSPA as well as predictions with potential ˛ow code SHIPFLOW® Motions.</p><p>As for the calm water simulations, both inviscid and viscous ˛ow computations are performed in combination with three mesh refinement levels.</p><p>Seakeeping simulations with regular head waves of different wavelengths are set-up correspondingly. Furthermore, different strategies for time discretization are investigated. With the given computational resources, it is not feasible to complete seakeeping simulations with a ˝ne mesh. However, already the coarse meshes give good agreement to experiments and SHIPFLOW® Motions' predictions. Viscous ˛ow simulations turn out to be more robust than Euler ˛ow computations and thus should be preferred. Regarding the time discretization, a fixed time discretization of 150 steps per wave period has shown the best balance between accuracy and speed. Based on these findings, a best-practice procedure for seakeeping performance predictions in FINEŠ/Marine is established.</p><p>Taking the most efficient settings obtained from head wave simulations, the vessel is subjected to oblique waves with 160° encounter angle. Under similar wave conditions, CFD predictions of a similar thesis show close agreement in terms of added wave resistance. Compared to the previous head wave conditions of this study, added resistance in 160° oblique waves is found to be significantly higher. This underlines that oblique bow quartering waves represent a relevant case for determining the maximum required power of a ship.</p><p>CFD and potential ˛ow show similar accuracy with respect to ship motions and added wave resistance, albeit potential ˛ow outperforms CFD in terms of computational speed. Hence, CFD should be applied in cases where viscous effects are known to have large influence on a vessel's seakeeping behavior. This can be the case if motion control and damping devices are to be evaluated, for instance.</p>

corrected abstract:
<p>With steadily increasing computational power, computational fluid dynamics (CFD) can be applied to unsteady problems such as seakeeping simulations. Therefore, a good balance between accuracy and computational speed is required. This thesis investigates the application of CFD to seakeeping performance predictions and aims to propose a best-practice procedure for efficient seakeeping simulations.</p><p>The widely used KVLCC2 research vessel serves as a test case for this thesis and FINE™/Marine software package is used for CFD computations. In order to validate the simulations, results are compared to recent experimental data from SSPA as well as predictions with potential flow code SHIPFLOW® Motions.</p><p>As for the calm water simulations, both inviscid and viscous flow computations are performed in combination with three mesh refinement levels.</p><p>Seakeeping simulations with regular head waves of different wavelengths are set-up correspondingly. Furthermore, different strategies for time discretization are investigated. With the given computational resources, it is not feasible to complete seakeeping simulations with a fine mesh. However, already the coarse meshes give good agreement to experiments and SHIPFLOW® Motions' predictions. Viscous flow simulations turn out to be more robust than Euler flow computations and thus should be preferred. Regarding the time discretization, a fixed time discretization of 150 steps per wave period has shown the best balance between accuracy and speed. Based on these findings, a best-practice procedure for seakeeping performance predictions in FINE™/Marine is established.</p><p>Taking the most efficient settings obtained from head wave simulations, the vessel is subjected to oblique waves with 160° encounter angle. Under similar wave conditions, CFD predictions of a similar thesis show close agreement in terms of added wave resistance. Compared to the previous head wave conditions of this study, added resistance in 160° oblique waves is found to be significantly higher. This underlines that oblique bow quartering waves represent a relevant case for determining the maximum required power of a ship.</p><p>CFD and potential flow show similar accuracy with respect to ship motions and added wave resistance, albeit potential flow outperforms CFD in terms of computational speed. Hence, CFD should be applied in cases where viscous effects are known to have large influence on a vessel's seakeeping behavior. This can be the case if motion control and damping devices are to be evaluated, for instance.</p>
----------------------------------------------------------------------
In diva2:1342462 
abstract is: 
<p>Some of the worlds most hostile and remote locations may seem like unsuitable places for a modern battery electric vehicle when range anxiety is a real issue in countries with a well developed power grid for example Sweden. Belonging to these remote locations are the dessert areas of Africa, Austalias outback along with the steppes of Siberia. The purpose of this thesis is to investigate the possibilities to modify a modern electric or fossil driven vehicle to function completely without the access to a power grid and solely rely on renewable energy mainly from the sun. Data on the amount of sun hours in Africa and Austalia was collected and together with specifications for solar panels an average power output of 123, 67 W/panel was calculated. Further for the analysis a theoretical car, a pickup truck, was chosen mainly because of the wide spreaded usage in the areas but also because of the ability to place two panels on the truck bed. The total area of the two panels was assumed to 4m2 and could the be estimated to charge a 30 kWh battery in 121.3 hours. To decrease the amount of time taken to charge a battery of that size the option to place panels on a small roof was chosen instead of the truck bed. Still completely without supply from the power grid the panels on the roof manage to charge the battery in 16.7 hours which means a 30 kWh battery can be charged within two days of optimal sunshine. In other words there is no efficient way to charge a battery using only solar panels mounted on a vehicle. Instead a larger area must be used, for example a house roof, to charge a battery within reasonable time without the need for a power grid.</p>

corrected abstract:
<p>Some of the worlds most hostile and remote locations may seem like unsuitable places for a modern battery electric vehicle when range anxiety is a real issue in countries with a well developed power grid for example Sweden. Belonging to these remote locations are the dessert areas of Africa, Austalias outback along with the steppes of Siberia. The purpose of this thesis is to investigate the possibilities to modify a modern electric or fossil driven vehicle to function completely without the access to a power grid and solely rely on renewable energy mainly from the sun. Data on the amount of sun hours in Africa and Austalia was collected and together with specifications for solar panels an average power output of 123,67 W/panel was calculated. Further for the analysis a theoretical car, a pickup truck, was chosen mainly because of the wide spreaded usage in the areas but also because of the ability to place two panels on the truck bed. The total area of the two panels was assumed to 4 m<sup>2</sup> and could the be estimated to charge a 30 kWh battery in 121.3 hours. To decrease the amount of time taken to charge a battery of that size the option to place panels on a small roof was chosen instead of the truck bed. Still completely without supply from the power grid the panels on the roof manage to charge the battery in 16.7 hours which means a 30 kWh battery can be charged within two days of optimal sunshine. In other words there is no efficient way to charge a battery using only solar panels mounted on a vehicle. Instead a larger area must be used, for example a house roof, to charge a battery within reasonable time without the need for a power grid.</p>

Note spelling errors in original:
w='spreaded' val={'c': 'spread', 's': 'diva2:1342462', 'n': 'error in original'}
w='Austalia' val={'c': 'Australia', 's': 'diva2:1342462', 'n': 'error in original'}
w='Austalias' val={'c': "Australia's", 's': 'diva2:1342462', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:560224 
abstract is: 
<p>In this bachelor thesis we study the propagation of electrons in crystals. The crystalline</p><p>structure is modeled as a one dimensional periodic potential primarily composed of delta</p><p>function potential barriers.</p><p>We use two different models to describe how a particle behaves in such a periodic</p><p>structure. The first model is the Kronig Penney model described by S. Gasiorowicz in</p><p>[</p><p>1]. The second model is described by Olsen and Vignale in the article "The Quantum</p><p>mechanics of electric conduction in crystals" [</p><p>3]. We show that there exists certain ranges</p><p>of energy for which electron propagation can occur, namely the allowed energy bands separeted</p><p>by regions of forbidden energies. In addition, we solve the Schrödinger equation</p><p>numerically for some simple cases and reproduce some of the results seen from the two</p><p>mentioned models.</p>


corrected abstract:
<p>In this bachelor thesis we study the propagation of electrons in crystals. The crystalline structure is modeled as a one dimensional periodic potential primarily composed of delta function potential barriers.</p><p>We use two different models to describe how a particle behaves in such a periodic structure. The first model is the Kronig Penney model described by S. Gasiorowicz in [1]. The second model is described by Olsen and Vignale in the article "The Quantum mechanics of electric conduction in crystals" [3]. We show that there exists certain ranges of energy for which electron propagation can occur, namely the allowed energy bands separeted by regions of forbidden energies. In addition, we solve the Schrödinger equation numerically for some simple cases and reproduce some of the results seen from the two mentioned models.</p>

Note spelling error in originaL:
w='separeted' val={'c': 'separated', 's': 'diva2:560224', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1220019 
abstract is: 
<p>This thesis in mathematical statistics, gives the reader an insight of which factors affect the study performance of a university student. Two health related factors such as the amount of sleep and physical activity were examined as well as the student’s parental academic background. A regression analysis was con- ducted in order to analyze the relationship between these factors and the study performance of a university student. The regression model was based on data collected solely by students from the Royal Institute of Technology in Stockholm through an internet based questionnaire.</p><p> </p><p>In summary, a positive relationship was identified between the student’s study performance and having at least one parent with an academical background, especially if the academical background was in the area of technical- and scientifical studies. A positive relationship was also found between the study perfor- mance and the interaction between sleep and age. However, could a relationship not be found between the physical activity and the study performance. The fi- nal regression model provided a low value for the coefficient of determination where the possible causes are well discussed in this report.</p>
w='nal' val={'c': 'final', 's': ['diva2:556492', 'diva2:1216849', 'diva2:1220019', 'diva2:1465506', 'diva2:648544', 'diva2:839851', 'diva2:648516', 'diva2:1350191', 'diva2:405436'], 'n': 'missing ligature'}

corrected abstract:
<p>This thesis in mathematical statistics, gives the reader an insight of which factors affect the study performance of a university student. Two health related factors such as the amount of sleep and physical activity were examined as well as the student’s parental academic background. A regression analysis was con ducted in order to analyze the relationship between these factors and the study performance of a university student. The regression model was based on data collected solely by students from the Royal Institute of Technology in Stockholm through an internet based questionnaire.</p><p>In summary, a positive relationship was identified between the student’s study performance and having at least one parent with an academical background, especially if the academical background was in the area of technical- and scientifical studies. A positive relationship was also found between the study performance and the interaction between sleep and age. However, could a relationship not be found between the physical activity and the study performance. The final regression model provided a low value for the coefficient of determination where the possible causes are well discussed in this report.</p>
----------------------------------------------------------------------
In diva2:509959 
abstract is: 
<p>This study examines how formal mathematics can be taught in the Swedish secondary school with its new curriculum for mathematics. The study examines what a teaching material in formal mathematics corresponding to the initial content of the course Mathematics 1c could look like, and whether formal mathematics can be taught to high school students.</p><p>The survey was conducted with second year students from the science programme. The majority of these students studied the course Mathematics D. The students described themselves as not being motivated towards mathematics.</p><p>The results show that the content of the curriculum can be presented with formal mathematics. This both in terms of requirements for content and students being able to comprehend this content. The curriculum also requires that this type of mathematics is introduced in the course Mathematics 1c.</p><p>The results also show that students are open towards and want more formal mathematics in their ordinary education. They initially felt it was strange because they had never encountered this type of mathematics before, but some students found the formal mathematics to be easier than the mathematics ordinarily presented in class.</p><p>The study ﬁnds no reason to postpone the meeting with the formal mathematics to university level. Students’ commitment to proof and their comprehention of content suggests that formal mathematics can be introduced in high school courses. This study thus concludes that the new secondary school course Mathematics 1c can be formalised and therefore makes possible a renewed mathematics education.</p>


corrected abstract:
<p>This study examines how formal mathematics can be taught in the Swedish secondary school with its new curriculum for mathematics. The study examines what a teaching material in formal mathematics corresponding to the initial content of the course Mathematics 1c could look like, and whether formal mathematics can be taught to high school students.</p><p>The survey was conducted with second year students from the science programme. The majority of these students studied the course Mathematics D.</p><p>The students described themselves as not being motivated towards mathematics.</p><p>The results show that the content of the curriculum can be presented with formal mathematics. This both in terms of requirements for content and students being able to comprehend this content. The curriculum also requires that this type of mathematics is introduced in the course Mathematics 1c.</p><p>The results also show that students are open towards and want more formal mathematics in their ordinary education. They initially felt it was strange because they had never encountered this type of mathematics before, but some students found the formal mathematics to be easier than the mathematics ordinarily presented in class.</p><p>The study finds no reason to postpone the meeting with the formal mathematics to university level. Students’ commitment to proof and their comprehention of content suggests that formal mathematics can be introduced in high school courses. This study thus concludes that the new secondary school course Mathematics 1c can be formalised and therefore makes possible a renewed mathematics education.</p>

Note spelling error in original:
w='comprehention' val={'c': 'comprehension', 's': ['diva2:509959', 'diva2:515592'], 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1795474 
abstract is: 
<p>Energy efficiency is a growing field that concerns more and more companies, individuals as well as governments in order to reduce costs and use less energy for a more sustainable development. Engie is no exception and values energy efficient projects.</p><p>In this paper will be documented three energy efficiency related projects that I carried out during my five month end of studies internship. This work spans different topics, the parallel operation of hydraulic distribution pumps, the modeling of an heat exchangers as well as pid calculations for a dedicated ehating network.</p><p>We delve into the technical aspect of each subject before introducing the tools developed, with some of their results.</p>

corrected abstract:
<p>Energy efficiency is a growing field that concerns more and more companies, individuals as well as governments in order to reduce costs and use less energy for a more sustainable development. Engie is no exception and values energy efficient projects.</p><p>In this paper will be documented three energy efficiency related projects that I carried out during my five month end of studies internship. This work spans different topics, the parallel operation of hydraulic distribution pumps, the modeling of an heat exchangers as well as pid calculations for a dedicated heating network.</p><p>We delve into the technical aspect of each subject before introducing the tools developed, with some of their results.</p>
----------------------------------------------------------------------
In diva2:1247272 
abstract is: 
<p>The purpose of this master thesis is to study the energy efficiency of a vehicle when it is cornering. To achieve this, a Simulink model was built from a simple basic bicycle model and theoretically validated. This model was then analysed and successively improved by adding velocity and yaw moment control. A study of the vehicle model behaviour by changing parameters such as cornering stiffness and centre of gravity position was the nconducted. The traction force needed for a constant radius was calculated and methods such as torque vectoring have been tested using the model to obtain the lowest traction force. The model was compared with different vehicle types and further validated by comparing the simulation results with experimental data acquired from a field test. The rolling resistance and aerodynamic resistance were taken into account when the model was validated with the experimental data and the result suggest that by distributing the required traction force (using torque vectoring between inner and outer driven wheels) the energy efficiency could be improved by 10%. This report ends with recommendations for future work.</p>

corrected abstract:
<p>The purpose of this master thesis is to study the energy efficiency of a vehicle when it is cornering. To achieve this, a Simulink model was built from a simple basic bicycle model and theoretically validated. This model was then analysed and successively improved by adding velocity and yaw moment control. A study of the vehicle model behaviour by changing parameters such as cornering stiffness and centre of gravity position was then conducted. The traction force needed for a constant radius was calculated and methods such as torque vectoring have been tested using the model to obtain the lowest traction force. The model was compared with different vehicle types and further validated by comparing the simulation results with experimental data acquired from a field test. The rolling resistance and aerodynamic resistance were taken into account when the model was validated with the experimental data and the result suggest that by distributing the required traction force (using torque vectoring between inner and outer driven wheels) the energy efficiency could be improved by 10%. This report ends with recommendations for future work.</p>
----------------------------------------------------------------------
In diva2:1592077 
abstract is: 
<p>This thesis addresses the issues of data sparsity in the sonar domain. A data pipeline is set up to generate and enhance sonar data. The possibilities and limitations of using cycleGAN as a tool to enhance simulated sonar images for the purpose of training neural networks for detection and classification is studied. A neural network is trained on the enhanced simulated sonar images and tested on real sonar images to evaluate the quality of these images.The novelty of this work lies in extending previous methods to a more general framework and showing that GAN enhanced simulations work for complex tasks on field data.Using real sonar images to enhance the simulated images, resulted in improved classification compared to a classifier trained on solely simulated images.</p>

corrected abstract:
<p>This thesis addresses the issues of data sparsity in the sonar domain. A data pipeline is set up to generate and enhance sonar data. The possibilities and limitations of using cycleGAN as a tool to enhance simulated sonar images for the purpose of training neural networks for detection and classification is studied. A neural network is trained on the enhanced simulated sonar images and tested on real sonar images to evaluate the quality of these images. The novelty of this work lies in extending previous methods to a more general framework and showing that GAN enhanced simulations work for complex tasks on field data.</p><p>Using real sonar images to enhance the simulated images, resulted in improved classification compared to a classifier trained on solely simulated images.</p>
----------------------------------------------------------------------
In diva2:1077058   - correct as is

Note spelling error:
w='examinined' val={'c': 'examined', 's': 'diva2:1077058', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1737033 
abstract is: 
<p>The problem of approximating a solution to the convection equation $\partial_t u +f\cdot\nabla_xu = h$ given data on the flux $f$, source function $h$ and a final condition $g$ is investigated. Specifically, two layer neural networks are used to approximate $f,h$ and $g$ and a solution is approximated using numerical integration. An upper bound to the expected square error of the approximated solution is derived which is dependent on the number of parameters in the approximating neural networks. The dependency of the error is investigated via numerical experiments concerning both synthetic and real world wind data. The neural networks used in the numerical experiments are trained first by the algorithm Adaptive Metropolis-Hastings and then by the SGD-type algorithm Adam. The rate of convergence of the approximation error is shown to be in line with the derived bound when approximating a solution close in time to the final condition $g$. The error is shown to decrease slower than what the derived bound suggests when approximating far away in time from the final condition $g$.</p>

corrected abstract:
<p>The problem of approximating a solution to the convection equation ∂<sub>𝑡</sub>𝑢 + 𝑓 · ∇<sub>𝑥</sub>𝑢 = ℎ given data on the flux 𝑓, source function ℎ and a final condition <em>g</em> is investigated. Specifically, two layer neural networks are used to approximate 𝑓, ℎ and 𝑔 and a solution is approximated using numerical integration. An upper bound to the expected square error of the approximated solution is derived which is dependent on the number of parameters in the approximating neural networks. The dependency of the error is investigated via numerical experiments concerning both synthetic and real world wind data. The neural networks used in the numerical experiments are trained first by the algorithm Adaptive Metropolis-Hastings and then by the SGD-type algorithm Adam. The rate of convergence of the approximation error is shown to be in line with the derived bound when approximating a solution close in time to the final condition 𝑔. The error is shown to decrease slower than what the derived bound suggests when approximating far away in time from the final condition 𝑔.</p>
----------------------------------------------------------------------
In diva2:1249684 
abstract is: 
<p>Responsible investing consists of buying more sustainable stocks, or green stocks, and selling the controversial ones. As a pension fund, and with the current climate regulations, it is a concern for Första AP-fonden to know if responsible investing is a plus value for financial aspects. Since our commissioner also has interests in factor strategies, rule-based systematic investment strategies, and possesses some, we will study and explain what are factor strategies. Financial beneﬁts from responsible investing could be explained by savings made on carbon taxes, if a company has a strong environmental policy. On the other hand, factor strategies have proven to work well historically, like the Fama-French value strategy which performed more than decently during the 80s, growing up to around 10 times the initial budget. By using an optimization approach, that maximizes ESG and factor scores with equal importance, we observed that half of the factors had lower performance when combined with ESG. Moreover, the factor portfolios lost their initial characteristics after ESG integration. We concluded that quality was the most promising candidate for a potential new systematic strategy.</p>

corrected abstract:
<p>Responsible investing consists of buying more sustainable stocks, or green stocks, and selling the controversial ones. As a pension fund, and with the current climate regulations, it is a concern for <span lang="sv">Första AP-fonden</span> to know if responsible investing is a plus value for financial aspects. Since our commissioner also has interests in factor strategies, rule-based systematic investment strategies, and possesses some, we will study and explain what are factor strategies. Financial benefits from responsible investing could be explained by savings made on carbon taxes, if a company has a strong environmental policy. On the other hand, factor strategies have proven to work well historically, like the Fama-French value strategy which performed more than decently during the 80s, growing up to around 10 times the initial budget. By using an optimization approach, that maximizes ESG and factor scores with equal importance, we observed that half of the factors had lower performance when combined with ESG. Moreover, the factor portfolios lost their initial characteristics after ESG integration. We concluded that quality was the most promising candidate for a potential new systematic strategy.</p>
----------------------------------------------------------------------
In diva2:1678931 
abstract is: 
<p>Studying the properties of newborn neutron stars is a complicated matter since they cannot be directly observed. Neutron stars are born when some massive stars go supernova (SN), where the expelled material from the explosion goes on to shield the young neutron star from our view by absorbing its radiation. To estimate properties such as their flux, luminosity and magnetic field strength, upper limits can be found by modeling the emission and absorption and then performing spectral fitting. The assumptions made when modeling can cause the results to differ, this thesis investigates which parameters in the model have the most impact by analysing an X-ray observation of SN 1909A. The varied model parameters are the photon index of the neutron star emission, the density of the SN ejecta, and the composition of the ejecta material. The density can vary depending on the line of sight since SN explosions are asymmetrical, and it is found that this parameter carries most significance, with maximal result variations of about 55% for most ejecta compositions. The least significant parameter is the assumed photon index of the emission from the neutron star, this is found to only cause maximal variations of around 24%. Furthermore, the upper limits on the total luminosity computed by assuming different model parameters, differ by a factor 2.5 at most. The minimum upper limit to the total luminosity of the neutron star of SN 1909A is found to be L_min = 3.6 * 10^6 L⊙ and the corresponding relation between its rotational period and magnetic field is B &lt; 1.88 * 10^20 P^2 G s^-1.</p>

corrected abstract:
<p>Studying the properties of newborn neutron stars is a complicated matter since they cannot be directly observed. Neutron stars are born when some massive stars go supernova (SN), where the expelled material from the explosion goes on to shield the young neutron star from our view by absorbing its radiation. To estimate properties such as their flux, luminosity and magnetic field strength, upper limits can be found by modeling the emission and absorption and then performing spectral fitting. The assumptions made when modeling can cause the results to differ, this thesis investigates which parameters in the model have the most impact by analysing an X-ray observation of SN 1909A. The varied model parameters are the photon index of the neutron star emission, the density of the SN ejecta, and the composition of the ejecta material. The density can vary depending on the line of sight since SN explosions are asymmetrical, and it is found that this parameter carries most significance, with maximal result variations of about 55% for most ejecta compositions. The least significant parameter is the assumed photon index of the emission from the neutron star, this is found to only cause maximal variations of around 24%. Furthermore, the upper limits on the total luminosity computed by assuming different model parameters, differ by a factor 2.5 at most. The minimum upper limit to the total luminosity of the neutron star of SN 1909A is found to be 𝐿<sub>min</sub> = 3.6 * 10<sup>6</sup> 𝐿<sub>☉</sub> and the corresponding relation between its rotational period and magnetic field is 𝐵 &lt; 1.88 * 10<sup>20</sup> P<sup>2</sup> G s<sup>-1</sup>.</p>
----------------------------------------------------------------------
In diva2:1188262 
abstract is: 
<p>The aim of this thesis is to estimate the high cycle fatigue life of welds using finite element methods and liner elastic fracture mechanics (LEFM). Develop a new method of calculating the fatigue life that can replace the old method in use today at GKN Driveline. This is a continuation of an earlier master thesis1. When gears have been subjected to high cycle fatigue testes, cracks have been observed propagating though the welds and resulted in component failure. The current method of estimating the welds fatigue life has a tendency to overestimate the life and thus a new method with better accuracy is sought after.The crack propagation is simulated by starting with an initial crack in the weld which is then incrementally increased in a pre-determined direction. The energy release rate for each node at the crack tip is calculated using virtual crack closure technique (VCCT). Due to multiaxial loads on the weld, an effective stress intensity factor is used together with a Paris Law type equation to estimate the propagation speed. The total amount of cycles it takes the crack to propagate between each of the increments is then estimated.The results show a good correlation to experimental data and takes approximately 40% longer time compared to the old method. The new method has a high dependency on good material parameters and thus important that these are chosen accurately, which can be especially hard for welds. The results can also be linearized which means that the life can be estimated for different load levels from one single simulation.</p>

corrected abstract:
<p>The aim of this thesis is to estimate the high cycle fatigue life of welds using finite element methods and liner elastic fracture mechanics (LEFM). Develop a new method of calculating the fatigue life that can replace the old method in use today at GKN Driveline. This is a continuation of an earlier master thesis<sup><a href="#fn1" id="ref1">1</a></sup>. When gears have been subjected to high cycle fatigue testes, cracks have been observed propagating though the welds and resulted in component failure. The current method of estimating the welds fatigue life has a tendency to overestimate the life and thus a new method with better accuracy is sought after.</p><p>The crack propagation is simulated by starting with an initial crack in the weld which is then incrementally increased in a pre-determined direction. The energy release rate for each node at the crack tip is calculated using virtual crack closure technique (VCCT). Due to multiaxial loads on the weld, an effective stress intensity factor is used together with a Paris Law type equation to estimate the propagation speed. The total amount of cycles it takes the crack to propagate between each of the increments is then estimated.</p><p>The results show a good correlation to experimental data and takes approximately 40% longer time compared to the old method. The new method has a high dependency on good material parameters and thus important that these are chosen accurately, which can be especially hard for welds. The results can also be linearized which means that the life can be estimated for different load levels from one single simulation.</p>
<div id="footnotes">
    <ol>
        <li id="fn1">Fredriksson, E., Accuracy Study in Predicting Fatigue Life for a Welding Joint, Master Thesis, KTH, Stockholm, Sweden, 2015 <a href="#ref1" aria-label="Back to reference">↩</a></ĺi>
    </ol>
</div>
----------------------------------------------------------------------
In diva2:1437676 
abstract is: 
<p>For a long time, second-order optimization methods have been regarded as computationally inefficient and intractable for solving the optimization problem associated with deep learning. However, proposed in recent research is an adaptation of Newton's method for optimization in which the Hessian is approximated by a Kronecker-factored approximate curvature matrix, known as KFAC. This work aims to assess its practicality for use in deep learning. Benchmarks were performed using abstract, binary, classification problems, as well as the real-world Boston Housing regression problem, and both deep and shallow network architectures were employed. KFAC was found to offer great savings in computational complexity compared to a naive approximate second-order implementation using the Gauss Newton matrix. Comparing performance in deep and shallow networks, the loss convergence of both stochastic gradient descent (SGD) and KFAC showed a dependency upon network architecture, where KFAC tended to converge quicker in deep networks, and SGD tended to converge quicker in shallow networks. The study concludes that KFAC can perform well in deep learning, showing competitive loss minimization versus basic SGD, but that it can be sensitive to initial weigths. This sensitivity could be remedied by allowing the first steps to be taken by SGD, in order to set KFAC on a favorable trajectory.</p>


corrected abstract:
<p>For a long time, 2<sup>nd</sup>-order optimization methods have been regarded as computationally inefficient and intractable for solving the optimization problem associated with deep learning. However, proposed in recent research is an adaptation of Newton's method for optimization in which the Hessian is approximated by a Kronecker-factored approximate curvature matrix, known as KFAC. This work aims to assess its practicality for use in deep learning. Benchmarks were performed using abstract, binary, classification problems, as well as the real-world Boston Housing regression problem, and both deep and shallow network architectures were employed. KFAC was found to offer great savings in computational complexity compared to a naive approximate second-order implementation using the Gauss Newton matrix. Comparing performance in deep and shallow networks, the loss convergence of both stochastic gradient descent (SGD) and KFAC showed a dependency upon network architecture, where KFAC tended to converge quicker in deep networks, and SGD tended to converge quicker in shallow networks. The study concludes that KFAC can perform well in deep learning, showing competitive loss minimization versus basic SGD, but that it can be sensitive to initial weigths. This sensitivity could be remedied by allowing the first steps to be taken by SGD, in order to set KFAC on a favorable trajectory.</p>

Note spelling error in original:
w='weigths' val={'c': 'weights', 's': ['diva2:1437676', 'diva2:608438']}
----------------------------------------------------------------------
In diva2:1849652 
abstract is: 
<p>In this thesis, the foundations of evaluating the performance of volatility forecasting methods are explored, and a mathematical framework is created to determine the overall forecasting performance based on observed daily returns across multiple financial instruments. Multiple volatility responses are investigated, and theoretical corrections are derived under the assumption that the log returns follow a normal distribution. Performance measures that are independent of the long-term volatility profile are explored and tested. Well-established volatility forecasting methods, such as moving average and GARCH (p,q) models, are implemented and validated on multiple volatility responses. The obtained results reveal no significant difference in the performances between the moving average and GARCH (1,1) volatility forecast. However, the observed non-zero bias and a separate analysis of the distribution of the log returns reveal that the theoretically derived corrections are insufficient in correcting the not-normally distributed log returns. Furthermore, it is observed that there is a high dependency of abslute performances on the considered evaluation period, suggesting that comparisons between periods should not be made.</p><p>This study is limited by the fact that the bootstrapped confidence regions are ill-suited for determining significant performance differences between forecasting methods. In future work, statistical significance can be gained by bootstrapping the difference in performance measures. Furthermore, a more in-depth analysis is needed to determine more appropriate theoretical corrections for the volatility responses based on the observed distribution of the log returns. This will increase the overall forecasting performance and improve the overall quality of the evaluation framework.</p>

corrected abstract:
<p>In this thesis, the foundations of evaluating the performance of volatility forecasting methods are explored, and a mathematical framework is created to determine the overall forecasting performance based on observed daily returns across multiple financial instruments. Multiple volatility responses are investigated, and theoretical corrections are derived under the assumption that the log returns follow a normal distribution. Performance measures that are independent of the long-term volatility profile are explored and tested. Well-established volatility forecasting methods, such as moving average and GARCH(p,q) models, are implemented and validated on multiple volatility responses. The obtained results reveal no significant difference in the performances between the moving average and GARCH(1,1) volatility forecast. However, the observed non-zero bias and a separate analysis of the distribution of the log returns reveal that the theoretically derived corrections are insufficient in correcting the not-normally distributed log returns. Furthermore, it is observed that there is a high dependency of absolute performances on the considered evaluation period, suggesting that comparisons between periods should not be made.</p><p>This study is limited by the fact that the bootstrapped confidence regions are ill-suited for determining significant performance differences between forecasting methods. In future work, statistical significance can be gained by bootstrapping the difference in performance measures. Furthermore, a more in-depth analysis is needed to determine more appropriate theoretical corrections for the volatility responses based on the observed distribution of the log returns. This will increase the overall forecasting performance and improve the overall quality of the evaluation framework.</p>
----------------------------------------------------------------------
In diva2:514154 
abstract is: 
<p>The human neck is especially vulnerable to severe injuries why it is of interest to gain a better understanding of the injury mechanisms involved. A 3D Finite Element (FE) model including the geometry of the individual neck muscles has been developed at Kungliga Tekniska Högskolan (KTH) and gives the possibility to study the strains inside the muscles. However, in this model the muscles are only hindered to interpenetrate and can glide relative each other with a roughly estimated coefficient of friction. The focus of this thesis has been to modify the FE-model in order to model the connectivity on a physiological basis using methods available in the used FE-software. The main reason for this was to get a more realistic muscle displacement without separations that are present in the current model. The connective tissue surrounding and connecting the muscles was modeled with two principal approaches. The first was based on a combination of FE-contacts where the material properties were contacts. The other approach was based on a new element type called Smoothed Particle Hydrodynamics (SPH) elements in combination with FE-contacts. Both approaches showed that the structural stiffness did not increase significantly, but the strain levels where somewhat elevated. Stability issues arouse with deleted elements and negative element volumes causing the FE‐contact based approach to fail prematurely. The SPH‐based approach had fewest deleted elements and completed the simulation but increased the calculation time with approximately 50 %. It was concluded that the implementation of a connection between the muscles had a relatively low influence on the strains and kinematics of the neck and could be used to avoid muscle separations. The influence on stability of the model was however more evident and the most stable result increased the calculation time significantly.</p>

corrected abstract:
<p>The human neck is especially vulnerable to severe injuries why it is of interest to gain a better understanding of the injury mechanisms involved. A 3D Finite Element (FE) model including the geometry of the individual neck muscles has been developed at Kungliga Tekniska Högskolan (KTH) and gives the possibility to study the strains inside the muscles. However, in this model the muscles are only hindered to interpenetrate and can glide relative each other with a roughly estimated coefficient of friction. The focus of this thesis has been to modify the FE‐model in order to model the connectivity on a physiological basis using methods available in the used FE‐software. The main reason for this was to get a more realistic muscle displacement without separations that are present in the current model.</p><p>The connective tissue surrounding and connecting the muscles was modeled with two principal approaches. The first was based on a combination of FE‐contacts where the material properties were coupled to the stiffness of the FE‐contacts. The other approach was based on a new element type called Smoothed Particle Hydrodynamics (SPH) elements in combination with FE‐contacts. Both approaches showed that the structural stiffness did not increase significantly, but the strain levels where somewhat elevated. Stability issues arouse with deleted elements and negative element volumes causing the FE‐contact based approach to fail prematurely. The SPH‐based approach had fewest deleted elements and completed the simulation but increased the calculation time with approximately 50 %.</p><p>It was concluded that the implementation of a connection between the muscles had a relatively low influence on the strains and kinematics of the neck and could be used to avoid muscle separations. The influence on stability of the model was however more evident and the most stable result increased the calculation time significantly.</p>


Note the Hyphen-Minus (0x2d) in DiVA entry were converted to Hyphen (0x2019, i.e., &dash;) as per the original abstract in the thesis.
‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐
In diva2:820013   - correct as is

Note spelling error:
    'pricest': {'c': 'prices', 's': 'diva2:820013', 'n': 'error in original'},
----------------------------------------------------------------------
In diva2:565743 
abstract is: 
<p>This is a master’s thesis provided by the Swedish Defence Research Agency FOI and is written for the Center for Naval Architecture at the Royal Institute of Technology KTH. The objective is to perform a systematic simulation campaign of the flow around an axisymmetric streamlined body in order to evaluate several RANS-based turbulence models implemented in the open source CFD software package OpenFOAM. Of most interest is the low velocity zone at the stern and the wake behind the body. The results are compared with experimental flow measurements and with data obtained by LES, in order to evaluate how accurately the flow is predicted by each turbulence model. The study leads to method recommendations for this type of flows. Simulation results for two different bodies are also compared in order to investigate how the different shapes affect the flow. Also included is an overview of the turbulence modeling theory behind the RANS-methods which are employed. The results demonstrate that the turbulence models k−!,RNGk−!,k−"andk−"SST are suitable for simulation of this class of flows and provide a good prediction of the mean flow around and behind the body.</p>

corrected abstract:
<p>This is a master’s thesis provided by the Swedish Defence Research Agency FOI and is written for the Center for Naval Architecture at the Royal Institute of Technology KTH. The objective is to perform a systematic simulation campaign of the flow around an axisymmetric streamlined body in order to evaluate several RANS-based turbulence models implemented in the open source CFD software package OpenFOAM. Of most interest is the low velocity zone at the stern and the wake behind the body. The results are compared with experimental flow measurements and with data obtained by LES, in order to evaluate how accurately the flow is predicted by each turbulence model. The study leads to method recommendations for this type of flows. Simulation results for two different bodies are also compared in order to investigate how the different shapes affect the flow. Also included is an overview of the turbulence modeling theory behind the RANS-methods which are employed. The results demonstrate that the turbulence models 𝑘-<em>&epsilon;</em>, <em>RNG</em> 𝑘-<em>&epsilon;</em>, 𝑘-<em>&omega;</em> and 𝑘-<em>&omega;</em> SST are suitable for simulation of this class of flows and provide a good prediction of the mean flow around and behind the body.</p>
----------------------------------------------------------------------
In diva2:1249033 
abstract is: 
<p>Additive manufacturing is a novel manufacturing technique, which has developed rapidly in recent years. The additive manufacturing process produces complex geometries, light weighted components and reduces the material waste. During the building process, a laser energy source is commonly used to melt the metal powder. Due to the presence of thermal gradients, residual stresses resides in the ﬁnal product. These residual stresses, when released, result in a distortion of the product. To predict the appearing residual stresses and distortions, simulation tools can be used and prevent costly trials of failed printed products. This thesis investigates whether a good prediction of residual stresses and distortions can be performed in additively manufactured components using MSC Simufact. The inherent strain method was used to predict the residual stresses and distortions of a cantilever beam respectively a pipe. The printed components were then compared with the simulations. The residual stresses were examined using a X-ray di↵ractometer and the distortions were analyzed by a laser scanner.Results showed that the predicted distortions of the pipe correlated well with the simulations. However, the residual stresses were dicult to compare with the simulations. The conclusion that Simufact Additive can predict distortions can thus be drawn.</p>

corrected abstract:
<p>Additive manufacturing is a novel manufacturing technique, which has developed rapidly in recent years. The additive manufacturing process produces complex geometries, light weighted components and reduces the material waste. During the building process, a laser energy source is commonly used to melt the metal powder. Due to the presence of thermal gradients, residual stresses resides in the final product. These residual stresses, when released, result in a distortion of the product. To predict the appearing residual stresses and distortions, simulation tools can be used and prevent costly trials of failed printed products. This thesis investigates whether a good prediction of residual stresses and distortions can be performed in additively manufactured components using MSC Simufact. The inherent strain method was used to predict the residual stresses and distortions of a cantilever beam respectively a pipe. The printed components were then compared with the simulations. The residual stresses were examined using a X-ray di↵ractometer and the distortions were analyzed by a laser scanner.</p><p>Results showed that the predicted distortions of the pipe correlated well with the simulations. However, the residual stresses were difficult to compare with the simulations. The conclusion that Simufact Additive can predict distortions can thus be drawn.</p>
----------------------------------------------------------------------
In diva2:550688 
abstract is: 
<p>The objective of this study was to evaluate the 　ying qualities of a light unmanned aerial vehicle (UAV) developed by the Thai company AVIA Satcom Co., Ltd. Based on the study changes in design was to be suggested to meet stability requirements and recommendations from European Aviation Safety was based on two dierent analyses. First, the stability characteristics in terms of stability modes were examined by creating a flight dynamics model of the studied airplane. Secondly the controllability of the vehicle was investigated by examining the control surfaces. It was found that the original design of the UAV was dynamically unstable and that the control surfaces were too large making the airplane di-cult to 　y in trim condition. By studying the stability characteristics of the simulated airplane it could be concluded that the UAV was dynamically stable for the improved design and thereby meet the requirements and recommendations.</p>

corrected abstract:
<p>The objective of this study was to evaluate the flying qualities of a light unmanned aerial vehicle (UAV) developed by the Thai company AVIA Satcom Co., Ltd. Based on the study changes in design was to be suggested to meet stability requirements and recommendations from European Aviation Safety Agency and Federal Aviation Administration. The evaluation was based on two different analyses. First, the stability characteristics in terms of stability modes were examined by creating a flight dynamics model of the studied airplane. Secondly the controllability of the vehicle was investigated by examining the control surfaces. It was found that the original design of the UAV was dynamically unstable and that the control surfaces were too large making the airplane difficult to fly in trim condition. By studying the stability characteristics of the simulated airplane it could be concluded that the UAV was dynamically stable for the improved design and thereby meet the requirements and recommendations.</p>
----------------------------------------------------------------------
In diva2:919846 
abstract is: 
<p>The department of Naval Architecture at the Royal Institute of Technology is in posession of one Autonomous Underwater Vehicle (AUV) and a second is under construction. A project for doing hydrographic mapping using an Autonomous Surface Vehicle (ASV) is also initiated. These projects raises the need for a software to easily send commands to vehicles and to review collected data. The ability to use each vehicle as a node in a network of vehicles is also requested. This thesis examines a software toolchain developed at the Underwater Systems and Technology Laboratory (LSTS) in Portugal for mission planning and control of networked autonomous vehicles. The toolchain constitutes primarily of Neptus, which provides an operator with a user interface for realtime control and feedback from vehicles, and DUNE. DUNE is a software running on-board vehicles and communicates with Neptus over a wireless network. As a ﬁrst step, and as a limitation to this thesis, the toolchain has been used to control an autonomous rover. An autopilot receives waypoints in form of latitude/longitude coordinates from DUNE and periodically sends position and various sensor readings back. DUNE is running on a GNU/Linux computer and is responsible for storing a mission of multiple waypoints and to keep track of the progress. DUNE forwards vehicle location and sensor data to Neptus for feedback in the user interface and generation of plots. In conclusion the author was able to create and execute missions of an arbitrary number of waypoints. Graphs of basically any sensor reading could be generated through the Mission Review and Analysis tool contained by Neptus. Implementing the toolchain on the departments marine vehicles releases valuable time during ﬁeld tests and will in the future provide a way for experimentation with deliberate planning tools; the next natural step toward complete autonomy.</p>

corrected abstract:
<p>The department of Naval Architecture at the Royal Institute of Technology is in posession of one Autonomous Underwater Vehicle (AUV) and a second is under construction. A project for doing hydrographic mapping using an Autonomous Surface Vehicle (ASV) is also initiated. These projects raises the need for a software to easily send commands to vehicles and to review collected data. The ability to use each vehicle as a node in a network of vehicles is also requested. This thesis examines a software toolchain developed at the Underwater Systems and Technology Laboratory (LSTS) in Portugal for mission planning and control of networked autonomous vehicles. The toolchain constitutes primarily of Neptus, which provides an operator with a user interface for realtime control and feedback from vehicles, and DUNE. DUNE is a software running on-board vehicles and communicates with Neptus over a wireless network. As a first step, and as a limitation to this thesis, the toolchain has been used to control an autonomous rover. An autopilot receives waypoints in form of latitude/longitude coordinates from DUNE and periodically sends position and various sensor readings back. DUNE is running on a GNU/Linux computer and is responsible for storing a mission of multiple waypoints and to keep track of the progress. DUNE forwards vehicle location and sensor data to Neptus for feedback in the user interface and generation of plots. In conclusion the author was able to create and execute missions of an arbitrary number of waypoints. Graphs of basically any sensor reading could be generated through the Mission Review and Analysis tool contained by Neptus. Implementing the toolchain on the departments marine vehicles releases valuable time during field tests and will in the future provide a way for experimentation with deliberate planning tools; the next natural step toward complete autonomy.</p>

Note spelling error:
w='posession' val={'c': 'possession', 's': 'diva2:919846', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1341413 
abstract is: 
<p>Stock market forecasting is considered to be a particularly challenging task due to the complexity and volatility of the stock market. In this project we evaluate the performance of existing machine learning techniques as methods for modeling and predicting patterns in the financial market. In our attempt to predict the Nestl\'e stock closing price point, linear LASSO and ARIMA models were implemented based on the assumption that the volatile data has some type of linear dependency. The methods was evaluated by calculating the Mean Absolute Deviation, Mean Squared Error and Mean Absolute Percentage Error values based on their performance in making short and long-term predictions. Our results suggest that the LASSO algorithm performs better in regards to short-term predictions whereas the ARIMA provides more accurate long-term predictions. In terms of prediction of future trends, both methods show good overall performance. Finally, we propose interesting areas to consider in order to make more precise predictions on volatile data.</p>

corrected abstract:
<p>Stock market forecasting is considered to be a particularly challenging task due to the complexity and volatility of the stock market. In this project we evaluate the performance of existing machine learning techniques as methods for modeling and predicting patterns in the financial market. In our attempt to predict the Nestlé stock closing price point, linear LASSO and ARIMA models were implemented based on the assumption that the volatile data has some type of linear dependency. The methods was evaluated by calculating the Mean Absolute Deviation, Mean Squared Error and Mean Absolute Percentage Error values based on their performance in making short and long-term predictions. Our results suggest that the LASSO algorithm performs better in regards to short-term predictions whereas the ARIMA provides more accurate long-term predictions. In terms of prediction of future trends, both methods show good overall performance. Finally, we propose interesting areas to consider in order to make more precise predictions on volatile data.</p>
----------------------------------------------------------------------
In diva2:1357321 
abstract is: 
<p>This paper aims to develop and to validate a methodology to realize pre-sizing studies on aircraft engine structural frames for Safran Aircraft Engines Product Innovation Lab activities. The members of this team are in charge of creating new propulsion systems architectures in accordance with product strategy guidelines or airframer needs. Aerospace industry being highly competitive, the Product Innovation Lab must be able to respond quickly and precisely to any demand emerging from aircraft manufacturers or strategy team.The main purpose of this project is to improve the methodology permitting to make weight status and feasibility estimations of the engine frame components in a preliminary design phase and in a limited amount of time. This methodology must lead to the creation of a consistent model that is closer to the requirements and specifications imposed. This paper more precisely focus on legacy commercial engine structural frame. Reflection has been conducted on the creation of a simplified parametrized model of an existing commercial engine structural frame and on the way to mesh it in order to find a good compromise between results fidelity and computation time. Regarding the weight status and feasibility results obtained with a first model, an optimization of the model configuration has finally been conducted in order to get results that fit with the specifications.Conclusion of the report is that the pre-sizing methodology can be adapted to existing commercial engine structural frame configuration. Results obtained in terms of weight status and feasibility are in accordance with the specifications and the computation time is in agreement with the expectations. It has permitted to create a model that will be taken as a reference to develop and design new engines having a configuration similar to the legacy commercial engine considered in this study. For that purpose, iterations and optimizations will be conducted on the simplified model implemented during the project in order to determine a new configuration of the pre-sized intermediate frame model which sticks with reality i.e. that respects the provided feasibility specifications.Note that due to the public nature of this report, sensitive information and data used and obtained during the project have been removed from the present paper. Nevertheless, the methodology followed has been presented and discussed in detail. Relative deviations between the results obtained and reference values have also been exposed in order to give the reader an idea of implemented model consistency.</p>

corrected abstract:
<p>This paper aims to develop and to validate a methodology to realize pre-sizing studies on aircraft engine structural frames for Safran Aircraft Engines Product Innovation Lab activities. The members of this team are in charge of creating new propulsion systems architectures in accordance with product strategy guidelines or airframer needs. Aerospace industry being highly competitive, the Product Innovation Lab must be able to respond quickly and precisely to any demand emerging from aircraft manufacturers or strategy team.</p><p>The main purpose of this project is to improve the methodology permitting to make weight status and feasibility estimations of the engine frame components in a preliminary design phase and in a limited amount of time. This methodology must lead to the creation of a consistent model that is closer to the requirements and specifications imposed. This paper more precisely focus on legacy commercial engine structural frame. Reflection has been conducted on the creation of a simplified parametrized model of an existing commercial engine structural frame and on the way to mesh it in order to find a good compromise between results fidelity and computation time. Regarding the weight status and feasibility results obtained with a first model, an optimization of the model configuration has finally been conducted in order to get results that fit with the specifications.</p><p>Conclusion of the report is that the pre-sizing methodology can be adapted to existing commercial engine structural frame configuration. Results obtained in terms of weight status and feasibility are in accordance with the specifications and the computation time is in agreement with the expectations. It has permitted to create a model that will be taken as a reference to develop and design new engines having a configuration similar to the legacy commercial engine considered in this study. For that purpose, iterations and optimizations will be conducted on the simplified model implemented during the project in order to determine a new configuration of the pre-sized intermediate frame model which sticks with reality i.e. that respects the provided feasibility specifications.</p>

Note: Should the text in the following note be included in the abstract:
<p>Note that due to the public nature of this report, sensitive information and data used and obtained during the project have been removed from the present paper. Nevertheless, the methodology followed has been presented and discussed in detail. Relative deviations between the results obtained and reference values have also been exposed in order to give the reader an idea of implemented model consistency.</p>
----------------------------------------------------------------------
In diva2:1807246 
abstract is: 
<p>We consider a simple model for invisible neutrino decay as a sub-leading effect in the standard three-flavor neutrino oscillation framework, and use the Cayley–Hamilton formalism to obtain a full set of neutrino oscillation probabilities in matter. These are given as analytical series expansions in the small parameters α ∼ O(λ^2) and s_13 ∼ O(λ), where λ ≡ 0.2 is a “book-keeping parameter” denoting the order of the expansion. We produce explicit formulas for P_eµ, P_eτ , P_µµ, P_µτ , and P_ττ to order O(λ^3), and for P_ee to order O(λ^2), all having first corrections of order O(λ^4). Moreover, we also present vacuum limits of our expressions, as well as discuss the effect of decay on unitarity. We show that all rows in the unitarity table have corrections of order O(λ^2), with the second and third rows having additional corrections of order O(1). In the limit of no decay, unitarity is restored, and we furthermore recover known results for all probabilities.</p>

corrected abstract:
<p>We consider a simple model for invisible neutrino decay as a sub-leading effect in the standard three-flavor neutrino oscillation framework, and use the Cayley–Hamilton formalism to obtain a full set of neutrino oscillation probabilities in matter. These are given as analytical series expansions in the small parameters α ∼ 𝒪(λ<sup>2</sup>) and 𝑠<sup>13</sub> &sim; 𝒪(λ), where λ ≡ 0.2 is a “book-keeping parameter” denoting the order of the expansion. We produce explicit formulas for 𝑃<sub>eµ</sub>, 𝑃<sub>eτ</sub>, 𝑃<sub>µµ</sub>, 𝑃<sub>µτ</sub>, and 𝑃<sub>ττ<(sub> to order 𝒪(λ<sup>3</sup>), and for 𝑃<sub>ee</sub> to order 𝒪(λ<sup>2</sup>), all having first corrections of order 𝒪(λ<sup>4</sup>). Moreover, we also present vacuum limits of our expressions, as well as discuss the effect of decay on unitarity. We show that all rows in the unitarity table have corrections of order 𝒪(λ<sup>2</sup>), with the second and third rows having additional corrections of order 𝒪(1). In the limit of no decay, unitarity is restored, and we furthermore recover known results for all probabilities.</p>
----------------------------------------------------------------------
In diva2:1640013 
abstract is: 
<p>This thesis investigates a process known as heat pickup which occurs in vehicle cooling system hoses. Heat pickup is an unwanted side effect of temperature differences in a car underhood compartment, whereby excess heat is tranferred into the coolant running through the cooling system hoses, affecting its operating temperature. In addition, the effectiveness of insulation in protecting against this phenomenon is investigated. This is studied experimentally by measuring the temperature change between the inlet and outlet, of coolant flowing through a hose which is subjected to a variety of air temperatures and air flow velocities. In addition to calculating the amount of heat pickup in the coolant, the overall heat transfer coefficient as a function of air temperature and air flow velocity could be quantified.</p><p>The experiment was conducted by building a miniaturies wind tunnel and placing this in a climate chamber capable of temperatures up to 140°C and circulating the coolant at 20°C and 0.1 l/s. The wind tunnel could reach air velocities up to 6 m/s and therefore, the phenomena was nvestigated at 70, 90 and 120°C  at air velocities between 2 and 6 m/s. In addition, the effect of coolant mass flow rate was investigated in order to see if there was a change in heat pickup due to any transition between the turbulent and laminar regime, since all flow rates were in the vicinity of the critical Reynolds number of 2300 for an internal pipe flow. In addition, a CFD model was developed in Star CCM+ to model the experiment and gain more insight into the underling physics of the problem.</p><p>The results show that the air velocity and temperature have significant effects on the heat picked up by the coolant, while only air velocity has large effects on the overall heat transfer coefficient. The insulation used in the experiments shows a 35% reduction in the heat picked up by the coolant and a 50% reduction in the overall heat transfer coefficient. The CFD model showed comparable results to the experiment but underestimated the heat pickup by approcimately 10%. These results can be used to improve the accuracy of the 1D CFD modelling of the whole cooling system and in turn, help improve the system's efficiency and reduce energy consumption.</p>

corrected abstract:
<p>This thesis investigates a process known as heat pickup which occurs in vehicle cooling system hoses. Heat pickup is an unwanted side effect of temperature differences in a car underhood compartment, whereby excess heat is transferred into the coolant running through the cooling system hoses, affecting its operating temperature. In addition, the effectiveness of insulation in protecting against this phenomenon is investigated. This is studied experimentally by measuring the temperature change between the inlet and outlet, of coolant flowing through a hose which is subjected to a variety of air temperatures and air flow velocities. In addition to calculating the amount of heat pickup in the coolant, the overall heat transfer coefficient as a function of air temperature and air flow velocity could be quantified.</p><p>The experiment was conducted by building a miniaturised wind tunnel and placing this in a climate chamber capable of temperatures up to 140°C and circulating the coolant at 20°C and 0.1 l/s. The wind tunnel could reach air velocities up to 6 m/s and therefore, the phenomena was investigated at 70, 90 and 120°C at air velocities between 2 and 6 m/s. In addition, the effect of coolant mass flow rate was investigated in order to see if there was a change in heat pickup due to any transition between the turbulent and laminar regime, since all flow rates were in the vicinity of the critical Reynolds number of 2300 for an internal pipe flow. In addition, a CFD model was developed in Star CCM+ to model the experiment and gain more insight into the underling physics of the problem.</p><p>The results show that the air velocity and temperature have significant effects on the heat picked up by the coolant, while only air velocity has large effects on the overall heat transfer coefficient. The insulation used in the experiments shows a 35% reduction in the heat picked up by the coolant and a 50% reduction in the overall heat transfer coefficient. The CFD model showed comparable results to the experiment but underestimated the heat pickup by approximately 10%. These results can be used to improve the accuracy of the 1D CFD modelling of the whole cooling system and in turn, help improve the system's efficiency and reduce energy consumption.</p>
----------------------------------------------------------------------
In diva2:1040728 - while is the title in the thesis and in DiVA in double quotes?

abstract is: 
<p>This thesis intends to asses the application of local flow control to wind turbines blades, in order to demonstrate that the normal root benging moment on the blades can be reduced, with the aim of increasing wind turbines life span, reducing regular maintenance and further to make a contribution to current research on wind turbines. It develops the subject by first assessing the flow characteristics of the Großer Windkannal (GroWiKa) in the Hermann-Föttinger Institute (HFI) at the Technical University of Berlin and continues with establishing the effectiveness of local flow control on the Reasearch Wind Turbine (BeRT) designed by the institute. It is divided into six chapters; the first chapter gives an overview of the literature review required, to study the different methodologies used during the experiments. In following the different experimental set-ups used will be explained, which will lead to the presentation of the different characteristics of the flow into the settling chamber of GroWiKa, so that its effects on the wind turbine can be evaluated by means of pitot tubes and hot-wires. Chapter 4 reflects the different effects of blockage calculated during the experiments and its corrections for a blockage of 40%, while chapter 5 gives an overview on the system dynamics of the flaps, where three servos has been used and compared with each other, in order to give an assesment on its effectiveness for the rotative system. The final chapter analyses the normal bending at the blade root, by yawing the wind turbine and setting the flaps into a simple prescribe motion, in pursuance of creating a 1 period disturbance and show how the normal bending at the root of the blade can be reduced by the action of local flow control. To this end it was found that the servo system was not effective enough to respond to sudden changes in the flow field. However, it will be shown that after accounting for the servos delay, it is possible to reduce the blade normal to rotor plane root bending moment up to 31%, when the correct phase shift is applied to the flap’s prescribed motion.</p>

corrected abstract:
<p>This thesis intends to asses the application of local flow control to wind turbines blades, in order to demonstrate that the normal root benging moment on the blades can be reduced, with the aim of increasing wind turbines life span, reducing regular maintenance and further to make a contribution to current research on wind turbines.</p><p>It develops the subject by first assessing the flow characteristics of the <span lang=de">Großer Windkannal</span> (GroWiKa) in the Hermann-Föttinger Institute (HFI) at the Technical University of Berlin and continues with establishing the effectiveness of local flow control on the Reasearch Wind Turbine (BeRT) designed by the institute.</p><p>It is divided into six chapters; the first chapter gives an overview of the literature review required, to study the different methodologies used during the experiments. In following the different experimental set-ups used will be explained, which will lead to the presentation of the different characteristics of the flow into the settling chamber of GroWiKa, so that its effects on the wind turbine can be evaluated by means of pitot tubes and hot-wires. Chapter 4 reflects the different effects of blockage calculated during the experiments and its corrections for a blockage of 40%, while chapter 5 gives an overview on the system dynamics of the flaps, where three servos has been used and compared with each other, in order to give an assesment on its effectiveness for the rotative system. The final chapter analyses the normal bending at the blade root, by yawing the wind turbine and setting the flaps into a simple prescribe motion, in pursuance of creating a 1 period disturbance and show how the normal bending at the root of the blade can be reduced by the action of local flow control.</p><p>To this end it was found that the servo system was not effective enough to respond to sudden changes in the flow field. However, it will be shown that after accounting for the servos delay, it is possible to reduce the blade normal to rotor plane root bending moment up to 31%, when the correct phase shift is applied to the flap’s prescribed motion.</p>


Note spelling errors in thesis:
w='benging' val={'c': 'bending', 's': 'diva2:1040728', 'n': 'error in original'}
w='Reasearch' val={'c': 'Research', 's': 'diva2:1040728', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:651323 
abstract is: 
<p>The goal of this project was to create an intuitive and clear demonstrator</p><p>of the dening properties of quantum mechanics using single slit diraction</p><p>of light, which has quantum mechanical properties because of light's waveparticle</p><p>duality. In this report we will describe the process and thoughts</p><p>behind our project of creating a portable demonstration of the uncertainty</p><p>principle. By designing and building both a physical setup with a laser, a</p><p>slit, mirrors, lenses, beam-splitters, attenuators and cameras, and developing</p><p>software which displays images from the cameras in a clear user interface</p><p>with calculations we hope that students from high schools and gymnasiums</p><p>that visit Vetenskapens Hus at Alba Nova will learn something new while</p><p>using the demonstrator.</p>
w='dening' val={'c': 'defining', 's': ['diva2:651323', 'diva2:612201'], 'n': 'missing ligature'}
w='diraction' val={'c': 'diffraction', 's': 'diva2:651323', 'n': 'missing ligature'}

corrected abstract:
<p>The goal of this project was to create an intuitive and clear demonstrator of the defining properties of quantum mechanics using single slit diffraction of light, which has quantum mechanical properties because of light's waveparticle duality. In this report we will describe the process and thoughts behind our project of creating a portable demonstration of the uncertainty principle. By designing and building both a physical setup with a laser, a slit, mirrors, lenses, beam-splitters, attenuators and cameras, and developing software which displays images from the cameras in a clear user interface with calculations we hope that students from high schools and gymnasiums that visit <span lang="sv">Vetenskapens Hus</span> at Alba Nova will learn something new while using the demonstrator.</p>
----------------------------------------------------------------------
In diva2:1440146 
abstract is: 
<p>Planning vessels can reach high operational speeds thanks to their hull design. Some hull forms will develop large volumes of spray attached to the hull surface. The whisker spray is a thin layer consisting in droplets of water which can ac-count for a large proportion of the total resistance. A new concept to redirect the spray, called de°ectors, has been developed by the Swedish company Petestep. These de°ectors indicate higher e˝ciency than the time-proven spray rails tech-nology by removing a bigger portion of the spray area. The spray is re°ected back-wards rather than to the sides, which allows kinetic energy contained in the spray sheet to be converted to additional forward thrust. However, there have only been a few studies conducted on the e˛ects of de°ectors and there is no precise method to analyze their e˝ciency over the full range of operating speeds. For the above-mentioned reason, experimental testing is needed to have a more complete under-standing of the phenomenon in calm water and waves. In this study, model scale tests of a modular planning hull are carried out at the Davidson Laboratory towing tank. The goal is to verify the beneÿts of the spray de°ectors by direct comparison with the bare hull conÿguration at the same trim angle.</p>

corrected abstract:
<p>Planning vessels can reach high operational speeds thanks to their hull design. Some hull forms will develop large volumes of spray attached to the hull surface. The whisker spray is a thin layer consisting in droplets of water which can account for a large proportion of the total resistance. A new concept to redirect the spray, called deflectors, has been developed by the Swedish company Petestep. These deflectors indicate higher efficiency than the time-proven spray rails technology by removing a bigger portion of the spray area. The spray is reflected backwards rather than to the sides, which allows kinetic energy contained in the spray sheet to be converted to additional forward thrust. However, there have only been a few studies conducted on the effects of deflectors and there is no precise method to analyze their efficiency over the full range of operating speeds. For the above-mentioned reason, experimental testing is needed to have a more complete understanding of the phenomenon in calm water and waves. In this study, model scale tests of a modular planning hull are carried out at the Davidson Laboratory towing tank. The goal is to verify the benefits of the spray deflectors by direct comparison with the bare hull configuration at the same trim angle.</p>
----------------------------------------------------------------------
In diva2:558033 
abstract is: 
<p>Fully developed turbulent pipe ows have been studied experimentally for more than a century and for more than two decades by means of Direct Numerical Simulations, nonetheless there are still unresolved and of fundamental nature issues. Among those are the scaling of the mean velocity pro le or the question whether the near-wall peak in the variance pro le is Reynolds number invariant. In this thesis new experimental results on high Reynolds number turbulent pipe ows, obtained by means of hot-wire anemometry, are carefully document and results are presented, thereby extending the Reynolds number range of an available in-house experimental database (Sattarzadeh 2011). The main threads of this thesis are the spatial resolution eects and the Reynolds number scaling of wall-bounded ows and were investigated acquiring the measurements with probes of four dierent wire-lengths at dierent Reynolds numbers covering the friction Reynolds number range of 550 &lt;R+&lt; 2 500. The small viscous length-scales encountered required a high accuracy in the wall-position. Therefore, a vibration analysis of the probe exposed to the ow was performed on two dierent traversing systems and on several probeholder/ probe con gurations, proving that the vibrations of the probe can be large and should be taken into account when choosing the traverse system and probe-holder geometry. Results of the hot-wire velocity measurements showed that when accounting for spatial resolution eects, a clear Reynolds number eect on the statistical and spectral quantities can be observed. The peak of velocity variance, for instance, appeared to increase with the Reynolds number and the growth seems to be justi ed from the increase of the low frequency modes. This result together with the appearance of an outer peak located in the low frequency range at higher Reynolds numbers suggests that the increase of the peak of the velocity variance is due to the inuence that the large-scale motions have on the near-wall cycle of the velocity uctuations. As a side results of the velocity measurements, temperature, i.e. passive scalar, mean and variance pro le were obtained by means of cold-wire anemometry. Also here, clear spatial resolution eect on the temperature variance pro le could be documented.</p>

corrected abstract:
<p>Fully developed turbulent pipe flows have been studied experimentally for more than a century and for more than two decades by means of Direct Numerical Simulations, nonetheless there are still unresolved and of fundamental nature issues. Among those are the scaling of the mean velocity profile or the question whether the near-wall peak in the variance profile is Reynolds number invariant.</p><p>In this thesis new experimental results on high Reynolds number turbulent pipe flows, obtained by means of hot-wire anemometry, are carefully document and results are presented, thereby extending the Reynolds number range of an available in-house experimental database (Sattarzadeh 2011). The main threads of this thesis are the spatial resolution effects and the Reynolds number scaling of wall-bounded flows and were investigated acquiring the measurements with probes of four different wire-lengths at different Reynolds numbers covering the friction Reynolds number range of 550 &lt; 𝑅<sup>+</sup> &lt; 2 500.</p><p>The small viscous length-scales encountered required a high accuracy in the wall-position. Therefore, a vibration analysis of the probe exposed to the flow was performed on two different traversing systems and on several probe-holder/probe configurations, proving that the vibrations of the probe can be large and should be taken into account when choosing the traverse system and probe-holder geometry.</p><p>Results of the hot-wire velocity measurements showed that when accounting for spatial resolution effects, a clear Reynolds number effect on the statistical and spectral quantities can be observed. The peak of velocity variance, for instance, appeared to increase with the Reynolds number and the growth seems to be justified from the increase of the low frequency modes. This result together with the appearance of an outer peak located in the low frequency range at higher Reynolds numbers suggests that the increase of the peak of the velocity variance is due to the influence that the large-scale motions have on the near-wall cycle of the velocity fluctuations.</p><p>As a side results of the velocity measurements, temperature, i.e. passive scalar, mean and variance profile were obtained by means of cold-wire anemometry. Also here, clear spatial resolution effect on the temperature variance profile could be documented.</p>
----------------------------------------------------------------------
In diva2:1440147 
abstract is: 
<p>Hydrodynamic analysis of high-speed craft is a complex problem. For years naval architects have tried improving the eÿciency of the hull, changing its form, the number of hulls, adding spray rails or steps. Spray deflectors, an innovative modification of commonly used spray rails by a Swedish company called Petestep, might be the next advancement in that process. Experimental validation of this concept is the topic of this study. Previous years of towing tank tests showed compelling results, but were unable to draw definite conclusions, mainly due to inability to compare the results of bare hull and deflector hull tests which was a result of unequal trim achieved for equal input parameters (such as weight and speed).With amended test speed range and deflector design modifications, this project aimed to yield unambigu-ous results. Three spray deflectors were designed for di˙erent design speeds, tested and compared with bare hull tests for drag and trim. The results showed that deflectors did not change the running position of the hull compared to bare hull, allowing for an unequivocal drag comparison. Deflectors reduced drag by up to 4.9%.A number of design improvements were proposed as next steps in this research. A further successful proof of concept (increased stability, lower resistance and fuel consumption) could provide a substantial improvement in high-speed craft design resulting in a cheaper and a more comfortable ride with a lower environmental impact.</p>

corrected abstract:
<p>Hydrodynamic analysis of high-speed craft is a complex problem. For years naval architects have tried improving the efficiency of the hull, changing its form, the number of hulls, adding spray rails or steps. Spray deflectors, an innovative modification of commonly used spray rails by a Swedish company called Petestep, might be the next advancement in that process. Experimental validation of this concept is the topic of this study. Previous years of towing tank tests showed compelling results, but were unable to draw definite conclusions, mainly due to inability to compare the results of bare hull and deflector hull tests which was a result of unequal trim achieved for equal input parameters (such as weight and speed).</p><p>With amended test speed range and deflector design modifications, this project aimed to yield unambiguous results. Three spray deflectors were designed for different design speeds, tested and compared with bare hull tests for drag and trim. The results showed that deflectors did not change the running position of the hull compared to bare hull, allowing for an unequivocal drag comparison. Deflectors reduced drag by up to 4.9%.</p><p>A number of design improvements were proposed as next steps in this research. A further successful proof of concept (increased stability, lower resistance and fuel consumption) could provide a substantial improvement in high-speed craft design resulting in a cheaper and a more comfortable ride with a lower environmental impact.</p>
----------------------------------------------------------------------
In diva2:814478 
abstract is: 
<p>In this thesis there will be an attempt to model the market price of cryptocurrencies. Since 2010 cryptocurrencies have gone from being fairly unknown to being familiar amongst the general public which increases the need for knowledge on what affects the market price of cryptocurrencies.</p><p>These connections will be found by statistical analysis and be applied on cryptocurrency data from January 2012 to January 2015. The data will be modeled by linear regression and implemented in R after the data have been formating in Excel. The results suggest that the price of cryptocurrencies depends heavily on the search traffic on the specific cryptocurrency name on Google’s search engine.</p>

corrected abstract:
<p>In this thesis there will be an attempt to model the market price of cryptocurrencies. Since 2010 cryptocurrencies have gone from being fairly unknown to being familiar amongst the general public which increases the need for knowledge on what affects the market price of cryptocurrencies.</p><p>These connections will be found by statistical analysis and be applied on cryptocurrency data from January 2012 to January 2015. The data will be modeled by linear regression and implemented in R after the data have been formating in Excel. The results suggest that the price of cryptocurrencies depends heavily on the search traffic on the specific cryptocurrency name on Google’s search engine.</p>

Note: spelling error
w='formating' val={'c': 'formatting', 's': ['diva2:814478', 'diva2:814186']}
----------------------------------------------------------------------
In diva2:1641445 
abstract is: 
<p>Proteomics is the large scale study of proteins in biological systems such as those found in human cells. The understanding of proteomes, i.e. the complete set of proteins expressed by an organism, has especially useful applications in the medical field such as genetic research and drug discovery. Cells that undergo biochemical processes affect the state of the contained proteins, therefore producing an abundance of physical variations and permutations of single proteins. The main goal of proteomic studies is to identify the proteins related to such processes in order to study their purpose and function. Successful reduction of the so-called "search space" in which proteins are identified, is a large determining factor in the resulting number of correct protein identifications. Fractionation processes attempt to reduce this search space through electrophoretic experiments such as IEF in order to identify individual protein properties such as the isoelectric point. Protein sample preparation relies heavily on isoelectric point values, denoted $p$I, and hence accurate theoretical prediction of these values would provide a benchmark to aid in analytical processes such as LC-MS. This dissertation explores the efficacy of using simple feature encoding to improve upon conventional theoretical $p$I predictions, based on the Henderson-Hasselbalch equation, in order to more accurately reflect experimental values. Simple feature encoding was used for two different optimisation techniques. Encoding chargeable amino acid residues in peptide sequences into various $k$-mer combinations produced a considerable improvement in predicted $p$I values compared to prediction solely based on reference $p$K$_a$ values. The approaches taken in this project highlighted, arguably at a fundamental level, the useful nature of peptide feature encoding to improve theoretical $p$I predictions. However, future research endeavours should consider extending the models discussed using more developed and complex modelling techniques for peptide sequences and more importantly, $p$K$_a$ constants.</p>

corrected abstract:
<p>Proteomics is the large scale study of proteins in biological systems such as those found in human cells. The understanding of proteomes, i.e. the complete set of proteins expressed by an organism, has especially useful applications in the medical field such as genetic research and drug discovery. Cells that undergo biochemical processes affect the state of the contained proteins, therefore producing an abundance of physical variations and permutations of single proteins. The main goal of proteomic studies is to identify the proteins related to such processes in order to study their purpose and function. Successful reduction of the so-called "search space" in which proteins are identified, is a large determining factor in the resulting number of correct protein identifications. Fractionation processes attempt to reduce this search space through electrophoretic experiments such as IEF in order to identify individual protein properties such as the isoelectric point. Protein sample preparation relies heavily on isoelectric point values, denoted 𝑝I, and hence accurate theoretical prediction of these values would provide a benchmark to aid in analytical processes such as LC-MS. This dissertation explores the efficacy of using simple feature encoding to improve upon conventional theoretical 𝑝I predictions, based on the Henderson-Hasselbalch equation, in order to more accurately reflect experimental values. Simple feature encoding was used for two different optimisation techniques. Encoding chargeable amino acid residues in peptide sequences into various 𝑘-mer combinations produced a considerable improvement in predicted 𝑝I values compared to prediction solely based on reference 𝑝K<em><sub>a</sub></em> values. The approaches taken in this project highlighted, arguably at a fundamental level, the useful nature of peptide feature encoding to improve theoretical 𝑝I predictions. However, future research endeavours should consider extending the models discussed using more developed and complex modelling techniques for peptide sequences and more importantly, 𝑝K<em><sub>a</sub></em> constants.</p>
----------------------------------------------------------------------
In diva2:1796308 
abstract is: 
<p>This thesis studies the Diffuse Supernova Neutrino Background (DSNB) that is expected to be detected in the future neutrino detector Hyper-Hamiokande. DSNB refers to the flux which is made up of neutrinos from all past core-collapsing supernova explosions. The ability to determine the mass hierarchy and the ability to determine parameters of the initial flux by using DNSB are investigated. For comparison is the neutrino flux detected in Hyper-Kamiokande coming from a galactic supernova at a distance of 10 kpc calculated and studied. Different aspects of the calculations are varied in order to see how they affect the ability to determine the mass hierarchy.</p><p>The result shows that the different models of the initial neutrino flux have the greatest impact on the ability to determine the mass hierarchy. According to the calculations, it will take between 3-36 years of detecting DSNB through inverted beta decay in the range of 16-30 MeV in order to determine the mass hierarchy to the confidence level of 3σ. The possibility of reducing the background noise using neutron tagging can reduce the number of years from 10 to 3 years. The detected spectrum from a single galactic supernova can provide a confidence level within the range of 43σ to 122σ. However, the DSNB can give a significant result regarding the mass hierarchy before a galactic or near-galactic supernova event is possible to detect.</p><p>The calculated confidence regions for the parameters of the initial flux show that a galactic supernova spectrum gives at least 30 times better precision than what DSNB collected over 10 years can give. The result also shows that mass hierarchy affects the size of the confidence region and the ability to determine some of the parameters.</p>

corrected abstract:
<p>This thesis studies the Diffuse Supernova Neutrino Background (DSNB) that is expected to be detected in the future neutrino detector Hyper-Kamiokande. DSNB refers to the flux which is made up of neutrinos from all past core-collapsing supernova explosions. The ability to determine the mass hierarchy and the ability to determine parameters of the initial flux by using DNSB are investigated. For comparison is the neutrino flux detected in Hyper-Kamiokande coming from a galactic supernova at a distance of 10 kpc calculated and studied. Different aspects of the calculations are varied in order to see how they affect the ability to determine the mass hierarchy.</p><p>The result shows that the different models of the initial neutrino flux have the greatest impact on the ability to determine the mass hierarchy. According to the calculations, it will take between 3-36 years of detecting DSNB through inverted beta decay in the range of 16-30 MeV in order to determine the mass hierarchy to the confidence level of 3σ. The possibility of reducing the background noise using neutron tagging can reduce the number of years from 10 to 3 years. The detected spectrum from a single galactic supernova can provide a confidence level within the range of 43σ to 122σ. However, the DSNB can give a significant result regarding the mass hierarchy before a galactic or near-galactic supernova event is possible to detect.</p><p>The calculated confidence regions for the parameters of the initial flux show that a galactic supernova spectrum gives at least 30 times better precision than what DSNB collected over 10 years can give. The result also shows that mass hierarchy affects the size of the confidence region and the ability to determine some of the parameters.</p>

Note: Spelling error:
w='DNSB' val={'c': 'DSNB', 's': 'diva2:1796308', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1528247 
abstract is: 
<p>In this study, instability and failure in complex fluids (Elastoviscoplastic fluids) is explored using the classic Considère <em>(F˙ &lt; 0)</em> and stress curvature <em>(σ¨ &lt; 0) </em>criteria. Employing the Saramito model, numerical simulation of the extensional protocol on non-Newtonian fluids is carried out. Validation is firstly performed (with a purely viscoelastic model) and in general found to be in agreement with previous works. Parameter variation of the Bingham number <em>(Bi)</em>, capillary number <em>(Ca)</em> and extension rate <em>(ε˙)</em> is then undertaken. It is found out that for Oldroyd-B based fluids, the stress curvature condition almost always occurs from inception of the flow for all cases. Additionally, increasing surface tension has a stabilizing effect on the extending fluid when it is below a critical value, above which it aids breakup. Increasing the yield stress, though, delays the onset of instability, but reduces the final length of the extending filament. At mild to high extension rates, the Considère criterion and the extension at the maximum stress are suit-able indicators of the final extension at strain-to-break(<em>εST B</em>). Furthermore, the rate of the of necking instability till final breakup varies with the<em> εST B</em> at moderate to high ε˙.</p>

corrected abstract:
<p>In this study, instability and failure in complex fluids (Elastoviscoplastic fluids<sup><a href="#fn1" id="ref1">1</a></sup>) is explored using the classic Considère (<em>Ḟ</em> &lt; 0) and stress curvature (<em>σ&#x308;</em> &lt; 0) criteria. Employing the Saramito model, numerical simulation of the extensional protocol on non-Newtonian fluids is carried out. Validation is firstly performed (with a purely viscoelastic model) and in general found to be in agreement with previous works.</p><p>Parameter variation of the Bingham number (𝐵𝑖), capillary number (𝐶𝑎) and extension rate (<em>ε&#x307;</em>) is then undertaken. It is found out that for Oldroyd-B based fluids, the stress curvature condition almost always occurs from inception of the flow for all cases. Additionally, increasing surface tension has a stabilizing effect on the extending fluid when it is below a critical value, above which it aids breakup. Increasing the yield stress, though, delays the onset of instability, but reduces the final length of the extending filament. At mild to high extension rates, the Considère criterion and the extension at the maximum stress are suitable indicators of the final extension at strain-to-break(<em>ε<sub>STB</sub></em>). Furthermore, the rate of the of necking instability till final breakup varies with the <em>ε<sub>STB</sub></em> at moderate to high <em>ε&#x307;</em>.</p>
<div id="footnotes">
    <ol>
        <li id="fn1">Subsequently referred to as EVP <a href="#ref1" aria-label="Back to reference">↩</a></ĺi>
    </ol>
</div>
----------------------------------------------------------------------
In diva2:1057135 
abstract is: 
<p>Wave energy is one of the possible solutions for meeting the future energy demand in a clean and sustainable way. Extracting large amounts of energy, a wave energy device would be subjected to extreme and fatigue loads from the waves. Designing such a device, a trade off needs to be done between making a device that is strong enough to withstand the loads and on the same time not too heavy making it inefficient and too costly. Having good estimations of extreme and fatigue loads are therefore critical when designing an efficient wave energy device. This thesis has aimed to create a tool that can be used between the already existing hydrodynamic and solid mechanic models available at CorPower Oceean. The goal has been that the tool shall extract the extreme and fatigue loads from the hydrodynamic model and format them in a way so that they can be used in the solid mechanical model. Four different tools have been created and compared for calculating fatigue using amplitude and spectral methods, where the amplitude methods also are able to estimate extreme loads. The fatigue tools have been evaluated against each other in a simple example showing that the estimated accumulated fatigue damage can be decreased by using several variables. An application of the tools has been done on a critical sub system of the wave energy device developed by CorPower Ocean. Where in this application critical points against extreme loading and fatigue have been localized. A new design has been suggested based on the strength analysis from the first one. Increasing the number of variables and using the tools developed in this thesis can significantly improve the fatigue damage estimations of the system. What fatigue method to use depends on the details for each case.</p>

corrected abstract:
<p>Wave energy is one of the possible solutions for meeting the future energy demand in a clean and sustainable way. Extracting large amounts of energy, a wave energy device would be subjected to extreme and fatigue loads from the waves. Designing such a device, a trade off needs to be done between making a device that is strong enough to withstand the loads and on the same time not too heavy making it inefficient and too costly. Having good estimations of extreme and fatigue loads are therefore critical when designing an efficient wave energy device. This thesis has aimed to create a tool that can be used between the already existing hydrodynamic and solid mechanic models available at CorPower Oceean. The goal has been that the tool shall extract the extreme and fatigue loads from the hydrodynamic model and format them in a way so that they can be used in the solid mechanical model.</p><p>Four different tools have been created and compared for calculating fatigue using amplitude and spectral methods, where the amplitude methods also are able to estimate extreme loads. The fatigue tools have been evaluated against each other in a simple example showing that the estimated accumulated fatigue damage can be decreased by using several variables. An application of the tools has been done on a critical sub system of the wave energy device developed by CorPower Ocean. Where in this application critical points against extreme loading and fatigue have been localized. A new design has been suggested based on the strength analysis from the first one. Increasing the number of variables and using the tools developed in this thesis can significantly improve the fatigue damage estimations of the system. What fatigue method to use depends on the details for each case.</p>

Note: spelling error:
w='Oceean' val={'c': 'Ocean', 's': 'diva2:1057135', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1087823 
abstract is: 
<p>One of today’s financial trends is securitization. Evaluating Securitization risk requires some strong quantitative skills and a deep understanding of both credit and market risk. For international securitization programs it is mandatory to take into account the exchange-rates-related risks. We will see the di˙erent methods to evaluate extreme variations of the exchange rates using the Extreme Value Theory and Monte Carlo simulations.</p>

corrected abstract:
<p>One of today’s financial trends is securitization. Evaluating Securitization risk requires some strong quantitative skills and a deep understanding of both credit and market risk. For international securitization programs it is mandatory to take into account the exchange-rates-related risks. We will see the different methods to evaluate extreme variations of the exchange rates using the Extreme Value Theory and Monte Carlo simulations.</p>
----------------------------------------------------------------------
In diva2:919081 
abstract is: 
<p>In this thesis, UN-U<sub>3</sub>Si<sub>2</sub> nuclear fuel was fabricated using spark plasma sintering and characterized to analyze the microstructure and crystal structure of the resulting pellets. This work was done in collaboration with accident tolerant fuel research, an effort which aims at developing nuclear fuel with superior safety and performance compared to currently used oxide fuels.</p><p>Uranium silicide was manufactured by arc melting to produce U<sub>3</sub>Si<sub>2</sub> and uranium mononitride was synthesized by using the hydriding-nitriding method. They were mixed in varying compositions (5 wt%, 10 wt%, 20 wt%, and 25 wt% U<sub>3</sub>Si<sub>2</sub>) in order to create composite fuel pellets. Characterization of the resulting pellets showed an apparent ternary phase of U-N-Si, prompting fabrication of four more pellets at varying temperatures (1200 °C, 1300 °C, 1400 °C, and 1500 °C) to try and identify the temperature of phase formation.</p><p>The presence of a probable ternary U-N-Si phase was confirmed to be present in all fuel pellets. Therefore, further investigation into the thermodynamic behavior of the ternary U-N-Si system is suggested before this fuel can be recommended for use in a reactor. </p>

corrected abstract:
<p>In this thesis, UN-U<sub>3</sub>Si<sub>2</sub> nuclear fuel was fabricated using spark plasma sintering and characterized to analyze the microstructure and crystal structure of the resulting pellets. This work was done in collaboration with accident tolerant fuel research, an effort which aims at developing nuclear fuel with superior safety and performance compared to currently used oxide fuels.</p><p>Uranium silicide was manufactured by arc melting to produce U<sub>3</sub>Si<sub>2</sub> and uranium mononitride was synthesized by using the hydriding-nitriding method. They were mixed in varying compositions (5 wt%, 10 wt%, 20 wt%, and 25 wt% U<sub>3</sub>Si<sub>2</sub>) in order to create composite fuel pellets. Characterization of the resulting pellets showed an apparent ternary phase of U-N-Si, prompting fabrication of four more pellets at varying temperatures (1200 °C, 1300 °C, 1400 °C, and 1500 °C) to try and identify the temperature of phase formation.</p><p>The presence of a probable ternary U-N-Si phase was confirmed to be present in all fuel pellets. Therefore, further investigation into the thermodynamic behavior of the ternary U-N-Si system is suggested before this fuel can be recommended for use in a reactor.</p>
----------------------------------------------------------------------
In diva2:1446594 
abstract is: 
<p>This report investigates what the main price drivers are for commercial real estate rentals in Stockholm and Gothenburg. The mathematical method applied in this thesis is multiple linear regression and statistical analysis. The models are built from data provided by Datscha, a commercial market information provider. The data sets contains 922 observations across 9 different metrics from 2019. The response variable used to explain the price drivers is taxated monthly rental. The most significant driving variables common to all three final models where market value, location, and taxated value. These results align with current macroeconomic theory; revenue streams stand in direct proportion to underlying asset, i.e market value. Furthermore, location stands out as significant due to its attractiveness to all interacting entities. The models constructed had satisfying predictabilty, with R2-values ranging from 0.725 − 0.896.</p>

corrected abstract:
<p>This report investigates what the main price drivers are for commercial real estate rentals in Stockholm and Gothenburg. The mathematical method applied in this thesis is multiple linear regression and statistical analysis. The models are built from data provided by Datscha, a commercial market information provider. The data sets contains 922 observations across 9 different metrics from 2019. The response variable used to explain the price drivers is taxated monthly rental. The most significant driving variables common to all three final models where market value, location, and taxated value. These results align with current macroeconomic theory; revenue streams stand in direct proportion to underlying asset, i.e market value. Furthermore, location stands out as significant due to its attractiveness to all interacting entities. The models constructed had satisfying predictabilty, with 𝑅<sup>2</sup>-values ranging from 0.725 − 0.896.</p>

Note - spelling error:
w='predictabilty' val={'c': 'predictability', 's': 'diva2:1446594', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1757012 
abstract is: 
<p>This thesis report analyzes how different variables affected housing pricing in Boston in the 1980s. The goal was to form a deeper understanding of what could affect pricing for properties, both now and then. The method for analyzing this is a multiple linear regression analysis. The theory behind the work is based on linear regression and macroeconomics. The model is based on data collected by Harrison, D. and Rubinfeld, D.L. and obtained from The Carnegie Mellon University, Pittsburgh, Pennsylvania. The data consists of 506 observations with 20 variables each. The median value of the properties in a specific area measured in thousands of dollars is used as the response variable.The results of the report show that the number of rooms per dwelling has the largest effect on dwelling price, accounting for almost 40% of the influence in the final model, among the variables that were used. In second place comes the number of teachers per student and in third place is the tax rate in the area.The model result taken from showed good ability to approximately predict housing prices, with an adjusted R2 value of 0.8065.</p>

corrected abstract:
<p>This thesis report analyzes how different variables affected housing pricing in Boston in the 1980s. The goal was to form a deeper understanding of what could affect pricing for properties, both now and then. The method for analyzing this is a multiple linear regression analysis. The theory behind the work is based on linear regression and macroeconomics. The model is based on data collected by Harrison, D. and Rubinfeld, D.L. and obtained from The Carnegie Mellon University, Pittsburgh, Pennsylvania. The data consists of 506 observations with 20 variables each. The median value of the properties in a specific area measured in thousands of dollars is used as the response variable.</p><p>The results of the report show that the number of rooms per dwelling has the largest effect on dwelling price, accounting for almost 40% of the influence in the final model, among the variables that were used. In second place comes the number of teachers per student and in third place is the tax rate in the area.</p><p>The model result taken from showed good ability to approximately predict housing prices, with an adjusted R<sup>2</sup> value of 0.8065.</p>
----------------------------------------------------------------------
In diva2:818905 
Note: no full text in DiVA

abstract is: 
<p>The standard numerical methods developed for handling evolutions of fronts use a set of marker points as a discrete representation of the interface on its path in a velocity field. In this study a multi resolution representation will be introduced, replacing the markers. The interface will be described by a set of marker related wavelet vectors. Like the markers the vectors satisfy ordinary dffierential equations. In much the same way as the markers, the vectors are traced in the same velocity field with the same properties. In addition, the vectors evolve slower for the finer spatial scaled vectors. The intent is to exploit this property by increasing the time step for finer scales in order to reduce computational cost without compromising accuracy. Two multi resolution ODE algorithms will be developed and examined with respect to computational cost and accuracy, both theoretical and empirical for comparison with their marker point representation.</p>

corrected abstract:
<p>The standard numerical methods developed for handling evolutions of fronts use a set of marker points as a discrete representation of the interface on its path in a velocity field. In this study a multi resolution representation will be introduced, replacing the markers. The interface will be described by a set of marker related wavelet vectors. Like the markers the vectors satisfy ordinary differential equations. In much the same way as the markers, the vectors are traced in the same velocity field with the same properties. In addition, the vectors evolve slower for the finer spatial scaled vectors. The intent is to exploit this property by increasing the time step for finer scales in order to reduce computational cost without compromising accuracy. Two multi resolution ODE algorithms will be developed and examined with respect to computational cost and accuracy, both theoretical and empirical for comparison with their marker point representation.</p>

Note - only change:
w='dffierential' val={'c': 'differential', 's': 'diva2:818905', 'n': 'no full text'}
----------------------------------------------------------------------
In diva2:1446465 - missing spaces in title:
"Fatigue and Ultimate StrengthAssessment of Post Weld TreatedStrenx 1100 Plus Butt Welds"
==>
"Fatigue and Ultimate Strength Assessment of Post Weld Treated Strenx 1100 Plus Butt Welds"
abstract is: 
<p>The demand of stronger and lighter constructions are rapidly growing. Today's applications vary over a large range industries where the requirements are di_erent from product to product. Therefore, the focus of much research are the 3rd generation of so called Advance High-Strength Steels (AHSS). These are high-strength steels that are taylormade to get desired mechanical properties. One AHSS manufactured by SSAB is their Strenx 1100Plus steel that is specially design to get excellent welding properties. In this project it was investigated if the two post weld treatments TIG dressing and HFMI has a larger e_ect in the material then the recommendations for steel in general by the International institute of Welding (IIW). By comparing nominal fatigue stresses to standards it could be determined that the post treatment improves the fatigue life with 1 more fatigue class then the recommendations for each treatment. E_ective notch stress simulations was used to derive S-N curves for comparative reasons.</p>

corrected abstract:
<p>The demand of stronger and lighter constructions are rapidly growing. Today's applications vary over a large range industries where the requirements are different from product to product. Therefore, the focus of much research are the 3rd generation of so called Advance High-Strength Steels (AHSS). These are high-strength steels that are taylormade to get desired mechanical properties. One AHSS manufactured by SSAB is their Strenx 1100Plus steel that is specially design to get excellent welding properties. In this project it was investigated if the two post weld treatments TIG dressing and HFMI has a larger effect in the material then the recommendations for steel in general by the International institute of Welding (IIW). By comparing nominal fatigue stresses to standards it could be determined that the post treatment improves the fatigue life with 1 more fatigue class then the recommendations for each treatment. Effective notch stress simulations was used to derive S-N curves for comparative reasons.</p>

Note: "taylormade" would notmally be spelled as "taylor-made" an adjective; however, it appears as one word in the thesis
----------------------------------------------------------------------
In diva2:462138 
abstract is: 
<p>A Finite Element Analysis of a load test of the LAS-937 adapter has been made to check if there is any risk that the aluminuim in the upper interface of the adapter yields. The analysis was done in ABAQUS CAE. The reason for doing the analysis is that the adapter was measured to be oval after the load-test. Several different analysises were run to check how sensitive the model was to a higher load or lower yield strength in the alumium. It was found that the resulting stresses in the FE-model interface is not high enough to explain the ovality of the the interface. Even with the higher load or lower yield strength properties the deformations in the FE-model were not as high as the measured ovality.</p>

corrected abstract:
<p>A Finite Element Analysis of a load test of the LAS-937 adapter has been made to check if there is any risk that the aluminuim in the upper interface of the adapter yields. The analysis was done in ABAQUS CAE. The reason for doing the analysis is that the adapter was measured to be oval after the load-test. Several different analysises were run to check how sensitive the model was to a higher load or lower yield strength in the alumium. It was found that the resulting stresses in the FE-model interface is not high enough to explain the ovality of the the interface. Even with the higher load or lower yield strength properties the deformations in the FE-model were not as high as the measured ovality.</p>

Note spelling errors:
w='analysises' val={'c': 'analyses', 's': 'diva2:462138', 'n': 'error in original'}
w='alumium' val={'c': 'aluminum', 's': 'diva2:462138', 'n': 'error in original'}
w='aluminuim' val={'c': 'aluminum', 's': 'diva2:462138', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1440153 
abstract is: 
<p>The trend of increasingly automated tightening stations in the motor vehicle powertrain production industry, is driving the tooling industry to adapt its of-fering. This thesis aims to answer the question if collaborative robots (cobots) are suitable for tightening applications in motor vehicle engine assembly lines. A collaborative robot is in principle similar to a conventional industrial robotic arm. The part that diﬀerentiates it from an industrial robot is its ability to detect and in some cases prevent a collision with a human of close proximity. In order to answer this question three case studies are conducted where the deployment of collaborative robots is assessed. To speciﬁcally answer the case of a motor vehicle engine line, two lines are mapped station by station. Then these stations are recreated with a collaborative robot and a tightening tool and a few applications are tested on an actual motor. The rest is simulated with equations, dictating how much time is needed to complete a sequence of tightenings.As a result, most stations on a motor vehicle assembly line could in theory be done by using tightening tools together with one or multiple collaborative robots. However, these kinds of lines are often highly automated. For this rea-son large industrial robots and multi-tool ﬁxtures are preferred today. The large volumes of these lines, mostly mitigate the expected ﬁnancial gains of replacing a human operator with a cobot. In many cases, the cobot would not have the time to complete all bolts of an application on an engine within the give takt time. Multi-cylinder engines (which is the absolute norm for passenger cars) have too many bolts for one cobot and tool to complete. If multiple cobots are to be used, the cost of the station further increases. Furthermore, re-balancing a line simply to accommodate a cobot would also not be beneﬁcial.Probably a much better application for collaborative robots and tightening tools are smaller, more operator dependent lines. Here the value of a cobot can be utilized to a greater extent. These manufacturers have greater product variance and re-balance their lines more often. As a suggestion for further research, this segment could be invesigated further.</p>

corrected abstract:
<p>The trend of increasingly automated tightening stations in the motor vehicle powertrain production industry, is driving the tooling industry to adapt its offering. This thesis aims to answer the question if collaborative robots (cobots) are suitable for tightening applications in motor vehicle engine assembly lines. A collaborative robot is in principle similar to a conventional industrial robotic arm. The part that differentiates it from an industrial robot is its ability to detect and in some cases prevent a collision with a human of close proximity. In order to answer this question three case studies are conducted where the deployment of collaborative robots is assessed. To specifically answer the case of a motor vehicle engine line, two lines are mapped station by station. Then these stations are recreated with a collaborative robot and a tightening tool and a few applications are tested on an actual motor. The rest is simulated with equations, dictating how much time is needed to complete a sequence of tightenings.</p><p>As a result, most stations on a motor vehicle assembly line could in theory be done by using tightening tools together with one or multiple collaborative robots. However, these kinds of lines are often highly automated. For this reason large industrial robots and multi-tool fixtures are preferred today. The large volumes of these lines, mostly mitigate the expected financial gains of replacing a human operator with a cobot. In many cases, the cobot would not have the time to complete all bolts of an application on an engine within the give takt time. Multi-cylinder engines (which is the absolute norm for passenger cars) have too many bolts for one cobot and tool to complete. If multiple cobots are to be used, the cost of the station further increases. Furthermore, re-balancing a line simply to accommodate a cobot would also not be beneficial.</p><p>Probably a much better application for collaborative robots and tightening tools are smaller, more operator dependent lines. Here the value of a cobot can be utilized to a greater extent. These manufacturers have greater product variance and re-balance their lines more often. As a suggestion for further research, this segment could be invesigated further.</p>

Note - spelling error:
w='invesigated' val={'c': 'investigated', 's': 'diva2:1440153', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:652623 
abstract is: 
<p>X-ray computed tomography (CT) is currently a vital diagnostic tool in hospitals</p><p>the world over. To be able to image a patient's interior quickly can facilitate a</p><p>diagnosis so that proper treatment can be administered. In some cases a contrast</p><p>agent is administered intravenously prior to the CT-scan, in order to enhance image</p><p>quality. Most contrast agents are heavy-element based and have a sudden increase</p><p>in attenuation at the k-edge due to photoelectric absorption of the photons.</p><p>New technology utilizing multi-bin spectral CT is under development. This tech-</p><p>nology opens up for the possibility to isolate the image contribution of this sudden</p><p>increase in attenuation at the the k-edge.</p><p>This report investigates the feasibility of using gadolinium-based contrast agents</p><p>(GBCA) in multi-bin spectral CT. With a high k-edge, gadolinium is well suited</p><p>for k-edge imaging. However, GBCA are currently only endorsed for magnetic res-</p><p>onance imaging (MRI), raising the question if currently used concentrations are</p><p>su-cient for CT practice.</p><p>We model two cross-sections containing targets of various areas and determine the</p><p>detectability of said targets for concentrations currently endorsed in MRI. Higher</p><p>concentrations are considered when motivated.</p><p>We conclude that detection of lesions and haemorrhaging in soft tissue, as well</p><p>as cerebral haemorrhaging, is possible with the concentrations currently exercised</p><p>in MRI. Additionally, we conclude that to detect residual blood 　ow in the case of</p><p>ischemia caused by thrombosis, higher concentrations must be considered.</p><p>These results clearly indicate that currently endorsed concentrations of GBCA, in</p><p>combination with k-edge imaging, could provide su-cient contrast in CT-practice.</p>


corrected abstract:
<p>X-ray computed tomography (CT) is currently a vital diagnostic tool in hospitals the world over. To be able to image a patient's interior quickly can facilitate a diagnosis so that proper treatment can be administered. In some cases a contrast agent is administered intravenously prior to the CT-scan, in order to enhance image quality. Most contrast agents are heavy-element based and have a sudden increase in attenuation at the k-edge due to photoelectric absorption of the photons.</p><p>New technology utilizing multi-bin spectral CT is under development. This technology opens up for the possibility to isolate the image contribution of this sudden increase in attenuation at the the k-edge.</p><p>This report investigates the feasibility of using gadolinium-based contrast agents (GBCA) in multi-bin spectral CT. With a high k-edge, gadolinium is well suited for k-edge imaging. However, GBCA are currently only endorsed for magnetic resonance imaging (MRI), raising the question if currently used concentrations are sufficient for CT practice.</p><p>We model two cross-sections containing targets of various areas and determine the detectability of said targets for concentrations currently endorsed in MRI. Higher concentrations are considered when motivated.</p><p>We conclude that detection of lesions and haemorrhaging in soft tissue, as well as cerebral haemorrhaging, is possible with the concentrations currently exercised in MRI. Additionally, we conclude that to detect residual blood flow in the case of ischemia caused by thrombosis, higher concentrations must be considered.</p><p>These results clearly indicate that currently endorsed concentrations of GBCA, in combination with k-edge imaging, could provide sufficient contrast in CT-practice.</p>
----------------------------------------------------------------------
In diva2:1502896 
abstract is: 
<p>This master thesis was carried out at Scania CV AB. The focus for this thesis is the prediction of welding distortions that can cause problems in the manufacturing process of Scania's after-treatment system. The after-treatment system is mainly assembled by sheet metal plates of the ferritic stainless steel EN 1.4509. The plates are welded together.</p><p>When welding, distortions and residual stresses occur, and they also depend on the sequence in the component was welded together. The distortions and residual stresses can cause tolerance related issues and a lower lifetime for the welded components. Experiments are expensive and therefore it is desirable to simulate the welding process, thereby controlling distortions and optimizing welding sequences.</p><p>To simulate the welding process and predict the welding distortions a thermo-mechanical FE-model was created for two typical welds found on the after-treatment system. The first scenario was two thin plates welded onto each other in an overlap weld joint and the second scenario was a thin plate welded onto a thick plate in a overlap weld joint. After the FE-model was compared to the experiments. An optimization of the welding sequences was also made on a larger component typically found on the after-treatment system.</p><p>The FE-model can predict the distortion shape with good accuracy for the T-fillet weld, while the model predicted a more symmetric distortion shape on the overlap weld compared to a more asymmetric shape found on the experiments, but the error is still not very large. The Fe-model can also be used to optimize the welding sequence for bigger components on the after-treatment system within a reasonable time span compared to doing the opimization manually in an experiment.</p>

corrected abstract:
<p>This master thesis was carried out at Scania CV AB. The focus for this thesis is the prediction of welding distortions that can cause problems in the manufacturing process of Scania's after-treatment system. The after-treatment system is mainly assembled by sheet metal plates of the ferritic stainless steel EN 1.4509. The plates are welded together.</p><p>When welding, distortions and residual stresses occur, and they also depend on the sequence in the component was welded together. The distortions and residual stresses can cause tolerance related issues and a lower lifetime for the welded components. Experiments are expensive and therefore it is desirable to simulate the welding process, thereby controlling distortions and optimizing welding sequences.</p><p>To simulate the welding process and predict the welding distortions a thermo-mechanical FE-model was created for two typical welds found on the after-treatment system. The first scenario was two thin plates welded onto each other in an overlap weld joint and the second scenario was a thin plate welded onto a thick plate in a overlap weld joint. After the FE-model was compared to the experiments. An optimization of the welding sequences was also made on a larger component typically found on the after-treatment system.</p><p>The FE-model can predict the distortion shape with good accuracy for the T-fillet weld, while the model predicted a more symmetric distortion shape on the overlap weld compared to a more asymmetric shape found on the experiments, but the error is still not very large. The Fe-model can also be used to optimize the welding sequence for bigger components on the after-treatment system within a reasonable time span compared to doing the opimization manually in an experiment.</p>

Note spelling error:
w='opimization' val={'c': 'optimization', 's': 'diva2:1502896', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1880226   - correct as is

Note spelling errors:
w='arrythmia' val={'c': 'arrhythmia', 's': 'diva2:1880226', 'n': 'error in original'}
w='arrythmias' val={'c': 'arrhythmias', 's': 'diva2:1880226', 'n': 'error in original'}
w='Arrythmia' val={'c': 'Arrhythmia', 's': 'diva2:1880226', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:658555 
abstract is: 
<p>In this thesis a model for predicting if someone has an over-risk for long-term sickness absence during the forthcoming year is developed. The model is a classification tree that classifies objects as having high or low risk for long-term sickness absence based on their answers on the Health-Watch form. The HealthWatch form is a questionnaire about health consisting of eleven questions, such as "How do you feel right now?", "How did you sleep last night?", "How is your job satisfaction right now?" etc. As a measure on risk for long-term sickness absence, the Oldenburg Burnout Inventory and a scale for performance based self-esteem are used. Separate models are made for men and for women. The model for women shows good enough performance on a test set for being acceptable as a general model and can be used for prediction. Some conclusions can also be drawn from the additional information given by the classification tree; workload and work atmosphere do not seem to contribute a lot to an in-creased risk for long-term sickness absence, while job satisfaction seems to be one of the most important factors. The model for men performs poorly on a test set, and therefore it is not advisable to use it for prediction or to draw other conclusions from it.</p>


corrected abstract:
<p>In this thesis a model for predicting if someone has an over-risk for long-term sickness absence during the forthcoming year is developed. The model is a classification tree that classifies objects as having high or low risk for long-term sickness absence based on their answers on the HealthWatch form. The HealthWatch form is a questionnare about health consisting of eleven questions, such as "How do you feel right now?", "How did you sleep last night?", "How is your job satisfaction right now?" etc. As a measure on risk for long-term sickness absence, the Oldenburg Burnout Inventory and a scale for performance based self-esteem are used.</p><p>Separate models are made for men and for women. The model for women shows good enough performance on a test set for being acceptable as a general model and can be used for prediction. Some conclusions can also be drawn from the additional information given by the classification tree; workload and work atmosphere do not seem to contribute a lot to an increased risk for long-term sickness absence, while job satisfaction seems to be one of the most important factors.</p><p>The model for men performs poorly on a test set, and therefore it is not adivsabled advisable to use it for prediction or to draw other conclusions from it.</p>


Note: Spelling errors:
    'questionnare': {'c': 'questionnaire', 's': 'diva2:658555', 'n': 'error in original'},
    'adivsable': {'c': 'advisable', 's': 'diva2:658555', 'n': 'error in original'},
----------------------------------------------------------------------
In diva2:1342411 
abstract is: 
<p>Reliable computational tools are important in general to understand the vari- ous processes around us, and the simulation of flow in reservoirs in particular is vital for responding to some of our time’s urgent demands. For instance, stable groundwater basins, sustainable oil extraction, and safe nuclear waste repositories all require reservoir simulation. In this thesis, two methods for discretizing Poisson’s equation with variable coecients will be implemented and compared. This equation is a partial di↵er- ential equation that describes stationary, incompressible fluid flow in a porous medium: an ubiquitous process in reservoirs. In the first method, Taylor ex- pansions are used for discrete points (the Finite Di↵erence method), and in the second one, the concept of flux makes it possible to state continuity relations be- tween cells (the Finite Volume method, here the O-method). The methods yield di↵erent stencils which may be thought of as recipes for discretizing the equa- tion. While the former is a derived to obtain a certain numerical consistency, the last method is derived on principles of local flux conservation. The stencils are judged by their convergence rate, and two tests are per- formed to see which is the more accurate stencil. Both are implemented on a Cartesian grid, and it is measured how far the numerical solutions are from the analytical one. The tests are carried out with a full, heterogenous permeability tensor, which in case 1 varies smoothly over the whole domain. In this case it is found that the error series of both stencils are proportional to h2, where h is the step size. The FDM stencil reduces the error with 12% compared to the FVM stencil. In case 2, an imposed discontinuity ”gap” in the permeability leads to a convergence rate in the interval [0.73, 1.11], but the FVM stencil has a higher convergence rate than the FDM stencil. Curiously, the convergence rate are not correlated to the magnitude of the gap. Experiments using other grid refinement techniques are suggested to carry the comparison further. The results illustrate that in the case of continuous media, a straight-forward Taylor expansion of the equation is converging with the same convergence rate as the more elaborate O-method. The testings can furthermore verify that the O-method converges to second order for heterogeneous media, which was proved for homogeneous media by Aavatsmark. Moreover, tests on discontinous media show that the O-method performs slightly better than the Taylor method while losing the second order convergence.</p>

corrected abstract:
<p>Reliable computational tools are important in general to understand the various processes around us, and the simulation of flow in reservoirs in particular is vital for responding to some of our time’s urgent demands. For instance, stable groundwater basins, sustainable oil extraction, and safe nuclear waste repositories all require reservoir simulation.</p><p>In this thesis, two methods for discretizing Poisson’s equation with variable coefficients will be implemented and compared. This equation is a partial differential equation that describes stationary, incompressible fluid flow in a porous medium: an ubiquitous process in reservoirs. In the first method, Taylor expansions are used for discrete points (the Finite Difference method), and in the second one, the concept of flux makes it possible to state continuity relations between cells (the Finite Volume method, here the O-method). The methods yield different <em>stencils</em> which may be thought of as recipes for discretizing the equation. While the former is a derived to obtain a certain numerical consistency, the last method is derived on principles of local flux conservation.</p><p>The stencils are judged by their convergence rate, and two tests are performed to see which is the more accurate stencil. Both are implemented on a Cartesian grid, and it is measured how far the numerical solutions are from the analytical one. The tests are carried out with a full, heterogenous permeability tensor, which in case 1 varies smoothly over the whole domain. In this case it is found that the error series of both stencils are proportional to ℎ<sup>2</sup>, where ℎ is the step size. The FDM stencil reduces the error with 12% compared to the FVM stencil. In case 2, an imposed discontinuity ”gap” in the permeability leads to a convergence rate in the interval [0.73, 1.11], but the FVM stencil has a higher convergence rate than the FDM stencil. Curiously, the convergence rate are not correlated to the magnitude of the gap. Experiments using other grid refinement techniques are suggested to carry the comparison further.</p><p>The results illustrate that in the case of continuous media, a straight-forward Taylor expansion of the equation is converging with the same convergence rate as the more elaborate O-method. The testings can furthermore verify that the O-method converges to second order for heterogeneous media, which was proved for homogeneous media by [5]. Moreover, tests on discontinous media show that the O-method performs slightly better than the Taylor method while losing the second order convergence.</p>


Note:
w='discontinous' val={'c': 'discontinuous', 's': 'diva2:1342411', 'n': 'error in original'}
w='Aavatsmark' val={'c': '[5]', 's': 'diva2:1342411', 'n': 'The actual thesis has the citation, someone has replaced this with the last name of the author of the 5th publication'}
----------------------------------------------------------------------
In diva2:1570997 
abstract is: 
<p>The current description of physics on the very smallest scales is given by the standard model (SM) of particle physics. It makes many good predictions and is a highly successful theory. There are however phenomena it cannot explain, and it is therefore not a complete theory of particle physics. Current efforts within theoretical particle physics are focused on theories beyond the SM which may amend its shortcomings.</p><p>The SM also has aesthetic problems, in that several of its features can be seen as unnatural. There are unexplained patterns in measured masses and mixing parameters, and the fact that the SM is based on three different symmetries is rather inelegant. This too prompts theories beyond the SM. The introduction of flavour symmetries could help explain the observed patterns, whereas grand unified theories (GUTs) unify the symmetries of the SM. GUTs manage this through some large gauge group which is spontaneously broken to the SM one below some large energy scale. One such proposed group is SU(5), which is studied in this thesis. The specific model chosen is renormalizable and non-sypersymmetric and includes a 45 and a 15 representation of scalars to predict correct fermion masses.</p><p>Previous studies have examined what possible flavour symmetries may be imposed on GUTs based on the group SO(10). This thesis carries out similar studies for the chosen SU(5) model. Such symmetries reduce the number of free parameters in the model and could increase the predicted proton lifetime, potentially causing better agreement with experiment. The couplings between fermions and scalars in the model are considered, and how they limit the possibilities for flavour symmetry. Several requirements are imposed on the coupling matrices of the model in the interest of agreeing with experimental data. Given these requirements, 25 distinct realistic cases with flavour symmetry are found. The symmetries of these cases are Z2, Z3, Z4 and U(1). Finally, numerical fits of the cases show that two of them are capable of fitting experimentally determined masses and mixing parameters. The two cases have Z2 and U(1) symmetry, where the second is a subcase of the first.</p>


corrected abstract:
<p>The current description of physics on the very smallest scales is given by the standard model (SM) of particle physics. It makes many good predictions and is a highly successful theory. There are however phenomena it cannot explain, and it is therefore not a complete theory of particle physics. Current efforts within theoretical particle physics are focused on theories beyond the SM which may amend its shortcomings.</p><p>The SM also has aesthetic problems, in that several of its features can be seen as unnatural. There are unexplained patterns in measured masses and mixing parameters, and the fact that the SM is based on three different symmetries is rather inelegant. This too prompts theories beyond the SM. The introduction of flavour symmetries could help explain the observed patterns, whereas grand unified theories (GUTs) unify the symmetries of the SM. GUTs manage this through some large gauge group which is spontaneously broken to the SM one below some large energy scale. One such proposed group is SU(5), which is studied in this thesis. The specific model chosen is renormalizable and non-sypersymmetric and includes a <strong>45</strong> and a <strong>15</strong> representation of scalars to predict correct fermion masses.</p><p>Previous studies have examined what possible flavour symmetries may be imposed on GUTs based on the group SO(10). This thesis carries out similar studies for the chosen SU(5) model. Such symmetries reduce the number of free parameters in the model and could increase the predicted proton lifetime, potentially causing better agreement with experiment. The couplings between fermions and scalars in the model are considered, and how they limit the possibilities for flavour symmetry. Several requirements are imposed on the coupling matrices of the model in the interest of agreeing with experimental data. Given these requirements, 25 distinct realistic cases with flavour symmetry are found. The symmetries of these cases are Z<sub>2</sub>, Z<sub>3</sub>, Z<sub>4</sub> and U(1). Finally, numerical fits of the cases show that two of them are capable of fitting experimentally determined masses and mixing parameters. The two cases have Z<sub>2</sub> and U(1) symmetry, where the second is a subcase of the first.</p>

Note spelling error:
w='non-sypersymmetric' val={'c': 'non-supersymmetric', 's': 'diva2:1570997', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:891472 
abstract is: 
<p>It is well known that the air in an aircraft cabin can be dry and uncomfortable. What is not commonly known is that the human resistants to bacteria and viruses is reduced in this dry air. For long flight the relative humidity inside an aircraft can be reduced to as low as values as 5%, sometimes even lower. This low relative humidity is very extreme and can normaly be found in some deserts. To solve this, the aircraft companies can install humidifiers and raise the relative humidity to more comfortable levels. CTT System has a product which consists of layers of glass fiber sheets which are made wet with a watering system. When the dry air flows through this wet block the water evaporates into the air. This air is then taken to selected parts of the cabin. This project was made to get a deeper understanding of this product. The goal was to investigate two dierent models to simulate the airflow through this humidifier. For this task SOLIDWORKS Flow Simulation was used. It is a commercial Fluid Dynamic solver. To validate the result from the simulation experimental testing have also been done. In the comparison between experimental data and simulated data both pressure drop over the humidifier and velocity profile close after the humidifier have been of interest. The first model is a high detailed CAD version of the humidifier. This model had high demand on mesh quality and the result was time demanding and poor. The second model was a less general model. It constist of a porous block with set properties to match the experimental data. This model had a better prospect and showed a better comparison the experimental data.</p>

corrected abstract:
<p>It is well known that the air in an aircraft cabin can be dry and uncomfortable. What is not commonly known is that the human resistants to bacteria and viruses is reduced in this dry air. For long flight the relative humidity inside an aircraft can be reduced to as low as values as 5%, sometimes even lower. This low relative humidity is very extreme and can normaly be found in some deserts. To solve this, the aircraft companies can install humidifiers and raise the relative humidity to more comfortable levels. CTT System has a product which consists of layers of glass fiber sheets which are made wet with a watering system. When the dry air flows through this wet block the water evaporates into the air. This air is then taken to selected parts of the cabin. This project was made to get a deeper understanding of this product. The goal was to investigate two different models to simulate the airflow through this humidifier. For this task SOLIDWORKS Flow Simulation was used. It is a commercial Fluid Dynamic solver. To validate the result from the simulation experimental testing have also been done. In the comparison between experimental data and simulated data both pressure drop over the humidifier and velocity profile close after the humidifier have been of interest. The first model is a high detailed CAD version of the humidifier. This model had high demand on mesh quality and the result was time demanding and poor. The second model was a less general model. It constist of a porous block with set properties to match the experimental data. This model had a better prospect and showed a better comparison the experimental data.</p>

Note spelling errors:
w='constist' val={'c': 'consists', 's': 'diva2:891472', 'n': 'error in original'}
w='normaly' val={'c': 'normally', 's': 'diva2:891472', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1045115 
abstract is: 
<p>When planning for road building and infrastructure, simulations of traffic flow are useful tools for further investigation. Two models are analyzed in this report, both supported by the assumption that traffic flow behaves as a continous fluid on a macroscopic scale. This means that the vehicle density will be considered instead of observing the vehicles as discrete objects.</p><p>Conservational laws along with piecewise constant initial data are called Riemann problems. Traffic flow in one dimension at a traffic light can be modeled as such a problem with the continuity equation and a scaled Heaviside function as initial data.</p><p>Two special cases of the general problem are solved, in the rst model the velocity of the cars is a linear function of the density while in the second model the velocty is a function of the denisty and its derivative. The rst case is solved analytically with a similarity solution. The second case, where diusion is included, is solved numerically and the method utilized is proved to be stable.</p><p>Simulations are presented in order to see how the density of vehicles develop in front of and after the traffic light. The results are used to determine a time for the traffic light to be green, given a fixed time for red, that optimizes the flow of vehicles.</p>



corrected abstract:
<p>When planning for road building and infrastructure, simulations of traffic flow are useful tools for further investigation. Two models are analyzed in this report, both supported by the assumption that traffic flow behaves as a continous fluid on a macroscopic scale. This means that the vehicle density will be considered instead of observing the vehicles as discrete objects.</p><p>Conservational laws along with piecewise constant initial data are called Riemann problems. Traffic flow in one dimension at a traffic light can be modeled as such a problem with the continuity equation and a scaled Heaviside function as initial data.</p><p>Two special cases of the general problem are solved, in the first model the velocity of the cars is a linear function of the density while in the second model the velocity is a function of the denisty and its derivative. The first case is solved analytically with a similarity solution. The second case, where diffusion is included, is solved numerically and the method utilized is proved to be stable.</p><p>Simulations are presented in order to see how the density of vehicles develop in front of and after the traffic light. The results are used to determine a time for the traffic light to be green, given a fixed time for red, that optimzes the flow of vehicles.</p>

Note spelling error:
w='denisty' val={'c': 'density', 's': ['diva2:1033239', 'diva2:1045115'], 'n': 'error in original'}
w='optimzes' val={'c': 'optimizes', 's': ['diva2:1033239', 'diva2:1045115']}
w='velocty' val={'c': 'velocity', 's': ['diva2:1033239', 'diva2:1045115']}
----------------------------------------------------------------------
In diva2:572278 
abstract is: 
<p>In today’s highly competitive trading climate it is getting more important</p><p>to understand the fundamentals of the orderbook and how it works</p><p>in practice in order to stand out in the competition. Better knowledge of</p><p>the orderbook statistics can be applied in areas such as high frequencyand</p><p>execution trading.</p><p>Therefore the focus of this study is to determine the probabilities</p><p>of the settlement prices of financial instruments traded on a public</p><p>order-driven market exchange i.e to predict the probabilities regarding</p><p>whether the next executed trade will happen on the bid or on the</p><p>ask side.</p><p>The mathematical methods used in this study are logistic regression</p><p>and Maximum Likelyhood (ML)-estimation for optimization of parameters.</p><p>First the parameter matrix was optimized using orderbook data</p><p>on the OMXS30 future and then use the estimated model on an out-ofsample</p><p>test.</p><p>Keywords: Logistic Regression, Orderbook, level-2 data</p>


corrected abstract:
<p>In today’s highly competitive trading climate it is getting more important to understand the fundamentals of the orderbook and how it works in practice in order to stand out in the competition. Better knowledge of the orderbook statistics can be applied in areas such as high frequency and execution trading.</p><p>Therefore the focus of this study is to determine the probabilities of the settlement prices of financial instruments traded on a public order-driven market exchange i.e to predict the probabilities regarding whether the next executed trade will happen on the bid or on the ask side.</p><p>The mathematical methods used in this study are logistic regression and Maximum Likelyhood (ML)-estimation for optimization of parameters. First the parameter matrix was optimized using orderbook data on the OMXS30 future and then use the estimated model on an out-ofsample test.</p>

Note - spelling error:
w='Likelyhood' val={'c': 'Likelyhood', 's': 'diva2:572278', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1353070 
abstract is: 
<p>Much of the planned installation of wind turbines in Sweden will be located in the northern region, characterized by a lower population density so that problems related to sound pollution and visual acceptance are of lower concern. This area is generally distinguished by complex topography and the presence of forest, that significantly affects the wind characteristics, complicating their modelling and simulation. There are concerns about how good an industrial code can simulate a forest, a question of paramount importance in the planning of new onshore farms.</p><p>As a first step, a sensitivity analysis was initially carried out to investigate the impact on the ow of different boundary conditions and cell discretization inside the forest for a 2D domain with a homogeneous forest. Subsequently, a comparative analysis between the industrial code WindSim and Large Eddy Simulation (LES) data from Segalini. et al. (2016) was performed with the same domain. Lastly, simulations for a real Swedish forest, Ryningsnäs, was conducted to compare a roughness map approach versus modelling the forest as a momentum sink and a turbulence source. All simulations were conducted for neutral stability conditions with the same domain size and refinement.</p><p>The main conclusions from each part can be summarized as follows. (i) The results from the sensitivity analysis showed that discretization of cells in the vertical direction inside the forest displayed a correlation between an increasing number of cells and a decreased streamwise wind speed above the canopy. (ii) The validation with the LES data displayed good agreement in terms of both horizontal mean wind speed and turbulence intensity. (iii) In terms of horizontal wind speed for Ryningsnäs, forest modelling was prevailing for all wind directions, where the most accurate simulation was found by employing a constant forest force resistive constant (C2) equal to 0.05. All forest models overestimated the turbulence intensity, whereas the roughness map approaches underestimated it. Based solely on the simulations for Ryningsnäas, a correlation between lower streamwise wind speed and higher turbulence intensity can be deduced.</p>

corrected abstract:
<p>Much of the planned installation of wind turbines in Sweden will be located in the northern region, characterized by a lower population density so that problems related to sound pollution and visual acceptance are of lower concern. This area is generally distinguished by complex topography and the presence of forest, that significantly affects the wind characteristics, complicating their modelling and simulation. There are concerns about how good an industrial code can simulate a forest, a question of paramount importance in the planning of new onshore farms.</p><p>As a first step, a sensitivity analysis was initially carried out to investigate the impact on the flow of different boundary conditions and cell discretization inside the forest for a 2D domain with a homogeneous forest. Subsequently, a comparative analysis between the industrial code WindSim and Large Eddy Simulation (LES) data from Segalini. et al. (2016) was performed with the same domain. Lastly, simulations for a real Swedish forest, Ryningsnäs, was conducted to compare a roughness map approach versus modelling the forest as a momentum sink and a turbulence source. All simulations were conducted for neutral stability conditions with the same domain size and refinement.</p><p>The main conclusions from each part can be summarized as follows. (i) The results from the sensitivity analysis showed that discretization of cells in the vertical direction inside the forest displayed a correlation between an increasing amount of cells and a decreased streamwise wind speed above the canopy. (ii) The validation with the LES data displayed good agreement in terms of both horizontal mean wind speed and turbulence intensity. (iii) In terms of horizontal wind speed for Ryningsnäs, forest modelling was prevailing for all wind directions, where the most accurate simulation was found by employing a constant forest force resistive constant (𝐶<sub>2</sub>) equal to 0.05. All forest models overestimated the turbulence intensity, whereas the roughness map approaches underestimated it. Based solely on the simulations for Ryningsnäs, a correlation between lower streamwise wind speed and higher turbulence intensity can be deduced.</p>
----------------------------------------------------------------------
In diva2:787485 
abstract is: 
<p>Traditionally components made from prepreg materials are laid up by hand, this is labor intense and thus expensive. An alternative is to lay up a prepreg stack using an ATL machine and then use drape forming to form this stack. To predict how this stack deforms it is important to know how it behaves during forming. Important parameters are friction between prepreg layers as well as well as the friction between the prepreg and the tool. For this thesis experiments were performed to obtain friction data between the tool and prepreg surfaces. This data showed that this friction is about 20-50% of the friction ompared to the friction between prepreg layers. Based on this data friction models adopted for software implementation were designed; these models were also implemented in the FEM software AniFrom.</p>

corrected abstract:
<p>Traditionally components made from prepreg materials are laid up by hand, this is labor intense and thus expensive. An alternative is to lay up a prepreg stack using an ATL machine and then use drape forming to form this stack. To predict how this stack deforms it is important to know how it behaves during forming. Important parameters are friction between prepreg layers as well as well as the friction between the prepreg and the tool. For this thesis experiments were performed to obtain friction data between the tool and prepreg surfaces. This data showed that this friction is about 20-50% of the friction compared to the friction between prepreg layers. Based on this data friction models adopted for software implementation were designed; these models were also implemented in the FEM software AniForm.</p>
----------------------------------------------------------------------
In diva2:756999 
abstract is: 
<p> </p><p>Saving fuel for heavy trucks travelling on a highway is possible by using the vechicles' weight and the knowledge about the road topography ahead. This can be done by a Look Ahead Cruise Controller. Such a controller can calculate an optimal driving strategy for the road segment ahead. The Look Ahead Cruise Controller is mostly used on an undulating road with a lot of up-and downhill slopes. Then the controller could increase the vehicle speed before an uphill or select the neutral gear and let the vehicle roll in a downhill slope in order to save fuel.</p><p>Particle and nitrogen oxides can have a harmful effect on humans and since these are created in the combustion process of an engine, it is important to limit their presence in the exhaust gas. Therefore the Euro VI legislations are now in effect. They limit the number of particles and amount of nitrogen oxides that can be released from the vehicle. In order to meet these legislation demands the exhaust gas needs to be cleaned. Selective catalytic reduction (SCR) can be used to reduce the emission of nitrogen oxides. In order for the reduction to work as efficient as possible the catalytic substrate needs to have a high temperature.</p><p>When the engine of a vehicle is working on a low engine speed and with a low torque demand, the exhaust gas temperature becomes low. This typically occurs when the vehicle is travelling in a downhill slope, either by rolling on neutral gear or rolling using engine brake. Thus the catalytic substrate is cooled, causing the catalytic process to slow down. A common way to avoid this is to let the engine work in a warming mode. However, this mode uses more fuel in order to heat the exhaust gas.</p><p>The aim of this thesis is to find out if the cooling of the SCR substrate could have an affect on the optimal driving strategy found by the Look Ahead Cruise Controller.</p>


corrected abstract:
<p>Saving fuel for heavy trucks travelling on a highway is possible by using the vechicles' weight and the knowledge about the road topography ahead. This can be done by a Look Ahead Cruise Controller. Such a controller can calculate an optimal driving strategy for the road segment ahead. The Look Ahead Cruise Controller is mostly used on an undulating road with a lot of up-and downhill slopes. Then the controller could increase the vehicle speed before an uphill or select the neutral gear and let the vehicle roll in a downhill slope in order to save fuel.</p><p>Particle and nitrogen oxides can have a harmful effect on humans and since these are created in the combustion process of an engine, it is important to limit their presence in the exhaust gas. Therefore the Euro VI legislations are now in effect. They limit the number of particles and amount of nitrogen oxides that can be released from the vehicle. In order to meet these legislation demands the exhaust gas needs to be cleaned. Selective catalytic reduction (SCR) can be used to reduce the emission of nitrogen oxides. In order for the reduction to work as efficient as possible the catalytic substrate needs to have a high temperature.</p><p>When the engine of a vehicle is working on a low engine speed and with a low torque demand, the exhaust gas temperature becomes low. This typically occurs when the vehicle is travelling in a downhill slope, either by rolling on neutral gear or rolling using engine brake. Thus the catalytic substrate is cooled, causing the catalytic process to slow down. A common way to avoid this is to let the engine work in a warming mode. However, this mode uses more fuel in order to heat the exhaust gas.</p><p>The aim of this thesis is to find out if the cooling of the SCR substrate could have an affect on the optimal driving strategy found by the Look Ahead Cruise Controller.</p>

Note spellinig error:
w='vechicles' val={'c': 'vehicles', 's': 'diva2:756999', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:740383 
abstract is: 
<p> </p><p>In the case of a severe accident in a nuclear power plant the reactor</p><p>may heat up, melt and mix with fuel material to form a substance called</p><p>corium. In today's nuclear power plants the primary strategy to cool the</p><p>corium in the event of a severe accident is to flood the ex-vessel cavity</p><p>with water. The reactions which occur when the liquid metal comes in</p><p>contact with the water, known as fuel coolant interaction (FCI), can be</p><p>violent and in the worst case scenario lead to containment failure. In</p><p>the MISTEE laboratory at KTH, small scale FCI experiments are conducted.</p><p>This thesis explores how dierent temperatures of liquid tin and</p><p>water aects the presence of steam explosion. Higher melt superheat and</p><p>lower water temperature was found to increase the likelihood of steam</p><p>explosions. Furthermore, a phenomenon was observed, hereby referred</p><p>to as immediate steam explosion, where the melt exploded immediately</p><p>upon contact with water. All previous research found states that steam</p><p>explosion only occurs in the later stages of FCI, thus the results are contradictory.</p><p>The thesis also includes research on jet breakup in the initial phase</p><p>of FCI and how it is affected by melt velocity, diameter and temperature</p><p>as well as</p><p>water temperature. The experiments performed did not yield</p><p>data which could be analyzed so no conclusions could be drawn.</p>

corrected abstract:
<p>In the case of a severe accident in a nuclear power plant the reactor may heat up, melt and mix with fuel material to form a substance called corium. In today's nuclear power plants the primary strategy to cool the corium in the event of a severe accident is to flood the ex-vessel cavity with water. The reactions which occur when the liquid metal comes in contact with the water, known as fuel coolant interaction (FCI), can be violent and in the worst case scenario lead to containment failure. In the MISTEE laboratory at KTH, small scale FCI experiments are conducted. This thesis explores how different temperatures of liquid tin and water affects the presence of steam explosion. Higher melt superheat and lower water temperature was found to increase the likelihood of steam explosions. Furthermore, a phenomenon was observed, hereby referred to as immediate steam explosion, where the melt exploded immediately upon contact with water. All previous research found states that steam explosion only occurs in the later stages of FCI, thus the results are contradictory.</p><p>The thesis also includes research on jet breakup in the initial phase of FCI and how it is affected by melt velocity, diameter and temperature as well as water temperature. The experiments performed did not yield data which could be analyzed so no conclusions could be drawn.</p>
----------------------------------------------------------------------
In diva2:570392 
abstract is: 
<p>ii</p><p>Multiple linear regression analysis has been applied in this project to examine the influence of different financial signals on market-adjusted return. The analysis is based on observed sets of data, gathered from annual reports and market prices from the years 2006-2010. The financial signals were analysed with share's future market-adjusted returns as well as historic adjusted return. F_SCORE, Book to Market ratio, year, business and market value were encompassed in the model as covariates, with market-adjusted return as the depended variable.</p><p>To analyse the model, OLS regressions were implemented, and the model was subsequently reduced by BIC tests. To investigate the accuracy of the OLS estimates the Bootstrap method was implemented and the financial input signals to F_SCORE were also examined, with linear regressions. Moreover, the Logit method was utilized to test the predictive capacity of the model.</p><p>Statistically significant results were obtained for F_SCORE at 10% significance level for historic returns, but insignificant for future adjusted returns. Book to Market ratio was found to be significantly correlated with future market adjusted return on 1% significance level.</p><p>The study showed that F_SCORE is positively correlated with historic market adjusted returns, but insignificant in its effect on future returns. This led to the conclusion that the market's efficiency result to absorb new financial information in the pricing of stocks instantaneously, and that this fundamental analysis cannot be applied to find high-yielding stocks on the market.</p>


corrected abstract:
<p>Multiple linear regression analysis has been applied in this project to examine the influence of different financial signals on market-adjusted return. The analysis is based on observed sets of data, gathered from annual reports and market prices from the years 2006-2010. The financial signals were analysed with share's future market-adjusted returns as well as historic adjusted return. F_SCORE, Book to Market ratio, year, business and market value were encompassed in the model as covariates, with market-adjusted return as the depended variable.</p><p>To analyse the model, OLS regressions were implemented, and the model was subsequently reduced by BIC tests. To investigate the accuracy of the OLS estimates the Bootstrap method was implemented and the financial input signals to F_SCORE were also examined, with linear regressions. Moreover, the Logit method was utilized to test the predictive capacity of the model.</p><p>Statistically significant results were obtained for F_SCORE at 10% significance level for historic returns, but insignificant for future adjusted returns. Book to Market ratio was found to be significantly correlated with future market adjusted return on 1% significance level.</p><p>The study showed that F_SCORE is positively correlated with historic market adjusted returns, but insignificant in its effect on future returns. This led to the conclusion that the market's efficiency result to absorb new financial information in the pricing of stocks instantaneously, and that this fundamental analysis cannot be applied to find high-yielding stocks on the market.</p>

w='F_SCORE' val={'c': 'F-score', 's': 'diva2:570392', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1198963 
abstract is: 
<p>This paper describes the ongoing development of Maribot Vane, an autonomous sailing vessel at the Maritime Robotics Laboratory of KTH, the Royal Institute of Technology, Stockholm. There is an ac-celerating need for ocean sensing where autonomous vehicles can play a key role in assisting scientists with environmental monitoring and collecting oceanographic data. The purpose of Maribot Vane is to offer a sus-tainable alternative for these autonomous missions by using wind and an energy efficient self-steering mecha-nism. The rig is composed of a free-rotating wing fitted with a coupled control surface. A completely novel wind vane self-steering solution has been developed and is being evaluated. A key point in the development of the vessel is robustness, with a goal of being able to sail in open seas for long period of times. The paper discusses some key concepts, the development method and presents initial results of the new systems.</p>

corrected abstract:
<p>The thesis work was performed at the Maritime Robotics Laboratory on the Maribot Vane project. The aim of the research project is to develop an autonomous sailing boat used to monitor and collect oceanic data for other research field (oceanography, meteorology, fishery&hellip;). The project aims at developing new techniques to make a robust platform, able to withstand very rough conditions on long trips (several months) without assistance. The propulsion of the boat is made by a free-rotating and self-adjusting rigid wing, that was designed and built in the first semester of 2017 (Tretow, 2017) and first tested during the summer 2017.</p><p>The thesis work consisted on two main parts: first the performance of the actual prototype was assessed through further development of the electronics system and extensive testing. The second part consisted in the development of a prototype of a self-steering system and the evaluation of its potential use on the long term for the project.</p><p>The self-steering system is one of the key feature that the Maribot Vane project is developing. The aim is to have an automatic steering system that enables the boat to sail at a constant apparent wind angle without any control form the onboard electronics, only using wind power. Energy is one of the key parameter to sail on long missions; with the self-steering system, the aim is to not consume electricity most of the time the boat sails, thus limiting the need to charge the batteries and enabling to sail in areas where it is difficult to harvest energy from the environment. As far as our knowledge goes, such a system has never been developed.</p><p>After the first series of test to assess the boat performance, a new internal structure was designed and built for the two main reasons that the previous structure was too weak for the desired level of robustness and because of the need of extra room and attachment possibilities for the steering system.</p>


Note that the abstract in the thesis is quite different from that in DiVA.
----------------------------------------------------------------------
In diva2:757660 
abstract is: 
<p>Let <em>G</em> be a group and <em>P</em> a Sylow <em>p</em>-subgroup of <em>G</em>. A <em>fusion system of G</em> on <em>P</em>, denoted by <em>FP</em> <em>(G)</em>, is the category with objects; subgroups of <em>P</em>, and morphisms induced by conjugation in <em>G</em>. This thesis gives a brief introduction to the theory fusion systems.</p><p>Two classical theorems of Burnside and Frobenius are stated and proved. These theorems may be seen as a starting point of the theory of fusion systems, even though the axiomatic foundation is due to Puig in the early 1990's.</p><p>An abstract fusion system<em> F</em> on a <em>p</em>-group <em>P</em> is dened and the notion of a saturated fusion system is discussed. It turns out that the fusion system of any nite group is saturated, but the converse; that a saturated fusion system is realizable on a nite group, is not always true.</p><p>Two versions of <em>Alperin's fusion theorem</em> are stated and proved. The first one is the classical formulation of Alperin and the second one, due to Puig, a version stated in the language of fusion systems. The differences between these two are investigated.</p><p>The fusion system <em>F</em> of <em>GL<sub>2 </sub></em>(3) on the Sylow 2-subgroup isomorphicto <em>SD<sub>16</sub></em> is determined and the subgroups generating <em>F</em> are found.</p>

corrected abstract:
<p>Let 𝐺 be a group and 𝑃 a Sylow 𝑝-subgroup of 𝐺. A <em>fusion system</em> of 𝐺 on 𝑃, denoted by &Fscr;<sub>𝑃</sub>(𝐺), is the category with objects; subgroups of 𝑃, and morphisms induced by conjugation in 𝐺. This thesis gives a brief introduction to the theory fusion systems.</p><p>Two classical theorems of Burnside and Frobenius are stated and proved. These theorems may be seen as a starting point of the theory of fusion systems, even though the axiomatic foundation is due to Puig in the early 1990's.</p><p>An abstract fusion system &Fscr; on a 𝑝-group 𝑃 is defined and the notion of a <em>saturated</em> fusion system is discussed. It turns out that the fusion system of any finite group is saturated, but the converse; that a saturated fusion system is realizable on a finite group, is not always true.</p><p>Two versions of <em>Alperin's fusion theorem</em> are stated and proved. The first one is the classical formulation of Alperin and the second one, due to Puig, a version stated in the language of fusion systems. The differences between these two are investigated.</p><p>The fusion system &Fscr; of 𝐺𝐿<sub>2</sub></em>(3) on the Sylow 2-subgroup isomorphic to <em>SD<sub>16</sub></em> is determined and the subgroups <em>generating</em> &Fscr; are found.</p>

***** Note to Chip *****
The &Fscr; does not appear in a consistent style!!! (Atleast when added to a Canvas wikipage). It appears in the first paragraph in chancery style, while in the 3rd paragraph it appears in roundhand style. One should be able to use a Unicode variation selector 1 and 2 to force these styles.
----------------------------------------------------------------------
In diva2:545132   - correct as is

Note spelling errors:
w='comparsion' val={'c': 'comparison', 's': 'diva2:545132', 'n': 'error in original'}
w='mouldning' val={'c': 'molding', 's': 'diva2:545132', 'n': 'error in original'}
w='theisis' val={'c': 'theisis', 's': 'diva2:545132', 'n': 'error in original'}
w='sweept' val={'c': 'swept', 's': 'diva2:545132', 'n': 'error in original'}
w='recomended' val={'c': 'recommended', 's': 'diva2:545132', 'n': 'error in original'}

Also two words seem to have been mereged in the original:
mc='overspeed' c='over speed'
mc='rotordiameter' c='rotor diameter'
----------------------------------------------------------------------
In diva2:1296150 
abstract is: 
<p>The Leksell Gamma Knife<sup>® </sup>is an instrument designed for high precision treatment of tumours and lesions located in the brain and upper spine. Today, the radioactive cobalt-60 sources can only move linearly along the radiation unit, but the machine could be modiﬁed to include rotational motion as well. We extend an existing linear programming approach to inverse planning for the Gamma Knife by examining the beneﬁts from rotational degrees of freedom.</p><p>The improvements oﬀered from rotations are limited, but easy to make use of. We investigate the model in four patient cases, and ﬁnd that an upper bound on the improvement of the optimization cost function is between 4.5% and 7.0% depending on case. With a total of four angles distributed uniformly over a 45 degree interval, one can in each case achieve a solution that performs up to within 1% of this bound. The average maximal improvements in terms of clinical metrics are 0.5% selectivity and 1.9% gradient index, at the cost of 5.9% worse beam-on time. No statistically signiﬁcant change in coverage is found.</p><p>A dynamic model based on column generation is proposed, which allows treatment during constant velocity angular motion and can achieve practically the same plan quality as the model with uniformly distributed angles at a signiﬁcantly lower problem size. A similar algorithm can be designed to locate the most eﬀective angles in a non-uniform manner that achieves better plans with fewer added rotational degrees of freedom.</p><p>Trade-oﬀs between memory and solution times are used to successively reduce the RAM occupation by around 90% and make signiﬁcantly larger models computationally feasible to solve. A voxel clustering approach with emphasis on surface voxels, adapted to the radiosurgical framework, can signiﬁcantly reduce the problem size while still producing competitive plans.</p>

corrected abstract:
<p>The Leksell Gamma Knife<sup>®</sup> is an instrument designed for high precision treatment of tumours and lesions located in the brain and upper spine. Today, the radioactive cobalt-60 sources can only move linearly along the radiation unit, but the machine could be modified to include rotational motion as well. We extend an existing linear programming approach to inverse planning for the Gamma Knife by examining the benefits from rotational degrees of freedom.</p><p>The improvements offered from rotations are limited, but easy to make use of. We investigate the model in four patient cases, and find that an upper bound on the improvement of the optimization cost function is between 4.5% and 7.0% depending on case. With a total of four angles distributed uniformly over a 45 degree interval, one can in each case achieve a solution that performs up to within 1% of this bound. The average maximal improvements in terms of clinical metrics are 0.5% selectivity and 1.9% gradient index, at the cost of 5.9% worse beam-on time. No statistically significant change in coverage is found.</p><p>A dynamic model based on column generation is proposed, which allows treatment during constant velocity angular motion and can achieve practically the same plan quality as the model with uniformly distributed angles at a significantly lower problem size. A similar algorithm can be designed to locate the most effective angles in a non-uniform manner that achieves better plans with fewer added rotational degrees of freedom.</p><p>Trade-offs between memory and solution times are used to successively reduce the RAM occupation by around 90% and make significantly larger models computationally feasible to solve. A voxel clustering approach with emphasis on surface voxels, adapted to the radiosurgical framework, can significantly reduce the problem size while still producing competitive plans.</p>
----------------------------------------------------------------------
In diva2:1127931 
abstract is: 
<p>The KTH Maritime Robotics Lab is developing a simulation framework for experimental autonomous underwater vehicles in MATLAB and Simulink. This project has developed a formulation for added mass of the vehicle, to be implemented in this simulation frame-work. The requirements of the solution is that it should require low computational power, be a general formulation applicable on arbitrarily shaped vehicles and be veri-ﬁed against literature. Diﬀerent existing methods and formulations for primitive bodies have been investigated, and combining these methods has resulted in a simpliﬁed but adequate method for calculating the added mass of arbitrarily shaped hulls and control surfaces, that is easy to implement in the existing simulation framework. The method has been veriﬁed by calculating added mass coeﬃcients for two existing vehicles, and comparing the values to the coeﬃcients already calculated for the vehicles in question. Some limitations have been identiﬁed, such as the interaction eﬀects between compo-nents of the vehicle not being taken into account. To determine the extent of the errors due to this simpliﬁcation and to fully validate and verify the model, future work in the form of CFD calculations or experiments on added mass measurements need to be con-ducted. There is also an uncertainty in the calculation of the coupled coeﬃcients m26 and m35, and results on these coeﬃcients should be handled with care.</p>

corrected abstract:
<p>The KTH Maritime Robotics Lab is developing a simulation framework for experimental autonomous underwater vehicles in MATLAB and Simulink. This project has developed a formulation for added mass of the vehicle, to be implemented in this simulation framework. The requirements of the solution is that it should require low computational power, be a general formulation applicable on arbitrarily shaped vehicles and be verified against literature. Different existing methods and formulations for primitive bodies have been investigated, and combining these methods has resulted in a simplified but adequate method for calculating the added mass of arbitrarily shaped hulls and control surfaces, that is easy to implement in the existing simulation framework. The method has been verified by calculating added mass coefficients for two existing vehicles, and comparing the values to the coefficients already calculated for the vehicles in question. Some limitations have been identified, such as the interaction effects between components of the vehicle not being taken into account. To determine the extent of the errors due to this simplification and to fully validate and verify the model, future work in the form of CFD calculations or experiments on added mass measurements need to be conducted. There is also an uncertainty in the calculation of the coupled coefficients m<sub>26</sub> and m<sub>35</sub>, and results on these coefficients should be handled with care.</p>
----------------------------------------------------------------------
In diva2:894206 
abstract is: 
<p>Simulations are run during early design conception phase in order to have a global overview of the performances and abilities of a given product. Building the aerodynamics and control of a flying device such as a missile requires time and resources. This study focuses on the development of a simplified but accurate enough model of a controlled airframe. The purpose of such a model is to get a good estimation of the behaviour of a given missile. The model has a generic structure which allows it to be adapted to any kind of missile. Given the commanded acceleration in missile frame of reference the simplified model gives the position and the attitude of the missile. A back-to-back validation with a pre-existing model of controlled airframe is performed in order to get the most accurate one. Such a study raises several challenges when it comes to modelling the autopilot or the aerodynamics of the missile. The aerodynamics of this controlled airframe is built such asto require as few aerodynamic parameters as possible. The performances of the simplified controlled airframe meet the expectations of its designers but users have to be aware that this simplified model is limited by its structure: a first-order linear and invariant model cannot describe accurately a whole flight. Further studies on the speed and acceleration ofthe platform when ˝ring and on the controlled but non-guided flight phase can reinforce the model accuracy.</p>

corrected abstract:
<p>Simulations are run during early design conception phase in order to have a global overview of the performances and abilities of a given product. Building the aerodynamics and control of a flying device such as a missile requires time and resources. This study focuses on the development of a simplified but accurate enough model of a controlled airframe. The purpose of such a model is to get a good estimation of the behaviour of a given missile. The model has a generic structure which allows it to be adapted to any kind of missile. Given the commanded acceleration in missile frame of reference the simplified model gives the position and the attitude of the missile. A back-to-back validation with a pre-existing model of controlled airframe is performed in order to get the most accurate one. Such a study raises several challenges when it comes to modelling the autopilot or the aerodynamics of the missile. The aerodynamics of this controlled airframe is built such as to require as few aerodynamic parameters as possible. The performances of the simplified controlled airframe meet the expectations of its designers but users have to be aware that this simplified model is limited by its structure: a first-order linear and invariant model cannot describe accurately a whole flight. Further studies on the speed and acceleration of the platform when firing and on the controlled but non-guided flight phase can reinforce the model accuracy.</p>
----------------------------------------------------------------------
In diva2:1120584 
abstract is: 
<p>This bachelor’s thesis gives a thorough introduction to geometric algebra (GA), an overview of conformal geometric algebra (CGA) and an application to the processing of single particle data from cryo-electron microscopy (cryo-EM). The geometric algebra over the vector space Rp;q, i.e. the Clifford algebra over an orthogonal basis of the space, is a strikingly simple algebraic construction built from the geometric product, which generalizes the scalar and cross products between vectors. In</p><p>terms of this product, a host of algebraically and geometrically meaningful operations can be defined. These encode linear subspaces, incidence relations, direct sums, intersections and orthogonal complements, as well as reflections and rotations. It is with good reason that geometric algebra is often referred to as a universal language of geometry. Conformal geometric algebra is the application of geometric algebra in the context of the conformal embedding of R3 into the Minkowski space R4;1. By way of this embedding, linear subspaces of R4;1 represent arbitrary points, lines, planes, point pairs, circles and spheres in R3. Reflections and rotations in R4;1 become conformal transformations in R3: reflections, rotations, translations, dilations and inversions. The analysis of single-particle cryo-electron microscopy data leads to the common curves problem. By a variant of the Fourier slice theorem, this problem involves hemispheres and their intersections. This thesis presents a rewriting, inspired by CGA, into a problem of planes and lines.</p>

corrected abstract:
<p>This bachelor’s thesis gives a thorough introduction to geometric algebra (GA), an overview of conformal geometric algebra (CGA) and an application to the processing of single particle data from cryo-electron microscopy (cryo-EM).</p><p>The geometric algebra over the vector space ℝ<sup><em>p,q</em></sup>, i.e. the Clifford algebra over an orthogonal basis of the space, is a strikingly simple algebraic construction built from the <em>geometric product</em>, which generalizes the scalar and cross products between vectors. In terms of this product, a host of algebraically and geometrically meaningful operations can be defined. These encode linear subspaces, incidence relations, direct sums, intersections and orthogonal complements, as well as reflections and rotations. It is with good reason that geometric algebra is often referred to as a universal language of geometry.</p><p>Conformal geometric algebra is the application of geometric algebra in the context of the <em>conformal embedding</em> of ℝ<sup>3</sup> into the Minkowski space ℝ<sup>4,1</sup>. By way of this embedding, linear subspaces of ℝ<sup>4,1</sup> represent arbitrary points, lines, planes, point pairs, circles and spheres in ℝ<sup>3</sup>. Reflections and rotations in ℝ<sup>4,1</sup> become conformal transformations in ℝ<sup>3</sup>: reflections, rotations, translations, dilations and inversions.</p><p>The analysis of single-particle cryo-electron microscopy data leads to the <em>common curves</em> problem. By a variant of the Fourier slice theorem, this problem involves hemispheres and their intersections. This thesis presents a rewriting, inspired by CGA, into a problem of planes and lines.  Concretely, an image in the Fourier domain is transformed by mapping points according to
<img style="display: block; margin-left: auto; margin-right: auto;" src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?http://www.diva-portal.org/cgi-bin/mimetex.cgi?x\mapsto\frac{x}{1-\sqrt{1-x^2}}" alt="LaTeX: x \mapsto \frac{x}{1 - \sqrt{1- x^2}}" /><br>
in suitable units. The inversive nature of this transformation causes certain issues that render its usage a trade-off rather than an unconditional advantage.</p>


Note the DiVA version was missing the last part with the equation: $x \mapsto \frac{x}{1 - \sqrt{1- x^2}}$
----------------------------------------------------------------------
In diva2:1880314 - the subtitle "Visualizing the Interplay of Algebraic Symmetries and Continuous Geometry" is missing in DiVA

abstract is: 
<p>The focus of this thesis is to provide an introduction to Lie groups—the study of continuous symmetries—and related concepts. Starting with topology and geometry, progressing through the construction of Cayley-Dickson algebras, the thesis ultimately explores a few of the most common Lie groups. Finally, geometric and visual interpretations of SO(3) and SU(2), as well as the relations between them, are investigated.</p>

corrected abstract:
<p>The focus of this thesis is to provide an introduction to Lie groups&mdash;the study of continuous symmetries—and related concepts. Starting with topology and geometry, progressing through the construction of Cayley-Dickson algebras, the thesis ultimately explores a few of the most common Lie groups. Finally, geometric and visual interpretations of <em>SO(3)</em> and <em>SU(2)</em>, as well as the relations between them, are investigated.</p>
----------------------------------------------------------------------
In diva2:1901232 
abstract is: 
<p>Luminescent solar concentrators (LSCs) are grabbing attention nowadays as building-integrated photovoltaics (BIPV) elements to accomplish "nearly zero energy buildings" or even "positive energy buildings" where the huge lateral sections of the building can be used as a supplemental PV source. An innovative design for luminescent solar concentrator is called “triplex laminate structure” where a polymer layer is sandwiched between two glass pieces. During the polymerization of off-stoichiometric thiol-ene (OSTE) polymer, the glass-OSTE interface experiences strain due to the contraction mismatch of the polymer and glass. As a result, it becomes difficult to adhere it on the un-functionalized glass surface and OSTE may eventually debond from the glass surface. This phenomenon is termed as “delamination”, and it reduces device efficiency due to the uneven light propagation. This overall scenario reduces the transparency, promotes reabsorption and reflection losses. To mitigate these issues, glass surface modification is the most important step to be considered.The aim of this project was to functionalize the glass surfaces of the luminescent solar concentrator device to increase the bonding strength of glass-OSTE interfaces in triplex laminate structure. The main motive is to diminish “delamination” and achieve better photovoltaic performance with improved aesthetic quality, where all processing steps should preferably be accomplished at room temperature. (3-Mercaptopropyl)trimethoxysilane was used for surface functionalization where a self-assembled monolayer of MPTMS has been formed onto the glass surface introducing thiol (-SH) functionality. These are very much capable of being the attachment link to polymer layer, while at the same time trimethoxysilane group of MPTMS binds with the glass surface. For the silanization process, several sets of solutions with different silane, ethanol and water ratios were prepared. During the testing time, two approaches (qualitative approach and characterization) were followed; different parameters such as dipping time, blow drying, rinsing the sample with ethanol were varied during the mechanical phase and the adhesive strength between the glass surface and OSTE layer was tested. On the other side, in characterization phase, parameters of silane:ethanol:water were varied and investigated using AFM, contact angle measurement and FTIR to confirm the successful and uniform glass surface functionalization for thiol-ene polymer adhesion with determined optimum conditional parameters. This phase included pre and post washing of the sample before the investigation to confirm the chemisorption of silane layer onto the glass surface after functionalization. A successful protocol for room-temperature surface functionalization has been developed.</p>

corrected abstract:
<p>Luminescent solar concentrators (LSCs) are grabbing attention nowadays as building-integrated photovoltaics (BIPV) elements to accomplish "nearly zero energy buildings" or even "positive energy buildings" where the huge lateral sections of the building can be used as a supplemental PV source. An innovative design for luminescent solar concentrator is called “triplex laminate structure” where a polymer layer is sandwiched between two glass pieces. During the polymerization of off-stoichiometric thiol-ene (OSTE) polymer, the glass-OSTE interface experiences strain due to the contraction mismatch of the polymer and glass. As a result, it becomes difficult to adhere it on the un-functionalized glass surface and OSTE may eventually debond from the glass surface. This phenomenon is termed as “delamination”, and it reduces device efficiency due to the uneven light propagation. This overall scenario reduces the transparency, promotes reabsorption and reflection losses. To mitigate these issues, glass surface modification is the most important step to be considered.</p><p>The aim of this project was to functionalize the glass surfaces of the luminescent solar concentrator device to increase the bonding strength of glass-OSTE interfaces in triplex laminate structure. The main motive is to diminish “delamination” and achieve better photovoltaic performance with improved aesthetic quality, where all processing steps should preferably be accomplished at room temperature. (3-Mercaptopropyl)trimethoxysilane was used for surface functionalization where a self-assembled monolayer of MPTMS has been formed onto the glass surface introducing thiol (-SH) functionality. These are very much capable of being the attachment link to polymer layer, while at the same time trimethoxysilane group of MPTMS binds with the glass surface. For the silanization process, several sets of solutions with different silane, ethanol and water ratios were prepared. During the testing time, two approaches (qualitative approach and characterization) were followed; different parameters such as dipping time, blow drying, rinsing the sample with ethanol were varied during the mechanical phase and the adhesive strength between the glass surface and OSTE layer was tested. On the other side, in characterization phase, parameters of silane:ethanol:water were varied and investigated using AFM, contact angle measurement and FTIR to confirm the successful and uniform glass surface functionalization for thiol-ene polymer adhesion with determined optimum conditional parameters. This phase included pre and post washing of the sample before the investigation to confirm the chemisorption of silane layer onto the glass surface after functionalization. A successful protocol for room-temperature surface functionalization has been developed.</p>

Note - the only change was to add the missing paragrpah break.
----------------------------------------------------------------------
In diva2:869025 
abstract is: 
<p>This thesis considers the estimation of undirected Gaussian graphical models especially in the high dimensional setting where the true observations are assumed to be non-Gaussian distributed.</p><p>The ﬁrst aim is to present and compare the performances of existing Gaussian graphical model estimation methods. Furthermore since the models rely heavily on the normality assumption, various methods for relaxing the normal assumption are presented. In addition to the existing methods, a modiﬁed version of the joint graphical lasso method is introduced which monetizes on the strengths of the community Bayes method. The community Bayes method is used to partition the features (or variables) of datasets consisting of several classes into several communities which are estimated to be mutually independent within each class which allows the calculations when performing the joint graphical lasso method, to be split into several smaller parts. The method is also inspired by the cluster graphical lasso and is applicable to both Gaussian and non-Gaussian data, assuming that the normal assumption is relaxed.</p><p>Results show that the introduced cluster joint graphical lasso method outperforms com-peting methods, producing graphical models which are easier to comprehend due to the added information obtained from the clustering step of the method. The cluster joint graphical lasso is applied to a real dataset consisting of p = 12582 features which resulted in computation gain of a factor 35 when comparing to the competing method which is very signiﬁcant when analysing large datasets. The method also allows for parallelization where computations can be spread across several computers greatly increasing the computational eﬃciency.</p>

corrected abstract:
<p>This thesis considers the estimation of undirected Gaussian graphical models especially in the high dimensional setting where the true observations are assumed to be non-Gaussian distributed.</p><p>The first aim is to present and compare the performances of existing Gaussian graphical model estimation methods. Furthermore since the models rely heavily on the normality assumption, various methods for relaxing the normal assumption are presented. In addition to the existing methods, a modified version of the joint graphical lasso method is introduced which monetizes on the strengths of the community Bayes method. The community Bayes method is used to partition the features (or variables) of datasets consisting of several classes into several communities which are estimated to be mutually independent within each class which allows the calculations when performing the joint graphical lasso method, to be split into several smaller parts. The method is also inspired by the cluster graphical lasso and is applicable to both Gaussian and non-Gaussian data, assuming that the normal assumption is relaxed.</p><p>Results show that the introduced cluster joint graphical lasso method outperforms competing methods, producing graphical models which are easier to comprehend due to the added information obtained from the clustering step of the method. The cluster joint graphical lasso is applied to a real dataset consisting of 𝑝 = 12582 features which resulted in computation gain of a factor 35 when comparing to the competing method which is very significant when analysing large datasets. The method also allows for parallelization where computations can be spread across several computers greatly increasing the computational efficiency.</p>


Note - it the Swedish abstract there is a ligature:
w='beräkningseﬀektiviteten' val={'c': 'beräkningseffektiviteten', 's': 'diva2:869025', 'n': 'ligature'}
----------------------------------------------------------------------
In diva2:1719082 
abstract is: 
<p>In this thesis, gravity and surface tension-driven water waves are investigated by designing an experimental setup to track wave patterns using a 300 fps high speed camera. This is done to reproduce the theoretical diagram of the dispersion relation for surface waves in different teaching contexts. Surface waves are dispersive, i.e. the phase speed depends on the wavelength.Initially, the background theory for surface waves is presented and the differences between gravity and surface tension-driven waves are described. The conditions for deep and shallow water are also studied. Thereafter, a literature study is conducted to study similar experiments. Test experiments are then carried out where both direct and indirect methods of observing the waves are examined to determine which method generates the best images.The water waves in the experiment are generated by dropping a 1.6 cm marble and a 4 mm water droplet into a 35 cm diameter hexagonal tank filled with 1–10 cm deep water. The waves are recorded from above and illuminated by backlighting with a 10 W LED panel. The experimental results show that a 1.6 cm marble generates wavelengths in the 0.4–3.5 cm range. Moreover, for a 4 mm water droplet, wavelengths in the range of 0.4–2.5 cm are generated.</p>
mc='images.The' c='images. The'
mc='wavelength.Initially' c='wavelength. Initially'

corrected abstract:
<p>In this thesis, gravity and surface tension-driven water waves are investigated by designing an experimental setup to track wave patterns using a 300 fps high speed camera. This is done to reproduce the theoretical diagram of the dispersion relation for surface waves in different teaching contexts. Surface waves are dispersive, i.e. the phase speed depends on the wavelength.</p><p>Initially, the background theory for surface waves is presented and the differences between gravity and surface tension-driven waves are described. The conditions for deep and shallow water are also studied. Thereafter, a literature study is conducted to study similar experiments. Test experiments are then carried out where both direct and indirect methods of observing the waves are examined to determine which method generates the best images.</p><p>The water waves in the experiment are generated by dropping a 1.6 cm marble and a 4 mm water droplet into a 35 cm diameter hexagonal tank filled with 1–10 cm deep water. The waves are recorded from above and illuminated by backlighting with a 10 W LED panel. The experimental results show that a 1.6 cm marble generates wavelengths in the 0.4–3.5 cm range. Moreover, for a 4 mm water droplet, wavelengths in the range of 0.4–2.5 cm are generated.</p>
----------------------------------------------------------------------
In diva2:575399 
There are no fonts ini the PDF file - it seems to have been scanned.

abstract is: 
<p>Nano materials can today be found in a wide range of consumer products and the number of new products on the market is expected to inrease. In the shadow of hope for nano materilas potential in various applications, low awareness of its health and environmental risks is hiding. Furthermore, the knowledge about people´s risk perception of nano materials is limited. Parallel to the situation described above, the scholls meet the challenge of incrasing the scientific literacy. To achieve this goal, skills in argumentation in science studies has been emphasized and the concept of socio-scientific issues has been emerged. Using focus groups as a method, this study aimed first, to explore young engineering student´s risk perception of nano materials, and secont to analyze the extent to which they apply scientific konwledge to argue about nano materials. The material from the focus group interviews was analyzed with respect to both content and interaction. To analyze the content, a thematic classification of the material was made. The interactive and communicative forms were highlighted by an analysis of arguments according to the SEE-SEP-model. Seven themes were indentified from the focus group material. It was also assumed that 55 percent of the participants´arguments were based on values, 25 percent on konwledge and 20 percent on personal experiences. Despite the absence of specific knowledge, the young engineering students have the ability to conduct a complex argumentation about nano materials where they involve the paradox; new opportunities, inresolved risks. Their risk perception is not primarily based on knowledge but on emotional expressions such as fascination, hope, resignation and fear.</p>

corrected abstract:
<p>Nano materials can today be found in a wide range of consumer products and the number of new products on the market is expected to inrease. In the shadow of hope for nano materilas potential in various applications, low awareness of its health and environmental risks is hiding. Furthermore, the knowledge about people´s risk perception of nano materials is limited. Parallel to the situation described above, the schools meet the challenge of increasing the scientific literacy. To achieve this goal, skills in argumentation in science studies has been emphasized and the concept of socio-scientific issues has been emerged. Using focus groups as a method, this study aimed first, to explore young engineering student´s risk perception of nano materials, and second to analyze the extent to which they apply scientific konwledge to argue about nano materials. The material from the focus group interviews was analyzed with respect to both content and interaction. To analyze the content, a thematic classification of the material was made. The interactive and communicative forms were highlighted by an analysis of arguments according to the SEE-SEP-model. Seven themes were identified from the focus group material. It was also assumed that 55 percent of the participants´ arguments were based on values, 25 percent on konwledge and 20 percent on personal experiences. Despite the absence of specific knowledge, the young engineering students have the ability to conduct a complex argumentation about nano materials where they involve the paradox; new opportunities, unresolved risks. Their risk perception is not primarily based on knowledge but on emotional expressions such as fascination, hope, resignation and fear.</p>
----------------------------------------------------------------------
In diva2:1436939 
abstract is: 
<p>It is still to this day a challenge for theoretical physicists to derive Fourier’s law from microscopic models. Motivated by this, we study in this thesis the thermal conduction properties of harmonic chains. A semi-analytical method and simulation are used to find that on average the conduction through harmonic chains resembles Fourier like conduction when impurities of the form k_i=kw_i and 1/m_i=1/m*w_i are introduced, where k_i and m_i are the spring constants and masses of the chain and w_i are weights drawn from a random distribution. A few of these distributions are studied in detail, with similar results.Also the classical field theory limit of this model is studied. It is shown by analytical means that heat is transported diffusively in this model when impurities are introduced, whereas the transport is completely ballistic in the absence of impurities.</p>

corrected abstract:
<p>It is still to this day a challenge for theoretical physicists to derive Fourier’s law from microscopic models. Motivated by this, we study in this thesis the thermal conduction properties of harmonic chains. A semi-analytical method and simulation are used to find that on average the conduction through harmonic chains resembles Fourier like conduction when impurities of the form 𝑘<sub>i</sub>=𝑘𝑤<sub>i</sub> and 1/𝑚<sub>i</sub>=(1/𝑚)𝑤<sub>i</sub> are introduced, where 𝑘<sub>i</sub> and 𝑚<sub>i</sub> are the spring constants and masses of the chain and 𝑤<sub>i</sub> are weights drawn from a random distribution. A few of these distributions are studied in detail, with similar results. Also the classical field theory limit of this model is studied. It is shown by analytical means that heat is transported diffusively in this model when impurities are introduced, whereas the transport is completely ballistic in the absence of impurities.</p>
----------------------------------------------------------------------
In diva2:1183292 
abstract is: 
<p>In this thesis project, brake performance of heavy vehicles is improved by the development of new wheel-based functions for a longitudinal slip control braking system using novel Fast Acting Braking Valves (FABVs). To achieve this goal, Volvo Trucks' vehicle dynamics model has been extended to incorporate the FABV system. After validating the updated model with experimental data, a slip-slope based recursive least squares friction estimation algorithm has been implemented. Using information about the tire-road friction coefifcient, the sliding mode slip controller has been made adaptive to different road surfaces by implementing a friction dependent reference slip signal and switching gain for the sliding mode controller. This switching gain is further optimized by means of a novel on-line optimization algorithm. Simulations show that the on-line friction estimation converges close to the reference friction level within one second for hard braking. Furthermore, using this information for the optimized controller has resulted in reduction of braking distance on most road surfaces of up to 20 percent, as well as in most cases a reduction in air usage.</p>
w='coefifcient' val={'c': 'coefficient', 's': 'diva2:1183292', 'n': 'correct in original'}

corrected abstract:
<p>In this thesis project, brake performance of heavy vehicles is improved by the development of new wheel-based functions for a longitudinal slip control braking system using novel Fast Acting Braking Valves (FABVs). To achieve this goal, Volvo Trucks' vehicle dynamics model has been extended to incorporate the FABV system. After validating the updated model with experimental data, a slip-slope based recursive least squares friction estimation algorithm has been implemented. Using information about the tire-road friction coefficient, the sliding mode slip controller has been made adaptive to different road surfaces by implementing a friction-dependent reference slip signal and switching gain for the sliding mode controller. This switching gain is further optimized by means of a novel on-line optimization algorithm. Simulations show that the on-line friction estimation converges close to the reference friction level within one second for hard braking. Furthermore, using this information for the optimized controller has resulted in reduction of braking distance on most road surfaces of up to 20 percent, as well as in most cases a reduction in air usage.</p>
----------------------------------------------------------------------
In diva2:630452 
abstract is: 
<p>Nasdaq OMX Valueguard-KTH Housing Index (HOX) is a hedonic price index that illustrates the price development of condominiums in Sweden, and that is obtained by using regression technique. Concerns have been raised regarding the influence of the monthly fee on the index. Low fee condominiums could be more popular because of the low monthly cost, high fee condominiums tend to sell for a lower price due to the high monthly cost. As the price of a condominium rises the importance of the monthly fee decreases. Because of this the monthly fee might affect the regression that produces the index. Furthermore,housing cooperatives are usually indebted. These loans are paid off by the monthly fee which can be considered to finance a debt that few are aware of.</p><p>This issue has been investigated by iteratively estimating the importance of the level of debt in order to find a model that better takes into account the possible impact of the monthly fee on the price development.</p><p>Due to a somewhat simplified model that produces index values with many cases of high standard deviation, no conclusive evidence has been found that confirms the initial hypothesis. Nevertheless, converting part of the monthly fee into debt has shown a general improvement of fitting a regression equation to the data. It is therefore recommended that real data on debt in housing cooperatives be tested in Valuegua</p>

corrected abstract:
<p>Nasdaq OMX Valueguard-KTH Housing Index (HOX) is a hedonic price index that illustrates the price development of condominiums in Sweden, and that is obtained by using regression technique. Concerns have been raised regarding the influence of the monthly fee on the index. Low fee condominiums could be more popular because of the low monthly cost, high fee condominiums tend to sell for a lower price due to the high monthly cost. As the price of a condominium rises the importance of the monthly fee decreases. Because of this the monthly fee might affect the regression that produces the index. Furthermore, housing cooperatives are usually indebted. These loans are paid off by the monthly fee which can be considered to finance a debt that few are aware of.</p><p>This issue has been investigated by iteratively estimating the importance of the level of debt in order to find a model that better takes into account the possible impact of the monthly fee on the price development.</p><p>Due to a somewhat simplified model that produces index values with many cases of high standard deviation, no conclusive evidence has been found that confirms the initial hypothesis. Nevertheless, converting part of the monthly fee into debt has shown a general improvement of fitting a regression equation to the data. It is therefore recommended that real data on debt in housing cooperatives be tested in Valueguards real model in order to see if any improvement can be found.</p>
----------------------------------------------------------------------
In diva2:753825 
abstract is: 
<p>The aim for this bachelor thesis is to determine the altitude of the auroral acceleration region occuring on the dayside. Substantial work has already been done on this topic, but for occurrence at the nightside. In this paper only negative quasi-static potential structures were considered, as they are the main contributor for producing aurora. The data for this study was obtained by the Cluster satellite constellation, and was processed by scripting in MATLAB in order to find the events for this paper. Namely, 17 passages of the auroral oval, especially occurring within two magnetic local time sectors, symmetrically around noon (12 MLT). The results show that the acceleration region occurs from below 2 RE up to an altitude of 4.5 RE with an average of 3.40 ± 0.84 RE, considerably higher than for the nightside. More specifically, fore and afternoon sectors have altitude averages of 2.44 ± 0.49 RE and 4.00 ± 0.26 RE, respectively. A significant difference between the two sectors. By regarding the pseudo altitude and classifying identified events as either -part of a larger scale coherent structure or - a small scaleauroral arc, a general occurrence pattern and height-dependence of the AAR was discovered. The large-scale arcs occur on average at a higher altitude (3.89 ± 0.22 RE) than the small-scale arcs (2.82 ± 0.80 RE).</p>

corrected abstract:
<p>The aim for this bachelor thesis is to determine the altitude of the auroral acceleration region occuring on the dayside. Substantial work has already been done on this topic, but for occurrence at the nightside. In this paper only negative quasi-static potential structures were considered, as they are the main contributor for producing aurora. The data for this study was obtained by the Cluster satellite constellation, and was processed by scripting in MATLAB in order to find the events for this paper. Namely, 17 passages of the auroral oval, especially occurring within two magnetic local time sectors, symmetrically around noon (12 MLT). The results show that the acceleration region occurs from below 2 R<sub>E</sub> up to an altitude of 4.5 R<sub>E</sub> with an average of 3.40 ± 0.84 R<sub>E</sub>, considerably higher than for the nightside. More specifically, fore- and afternoon sectors have altitude averages of 2.44 ± 0.49 R<sub>E</sub> and 4.00 ± 0.26 R<sub>E</sub>, respectively. A significant difference between the two sectors. By regarding the pseudo altitude and classifying identified events as either -part of a larger scale coherent structure or -a small scale auroral arc, a general occurrence pattern and height-dependence of the AAR was discovered. The large-scale arcs occur on average at an higher altitude (3.89 ± 0.22 R<sub>E</sub>) than the small-scale arcs (2.82 ± 0.80 R<sub>E</sub>).</p>


Note - spelling error:
w='occuring' val={'c': 'occurring', 's': ['diva2:787497', 'diva2:753825'], 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1342420 
abstract is: 
<p>In this report we study a class of numerical algorithms under the name of heterogeneous multiscale methods. We present an introduction to the subject and analyze the stability and accuracy of a few implementations of dissipative and oscillatory problems. Furthermore, we demonstrate its superiority in comparison to tradtional numerical methods for these types of problems. Finally, we also present an implementation on a larger problem, namely the well known three-body problem.</p>

corrected abstract:
<p>Heterogenous multiscale methods (HMM) is a framework for numerical methods with the purpose of approximating multiscale problems. In this report it is studied how they can be utilized to solve ordinary differential equations with different time scales. We provide an introduction to the theoretical background and analyze examples of both oscillatory and dissipative systems. Primarily, the focus is on the stability, accuracy and limitations in comparison with traditional methods.</p>


Note that the abstract in DiVA is quite different from what is actually in the thesis.
----------------------------------------------------------------------
In diva2:1867281 
abstract is: 
<p>Melanoidin, a high molecular-weight heterogeneous pollutant in the effluents of molasses-using industries, poses a significant threat to the environment due to its non-biodegradability, unpleasant odour, and harmful effect. Various advanced oxidation processes (AOPs) that produce reactive oxygen species (ROS) are used for the degradation of melanoidins. Heterogeneous photo-Fenton process has emerged as a promising alternative approach in wastewater treatment owing to its enhanced catalytic activity, lower hydrogen peroxide (H2O2) consumption, and reusability of the catalyst. In this work, we investigated the photo-Fenton oxidation of melanoidins using heterogeneous catalyst ZnO@SnOx@nZVI supported on a flexible substrate. The effects of reaction pH, amount of catalyst and H2O2 on color and chemical oxygen demand (COD) removal, as well as the removal kinetics, were studied. It is found that the developed heterogeneous catalyst can lead to 97.3% decolorization of melanoidin (7000 COD) after 4 hours irradiation of simulated sunlight at pH 6 with a much lower dosage of H2O2 than the stoichiometric ratio with respect to COD. The generation of ·OH free radicals and residual H2O2 were monitored to understand the mechanism of melanoidin degradation at near-neutral pH with insufficient H2O2. The findings of this work have significant implications in treatment of non-biodegradable organic pollutants, owing to the advantages in avoiding low pH treatment condition, reducing the cost of H2O2, and enhanced chemical stability of photo-Fenton catalyst.</p>

corrected abstract:
<p>Melanoidin, a high molecular-weight heterogeneous pollutant in the effluents of molasses-using industries, poses a significant threat to the environment due to its non-biodegradability, unpleasant odour, and harmful effect. Various advanced oxidation processes (AOPs) that produce reactive oxygen species (ROS) are used for the degradation of melanoidins. Heterogeneous photo-Fenton process has emerged as a promising alternative approach in wastewater treatment owing to its enhanced catalytic activity, lower hydrogen peroxide (H<sub>2</sub>O<sub>2</sub>) consumption, and reusability of the catalyst. In this work, we investigated the photo-Fenton oxidation of melanoidins using heterogeneous catalyst ZnO@SnO<sub>x</sub>@nZVI supported on a flexible substrate. The effects of reaction pH, amount of catalyst and H<sub>2</sub>O<sub>2</sub> on color and chemical oxygen demand (COD) removal, as well as the removal kinetics, were studied. It is found that the developed heterogeneous catalyst can lead to 97.3% decolorization of melanoidin (7000 COD) after 4 hours irradiation of simulated sunlight at pH 6 with a much lower dosage of H<sub>2</sub>O<sub>2</sub> than the stoichiometric ratio with respect to COD. The generation of ·OH free radicals and residual H<sub>2</sub>O<sub>2</sub> were monitored to understand the mechanism of melanoidin degradation at near-neutral pH with insufficient H<sub>2</sub>O<sub>2</sub>. The findings of this work have significant implications in treatment of non-biodegradable organic pollutants, owing to the advantages in avoiding low pH treatment condition, reducing the cost of H<sub>2</sub>O<sub>2</sub>, and enhanced chemical stability of photo-Fenton catalyst.</p>
----------------------------------------------------------------------
In diva2:1816899 
abstract is: 
<p>Detecting a malicious hacker intruding on a network system can be difficult. This challenge is made even more complex by the network activity generated by normal users and by the fact that it is impossible to know the hacker’s exact actions. Instead, the defender of the network system has to infer the hacker’s actions by statistics collected by the intrusion detection system, IDS. This thesis investigates the performance of hidden Markov models, HMM, to detect an intrusion automatically under different background activities generated by normal users. Furthermore, background subtraction techniques with inspiration from computer vision are investigated to see if normal users’ activity can be filtered out to improve the performance of the HMMs.The results suggest that the performance of HMMs are not sensitive to the type of background activity but rather to the number of normal users present. Furthermore, background subtraction enhances the performance of HMMs slightly. However, further investigations into how background subtraction performs when there are many normal users must be done before any definitive conclusions.</p>

corrected abstract:
<p>Detecting a malicious hacker intruding on a network system can be difficult. This challenge is made even more complex by the network activity generated by normal users and by the fact that it is impossible to know the hacker’s exact actions. Instead, the defender of the network system has to infer the hacker’s actions by statistics collected by the intrusion detection system, IDS. This thesis investigates the performance of hidden Markov models, HMM, to detect an intrusion automatically under different background activities generated by normal users. Furthermore, background subtraction techniques with inspiration from computer vision are investigated to see if normal users’ activity can be filtered out to improve the performance of the HMMs. The results suggest that the performance of HMMs are not sensitive to the type of background activity but rather to the number of normal users present. Furthermore, background subtraction enhances the performance of HMMs slightly. However, further investigations into how background subtraction performs when there are many normal users must be done before any definitive conclusions.</p>
----------------------------------------------------------------------
In diva2:1106221 
abstract is: 
<p>This thesis aims to discern what factors and assumptions are the most important in market risk modeling through examining a broad range of models, for different risk measures (VaR<sub>0.01</sub>, S<sub>0:01</sub> and ES<sub>0:025</sub>) and using hierarchical clustering to identify similarities and dissimilarities between the models. The data used is daily log returns for OMXS30 stock index and Bloomberg Barclays US aggregate bond index (AGG) from which daily risk estimates are simulated.</p><p>In total, 33 market risk models are included in the study. These models consist of unconditional variance models (Student's t distribution, Normal distribution, Historical simulation and Extreme Value Theory (EVT) with Generalized Pareto Tails (GPD)) and conditional variance models (ARCH, GARCH, GJR-GARCH and EGARCH). The conditional models are used in filtered and unfiltered market risk models.</p><p>The hierarchical clustering is done for all risk measures and for both time series, and a comparison is made between VaR<sub>0:01</sub> and ES<sub>0:025</sub>. </p><p>The thesis shows that the most important assumption is whether the models have conditional or unconditional variance. The hierarchy for assumptions then differ depending on time series and risk measure. For OMXS30, the clusters for VaR<sub>0:01</sub> and ES<sub>0:025</sub> are the same and the largest dividing factors for the conditional models are (in descending order):</p><ul><li>Leverage component (EGARCH or GJR-GARCH models) or no leverage component (GARCH or ARCH)</li><li>Filtered or unfiltered models</li><li>Type of variance model (EGARCH/GJR-GARCH and GARCH/ARCH)</li></ul><p>The ES<sub>0:01</sub> cluster shows that ES<sub>0:01</sub> puts a higher emphasis on normality or non-normality assumptions in the models.</p><p>The similarities in the different clusters are more prominent for OMXS30 than for AGG. The hierarchical clustering for AGG is also more sensitive to the choice of risk measure. For AGG the variance models are generally less important and more focus lies in the assumed distributions in the variance models (normal innovations or student's t innovations) and the assumed final log return distribution (Normal, Student's t, HS or EVT-tails).</p><p>In the lowest level clusters, the transition from VaR<sub>0:01</sub> to ES<sub>0:025</sub> result in a smaller model disagreement.</p>

corrected abstract:
<p>This thesis aims to discern what factors and assumptions are the most important in market risk modeling through examining a broad range of models, for different risk measures (VaR<sub>0.01</sub>, ES<sub>0.01</sub> and ES<sub>0.025</sub>) and using hierarchical clustering to identify similarities and dissimilarities between the models. The data used is daily log returns for OMXS30 stock index and Bloomberg Barclays US aggregate bond index (AGG) from which daily risk estimates are simulated.</p><p>In total, 33 market risk models are included in the study. These models consist of unconditional variance models (Student's t distribution, Normal distribution, Historical simulation and Extreme Value Theory (EVT) with Generalized Pareto Tails (GPD)) and conditional variance models (ARCH, GARCH, GJR-GARCH and EGARCH). The conditional models are used in filtered and unfiltered market risk models.</p><p>The hierarchical clustering is done for all risk measures and for both time series, and a comparison is made between VaR<sub>0.01</sub> and ES<sub>0.025</sub>.</p><p>The thesis shows that the most important assumption is whether the models have conditional or unconditional variance. The hierarchy for assumptions then differ depending on time series and risk measure. For OMXS30, the clusters for VaR<sub>0.01</sub> and ES<sub>0.025</sub> are the same and the largest dividing factors for the conditional models are (in descending order):<ul><li>Leverage component (EGARCH or GJR-GARCH models) or no leverage component (GARCH or ARCH)</li><li>Filtered or unfiltered models</li><li>Type of variance model (EGARCH/GJR-GARCH and GARCH/ARCH)</li></ul>The ES<sub>0.01</sub> cluster shows that ES<sub>0.01</sub> puts a higher emphasis on normality or non-normality assumptions in the models.</p><p>The similarities in the different clusters are more prominent for OMXS30 than for AGG.</p><p>The hierarchical clustering for AGG is also more sensitive to the choice of risk measure. For AGG the variance models are generally less important and more focus lies in the assumed distributions in the variance models (normal innovations or student's t innovations) and the assumed final log return distribution (Normal, Student's t, HS or EVT-tails).</p><p>In the lowest level clusters, the transition from VaR<sub>0.01</sub> to ES<sub>0.025</sub> result in a smaller model disagreement.</p>
----------------------------------------------------------------------
In diva2:1237805 
abstract is: 
<p>For the same maximum local stress, a structure with a stress concentration has a higher endurance limit than a structure submitted to an homogeneous stress ﬁeld. This is what is called the scale eﬀect. In this work, one aspect of the scale eﬀect is studied; the volume eﬀect. When there is stress concentration in a component, the highly loaded volume is very small. The probability to ﬁnd a major defect in the small volume is then quite low and the endurance limit is increased compared to an homogeneous stress ﬁeld. Safran Aircraft Engines wants to integrate the eﬀect in the high cycle fatigue design of the turbine blades to prevent the design to be overly conservative. The considered material is a single crystal nickel-based alloy used to make high pressure turbine blades. Only the blade root is considered so that the temperature range of interest is around 650 °C. At this temperature, the behaviour of the material is modeled by an elastoplastic behaviour. First, a volume model using Weibull density and the weakest link hypothesis integrating plasticity is built for 650 °C and several load ratios. Then, the model is also identiﬁed at 20 °C and applied to a tested blade in HCF at room temperature in order to compare the results from fatigue computation with experimental results, for validation. Finally, the limits of the model are discussed and perspectives of improvement are listed.</p>

corrected abstract:
<p>For the same maximum local stress, a structure with a stress concentration has a higher endurance limit than a structure submitted to an homogeneous stress field. This is what is called the scale effect. In this work, one aspect of the scale effect is studied; the volume effect. When there is stress concentration in a component, the highly loaded volume is very small. The probability to find a major defect in the small volume is then quite low and the endurance limit is increased compared to an homogeneous stress field. Safran Aircraft Engines wants to integrate the effect in the high cycle fatigue design of the turbine blades to prevent the design to be overly conservative. The considered material is a single crystal nickel-based alloy used to make high pressure turbine blades. Only the blade root is considered so that the temperature range of interest is around 650 °C. At this temperature, the behaviour of the material is modeled by an elastoplastic behaviour. First, a volume model using Weibull density and the weakest link hypothesis integrating plasticity is built for 650 °C and several load ratios. Then, the model is also identified at 20 °C and applied to a tested blade in HCF at room temperature in order to compare the results from fatigue computation with experimental results, for validation. Finally, the limits of the model are discussed and perspectives of improvement are listed.</p>
----------------------------------------------------------------------
In diva2:559575 
abstract is: 
<p>examples - we need models for selecting a small subset of useful features from high-dimensional data, where the useful features are both rare and weak, this being crucial for e.g. supervised classfication of sparse high- dimensional data. A preceding step is to detect the presence of useful features, signal detection. This problem is related to testing a very large number of hypotheses, where the proportion of false null hypotheses is assumed to be very small. However, reliable signal detection will only be possible in certain areas of the two-dimensional sparsity-strength parameter space, the phase space.</p><p>In this report, we focus on two families of distributions, N and χ<sup>2</sup>. In the former case, features are supposed to be independent and normally distributed. In the latter, in search for a more sophisticated model, we suppose that features depend in blocks, whose empirical separation strength asymptotically follows the non-central χ<sup>2</sup><sub>ν</sub>-distribution.</p><p>Our search for informative features explores Tukey's higher criticism (HC), which is a second-level significance testing procedure, for comparing the fraction of observed signi cances to the expected fraction under the global null.</p><p>Throughout the phase space we investgate the estimated error rate,</p><p><strong>Err</strong> = (#Falsely rejected H0+ #Falsely rejected H1)/#Simulations,</p><p>where H0: absence of informative signals, and H1: presence of informative signals, in both the N-case and the χ<sup>2</sup><sub>ν</sub>-case, for ν= 2; 10; 30. In particular, we find, using a feature vector of the approximately same size as in genomic applications, that the analytically derived detection boundary is too optimistic in the sense that close to it, signal detection is still failing, and we need to move far from the boundary into the success region to ensure reliable detection. We demonstrate that <strong>Err</strong> grows fast and irregularly as we approach the detection boundary from the success region.</p><p>In the χ<sup>2</sup><sub>ν</sub>-case, ν &gt; 2, no analytical detection boundary has been derived, but we show that the empirical success region there is smaller than in the N-case, especially as ν increases.</p>

corrected abstract:
<p>In several application fields today − genomics and proteomics are examples − we need models for selecting a small subset of useful features from high-dimensional data, where the useful features are both rare and weak, this being crucial for e.g. supervised classification of sparse high-dimensional data. A preceding step is to detect the presence of useful features, <em>signal detection</em>. This problem is related to testing a very large number of hypotheses, where the proportion of false null hypotheses is assumed to be very small. However, reliable signal detection will only be possible in certain areas of the two-dimensional sparsity-strength parameter space, the <em>phase space</em>.</p><p>In this report, we focus on two families of distributions, 𝒩 and χ<sup>2</sup>. In the former case, features are supposed to be independent and normally distributed. In the latter, in search for a more sophisticated model, we suppose that features depend in blocks, whose empirical separation strength asymptotically follows the non-central χ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>2</sup><sub>&vscr;</sub></span></span>-distribution.</p><p>Our search for informative features explores Tukey's <em>higher criticism (HC)</em>, which is a <em>second-level significance testing procedure</em>, for comparing the fraction of observed significances to the expected fraction under the global null.</p><p>Throughout the phase space we investgate the estimated error rate, <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cwidehat%7BErr%7D" data-classname="equation" data-title="" /> = (#Falsely rejected H<sub>0</sub>+ #Falsely rejected H<sub>1</sub>/#Simulations, where H<sub>0</sub>: absence of informative signals, and H<sub>1</sub>: presence of informative signals, in both the 𝒩-case and the χ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>2</sup><sub>&vscr;</sub></span></span>-cases, for &vscr; = 2, 10, 30.</p><p>In particular, we find, using a feature vector of the approximately same size as in genomic applications, that the analytically derived <em>detection boundary</em> is too optimistic in the sense that close to it, signal detection is still failing, and we need to move far from the boundary into the <em>success region</em> to ensure reliable detection. We demonstrate that <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cwidehat%7BErr%7D" data-classname="equation" data-title="" /> grows fast and irregularly as we approach the detection boundary from the success region.</p><p>In the χ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>2</sup><sub>&vscr;</sub></span></span>-case, &vscr; &gt; 2, no analytical detection boundary has been derived, but we show that the empirical success region there is smaller than in the 𝒩-case, especially as &vscr; increases.</p>
----------------------------------------------------------------------
In diva2:1691401 
abstract is: 
<p>This report discusses higher differential forms with applications in the study of topological phenomena. The integer quantum Hall effect is first discussed, demonstrating a connection between models on a lattice and quantum field theories bridged by a topological invariant, namely the Chern number. Next, for parametrized models on a lattice, the higher Berry curvature is described. This is a rank-(d + 2) differential form on a (d + 2)-dimensional parameter manifold which provides a relation between models in a bulk and on a lower-dimensional interface. Finally, a family of quantum field theories connected to a (d + 1)-dimensional manifold, termed a target space, is constructed. This connection is realized through the incorporation of a set of classical fields, and the effective action of the full field theories all contain a Wess-Zumino-Witten term given by the pullback of a rank-(d + 1) differential form from the target space to spacetime. By performing an extension of spacetime, a (d + 2)-form on a (d + 2)-dimensional target space is constructed in a similar way. Extending a theory in d dimensions thus yields a form on a target space of the same dimension as that of a (d + 1)-dimensional theory without extension, defining a dimensional hierarchy. The dimensional relations inherent in the two higher forms studied indicate the possibility of a relation between them.</p>

corrected abstract:
<p>This report discusses higher differential forms with applications in the study of topological phenomena. The integer quantum Hall effect is first discussed, demonstrating a connection between models on a lattice and quantum field theories bridged by a topological invariant, namely the Chern number. Next, for parametrized models on a lattice, the higher Berry curvature is described. This is a rank-𝑑 + 2 differential form on a 𝑑 + 2-dimensional parameter manifold which provides a relation between models in a bulk and on a lower-dimensional interface. Finally, a family of quantum field theories connected to a 𝑑 + 1-dimensional manifold, termed a target space, is constructed. This connection is realized through the incorporation of a set of classical fields, and the effective action of the full field theories all contain a Wess-Zumino-Witten term given by the pullback of a rank-𝑑 + 1 differential form λ<sup>(𝑑+1)</sup> from the target space to spacetime. By performing an extension of spacetime, a 𝑑 + 2-form λ<sup>(𝑑+2),ext</sup> on a 𝑑 + 2-dimensional target space is constructed in a similar way. Extending a theory in 𝑑 dimensions thus yields a form on a target space of the same dimension as that of a 𝑑 + 1-dimensional theory without extension, defining a dimensional hierarchy. The dimensional relations inherent in the higher Berry curvature and the forms λ<sup>(𝑑+2),ext</sup> indicate the possibility of a relation between these two forms.</p>
----------------------------------------------------------------------
In diva2:1829761 
abstract is: 
<p>The problem of constructing a secure encryption scheme that allows for computation on encrypted data was an open problem for more than 30 years. In 2009, Craig Gentry solved the problem, constructing the first fully homomorphic encryption (FHE) scheme. The challenge of constructing a homomorphic encryption scheme can be divided into three main components: 1) Constructing a decryption algorithm that is a ring homomorphism. 2) Proving CPA security. 3) Managing noise growth of evaluated ciphertexts. This thesis presents a formal mathematical background to FHE and discusses these three components. To compute on ciphertexts, the decryption algorithm needs to be homomorpic with respect to multiplication and addition operation. This means, theoretically, computing and then decrypting is equivalent to decrypting and then computing. Security is proved by reductions and hardness assumptions. The standard hardness assumptions used today is the $\operatorname{LWE}$ and $\operatorname{RLWE}$ assumptions. All existing encryption schemes are based on introducing noise when encrypting. This noise grows with each ciphertext operation and without controlling noise, decryption fails given sufficiently many operations. Gentry showed that a scheme that can evaluate its own decryption circuit can be used to reduce the noise and bootstrapped into a fully homomorphic encryption scheme.</p>

corrected abstract:
<p>The problem of constructing a secure encryption scheme that allows for computation on encrypted data was an open problem for more than 30 years. In 2009, Craig Gentry solved the problem, constructing the first fully homomorphic encryption (FHE) scheme. The challenge of constructing a homomorphic encryption scheme can be divided into three main components: 1) Constructing a decryption algorithm that is a ring homomorphism. 2) Proving CPA security. 3) Managing noise growth of evaluated ciphertexts. This thesis presents a formal mathematical background to FHE and discusses these three components. To compute on ciphertexts, the decryption algorithm needs to be homomorpic with respect to multiplication and addition operation. This means, theoretically, computing and then decrypting is equivalent to decrypting and then computing. Security is proved by reductions and hardness assumptions. The standard hardness assumptions used today is the &ThinSpace;LWE&ThinSpace; and &ThinSpace;RLWE&ThinSpace; assumptions. All existing encryption schemes are based on introducing noise when encrypting. This noise grows with each ciphertext operation and without controlling noise, decryption fails given sufficiently many operations. Gentry showed that a scheme that can evaluate its own decryption circuit can be used to reduce the noise and bootstrapped into a fully homomorphic encryption scheme.</p>
----------------------------------------------------------------------
In diva2:1163212 
Note: no full text in DiVA

abstract is: 
<p>This thesis seeks to examine, through the method of linear regression, if the profits of a movie can be predicted using information known prior to the release of the movie. The predictive variables were obtained from a dataset containing information available on the website IMDB. The films were split up into two groups and two different models were created: one for films with an experienced director and lead actor and one where experince wasn't taken into consideration. The results showed that while budget is consistently the driving factor for a films success, the involvement of an experienced director make predictions of success much easier.</p>

corrected abstract:
<p>This thesis seeks to examine, through the method of linear regression, if the profits of a movie can be predicted using information known prior to the release of the movie. The predictive variables were obtained from a dataset containing information available on the website IMDB. The films were split up into two groups and two different models were created: one for films with an experienced director and lead actor and one where experience wasn't taken into consideration. The results showed that while budget is consistently the driving factor for a films success, the involvement of an experienced director make predictions of success much easier.</p>

Note:
w='experince' val={'c': 'experience', 's': 'diva2:1163212', 'n': 'no full text'}
w='IMDB' val={'c': 'IMDb', 's': 'diva2:1163212', 'n': 'error in original; no full text in DiVA'}
----------------------------------------------------------------------
In diva2:1830369   - correct as is

Note spelling errors:
w='heterogenerous' val={'c': 'heterogeneous', 's': 'diva2:1830369', 'n': 'error in original and in the subtitle (which is missing in DiVA)'}
w='interpredability' val={'c': 'interpretability', 's': 'diva2:1830369', 'n': 'error in original'}
w='Ensamble' val={'c': 'Ensemble', 's': 'diva2:1830369', 'n': 'error in original; not also error in title!'}
----------------------------------------------------------------------
In diva2:796765 
abstract is: 
<p>This thesis dene a limit for when hydroelasticity is necessary to include in an analysis of a large oating semi-submersible wind turbine platform in waves. The thesis also includes a description of how to include hydroelasticity in the design of such a structure.</p><p>A simple analysis studying two two-dimensional beams' hydroelastic behaviour in waves is also conducted, observing resonance, large deformations and stresses in the vicinity of the rst elastic natural frequency.</p><p>Hydroelasticity concerns the combined uid-structure interaction for oating exible structures in waves. In a hydroelastic analysis the uid forces and structural deformations are coupled to account for dynamic and kinematic eects. In this thesis the analysed structure is assumed to be beam-like and Euler beam theory is used. The hydrodynamic forces are determined using a linearised Morison's equation. The hydroelastic response is performed in the frequency domain using a modal analysis and it is modelled in a self-developed model using Matlab.</p><p>Most of the concepts and prototypes of oating wind turbines of today have one turbine installed on a oater and the structure is assumed to be rigid. When modelling a structure as exible, elastic responses is observed around the elastic natural frequencies.</p><p>The analysis has been performed on two beams with dierent lengths and stiness' to observe a hydroelastic behavior: 1) when the rst wet elastic natural frequency is about four times the peak frequency of the sea spectra and 2) when the rst wet elastic natural frequency is almost within the sea spectra.</p><p>It has been found that if the rst wet elastic natural frequency of the structure is higher than about 2-5 times than the wave frequency in regular waves or about ve times the peak frequency, a quasi-static assumption is reliable. If the rst wet elastic natural frequency is less than that, hydroelasticity needs to be considered. The actual limit for a quasi-static/hydroelastic assumption needs to be further investigated.</p>

corrected abstract:
<p>This thesis define a limit for when hydroelasticity is necessary to include in an analysis of a large floating semi-submersible wind turbine platform in waves. The thesis also includes a description of how to include hydroelasticity in the design of such a structure. A simple analysis studying two two-dimensional beams' hydroelastic behaviour in waves is also conducted, observing resonance, large deformations and stresses in the vicinity of the first elastic natural frequency.</p><p>Hydroelasticity concerns the combined fluid-structure interaction for floating flexible structures in waves. In a hydroelastic analysis the fluid forces and structural deformations are coupled to account for dynamic and kinematic effects. In this thesis the analysed structure is assumed to be beam-like and Euler beam theory is used. The hydrodynamic forces are determined using a linearised Morison's equation. The hydroelastic response is performed in the frequency domain using a modal analysis and it is modelled in a self-developed model using Matlab.</p><p>Most of the concepts and prototypes of floating wind turbines of today have one turbine installed on a floater and the structure is assumed to be rigid. When modelling a structure as flexible, elastic responses is observed around the elastic natural frequencies.</p><p>The analysis has been performed on two beams with different lengths and stiffness' to observe a hydroelastic behavior: 1) when the first wet elastic natural frequency is about four times the peak frequency of the sea spectra and 2) when the first wet elastic natural frequency is almost within the sea spectra.</p><p>It has been found that if the first wet elastic natural frequency of the structure is higher than about 2-5 times than the wave frequency in regular waves or about five times the peak frequency, a quasi-static assumption is reliable. If the first wet elastic natural frequency is less than that, hydroelasticity needs to be considered. The actual limit for a quasi-static/hydroelastic assumption needs to be further investigated.</p>
----------------------------------------------------------------------
In diva2:756141 
abstract is: 
<p>In this thesis crackling systems have been investigated. Applying an external force on these systems they respond with events of all sizes. The size distribution follows a power law of the form <em>S</em>¯ <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Ctau" />. Dierent types appear in nature like avalanches or earthquakes. A magnet exposed to an external magnetic eld will "crackle" as well during its magnetization. Applying the Ising model on the system, a <em>C</em>++ code has been developed to simulate this process. The resulting data has been used to discuss the system with concepts of statistical mechanics like universality or scale invariance. Varying the level of impurity of the magnet, the disorder <em>R</em>, the system exhibits a continuous nonequilibrium phase transition. At its critical point <em>RC</em> the described power law behavior occurs. The critical exponent has been extracted fitting the histogram data with a power law curve. <em>RC</em> has been determined using the behavior of a scaling function for different values of <em>R</em>.</p>

corrected abstract:
<p>In this thesis "crackling" systems have been investigated. Applying an external force on these systems they respond with events of all sizes. The size distribution follows a power law of the form 𝑆<sup>-&tau;</sup>. Different types appear in nature like avalanches or earthquakes. A magnet exposed to an external magnetic field will "crackle" as well during its magnetization. Applying the Ising model on the system, a 𝐶++ code has been developed to simulate this process. The resulting data has been used to discuss the system with concepts of statistical mechanics like universality or scale invariance. Varying the level of impurity of the magnet, the disorder 𝑅, the system exhibits a continuous nonequilibrium phase transition. At its critical point 𝑅<sub>𝐶</sub> the described power law behavior occurs. The critical exponent &tau; has been extracted fitting the histogram data with a power law curve. 𝑅<sub>𝐶</sub> has been determined using the bahavior of a scaling function for different values of 𝑅.</p>


Note spelling error:
"bahavior" should be "behavior" - error in the original
----------------------------------------------------------------------
In diva2:1849673 
abstract is: 
<p>In this project, the inverse problem of determining regional pressure variations from measured blood velocity data in the contect of a cardiovascular setting has been approached. A common esimator, the pressure poisson estimator (PPE) has been implemented in a non-variational setting and evaluated for clinically relevant synthetic flow cases, over dynamically varying domains, mimicking or directly representing the intra-cardiac space: A synthetic dynamic domain benchmark problem and a patient specific model of the left ventricle. The results obtained show under ideal condition the capability of the approach to tackle complex domains successfully and to obtain regional pressure fields to a high degree of accuracy when compared to a locally provided state of the art estimator, the stokes estimator (STE). Under noise, results obtained suggest that divergence may occur with finer temporal resolution. Spatially convergence in a setting mimicking an image scenario is observed with minor exceptions though to stem from the specific composition of the flow field between discretizations. The implementation at hand avoids common problems in the non-variational approaches of this estimator stemming from domain complexity and leads to a simple application of the pure neumann boundary conditions required to compute the relative pressure field while avoiding the need to estimate boundary normals or use an embedded approach. The resulting linear system has desirable properties such as symmetry and compliance with the discrete compatibility condition by construction.</p>

corrected abstract:
<p>In this project, the inverse problem of determining regional pressure variations from measured blood velocity data in a cardiovascular setting has been approached. A common estimator, the pressure Poisson estimator (PPE), has been implemented in a non-variational setting. It has been evaluated for clinically relevant synthetic flow cases over dynamically varying domains: A dynamic domain benchmark problem mimicking the intra-cardiac space and a patient-specific model of the left ventricle. The results obtained show under ideal conditions the capability of the approach to tackle complex domains successfully and to obtain regional pressure fields to a high degree of accuracy, comparable to that of a locally provided state-of-the-art estimator, the Stokes estimator (STE). Under noise, results obtained suggest that divergence may occur with finer temporal resolution. Error reduction with increasingly finer spatial resolution is still observed in a setting mimicking an imaging scenario. The implementation at hand avoids common problems in the non-variational approaches of this estimator stemming from domain complexity. It allows for a simple application of the pure Neumann boundary conditions required to compute the relative pressure field while avoiding the need to estimate boundary normals or use an embedded approach.  The resulting linear system has desirable properties such as symmetry and satisfies the discrete compatibility condition associated with the Pressure Poisson Equation by construction.</p>

Note:
w='contect' val={'c': 'conext', 's': 'diva2:1849673', 'n': 'DiVA abstract and thesis have different text'}
----------------------------------------------------------------------
In diva2:1642174 
abstract is: 
<p>Walking without pain in the toe has a significant impact on a person’s well-being. Human mobility will be impaired in osteoarthritis of the big toe, and pain will occur during walking. By replacing the cartilage or bone injury with an individualised implant, osteochondral injuries to the knee and ankle can be treated. One company that develops and produces these implants together with associated surgical instruments is called Episurf Medical AB. The company can evaluate a lesion and design the individual implant with its associated instrument based on magnetic resonance imaging or computed tomography images. Episurf currently has a production of implants with associated surgical instruments for the knee and ankle but wants to expand further to implants for the metatarsophalangeal joint (MTP joint), commonly named the big toe joint. In order to perform the work process and create the implant and the surgical instruments, Episurf needs qualitative images taken with MRI or CT. Episurf has specific protocols for CT and MRI for imaging the knee and ankle, but no protocol exists yet for the MTP. In this project, CT and MRI have been used to scan the MTP, where various parameters such as foot position, image plane, slice thickness, slice increment and FOV were tested. For CT, different tube currents and tube voltages and their effect on image quality was also tested. In MRI, different sorts of sequences to use when taking pictures of MTP were evaluated. In addition to CT, a scan was also performed with cone-beam computed tomography(CBCT) to see if it could be an additional imaging modality. When evaluating the images for all imaging modalities, Signal to Noise Ratio(SNR), spatial resolution and contrast were considered. For CT, the radiation was evaluated against image quality, and for MRI, the time aspect was evaluated. For images taken with a CT, the parameter setting for the slice thickness should be 0.5-0.8 mm, and the slice increment should be 50% of the slice thickness. As the foot is not a radiation-sensitive region, the radiation can be high. Since the radiation is recommended to be high and the only critical area to evaluate is the big toe, it is enough to include only the forefoot. Finally, the patient’s foot should be in the standing position to have the best possible evaluation opportunities. MRI needs additional tests to find the best relationship between time, SNR, slice increment and slice thickness. More tests also need to be performed for CBCT, where its technology is examined to create an acceptable segmented 3D model, as it was diﬀicult in this project.</p>

corrected abstract:
<p>Walking without pain in the toe has a significant impact on a person’s well-being. Human mobility will be impaired in osteoarthritis of the big toe, and pain will occur during walking. By replacing the cartilage or bone injury with an individualised implant, osteochondral injuries to the knee and ankle can be treated. One company that develops and produces these implants together with associated surgical instruments is called Episurf Medical AB. The company can evaluate a lesion and design the individual implant with its associated instrument based on magnetic resonance imaging or computed tomography images. Episurf currently has a production of implants with associated surgical instruments for the knee and ankle but wants to expand further to implants for the metatarsophalangeal joint (MTP joint), commonly named the big toe joint. In order to perform the work process and create the implant and the surgical instruments, Episurf needs qualitative images taken with MRI or CT. Episurf has specific protocols for CT and MRI for imaging the knee and ankle, but no protocol exists yet for the MTP. In this project, CT and MRI have been used to scan the MTP, where various parameters such as foot position, image plane, slice thickness, slice increment and FOV were tested. For CT, different tube currents and tube voltages and their effect on image quality was also tested. In MRI, different sorts of sequences to use when taking pictures of MTP were evaluated. In addition to CT, a scan was also performed with cone-beam computed tomography(CBCT) to see if it could be an additional imaging modality. When evaluating the images for all imaging modalities, Signal to Noise Ratio(SNR), spatial resolution and contrast were considered. For CT, the radiation was evaluated against image quality, and for MRI, the time aspect was evaluated. For images taken with a CT, the parameter setting for the slice thickness should be 0.5-0.8 mm, and the slice increment should be 50% of the slice thickness. As the foot is not a radiation-sensitive region, the radiation can be high. Since the radiation is recommended to be high and the only critical area to evaluate is the big toe, it is enough to include only the forefoot. Finally, the patient’s foot should be in the standing position to have the best possible evaluation opportunities. MRI needs additional tests to find the best relationship between time, SNR, slice increment and slice thickness. More tests also need to be performed for CBCT, where its technology is examined to create an acceptable segmented 3D model, as it was difficult in this project.</p>
----------------------------------------------------------------------
In diva2:1163154 - it is likely there is a space missing from the title:
"Impact of maternal characteristics on fetalgrowth in early pregnancies: a statistical analysis"
==>
"Impact of maternal characteristics on fetal growth in early pregnancies: a statistical analysis"
Note: no full text in DiVA

abstract is: 
<p>As a bachelor thesis of the Royal Institute of Technology (Kungliga tekniska hogskolan) in Sweden, a statistical analysis of how mothers characteristics impact their fetuses growth in early pregnancies (between week 12 and 18) has been conducted in collaboration with Dr. Marija Simics's of Karolinska Institutet. The problem consists of nding variables that aect the growth, and how much, and also if some variables aect the growth dierently for some groups, such as boys/girls, smokers/non-smokers, etc. The population consists of 69550 women giving birth in the Stockholm-Gotland region during the years 2008- 2014, and the data is gathered from the obstetric data base Obstetrix. Logistic regression was used as the statistical model, where odds ratios where calculated to see the odds of fetuses growing more or less than expected, and the Wald inference is used to examine the variables signicance. The results show that boys grow more than girls in early pregnancy with the odds ratio for girls being about half that of boys. The study shows that maternal age, height, BMI, and parity is signicant to boys growth, where infants' odds of growing more than expected increases when the mother is older, taller, and larger than the median. The infants' odds decreases the higher the parity. Only height and parity is signicant for girls growth, in the same way as for boys. Mothers pregnant by ivf, pre-pregnancy diabetes and hypertension, and smoking does not aect the growth. The infant also grow slightly more than expected if the mother's country of origin was non-Nordic, but there was no dierence between Sweden and other Nordic countries.</p>

corrected abstract:
<p>As a bachelor thesis of the Royal Institute of Technology (Kungliga tekniska hogskolan) in Sweden, a statistical analysis of how mothers characteristics impact their fetuses growth in early pregnancies (between week 12 and 18) has been conducted in collaboration with Dr. Marija Simics's of Karolinska Institutet. The problem consists of finding variables that affect the growth, and how much, and also if some variables affect the growth differently for some groups, such as boys/girls, smokers/non-smokers, etc. The population consists of 69550 women giving birth in the Stockholm-Gotland region during the years 2008- 2014, and the data is gathered from the obstetric data base Obstetrix. Logistic regression was used as the statistical model, where odds ratios where calculated to see the odds of fetuses growing more or less than expected, and the Wald inference is used to examine the variables significance. The results show that boys grow more than girls in early pregnancy with the odds ratio for girls being about half that of boys. The study shows that maternal age, height, BMI, and parity is significant to boys growth, where infants' odds of growing more than expected increases when the mother is older, taller, and larger than the median. The infants' odds decreases the higher the parity. Only height and parity is significant for girls growth, in the same way as for boys. Mothers pregnant by ivf, pre-pregnancy diabetes and hypertension, and smoking does not affect the growth. The infant also grow slightly more than expected if the mother's country of origin was non-Nordic, but there was no difference between Sweden and other Nordic countries.</p>
----------------------------------------------------------------------
In diva2:919831 
abstract is: 
<p>This is a master thesis project conducted for Scania CV AB in collaboration with Swerea Kimab. The purpose is to examine how oils and coatings on the surface aﬀect the adhesion of adhesives. Earlier work done by Scania indicate that the amount of oil applied may have an impact on the adhesion. Substrates tested are hot dipped galvanised steel, electro galvanised. AlSi and ZnMg. Oils used are Anticorit RP 3802 that is an anti-corrosive oil and Renoform 3802 that is a drawing oil. The two adhesives used are Betamate 1496f and SikaPower 498.The performance of adhesive bonds is strongly dependent on the surface it adheres to and any contam-inates such as oil present on the surface. These factors may greatly decrease the performance of the bond. There are adhesives that have been designed to tolerate a speciﬁc amount of oil on the surfaces and should develop a satisfactory bond with oil present.This project has ﬁrstly developed a routine for easy application of oil with the result of a known amount of oil and a uniform oil distribution on the surface of the coated sample. Secondly lap-shear tests have been performed for various amounts and types of oil in combination with four diﬀerent coatings and two adhesives. For the evaluation of failure mode a program using k-mean factoring was written to provide an objective method to characterise the bond.Lap-shear tests show that there does not seem to be any apparent diﬀerence in bond strength for various amounts of oil and for the two diﬀerent adhesives except for one combination. Even thought the two adhesives develop the same strength the failure mode diﬀers between the two.</p>

corrected abstract:
<p>This is a master thesis project conducted for Scania CV AB in collaboration with Swerea Kimab. The purpose is to examine how oils and coatings on the surface affect the adhesion of adhesives. Earlier work done by Scania indicate that the amount of oil applied may have an impact on the adhesion. Substrates tested are hot dipped galvanised steel, electro galvanised. AlSi and ZnMg. Oils used are Anticorit RP 3802 that is an anti-corrosive oil and Renoform 3802 that is a drawing oil. The two adhesives used are Betamate 1496f and SikaPower 498.</p><p>The performance of adhesive bonds is strongly dependent on the surface it adheres to and any contaminates such as oil present on the surface. These factors may greatly decrease the performance of the bond. There are adhesives that have been designed to tolerate a specific amount of oil on the surfaces and should develop a satisfactory bond with oil present.</p><p>This project has firstly developed a routine for easy application of oil with the result of a known amount of oil and a uniform oil distribution on the surface of the coated sample. Secondly lap-shear tests have been performed for various amounts and types of oil in combination with four different coatings and two adhesives. For the evaluation of failure mode a program using k-mean factoring was written to provide an objective method to characterise the bond.</p><p>Lap-shear tests show that there does not seem to be any apparent difference in bond strength for various amounts of oil and for the two different adhesives except for one combination. Even thought the two adhesives develop the same strength the failure mode differs between the two.</p>
----------------------------------------------------------------------
In diva2:839830 - 'diva2:1040695' is a duplicate 
abstract is: 
<p>The main subject of the thesis is impact simulation of an elastic fuel tank</p><p>reinforced with a polymer exoskeleton. Thanks to its light weight and failure</p><p>resistance, this type of design shows potential to be used in aerospace</p><p>applications. The simulation imitates a drop test from the height of 20 m</p><p>on a rigid surface, in accordance with Military Handbook testing guidelines</p><p>for fuel tanks. The focus is on establishing the best practices for modelling</p><p>and solving this type of problems. The computational methods are tested</p><p>on a generic model of a rectangular prismatic tank with rounded edges. The</p><p>walls of the tank are made of a combination hyperelastic rubber material orthotropic</p><p>fabrics. The simulation is performed for the 70% and 100% water</p><p>lled tank. All calculations are performed using the Altair HyperWorks 13.0</p><p>software suite, in particular the RADIOSS solver and OptiStruct Solver and</p><p>Optimiser. The criteria for evaluation of model and simulation quality are</p><p>suggested. Comparison is made between various uid (ALE and SPH) and</p><p>solid (composite and hyperelastic) modelling approaches. Material parameters</p><p>are found using the least-squares t to the experimental measurements.</p><p>The nal, most robust and accurate model serves as a basis for establishing</p><p>a design optimisation procedure, aiming at reduction of mass of tank</p><p>components while ensuring the structural integrity. Furthermore, the advantages</p><p>and drawbacks of di erent modelling approaches are discussed. The</p><p>main conclusions from the study are summarised and suggested directions</p><p>for future work are given.</p>

corrected abstract:
<p>The main subject of the thesis is impact simulation of an elastic fuel tank reinforced with a polymer exoskeleton. Thanks to its light weight and failure resistance, this type of design shows potential to be used in aerospace applications. The simulation imitates a drop test from the height of 20 m on a rigid surface, in accordance with Military Handbook testing guidelines for fuel tanks. The focus is on establishing the best practices for modelling and solving this type of problems. The computational methods are tested on a generic model of a rectangular prismatic tank with rounded edges. The walls of the tank are made of a combination hyperelastic rubber material orthotropic fabrics. The simulation is performed for the 70% and 100% water filled tank. All calculations are performed using the Altair HyperWorks 13.0 software suite, in particular the RADIOSS solver and OptiStruct Solver and Optimiser. The criteria for evaluation of model and simulation quality are suggested. Comparison is made between various fluid (ALE and SPH) and solid (composite and hyperelastic) modelling approaches. Material parameters are found using the least-squares fit to the experimental measurements. The final, most robust and accurate model serves as a basis for establishing a design optimisation procedure, aiming at reduction of mass of tank components while ensuring the structural integrity. Furthermore, the advantages and drawbacks of different modelling approaches are discussed. The main conclusions from the study are summarised and suggested directions for future work are given.</p>
----------------------------------------------------------------------
In diva2:753649 
abstract is: 
<p>Computational models of neural activity and neural networks have been an active area of research as long as there have been computers, and have led several important discoveries in the eld of machine learning. One kind of articial network proposed by John J. Hopeld in 1982 has been among the more successful ones, and is still in active use today. It has been suggested that in addition to its merits in machine learning, it could also serve as a foundation of the explanation of human ability of recollection and association. However, Hopeld's original design used a very simplied model of neurons. By using so called integrate-and-remodels, higher realism can be achieved.</p><p>This report begins with a discussion of mechanistic and quantitative description of neurons, in particular the induction of action potentials, and furthermore why an integrate-and-re model is a reasonable choice for a model of intermediate complexity. By explicitly describing individual spikes, a fundamental but often neglected characteristic of communication between neurons is captured. Integrate-and-re models are included in the Neural Simulation Tool (NEST), and in this report such a neural model is applied to Hopeld networks. Both spike-rate coding and temporal coding are studied, as well as a simple model of synaptic Spike-Timing DependentPlasticity (STDP) for online learning. The networks' robustness is evaluated with respect to changes in (a) global scaling of the synaptic weights, (b) delays in the synaptic connections, (c) level of noise and (d) strength of input stimuli. They are found to be somewhat sensitive, with (a) giving the most denite results, suggesting that the used description of Hopfield networks might not be an immediately plausible biological model. In particular, networks using temporal coding are found to be especially difficult to calibrate. This could reveal a potential weakness in relatively recent and apparently successful models.</p>

corrected abstract:
<p>Computational models of neural activity and neural networks have been an active area of research as long as there have been computers, and have led several important discoveries in the field of machine learning. One kind of artificial network proposed by John J. Hopfield in 1982 has been among the more successful ones, and is still in active use today. It has been suggested that in addition to its merits in machine learning, it could also serve as a foundation of the explanation of human ability of recollection and association. However, Hopfield's original design used a very simplified model of neurons. By using so called integrate-and-fire models, higher realism can be achieved.</p><p>This report begins with a discussion of mechanistic and quantitative description of neurons, in particular the induction of action potentials, and furthermore why an integrate-and-fire model is a reasonable choice for a model of intermediate complexity. By explicitly describing individual spikes, a fundamental but often neglected characteristic of communication between neurons is captured. Integrate-and-fire models are included in the Neural Simulation Tool (NEST), and in this report such a neural model is applied to Hopfield networks. Both spike-rate coding and temporal coding are studied, as well as a simple model of synaptic Spike-Timing Dependent Plasticity (STDP) for online learning. The networks' robustness is evaluated with respect to changes in (a) global scaling of the synaptic weights, (b) delays in the synaptic connections, (c) level of noise and (d) strength of input stimuli. They are found to be somewhat sensitive, with (a) giving the most definite results, suggesting that the used description of Hopfield networks might not be an immediately plausible biological model. In particular, networks using temporal coding are found to be especially difficult to calibrate. This could reveal a potential weakness in relatively recent and apparently successful models.</p>
----------------------------------------------------------------------
In diva2:1055187 
abstract is: 
<p>An asset manager's goal is to provide a high return relative the risk taken, and thus faces the challenge of how to choose an optimal portfolio. Many mathematical methods have been developed to achieve a good balance between these attributes and using di erent risk measures. In thisthesis, we test the use of a relatively simple and common approach: the Markowitz mean-variance method, and a more quantitatively demanding approach: the tail optimization method. Using active portfolio based on data provided by the Swedish fund management company Enter Fonderwe implement these approaches and compare the results. We analyze how each method weighs theunderlying assets in order to get an optimal portfolio.</p>
mc='theunderlying' c='the underlying'

corrected abstract:
<p>An asset manager's goal is to provide a high return relative the risk taken, and thus faces the challenge of how to choose an optimal portfolio. Many mathematical methods have been developed to achieve a good balance between these attributes and using different risk measures. In this thesis, we test the use of a relatively simple and common approach: the Markowitz mean-variance method, and a more quantitatively demanding approach: the tail optimization method. Using a fictive portfolio based on data provided by the Swedish fund management company Enter Fonder we implement these approaches and compare the results. We analyze how each method weighs the underlying assets in order to get an optimal portfolio.</p>
----------------------------------------------------------------------
In diva2:1478013 - missing space in title:
"Implementation of moisturedependent constitutive model for paperboard"
==>
"Implementation of moisture dependent constitutive model for paperboard"

abstract is: 
<p>There has been a considerable increase in the usage of paper products due to its sustainability in the product cycle. Many environmental and process variables can affect the mechanical behavior of paper from its making to finished products. Of these variables, moisture is of particular importance and strongly influences both papermaking, converting, and end-use of the paper products.</p><p>Experimental investigations at different humidity levels reveals that normalized in-plane constitutive parameters, such as elastic parameters and the linear hardening modulus, in both MD and CD1) follow a linear relationship with normalized moisture ratio. This relation is found to be acceptable for a wide range of commercial paperboards. To capture this observation, a novel material model with orthotropic elasticity and anisotropic hardening2 is proposed. An associative flow rule for the evolution of plastic strain is proposed. The proposed flow rule is such that all stresses contribute to plastic flow rather than an effective stress. A simple version using anisotropic linear hardening is implemented. The mechanical properties, such as elastic parameters and hardening moduli are considered functions of the moisture ratio. An implicit variant of the material model is implemented in LS-DYNA®. The simulations with the proposed material model at different humidity levels follow the experimental results well for uniaxial loading, but discrepancies are obtained for simulation of biaxial loading tests.</p><p>The moisture is assumed constant in the proposed model since the experiments are done in a moisture-controlled environment.</p>

corrected abstract:
<p>There has been a considerable increase in the usage of paper products due to its sustainability in the product cycle. Many environmental and process variables can affect the mechanical behavior of paper from its making to finished products. Of these variables, moisture is of particular importance and strongly influences both papermaking, converting, and end-use of the paper products.</p><p>Experimental investigations at different humidity levels reveals that normalized in-plane constitutive parameters, such as elastic parameters and the linear hardening modulus, in both MD and CD<sup><a href="#fn1" id="ref1">1</a></sup>) follow a linear relationship with normalized moisture ratio. This relation is found to be acceptable for a wide range of commercial paperboards. To capture this observation, a novel material model with orthotropic elasticity and anisotropic hardening<sup><a href="#fn2" id="ref2">2</a></sup> is proposed. An associative flow rule for the evolution of plastic strain is proposed. The proposed flow rule is such that all stresses contribute to plastic flow rather than an effective stress. A simple version using anisotropic linear hardening is implemented. The mechanical properties, such as elastic parameters and hardening moduli are considered functions of the moisture ratio. An implicit variant of the material model is implemented in LS-DYNA®. The simulations with the proposed material model at different humidity levels follow the experimental results well for uniaxial loading, but discrepancies are obtained for simulation of biaxial loading tests.</p><p>The moisture is assumed constant in the proposed model since the experiments are done in a moisture-controlled environment.</p>
<div id="footnotes">
    <ol>
        <li id="fn1">MD – Machine Direction; CD – Cross Direction <a href="#ref1" aria-label="Back to reference">↩</a></ĺi>
        <li id="fn2">Anisotropic Hardening – Different Hardening modulus in different directions <a href="#ref2" aria-label="Back to reference">↩</a></li>
    </ol>
</div>
----------------------------------------------------------------------
In diva2:1287155 
abstract is: 
<p>This thesis investigates the possibilities of implementing thermal aspects in Topology Optimization (TO) of hot engine structures. Topology Optimization is an effective tool for conceptual design in numerous field of applications. At GKN, this optimization technique has previously only been successfully implemented for structures affected by mechanical loads. The aim with this study and the ongoing research at GKN, is to improve the in-house developed multi-disciplinary optimization procedure called Engineering WorkBench (EWB). By expanding the applicability of a more comprehensive TO which includes the thermal expansion. However, since there is no straight forward solution provided by the FE software’s, a better understanding of TO in general and for thermally loaded structures in particular, is needed before deciding on an application strategy.Two approaches for the thermal implementation in TO of the Turbine Rear Structure (TRS) have been studies and evaluated. The first is a stress constrained optimization procedure, based on requirements for the number of thermal load cycles, calculated in CUMFAT, an in-house developed program for life prediction. The second approach is a case trial study of the coupled thermal-mechanical structural optimization. The trials are performed systematically to illustrate what type of geometrical variations one can expect in the TO outcome when varying different factors in the optimization set up, such as the load type magnitude and optimization formulation. The evaluation of these to different approaches will increase the understanding of the challenges involved when performing TO of this type of structure.The complexity of this implementation is clearly demonstrated by the variation in optimization outcome. The results shows the importance of having substantial knowledge about the model, load cases and the optimization purpose before defining the optimization problem. Finally, suggestions for the continuation and implementation of thermal TO in the EWB, are presented.</p>

corrected abstract:
<p>This thesis investigates the possibilities of implementing thermal aspects in Topology Optimization (TO) of hot engine structures. Topology Optimization is an effective tool for conceptual design in numerous field of applications. At GKN, this optimization technique has previously only been successfully implemented for structures affected by mechanical loads. The aim with this study and the ongoing research at GKN, is to improve the in-house developed multi-disciplinary optimization procedure called Engineering WorkBench (EWB). By expanding the applicability of a more comprehensive TO which includes the thermal expansion. However, since there is no straight forward solution provided by the FE software’s, a better understanding of TO in general and for thermally loaded structures in particular, is needed before deciding on an application strategy.</p><p>Two approaches for the thermal implementation in TO of the Turbine Rear Structure (TRS) have been studies and evaluated. The first is a stress constrained optimization procedure, based on requirements for the number of thermal load cycles, calculated in CUMFAT, an in-house developed program for life prediction. The second approach is a case trial study of the coupled thermal-mechanical structural optimization. The trials are performed systematically to illustrate what type of geometrical variations one can expect in the TO outcome when varying different factors in the optimization set up, such as the load type magnitude and optimization formulation. The evaluation of these to different approaches will increase the understanding of the challenges involved when performing TO of this type of structure.</p><p>The complexity of this implementation is clearly demonstrated by the variation in optimization outcome. The results shows the importance of having substantial knowledge about the model, load cases and the optimization purpose before defining the optimization problem. Finally, suggestions for the continuation and implementation of thermal TO in the EWB, are presented.</p>
----------------------------------------------------------------------
In diva2:1272346 
abstract is: 
<p>There is a desire both from the patient and the society to have eﬃcient tools for diﬀerential diagnostics. Mathematical relationships between diseases and observable consequences are deﬁned in the thesis. Speciﬁcally artiﬁcial neural networks are considered in the modeling of the doctor’s methodology. To suggest further lab tests or symptoms to look for the network is inverted by looking at a minimization problem where the objective function gradient can be analytically calculated. Due to diﬃculties in obtaining real life medical data a program was constructed to generate artiﬁcial patient data sets. These data sets will be used to establish proof of concepts. Some data set quality measures are deﬁned and used to model the network accuracy and training time. It is then estimated that a problem with 4000 diagnoses and 20 000 observable consequences would require 200 000 patients to obtain a classiﬁcation accuracy of 99% with a training time of 50 hours depending on the computational power. Overall the solution strategy seems promising but studies on real life data is required for deﬁnitive answers.</p>

corrected abstract:
<p>There is a desire both from the patient and the society to have efficient tools for differential diagnostics. Mathematical relationships between diseases and observable consequences are defined in the thesis. Specifically artificial neural networks are considered in the modeling of the doctor’s methodology. To suggest further lab tests or symptoms to look for the network is inverted by looking at a minimization problem where the objective function gradient can be analytically calculated. Due to difficulties in obtaining real life medical data a program was constructed to generate artificial patient data sets. These data sets will be used to establish proof of concepts. Some data set quality measures are defined and used to model the network accuracy and training time. It is then estimated that a problem with 4000 diagnoses and 20 000 observable consequences would require 200 000 patients to obtain a classification accuracy of 99% with a training time of 50 hours depending on the computational power. Overall the solution strategy seems promising but studies on real life data is required for definitive answers.</p>
----------------------------------------------------------------------
In diva2:1587914 
abstract is: 
<p>The use of fluorescent proteins as fluorescent markers has exploded over the last decades. In particular due to the development of advanced microscopy for live cell measurements, dynamic molecular studies down to single molecule levels and for superresolution microscopy. Many variants of fluorescent proteins exist with varying properties, such as emission color, photostability and brightness. These properties enable advanced applications, like timeresolved imaging or imaging below the diffraction limit. However, the photophysics of fluorescent proteins are complex and in many aspects quite unexplored. The triplet state in particular, is a central photophysical state because it is an entrance gate to an ensamble of deleterious photochemical processes that compromise the photostability of fluorescent proteins.The Pixel team at Institute de Biologie Structurale in France, is mainly focused on developing fluorescent proteins for advanced fluorescence imaging. One of the goals is to understand the influence of photochemistry on the properties of fluorescent proteins.In this project, a method to indirectly observe the triplet state in the prototypical EGFP fluorescent protein was developed. The introduction of new hardware and software, coupled to biophysical experiments, required an interdisciplinary strategy to tackle the obstacles during the route. Experiments under different environmental conditions to test the influence on the population of the triplet state of viscosity, pH, UV and infrared light, triplet state quenchers and temperature were performed.The results show that temperature and laser power greatly influence the triplet state kinetics in EGFP. Notably, it was found that the triplet state lifetime strongly increases at cryotemperature in comparison to roomtemperature. Overall, the newly developed setup and our preliminary results on EGFP open the door to novel studies on the photophysical properties of fluorescent proteins.</p>

corrected abstract:
<p>The use of fluorescent proteins as fluorescent markers has exploded over the last decades. In particular due to the development of advanced microscopy for live cell measurements, dynamic molecular studies down to single molecule levels and for super-resolution microscopy. Many variants of fluorescent proteins exist with varying properties, such as emission color, photostability and brightness. These properties enable advanced applications, like time-resolved imaging or imaging below the diffraction limit. However, the photophysics of fluorescent proteins are complex and in many aspects quite unexplored. The triplet state in particular, is a central photophysical state because it is an entrance gate to an ensamble of deleterious photochemical processes that compromise the photostability of fluorescent proteins.</p><p>The <em>Pixel</em> team at Institute de Biologie Structurale in France, is mainly focused on developing fluorescent proteins for advanced fluorescence imaging. One of the goals is to understand the influence of photochemistry on the properties of fluorescent proteins.</p><p>In this project, a method to indirectly observe the triplet state in the prototypical EGFP fluorescent protein was developed. The introduction of new hardware and software, coupled to biophysical experiments, required an interdisciplinary strategy to tackle the obstacles during the route. Experiments under different environmental conditions to test the influence on the population of the triplet state of viscosity, pH, UV and infrared light, triplet state quenchers and temperature were performed.</p><p>The results show that temperature and laser power greatly influence the triplet state kinetics in EGFP. Notably, it was found that the triplet state lifetime strongly increases at cryo-temperature in comparison to room-temperature. Overall, the newly developed setup and our preliminary results on EGFP open the door to novel studies on the photophysical properties of fluorescent proteins.</p>
----------------------------------------------------------------------
In diva2:1057236 
abstract is: 
<p>The frequency response of an in-line pressure measuring setup was investigated in order to evaluate its suitability for use in wind tunnel measurements. Testing was conducted in a laboratory environment and a general procedure was developed for both the calibration of the sensors and obtaining the transfer functions for the setup. The tests showed that the in-line setup displays a distinct frequency response, which needs to be accounted for. The developed methods were also tested as a part of a wind tunnel measurement with the calibrations and the measurement of the frequency response done on the sensors in the model, which included both terminal and in-line congurations. Finally, data for a terminal sensor obtained from the testing was corrected by using a previously measured transfer function.</p>

corrected abstract:
<p>The frequency response of an in-line pressure measuring setup was investigated in order to evaluate its suitability for use in wind tunnel measurements. Testing was conducted in a laboratory environment and a general procedure was developed for both the calibration of the sensors and obtaining the transfer functions for the setup. The tests showed that the in-line setup displays a distinct frequency response, which needs to be accounted for. The developed methods were also tested as a part of a wind tunnel measurement with the calibrations and the measurement of the frequency response done on the sensors in the model, which included both terminal and in-line configurations. Finally, data for a terminal sensor obtained from the testing was corrected by using a previously measured transfer function.</p>
----------------------------------------------------------------------
In diva2:725585 
abstract is: 
<p>In recent years investors in the financial markets around the globe have begun to focus on non-financial factors in their portfolio selection processes. Three main areas of concerns are: Environmental, Social and corporate Governance (ESG). Previous research has mainly focused on implementing these concerns using qualitative methods, e.g. negative screening. Our thesis integrates these concerns in a Multi-Criteria Decision-Making (MCDM) framework, making it possible for investors to view the portfolio selection as a trade-o_ between three criteria: Return, Risk and ESG. This extends the traditional Markowitz frontier from two to three dimensions. Companies included are the ones in the index OMXS30. Return and risk are estimated using the single-index model. The ESG criterion is implemented as a linear function and estimated using two public ESG indices.</p><p>We will use two different optimization methods, the weighted sum approach and the "-constraint method to compute the efficient frontier. These are evaluated and we conclude that each method has its own strengths and weaknesses. We can see that integrating ESG concerns as a third objective in addition to risk and return alters the portfolio selection process. It increases the complexity of choosing a portfolio, but also yielding a better decision basis for the investor. To mitigate the increase of complexity we propose the ESG-to-variability ratio in analogy with the Sharpe ratio, effectively reducing the number of portfolios an investor should consider.</p><p> </p>


corrected abstract:
<p>In recent years investors in the financial markets around the globe have begun to focus on non-financial factors in their portfolio selection processes. Three main areas of concerns are: Environmental, Social and corporate Governance (ESG). Previous research has mainly focused on implementing these concerns using qualitative methods, e.g. negative screening. Our thesis integrates these concerns in a Multi-Criteria Decision-Making (MCDM) framework, making it possible for investors to view the portfolio selection as a trade-off between three criteria: Return, Risk and ESG. This extends the traditional Markowitz frontier from two to three dimensions. Companies included are the ones in the index OMXS30. Return and risk are estimated using the single-index model. The ESG criterion is implemented as a linear function and estimated using two public ESG indices.</p><p>We will use two different optimization methods, the weighted sum approach and the &#x1D700;-constraint method to compute the efficient frontier. These are evaluated and we conclude that each method has its own strengths and weaknesses. We can see that integrating ESG concerns as a third objective in addition to risk and return alters the portfolio selection process. It increases the complexity of choosing a portfolio, but also yielding a better decision basis for the investor. To mitigate the increase of complexity we propose the ESG-to-variability ratio in analogy with the Sharpe ratio, effectively reducing the number of portfolios an investor should consider.</p>

Note &#x1D700; is the symbol: Mathematical Italic Small Epsilon
----------------------------------------------------------------------
In diva2:1476282 
Note: no full text in DiVA

abstract is: 
<p>In the view of global energy crisis due to overconsumption of fossil fuels, increasing the efficiency and the affordability of clean energy sources, such as solar power, are of great importance. The market today is dominated by silicon based solar cells, which suffer from high material usage and thus cost. One competing technology is thin film cells which is cheaper but the efficiencies cannot yet compete with traditional Si cells. We propose an indium phosphide nanoresonator array on silicon thin film solar cell as a solution to minimize cost and maximize power efficiency. This thesis is a study of the antireflection and absorption properties of indium phosphide (InP) nanodisk array-on-silicon (Si). Lumerical FDTD: 3D electromagnetic simulator was used to simulate and optimize the reflectance and absorbance of the proposed design. By varying the height and radius of the InP nanodisks on Si substrate, together with the array pitch, a weighted reflectance minimum of 2.89% was obtained in the wavelength range of 400-1100 nm. By covering the structures with aluminum zinc oxide (AZO), a widely used front contact material, the reflectance could further be reduced to 1.43%. The antireflective properties were found to be a combination of a Mie-resonance induced suppression of backwards scattering and an effective index matching to the Si substrate. Absorption of incident light was also found to be enhanced in the InP nanodisk/Si structures in comparison to bare Si and Si nanodisk/Si structures. At a depth of 5 µm from the surface of the substrate, the absorption values were found to be 74.6% for the AZO covered InP nanodisk /Si structure and 45.3% for a bare Si substrate. This shows that the high-index nanoresonator arrays on thin film Si solar cells can be a novel design to enhance the absorption efficiency of the cell. Embedding the InP arrays in a PDMS substrate resulted in narrow transmission supression peaks and peaks of near 100% absorption of incident light.</p>


corrected abstract:
<p>In the view of global energy crisis due to overconsumption of fossil fuels, increasing the efficiency and the affordability of clean energy sources, such as solar power, are of great importance. The market today is dominated by silicon based solar cells, which suffer from high material usage and thus cost. One competing technology is thin film cells which is cheaper but the efficiencies cannot yet compete with traditional Si cells. We propose an indium phosphide nanoresonator array on silicon thin film solar cell as a solution to minimize cost and maximize power efficiency. This thesis is a study of the antireflection and absorption properties of indium phosphide (InP) nanodisk array-on-silicon (Si). Lumerical FDTD: 3D electromagnetic simulator was used to simulate and optimize the reflectance and absorbance of the proposed design. By varying the height and radius of the InP nanodisks on Si substrate, together with the array pitch, a weighted reflectance minimum of 2.89% was obtained in the wavelength range of 400-1100 nm. By covering the structures with aluminum zinc oxide (AZO), a widely used front contact material, the reflectance could further be reduced to 1.43%. The antireflective properties were found to be a combination of a Mie-resonance induced suppression of backwards scattering and an effective index matching to the Si substrate. Absorption of incident light was also found to be enhanced in the InP nanodisk/Si structures in comparison to bare Si and Si nanodisk/Si structures. At a depth of 5 µm from the surface of the substrate, the absorption values were found to be 74.6% for the AZO covered InP nanodisk /Si structure and 45.3% for a bare Si substrate. This shows that the high-index nanoresonator arrays on thin film Si solar cells can be a novel design to enhance the absorption efficiency of the cell. Embedding the InP arrays in a PDMS substrate resulted in narrow transmission suppression peaks and peaks of near 100% absorption of incident light.</p>
----------------------------------------------------------------------
In diva2:1448896   - correct as is

Note spelling errors:
w='hypotheis' val={'c': 'hypothesis', 's': 'diva2:1448896', 'n': 'error in original'}
w='thermomechancial' val={'c': 'thermomechanical', 's': 'diva2:1448896', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1082039 
abstract is: 
<p>Technology&amp;Strategy is a French-German consulting company based in Strasbourg, France. Most of its activities take place in the automotive sector with many of their consultants working for Bosch Chassis Control in Abstatt, Germany. Bosch Chassis Control is responsible for the development of active safety control systems for many car manufacturers worldwide. The ESP (Electronic Stability Program) was created by Bosch and is now compulsory in every European car since 2014. The development of the ESP includes an portant part of onboard testing before the production release.</p><p>The application team is responsible for achieving those tests and analyzing the data recorded. One of the features of the ESP is the TCS (Traction Control System) allowing the vehicle to remain at its maximum friction available depending on several parameters such as road friction, speed or wheel angle. The goal of this master thesis is to study the control logic of the TCS and highlight the inuence of several parameters from the software to the vehicle dynamics. The rst part of this thesis was dedicated to the development of a MATLAB tool used to visualize measurements out of di erent test sessions on the same plot. The current tool used is limited to a single measurement display.</p><p>The second part focuses on the analysis of TCS data taken through several test sessions with di erent parameters. The two main controllers of the TCS, PTC and BTC (Powertrain Traction Control and Brake Traction Control) are the core of this analysis.</p>

corrected abstract:
<p>Technology&amp;Strategy is a French-German consulting company based in Strasbourg, France. Most of its activities take place in the automotive sector with many of their consultants working for Bosch Chassis Control in Abstatt, Germany.</p><p>Bosch Chassis Control is responsible for the development of active safety control systems for many car manufacturers worldwide. The ESP (Electronic Stability Program) was created by Bosch and is now compulsory in every European car since 2014. The development of the ESP includes an important part of onboard testing before the production release. The application team is responsible for achieving those tests and analyzing the data recorded.</p><p>One of the features of the ESP is the TCS (Traction Control System) allowing the vehicle to remain at its maximum friction available depending on several parameters such as road friction, speed or wheel angle. The goal of this master thesis is to study the control logic of the TCS and highlight the influence of several parameters from the software to the vehicle dynamics.</p><p>The first part of this thesis was dedicated to the development of a MATLAB tool used to visualize measurements out of different test sessions on the same plot. The current tool used is limited to a single measurement display.</p><p>The second part focuses on the analysis of TCS data taken through several test sessions with different parameters. The two main controllers of the TCS, PTC and BTC (Powertrain Traction Control and Brake Traction Control) are the core of this analysis.</p>
----------------------------------------------------------------------
In diva2:491817 
abstract is: 
<p>The purpose of the present thesis is to study the influence of Proton Exchange on Electric Field assisted Poling of congruent Lithium Niobate and its applications on periodically patterned structures. Moreover, the possibility of using Proton Exchange to avoid neighbours domains merging is studied and successfully demonstrated for period shorter than 10μm.Before approaching the poling of periodically patterned LiNbO3 samples, the main characteristics of the evolution of the poling of uniform samples in different masking conditions are investigated. It is well known that the kinetics of domains switching is highly dependent on the poling setup and on the quality/type of electrode employed to contact the crystal to the high voltage. We used a thin layer of Titanium both as mask for proton diffusion and as metal electrode for poling experiments. Moreover different masking configurations are pre-sented and characterized.The second part of this work deals with the periodic poling of 0.5mm-thick congruent lithium niobate. 9x4 mm2 1-D Ti gratings with 8.66μm and 8.03μm period were first fabricated on the +z side of the crystal and a superficial chemical pattern was reproduced via acid bath. Three different types of samples were obtained and before the poling the metallic mask was removed whereas in one configuration it was left assuring better homogeneity of the inverted areas.The results we obtained suggest it could be possible to achieve periodically poled congruent lithium niobate gratings with period shorter than 4μm in ~500μm thick samples and hence obtain aspect ratios of more than 250.</p>

corrected abstract:
<p>The purpose of the present thesis is to study the influence of Proton Exchange on Electric Field assisted Poling of congruent Lithium Niobate and its applications on periodically patterned structures. Moreover, the possibility of using Proton Exchange to avoid neighbours domains merging is studied and successfully demonstrated for period shorter than 10μm.</p><p>Before approaching the poling of periodically patterned LiNbO<sub>3</sub> samples, the main characteristics of the evolution of the poling of uniform samples in different masking conditions are investigated. It is well known that the kinetics of domains switching is highly dependent on the poling setup and on the quality/type of electrode employed to contact the crystal to the high voltage. We used a thin layer of Titanium both as mask for proton diffusion and as metal electrode for poling experiments. Moreover different masking configurations are presented and characterized.</p><p>The second part of this work deals with the periodic poling of 0.5mm-thick congruent lithium niobate. 9x4 mm<sup>2</sup> 1-D Ti gratings with 8.66µm and 8.03µm period were first fabricated on the +z side of the crystal and a superficial chemical pattern was reproduced via acid bath. Three different types of samples were obtained and before the poling the metallic mask was removed whereas in one configuration it was left assuring better homogeneity of the inverted areas.</p><p>The results we obtained suggest it could be possible to achieve periodically poled congruent lithium niobate gratings with period shorter than 4µm in ~500µm thick samples and hence obtain aspect ratios of more than 250.</p>
----------------------------------------------------------------------
In diva2:1640371 
abstract is: 
<p>The spontaneous spreading of a droplet on a dry surface is a phenomenon pre-valent in both natural and industrial processes. Droplet spreading has been extensively studied in the context of Newtonian ﬂuids. Recently, however, the study of non-Newtonian spreading has been getting increasing attention. This study investigates the inﬂuence shear-thinning eﬀects have on the spreading be-haviour of a droplet. This is done by adding to water increasing concentrations of a polymer causing shear-thinning. Both rapid spreading (∼ 10−3 s) and late spreading (∼ 1 s) are investigated. During rapid spreading the Flopam concentration showed limited inﬂuence on spreading speed and the Flopam solution spread as fast as water. The high shear stress present near the contact line in this phase caused the viscosity to be low enough so as to not aﬀect the spreading rate. For late spreading, the increase in Flopam concentration inﬂuenced the sprea-ding rate. A Flopam concentration of 1000 ppm acted as a threshold for sprea-ding behaviour. Below the threshold, spreading rate was higher than for water. Above the threshold, spreading rate was lower than for water. Scaling of the late spreading with a viscous timescale was attempted using two diﬀerent approaches, a ﬁxed shear rate and chosen length scale.Both methods provided a more accurate collapse for higher shear rates. However, the chosen length scale method of scaling provided a more accurate collapse of the curves.</p>

corrected abstract:
<p>The spontaneous spreading of a droplet on a dry surface is a phenomenon prevalent in both natural and industrial processes. Droplet spreading has been extensively studied in the context of Newtonian fluids. Recently, however, the study of non-Newtonian spreading has been getting increasing attention. This study investigates the influence shear-thinning effects have on the spreading behaviour of a droplet. This is done by adding to water increasing concentrations of a polymer causing shear-thinning. Both rapid spreading (∼ 10<sup>−3</sup> s) and late spreading (∼ 1 s) are investigated.</p><p>During rapid spreading the Flopam concentration showed limited influence on spreading speed and the Flopam solution spread as fast as water. The high shear stress present near the contact line in this phase caused the viscosity to be low enough so as to not affect the spreading rate.</p><p>For late spreading, the increase in Flopam concentration influenced the spreading rate. A Flopam concentration of 1000 ppm acted as a threshold for spreading behaviour. Below the threshold, spreading rate was higher than for water.</p><p>Above the threshold, spreading rate was lower than for water.</p><p>Scaling of the late spreading with a viscous timescale was attempted using two different approaches, a fixed shear rate and chosen length scale.</p><p>Both methods provided a more accurate collapse for higher shear rates. However, the chosen length scale method of scaling provided a more accurate collapse of the curves.</p>
----------------------------------------------------------------------
In diva2:515843 
abstract is: 
<p>This work studied the information flow and information handling among different parties. The study focused on how educational materials on national tests from The Swedish National Agency for Education is managed by school principals and teachers and how the information is passed on to students. The data material was based on observations and interviews at two different high schools. Six teachers and their classes and principals participated in the study. The Agency was also interviewed. The results showed that opinions on the role of the national test varies between different teachers and the Agency. The study also showed that the dissemination of information in the schools is inadequate, information documents do not reach their target audience, and those who have access to the information do not read enough. This is partly due to lack of follow-up from the responsibles at the schools, but also teachers who do not seek out information that is available themselves. These shortcomings result in teachers carrying out national tests with their classes without being properly briefed, which could have consequences for both the students and the entire test system.</p>

corrected abstract:
<p>This work studied the information flow and information handling among different parties. The study focused on how educational materials on national tests from The Swedish National Agency for Education is managed by school principals and teachers and how the information is passed on to students. The data material was based on observations and interviews at two different high schools. Six teachers and their classes and principals participated in the study. The Agency was also interviewed. The results showed that opinions on the role of the national test varies between different teachers and the Agency. The study also showed that the dissemination of information in the schools is inadequate, information documents do not reach their target audience, and those who have access to the information do not read enough. This is partly due to lack of follow-up from the responsibles at the schools, but also teachers who do not seek out information that is available themselves. These shortcomings result in teachers carrying out national tests with their classes without being properly briefed, which could have consequences for both the students and the entire test system.</p>
----------------------------------------------------------------------
In diva2:408819 
abstract is: 
<p>With the upper secondary school reform in 1994, all students´ got a first</p><p>common course in matematic, course A, and a common national test in</p><p>the course. More than 40 percent of the students on the most vocational</p><p>programmes do not pass the national tests in the Mathematic A course</p><p>and 30 percent of the students are expected not to pass the course. One of</p><p>the biggest reasons the students leave upper secondary school without</p><p>completed grades, is the lack of motivation. There is a great need to</p><p>oversee what changes can be done in the theoretical subjects to increase</p><p>the students’ motivation and belief in self-efficiency concerning the</p><p>practical programmes.</p><p>This report investigates how the students’ motivation, trust in their own</p><p>ability as well as the understanding for the need of knowledge in</p><p>mathematics for future occupations is affected after one semester with</p><p>subject integrated studies. The investigation is performed on first grade</p><p>students in one of the construction company Peab’s upper secondary</p><p>schools and their mathematics teaching during the autumn term of 2009.</p><p>By starting from elements in the project “House construction”, where</p><p>mathematics is used naturally in the construction process, the students</p><p>can achieve the criterions in Mathematics A by practising on reality-based</p><p>tasks in the building area and in the classroom.</p><p>As qualitative research method, eight students before and after the</p><p>subject integrated project were chosen for interviews as well as a poll on</p><p>the first grade students of the mandatory construction project. The results</p><p>from the interviews indicate an increase for the majority of the students in</p><p>motivation for mathematics. The result from both the poll and the</p><p>interviews indicate an increase in self-efficiency beliefs for mathematics</p><p>after one semester with subject integrated studies for the majority of the</p><p>students. At the time for the second poll more students had realized how</p><p>often they actually use mathematics in the building area and will continue</p><p>to use it in future occupations.</p>

corrected abstract:
<p>With the upper secondary school reform in 1994, all students´ got a first common course in matematic, course A, and a common national test in the course. More than 40 percent of the students on the most vocational programmes do not pass the national tests in the Mathematic A course and 30 percent of the students are expected not to pass the course. One of the biggest reasons the students leave upper secondary school without completed grades, is the lack of motivation. There is a great need to oversee what changes can be done in the theoretical subjects to increase the students’ motivation and belief in self-efficiency concerning the practical programmes.</p><p>This report investigates how the students’ motivation, trust in their own ability as well as the understanding for the need of knowledge in mathematics for future occupations is affected after one semester with subject integrated studies. The investigation is performed on first grade students in one of the construction company Peab’s upper secondary schools and their mathematics teaching during the autumn term of 2009. By starting from elements in the project “House construction”, where mathematics is used naturally in the construction process, the students can achieve the criterions in Mathematics A by practising on reality-based tasks in the building area and in the classroom.</p><p>As qualitative research method, eight students before and after the subject integrated project were chosen for interviews as well as a poll on the first grade students of the mandatory construction project. The results from the interviews indicate an increase for the majority of the students in motivation for mathematics. The result from both the poll and the interviews indicate an increase in self-efficiency beliefs for mathematics after one semester with subject integrated studies for the majority of the students. At the time for the second poll more students had realized how often they actually use mathematics in the building area and will continue to use it in future occupations.</p>

Note spelling error:
w='matematic' val={'c': 'mathematic', 's': 'diva2:408819', 'n': 'error in original'}
w='criterions' val={'c': 'criteria', 's': ['diva2:508575', 'diva2:408819'], 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:515568 
abstract is: 
<p>E-assessment, activities in which digital techniques are used to assess students’ knowledge, in mathematics is an area that has not developed as fast as e-assessments in other areas. This is likely caused by the large elements of symbols and non-standard characters, which is characteristic of mathematical language and distinguishes it from ordinary written language. One problem that has arisen is that the input of mathematical expressions in a digital system in many cases has been slow and difficult to comprehend. This report aims to investigate three categories of input methods from three different aspects: speed, correctness and perceived ease in order to get an idea of which of the technologies are best suited to implement in an e-assessment application directed against the Swedish upper secondary school. To meet this objective three interrelated techniques where investigated by allowing upper secondary school students to use these techniques to input mathematical expressions and analyze these entries based on time, correctness and perceived ease.The results show that there are clear differences between the studied techniques for all examined aspects. Notably, the average time viewed over all techniques and expressions is 1 minute and 32 seconds which in relation to the activity these techniques are meant to compete with, writing mathematical expressions with pen and paper, is high. An additional notable finding is that in this survey a total of 127 expressions were processed and of those 49.6 % were not properly entered with this report's view of correctness. Although none of today's technologies are perfect, there are good elements to build on. These include ASCIIMathML's perceived ease and its ability to determine whether an expression is mathematically or not and Math Input Panels suggested alternative expressions, good correction ability and its similarity to traditional input. There are also questions of a more practical nature relating to things such as licenses and export of data which need to be addressed before deciding on a specific input technology for an application.</p>

corrected abstract:
<p>E-assessment, activities in which digital techniques are used to assess students’ knowledge, in mathematics is an area that has not developed as fast as e-assessments in other areas. This is likely caused by the large elements of symbols and non-standard characters, which is characteristic of mathematical language and distinguishes it from ordinary written language. One problem that has arisen is that the input of mathematical expressions in a digital system in many cases has been slow and difficult to comprehend. This report aims to investigate three categories of input methods from three different aspects: speed, correctness and perceived ease in order to get an idea of which of the technologies are best suited to implement in an e-assessment application directed against the Swedish upper secondary school. To meet this objective three interrelated techniques where investigated by allowing upper secondary school students to use these techniques to input mathematical expressions and analyze these entries based on time, correctness and perceived ease.</p><p>The results show that there are clear differences between the studied techniques for all examined aspects. Notably, the average time viewed over all techniques and expressions is 1 minute and 32 seconds which in relation to the activity these techniques are meant to compete with, writing mathematical expressions with pen and paper, is high. An additional notable finding is that in this survey a total of 127 expressions were processed and of those 49.6 % were not properly entered with this report's view of correctness. Although none of today's technologies are perfect, there are good elements to build on. These include ASCIIMathML's perceived ease and its ability to determine whether an expression is mathematically or not and Math Input Panels suggested alternative expressions, good correction ability and its similarity to traditional input. There are also questions of a more practical nature relating to things such as licenses and export of data which need to be addressed before deciding on a specific input technology for an application.</p>
----------------------------------------------------------------------
In diva2:550437 
abstract is: 
<p>The pantograph - catenary interaction is one of the most important features in high speed trains, and to guarantee a reliable current collection is the target that every railway system must take into consideration in order to speed up trains. The problem that goes against this direction is mainly the variation of the overhead equipment's stiness.</p><p>To understand the phenomenon a lumped mass model of the pantograph with a rigid body attached to the ground representing the contact wire were built up; in this way a complete lumped mass model is developed. All information regarding both wire and pantograph set up is introduced as lumped parameters. Creating the model, dierent active control strategies as ideal control, PID control and optimal control are introduced. All simulations are made in GENSYS, while the control part is made inSIMULINK; a connection between those two softwares was created as part of the thesis using TCP/IP protocol.</p><p>Results compared to experimental acquisition are satisfactory in terms of contact force representation. The standard deviation and average value's errors of the contact force are lower than 10%; regarding the control system, typically 20% of reduction of the standard deviation compared to the passive case is achieved.</p><p>Also a comparison with a nite element program is done in order to better understand the limits of the model compared with a more sophisticated one. The comparison shows a good accordance up to 60 % of the average speed of the wave propagation in the catenary. The last feature analyzed is how the behavior of the controlled system changes introducing a real actuator: results shows that the performance is reduced in dierent ways considering dierent speeds, but no instabilities occur.</p>

corrected abstract:
<p>The pantograph - catenary interaction is one of the most important features in high speed trains, and to guarantee a reliable current collection is the target that every railway system must take into consideration in order to speed up trains. The problem that goes against this direction is mainly the variation of the overhead equipment's stiffness.</p><p>To understand the phenomenon a lumped mass model of the pantograph with a rigid body attached to the ground representing the contact wire were built up; in this way a complete lumped mass model is developed. All information regarding both wire and pantograph set up is introduced as lumped parameters. Creating the model, different active control strategies as ideal control, PID control and optimal control are introduced. All simulations are made in <em>GENSYS</em>, while the control part is made in <em>SIMULINK</em>; a connection between those two softwares was created as part of the thesis using TCP/IP protocol.</p><p>Results compared to experimental acquisition are satisfactory in terms of contact force representation. The standard deviation and average value's errors of the contact force are lower than 10%; regarding the control system, typically 20% of reduction of the standard deviation compared to the passive case is achieved.</p><p>Also a comparison with a finite element program is done in order to better understand the limits of the model compared with a more sophisticated one. The comparison shows a good accordance up to 60 % of the average speed of the wave propagation in the catenary.</p><p>The last feature analyzed is how the behavior of the controlled system changes introducing a real actuator: results shows that the performance is reduced in different ways considering different speeds, but no instabilities occur.</p>
----------------------------------------------------------------------
In diva2:1803505 
abstract is: 
<p>The Gross-Neveu model is a quantum field theory of interacting N-flavor fermions in 1+1dimensions, with interaction term $(\bar{\psi}_f\psi_f )^2$. This model is studied using the property offactorized scattering. The spectrum of bound states including the kinks are discussed andthe thermodynamic state equations are derived using the thermodynamic Bethe ansatz.The full particle-particle integral kernel and corresponding S-matrix is derived startingfrom the Gross-Neveu version of the Y -system introduced by Zamolodchikov.</p>

corrected abstract:
<p>The Gross-Neveu model is a quantum field theory of interacting N-flavor fermions in 1+1 dimensions, with interaction term (<span style="text-decoration: overline;"><em>&psi;</em></span><sub>𝑓</sub><em>&psi;</em><sub>𝑓</sub>)<sup>2</sup>. This model is studied using the property of factorized scattering. The spectrum of bound states including the kinks are discussed and the thermodynamic state equations are derived using the thermodynamic Bethe ansatz. The full particle-particle integral kernel and corresponding 𝑆-matrix is derived starting from the Gross-Neveu version of the 𝑌-system introduced by Zamolodchikov.</p>
----------------------------------------------------------------------
In diva2:408826 
abstract is: 
<p>Vattenfall Nordic has identified a coming increase in demand on the recruitment of new employees the coming ten years due to the high rate of current employees going into retirement. As a step in improving the standard and function of the introductory process for each new employee, Vattenfall has suggested a need for standardization among their business units. This investigation was conducted to establish how such improvements could be achieved. This report consists of a theory-based part where relevant work from the field of pedagogical research is considered in the context of introducing employees in a new work environment. Following this a set of interviews were conducted with those in charge of induction procedure, and a larger e-mail survey was conducted with Vattenfall employees who recently went through the same introduction procedure.In conclusion, there is a notable opportunity to form common guidelines from Vattenfall Nordic's HR department, and also to improve and clearify the structure of the entire process. There is a need for an increased focus on the social relations of new employees, and structures for social support and guidance from more experienced co-workers as well as other newly recruited should be taken into consideration.Finally, to be able to uphold the quality and standard of the induction process, evaluation and feedback loops are essential. Follow-up evaluation should be made regularly not only to uphold dynamic improvement of the process but also to clearly emphasize to the employees that management makes the introduction to Vattenfall a high priority and that the opinion of new employees is a valued asset.</p>

corrected abstract:
<p>Vattenfall Nordic has identified a coming increase in demand on the recruitment of new employees the coming ten years due to the high rate of current employees going into retirement. As a step in improving the standard and function of the introductory process for each new employee, Vattenfall has suggested a need for standardization among their business units. This investigation was conducted to establish how such improvements could be achieved. This report consists of a theory-based part where relevant work from the field of pedagogical research is considered in the context of introducing employees in a new work environment. Following this a set of interviews were conducted with those in charge of induction procedure, and a larger e-mail survey was conducted with Vattenfall employees who recently went through the same introduction procedure.</p><p>In conclusion, there is a notable opportunity to form common guidelines from Vattenfall Nordic's HR department, and also to improve and clearify the structure of the entire process. There is a need for an increased focus on the social relations of new employees, and structures for social support and guidance from more experienced co-workers as well as other newly recruited should be taken into consideration.</p><p>Finally, to be able to uphold the quality and standard of the induction process, evaluation and feedback loops are essential. Follow-up evaluation should be made regularly not only to uphold dynamic improvement of the process but also to clearly emphasize to the employees that management makes the introduction to Vattenfall a high priority and that the opinion of new employees is a valued asset.</p>

Note spelling error:
w='clearify' val={'c': 'clarify', 's': ['diva2:408826', 'diva2:512408'], 'n': 'error in original - '}
----------------------------------------------------------------------
In diva2:753655 
abstract is: 
<p>We map real articulation data for five Swedish phonemen sequences onto parameters of a biomechanical tongue model that produces movement and muscle activations from position data. The movement produced is broadly correct and could likely be improved by a more fine tuned mapping. The muscle activations were found to vary between simulations using the same data but matched well with previous studies on muscle activity for vowels.</p>

corrected abstract:
<p>We map real articulation data for five Swedish phoneme sequences onto parameters of a biomechanical tongue model that produces movement and muscle activations from position data. The movement produced is broadly correct and could likely be improved by a more fine tuned mapping. The muscle activations were found to vary between simulations using the same data but matched well with previous studies on muscle activity for vowels.</p>


Note the only change was to correct the spelling in the DiVA entry:
w='phonemen' val={'c': 'phoneme', 's': 'diva2:753655', 'n': 'correct in original'}
----------------------------------------------------------------------
In diva2:1360268 
abstract is: 
<p>Steering of prepreg tows in an automated fibre placement (AFP) process allows geodesic layup trajectory over a doubly-curved surface, as well as the potential to improve the efficiency of composite structures by tailoring their stiffness. However, defects (such as out-of-plane buckling and tow pull off) are commonly generated when the steering radius exceeds a critical limit, which impact the mechanical properties of the finished product.</p><p>The in- and out-of-plane material properties of the prepreg tows have been shown to significantly influence the quality of the layup. In this thesis, in-plane shear behaviour of uncured IMA-M21 unidirectional (UD) prepreg was characterised using an off-axis tensile test to derive material parameters for process models to predict steering limits and defects.</p><p>Test parameters, such as shear strain rates and temperatures, that were investigated were consistent to the actual AFP process. The results demonstrates the expected strain-rate and temperature dependencies related to the resins viscoelastic behaviour. Subsequently, a novel micro-mechanical finite element (FE) simulation of a 3-dimensional UD prepreg unit cell in pure shear was conducted to gain qualitative insights into the complex rheological behaviour at play. It effectively demonstrates how fibre friction, resin viscosity and shear strain rates influence the load transfer between fibres and melt, as well as the movement of fibres during the shearing process. These were reflected in the shear stress-strain curves generated in the simulation and elastic micro-buckling observed in the fibre elements. The results pave a way for future development of a robust material model for predicting the critical process parameters to achieve quality layups from AFP steering process.</p>

corrected abstract:
<p>Steering of prepreg tows in an automated fibre placement (AFP) process allows geodesic layup trajectory over a doubly-curved surface, as well as the potential to improve the efficiency of composite structures by tailoring their stiffness. However, defects (such as out-of-plane buckling and tow pull off) are commonly generated when the steering radius exceeds a critical limit, which impact the mechanical properties of the finished product. The in- and out-of-plane material properties of the prepreg tows have been shown to significantly influence the quality of the layup. In this thesis, in-plane shear behaviour of uncured IMA-M21 unidirectional (UD) prepreg was characterised using an off-axis tensile test to derive material parameters for process models to predict steering limits and defects. Test parameters, such as shear strain rates and temperatures, that were investigated were consistent to the actual AFP process. The results demonstrates the expected strain-rate and temperature dependencies related to the resins viscoelastic behaviour. Subsequently, a novel micro-mechanical finite element (FE) simulation of a 3-dimensional UD prepreg unit cell in pure shear was conducted to gain qualitative insights into the complex rheological behaviour at play. It effectively demonstrates how fibre friction, resin viscosity and shear strain rates influence the load transfer between fibres and melt, as well as the movement of fibres during the shearing process. These were reflected in the shear stress-strain curves generated in the simulation and elastic micro-buckling observed in the fibre elements. The results pave a way for future development of a robust material model for predicting the critical process parameters to achieve quality layups from AFP steering process.</p>
----------------------------------------------------------------------
In diva2:1816799 
Note: no full text in DiVA

abstract is: 
<p>Aircraft taxying over a rough surface can be challenging and potentially dangerous, placing significant stresses on the aircraft’s structure and landing gears, increasing the risk of damage or failure. Accurate simulation models have become increasingly important for studying the complex physics involved in taxying over rough surfaces, particularly regarding loads and dynamics. A Stick Model (SM) is a popular simulation model that uses a beam model of the aircraft and more particularly of the wing to simulate its wing bending and torsion during ground operations. By capturing these important dynamics, the Stick Model provides valuable insights into the displacements, accelerations and loads placed on the aircraft. This paper aims to explore the potential of stick model to accurately capture the elastic and dynamic effects while taxying over rough runways.</p>

corrected abstract:
<p>Aircraft taxying over a rough surface can be challenging and potentially dangerous, placing significant stresses on the aircraft’s structure and landing gears, increasing the risk of damage or failure. Accurate simulation models have become increasingly important for studying the complex physics involved in taxying over rough surfaces, particularly regarding loads and dynamics. A Stick Model (SM) is a popular simulation model that uses a beam model of the aircraft and more particularly of the wing to simulate its wing bending and torsion during ground operations. By capturing these important dynamics, the Stick Model provides valuable insights into the displacements, accelerations and loads placed on the aircraft. This paper aims to explore the potential of stick model to accurately capture the elastic and dynamic effects while taxying over rough runways.</p>

Note regarding spelling:
The thesis uses "taxying", but it probably should be "taxiing".
----------------------------------------------------------------------
In diva2:839893 - looks like diva2:737929 is a duplicate
abstract is: 
<p>Wake interactions on a floating platform for offshore wind energy applications were investigated. The study is performed in collaboration with Hexicon AB which has a patent family for innovative floating platforms, which are able to turn automatically. The Jensen model is used for wake effect calculations and the simulations were performed in MATLAB. The present study starts with wind speed and wind direction data analysis for the specific site that Hexicon AB plans to construct its first platform. Data analysis is followed by wake interaction studies for H4-24MW type Hexicon AB platform. Wake interaction simulations were performed for three different cases. Fixed turbine and platform, Nacelle yawing and fixed platform and Nacelle yawing and turned platform. Different cases were investigated in order to see wake interactions for different wind directions. Wind direction effect on wake interactions were performed between _90_ and 90_ with an increment of 10_. After having the simulation results for Nacelle yawing and turned platform case the results were compared with ANSYS - CFX simulations results. The results didn’t match exactly but they were very close, which is an indicator to the validity of the Jensen Model. After finding out the possible behavior of wake interactions for different wind directions, power calculations were performed for the same three cases. In order to perform the power calculations the wake interactions for different wind directions were taken into account. In case of platform turning it was assumed that power losses were caused both by wake interactions and in case of thrusters activation. The losses that would be caused by different thrust forces on the turbine blades were not included. The last study was performed to suggest different layouts. In the second case, Nacelle yawing and fixed platform, it was found out that nacelle yawing for most of the angles is not possible because it creates wake regions in front of the rotor area. It was decided to propose new turbine configurations on the platform which are tolerant to different nacelle yawing angles. The simulations were run without considering any constructions limitations, meaning that the availability of platform structure was not included. The study is ended by performing some probabilistic results for platform turning behavior.</p><p></p>

corrected abstract:
<p>Wake interactions on a floating platform for offshore wind energy applications were investigated. The study is performed in collaboration with Hexicon AB which has a patent family for innovative floating platforms, which are able to turn automatically. The Jensen model is used for wake effect calculations and the simulations were performed in MATLAB. The present study starts with wind speed and wind direction data analysis for the specific site that Hexicon AB plans to construct its first platform. Data analysis is followed by wake interaction studies for H4-24MW type Hexicon AB platform. Wake interaction simulations were performed for three different cases. Fixed turbine and platform, Nacelle yawing and fixed platform and Nacelle yawing and turned platform. Different cases were investigated in order to see wake interactions for different wind directions. Wind direction effect on wake interactions were performed between -90° and 90° with an increment of 10°. After having the simulation results for Nacelle yawing and turned platform case the results were compared with ANSYS - CFX simulations results. The results didn’t match exactly but they were very close, which is an indicator to the validity of the Jensen Model.</p><p>After finding out the possible behavior of wake interactions for different wind directions, power calculations were performed for the same three cases. In order to perform the power calculations the wake interactions for different wind directions were taken into account. In case of platform turning it was assumed that power losses were caused both by wake interactions and in case of thrusters activation. The losses that would be caused by different thrust forces on the turbine blades were not included.</p><p>The last study was performed to suggest different layouts. In the second case, Nacelle yawing and fixed platform, it was found out that nacelle yawing for most of the angles is not possible because it creates wake regions in front of the rotor area. It was decided to propose new turbine configurations on the platform which are tolerant to different nacelle yawing angles. The simulations were run without considering any constructions limitations, meaning that the availability of platform structure was not included.</p><p>The study is ended by performing some probabilistic results for platform turning behavior.</p>
----------------------------------------------------------------------
In diva2:408868 
abstract is: 
<p>The feasibility of a simple and low-cost 3D time-of-flight imaging system isexperimentally investigated. Using a conventional CCD camera, a fast optoelectronic shutter and a modulated light, the distance of an object in a scene is measured. The measurements are computed using a homodyne approach.</p>
<p>The key component of the system is the opto-electronic shutter, for which amodulator developed by Acreo AB is used, and special attention was brought to characterize its properties. The principle has been successfully demonstrated. For a modulation frequency of 15 MHz and a modulator contrast of 3:1, the depth accuracy of the system is evaluated to 3.3 cm at worst and the repetability of the measurements to 3.6 cm on a range of 2 m. Indications to improve the system’s resolution are detailed for future work.</p>

corrected abstract:
<p>The feasibility of a simple and low-cost 3D time-of-flight imaging system is experimentally investigated. Using a conventional CCD camera, a fast optoelectronic shutter and a modulated light, the distance of an object in a scene is measured. The measurements are computed using a homodyne approach. The key component of the system is the opto-electronic shutter, for which a modulator developed by Acreo AB is used, and special attention was brought to characterize its properties.</p><p>The principle has been successfully demonstrated. For a modulation frequency of 15 MHz and a modulator contrast of 3:1, the depth accuracy of the system is evaluated to 3.3 cm at worst and the repetability of the measurements to 3.6 cm on a range of 2 m. Indications to improve the system’s resolution are detailed for future work.</p>

Note spelling error:
w='repetability' val={'c': 'repeatability', 's': 'diva2:408868', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:894067 
abstract is: 
<p>The background of this master thesis is the increased importance of improving vehicle fuel economy due to factors such as decreasing oil resources and growing fuel prices. Earlier performed tests have shown that real-world fuel economy is deviating signif-icantly from fuel economy (FE) measured in simulated road driving. In this thesis the accuracy of the fuel economy measurement methods used in such measurements are investigated. It is done by examining the performance of diﬀerent fuel economy measurement devices and by performing a test series with subsequently increased complexity. The test series consists both of chassis dynamometer and on road test-ing. All tests are performed with a Scania G450 long haulage truck which has been equipped with a portable fuel ﬂow meter and a portable emissions measurement sys-tem (PEMS). Variables such as temperatures, engine mode and torques taken up by diﬀerent auxiliary devices are analysed to improve the understanding about how the vehicle state is diﬀering between diﬀerent test drives. It is investigated if sensor fusion can be used to improve accuracy and repeatability in cases when multiple fuel consumption (FC) measurement devices are used. Obtained results show that the accuracy of the diﬀerent fuel economy measurement methods investigated has an order of magnitude of 1 % for real-world on-road testing. The results do also show that a change of engine frictional losses are inﬂuencing the fuel economy signiﬁcant in controlled environments. Finally it is concluded that the vehicle internal fuel economy estimation is reacting to changes in fuel economy in a similar way as the fuel ﬂow meter estimation. The method based on exhaust gas analysis is deviating from this behaviour.</p>

corrected abstract:
<p>The background of this master thesis is the increased importance of improving vehicle fuel economy due to factors such as decreasing oil resources and growing fuel prices. Earlier performed tests have shown that real-world fuel economy is deviating significantly from fuel economy (FE) measured in simulated road driving. In this thesis the accuracy of the fuel economy measurement methods used in such measurements are investigated. It is done by examining the performance of different fuel economy measurement devices and by performing a test series with subsequently increased complexity. The test series consists both of chassis dynamometer and on road testing. All tests are performed with a Scania G450 long haulage truck which has been equipped with a portable fuel flow meter and a portable emissions measurement system (PEMS). Variables such as temperatures, engine mode and torques taken up by different auxiliary devices are analysed to improve the understanding about how the vehicle state is differing between different test drives. It is investigated if sensor fusion can be used to improve accuracy and repeatability in cases when multiple fuel consumption (FC) measurement devices are used. Obtained results show that the accuracy of the different fuel economy measurement methods investigated has an order of magnitude of 1 % for real-world on-road testing. The results do also show that a change of engine frictional losses are influencing the fuel economy significant in controlled environments. Finally it is concluded that the vehicle internal fuel economy estimation is reacting to changes in fuel economy in a similar way as the fuel flow meter estimation. The method based on exhaust gas analysis is deviating from this behaviour.</p>
----------------------------------------------------------------------
In diva2:1183313 
abstract is: 
<p>Active anti-roll bars have recently found greater acceptance among premium car manufacturers and optimal application of this technology has emerged as an important field of research. This thesis investigates the potential of implementing active anti-roll bars in a passenger vehicle with the purpose of increasing customer value. For active anti-roll bars, customer value is defined in terms of vehicle’s ride comfort and handling performance. The objective with this thesis is to demonstrate this value through development of a control algorithm that can reflect the potential improvement in ride comfort and handling. A vehicle with passive anti-roll bars is simulated for different manoeuvres to identify the potential and establish a reference for the development of a control algorithm and for the performance of active anti-roll bars. While ride is evaluated using single-sided cosine wave and single-sided ramps, handling is evaluated using standardized constant radius, frequency response and sine with dwell manoeuvres.The control strategy developed implements a combination of sliding mode control, feed forward and PI-controllers. Simulations with active anti-roll bars showed significant improvement in ride and handling performance in comparison to passive anti-roll bars. In ride comfort, the biggest benefit was seen in the ability to increase roll damping and isolating low frequency road excitations. For handling, most significant benefits are through the system’s ability of changing the understeer behaviour of the vehicle and improving the handling stability in transient manoeuvres. Improvement in the roll reduction capability during steady state cornering is also substantial. In conclusion, active anti-roll bars are undoubtedly capable of improving both ride comfort and handling performance of a vehicle. Although the trade-off between ride and handling performance is significantly less, balance in requirements is critical to utilise the full potential of active anti-roll bars. With a more comprehensive control strategy, they also enable the vehicle to exhibit different driving characteristics without the need for changing any additional hardware.</p>

corrected abstract:
<p>Active anti-roll bars have recently found greater acceptance among premium car manufacturers and optimal application of this technology has emerged as an important field of research. This thesis investigates the potential of implementing active anti-roll bars in a passenger vehicle with the purpose of increasing customer value. For active anti-roll bars, customer value is defined in terms of vehicle’s ride comfort and handling performance. The objective with this thesis is to demonstrate this value through development of a control algorithm that can reflect the potential improvement in ride comfort and handling.</p><p>A vehicle with passive anti-roll bars is simulated for different manoeuvres to identify the potential and establish a reference for the development of a control algorithm and for the performance of active anti-roll bars. While ride is evaluated using single-sided cosine wave and single-sided ramps, handling is evaluated using standardized constant radius, frequency response and sine with dwell manoeuvres.</p><p>The control strategy developed implements a combination of sliding mode control, feed forward and PI-controllers. Simulations with active anti-roll bars showed significant improvement in ride and handling performance in comparison to passive anti-roll bars. In ride comfort, the biggest benefit was seen in the ability to increase roll damping and isolating low frequency road excitations. For handling, most significant benefits are through the system’s ability of changing the understeer behaviour of the vehicle and improving the handling stability in transient manoeuvres. Improvement in the roll reduction capability during steady state cornering is also substantial.</p><p>In conclusion, active anti-roll bars are undoubtedly capable of improving both ride comfort and handling performance of a vehicle. Although the trade-off between ride and handling performance is significantly less, balance in requirements is critical to utilise the full potential of active anti-roll bars. With a more comprehensive control strategy, they also enable the vehicle to exhibit different driving characteristics without the need for changing any additional hardware.</p>

Note - the only changes were to split the text into paragraphs as per the original
----------------------------------------------------------------------
In diva2:531096 - title missing ligature
"Investigation of Arti cial Bacterial Flagella Propulsion"
==>
"Investigation of Artificial Bacterial Flagella Propulsion"

abstract is: 
<p>This thesis covers an implementation of the Boundary Element Method (BEM) to simulate an arti cial swimmer at zero Reynolds number. The swimmer has a spheroidal head, and a singular, helically shaped tail, called a agella. By imposing a rotating magnetic eld, the swimmer also rotates. This rotary motion causes the swimmer to propel forward. The implementation is inspired by a real nano robot currently being esearched by a Swiss research group 1. We explain the physical concept behind the swimmer, the governing equations and simpli cations, and the theoretical concepts behind the solution method used.</p>

corrected abstract:
<p>This thesis covers an implementation of the <em>Boundary Element Method</em> (BEM) to simulate an artificial swimmer at zero Reynolds number. The swimmer has a spheroidal head, and a singular, helically shaped tail, called a flagella. By imposing a rotating magnetic field, the swimmer also rotates. This rotary motion causes the swimmer to propel forward. The implementation is inspired by a real nano robot currently being researched by a Swiss research group<sup><a href="#fn1" id="ref1">1</a></sup>. We explain the physical concept behind the swimmer, the governing equations and simplifications, and the theoretical concepts behind the solution method used.</p>
<div id="footnotes">
    <ol>
        <li id="fn1">Institute of Robotics and Intelligent Systems, ETH Zurich, CH-8092, Zurich Switzerland. E-mail: bnelson@ethz.ch <a href="#ref1" aria-label="Back to reference">↩</a></ĺi>
    </ol>
</div>
----------------------------------------------------------------------
In diva2:1040707 
abstract is: 
<p>The human society has throughout history been faced with great challenges. These challenges however hold the opportunity for us humans to learn and grow as a species in whole. As the human population is increasing, more attention has been focused to the medical eld to deal with the challenge of curing and treating people in larger scales at a faster rate. A particular challenge today is to meet the high demand in organ transplants. The number of human donors is scarce relative to the demand, and the transplantation is never guaranteed to be successful. Therefore allot of research is being conducted regarding the potential of 3D-bioprinting. 3D-bioprinting is an interesting eld with a lot of potential where the ultimate goal is to produce human organs for transplantation with the use of a 3D-printer. However, there are still many cases in which the cell viability in the bioprinting process is signicantly low. If the reason is biological or mechanical due to the strains in the ow through the bio- printer is sometimes unclear. Here presented is an investigation on the uid stresses present in the nozzle of the bioprinter. This is done by simulating the ow through the nozzle tip using CFD software and calculating the principal stresses on the cells in the post processing step. By using simple elastic deformation models the total area strain is calculated along the particle track of a cell to predict how the cells may deform throughout its particle track in the nozzle. It is found that the uid stresses present in the converging nozzle considered in this case are signicant, and cannot be excluded as a prime reason for the death of the cells in the bioprinting process. Due to the non-newtonian character of the bioink considered in this case, the cells close to the wall experience principal stresses signicantly higher than in the mainow. Generally the character of the stresses experienced by the cells along their particle tracks is observed to be highly exponential, thus it proposed for future work to investigate how much the maximum magnitude of the stresses at the outlet can be decreased by considering shorter nozzle tips.</p>

corrected abstract:
<p>The human society has throughout history been faced with great challenges. These challenges however hold the opportunity for us humans to learn and grow as a species in whole. As the human population is increasing, more attention has been focused to the medical field to deal with the challenge of curing and treating people in larger scales at a faster rate. A particular challenge today is to meet the high demand in organ transplants. The number of human donors is scarce relative to the demand, and the transplantation is never guaranteed to be successful. Therefore allot of research is being conducted regarding the potential of 3D-bioprinting. 3D-bioprinting is an interesting field with a lot of potential where the ultimate goal is to produce human organs for transplantation with the use of a 3D-printer. However, there are still many cases in which the cell viability in the bioprinting process is significantly low. If the reason is biological or mechanical due to the strains in the flow through the bioprinter is sometimes unclear. Here presented is an investigation on the fluid stresses present in the nozzle of the bioprinter. This is done by simulating the flow through the nozzle tip using CFD software and calculating the principal stresses on the cells in the post processing step. By using simple elastic deformation models the total area strain is calculated along the particle track of a cell to predict how the cells may deform throughout its particle track in the nozzle. It is found that the fluid stresses present in the converging nozzle considered in this case are significant, and cannot be excluded as a prime reason for the death of the cells in the bioprinting process. Due to the non-newtonian character of the bioink considered in this case, the cells close to the wall experience principal stresses significantly higher than in the mainflow. Generally the character of the stresses experienced by the cells along their particle tracks is observed to be highly exponential, thus it proposed for future work to investigate how much the maximum magnitude of the stresses at the outlet can be decreased by considering shorter nozzle tips.</p>
----------------------------------------------------------------------
In diva2:805309 
abstract is: 
<p>The purpose of this thesis was to investigate consolidation performed robotically of stacked epoxy pre-impregnated carbon ber. Consolidation is when the composite is compacted by applying pressure. This removes voids in the composite which increases mechanical properties. The process of consolidation is generally assumed to be viscoelastic. Two tests were made to investigate the viscoelastic properties of the uncured composite material: One relaxation test to investigate how temperature, stacking sequence, strain level and sample size a ects timedependent viscoelastic properties. The second experiment was to study the level of compaction achieved from local consolidation of the composite material, the latter obtained when using an industrial robot. The relaxation test showed that the compression response was decreased with higher temperatures and increased with higher strain levels. Samples with a reduced area showed larger relaxation e ects than unreduced samples. Unidirectionally stacked samples had larger relaxation e ects than samples stacked cross-directionally. The experiment with robotically applied consolidation did not show any measurable compaction at the pressure levels investigated. The lack of compaction is assumed to be caused by shearing of the upper layers due to high tool friction and that the applied pressures were not high enough to noticeably deform the composite material.</p>

corrected abstract:
<p>The purpose of this thesis was to investigate consolidation performed robotically of stacked epoxy pre-impregnated carbon fiber. Consolidation is when the composite is compacted by applying pressure. This removes voids in the composite which increases mechanical properties. The process of consolidation is generally assumed to be viscoelastic. Two tests were made to investigate the viscoelastic properties of the uncured composite material: One relaxation test to investigate how temperature, stacking sequence, strain level and sample size affects timedependent viscoelastic properties. The second experiment was to study the level of compaction achieved from local consolidation of the composite material, the latter obtained when using an industrial robot. The relaxation test showed that the compression response was decreased with higher temperatures and increased with higher strain levels. Samples with a reduced area showed larger relaxation effects than unreduced samples. Unidirectionally stacked samples had larger relaxation effects than samples stacked cross-directionally. The experiment with robotically applied consolidation did not show any measurable compaction at the pressure levels investigated. The lack of compaction is assumed to be caused by shearing of the upper layers due to high tool friction and that the applied pressures were not high enough to noticeably deform the composite material.</p>
----------------------------------------------------------------------
In diva2:1763236 
abstract is: 
<p>Gamma-ray bursts (GRBs) are the most energetic electromagnetic events in the universe, but there are still unanswered questions about them, like the underlying radiation mechanisms that cause the different parts of their light curves. Given that Wolf-Rayet (WR) stars with circumburst rings could be the progenitor of GRBs with late flares \cite{complex}, the purpose of this thesis was to determine if the precursor and the main emission of GRBs with late time flares might originate due to different radiation mechanisms, and thereby if WR stars could be the progenitors. 271 of the longest GRBs with flux above 10 photons/cm$^2$/s were studied and all GRBs with a precursor and a defined quiescent period were chosen for further studies. The chosen 39 GRBs were divided into different categories depending on the appearance of their light curves. A gold sample with $R_{P, max}/R_{D, max} &lt; 0.4$ and $T_Q/T_{tot} &gt; 0.5$, for the maximum count rate of the precursor $R_{P, max}$, the dominant emission $R_{D, max}$, and for the normalised quiescent period $T_Q/T_{tot}$ was concluded to have similar characteristics and to fit what would be expected if WR stars would be the progenitors. This group of GRBs all have a short and less bright precursor, a long quiescent period and a main emission which is brighter and longer than the precursor. The distributions of the photon index $\alpha$ for the precursor and the dominant emission for the gold sample indicate that the precursor is due to photospheric emission and the dominant emission is due to synchrotron emission. This is consistent with the interpretation that the precursor is a result of the jet interacting with the photosphere and the dominant emission is a result of interactions with the circumburst ring of a star like the WR stars. The next step in this investigation would be to study GRBs with more than one precursor that otherwise fit the description of the gold sample, to determine if these fit into the gold sample as well.</p>


corrected abstract:
<p>Gamma-ray bursts (GRBs) are the most energetic electromagnetic events in the universe, but there are still unanswered questions about them, like the underlying radiation mechanisms that cause the different parts of their light curves. Given that Wolf-Rayet (WR) stars with circumburst rings could be the progenitor of GRBs with late flares [1], the purpose of this thesis was to determine if the precursor and the main emission of GRBs with late time flares might originate due to different radiation mechanisms, and thereby if WR stars could be the progenitors. 271 of the longest GRBs with flux above 10 photons/cm<sup>2</sup>/s were studied and all GRBs with a precursor and a defined quiescent period were chosen for further studies. The chosen 39 GRBs were divided into different categories depending on the appearance of their light curves. A gold sample with 𝑅<sub>𝑃, 𝑚𝑎𝑥</sub>/R<sub>D, 𝑚𝑎𝑥</sub> &lt; 0.4 and 𝑇<sub>𝑄</sub>/𝑇<sub>𝑡𝑜𝑡</sub> &gt; 0.5, for the maximum count rate of the precursor 𝑅<sub>𝑃, 𝑚𝑎𝑥</sub>, the dominant emission 𝑅<sub>D, 𝑚𝑎𝑥</sub>, and for the normalised quiescent period 𝑇<sub>𝑄</sub>/𝑇<sub>𝑡𝑜𝑡</sub> was concluded to have similar characteristics and to fit what would be expected if WR stars would be the progenitors. This group of GRBs all have a short and less bright precursor, a long quiescent period and a main emission which is brighter and longer than the precursor. The distributions of the photon index <em>&alpha;</em> for the precursor and the dominant emission for the gold sample indicate that the precursor is due to photospheric emission and the dominant emission is due to synchrotron emission. This is consistent with the interpretation that the precursor is a result of the jet interacting with the photosphere and the dominant emission is a result of interactions with the circumburst ring of a star like the WR stars. The next step in this investigation would be to study GRBs with more than one precursor that otherwise fit the description of the gold sample, to determine if these fit into the gold sample as well.</p>
----------------------------------------------------------------------
In diva2:1780143 
abstract is: 
<p>We present Lax pairs and functions called Lax functions corresponding to Calogero- Moser-Sutherland (CMS) systems. We present the Benjamin-Ono (BO) equation and a pole ansatz to the BO equation, constructed from a specific type of Lax function called a special Lax function corresponding to Rational and Trigonometric CMS systems. We present a generalization of the BO equation called the non-chiral Intermediate wave (ncILW) equation and show that a family of solutions to the ncILW equation can be constructed from the special Lax function corresponding to the hyperbolic CMS system. We present the Szegö equation on the circle and the real line. We obtain a family of solutions to the Szegö equation on the real line using a pole ansatz. Using numerical methods, we display solution plots to the BO equation and Szegö equation.</p><p> </p>

corrected abstract:
<p>We present Lax pairs and functions called <em>Lax functions</em> corresponding to Calogero-Moser-Sutherland (CMS) systems. We present the Benjamin-Ono (BO) equation and a pole ansatz to the BO equation, constructed from a specific type of Lax function called a <em>special Lax function</em> corresponding to Rational and Trigonometric CMS systems. We present a generalization of the BO equation called the non-chiral Intermediate wave (ncILW) equation and show that a family of solutions to the ncILW equation can be constructed from the special Lax function corresponding to the hyperbolic CMS system. We present the Szegö equation on the circle 𝕊<sup>1</sup> and the real line ℝ. We obtain a family of solutions to the Szegö equation on ℝ using a pole ansatz. Using numerical methods, we display solution plots to the BO equation and Szegö equation.</p>
----------------------------------------------------------------------
In diva2:1768865 
abstract is: 
<p>The Kompaneets equation describes the spectrum formation when hot fermions are injected into an opaque fermion-photon plasma in thermal equilibrium and inverse Compton-scatterings occur. The equation has three free parameters, the initial plasma thermal energy θ_u, the final plasmathermal energy θ_r and the average photon energy gain y_r. In this study I use the parameters R = θ_r/θ_u, θ_r and y_r. The created spectrum has different properties, of which an important one is the mean photon energy ε_d. In this thesis, I aim to find the dependencies of the mean photon energy on the three Kompaneets parameters. I chart the parameter spaces and find correlation between the mean photon energy and the individual parameters. I describe the relations between the mean photon energy and the Kompaneets quantities empirically, constructing a function where I try to separate the variables as far as possible. For general results, I study a wide range of each parameter, which forces me to make a broken power-law description of the mean energy. I arrive at a function of the form ε_d(R, y_r, θ_r) = g(R, y_r) R^k(y_r) 3θ_r. I perform an error analysis and see that the majority of the errors of the new method is ≲ 2%, while the previous (tabulated value based approximation) method had the majority of the errors ≲ 70%. This means an effective improvement of the method by a factor 35. Then, I discuss the behaviour of the mean energy in the Kompaneets parameterspace. Finally, I outline a similar analysis of another property of the Kompaneets spectrum, the Compton temperature.</p>

corrected abstract:
<p>The Kompaneets equation describes the spectrum formation when hot fermions are injected into an opaque fermion-photon plasma in thermal equilibrium and inverse Compton-scatterings occur. The equation has three free parameters, the initial plasma thermal energy &#x1D703;<sub>𝑢</sub>, the final plasma thermal energy &#x1D703;<sub>𝑟</sub> and the average photon energy gain 𝑦<sub>𝑟</sub>. In this study I use the parameters 𝑅 = &#x1D703;<sub>𝑟</sub>/&#x1D703;<sub>𝑢</sub>, &#x1D703;<sub>𝑟</sub> and 𝑦<sub>𝑟</sub>. The created spectrum has different properties, of which an important one is the mean photon energy <span style="text-decoration: overline;">ε</span><sub>𝑑</sub>. In this thesis, I aim to find the dependencies of the mean photon energy on the three Kompaneets parameters. I chart the parameter spaces and find correlation between the mean photon energy and the individual parameters. I describe the relations between the mean photon energy and the Kompaneets quantities empirically, constructing a function where I try to separate the variables as far as possible. For general results, I study a wide range of each parameter, which forces me to make a broken power-law description of the mean energy. I arrive at a function of the form <span style="text-decoration: overline;">ε</span><sub>𝑑</sub>(𝑅, 𝑦<sub>𝑟</sub>, &#x1D703;<sub>𝑟</sub>) = 𝑔(𝑅, 𝑦<sub>𝑟</sub>) 𝑅<sup>𝑘(𝑦<sub>𝑟</sub>)</sup> 3&#x1D703;<sub>𝑟</sub>. I perform an error analysis and see that the majority of the errors of the new method is ≲ 2%, while the previous (tabulated value based approximation) method had the majority of the errors ≲ 70%. This means an effective improvement of the method by a factor 35. Then, I discuss the behaviour of the mean energy in the Kompaneets parameter space. Finally, I outline a similar analysis of another property of the Kompaneets spectrum, the Compton temperature.</p>
----------------------------------------------------------------------
In diva2:1577149 
abstract is: 
<p>This thesis report investigates the effect on cellcell junction proteins when the Na,K-ATPase (NKA) is inhibited. The main goal is to develop an understanding of how the NKA activity regulates the cell junction proteins. The investigated proteins are the adherens junction protein ECadherin, and the tight junction proteins Occludin and Claudin7.The NKA is inhibited by introducing the cardiotonic steroid Ouabain to the cells. The treatment is tested for different time lapse and different concentrations. The results show that all proteins are down regulated when treated with high concentrations (500 nM) of Ouabain. ECadherin is up regulated when treated with lower concentrations (10 nM) of Ouabain while Claudin7 is down regulated at low levels.</p>


corrected abstract:
<p>This thesis report investigates the effect on cell-cell junction proteins when the Na,K-ATPase (NKA) is inhibited. The main goal is to develop an understanding of how the NKA activity regulates the cell junction proteins. The investigated proteins are the adherens junction protein E-Cadherin, and the tight junction proteins Occludin and Claudin-7. The NKA is inhibited by introducing the cardiotonic steroid Ouabain to the cells. The treatment is tested for different time lapse and different concentrations. The results show that all proteins are down regulated when treated with high concentrations (500 nM) of Ouabain. E-Cadherin is up regulated when treated with lower concentrations (10 nM) of Ouabain while Claudin-7 is down regulated at low levels.</p>
----------------------------------------------------------------------
In diva2:1716282   - correct as is

Note spelling errors:
w='commecial' val={'c': 'commercial', 's': 'diva2:1716282', 'n': 'error in original'}
w='composities' val={'c': 'composites', 's': 'diva2:1716282', 'n': 'error in original'}
w='pheonomenon' val={'c': 'phenomenon', 's': 'diva2:1716282', 'n': 'error in original'}
w='surafce' val={'c': 'surface', 's': 'diva2:1716282', 'n': 'error in original'}
w='Camparison' val={'c': 'Comparison', 's': 'diva2:1716282', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1342233 
abstract is: 
<p>Deep neural networks (DNN) are used in a wide range of tasks such as voice recognition, image classification and spam-mail detection. However, it has been shown that the networks can misclassify an input when a small, carefully chosen perturbation is added to it. Many adversarial machine learning attacks have been proposed to create such samples, often with the goal of finding the smallest possible perturbation. In this study, three DNN image classification algorithms are constructed. Two adversarial methods (IFGM and DeepFool) are then analyzed and compared in terms of finding the smallest perturbation to cause misclassification of the networks, with the least computational effort. The two iterative, gradient-based methods are implemented in four distance metrics: L0, L1, L2 and Linfinity. The surprising result is that even though DeepFool uses a more sophisticated optimization strategy it does not perform significantly better than IFGM. Furthermore, IFGM actually finds a smaller perturbation with the same amount of time given. Also, in the targeted regime, IFGM performs better than DeepFool. Lastly, a fast L0-attack is suggested that strives to perturb as few pixels as possible to cause misclassification.</p>

corrected abstract:
<p>Deep neural networks (DNN) are used in a wide range of tasks such as voice recognition, image classification and spam-mail detection. However, it has been shown that the networks can misclassify an input when a small, carefully chosen perturbation is added to it. Many adversarial machine learning attacks have been proposed to create such samples, often with the goal of finding the smallest possible perturbation. In this study, three DNN image classification algorithms are constructed. Two adversarial methods (IFGM and DeepFool) are then analyzed and compared in terms of finding the smallest perturbation to cause misclassification of the networks, with the least computational effort. The two iterative, gradient-based methods are implemented in four distance metrics: L<sub>0</sub>, L<sub>1</sub>, L<sub>2</sub> and L<sub>&infin;</sub>. The surprising result is that even though DeepFool uses a more sophisticated optimization strategy it does not perform significantly better than IFGM. Furthermore, IFGM actually finds a smaller perturbation with the same amount of time given. Also, in the targeted regime, IFGM performs better than DeepFool. Lastly, a fast L<sub>0</sub>-attack is suggested that strives to perturb as few pixels as possible to cause misclassification.</p>
----------------------------------------------------------------------
In diva2:408833 
abstract is: 
<p>The aim of this thesis is to analyse and if possible improve the communication, mainly seen from a pedagogical perspective, between the IT company (Nostratic) and their clients in a first meeting. The analysis covers the preparation part, communication during the meetings, the technical terminology used during the meetings as well as the follow up of the meetings. Due to the lack of research within this area and the lack of personal experience on my part, the conducted study has been explorative with the use of qualitative methods. Although my first aim was to put emphisis on the pedagogical part of the communication, I found while conducting the study that there were other areas of higher importance to analyse in order to improve the customer meetings. Unfortunately the company went bankrupt in the middle of my study which resulted in me not beeing able to try out the potential improvements. My conclusion of the analysis is that the problem lies not only, as I first thought, in the lack of pedagogical skills of the company representatives but also, and even more apparent, were the company’s lack of organisation and routines concerning meetings.Key</p>


corrected abstract:
<p>The aim of this thesis is to analyse and if possible improve the communication, mainly seen from a pedagogical perspective, between the IT company (Nostratic) and their clients in a first meeting. The analysis covers the preparation part, communication during the meetings, the technical terminology used during the meetings as well as the follow up of the meetings. Due to the lack of research within this area and the lack of personal experience on my part, the conducted study has been explorative with the use of qualitative methods. Although my first aim was to put emphisis on the pedagogical part of the communication, I found while conducting the study that there were other areas of higher importance to analyse in order to improve the customer meetings. Unfortunately the company went bankrupt in the middle of my study which resulted in me not beeing able to try out the potential improvements. My conclusion of the analysis is that the problem lies not only, as I first thought, in the lack of pedagogical skills of the company representatives but also, and even more apparent, were the company’s lack of organisation and routines concerning meetings.</p>

Note - only change is to delete the word "Key" at the end of the paragraph, as this was part of "Key words".
Note spelling errors:
w='beeing' val={'c': 'being', 's': 'diva2:408833', 'n': 'error in original'}
w='emphisis' val={'c': 'emphasis', 's': 'diva2:408833', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1341350 
abstract is: 
<p>During the spring semester 2019 the students partaking in the aerospace bachelor thesis were tasked to design an electric airplane to study the plausibility of an electric future for the aviation industry. It comes as no surprise that commercial aviation is a big polluter and electric powertrains could help drive it towards a sustainable future. The airplane developed in this report is an electric airplane in the light business jet class. Due to the required development of completely new technologies and the fact that the aircraft would be manufactured in small quantities the initial price of the craft would be rather high. This ties in nicely with the target audience of politicians and executives as they tend to be more capable of purchasing these products. With a carrying capacity of 8 passengers, corresponding to a payload up to 960 kg (including luggage) and a maximum range of 1500 km, the airplane is perfect for flights to cities on the continent. For example a flight from Stockholm Arlanda to Berlin Tegel (830 km) or Arlanda to Wien Flughafen (1240 km). By examining historical data on planes in the same class the maximum take-off weight was estimated to 11000 kg. This value was later used to construct the constraint diagram. By considering the usage of the plane specific constraints were set. These constraints were a function of wing loading and by plotting them together in a diagram an optimal point could be found. This point was 250 kg/m^2 which meant a power rating of 1580 BHP and a wing area of 43.8 square meters. With the maximum take-off weight set an estimation of the fuselage length could be done by comparing it with historical data on similar planes. The fuselage length was estimated to be 16,1 m. The wing was placed on top of the fuselage in a high wing configuration. To complete the 1500 km journey the aircraft needs batteries with an energy density of 3045 Wh/kg. By examining historical data on the energy densities of lithium-ion battieries and extrapolating it to the year 2030, an estimated energy density of 380 Wh/kg is attained. This is considerably lower than what is required. The best batteries today (2019) have a specific energy of about 280 Wh/kg, meaning the capacity of batteries would have to increase more than ten times to reach the needed density level of 3045 Wh/kg for the aircraft to be able to perform adequately.</p>

corrected abstract:
<p>During the spring semester 2019 the students partaking in the aerospace bachelor thesis were tasked to design an electric airplane to study the plausibility of an electric future for the aviation industry. It comes as no surprise that commercial aviation is a big polluter and electric powertrains could help drive it towards a sustainable future. The airplane developed in this report is an electric airplane in the light business jet class. Due to the required development of completely new technologies and the fact that the aircraft would be manufactured in small quantities the initial price of the craft would be rather high. This ties in nicely with the target audience of politicians and executives as they tend to be more capable of purchasing these products.</p><p>With a carrying capacity of 8 passengers, corresponding to a payload up to 960 kg (including luggage) and a maximum range of 1500 km, the airplane is perfect for flights to cities on the continent. For example a flight from Stockholm Arlanda to Berlin Tegel (830 km) or Arlanda to Wien Flughafen (1240 km).</p><p>By examining historical data on planes in the same class the maximum take-off weight was estimated to 11000 kg. This value was later used to construct the constraint diagram. By considering the usage of the plane specific constraints were set. These constraints were a function of wing loading and by plotting them together in a diagram an optimal point could be found. This point was 250 kg/m<sup>2</sup> which meant a power rating of 1580 BHP and a wing area of 43.8 m<sup>2</sup>.</p><p>With the maximum take-off weight set an estimation of the fuselage length could be done by comparing it with historical data on similar planes. The fuselage length was estimated to be 16,1 m. The wing was placed on top of the fuselage in a high wing configuration.</p><p>To complete the 1500 km journey the aircraft needs batteries with an energy density of 3045 Wh/kg. By examining historical data on the energy densities of lithium-ion battieries and extrapolating it to the year 2030, an estimated energy density of 380 Wh/kg is attained. This is considerably lower than what is required. The best batteries today (2019) have a specific energy of about 280 Wh/kg, meaning the capacity of batteries would have to increase more than ten times to reach the needed density level of 3045 Wh/kg for the aircraft to be able to perform adequately.</p>

Note spelling error:
w='battieries' val={'c': 'batteries', 's': 'diva2:1341350', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1216718 
abstract is: 
<p>Is it possible for Saab's JAS 39 Gripen E to perform an airborne satellite launch? This question has been answered in this project based on a design-like equivalent fighter aircraft. The designed aircraft was developed at a conceptual level with the satellite launch mission as its starting point.</p><p>The purpose of the project was to conceptually design an aircraft capable of performing an airborne satellite launch. The delimitations that were made were, among other things, that the designed aircraft would imitate JAS 39 Gripen E's abilities as much as possible. Another demarcation that was made was the geographical starting point of Saab's production in Linköping, Sweden. From the starting point in Linköping, a range of 800 km one way was set to the Norwegian Sea. There the aircraft was assumed to ascend to 16 km and then release the rocket. The rocketmass was calculated to 1530 kg including the 30 kg microsatellite that aimed to be launched to a low orbit around Earth. After performing the mission of launching the rocket, the aircraft will return to the starting point without refueling.</p><p>The design of the aircraft was initiated in specific requirements for the aircraft based on data about JAS 39 Gripen E, taken from a data sheet published by Saab (2016). Some values that where not presented in the data sheet were assumed based on aircraft design literature. The requirements together with a mission profile led to a weight estimation of the aircraft's total weight, empty weight and fuel weight. From the weights and weight ratios, a constraint diagram was created based on the performance parameters; maximum take-off distance, rate-of-climb, cruise speed, constant velocity turn and maximum speed. This resulted in an optimal design point that gave a thurst-to-weight ratio as well as the wing loading. The conditions set a requirement on the engine and gave an estimated value of the wing area, which resulted in wing and tail design, wing geometry and center of gravity.</p><p>The result of the designed aircraft shows that the requirements and values calculated is similar both in appearance and value specifications of the JAS 39 Gripen E. The conclusion is therefore, based on the mission profile, that it is possible for the JAS 39 Gripen E to perform an airborne satellite launch. Further work with the project may be to refine the weight estimates and specify the mission profile even more, for example with specific speeds. Further development of the project may be to design a converter or installation device to the fighter to be able attach the rocket and perform the mission in real.</p>

corrected abstract:
<p>Is it possible for Saab's JAS 39 Gripen E to perform an airborne satellite launch? This question has been answered in this project based on a design-like equivalent fighter aircraft. The designed aircraft was developed at a conceptual level with the satellite launch mission as its starting point.</p><p>The purpose of the project was to conceptually design an aircraft capable of performing an airborne satellite launch. The delimitations that were made were, among other things, that the designed aircraft would imitate JAS 39 Gripen E's abilities as much as possible. Another demarcation that was made was the geographical starting point of Saab's production in Linköping, Sweden. From the starting point in Linköping, a range of 800 km one way was set to the Norwegian Sea. There the aircraft was assumed to climb to its full height of 16 km and then release the rocket. The rocket was calculated to weigh 1530 kg including the 30 kg microsatellite that aimed to be launched to a low orbit around Earth. After performing the mission of launching the rocket, the aircraft was dimensioned to return to the starting point without refueling.</p><p>The design of the aircraft was initiated in specific requirements for the aircraft based on data about JAS 39 Gripen E, taken from a data sheet published by Saab (2016). Some values that where not presented in the data sheet were assumed based on aircraft design literature. The requirements together with a mission profile led to a weight estimation of the aircraft's total weight, empty weight and fuel weight. From the weights and weight ratios, a constraint diagram was created based on the performance parameters; maximum take-off distance, rate-of-climb, cruise speed, constant velocity turn and maximum speed. This resulted in an optimal design point that gave a thurst-to-weight ratio as well as the wing loading. The conditions set a requirement on the engine and gave an estimated value of the wing area, which resulted in wing and tail design, wing geometry and center of gravity.</p><p>The result of the designed aircraft shows that the requirements and values calculated is similar both in appearance and value specifications of the JAS 39 Gripen E. The conclusion is therefore, based on the mission profile, that it is possible for the JAS 39 Gripen E to perform an airborne satellite launch. Further work with the project may be to refine the weight estimates and specify the mission profile even more, for example with specific speeds. Further development of the project may be to design a converter or installation device to the fighter to be able attach the rocket and perform the mission in real.</p>

Note - there are a number of places where the DiVA wording was different than in the thesis.
Note spelling error:
w='thurst-to-weight' val={'c': 'thrust-to-weight', 's': 'diva2:1216718', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1109465 
abstract is: 
<p>In many situations, fluid flows can exhibit a wide range of temporal and spatial phenomena. It has become common to extract physically important features, called modes, as a first step in the analysis of flows with high complexity. One of the most prominent modal analysis techniques in the context of fluid dynamics is Proper Orthogonal Decomposition (POD), which enables extraction of energetically coherent structures present in the flow field. This method does, however, suffer from the lack of connection with the mathematical theory of dynamical systems and its utility in the analysis of arbitrarily complex flows might therefore be limited. In the present work, we instead consider application of the Koopman Mode Decomposition (KMD), which is an approach based on spectral decomposition of the Koopman operator. This technique is employed for modal analysis of the incompressible, two-dimensional ow past two side-by-side cylinders at Re = 60 and with a non-dimensional cylinder gap spacing g* = 1. This particular configuration yields a wake ow which exhibits in-phase vortex shedding during finite time, while later transforming into the so-called flip-flopping phenomena, which is characterised by a slow, periodic switching of the gap ow direction during O(10) vortex shedding cycles. The KMD approach yields modal structures which, in contrary to POD, are associated with specific oscillation frequencies. Specifically, these structures are here vorticity modes. By studying these modes, we are able to extract the ow components which are responsible for the flip-flop phenomenon. In particular, it is found that the flip-flop instability is mainly driven by three different modal structures, oscillating with Strouhal frequencies St1 = 0:023, St2 = 0:121 and St3 = 0:144, where it is noted that St3 = St1 + St2. In addition, we study the in-phase vortex shedding regime, as well as the transient regime connecting the two states of the flow. The study of the in-phase vortex shedding reveals| - not surprisingly - the presence of a single fundamental frequency, while the study of the transient reveals a Koopman spectrum which might indicate the existence of a bifurcation in the phase space of the flow field; this idea has been proposed before in Carini et al. (2015b). We conclude that the KMD offers a powerful framework for analysis of this ow case, and its range of applications might soon include even more complex flows.</p>

corrected abstract:
<p>In many situations, fluid flows can exhibit a wide range of temporal and spatial phenomena. It has become common to extract physically important features, called <em>modes</em>, as a first step in the analysis of flows with high complexity. One of the most prominent modal analysis techniques in the context of fluid dynamics is <em>Proper Orthogonal Decomposition</em> (POD), which enables extraction of energetically coherent structures present in the flow field. This method does, however, suffer from the lack of connection with the mathematical theory of dynamical systems and its utility in the analysis of arbitrarily complex flows might therefore be limited. In the present work, we instead consider application of the <em>Koopman Mode Decomposition</em> (KMD), which is an approach based on spectral decomposition of the <em>Koopman operator</em>. This technique is employed for modal analysis of the incompressible, two-dimensional flow past two side-by-side cylinders at 𝑅𝑒 = 60 and with a non-dimensional cylinder gap spacing 𝑔<sup>*</sup> = 1. This particular configuration yields a wake flow which exhibits in-phase vortex shedding during finite time, while later transforming into the so-called <em>flip-flopping</em> phenomena, which is characterised by a slow, periodic switching of the gap flow direction during 𝒪(10) vortex shedding cycles. The KMD approach yields modal structures which, in contrary to POD, are associated with specific oscillation frequencies. Specifically, these structures are here vorticity modes. By studying these modes, we are able to extract the flow components which are responsible for the flip-flop phenomenon. In particular, it is found that the flip-flop instability is mainly driven by three different modal structures, oscillating with Strouhal frequencies 𝑆𝑡<sub>1</sub> = 0.023, 𝑆𝑡<sub>2</sub> = 0.121 and 𝑆𝑡<sub>3</sub> = 0.144, where it is noted that 𝑆𝑡<sub>3</sub> = 𝑆𝑡<sub>1</sub> + 𝑆𝑡<sub>2</sub>. In addition, we study the in-phase vortex shedding regime, as well as the transient regime connecting the two states of the flow. The study of the in-phase vortex shedding reveals &mdash; not surprisingly &mdash; the presence of a single fundamental frequency, while the study of the transient reveals a Koopman spectrum which might indicate the existence of a bifurcation in the phase space of the flow field; this idea has been proposed before in Carini <em>et al.</em> (2015b). We conclude that the KMD offers a powerful framework for analysis of this flow case, and its range of applications might soon include even more complex flows.</p>
----------------------------------------------------------------------
In diva2:1440770 
abstract is: 
<p>When studying to become an engineer the education is mainly theoretical and to confirm the theory laboratory exercises are incorporated in the curriculum. Being able to visualize the phenomenons studied helps in giving the students a deeper understanding. The purpose of this report is to aid the department of fluid mechanics at KTH in designing an experiment to help the students understand the theory of oblique shock waves. The project is divided in two parts. Part one the design of the wedge and base plate and part two the creation of the actual experiment and lab PM. The design of the wedge is quite simple. It is a sharp wedge with two angles measured from the horizontal plane, 8° at the top and 4° at the bottom, this is so that two measurements can be made per experiment. Writing the PM and the design of the exercise is the main part of this project. The result is a laboratory exercise where the students compare the flow around a wedge in a shock tube for three different Mach numbers, subsonic, transonic and supersonic. Using shadowgraph optics and a high speed camera the image of the shock is captured. The different regimes are then discussed and compared with the help of theoretical calculations and the measured values.</p><p> </p>

corrected abstract:
<p>When studying to become an engineer the education is mainly theoretical and to confirm the theory laboratory exercises are incorporated in the curriculum. Being able to visualize the phenomenons studied helps in giving the students a deeper understanding. The purpose of this report is to aid the department of fluid mechanics at KTH in designing an experiment to help the students understand the theory of oblique shock waves.</p><p>The project is divided in two parts. Part one the design of the wedge and base plate and part two the creation of the actual experiment and lab PM. The design of the wedge is quite simple. It is a sharp wedge with two angles measured from the horizontal plane, 8° at the top and 4° at the bottom, this is so that two measurements can be made per experiment.</p><p>Writing the PM and the design of the exercise is the main part of this project. The result is a laboratory exercise where the students compare the flow around a wedge in a shock tube for three different Mach numbers, subsonic, transonic and supersonic. Using shadowgraph optics and a high speed camera the image of the shock is captured. The different regimes are then discussed and compared with the help of theoretical calculations and the measured values.</p>

Note spelling error:
w='phenomenons' val={'c': 'phenomena', 's': ['diva2:1879381', 'diva2:1040684', 'diva2:1440770', 'diva2:839851']}
----------------------------------------------------------------------
In diva2:1896432 
abstract is: 
<p>In the spectrum of carbone dioxyde (CO2) reduction, new configurations of turbofans are designed. The openrotor is expected to achieve a reduction of 20% of emissions compared to current configurations. Nonetheless, it presents many challenges such as the influence of the boundary layer transition on the performances. This study aims to propose a methodology to analyse the boundary layer transition on an openrotor with Large Eddy Simulation (LES). A monogrid study is proposed. Some methodological challenges are covered, related to the mesh generation to avoid numerical instabilities. Limitations for the use of the 2.5D hypothesis are shown, leading to an axisymmetric simulation of a stator between two streamsurfaces. The results of this configuration are discussed in light of boundary layer transition.</p><p> </p>

corrected abstract:
<p>In the spectrum of carbone dioxyde (𝐶𝑂<sub>2</sub>) reduction, new configurations of turbofans are designed. The openrotor is expected to achieve a reduction of 20% of emissions compared to current configurations. Nonetheless, it presents many challenges such as the influence of the boundary layer transition on the performances. This study aims to propose a methodology to analyse the boundary layer transition on an openrotor with Large Eddy Simulation (LES). A monogrid study, performed on the stator, is proposed using AVBP, an LES explicit solver developed by the CERFACS. Some methodological challenges are covered, related to the mesh generation to avoid numerical instabilities. Limitations for the use of the 2.5D hypothesis are shown, leading to an axisymmetric simulation of a stator between two streamsurfaces. The results of this configuration are discussed in light of boundary layer transition.  If the 2.5D hypothesis cannot be used in this case, the axisymmetric formalism shows interesting insights and a more robust behaviour. The axisymmetric methodology enables to accurately reproduce a sectorial RANS calculation but still requires a calibration in both the outlet static pressure and the inlet angle of attack.</p>


Note substantial difference in woring between DiVA abstract and original.
Note spelling errors:
w='carbone' val={'c': 'carbon', 's': 'diva2:1896432', 'n': 'error in original'}
w='dioxyde' val={'c': 'dioxide', 's': 'diva2:1896432', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:891555 
abstract is: 
<p>This is a master thesis, done as a part of the ongoing research projects ”Vattenvägen 365” and ”LightIce - Light weight marine vessels operating in brash Ice”. This work is a study of the design of a high speed craft, intended for use in the public transportation network in the Stockholm area. Emphasis is put on investigating the consequences of the ice conditions in the area during winter. The work is primarily done as a case study, based on the design of a high speed passenger vessel, intended to navigate in ice. The ice conditions considered are light ice conditions, like brash ice and thin ice sheets. In connection with the high speed craft design, work is done on the eﬀect of the choice of material concept. The scope of the work is as follows. To look into the existing literature on the topics of ice loads on high speed crafts and the use of lightweight materials in the construction of ice-going vessels. Create an initial design of a high speed craft suitable for passenger transportation in the Stockholm area. Gain insight into the ice loading on a high speed craft, and looking into the use of composite and lightweight materials.A literature review on literature related to ice loading of high speed crafts is done. A vessel is brought into the initial design stage, basing the design on a operational proﬁle developed for a suggested route for commuter transportation in the Stockholm area. This high speed craft is used as a basis for comparison of diﬀerent methods for estimation of ice loading. Testing is planned to investigate the ice loading of high speed crafts, based on the operational proﬁle and vessel design. In connection with testing, a loading scenario is created by estimating the expected maximum ice thickness during a winter in the intended area of operation, an impact scenario is deﬁned and work is done with regards to estimation of ice ﬂoe size and modelling of the impact between the vessel under normal operational conditions and an ice ﬂoe. For the testing, full scale hull panels are designed in four diﬀerent materials, both metal and composite. Testing is planned using the deﬁned loading and the designed hull panels.The thesis serves as a basis for conducting further research into the topic of ice loading of high speed crafts.</p>

corrected abstract:
<p>This is a master thesis, done as a part of the ongoing research projects ”Vattenvägen 365” and ”LightIce - Light weight marine vessels operating in brash Ice”. This work is a study of the design of a high speed craft, intended for use in the public transportation network in the Stockholm area. Emphasis is put on investigating the consequences of the ice conditions in the area during winter. The work is primarily done as a case study, based on the design of a high speed passenger vessel, intended to navigate in ice. The ice conditions considered are light ice conditions, like brash ice and thin ice sheets. In connection with the high speed craft design, work is done on the effect of the choice of material concept. The scope of the work is as follows. To look into the existing literature on the topics of ice loads on high speed crafts and the use of lightweight materials in the construction of ice-going vessels. Create an initial design of a high speed craft suitable for passenger transportation in the Stockholm area. Gain insight into the ice loading on a high speed craft, and looking into the use of composite and lightweight materials.</p><p>A literature review on literature related to ice loading of high speed crafts is done. A vessel is brought into the initial design stage, basing the design on a operational profile developed for a suggested route for commuter transportation in the Stockholm area. This high speed craft is used as a basis for comparison of different methods for estimation of ice loading. Testing is planned to investigate the ice loading of high speed crafts, based on the operational profile and vessel design. In connection with testing, a loading scenario is created by estimating the expected maximum ice thickness during a winter in the intended area of operation, an impact scenario is defined and work is done with regards to estimation of ice floe size and modelling of the impact between the vessel under normal operational conditions and an ice floe. For the testing, full scale hull panels are designed in four different materials, both metal and composite. Testing is planned using the defined loading and the designed hull panels.</p><p>The thesis serves as a basis for conducting further research into the topic of ice loading of high speed crafts.</p>
----------------------------------------------------------------------
In diva2:1471348 
abstract is: 
<p>Lozenges are polygons constructed by gluing two equilateral triangles along an edge. We can fit lozenge pieces together to form larger polygons and given an appropriate polygon we can tile it with lozenges. Lozenge tilings of the semi-regular hexagon with sides A,B,C can be viewed as the 2D picture of a stack of cubes in a A x B x C box. In this project we investigate the typical shape of a tiling as the sides A,B,C of the box grow uniformly to infinity and we consider two cases: The uniform case where all tilings occur with equal probability and the q^Volume case where the probability of a tiling is proportional to the volume taken up by the corresponding stack of cubes. To investigate lozenge tilings we transform it into a question on families of non-intersecting paths on a corresponding graph representing the hexagon. Using the Lindström–Gessel–Viennot theorem we can define the probability of a non-intersecting path crossing a particular point in the hexagon both for the uniform and the $q$-Volume case. In each case this probability function is connected to either the Hahn or the $q$-Hahn orthogonal polynomials. The orthogonal polynomials depend on the sides of the hexagon and so we consider the asymptotic behaviour of the polynomials as the sides grow to infinity using a result due to Kuijlaars and Van Assche. This determines the density of non-intersecting paths through every point in the hexagon, which we calculate, and a ``Arctic curve" result which shows that the six corners of the hexagon are (with probability one) tiled with just one type of lozenge.</p>

corrected abstract:
<p>Lozenges are polygons constructed by gluing two equilateral triangles along an edge. We can fit lozenge pieces together to form larger polygons and given an appropriate polygon we can tile it with lozenges. Lozenge tilings of the semi-regular hexagon with sides 𝐴, 𝐵, 𝐶 can be viewed as the 2D picture of a stack of cubes in a 𝐴 × 𝐵 × 𝐶 box.</p><p>In this project we investigate the typical tiling of the hexagon as the sides 𝐴, 𝐵, 𝐶 of the box all grow according to some 𝑁 → ∞. We consider two cases: In the uniform case all tilings occur with equal probability. This is a special case of the general 𝑞<sup>Volume</sup>-tiling where the probability is proportional to the volume taken up by a corresponding stack of cubes. We transform the problem into a question on families of non-intersecting paths and define a probability function on the paths for the 𝑞-Volume case. This probability function can be expressed in terms of the 𝑞-Hahn orthogonal polynomials.</p><p>We then study the behaviour of non-intersecting paths as the sides of the hexagon grow to infinity by analysing the asymptotic behaviour of the corresponding polynomials. We determine the probability density for where these non-intersecting paths cross. Furthermore we characterise the “Arctic curve" in the hexagon. Outside of this curve the probability density is constant. This result shows that the six corners of the hexagon are (with probability one) tiled with just one type of lozenge.</p>
----------------------------------------------------------------------
In diva2:1849658 
abstract is: 
<p>In this thesis, a method to calculate eigenpairs is implemented for the Multipsi library. While the standard implemtentations use the Davidson method with Rayleigh-Ritz extraction to calculate the eigenpairs with the lowest eigenvalues, the new method uses the harmonic Davidson method with the harmonic Rayleigh-Ritz extraction to calculate eigenpairs with eigenvalues near a chosen target. This is done for Configuration Interaction calculations and for Multiconfigurational methods. From calculations, it seems the new addition to the Multipsi library is worth investigating further as convergence for difficult systems with a lot of near-degeneracy was improved.</p>

corrected abstract:
<p>In this thesis, a method to calculate eigenpairs is implemented for the Multipsi library. While the standard implementations use the Davidson method with Rayleigh-Ritz extraction to calculate the eigenpairs with the lowest eigenvalues, the new method uses the harmonic Davidson method with the harmonic Rayleigh-Ritz extraction to calculate eigenpairs with eigenvalues near a chosen target. This is done for Configuration Interaction calculations and for Multiconfigurational methods. From calculations, it seems the new addition to the Multipsi library is worth investigating further as convergence for difficult systems with a lot of near-degeneracy was improved.</p>
----------------------------------------------------------------------
In diva2:1253239 
abstract is: 
<p>The Swedish armed forces, Försvarsmakten (FM), is the governing body in charge of defending the sovereignty of Sweden. In FM, a large part of the personnel is exposed to high levels of noise as part of their work with machines and weaponry. The Swedish authority on workplace related hazards, Arbetsmiljöverket, has set out limitations for noise exposure during work in the publication: AFS 2005:16. To reduce the total noise exposure of the employees, FM have implemented multiple changes in their work. These changes include the use of hearing protection devices (HPD) but also include changes in routine and equipment.</p><p>Multiple versions of the hearing protectors include radio communications to facilitate communication while maintaining a high level of noise reduction. The methods set out in AFS 2005:16 to measure noise exposure are not applicable when sound is generated inside the ear canal. Therefore, the total noise exposure cannot be determined accurately with regards to the set limitations. The thesis aims to determine a method to measure the total noise exposure in representative conditions and how the use of this method along with other objective measures of sound to set system settings.</p><p>The method of choice is to use a HATS or MIRE measurement in accordance with ISO11904- 1:2002 or ISO11904-2:2004 to properly capture the influence of the operator and calculate the contribution from the communication systems. The contribution from external sources need to be evaluated separately as the attenuation of hearing protection devices may not be properly measured when using MIRE or HATS. When such contribution is calculated, insertion loss (IL) determined using the REAT method should be used.</p><p>The exposure using three weapon systems was measured using a HATS at representative head position and a selection of HPD. The resulting IL was compared to other methods of measuring IL and total exposure using these results. The IL in in-situ measurements was influenced by the presence of a disturbance in the HATS and consequently overestimated exposure. IL was also measured in a reverberation chamber and an echo-free chamber using a HATS. The resulting IL seems to differ from REAT measurements and did in some cases severely overestimate the IL.</p><p>The thesis also simulated the influence of three idealized types of noise on the Speech Transmission Index (STI), an objective measure of speech intelligibility. To measure STI using HATS or MIRE measurements, a transfer function like the one from ISO11904-2:2004 needs to be applied if noise or HPD:s will be used. Previous work recommends a speech-to-noise ratio of at least 12dB to achieve good speech intelligibility. The rule of thumb seems to have some advantages when using a simulated Matrix Transfer Function but does overestimate the required level for low noise levels. For higher levels, excellent STI (0,75) does not seem to be achievable. The required signal-to- noise is dependent on both the spectral content of the noise as well as the level, which both is influenced by the use of hearing protection devices. To select a proper signal to noise level, a representative recording of speech using relevant equipment is required. The influence of noise can be added separately using the methods in IEC60268-16 and REAT measured attenuation.</p>


corrected abstract:
<p>The Swedish armed forces, Försvarsmakten (FM), is the governing body in charge of defending the sovereignty of Sweden. In FM, a large part of the personnel is exposed to high levels of noise as part of their work with machines and weaponry. The Swedish authority on workplace related hazards, Arbetsmiljöverket, has set out limitations for noise exposure during work in the publication: AFS 2005:16. To reduce the total noise exposure of the employees, FM have implemented multiple changes in their work. These changes include the use of hearing protection devices but also include changes in routine and equipment.</p><p>Multiple versions of the hearing protectors include radio communications to facilitate communication while maintaining a high level of noise reduction. The methods set out in AFS 2005:16 to measure noise exposure are not applicable when sound is generated inside the ear canal. Therefore, the total noise exposure cannot be determined accurately with regards to the set limitations. The thesis aims to determine a method to measure the total noise exposure in representative conditions and how the use of this method along with other objective measures of sound to set system settings.</p><p>The method of choice is to use a HATS or MIRE measurement in accordance with ISO11904- 1:2002 or ISO11904-2:2004 to properly capture the influence of the operator and calculate the contribution from the communication systems. The contribution from external sources need to be evaluated separately as the attenuation of hearing protection devices may not be properly measured when using MIRE or HATS. When such contribution is calculated, insertion loss (IL) determined using the REAT method should be used.</p><p>The exposure using three weapon systems was measured using a HATS at representative head position and a selection of HPD. The resulting IL was compared to other methods of measuring IL and total exposure using these results. The IL in in-situ measurements was influenced by the presence of a disturbance in the HATS and consequently overestimated exposure. IL was also measured in a reverberation chamber and an echo-free chamber using a HATS. The resulting IL seems to differ from REAT measurements and did in some cases severely overestimate the IL.</p><p>The thesis also simulated the influence of three idealized types of noise on the Speech Transmission Index (STI), an objective measure of speech intelligibility. To measure STI using HATS or MIRE measurements, a transfer function like the one from ISO11904-2:2004 needs to be applied if noise or HPD:s will be used. Previous work recommends a speech-to-noise ratio of at least 12dB to achieve good speech intelligibility. The rule of thumb seems to have some advantages when using a simulated Matrix Transfer Function but does overestimate the required level for low noise levels. For higher levels, excellent STI (0,75) does not seem to be achievable. The required signal-to-noise is dependent on both the spectral content of the noise as well as the level, which both is influenced by the use of hearing protection devices. To select a proper signal to noise level, a representative recording of speech using relevant equipment is required. The influence of noise can be added separately using the methods in IEC60268-16 and REAT measured attenuation.</p>

Note the "(HPD)" in the DiVA abstract is not in the thesis.
Note spelling error:
w='HPD:s' val={'c': 'HPDs', 's': 'diva2:1253239', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1335459 
abstract is: 
<p>The bachelor thesis report in the field of sound and vibrations aims to see the behaviour of sound reduction within low frequencies and see the tipping point for breakout noise, leakage of sound through the material. The focus is on whether or not there is saturation in noise reduction, coming from theory, of 6 dB per meter of silencer below 250 Hz. The silencer used in this project is the CLA-A-250 from Swegon. Measurements are made in the MWL laboratory at KTH. The project arose from the company LN Akustikmiljö as they experienced different behaviour during projects of their own. As can be seen in results, the saturation of 6 dB per meter of silencer does not apply and is not a correct assumption. Regarding the breakout noise, it is clear that as long as the duct structure and its outlet are in separate rooms it is not a problem. This conclusion is safe to make according to standards and the margins considered in the calculations.The project served as great insight in real life problems and applications of an acoustician.</p>

corrected abstract:
<p>The bachelor thesis report in the field of sound and vibrations aims to see the behaviour of sound reduction within low frequencies and see the tipping point for breakout noise, leakage of sound through the material. The focus is on whether or not there is saturation in noise reduction, coming from theory, of 6 <em>dB</em> per meter of silencer below 250 <em>Hz</em>.<sup>1</sup> The silencer used in this project is the CLA-A-250 from Swegon. Measurements are made in the MWL laboratory at KTH. The project arose from the company LN Akustikmiljö as they experienced different behaviour during projects of their own. As can be seen in results, the saturation of 6 <em>dB</em> per meter of silencer does not apply and is not a correct assumption. Regarding the breakout noise, it is clear that as long as the duct structure and its outlet are in separate rooms it is not a problem. This conclusion is safe to make according to standards and the margins considered in the calculations.<sup>2</sup> The project served as great insight in real life problems and applications of an acoustician.</p>


Note there are superscripts 1 & 2 for citations.
----------------------------------------------------------------------
In diva2:919804 
abstract is: 
<p>As we progress beyond the information age, there is a growing urgency towards sustainability. This word is synonymous with the way we produce energy and there is an awareness to gradually shift towards green energy production. Corpower Ocean aims at producing energy by utilizing the perpetual motion of ocean waves through the motion of small floating buoys. Unlike previous designs, this buoy utilizes the phenomenon of Resonance thus greatly enhancing the energy output.</p><p>In the thesis, the simulation model developed by Corpower Ocean to virtually describe the buoy in operation was validated. This was done by comparing forces obtained from buoy scale model experiments, simulation model and ORCAFELXTM software. After satisfactory validation was established, the shortcomings in the simulation model were identified. Next the simulation model was used to generate data for all sea states for a target site with given annual sea state distribution. This information was then used to predict ultimate loads, statistical loads, motions and equivalent load for a given fatigue life and loading cycles. The results obtained are then treated with a statistical tool called Variation Mode and Effect Analysis to quantify the uncertainty in design life prediction and estimate the factor of safety. The information will be used by the design team to develop the buoy design further. Finally the issue of survivability was addressed by checking buoy behavior in extreme waves in ORCAFLEXTM<strong>. </strong>Different survivability strategies were tested and videos were captured for identifying slack events and studying buoy behavior in Extreme conditions.</p><p>The work aims at validating a technology that is green from environmental and economic point of view.</p>

corrected abstract:
<p>As we progress beyond the information age, there is a growing urgency towards sustainability. This word is synonymous with the way we produce energy and there is an awareness to gradually shift towards green energy production. Corpower Ocean aims at producing energy by utilizing the perpetual motion of ocean waves through the motion of small floating buoys. Unlike previous designs, this buoy utilizes the phenomenon of Resonance thus greatly enhancing the energy output.</p><p>In the thesis, the simulation model developed by Corpower Ocean to virtually describe the buoy in operation was validated. This was done by comparing forces obtained from buoy scale model experiments, simulation model and ORCAFELXTM software. After satisfactory validation was established, the shortcomings in the simulation model were identified. Next the simulation model was used to generate data for all sea states for a target site with given annual sea state distribution. This information was then used to predict ultimate loads, statistical loads, motions and equivalent load for a given fatigue life and loading cycles. The results obtained are then treated with a statistical tool called Variation Mode and Effect Analysis to quantify the uncertainty in design life prediction and estimate the factor of safety. The information will be used by the design team to develop the buoy design further. Finally the issue of survivability was addressed by checking buoy behavior in extreme waves in ORCAFLEXTM. Different survivability strategies were tested and videos were captured for identifying slack events and studying buoy behavior in Extreme conditions.</p><p>The work aims at validating a technology that is green from environmental and economic point of view.</p>

Note - only change was to remove the unnecessary "<strong>" and "</strong>"
w='ORCAFELXTM' val={'c': 'Orcaflex™', 's': 'diva2:919804', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1475542 
abstract is: 
<p>In the mining industry, finite element analysis (FEA) and experimental tests are essential for the verification of mechanical properties and for performing parametric studies. Such practices require lots of resources and, sometimes, long computational time. Therefore, there is an increased need for alternative time efficient methods with a low requirement of resources. On the other hand, simulations of multi body mechanical models is a way to achieve that. The current work investigates a structural design that is modelled as a system of rigid bodies, which is firstly derivced in Python and then simulated in Matlab. As input, structural properties from previous experimental and FEM studies were used.</p><p>The aim of the project is to provide an understanding of the mechanical behaviour of the rock and the mining machine, during the cutting procedure. Parametric studies by altering the strucutral parameters of stiffness and dam,ing and by varying cutter-wheel placements provide us with several significant results. A correlation between the structural parameters and the fatique loads is observed. The influence of each individual structural parameter on the fatique loads are explicitly described. Overall, this mehodology leads to faster calculations since there is a massive decrease in the number of degrees of freedom compared to a finite element simulation, and therefore to the ability to perform extensive parametric studies.</p>

corrected abstract:
<p>In the mining industry, finite element analysis (FEA) and experimental tests are essential for the verification of mechanical properties and for performing parametric studies. Such practices require lots of resources and, sometimes, long computational time. Therefore, there is an increased need for alternative time efficient methods with a low requirement of resources. On the other hand, simulations of multi body mechanical models is a way to achieve that. The current work investigates a structural design that is modelled as a system of rigid bodies, which is firstly derived in Python and then simulated in Matlab. As input, structural properties from previous experimental and FEM studies were used.</p><p>The aim of the project is to provide an understanding of the mechanical behaviour of the rock and the mining machine, during the cutting procedure. Parametric studies by altering the structural parameters of stiffness and damping and by varying cutter-wheel placements provide us with several significant results. A correlation between the structural parameters and the fatigue loads is observed. The influence of each individual structural parameter on the fatigue loads are explicitly described. Overall, this methodology leads to faster calculations since there is a massive decrease in the number of degrees of freedom compared to a finite element simulation, and therefore to the ability to perform extensive parametric studies.</p>
----------------------------------------------------------------------
In diva2:1216647 
abstract is: 
<p>This thesis was conducted to investigate what factors impact the outcome of a patent application for the Swedish market. The method used was logistic regression and the data was extracted from the database of The Swedish Patent and Registration Offi ce, PRV. The analysis in this thesis started with 47 covariates, including the 35 IPO technical fields, resulting in a model consisting of five covariates. The most important covariates were determined to be the number of notices issued by PRV, whether or not a patent attorney was used and applicant type. The number of notices had a positive impact on the probability of the success of a patent application. Being a company and hiring a patent attorney also increase the chances of the patent being granted. The derived final model showed a high predictive ability and provides insight of significant factors of a successful patent application.</p>

corrected abstract:
<p>This thesis was conducted to investigate what factors impact the outcome of a patent application for the Swedish market. The method used was logistic regression and the data was extracted from the database of The Swedish Patent and Registration Office, PRV. The analysis in this thesis started with 47 covariates, including the 35 IPO technical fields, resulting in a model consisting of five covariates. The most important covariates were determined to be the number of notices issued by PRV, whether or not a patent attorney was used and applicant type. The number of notices had a positive impact on the probability of the success of a patent application. Being a company and hiring a patent attorney also increase the chances of the patent being granted. The derived final model showed a high predictive ability and provides insight of significant factors of a successful patent application.</p>
----------------------------------------------------------------------
In diva2:742118 
abstract is: 
<p>Coronary artery heart disease (CAD) is a common condition which can impair the quality of life and lead to cardiac infarctions. Traditional criteria during exercise tests are good but far from perfect. A lot of patients with inconclusive tests are referred to radiological examinations. By finding better evaluation criteria during the exercise test we can save a lot of money and let the patients avoid unnecessary examinations.</p><p>Computers record amounts of numerical data during the exercise test. In this retrospective study 267 patients with inconclusive exercise test and performed radiological examinations were included. The purpose was to use clinical considerations as-well as mathematical statistics to be able to find new diagnostic criteria.</p><p>We created a few new parameters and evaluated them together with previously used parameters. For women we found some interesting univariable results where new parameters discriminated better than the formerly used. However, the number of females with observed CAD was small (14) which made it impossible to obtain strong significance. For men we computed a multivariable model, using logistic regression, which discriminates way better than the traditional parameters for these patients. The area under the ROC curve was 0:90 (95 % CI: 0.83-0.97) which is excellent to outstanding discrimination in a group initially included due to their inconclusive results.</p><p>If the model can be proved to hold for another population it could contribute a lot to the diagnostics of this common medical conditions</p>

corrected abstract:
<p>Coronary artery heart disease (CAD) is a common condition which can impair the quality of life and lead to cardiac infarctions. Traditional criteria during exercise tests are good but far from perfect. A lot of patients with inconclusive tests are referred to radiological examinations. By finding better evaluation criteria during the exercise test we can save a lot of money and let the patients avoid unnecessary examinations.</p><p>Computers record amounts of numerical data during the exercise test. In this retrospective study 267 patients with inconclusive exercise test and performed radiological examinations were included. The purpose was to use clinical considerations as-well as mathematical statistics to be able to find new diagnostic criteria.</p><p>We created a few new parameters and evaluated them together with previously used parameters. For women we found some interesting univariable results where new parameters discriminated better than the formerly used. However, the number of females with observed CAD was small (14) which made it impossible to obtain strong significance.</p><p>For men we computed a multivariable model, using logistic regression, which discriminates way better than the traditional parameters for these patients. The area under the ROC curve was 0.90 (95 % CI: 0.83-0.97) which is excellent to outstanding discrimination in a group initially included due to their inconclusive results.</p><p>If the model can be proved to hold for another population it could contribute a lot to the diagnostics of this common medical conditions</p>
----------------------------------------------------------------------
In diva2:1082651 
abstract is: 
<p>The main issues discussed in this paper are related to the refinement of the on-ground casualty risk computation for the specific case of INTErnational Gamma-Ray Astrophysics Laboratory (INTEGRAL). The current approach, unable to predict the spacecraft attitude motion, assumes random tumbling motion as initial condition to simulate the fragmentation process. The wide experience in break-up analysis, acquired after years of practice with simulation, identified attitude to be one of the major drivers of uncertainty. The Space Debris Office (SDO) demanded a specific research in the field of the long-term propagation applied to the attitude motion and INTEGRAL offered the perfect test bench to conduct a preliminary study in this direction. In particular, observing whether environmental torques were able to trigger stable attitude motion, maintainable till re-entry, was considered to be the major challenge. The propagation of coupled orbital􀀀attitude motion for a random attitude configuration represents only one side of the coin. Indeed, chaos theory analysis constituted the other. The use of the Poincaré map in a non-canonical way managed to bring evidence for constrained motion in the angular rate motion of INTEGRAL, under gravity perturbations. Such results allowed to conduct further investigation on the overall attitude motion and estimate that the attitude configuration at the re-entry appears as precession about the maximum axis of inertia, in the majority of the cases.</p>

corrected abstract:
<p>The main issues discussed in this paper are related to the refinement of the on-ground casualty risk computation for the specific case of INTErnational Gamma-Ray Astrophysics Laboratory (INTEGRAL). The current approach, unable to predict the spacecraft attitude motion, assumes random tumbling motion as initial condition to simulate the fragmentation process. The wide experience in break-up analysis, acquired after years of practice with simulation, identified attitude to be one of the major drivers of uncertainty. The Space Debris Office (SDO) demanded a specific research in the field of the long-term propagation applied to the attitude motion and INTEGRAL offered the perfect test bench to conduct a preliminary study in this direction. In particular, observing whether environmental torques were able to trigger stable attitude motion, maintainable till re-entry, was considered to be the major challenge. The propagation of coupled orbital&mdash;attitude motion for a random attitude configuration represents only one side of the coin. Indeed, chaos theory analysis constituted the other. The use of the Poincaré map in a non-canonical way managed to bring evidence for constrained motion in the angular rate motion of INTEGRAL, under gravity perturbations. Such results allowed to conduct further investigation on the overall attitude motion and estimate that the attitude configuration at the re-entry appears as precession about the maximum axis of inertia, in the majority of the cases.</p>

Note only chanhe was to replace 􀀀 (i.e., '\U00100000' a private use Unicode character) with &mdash; 
----------------------------------------------------------------------
In diva2:808180 
abstract is: 
<p>Simulations of particle systems at constant temperature may be used to estimate several of the system’s physical properties, and some require integration over very long time to be accurate. To achieve sufficient accuracy in finite time the choice of numerical scheme is important and we suggest to use the symplectic Euler method combined with a step in an Ornstein-Uhlenbeck process. This scheme is computationally very cheap and is often used in applications of molecular dynamics. This thesis strives to motivate the usage of the scheme due to the lack of theoretical results and comparisons to alternative methods. We conduct three numerical experiments to evaluate the scheme. The design of each experiment aims to expose weaknesses or strengths of the method. For both model problems and more realistic experiments are the results positive in favor of the method; the symplectic Euler method combined with an Ornstein- Uhlenbeck step does perform well over long times.</p>

corrected abstract:
<p>Simulations of particle systems at constant temperature may be used to estimate several of the system’s physical properties, and some require integration over very long time to be accurate. To achieve sufficient accuracy in finite time the choice of numerical scheme is important and we suggest to use the symplectic Euler method combined with a step in an Ornstein-Uhlenbeck process. This scheme is computationally very cheap and is often used in applications of molecular dynamics. This thesis strives to motivate the usage of the scheme due to the lack of theoretical results and comparisons to alternative methods. We conduct three numerical experiments to evaluate the scheme. The design of each experiment aims to expose weaknesses or strengths of the method. For both model problems and more realistic experiments are the results positive in favor of the method; the symplectic Euler method combined with an Ornstein-Uhlenbeck step does perform well over long times.</p>

Note only change was to eliminate the space after the hyphen in the last sentence.
----------------------------------------------------------------------
