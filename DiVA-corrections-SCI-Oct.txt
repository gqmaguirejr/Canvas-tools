Duplicates?   'diva2:408826' and 'diva2:512408'
----------------------------------------------------------------------
Duplicates: 'diva2:1221420', 'diva2:1229928' - note that one has the full
text.
----------------------------------------------------------------------
Duplicates: 'diva2:1221420', 'diva2:1229928' - note that one has the full
text.
----------------------------------------------------------------------
Duplicates?   'diva2:408826' and 'diva2:512408' 
Not duplicates - but an error in document uploaded
----------------------------------------------------------------------
Duplicates - ['diva2:509959', 'diva2:515592' both have full text but one also
has a compenfium
----------------------------------------------------------------------
Are these duplicates: diva2:1033230 diva2:1033189 ?
----------------------------------------------------------------------
Duplicates? 'diva2:1033239', 'diva2:1045115'
----------------------------------------------------------------------
===============================================================================
Above were all reported on or before 2024-10-11
----------------------------------------------------------------------
I get a error when trying to get the fill text for diva2:1900086 - via
https://kth.diva-portal.org/smash/get/diva2:1900086/FULLTEXT01.pdf - the file
will not open and it appears to have been truncated - as the usual PDF file
ending is not present.
----------------------------------------------------------------------
The full text fo diva2:1873390 
https://kth.diva-portal.org/smash/get/diva2:1873390/FULLTEXT01.pdf
seems to be blank pages - the file is truncated and cannot be repaired.
----------------------------------------------------------------------
diva2:736211 has the text of the summary and not that of the abstract, as the DiVA abstract
----------------------------------------------------------------------
Are 'diva2:754257' and  'diva2:753742' duplicates?
----------------------------------------------------------------------
The Swedish abstract for diva2:1595164 is actually in English.
----------------------------------------------------------------------
Are diva2:1045047 and diva2:1033220 duplicates?
----------------------------------------------------------------------
Are diva2:839893 and diva2:737929 duplicates?
The abstract for the second is the same except for two spaces.
----------------------------------------------------------------------
In diva2:644350 - missing symbols, missing ligatures, and colons rather than decimal points::

<p>In this thesis a detailed discussion of the topic percolation theory in squared lattices in</p><p>two dimensions will be conducted. To support this discussion numerical calculations will</p><p>be done. For the data analysis and simulations the Hoshen-Kopelman-Algorithm [2] will</p><p>be used. All concepts deduced will nally lead to the determination of the conductance's</p><p>exponent</p><p>t in random resistor networks. Using Derrida's transfer matrix program to</p><p>calculate the conductivity of random resistors in two and three dimensions [11] and</p><p>the nite-size scaling approach were used. In two dimensions</p><p>t= = 0:955 0:006 was</p><p>obtained. Were</p><p>is the exponent of the correlation length in innite lattices. This</p><p>value is in excellent agreement with Derrida (</p><p>t= = 0:960:02, [11]) and slightly smaller</p><p>than Sahimi (</p><p>t= = 0:97480:001, [21]). In three dimensions the same approach yielded</p><p>t=</p><p>= 2:155 0:012 which some what smaller than the value found by Sahimi t= =</p><p>2</p><p>:27 0:20 [21] and Gingold and Lobb t= = 2:276 0:012 [25].</p>

abstract should be:

diva2:644350: <p>In this thesis a detailed discussion of the topic percolation theory in squared lattices in two dimensions will be conducted. To support this discussion numerical calculations will be done. For the data analysis and simulations the Hoshen-Kopelman-Algorithm [2] will be used. All concepts deduced will nally lead to the determination of the conductance's exponent t in random resistor networks. Using Derrida's transfer matrix program to calculate the conductivity of random resistors in two and three dimensions [11] and the finite-size scaling approach were used. In two dimensions <em>t/&nu;</em> = 0.955&pm;0.006 was obtained. Were is the exponent of the correlation length in infinite lattices. This value is in excellent agreement with Derrida ( <em>t/&nu;</em> = 0.96&pm;0.02, [11]) and slightly smaller than Sahimi ( <em>t/&nu;</em> = 0.9748&pm;0.001, [21]). In three dimensions the same approach yielded <em>t/&nu;</em> = 2.155&pm;0.012 which some what smaller than the value found by Sahimi <em>t/&nu;</em> = 2.:27&pm;0.20 [21] and Gingold and Lobb <em>t/&nu;</em> = 2.276&pm;0.012 [25].</p>
----------------------------------------------------------------------
The full text for diva2:1210790 has a different thesis: https://kth.diva-portal.org/smash/get/diva2:1210790/FULLTEXT02.pdf
----------------------------------------------------------------------
In diva2:571089

<p>Curve fitting is used in a variety of fields, especially in physics, mathematics and economics.</p><p>The method is often used to smooth noisy data and for doing path planning. In this bachelor</p><p>thesis calculus of variations will be used to derive a formula for finding an optimal curve to fit a</p><p>set of data points. We evaluate a cost function (defined on the set of all curves</p><p></p><p>f on the interval</p><p>[</p><p></p><p>a; b]) given by F(f) =</p><p>R</p><p></p><p>b</p><p>a</p><p></p><p>(f00(x))2dx +</p><p>P</p><p></p><p>n</p><p>i</p><p></p><p>=1(f(xi) 􀀀 yi)2. The integral term represents the</p><p>smoothness of the curve, the interpolation error is given by the summation term and</p><p></p><p>&gt; 0 is</p><p>defined as the interpolation parameter. An ideal curve minimizes the interpolation error and</p><p>is relatively smooth. This is problematic since a smooth function generally has a large interpolation</p><p>error when doing curve fitting, and therefore the interpolation parameter</p><p></p><p>is needed</p><p>to decide how much consideration should be given to each attribute. For the cost function</p><p></p><p>F</p><p>a larger value of</p><p></p><p>decreases the interpolation error of the curve. The analytical calculations</p><p>performed made it possible to construct a</p><p></p><p>Matlab program, that could be used to solve the</p><p>minimization problem. In the result part some examples are presented for different values of</p><p></p><p>.</p><p>The conclusion is that a larger value of the interpolation parameter</p><p></p><p>is generally needed when</p><p>using more data points and if the points are closely placed on the x-axis. Further on, a method</p><p>called Ordinary Cross Validation (OCV) is evaluated to find an optimal value of</p><p></p><p>. This method</p><p>gave good results, except for the case when the points could almost be fitted with a straight line.</p>

corrected abstract:

<p>Curve fitting is used in a variety of fields, especially in physics, mathematics and economics. The method is often used to smooth noisy data and for doing path planning. In this bachelor thesis calculus of variations will be used to derive a formula for finding an optimal curve to fit a set of data points. We evaluate a cost function (defined on the set of all curves <em>f</em> on the interval [a, b]) given by <em>F(f) = &int;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup style="position: relative; top: -0.5rem; left: 0.05rem;">b</sup> <sub style="position: relative; bottom: -0.3rem; left: -0.1rem;">a</sub></span></span>(f&Prime;(x))<sup>2</sup> dx + &lambda; &sum;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup style="position: relative; top: -0.5rem; left: 0.05rem;">n</sup><sub style="position: relative; bottom: -0.5rem; left: 0.05rem;">i=1</sub></span></span>(f(x<sub>i</sub>) - y<sub>i</sub>)<sup>2</sup></em>. The integral term represents the smoothness of the curve, the interpolation error is given by the summation term and &lambda; &gt; 0 is defined as the interpolation parameter. An ideal curve minimizes the interpolation error and is relatively smooth. This is problematic since a smooth function generally has a large interpolation error when doing curve fitting, and therefore the interpolation parameter &lambda; is needed to decide how much consideration should be given to each attribute. For the cost function <em>F</em> a larger value of &lambda; decreases the interpolation error of the curve. The analytical calculations performed made it possible to construct a  Matlab program, that could be used to solve the minimization problem. In the result part some examples are presented for different values of &lambda;. The conclusion is that a larger value of the interpolation parameter &lambda; is generally needed when using more data points and if the points are closely placed on the x-axis. Further on, a method called Ordinary Cross Validation (OCV) is evaluated to find an optimal value of &lambda;. This method gave good results, except for the case when the points could almost be fitted with a straight line.</p>

If MathJax were installed one could replace the long HTML with:
 $F(f) = \int_{a}^{b}(f^{\prime\prime}(x))^2 dx + \lambda \sum_{i=1}^{n} (f(x_i) -y_i)^2$
----------------------------------------------------------------------
In diva2:557257

<p>For a field k and a grading of the polynomial ringk[t] with Hilbert functionh, we consider the Quot functor Quoth V , where V = ? di =1k[t] is a finitely generated and free k[t]-module. The Quot functor parametrizes, for any k-algebra B, homogeneous B [t]-submodulesN⊆B⊗kV such that the graded components of the quotient( B⊗kV)/Nare locally freeB-modules of rank given byh. We find that it is locallyrepresentable by a polynomial ring over kin a finite number of variables. Finally, weshow that there is a scheme that represents the Quot functor that is both smooth and irreducible.</p>


Corrected abstract:
<p>For a field <em>k</em> and a grading of the polynomial ring <em>k[t]</em> with Hilbert function <em>h</em>, we consider the Quot functor Quoth <em>V<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>h</sup><sub>V</sub></span></span></em> , where <em>V = &otimes; b<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>d</sup><sub>i=1</sub></span></span> k[t]</em> is a finitely generated and free <em>k[t]</em>-module. The Quot functor parametrizes, for any <em>k</em>-algebra <em>B</em>, homogeneous <em>B[t]</em>-submodules <em>N ⊆ B⊗<sub>k</sub>V</em> such that the graded components of the quotient (<em>B⊗<sub>k</sub>V</em>)/N are locally free <em>B</em>-modules of rank given by <em>h</em>. We find that it is locally representable by a polynomial ring over <em>k</em> in a finite number of variables. Finally, we show that there is a scheme that represents the Quot functor that is both smooth and irreducible.</p>

----------------------------------------------------------------------
In diva2:1576394 custom font encoding have been used - you cannot select or search for text.
----------------------------------------------------------------------
In diva2:1816881, many words were merged in the abstract:

<p>Gas turbines can experience various changes that affect their performance.Compressor fouling is one of the leading causes that deteriorate the gas turbineperformance. This research aims to investigate the impact of compressorfouling on the performance of gas turbines and the rotodynamic behaviorof gas turbines. Fouling was simulated as a reduction of mass flow and areduction of compressor isentropic efficiency by using Turbomatch software.A rotor–bearing model was created to analyze the vibration behavior dueto compressor fouling by using MADYN 2000 software and that particledeposition leads to rotor imbalance. The results show that the main variationsfor performance are power output, pressure ratio and EGT. For the rotodynamicmodel, the result illustrates an increase in vibration level for the first andsecond bearings and a decrease for the third bearing. The results also predictedthat parameters mass flow, compressor discharge temperature or specific fuelconsumption show a similar trend compared to the increase in vibrations. Thisresult can be used in conjunction with GPA analysis to predict the foulingcondition and help in identifying the severity of the fouling condition.</p>

Corrected abstract:

diva2:1816881: <p>Gas turbines can experience various changes that affect their performance. Compressor fouling is one of the leading causes that deteriorate the gas turbine performance. This research aims to investigate the impact of compressor fouling on the performance of gas turbines and the rotodynamic behavior of gas turbines. Fouling was simulated as a reduction of mass flow and a reduction of compressor is entropic efficiency by using Turbomatch software. A rotor–bearing model was created to analyze the vibration behavior dueto compressor fouling by using MADYN 2000 software and that particle deposition leads to rotor imbalance. The results show that the main variations for performance are power output, pressure ratio and EGT. For the rotodynamic model, the result illustrates an increase in vibration level for the first and second bearings and a decrease for the third bearing. The results also predicted that parameters mass flow, compressor discharge temperature or specific fuel consumption show a similar trend compared to the increase in vibrations. This result can be used in conjunction with GPA analysis to predict the fouling condition and help in identifying the severity of the fouling condition.</p>
----------------------------------------------------------------------
In diva2:1216708 merged words and innecessary text:

<p>In this project, we aim to find a method for obtainingthe factors in a bull/bear market factor model for asset returnand variance, given an optimal portfolio. The proposed methodwas derived using the Karush-Kuhn-Tucker (KKT) conditionsfor optimal solutions to the convex Markowitz portfolio selectionproblem. For synthetic data where all necessary parameters wereknown exactly, the method could give bounds on the factors. Theexact values of the factors were obtained when short selling wasallowed, and in some instances when short selling was forbidden.The method was evaluated on real-world data with varyingresults, possibly due to estimation errors and invalid assumptionsabout the model of the investor.I. INTRODUC</p>


Corrected abstract:

<p>In this project, we aim to find a method for obtainingthe factors in a bull/bear market factor model for asset returnand variance, given an optimal portfolio. The proposed method was derived using the Karush-Kuhn-Tucker (KKT) conditions for optimal solutions to the convex Markowitz portfolio selection problem. For synthetic data where all necessary parameters were known exactly, the method could give bounds on the factors. The exact values of the factors were obtained when short selling was allowed, and in some instances when short selling was forbidden. The method was evaluated on real-world data with varying results, possibly due to estimation errors and invalid assumptions about the model of the investor.</p>

----------------------------------------------------------------------
In diva2:1900086 there is an error in the PDF, it will fail to load - see
https://kth.diva-portal.org/smash/get/diva2:1900086/FULLTEXT01.pdf

The file seems to have been cut off after 14,680,064 bytes.
----------------------------------------------------------------------
In diva2:1348434, many equations are incorrect and the words are not correct:

<p>Better Shelter RHU is a social enterprise developing and providing temporary Refugee Housing Units to aid regions of crisis. The shelters are deployed worldwide and they are subjected to harsh weather conditions particularly to heavy wind loads. To maximise the amount of units deployed, the shelters have to be cost-efficient and material lead times need to be short. In order to achieve this, an evaluation to use lesser strength materials in the load bearing structure specifically the main joint named Joint1 - is assessed in this thesis. To assess the feasibility to change the material of the joint to an alternative steel with lower tensile strength and elongation ratio, the current performance is first analysed and then compared to the performance with an alternative cheaper material available for the production method. Modeling of the wind loads were made with fluid analysis and the resulting pressures were transferred on to the load bearing frame. From the frame, displacements were derived which were subsequently transferred to a subassembly with Joint1 in focus. From the sub-assembly, stresses for a wind load of 28 m/s could be evaluated for the joint. For the current material, which has a yield strength denoted RC shown in regions of about 1.09 · RC has a yield strength denoted  and a tensile strength denoted RC p02 or 0.6 · RC m, incipient plasticity were p02 and a tensile strength denoted RA m. The alternative material, which m, plasticity was shown in similar regions but also areas where the stresses reached tensile strength (1.03 · RA m) at the same wind speed. Conclusively, the alternative material appears as more hazardous because of the lower tensile strength compared to the current material. These results are based on conservative assumptions where minimum values of material data are used and the simulated models are simplified.</p><p> </p>

Corrected abstract:
<p><em>Better Shelter RHU</em> is a social enterprise developing and providing temporary Refugee Housing Units to aid regions of crisis. The shelters are deployed worldwide and they are subjected to harsh weather conditions particularly to heavy wind loads. To maximise the amount of units deployed, the shelters have to be cost-efficient and material lead times need to be short. In order to achieve this, an evaluation to use lesser strength materials in the load bearing structure specifically the main joint named <em>Joint1</em> - is assessed in this thesis.</p>
<p>To assess the feasibility to change the material of the joint to an alternative steel with lower tensile strength and elongation ratio, the current performance is first analyzed and then compared to the performance with an alternative cheaper material available for the production method.</p>
<p>Modeling of the wind loads were made with fluid analysis and the resulting pressures were transferred on to the load bearing frame. From the frame, displacements were derived which were subsequently transferred to a subassembly with Joint1 in focus. From the sub-assembly, stresses for a wind load of 28 m/s could be evaluated for the joint. For the current material, which has a yield strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>p02</sub></span></span></em> and a tensile strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>m</sub></span></span></em>, incipient plasticty were shown in regions of about <em>1.09 &middot; R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>p02</sub></span></span></em> or <em>0.6 &middot; R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>C</sup><sub>m</sub></span></span></em>. The alternative material, which has a yield strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>A</sup><sub>p02</sub></span></span></em> and a tensile strength called <em>R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>A</sup><sub>m</sub></span></span></em>, plasticity was shown in similar regions but also areas where the stresses reached tensile strength (<em>1.03 &middot; R<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>A</sup><sub>m</sub></span></span></em>) at the same wind speed. Conclusively, the alternative material appears as more hazardous because of the lower tensile strength compared to the current material. These results are based on conservative assumptions where minimum values of material data are used and the simulated models are simplified.</p>
----------------------------------------------------------------------
Possible duplicates:
diva2:1045047: <p>In this project we are implementing group formations in to the game engine Unity3D. Groups</p><p>of people move in a certain way when avoiding other groups and obstacles. With the use of the</p><p>steering package UnitySteer we have made some example scenes using di erent steering behaviors.</p><p>We have made an implementation of the RVO2 package in Unity. With this implementation we</p><p>can simulate various scenarios with agents avoiding each other. Human pedestrians tend to walk</p><p>in groups. We adapted this behavior in our program to make our simulations as real as possible.</p><p>Finally we evaluated the performance of our simulation by checking the frames per seconds when</p><p>increasing the number of agents.</p><p>Keywords: Agent, Steering, Obstacle Avoidance, RVO, Groups, Formations, Real-Time, Unity.</p>
diva2:1033220: <p>In this project we are implementing group formations in to the game engine Unity3D. Groups</p><p>of people move in a certain way when avoiding other groups and obstacles. With the use of the</p><p>steering package UnitySteer we have made some example scenes using di erent steering behaviors.</p><p>We have made an implementation of the RVO2 package in Unity. With this implementation we</p><p>can simulate various scenarios with agents avoiding each other. Human pedestrians tend to walk</p><p>in groups. We adapted this behavior in our program to make our simulations as real as possible.</p><p>Finally we evaluated the performance of our simulation by checking the frames per seconds when</p><p>increasing the number of agents.</p><p>Keywords: Agent, Steering, Obstacle Avoidance, RVO, Groups, Formations, Real-Time, Unity.</p>
----------------------------------------------------------------------
In diva2:516126 missing equations:

<p>Uncertainties in radiative effects of the quarks in -background in the form of final state radiation (FSR) are significant when it comes to reducing all forms of systematics that can arise from measuring the jets energy. Analysis on FSR is in general conducted on different simulated samples where one has included the radiative effect using algorithms such as PYTHIA[29]. The hypothesis is that through the re-weighting of the -background nominal sample one could add a better representation of the FSR effect. Finding a simple way to include a better description of FSR would not only save time in the simulation process but it would also be a way to reduce the systematic errors originating from limited MC statistics. Due to statistical effects coming from the simulations one cannot use the basic approach to define the effect of FSR as simply the difference between nominal and FSR. Two methods are tested to estimate the FSR effects; the first method uses a set of efficiency factors to represent the signal regions, the second method is to add a weight to the events of the nominal sample. The first method show positive results, especially in SR2, compared to a basic analysis, with an uncertainty of the FSR effect of: SR1:±29% SR2: ±51% SR3: ±37%. While a basic analysis gave an uncertainty of ±42%, ±122% and 36%. The second method shows positive signs where the re-weighted sample moves closer to the behaviour of the FSR sample. However, both methods are based on insufficient amount of statistics to draw any absolute conclusions.</p>

Corrected abstract:

diva2:516126: <p>Uncertainties in radiative effects of the quarks in <em>t<span style="text-decoration: overline;">t</span></em>-background in the form of final state radiation (FSR) are significant when it comes to reducing all forms of systematics that can arise from measuring the jets energy. Analysis on FSR is in general conducted on different simulated samples where one has included the radiative effect using algorithms such as PYTHIA[29]. The hypothesis is that through the re-weighting of the <em>t<span style="text-decoration: overline;">t</span></em>-background nominal sample one could add a better representation of the FSR effect. Finding a simple way to include a better description of FSR would not only save time in the simulation process but it would also be a way to reduce the systematic errors originating from limited MC statistics. Due to statistical effects coming from the simulations one cannot use the basic approach to define the effect of FSR as simply the difference between nominal and FSR. Two methods are tested to estimate the FSR effects; the first method uses a set of efficiency factors to represent the signal regions, the second method is to add a weight to the events of the nominal sample. The first method show positive results, especially in SR2, compared to a basic analysis, with an uncertainty of the FSR effect of: SR1:±29% SR2: ±51% SR3: ±37%. While a basic analysis gave an uncertainty of ±42%, ±122% and 36%. The second method shows positive signs where the re-weighted sample moves closer to the behaviour of the FSR sample. However, both methods are based on insufficient amount of statistics to draw any absolute conclusions.</p>
----------------------------------------------------------------------
In diva2:783990 abstract is:
<p>During construction of large sandwich structures one of or perhaps the greatest difficulty lies increating good joints. The joint is often the first component to fail and constitute a large part of theweight of the structure and production workload. The study is focused on investigating if noveljoining methods could affect how military vessels are able to handle an internal blast withoutcatastrophic damage to the sections surrounding the area of the explosion. If the conventionalmethod of joining sandwich panels was to be used the result of an internal blast would mostprobably be a complete failure of the surrounding joints. This is based on that the conventional Xjointslacks continuous fibers through the joint in at least one direction and are therefore weak whensubjected to tensile forces in that direction. The belief is that a joint with continuous fibers wouldhave a smaller risk of failure or at least the failure would be less severe than with the conventionaljoints and could therefore maintain its structural integrity until reparations can be made.The purpose of this study is to manufacture small scale samples of novel X-joint design concepts.These concepts are then compared by manufacturability, tensile strength in either direction of thejoint and the joints flexibility. How well the promising novel concepts handle an internal blast cannotbe seen in this study, since full scale samples and advanced blast trials would be required.Six novel methods for joining of composite sandwich panels and a reference conventional joint,referred to as the standard joint, were manufactured and evaluated through three tensile tests. Outof the six concepts, all except the 3D-woven joint was found to have advantages over the standardtype, which only surpassed the novel joints in the aspect of bulkhead strength, and the lath and thebundle joint concepts were found to be the most promising. The tensile strength tests gave that theultimate strength of lath joint was 86% of the reference value in its deck direction and at least 75%in the bulkhead direction when reinforced with laths (49% unreinforced), where the reference value(100%) was the ultimate strength of the standard joint bulkhead. The tensile test results for thebundle joint, which did not have a reinforced bulkhead, reached 77% and 66% (deck respectivelybulkhead) of the reference strength value. These results supports the theory that the circular holesof the bundle concept has less impact on the strength of the bulkhead than the rectangular holes ofthe lath concept. The fiber bundles, however, are difficult and time consuming to work with,especially since the fiber bundles needs to be flattened and spread out to increase the area ofattachment when adhered to the sandwich panels. In contrast, the lath pieces are easy tomanufacture and work with during the final assembly and therefore more suitable to use whenreinforcement of the bulkhead is required.The joint flexibility test indicated that the most flexible joint was, as might be expected, the taperedfinger joint. If the tapered panel could be designed to counteract or avoid delamination of thetapering area and if the panels of the joint could be prevented from bending to the point fracture ofthe single skin laminates, this concept could be a suitable solution for confining a blast in terms ofrapidly developing membrane forces. The lath and the bundle joints were once again the strongestand reacted very similar. Unfortunately no difference could be found between the two jointconcepts because the hinges used to attach the samples in the test broke before the samples werebroken.Since the fiber properties and amounts were not compared it is not definite that either of the lath orthe bundle concept is better than the other in terms of mechanical properties. The conclusion is thatboth these two concepts work, are relatively easy to produce and have a far greater potential thanthat of the standard joint, in the internal blast situation and as a joining method in general.</p>

corrected abstract:
<p>During construction of large sandwich structures one of or perhaps the greatest difficulty lies in creating good joints. The joint is often the first component to fail and constitute a large part of the weight of the structure and production workload. The study is focused on investigating if novel joining methods could affect how military vessels are able to handle an internal blast without catastrophic damage to the sections surrounding the area of the explosion. If the conventional method of joining sandwich panels was to be used the result of an internal blast would most probably be a complete failure of the surrounding joints. This is based on that the conventional X-joints lacks continuous fibers through the joint in at least one direction and are therefore weak when subjected to tensile forces in that direction. The belief is that a joint with continuous fibers would have a smaller risk of failure or at least the failure would be less severe than with the conventional joints and could therefore maintain its structural integrity until reparations can be made.</p><p>The purpose of this study is to manufacture small scale samples of novel X-joint design concepts. These concepts are then compared by manufacturability, tensile strength in either direction of the joint and the joints flexibility. How well the promising novel concepts handle an internal blast cannot be seen in this study, since full scale samples and advanced blast trials would be required.</p><p>Six novel methods for joining of composite sandwich panels and a reference conventional joint, referred to as the standard joint, were manufactured and evaluated through three tensile tests. Out of the six concepts, all except the 3D-woven joint was found to have advantages over the standard type, which only surpassed the novel joints in the aspect of bulkhead strength, and the lath and the bundle joint concepts were found to be the most promising. The tensile strength tests gave that the ultimate strength of lath joint was 86% of the reference value in its deck direction and at least 75% in the bulkhead direction when reinforced with laths (49% unreinforced), where the reference value (100%) was the ultimate strength of the standard joint bulkhead. The tensile test results for the bundle joint, which did not have a reinforced bulkhead, reached 77% and 66% (deck respectively bulkhead) of the reference strength value. These results supports the theory that the circular holes of the bundle concept has less impact on the strength of the bulkhead than the rectangular holes of the lath concept. The fiber bundles, however, are difficult and time consuming to work with, especially since the fiber bundles needs to be flattened and spread out to increase the area of attachment when adhered to the sandwich panels. In contrast, the lath pieces are easy to manufacture and work with during the final assembly and therefore more suitable to use when reinforcement of the bulkhead is required.</p><p>The joint flexibility test indicated that the most flexible joint was, as might be expected, the tapered finger joint. If the tapered panel could be designed to counteract or avoid delamination of the tapering area and if the panels of the joint could be prevented from bending to the point fracture of the single skin laminates, this concept could be a suitable solution for confining a blast in terms of rapidly developing membrane forces. The lath and the bundle joints were once again the strongest and reacted very similar. Unfortunately no difference could be found between the two joint concepts because the hinges used to attach the samples in the test broke before the samples were broken.</p><p>Since the fiber properties and amounts were not compared it is not definite that either of the lath or the bundle concept is better than the other in terms of mechanical properties. The conclusion is that both these two concepts work, are relatively easy to produce and have a far greater potential than that of the standard joint, in the internal blast situation and as a joining method in general.</p>
----------------------------------------------------------------------
In diva2:1110812 abstract is:
<p>Automotive development has always been need-based and the product of today is an evolutionover several decades and a diversied technology application to deliver better products to theend users. Steady increase in the deployment of on-board electronics and software is characterizedby the demand and stringent regulations. Today, almost every function on-board a modernvehicle is either monitored or controlled electronically.One such specic demand for AB Volvo arose out of construction trucks in the US market. Usersseldom have/had a view of the operational boundaries of the drivetrain components, resultingin inappropriate use causing damage, poor traction and steering performance. Also, AB Volvo'sstand-alone traction assistance functions were not suciently capable to handle the vehicle useconditions. Hence, the goal was set to automate and synchronize the traction assistance devicesand software functions to improve the traction and steerability under a variety of road conditions.The rst steps in this thesis involved understanding the drivetrain components from design andoperational boundary perspective. The function descriptions of the various traction softwarefunctions were reviewed and a development/integration plan drafted. A literature survey wascarried out seeking potential improvement in traction from dierential locking and also its eectson steerability. A benchmarking exercise was carried out to identify competitor and suppliertechnologies available for the traction device automation task.The focus was then shifted to developing and validating the traction controller in a simulationenvironment. Importance was given to modeling of drivetrain components and renement ofvehicle behavior to study and understand the eects of dierential locking and develop a differentiallock control strategy. The modeling also included creating dierent road segments toreplicate use environment and simulating vehicle performance in the same, to reduce test timeand costs. With well-correlated vehicle performance results, a dierential lock control strategywas developed and simulated to observe traction improvement. It was then implemented onan all-wheel drive construction truck using dSPACE Autobox to test, validate and rene thecontroller.Periodic test sessions carried out at Hallered proving ground, Sweden were important to re-ne the control strategy. Feedback from test drivers and inputs from cross-functional teamswere essential to develop a robust controller and the same was tested for vehicle suitability andrepeatability of results. When comparing with the existing traction software functions, the integrateddierential lock and transfer case lock controller showed signicantly better performanceunder most test conditions. Repeatable results proved the reliability of developed controller.The correlation between vehicle test scenarios and simulation environment results indicated theaccuracy of software models and control strategy, bi-directionally.Finally, the new traction assistance device controller function was demonstrated within ABVolvo to showcase the traction improvement and uncompromising steerability.</p>

corrected abstract:
<p>Automotive development has always been need-based and the product of today is an evolution over several decades and a diversified technology application to deliver better products to the end users. Steady increase in the deployment of on-board electronics and software is characterized by the demand and stringent regulations. Today, almost every function on-board a modern vehicle is either monitored or controlled electronically.</p><p>One such specific demand for AB Volvo arose out of construction trucks in the US market. Users seldom have/had a view of the operational boundaries of the drivetrain components, resulting in inappropriate use causing damage, poor traction and steering performance. Also, AB Volvo's stand-alone traction assistance functions were not sufficiently capable to handle the vehicle use conditions. Hence, the goal was set to automate and synchronize the traction assistance devices and software functions to improve the traction and steerability under a variety of road conditions.</p><p>The first steps in this thesis involved understanding the drivetrain components from design and operational boundary perspective. The function descriptions of the various traction software functions were reviewed and a development/integration plan drafted. A literature survey was carried out seeking potential improvement in traction from differential locking and also its effects on steerability. A benchmarking exercise was carried out to identify competitor and supplier technologies available for the traction device automation task.</p><p>The focus was then shifted to developing and validating the traction controller in a simulation environment. Importance was given to modeling of drivetrain components and refinement of vehicle behavior to study and understand the effects of differential locking and develop a differential lock control strategy. The modeling also included creating different road segments to replicate use environment and simulating vehicle performance in the same, to reduce test time and costs. With well-correlated vehicle performance results, a differential lock control strategy was developed and simulated to observe traction improvement. It was then implemented on an all-wheel drive construction truck using dSPACE Autobox to test, validate and refine the controller.</p><p>Periodic test sessions carried out at Hällered proving ground, Sweden were important to refine the control strategy. Feedback from test drivers and inputs from cross-functional teams were essential to develop a robust controller and the same was tested for vehicle suitability and repeatability of results. When comparing with the existing traction software functions, the integrated differential lock and transfer case lock controller showed significantly better performance under most test conditions. Repeatable results proved the reliability of developed controller. The correlation between vehicle test scenarios and simulation environment results indicated the accuracy of software models and control strategy, bi-directionally.</p><p>Finally, the new traction assistance device controller function was demonstrated within AB Volvo to showcase the traction improvement and uncompromising steerability.</p>
----------------------------------------------------------------------
In diva2:492776 abstract is:
<p>In recent years the interest for smaller, cheaper and more energy efficient vehicles hasincreased significantly. These vehicles are intended to be used in urban areas, where theactual need of large heavy cars is generally minor. The travelled distance is on average lessthan 56km during a day and most often there is only one person travelling in the vehicle. Manyof the established car manufacturers have recently started to take interest into this marketsegment, but the majority of these small vehicles are still manufactured by smaller companiesat a low cost and with little or no research done on vehicle traffic safety. This may be becausethere are still no legal requirements on crash testing of this type of vehicles.This report will examine road safety for Urban Light-weight Vehicle (ULV) to find criticalcrash scenarios from which future crash testing methods for urban vehicles can be derived.The term ULV is specific to this report and is the title for all engine powered three- and fourwheeledvehicles categorized by the European Commission. Other attributes than the wheelgeometry is engine power and the vehicles unladen mass. The maximum allowed weight for athree-wheeled ULV is 1 000kg and 400kg for a four-wheeled one.By studying current crash test methods used in Europe by Euro NCAP it has beenconcluded that these tests are a good way of assessing car safety. For light-weight urbanvehicles it has been concluded that some of these tests need to be changed and that some newtest scenarios should be added when assessing road safety. The main reasons for this is linkedto that vehicle’s with a weight difference of more than 150kg cannot be compared withcurrent test methods, and that crash tests are performed with crash objects with similar orequal mass in current safety assessment programs. This correlates poorly to the trafficsituation for light-weight urban vehicles since it would most likely collide with a far heaviervehicle than itself in an accident event.To verify the actual traffic situation in urban areas, accident statistics have beenexamined closely. The research has shown that there are large differences between rural andurban areas. For instance; 66% of all severe and fatal traffic accident occurs in rural areaseven though they are less populated. Even the distribution of accident categories has showndifferent in rural and urban areas. The United Nations Economic Commission for Europe(UNECE) has defined accident categories in their database which is widely used within theEuropean Union. By comparing each accident category’s occurrence, injury and fatality rate,the most critical urban accident categories were found in the following order.</p>
<p>1. Collision due to crossing or turning</p>
<p>2. Vehicle and pedestrian collision</p>
<p>3. Rear-end collision</p>
<p>4. Single-vehicle accident</p>
<p>5. Other collisions</p>
<p>6. Head-on collision</p>
<p>Statistics also show that of all fatally injured crash victims in urban trafficapproximately; one third is travelling by car; one third by motorcycle, moped or pedal-cycle;and one third are pedestrians. This means that unprotected road travelers correspond to twothirds of all fatal urban traffic accidents, a fact that has to be taken into account in future crashtesting of urban vehicles. With all the information gathered a total of four new crash testscenarios for light-weight urban vehicles have been presented:</p>
<p>• Vehicle-to-vehicle side impact at 40km/h with a 1 300kg striking vehicle to evaluate theoccupant protection level of the light-weight vehicle.</p>
<p>• Vehicle-to-motorcycle side impact at 40km/h with motorcycle rider protection evaluation.</p>
<p>• Pedestrian protection assessment at 40km/h over the whole vehicle front and roof area.</p>
<p>• Rigid barrier impact at 40km/h corresponding to an urban single vehicle accident with aroad side object or a collision with a heavier or similar sized vehicle.</p>

corrected abstract:
<p>In recent years the interest for smaller, cheaper and more energy efficient vehicles has increased significantly. These vehicles are intended to be used in urban areas, where the actual need of large heavy cars is generally minor. The travelled distance is on average less than <em>56km</em> during a day and most often there is only one person travelling in the vehicle. Many of the established car manufacturers have recently started to take interest into this market segment, but the majority of these small vehicles are still manufactured by smaller companies at a low cost and with little or no research done on vehicle traffic safety. This may be because there are still no legal requirements on crash testing of this type of vehicles.</p><p>This report will examine road safety for <em>Urban Light-weight Vehicle (ULV)</em> to find critical crash scenarios from which future crash testing methods for urban vehicles can be derived. The term <em>ULV</em> is specific to this report and is the title for all engine powered three- and four-wheeled vehicles categorized by the <em>European Commission</em>. Other attributes than the wheel geometry is engine power and the vehicles unladen mass. The maximum allowed weight for a three-wheeled <em>ULV</em> is <em>1 000kg</em> and <em>400kg</em> for a four-wheeled one.</p><p>By studying current crash test methods used in Europe by <em>Euro NCAP</em> it has been concluded that these tests are a good way of assessing car safety. For light-weight urban vehicles it has been concluded that some of these tests need to be changed and that some new test scenarios should be added when assessing road safety. The main reasons for this is linked to that vehicle’s with a weight difference of more than <em>150kg</em> cannot be compared with current test methods, and that crash tests are performed with crash objects with similar or equal mass in current safety assessment programs. This correlates poorly to the traffic situation for light-weight urban vehicles since it would most likely collide with a far heavier vehicle than itself in an accident event.</p><p>To verify the actual traffic situation in urban areas, accident statistics have been examined closely. The research has shown that there are large differences between rural and urban areas. For instance; 66% of all severe and fatal traffic accident occurs in rural areas even though they are less populated. Even the distribution of accident categories has shown different in rural and urban areas. The <em>United Nations Economic Commission for Europe (UNECE)</em> has defined accident categories in their database which is widely used within the <em>European Union</em>. By comparing each accident category’s occurrence, injury and fatality rate, the most critical urban accident categories were found in the following order.</p>
<em><ol>
<li>Collision due to crossing or turning</li>
<li>Vehicle and pedestrian collision</li>
<li>Rear-end collision</li>
<li>Single-vehicle accident</li>
<li>Other collisions</li>
<li>Head-on collision</li>
</ol></em>
<p>Statistics also show that of all fatally injured crash victims in urban traffic approximately; one third is travelling by <em>car</em>; one third by <em>motorcycle</em>, moped or pedal-cycle; and one third are <em>pedestrians</em>. This means that unprotected road travelers correspond to two thirds of all fatal urban traffic accidents, a fact that has to be taken into account in future crash testing of urban vehicles. With all the information gathered a total of four new crash test scenarios for light-weight urban vehicles have been presented:</p>
<em><ul>
<li>Vehicle-to-vehicle side impact at 40km/h with a 1 300kg striking vehicle to evaluate the occupant protection level of the light-weight vehicle.</li>
<li>Vehicle-to-motorcycle side impact at 40km/h with motorcycle rider protection evaluation.</li>
<li>Pedestrian protection assessment at 40km/h over the whole vehicle front and roof area.</li>
<li>Rigid barrier impact at 40km/h corresponding to an urban single vehicle accident with a road side object or a collision with a heavier or similar sized vehicle.</li>
</ul><em>
----------------------------------------------------------------------
In diva2:895396 abstract is:
<p>A lattice physics code is a vital tool, forming a base of reactor coreanalysis. It enables the neutronic properties of the fuel assembly to becalculated and generates a proper set of data to be used by a 3-D full coresimulator. Due to advancement and complexity of modern Boiling WaterReactor assembly designs, a new deterministic lattice physics codeis being developed at Westinghouse Sweden AB, namely PHOENIX5.Each time a new code is written, its methodology of solving the neutrontransport equation, has to be validated to make sure it providesreliable output. In a wake of preparation for PHOENIX5 release andconsecutive validation efforts, a set of reference Monte Carlo calculationswas prepared, using the code Serpent. A depletion calculation with achosen type of branch cases was conducted. Methods implemented inPHOENIX5 are based on the Current Coupling Collision Probabilitymethod used in older versions of the code HELIOS. Therefore, a comparisonbetween reference Monte Carlo simulations and HELIOS 1.8.1is made, in order to discover problems inherent to the said method ofsolving the neutron transport equation. A special care should be givenduring PHOENIX5 validation, to issues highlighted in this work.Discrepancies in results of Serpent and HELIOS are attributed mostlyto disparities in the basic nuclear data used by the codes, as well as arange of approximations and corrections adopted by the deterministiccode.Serpent and HELIOS showed a good agreement in a typical voidrange (up to 90 % void) and ‘less’ challenging branches (coolant void,fuel temperature and spacer grid branches). More significant discrepanciesappeared for extreme cases with a very high void and control rodpresence (k1 differences as high as 1000 pcm) and rather pronouncedconcentrations of the natural boron dissolved in coolant (absolute differencesroughly at a level of 900 pcm). The issues do not seem to stemsolely from discrepancies in the nuclear data libraries used by Serpentand HELIOS.Moreover, a coolant void bias was consistently found in the resultsof branch calculation at changing coolant void. This confirms the analogousphenomenon found in previous studies of the CCCP based deterministiccodes. It most probably stems from the assumptions used bythe method while tackling the neutron transport equation, such as theflat source approximation, the isotropic scattering assumption and thetransport correction. An alternative transport correction approximationis proposed to alleviate this issue.</p>

corrected abstract:
<p>A lattice physics code is a vital tool, forming a base of reactor core analysis. It enables the neutronic properties of the fuel assembly to be calculated and generates a proper set of data to be used by a 3-D full core simulator. Due to advancement and complexity of modern Boiling Water Reactor assembly designs, a new deterministic lattice physics code is being developed at Westinghouse Sweden AB, namely PHOENIX5. Each time a new code is written, its methodology of solving the neutron transport equation, has to be validated to make sure it provides reliable output. In a wake of preparation for PHOENIX5 release and consecutive validation efforts, a set of reference Monte Carlo calculations was prepared, using the code Serpent. A depletion calculation with a chosen type of branch cases was conducted. Methods implemented in PHOENIX5 are based on the Current Coupling Collision Probability method used in older versions of the code HELIOS. Therefore, a comparison between reference Monte Carlo simulations and HELIOS 1.8.1 is made, in order to discover problems inherent to the said method of solving the neutron transport equation. A special care should be given during PHOENIX5 validation, to issues highlighted in this work.</p><p>Discrepancies in results of Serpent and HELIOS are attributed mostly to disparities in the basic nuclear data used by the codes, as well as a range of approximations and corrections adopted by the deterministic code.</p><p>Serpent and HELIOS showed a good agreement in a typical void range (up to 90 % void) and ‘less’ challenging branches (coolant void, fuel temperature and spacer grid branches). More significant discrepancies appeared for extreme cases with a very high void and control rod presence (k<sub>&infin;</sub> differences as high as 1000 pcm) and rather pronounced concentrations of the natural boron dissolved in coolant (absolute differences roughly at a level of 900 pcm). The issues do not seem to stem solely from discrepancies in the nuclear data libraries used by Serpent and HELIOS.</p><p>Moreover, a coolant void bias was consistently found in the results of branch calculation at changing coolant void. This confirms the analogous phenomenon found in previous studies of the CCCP based deterministic codes. It most probably stems from the assumptions used by the method while tackling the neutron transport equation, such as the flat source approximation, the isotropic scattering assumption and the transport correction. An alternative transport correction approximation is proposed to alleviate this issue.</p>
----------------------------------------------------------------------
In diva2:1740195 abstract is:
<p>Carbon fibers submitted to high temperatures (&gt;2000 °C) experience a permanent increasein their thermal conductivity. This change has been attributed to a change in the molecularstructure due to graphitisation. Graphitisation occurs when amorphous carbons are exposed tohigh temperatures (&gt; 1000°C) for a prolonged period of time and describes the process in whichcarbon atoms are rearranged from their amorphous form into structured hexagonal ringed latticesheets. To characterise the extent of this process, one needs to determine certain ring statisticswhich provide information on the bonding structure. In this work, we develop and verify a ringstatistics tool that can be used to analyze the resulting structure of atomistic simulations, and useit in a novel approach to characterise the extent of graphitisation in Molecular Dynamics (MD)simulations of carbon. Different ring definitions, such as Franzblau, Leroux, Hybrid and King arecompared to determine the most appropriate definition for the investigation of carbon structures.A new ring definition, Hybrid, is introduced as an extension of Leroux’s definition, exploiting theefficiency of Leroux’s definition while making the definition more appropriate for carbon systemsby removing shortcuts of length 1. It was found that Franzblau rings most accurately capturecarbon structures, and are most optimal for the investigation of amorphous and graphitisedcarbons. We then apply this tool to two MD simulations of amorphous carbons undergoing anannealing process at 4000K for 300 ps to characterise the extent of graphitisation. We found aprevalence of ∼0.1 hexagonal rings per atom in amorphous carbons prior to annealing, comparedto ∼0.33 hexagonal rings per atom in graphitised carbon after annealing. The likelihood of a ringbeing hexagonal in amorphous carbon was ∼30%, as opposed to ∼75% in graphitised samples.Calculating the ratio in the number of hexagonal rings per atom to the number of hexagonalrings per atom in a fully graphitised system, the extent of graphitisation can be quantified. Sincethis value is normalized by the number of atoms in the simulation this method can be appliedto any domain size. This successful application of the ring statistics tool opens the door toapply it to more realistic and complex systems. The tool has already been expanded to considermulti-component systems and molecule identification. Hence, the tool could already be appliedto more complex cases, such as doped or contaminated systems, investigating the effects on bondstructure. In its current state, the tool could also be used to investigate how the extent andrate of graphitisation changes at different depths in a system. Potentially characterising therate at which graphitisation penetrates a system under various conditions. The tool also hasthe potential to be expanded to consider localisation and identification of defects, bond angles,bond creation and destruction and the structural classification and identification of systems.Combining this tool with MDSuite, a software in development by the Institute for ComputationalPhysics (ICP) at the University of Stuttgart with the collaboration of the von Karman Institutefor Fluid Dynamics (VKI) to analyse MD trajectories, could offer a package that can providedeep system information for minimal cost.</p>

corrected abstract:
<p>Carbon fibers submitted to high temperatures (&gt;2000 °C) experience a permanent increase in their thermal conductivity. This change has been attributed to a change in the molecular structure due to graphitisation. Graphitisation occurs when amorphous carbons are exposed to high temperatures (&gt; 1000°C) for a prolonged period of time and describes the process in which carbon atoms are rearranged from their amorphous form into structured hexagonal ringed lattice sheets. To characterise the extent of this process, one needs to determine certain ring statistics which provide information on the bonding structure. In this work, we develop and verify a ring statistics tool that can be used to analyze the resulting structure of atomistic simulations, and use it in a novel approach to characterise the extent of graphitisation in Molecular Dynamics (MD) simulations of carbon. Different ring definitions, such as Franzblau, Leroux, Hybrid and King are compared to determine the most appropriate definition for the investigation of carbon structures. A new ring definition, Hybrid, is introduced as an extension of Leroux’s definition, exploiting the efficiency of Leroux’s definition while making the definition more appropriate for carbon systems by removing shortcuts of length 1. It was found that Franzblau rings most accurately capture carbon structures, and are most optimal for the investigation of amorphous and graphitised carbons. We then apply this tool to two MD simulations of amorphous carbons undergoing an annealing process at 4000K for 300 ps to characterise the extent of graphitisation. We found a prevalence of ∼0.1 hexagonal rings per atom in amorphous carbons prior to annealing, compared to ∼0.33 hexagonal rings per atom in graphitised carbon after annealing. The likelihood of a ring being hexagonal in amorphous carbon was ∼30%, as opposed to ∼75% in graphitised samples. Calculating the ratio in the number of hexagonal rings per atom to the number of hexagonal rings per atom in a fully graphitised system, the extent of graphitisation can be quantified. Since this value is normalized by the number of atoms in the simulation this method can be applied to any domain size. This successful application of the ring statistics tool opens the door to apply it to more realistic and complex systems. The tool has already been expanded to consider multi-component systems and molecule identification. Hence, the tool could already be applied to more complex cases, such as doped or contaminated systems, investigating the effects on bond structure. In its current state, the tool could also be used to investigate how the extent and rate of graphitisation changes at different depths in a system. Potentially characterising the rate at which graphitisation penetrates a system under various conditions. The tool also has the potential to be expanded to consider localisation and identification of defects, bond angles, bond creation and destruction and the structural classification and identification of systems. Combining this tool with MDSuite, a software in development by the Institute for Computational Physics (ICP) at the University of Stuttgart with the collaboration of the von Karman Institute for Fluid Dynamics (VKI) to analyse MD trajectories, could offer a package that can provide deep system information for minimal cost.</p>
----------------------------------------------------------------------
In diva2:1078086 abstract is:
<p>There is a renewed interest in the wind estimate over and around forest areas dueto the increasing demand of wind-energy resources. Many researches have been donewith simplied forest models. However, the introduction of a simple two-dimensionalclearing adds further parameters, such as the width of the clearing, which furthercomplicates the analysis. The main purpose of the present experimental and nu-merical efforts is, therefore, to characterize the ow over the forest clearing and tosuggest the suitable location for the wind-power generation over the forest clearing.The experiments were performed in the Minimum Turbulence Level (MTL) windtunnel at KTH in Stockholm, and PIV data evaluation and analysis were carried out.The canopy model consists of several wooden at plates, and to each of the plateswooden cylindrical pins were clamped in a staggered layout to mimic a homogeneoushigh-density forest. The total length of the forest model is 40hc, where hc indicatesthe canopy height. Two cases were experimentally investigated, one with a fullforest conguration and the other with the presence of a clearing that starts fromx=hc = 20 and ends at x=hc = 30, where x is a streamwise coordinate that startsat the forest windward edge. Particle Image Velocimetry (PIV) was performed, andplanar velocity snapshots were taken at the downwind edge of the clearing.Large Eddy Simulations (LES) were also conducted to complement the experi-mental information. The present LES code was developed by modifying the DirectNumerical Simulation (DNS) code of the turbulent boundary layer ow by Kametani&amp; Fukagata (2011), by adding the subgrid scale model part and the empirical canopymodel part into the DNS code.Both the experimental and the numerical results indicate that the clearing is as-sociated to a streamwise velocity defect in the mean prole mainly due to the strongturbulent diffusion into the clearing region. The turbulence is redistributed amongstthe various velocity components so that the streamwise velocity variance is reduced,while the vertical velocity variance is enhanced. The streamwise velocity varianceis in fact damped due to the absence of the canopy drag from x=hc = 20, whileenhanced vertical-velocity uctuations can be observed at the end of the clearing.However, the effects are immediately weakened both by a ow re-acceleration andby a new surface layer development right after passing the downwind clearing edge.The clearing effect seems to be dominant in the roughness sublayer at least for theneutral atmospheric conditions. The clearing perturbation seems to be associatedto turbulent mixing at its initial stage near y hc, followed by a rapid distorsionnear the clearing trailing edge. This phenomenon is highlighted by the low valueof the vertical correlation length scale that, after the clearing trailing edge, risesagain towards to homogenous forest condition. The LES results further show thata suitable area for the wind-turbine operation is close to the upwind clearing edgewhere the energy contents is the highest, while the turbulent intensity is lowestbetween the clearing. They also indicate that wind-speed enhancement can be ex-pected downstream of the short forest edge, implying that the ow can be optimizedfor wind-power generation just by changing the forest conguration.</p>

corrected abstract:
<p>There is a renewed interest in the wind estimate over and around forest areas due to the increasing demand of wind-energy resources. Many researches have been done with simplified forest models. However, the introduction of a simple two-dimensional clearing adds further parameters, such as the width of the clearing, which further complicates the analysis. The main purpose of the present experimental and numerical efforts is, therefore, to characterize the flow over the forest clearing and to suggest the suitable location for the wind-power generation over the forest clearing.</p><p>The experiments were performed in the Minimum Turbulence Level (MTL) wind tunnel at KTH in Stockholm, and PIV data evaluation and analysis were carried out. The canopy model consists of several wooden flat plates, and to each of the plates wooden cylindrical pins were clamped in a staggered layout to mimic a homogeneous high-density forest. The total length of the forest model is 40hc, where hc indicates the canopy height. Two cases were experimentally investigated, one with a full forest configuration and the other with the presence of a clearing that starts from <em>x/h<sub>c<sub> = 20</em> and ends at <em>x/h<sub>c</sub> = 30</em>, where <em>x</em> is a streamwise coordinate that starts at the forest windward edge. Particle Image Velocimetry (PIV) was performed, and planar velocity snapshots were taken at the downwind edge of the clearing.</p><p>Large Eddy Simulations (LES) were also conducted to complement the experimental information. The present LES code was developed by modifying the Direct Numerical Simulation (DNS) code of the turbulent boundary layer flow by Kametani &amp; Fukagata (2011), by adding the subgrid scale model part and the empirical canopy model part into the DNS code.</p><p>Both the experimental and the numerical results indicate that the clearing is associated to a streamwise velocity defect in the mean profile mainly due to the strong turbulent diffusion into the clearing region. The turbulence is redistributed amongst the various velocity components so that the streamwise velocity variance is reduced, while the vertical velocity variance is enhanced. The streamwise velocity variance is in fact damped due to the absence of the canopy drag from <em>x/h<sub>c</sub> = 20</em>, while enhanced vertical-velocity fluctuations can be observed at the end of the clearing. However, the effects are immediately weakened both by a flow re-acceleration and by a new surface layer development right after passing the downwind clearing edge. The clearing effect seems to be dominant in the roughness sublayer at least for the neutral atmospheric conditions. The clearing perturbation seems to be associated to turbulent mixing at its initial stage near <em>y ≈ h<sub>c</sub></em>, followed by a rapid distorsion near the clearing trailing edge. This phenomenon is highlighted by the low value of the vertical correlation length scale that, after the clearing trailing edge, rises again towards to homogenous forest condition. The LES results further show that a suitable area for the wind-turbine operation is close to the upwind clearing edge where the energy contents is the highest, while the turbulent intensity is lowest between the clearing. They also indicate that wind-speed enhancement can be expected downstream of the short forest edge, implying that the flow can be optimized for wind-power generation just by changing the forest configuration.</p>
----------------------------------------------------------------------
In diva2:1247161 abstract is:
<p>Nowadays, increasing pressure from legislation and customer demands in the automotive industryare forcing manufacturers to produce greener vehicles with lower emissions and fuel consumption.As a result, electrified and hybrid vehicles are a growing popular alternative to traditional internalcombustion engines (ICE). The noise from an electric vehicle comes mainly from contact betweentyres and road, wind resistance and driveline. The noise emitted from the driveline is for the mostpart related to the gearbox. When developing a driveline, it is a factor of importance to estimatethe noise radiating from the gearbox to achieve an acceptable design.Gears are used extensively in the driveline of electric vehicles. As the gears are in mesh, a mainintrusive concern is known as gear whine noise. Gear whine noise is an undesired vibroacousticphenomenon and is likely to originate through the gear contacts and be transferred through themechanical components to the housing where the vibrations are converted into airborne andstructure-borne noise. The gear whine noise originates primarily from the excitation coming fromtransmission error (TE). Transmission error is defined as the difference between the ideal smoothtransfer of motion of a gear and what is in practice due to lack of smoothness.The main objective of this study is to simulate the vibrations generated by the gear whine noise inan electric powertrain line developed by AVL Vicura. The electric transmission used in this studyprovides only a fixed overall gear ratio, i.e. 9.59, under all operation conditions. It is assumed thatthe system is excited only by the transmission error and the mesh stiffness of the gear contacts. Inorder to perform NVH analysis under different operating conditions, a multibody dynamics modelaccording to the AVL Excite program has been developed. The dynamic simulations are thencompared with previous experimental measurements provided by AVL Vicura.Two validation criteria have been used to analyse the dynamic behaviour of the AVL Excite model:signal processing using the FFT method and comparison with the experimental measurements.The results from the AVL Excite model show that the FFT criterion is quite successful and allexcitation frequencies are properly observed in FFT plots. Nevertheless, when it comes to thesecond criterion, as long as not all dynamic parameters of the system such as damping or stiffnesscoefficients are provided with certainty in the model, it is too difficult to investigate the accuracy ofthe AVL Excite model.Another investigation is a numerical design study to analyses how the damping coefficientsinfluence the response. After reducing the damping parameters, the results show that the housingand bearings have the highest influence on the response. If more acceptable results are desired,future studies must be concentrated on these to obtain more acceptable damping values.</p>


corrected abstract:
<p>Nowadays, increasing pressure from legislation and customer demands in the automotive industry are forcing manufacturers to produce greener vehicles with lower emissions and fuel consumption. As a result, electrified and hybrid vehicles are a growing popular alternative to traditional internal combustion engines (ICE). The noise from an electric vehicle comes mainly from contact between tyres and road, wind resistance and driveline. The noise emitted from the driveline is for the most part related to the gearbox. When developing a driveline, it is a factor of importance to estimate the noise radiating from the gearbox to achieve an acceptable design.</p><p>Gears are used extensively in the driveline of electric vehicles. As the gears are in mesh, a main intrusive concern is known as gear whine noise. Gear whine noise is an undesired vibroacoustic phenomenon and is likely to originate through the gear contacts and be transferred through the mechanical components to the housing where the vibrations are converted into airborne and structure-borne noise. The gear whine noise originates primarily from the excitation coming from transmission error (TE). Transmission error is defined as the difference between the ideal smooth transfer of motion of a gear and what is in practice due to lack of smoothness.</p><p>The main objective of this study is to simulate the vibrations generated by the gear whine noise in an electric powertrain line developed by AVL Vicura. The electric transmission used in this study provides only a fixed overall gear ratio, i.e. 9.59, under all operation conditions. It is assumed that the system is excited only by the transmission error and the mesh stiffness of the gear contacts. In order to perform NVH analysis under different operating conditions, a multibody dynamics model according to the AVL Excite program has been developed. The dynamic simulations are then compared with previous experimental measurements provided by AVL Vicura.</p><p>Two validation criteria have been used to analyse the dynamic behaviour of the AVL Excite model: signal processing using the FFT method and comparison with the experimental measurements. The results from the AVL Excite model show that the FFT criterion is quite successful and all excitation frequencies are properly observed in FFT plots. Nevertheless, when it comes to the second criterion, as long as not all dynamic parameters of the system such as damping or stiffness coefficients are provided with certainty in the model, it is too difficult to investigate the accuracy of the AVL Excite model.</p><p>Another investigation is a numerical design study to analyses how the damping coefficients influence the response. After reducing the damping parameters, the results show that the housing and bearings have the highest influence on the response. If more acceptable results are desired, future studies must be concentrated on these to obtain more acceptable damping values.</p>
----------------------------------------------------------------------
In diva2:1078069 - missing space in title:
"Partially Premixed Combustion (PPC) for low loadconditions in marine engines using computationaland experimental techniques"
==>
"Partially Premixed Combustion (PPC) for low loadconditions in marine engines using computational and experimental techniques"

abstract is:
<p>Diesel Engine has been the most powerful and relevant source of power in the automobile industryfor decades due to their excellent performance, efficiency and power. On the contrary, there arenumerous environmental issues of the diesel engines hampering the environment. It has been agreat challenge for the researchers and scientists to minimize these issues. In the recent years, severalstrategies have been introduced to eradicate the emissions of the diesel engines. Among them,Partially Premixed Combustion (PPC) is one of the most emerging and reliable strategies. PPC is acompression ignited combustion process in which ignition delay is controlled. PPC is intended toendow with better combustion with low soot and NOx emission.The engine used in the present study is a single-cylinder research engine, installed in Aalto UniversityInternal Combustion Engine Laboratory with the bore diameter of 200 mm. The thesis presentsthe validation of the measurement data with the simulated cases followed by the study of the sprayimpingement and fuel vapor mixing in PPC mode for different injection timing. A detailed study ofthe correlation of early injection with the fuel vapor distribution and wall impingement has beenmade.The simulations are carried out with the commercial CFD software STAR CD. Different injectionparameters have been considered and taken into an account to lower the wall impingement and toproduce better air-fuel mixing with the purpose of good combustion and reduction of the emissions.The result of the penetration length of the spray and the fuel vapor distribution for different earlyinjection cases have been illustrated in the study. Comparisons of different thermodynamic propertiesand spray analysis for different injection timing have been very clearly illustrated to get insightof effect of early injection. The parameters like injection timing, injection period, injection pressure,inclusion angle of the spray have an influence the combustion process in PPC mode. Extensivestudy has been made for each of these parameters to better understand their effects in the combustionprocess. Different split injection profiles have been implemented for the study of better fuelvapor distribution in the combustion chamber.The final part of the thesis includes the study of the combustion and implementation of EGR tocontrol the temperature so as to get more prolonged ignition delay to accompany the PPC strategyfor standard piston top and deep bowl piston top. With the injection optimization and implementationof EGR, NOx has been reduced by around 44%, CO by 60% and Soot by 66% in the standardpiston top. The piston optimization resulted in more promising result with 58% reduction in NOx,55% reduction in CO and 67% reduction in Soot. In both cases the percentage of fuel burnt wasincreased by around 8%.</p>



corrected abstract:
<p>Diesel Engine has been the most powerful and relevant source of power in the automobile industry for decades due to their excellent performance, efficiency and power. On the contrary, there are numerous environmental issues of the diesel engines hampering the environment. It has been a great challenge for the researchers and scientists to minimize these issues. In the recent years, several strategies have been introduced to eradicate the emissions of the diesel engines. Among them, Partially Premixed Combustion (PPC) is one of the most emerging and reliable strategies. PPC is a compression ignited combustion process in which ignition delay is controlled. PPC is intended to endow with better combustion with low soot and NOx emission.</p><p>The engine used in the present study is a single-cylinder research engine, installed in Aalto University Internal Combustion Engine Laboratory with the bore diameter of 200 mm. The thesis presents the validation of the measurement data with the simulated cases followed by the study of the spray impingement and fuel vapor mixing in PPC mode for different injection timing. A detailed study of the correlation of early injection with the fuel vapor distribution and wall impingement has been made.</p><p>The simulations are carried out with the commercial CFD software STAR CD. Different injection parameters have been considered and taken into an account to lower the wall impingement and to produce better air-fuel mixing with the purpose of good combustion and reduction of the emissions. The result of the penetration length of the spray and the fuel vapor distribution for different early injection cases have been illustrated in the study. Comparisons of different thermodynamic properties and spray analysis for different injection timing have been very clearly illustrated to get insight of effect of early injection. The parameters like injection timing, injection period, injection pressure, inclusion angle of the spray have an influence the combustion process in PPC mode. Extensive study has been made for each of these parameters to better understand their effects in the combustion process. Different split injection profiles have been implemented for the study of better fuel vapor distribution in the combustion chamber.</p><p>The final part of the thesis includes the study of the combustion and implementation of EGR to control the temperature so as to get more prolonged ignition delay to accompany the PPC strategy for standard piston top and deep bowl piston top. With the injection optimization and implementation of EGR, NOx has been reduced by around 44%, CO by 60% and Soot by 66% in the standard piston top. The piston optimization resulted in more promising result with 58% reduction in NOx, 55% reduction in CO and 67% reduction in Soot. In both cases the percentage of fuel burnt was increased by around 8%.</p>
----------------------------------------------------------------------
In diva2:1816751 abstract is:
<p>This thesis investigates reasons for signiﬁcant uncertainties in added wave resistance predictionsand how wave conditions can potentially aﬀect the design of RoPax ferries. The objectiveis to ﬁnd a suitable prediction method of added wave resistance for the RoPax ferry designapplication. Furthermore, the wave environment on the route strongly inﬂuences this delicateand complex phenomenon. Thus, the emphasis is to understand the added wave resistancethrough a case study with a probabilistic wave environment.The fast transition into decarbonization and regulations regarding energy-eﬃcient ship designhave ramped up the awareness of the inﬂuence of seaways. For lower speeds, the addedresistance becomes a more signiﬁcant part of total resistance, with concerns regarding minimumpropulsion power and safe maneuvering in adverse sea conditions. Consequently, the demandhas rocketed for profound insight and accurate prediction methods of added wave resistance. Inaddition, with new larger ships, added wave resistance domain for short waves becomes essentialand an additional challenge regarding predictions. Nevertheless, added wave resistancepredictions are complex and contain many pitfalls, so accurate estimations of the ship’s addedwave resistance response and wave environment are crucial. In addition, added wave resistanceis very ship-speciﬁc, and published research for RoPax ferries is rare.Due to signiﬁcant uncertainties for general numerical methods, the study investigates a new(modiﬁed NTUA) semi-empirical method reﬁned for ships with a large beam-to-draft ratio.In addition, a realistic wave environment is included by selecting relevant wave spectra forconditions on the route.The study shows that signiﬁcant variances of added wave resistance predictions arise fromselecting prediction methods beyond the range of applicability and rough assumptions of waveconditions and spectra. The case study discovered that errors might also be introduced bythe classiﬁcation society deﬁnitions, which gives reasons to rethink the applied deﬁnition of"average BF 8" wave conditions for Safe Return to Port (SRtP) assessments. This can causea false illusion of the ship’s performance and safety in waves. Only the misjudgment ofthe most critical peak period resulted in a rough underestimation (&gt; 40%) of mean addedwave resistance. The error corresponded to 215% of the still water resistance for the SRtPassessment. In addition, the nature of added wave resistance is very ship speciﬁc. Therefore, theauthor emphasizes caution when selecting the prediction method, especially for semi-empiricalmethods. Despite the ﬁrst promising glance of the applied semi-empirical method, it appearsthat the ship database correlates poorly for RoPax ferries. Reliability for the method is weak forshort waves, with a tendency to large overestimations. The lack of references of RoPax vesselsfor validations, accident statics in adverse sea conditions and recent insight into nonlinear eﬀectsrequest further research on added wave resistance for modern hull shapes.</p>

corrected abstract:
<p>This thesis investigates reasons for significant uncertainties in added wave resistance predictions and how wave conditions can potentially affect the design of RoPax ferries. The objective is to find a suitable prediction method of added wave resistance for the RoPax ferry design application. Furthermore, the wave environment on the route strongly influences this delicate and complex phenomenon. Thus, the emphasis is to understand the added wave resistance through a case study with a probabilistic wave environment.</p><p>The fast transition into decarbonization and regulations regarding energy-efficient ship design have ramped up the awareness of the influence of seaways. For lower speeds, the added resistance becomes a more significant part of total resistance, with concerns regarding minimum propulsion power and safe maneuvering in adverse sea conditions. Consequently, the demand has rocketed for profound insight and accurate prediction methods of added wave resistance. In addition, with new larger ships, added wave resistance domain for short waves becomes essential and an additional challenge regarding predictions. Nevertheless, added wave resistance predictions are complex and contain many pitfalls, so accurate estimations of the ship’s added wave resistance response and wave environment are crucial. In addition, added wave resistance is very ship-specific, and published research for RoPax ferries is rare.</p><p>Due to significant uncertainties for general numerical methods, the study investigates a new (modified NTUA) semi-empirical method refined for ships with a large beam-to-draft ratio. In addition, a realistic wave environment is included by selecting relevant wave spectra for conditions on the route.</p><p>The study shows that significant variances of added wave resistance predictions arise from selecting prediction methods beyond the range of applicability and rough assumptions of wave conditions and spectra. The case study discovered that errors might also be introduced by the classification society definitions, which gives reasons to rethink the applied definition of "average BF 8" wave conditions for Safe Return to Port (SRtP) assessments. This can cause a false illusion of the ship’s performance and safety in waves. Only the misjudgment of the most critical peak period resulted in a rough underestimation (&gt; 40%) of mean added wave resistance. The error corresponded to 215% of the still water resistance for the SRtP assessment. In addition, the nature of added wave resistance is very ship specific. Therefore, the author emphasizes caution when selecting the prediction method, especially for semi-empirical methods. Despite the first promising glance of the applied semi-empirical method, it appears that the ship database correlates poorly for RoPax ferries. Reliability for the method is weak for short waves, with a tendency to large overestimations. The lack of references of RoPax vessels for validations, accident statics in adverse sea conditions and recent insight into nonlinear effects request further research on added wave resistance for modern hull shapes.</p>
----------------------------------------------------------------------
In diva2:1465518 - missing space in title:
"Accelerated test for vehicle body durability basedon vehicle dynamics simulations using pseudo damage"
==>
"Accelerated test for vehicle body durability based on vehicle dynamics simulations using pseudo damage"

abstract is: <p>Vehicle body durability evaluation strongly relies on the possibility of testing areal prototype on different testing surfaces, such as proving grounds, test rigsand real roads. Although many efforts have been made to reduce time requiredfor testing, this still remains one of the main resource-demanding phases in avehicle development. As direct consequence, more and more companies aim tooptimise and to improve vehicle development by means of CAE tools.This master thesis is a step towards virtual testing of a vehicle body, aimingto investigate and to select the most important measurements for a bodydurability evaluation and to reproduce an invariant excitation that could beapplied to other vehicle models for new durability assessments without theneed of real measurements from other models. Moreover, a comparison betweenfrequency-based methods and time-based methods and their differences werehighlighted and the validity of ISO8608:2016 discussed.The method relied on small sets of simple and easy-to-get internal measurements,called desired signal, that allowed back-calculation of wheel hubdisplacements and other excitations by means of the product of the model’stransfer function and desired signal. Then, the iteration procedure allowedto drastically reduce the error between the desired signal and the computedone and it proved to be essential in the process. Thanks to this procedure,damage information of also the not-measured signals could be computed andtheir damage assessed and thus available for durability purposes. Moreover, thechance of applying the same measurements taken from a real vehicle to a modelof a newer generation was investigated. This would avoid the need of buildinga running prototype, allowing a more accurate durability assessment alreadyavailable in the pre-design phase.As a result, using a specific set of 8 measurements, other 22 other forces,displacements and velocities of several components were precisely reproduced,showing that not all measurement are equally valuable in a durability evaluation.A method for the measurement selection, called Observability Method, wasthen developed and compared with a set of measurements selected by meansof experience, showing a better convergence of the pseudo damage and similarpseudo damage values. Eventually a small set of measures from an older carwas applied to a newer version. It was proved that a detailed knowledge ofthe car model is needed, in order to successfully back-calculate the relevantmeasurements.</p>



corrected abstract:
<p>Vehicle body durability evaluation strongly relies on the possibility of testing a real prototype on different testing surfaces, such as proving grounds, test rigs and real roads. Although many efforts have been made to reduce time required for testing, this still remains one of the main resource-demanding phases in a vehicle development. As direct consequence, more and more companies aim to optimise and to improve vehicle development by means of CAE tools.</p><p>This master thesis is a step towards virtual testing of a vehicle body, aiming to investigate and to select the most important measurements for a body durability evaluation and to reproduce an invariant excitation that could be applied to other vehicle models for new durability assessments without the need of real measurements from other models. Moreover, a comparison between frequency-based methods and time-based methods and their differences were highlighted and the validity of ISO8608:2016 discussed.</p><p>The method relied on small sets of simple and easy-to-get internal measurements, called desired signal, that allowed back-calculation of wheel hub displacements and other excitations by means of the product of the model’s transfer function and desired signal. Then, the iteration procedure allowed to drastically reduce the error between the desired signal and the computed one and it proved to be essential in the process. Thanks to this procedure, damage information of also the not-measured signals could be computed and their damage assessed and thus available for durability purposes. Moreover, the chance of applying the same measurements taken from a real vehicle to a model of a newer generation was investigated. This would avoid the need of building a running prototype, allowing a more accurate durability assessment already available in the pre-design phase.</p><p>As a result, using a specific set of 8 measurements, other 22 other forces, displacements and velocities of several components were precisely reproduced, showing that not all measurement are equally valuable in a durability evaluation. A method for the measurement selection, called Observability Method, was then developed and compared with a set of measurements selected by means of experience, showing a better convergence of the pseudo damage and similar pseudo damage values. Eventually a small set of measures from an older car was applied to a newer version. It was proved that a detailed knowledge of the car model is needed, in order to successfully back-calculate the relevant measurements.</p>
----------------------------------------------------------------------
In diva2:915628 abstract is:
<p>Resistance spot welding (RSW) is the dominant joining technology in the automotive industry. This is due to its low costs and high efficiency. Other advantages with RSW is the high ability for automation,low consumption of energy, lack of need for added materials and low degree of pollution,no expensive equipment or education of personal compared to arc welding and laser welding. A modern automobile contains approximately 4000 resistance spot welds,which is why it is of great interest to be ableto predict the properties of the resistance spotwelds. The most important measurement used to ensure the quality of the weld is thenugget size, as it correlates to the weldsmechanical strength. This is usually measured by destructive testing, and the most common method is the coach peel test. Thistest is performed by manually peeling the specimen and then measuring the largest and smallest nugget diameter. It is also possibleto perform non-destructive testing on resistance spot welds with both ultrasonic and x-ray tests, however none of these methods have the same accuracy as the destructive methods and they are cumbersome to use in large-scale. Toimprove the efficiency and lower the cost forthe optimization of the welding parameters,simulation tools have been developed. Thereare both 2D- and 3D-simulation software tomodel the RSW process. When the spotwelds are simulated with 2D or 2D axis symmetry,the number of elements is lowercompared to a full 3D model, which reducesthe computation times. The disadvantageswith the 2D model are the inabilities tomodel misalignments or other asymmetricalgeometries. In contrast, the 3D-simulationsare not limited by these factors, and they arealso capable of modeling the shunt effectsoccurring when a weld is placed close to aprevious weld.The aims of this thesis was to evaluate such a3D-simulation software, Sorpas 3D, and itspotential to be used in industrial processplanning, and to assess the software’s usefulness for both simple and more complexcases.The results from this work show a good correspondence between the simulations andthe physical tests. However, in order to achieve these results a number of modifications in the material properties were  required. Another critical limitation in the software is that no expulsion criterion isimplemented. Considering the possibility that the problems can be solved with a number ofupdates in the software, Sorpas 3D can be auseful tool in the process planning industry inorder to decrease process times and materialcosts and improve the weld quality in thefuture.</p>

corrected abstract:
<p>Resistance spot welding (RSW) is the dominant joining technology in the automotive industry. This is due to its low costs and high efficiency. Other advantages with RSW is the high ability for automation, low consumption of energy, lack of need for added materials and low degree of pollution, no expensive equipment or education of personal compared to arc welding and laser welding. A modern automobile contains approximately 4000 resistance spot welds, which is why it is of great interest to be able to predict the properties of the resistance spot welds. The most important measurement used to ensure the quality of the weld is the nugget size, as it correlates to the welds mechanical strength. This is usually measured by destructive testing, and the most common method is the coach peel test. This test is performed by manually peeling the specimen and then measuring the largest and smallest nugget diameter. It is also possible to perform non-destructive testing on resistance spot welds with both ultrasonic and x-ray tests, however none of these methods have the same accuracy as the destructive methods and they are cumbersome to use in large-scale. To improve the efficiency and lower the cost for the optimization of the welding parameters, simulation tools have been developed. There are both 2D- and 3D-simulation software to model the RSW process. When the spot welds are simulated with 2D or 2D axis-symmetry, the number of elements is lower compared to a full 3D model, which reduces the computation times. The disadvantages with the 2D model are the inabilities to model misalignments or other asymmetrical geometries. In contrast, the 3D-simulations are not limited by these factors, and they are also capable of modeling the shunt effects occurring when a weld is placed close to a previous weld.</p><p>The aims of this thesis was to evaluate such a 3D-simulation software, Sorpas 3D, and its potential to be used in industrial process planning, and to assess the software’s usefulness for both simple and more complex cases.</p><p>The results from this work show a good correspondence between the simulations and the physical tests. However, in order to achieve these results a number of modifications in the material properties were required. Another critical limitation in the software is that no expulsion criterion is implemented. Considering the possibility that the problems can be solved with a number of updates in the software, Sorpas 3D can be a useful tool in the process planning industry in order to decrease process times and material costs and improve the weld quality in the future.</p>
----------------------------------------------------------------------
In diva2:1880451 abstract is:
<p>Positron emission tomography (PET) is a nuclear medicine imaging techniquethat uses radiotracers to visualize processes like metabolism and perfusion. Theradiotracer emits positrons, which collide with shell electrons of the atomsthat make up the surrounding tissue. Such a collision produces two gammarayphotons, emitted roughly 180 degrees apart [1]. PET captures thesephotons using a cylindrical arrangement of detectors. When two photons aredetected simultaneously by different detectors, it registers as a line of response(LOR). These LORs are then pre-processed into a sinogram. A mathematicalreconstruction method is used to computationally recover the 3D distribution ofthe radiotracer (activity map) from the sinogram. However, genuine LORs can becorrupted by false LORs that come from scattering, random events, and spuriousevents. Mitigating these in reconstruction algorithms is essential for improvingPET imaging accuracy and reliability.This paper explores the theoretical foundation of the Time of Flight (TOF) SingleScatter Simulation (SSS) model by Watson (2007) [2]. It also includes a Pythonimplementation of the MATLAB code associated with [2]. The model modelsCompton scattering to accurately estimate scattered photons in PET.Incorporating TOF data into the SSS model improves estimation accuracy, albeitat the cost of increased computational time. To expedite computations, thealgorithm was simplified by restricting operations to a subset of rings anddetectors and by pre-processing images through cropping and downscaling.Interpolation fills in missing data, ensuring complete estimation.The outcome of this project is a Python implementation that exhibited a strongcorrelation with the estimates obtained using the MATLAB implementation. Anotable issue arose during the comparison between the main components ofthe SSS algorithm in Python and MATLAB. The Euclidean norm between theresults from these two implementations was significant, indicating that they wereon different scales. Nevertheless, both implementations accurately predictedthe scatter in the same locations and relative magnitudes, despite the scalediscrepancy. Investigation into the discrepancy’s cause is ongoing, but theproject demonstrates the feasibility of implementing the TOF SSS algorithm inPython.</p>


corrected abstract:
<p>Positron emission tomography (PET) is a nuclear medicine imaging technique that uses radiotracers to visualize processes like metabolism and perfusion. The radiotracer emits positrons, which collide with shell electrons of the atoms that make up the surrounding tissue. Such a collision produces two gamma-ray photons, emitted roughly 180 degrees apart [1]. PET captures these photons using a cylindrical arrangement of detectors. When two photons are detected simultaneously by different detectors, it registers as a line of response (LOR). These LORs are then pre-processed into a sinogram. A mathematical reconstruction method is used to computationally recover the 3D distribution of the radiotracer (activity map) from the sinogram. However, genuine LORs can be corrupted by false LORs that come from scattering, random events, and spurious events. Mitigating these in reconstruction algorithms is essential for improving PET imaging accuracy and reliability.</p><p>This paper explores the theoretical foundation of the Time of Flight (TOF) Single Scatter Simulation (SSS) model by Watson (2007) [2]. It also includes a Python implementation of the MATLAB code associated with [2]. The model models Compton scattering to accurately estimate scattered photons in PET.</p><p>Incorporating TOF data into the SSS model improves estimation accuracy, albeit at the cost of increased computational time. To expedite computations, the algorithm was simplified by restricting operations to a subset of rings and detectors and by pre-processing images through cropping and downscaling. Interpolation fills in missing data, ensuring complete estimation.</p><p>The outcome of this project is a Python implementation that exhibited a strong correlation with the estimates obtained using the MATLAB implementation. A notable issue arose during the comparison between the main components of the SSS algorithm in Python and MATLAB. The Euclidean norm between the results from these two implementations was significant, indicating that they were on different scales. Nevertheless, both implementations accurately predicted the scatter in the same locations and relative magnitudes, despite the scale discrepancy. Investigation into the discrepancy’s cause is ongoing, but the project demonstrates the feasibility of implementing the TOF SSS algorithm in Python.</p>
----------------------------------------------------------------------
In diva2:1110752 - error in title:
"Identification and modelling of noise sources on a realisticnose landing gear using phased array methods applied tocomputational data"
==>
"Identification and modelling of noise sources on a realistic nose landing gear using phased array methods applied to computational data"


abstract is:
<p>Due to the recent development of quieter turbofan engines, airframe noise has started to emerge asthe most important noise source. This is particularly true during the approach/landing phase, whenthe engines are operated at low-thrust levels. In order to meet future noise level regulations, thecharacterization and subsequent reduction of landing gear induced noise is necessary. Wind-tunnelaeroacoustic tests have always been the favoured method for assessing and studying the noise generatedby landing gears, but their prohibitive cost has steered the attention towards numerical methods.Since direct flow noise simulations are still too demanding in computer resources, there is astrong interest in developing coupled CFD-CAA simulations as a tool to model and identify flownoise sources. More recently, they have been coupled with phased array methods in order to conductaeroacoustic studies on scaled-down, or simplified, aircraft components. This project investigates theaerodynamic sound sources on a realistic nose landing gear using numerical phased array methods,based on array data extracted from compressible Detached Eddy Simulations of the flow. Assumingmonopole and dipole modes of propagation, the sound sources are identified in the source regionthrough beamforming approaches: conventional beamforming, dual linear programming (dual-LP)deconvolution, orthogonal beamforming and CLEAN-SC. To assess the accuracy of the employedmethods, beamforming maps from flyover, sideline and forward point of views are obtained andcompared to experimental ones originating from wind-tunnel experiments performed on the samenose landing gear configuration by industrial and academic partners of the ALLEGRA project. Anarray design metric is defined to quantitatively assess the fitness of the employed arrays with respectto the different frequencies and distances separating the beamforming and array planes. A geneticalgorithm based on the Differential Evolution method is used to generate optimized arrays for selectedfrequencies in order to reduce the computational size of the problems solved. The modelledsources are used to generate far-field spectra which are subsequently compared to the ones obtainedwith the FfowcsWilliams and Hawkings acoustic analogy. The results show a good concordance betweenthe numerical phased array beamforming maps and the experimental ones, and a good matchbetween the far-field spectra up to a certain frequency threshold corresponding to the quality of themesh used. The presence of specific noise sources has been validated and their contribution to theoverall generated noise has been quantified. The results obtained demonstrate the potential of numericalphased array methods as a legitimate tool for aeroacoustic simulations in general and as atool to gain insight into the noise generation mechanisms of landing gear components in particular.</p>



corrected abstract:
<p>Due to the recent development of quieter turbofan engines, airframe noise has started to emerge as the most important noise source. This is particularly true during the approach/landing phase, when the engines are operated at low-thrust levels. In order to meet future noise level regulations, the characterization and subsequent reduction of landing gear induced noise is necessary. Wind-tunnel aeroacoustic tests have always been the favoured method for assessing and studying the noise generated by landing gears, but their prohibitive cost has steered the attention towards numerical methods. Since direct flow noise simulations are still too demanding in computer resources, there is a strong interest in developing coupled CFD-CAA simulations as a tool to model and identify flow noise sources. More recently, they have been coupled with phased array methods in order to conduct aeroacoustic studies on scaled-down, or simplified, aircraft components. This project investigates the aerodynamic sound sources on a realistic nose landing gear using numerical phased array methods, based on array data extracted from compressible Detached Eddy Simulations of the flow. Assuming monopole and dipole modes of propagation, the sound sources are identified in the source region through beamforming approaches: conventional beamforming, dual linear programming (dual-LP) deconvolution, orthogonal beamforming and CLEAN-SC. To assess the accuracy of the employed methods, beamforming maps from flyover, sideline and forward point of views are obtained and compared to experimental ones originating from wind-tunnel experiments performed on the same nose landing gear configuration by industrial and academic partners of the ALLEGRA project. An array design metric is defined to quantitatively assess the fitness of the employed arrays with respect to the different frequencies and distances separating the beamforming and array planes. A genetic algorithm based on the Differential Evolution method is used to generate optimized arrays for selected frequencies in order to reduce the computational size of the problems solved. The modelled sources are used to generate far-field spectra which are subsequently compared to the ones obtained with the Ffowcs Williams and Hawkings acoustic analogy. The results show a good concordance between the numerical phased array beamforming maps and the experimental ones, and a good match between the far-field spectra up to a certain frequency threshold corresponding to the quality of the mesh used. The presence of specific noise sources has been validated and their contribution to the overall generated noise has been quantified. The results obtained demonstrate the potential of numerical phased array methods as a legitimate tool for aeroacoustic simulations in general and as a tool to gain insight into the noise generation mechanisms of landing gear components in particular.</p>
----------------------------------------------------------------------
In diva2:618564 merged words and missing ligatures - note the corrected abstract needs to be checked agains the original abstract in the thesis - but there is no full text in DiVA.

abstract is:
<p>Due to progress in CFD (Computational Fluid Dynamics), it is now possible to compute and analyzesteady ow and phenomena for turbomachine design. Unsteady instability predictions are important tocertify that a turbomachine will not encounter high vibration levels in operation. Flutter is one of the mostcommon fan instabilities. Thus, the fan design is bound to respect a given Flutter Margin. It guaranteesa certain operation envelope for the engine and its fan, refered as operability. The operation envelope ofan engine is dened in the fan map mass ow rate - pressure ratio as the space in which the engine can berun in during operation. The fan map is made of isovelocities. An isovelocity is a line described by varyingmass ow rate and keeping constant the rotational speed. This domain is bounded by phenomena such asrotating stall, surge, utter, etc which are hazardous for engine mechanical integrity. Fig. 1 highlights howan operating envelope is bounded. When operation envelope is too small, the fan blade geometry needsto be modied to improve its utter behavior and therefore increase the size of the envelope. Eciency,pressure ratio and operability can be strongly impacted by the changes made. Therefore design parametersinuencing utter must be precisely spotted. This can be done if mechanisms which trigger utter are wellunderstood. Thus the blade can be reshaped to lower the contribution of a given phenomenon. However,when a phenomenon is identied, one should quantify its contribution to the globally stable or unstablebehavior. In fact, to reduce the geometrical changes and therefore consequences on operability, one shouldact on the most critical phenomenon.The study has been performed on a single fan blade with dierent congurations of back pressure androtational speed. Consequently, two kinds of utter are investigated : stall utter and transonic utter. Therst one occurs at low rotational speed. It corresponds to zone 1 in Fig. 1. It is commonly driven by owseparation. As described in,1 separation on the suction side of a blade can be responsible for utter. Article1shows that unsteady pressure and blade motion are out of phase in the separated zone. Furthermore, studiesin2 reveal that traveling Mach waves on the blade surface can destabilize it. It depends on the phase betweenthe waves and the blade motion. Acoustic interference is also studied in.1 Transonic utter occurs at higherrotational speed when the blade is shocked. It corresponds to zone 2 in Fig. 1. There are two main sourcesthat destabilize the blade: interaction between the shock waves and the boundary layer and the shock waveoscillation as presented in.3 That study divides the prole into four parts. Each part corresponds to a givenmechanism: supersonic part (stabilizing), shocked part (both stabilizing and destabilizing if the shock wavesoscillates), downstream the shock (destabilizing due to separation) on the suction side and the pressure side(stabilizing).The objective of this paper is not only to identify mechanisms responsible for utter at low and high rotationalspeed but also to follow their evolution along an isovelocity. A global approach from the mechanismsidentication to the quantication of the phenomenon is then described.The critical mechanisms responsiblefor fan blade utter for a given conguration can be pointed out.</p>

corrected abstract:
<p>Due to progress in CFD (Computational Fluid Dynamics), it is now possible to compute and analyze steady flow and phenomena for turbomachine design. Unsteady instability predictions are important to certify that a turbomachine will not encounter high vibration levels in operation. Flutter is one of the most common fan instabilities. Thus, the fan design is bound to respect a given Flutter Margin. It guarantees a certain operation envelope for the engine and its fan, refered as operability. The operation envelope of an engine is defined in the fan map mass flow rate - pressure ratio as the space in which the engine can be run in during operation. The fan map is made of isovelocities. An isovelocity is a line described by varying mass flow rate and keeping constant the rotational speed. This domain is bounded by phenomena such as rotating stall, surge, flutter, etc which are hazardous for engine mechanical integrity. Fig. 1 highlights how an operating envelope is bounded. When operation envelope is too small, the fan blade geometry needs to be modified to improve its flutter behavior and therefore increase the size of the envelope. Efficiency, pressure ratio and operability can be strongly impacted by the changes made. Therefore design parameters influencing flutter must be precisely spotted. This can be done if mechanisms which trigger flutter are well understood. Thus the blade can be reshaped to lower the contribution of a given phenomenon. However, when a phenomenon is identied, one should quantify its contribution to the globally stable or unstable behavior. In fact, to reduce the geometrical changes and therefore consequences on operability, one should act on the most critical phenomenon. The study has been performed on a single fan blade with different configurations of back pressure and rotational speed. Consequently, two kinds of flutter are investigated : stall flutter and transonic flutter. The first one occurs at low rotational speed. It corresponds to zone 1 in Fig. 1. It is commonly driven by flow separation. As described in, 1 separation on the suction side of a blade can be responsible for flutter. Article 1 shows that unsteady pressure and blade motion are out of phase in the separated zone. Furthermore, studies in 2 reveal that traveling Mach waves on the blade surface can destabilize it. It depends on the phase between the waves and the blade motion. Acoustic interference is also studied in .1 Transonic flutter occurs at higher rotational speed when the blade is shocked. It corresponds to zone 2 in Fig. 1. There are two main sources that destabilize the blade: interaction between the shock waves and the boundary layer and the shock wave oscillation as presented in.3 That study divides the prole into four parts. Each part corresponds to a given mechanism: supersonic part (stabilizing), shocked part (both stabilizing and destabilizing if the shock waves oscillates), downstream the shock (destabilizing due to separation) on the suction side and the pressure side(stabilizing). The objective of this paper is not only to identify mechanisms responsible for flutter at low and high rotational speed but also to follow their evolution along an isovelocity. A global approach from the mechanisms identication to the quantication of the phenomenon is then described. The critical mechanisms responsible for fan blade flutter for a given configuration can be pointed out.</p>
----------------------------------------------------------------------
In diva2:1528140 abstract is:
<p>Due to the accelerating need for decarbonization in the shipping sector, wind-assisted cargo shipsare able to play a key role in achieving the IMO 2050 targets on reducing the total annual GHGemissions from international shipping by at least 50%. The aim of this Master’s Thesis project is todevelop a Performance Prediction Program for wind-assisted cargo ships to contribute knowledgeon the performance of this technology. The three key characteristics of this model are its genericstructure, the small number of input data needed and its ability to predict the performance of threepossible Wind-Assisted Propulsion Systems (WAPS): Rotor Sails, Rigid Wing Sails and DynaRigs.It is a fast and easy tool able to predict, to a good level of accuracy and really low computationaltime, the performance of any commercial ship with these three WAPS options installed with onlythe main particulars and general dimensions as input data.The hull and WAPS models predict the forces and moments, which the program balances in 6degrees of freedom to predict the theoretical sailing performance of the wind-assisted cargo shipwith the specified characteristics for various wind conditions. The model is able to play with differentoptimization objectives. This includes maximizing sailing speed if a VPP is run or maximizingtotal power savings if it is a PPP. The program is based on semi-empirical methods and a WAPSaerodynamic database created from published data on lift and drag coefficients. All WAPS datacan be interpolated with the aim to scale to different sizes and configurations such as number ofunits and different aspect ratios.A model validation is carried out to evaluate its reliability. The model results are compared withthe real sailing data of the Long Range 2 (LR2) class tanker vessel, the Maersk Pelican, whichwas recently fitted with two 30 meter high Rotor Sails; and results from another performanceprediction program. In general, the two performance prediction programs and some of the realsailing measurements show good agreement. However, for some downwind sailing conditions, theperformance predictions are more conservative than the measured values.Results showing and comparing power savings, thrust and side force coefficients for the differentWAPS are also presented and discussed. The results of this Master’s Thesis project show howWind-Assisted Propulsion Systems have high potential in playing a key role in the decarbonizationof the shipping sector. WAPS can prove substantial power, fuel, cost, and emissions savings.Tankers and bulk-carriers are specially suitable for wind propulsion thanks to their available deckspace and relatively low design speeds.The Performance Prediction Program for wind-assisted cargo ships developed in this Master’sThesis shows promising results with a good level of accuracy despite its generic and small numberof input data. It can be a useful tool in early project stages to quickly and accurately assess thepotential and performance of WAPS systems.</p>


corrected abstract:
<p>Due to the accelerating need for decarbonization in the shipping sector, wind-assisted cargo ships are able to play a key role in achieving the IMO 2050 targets on reducing the total annual GHG emissions from international shipping by at least 50%. The aim of this Master’s Thesis project is to develop a Performance Prediction Program for wind-assisted cargo ships to contribute knowledge on the performance of this technology. The three key characteristics of this model are its generic structure, the small number of input data needed and its ability to predict the performance of three possible Wind-Assisted Propulsion Systems (WAPS): Rotor Sails, Rigid Wing Sails and DynaRigs. It is a fast and easy tool able to predict, to a good level of accuracy and really low computational time, the performance of any commercial ship with these three WAPS options installed with only the main particulars and general dimensions as input data.</p><p>The hull and WAPS models predict the forces and moments, which the program balances in 6 degrees of freedom to predict the theoretical sailing performance of the wind-assisted cargo ship with the specified characteristics for various wind conditions. The model is able to play with different optimization objectives. This includes maximizing sailing speed if a VPP is run or maximizing total power savings if it is a PPP. The program is based on semi-empirical methods and a WAPS aerodynamic database created from published data on lift and drag coefficients. All WAPS data can be interpolated with the aim to scale to different sizes and configurations such as number of units and different aspect ratios.</p><p>A model validation is carried out to evaluate its reliability. The model results are compared with the real sailing data of the Long Range 2 (LR2) class tanker vessel, the Maersk Pelican, which was recently fitted with two 30 meter high Rotor Sails; and results from another performance prediction program. In general, the two performance prediction programs and some of the real sailing measurements show good agreement. However, for some downwind sailing conditions, the performance predictions are more conservative than the measured values.</p><p>Results showing and comparing power savings, thrust and side force coefficients for the different WAPS are also presented and discussed. The results of this Master’s Thesis project show how Wind-Assisted Propulsion Systems have high potential in playing a key role in the decarbonization of the shipping sector. WAPS can prove substantial power, fuel, cost, and emissions savings. Tankers and bulk-carriers are specially suitable for wind propulsion thanks to their available deck space and relatively low design speeds.</p><p>The Performance Prediction Program for wind-assisted cargo ships developed in this Master’s Thesis shows promising results with a good level of accuracy despite its generic and small number of input data. It can be a useful tool in early project stages to quickly and accurately assess the potential and performance of WAPS systems.</p>
----------------------------------------------------------------------
In diva2:1083484 abstract is:
<p>Cable-stayed bridges have become very popular over the last ve decades due totheir aesthetic appeal, structural eciency, the limited amount of material usageand nancial benets. The rapid increase of new techniques creating longer spans,slender decks and more spectacular design has given rise to a major concern ofthe dynamic behavior of cable-stayed bridges. This has resulted in a more carefulmodelling procedure that will represent the reality in the most particular way. Amodel is simply an approximation of the reality, thus it is important to establishwhat simplications and approximations that are reasonable to make in order forthe model to be as accurate as possible.The Millau Viaduct is a cable-stayed bridge unique of its kind. At the time thatit was built it was breaking many records: span length, height of deck above thefoundations and the short construction time in just three years. Due to the slendernessof the structure, the extreme height and the location in a deep valley, theviaduct is naturally subjected to external loads. This thesis attempts to describea performed dynamic nonlinear analysis of two models of the Millau Viaduct usingthe FEA packages SAP2000 and BRIGADE/Plus. The models have been renedin order to be compared between the programs and to the reality i.e. the measuredmode shapes and frequencies obtained from reports.The viaduct required many specically designed solutions in order to obtain theelegance and the aesthetic appeal. Approximations in geometry has been essentialdue to the many details that the viaduct consists of, but the details are nonethelessimportant to capture to get the structural mechanics correct. The support conditionshas been considered as important as these were designed to allow for movementthat were caused by a combination of the external loads and the slenderness of thestructure. The most critical support conditions were the deck-pier connection inwhich the piers are split into two columns equipped with spherical bearings allowingfor angular rotation. The two shafts were modelled by one single column and thespherical bearings were simulated by creating two alternative models; one assignedwith a pinned constraint to allow for the angular rotation and the second, since thissupport condition is in fact rigid has been assigned as xed.The SAP and BRIGADE models showed to be consistent with each other, thoughthe beam theories, Euler-Bernoulli were applied to the SAP model and Timoshenkoin BRIGADE. The alternative models with the dierent constraints generated fairresults yet diers signicantly from each other. Alternative approaches towards themodelling have been addressed in the conclusions.</p>


corrected abstract:
<p>Cable-stayed bridges have become very popular over the last five decades due to their aesthetic appeal, structural efficiency, the limited amount of material usage and financial benefits. The rapid increase of new techniques creating longer spans, slender decks and more spectacular design has given rise to a major concern of the dynamic behavior of cable-stayed bridges. This has resulted in a more careful modelling procedure that will represent the reality in the most particular way. A model is simply an approximation of the reality, thus it is important to establish what simplifications and approximations that are reasonable to make in order for the model to be as accurate as possible.</p><p>The Millau Viaduct is a cable-stayed bridge unique of its kind. At the time that it was built it was breaking many records: span length, height of deck above the foundations and the short construction time in just three years. Due to the slenderness of the structure, the extreme height and the location in a deep valley, the viaduct is naturally subjected to external loads. This thesis attempts to describe a performed dynamic nonlinear analysis of two models of the Millau Viaduct using the FEA packages SAP2000 and BRIGADE/Plus. The models have been refined in order to be compared between the programs and to the reality i.e. the measured mode shapes and frequencies obtained from reports.</p><p>The viaduct required many specifically designed solutions in order to obtain the elegance and the aesthetic appeal. Approximations in geometry has been essential due to the many details that the viaduct consists of, but the details are nonetheless important to capture to get the structural mechanics correct. The support conditions has been considered as important as these were designed to allow for movement that were caused by a combination of the external loads and the slenderness of the structure. The most critical support conditions were the deck-pier connection in which the piers are split into two columns equipped with spherical bearings allowing for angular rotation. The two shafts were modelled by one single column and the spherical bearings were simulated by creating two alternative models; one assigned with a pinned constraint to allow for the angular rotation and the second, since this support condition is in fact rigid has been assigned as fixed.</p><p>The SAP and BRIGADE models showed to be consistent with each other, though the beam theories, Euler-Bernoulli were applied to the SAP model and Timoshenko in BRIGADE. The alternative models with the different constraints generated fair results yet differs significantly from each other. Alternative approaches towards the modelling have been addressed in the conclusions.</p>
----------------------------------------------------------------------
In diva2:876188 abstract is:
<p>Every computational fluid dynamics engineer deals with a never ending story – limitedcomputer resources. In computational fluid dynamics there is practically never enoughcomputer power. Limited computer resources lead to long calculation times which result inhigh costs and one of the main reasons is that large quantity of elements are needed in acomputational mesh in order to obtain accurate and reliable results.Although there exist established meshing approaches for the Siemens 4th generation DLEburner, mesh dependency has not been fully evaluated yet. The main goal of this work istherefore to better optimize accuracy versus cell count for this particular burner intended forsimulation of air/gas mixing where eddy-viscosity based turbulence models are employed.Ansys Fluent solver was used for all simulations in this work. For time effectivisationpurposes a 30° sector model of the burner was created and validated for the meshconvergence study. No steady state solutions were found for this case therefore timedependent simulations with time statistics sampling were employed. The mesh convergencestudy has shown that a coarse computational mesh in air casing of the burner does not affectflow conditions downstream where air/gas mixing process is taking place and that a majorpart of the combustion chamber is highly mesh independent. A large reduction of cell count inthose two parts is therefore allowed. On the other hand the RPL (Rich Pilot Lean) and thepilot burner turned out to be highly mesh density dependent. The RPL and the Pilot burnerneed to have significantly more refined mesh as it has been used so far with the establishedmeshing approaches. The mesh optimization has finally shown that at least as accurate resultsof air/gas mixing results may be obtained with 3x smaller cell count. Furthermore it has beenshown that significantly more accurate results may be obtained with 60% smaller cell count aswith the established meshing approaches.A short mesh study of the Siemens 3rd generation DLE burner in ignition stage of operationwas also performed in this work. This brief study has shown that the established meshingapproach for air/gas mixing purposes is sufficient for use with Ansys Fluent solver whilecertain differences were discovered when comparing the results obtained with Ansys Fluentagainst those obtained with Ansys CFX solver. Differences between Fluent and CFX solverwere briefly discussed in this work as identical simulation set up in both solvers producedslightly different results. Furthermore the obtained results suggest that Fluent solver is lessmesh dependent as CFX solver for this particular case.</p>


corrected abstract:
<p>Every computational fluid dynamics engineer deals with a never ending story – limited computer resources. In computational fluid dynamics there is practically never enough computer power. Limited computer resources lead to long calculation times which result in high costs and one of the main reasons is that large quantity of elements are needed in a computational mesh in order to obtain accurate and reliable results.</p><p>Although there exist established meshing approaches for the Siemens 4<sup>th</sup> generation DLE burner, mesh dependency has not been fully evaluated yet. The main goal of this work is therefore to better optimize accuracy versus cell count for this particular burner intended for simulation of air/gas mixing where eddy-viscosity based turbulence models are employed. Ansys Fluent solver was used for all simulations in this work. For time effectivisation purposes a 30° sector model of the burner was created and validated for the mesh convergence study. No steady state solutions were found for this case therefore time dependent simulations with time statistics sampling were employed. The mesh convergence study has shown that a coarse computational mesh in air casing of the burner does not affect flow conditions downstream where air/gas mixing process is taking place and that a major part of the combustion chamber is highly mesh independent. A large reduction of cell count in those two parts is therefore allowed. On the other hand the RPL (Rich Pilot Lean) and the pilot burner turned out to be highly mesh density dependent. The RPL and the Pilot burner need to have significantly more refined mesh as it has been used so far with the established meshing approaches. The mesh optimization has finally shown that at least as accurate results of air/gas mixing results may be obtained with 3x smaller cell count. Furthermore it has been shown that significantly more accurate results may be obtained with 60% smaller cell count as with the established meshing approaches.</p><p>A short mesh study of the Siemens 3<sup>rd</sup> generation DLE burner in ignition stage of operation was also performed in this work. This brief study has shown that the established meshing approach for air/gas mixing purposes is sufficient for use with Ansys Fluent solver while certain differences were discovered when comparing the results obtained with Ansys Fluent against those obtained with Ansys CFX solver. Differences between Fluent and CFX solver were briefly discussed in this work as identical simulation set up in both solvers produced slightly different results. Furthermore the obtained results suggest that Fluent solver is less mesh dependent as CFX solver for this particular case.</p>
----------------------------------------------------------------------
In diva2:854573 abstract is:
<p>Populations in big cities keep a constant inflation. It is estimated that 60% of thepopulation will move into a big city in the next 20 years, regarding this reason there arehigh demands for new solutions to the modern transportation system. A means ofmeasure that a lot of industrialized countries have implemented are high speed railwaytrains. The railway train covers the transportation needs. What would happen if amaglev train would be implemented instead?</p><p>The purpose with this report is to get an estimate if maglev trains can be a superiorsolution than the conventional railway train. In order to proceed this task, an analyszeswith respect to the Shinkansen N700A and ICE3 within railway as well as TransrapidTR09 and SCMaglev MLX01 within Maglev train was carried out. Aspects such asSafety, energy consumption, environmental impact and cost were the four models thatwere investigated.</p><p>Concerning safety, you could establish that both systems keep a high security standard,both systems have been involved in accidents, which is in a way a positive trait, keepsthe companies developing and improving the security measures for their trains, keepingthem safer. There is no guarantee that collisions or derailing occurs. Regarding energyconsumption, the results were that TR09 and ICE3 consume the same amount of energyat maximum allowed speed as well as Shinkansen N700A consumes considerably lessenergy than MLX01. Considering that energy consumption is proportional with thecarbon dioxide emissions results that N700A contributes to less carbon dioxideemissions than MLX01 as well as the TR09 and ICE3 contributed equally.</p><p>None of this aspects were decisive to demonstration if one system was moreadvantageous than the other, but how does the cost portion differentiate with thedifferent train systems? The result shows that the infrastructure cost for the maglev trainwere extremely high compared to the railway train. Germany (pioneers within themaglev technology) have shut down there maglev projects. Difference regardingoperation cost were not significant for the Transrapid and the Intercity-Express trains.Observing the discovered information on vehicle cost, a complete vehicle cost analyzescould not be established due to the lack of information on the vehicle cost on SCMaglevML0X1. With respect to the other three high speed trains, the Shinkasen N700A was themost expensive with a vehicle cost on $44 millions per unit, followed by the Transrapidwith a vehicle cost on $12.9 millions and cheapest train is the ICE3, costing $3.7millions per vehicle.</p><p>Keeping in thought all gathered information and data, the conclusion drawn is that it isnot profitable to construct maglev train with modern technology. If technical aspectsimproves further so that specifically the infrastructure cost decreases considerably thena maglev system could be a worthy solution for the foreseeable future.</p>

corrected abstract:
<p>Populations in big cities keep a constant inflation. It is estimated that 60% of the population will move into a big city in the next 20 years, regarding this reason there are high demands for new solutions to the modern transportation system. A means of measure that a lot of industrialized countries have implemented are high speed railway trains. The railway train covers the transportation needs. What would happen if a maglev train would be implemented instead?</p><p>The purpose with this report is to get an estimate if maglev trains can be a superior solution than the conventional railway train. In order to proceed this task, an analyszes with respect to the Shinkansen N700A and ICE3 within railway as well as Transrapid TR09 and SCMaglev MLX01 within Maglev train was carried out. Aspects such as Safety, energy consumption, environmental impact and cost were the four models that were investigated.</p><p>Concerning safety, you could establish that both systems keep a high security standard, both systems have been involved in accidents, which is in a way a positive trait, keeps the companies developing and improving the security measures for their trains, keeping them safer. There is no guarantee that collisions or derailing occurs. Regarding energy consumption, the results were that TR09 and ICE3 consume the same amount of energy at maximum allowed speed as well as Shinkansen N700A consumes considerably less energy than MLX01. Considering that energy consumption is proportional with the carbon dioxide emissions results that N700A contributes to less carbon dioxide emissions than MLX01 as well as the TR09 and ICE3 contributed equally.</p><p>None of this aspects were decisive to demonstration if one system was more advantageous than the other, but how does the cost portion differentiate with the different train systems? The result shows that the infrastructure cost for the maglev train were extremely high compared to the railway train. Germany (pioneers within the maglev technology) have shut down there maglev projects. Difference regarding operation cost were not significant for the Transrapid and the Intercity-Express trains. Observing the discovered information on vehicle cost, a complete vehicle cost analyzes could not be established due to the lack of information on the vehicle cost on SCMaglev ML0X1. With respect to the other three high speed trains, the Shinkasen N700A was the most expensive with a vehicle cost on $44 millions per unit, followed by the Transrapid with a vehicle cost on $12.9 millions and cheapest train is the ICE3, costing $3.7 millions per vehicle.</p><p>Keeping in thought all gathered information and data, the conclusion drawn is that it is not profitable to construct maglev train with modern technology. If technical aspects improves further so that specifically the infrastructure cost decreases considerably then a maglev system could be a worthy solution for the foreseeable future.</p>
----------------------------------------------------------------------
In diva2:1741184 abstract is:
<p>Accurate estimations of the reverberation time of furnished office spacesis essential as an acoustic consultant. A time efficient way to predict thefurniture’s effect is to investigate it by software modelling. The room acousticsoftware Odeon is suitable for doing this. In today’s geometrical room acousticmodellers (such as Odeon) a parameter called the scattering factor was usedwhich was of large importance in this thesis. This thesis set out to investigatehow well Odeon predicts the reverberation time in smaller and larger officespaces given that the correct scattering factor for each type of furniturecould be established. The method for the investigation was to perform fieldmeasurements in two office spaces of different sizes and geometry. Thenuse different setups of furniture to examine the furniture’s effect on thereverberation time connected to their sound scattering properties. The modelwas designed with 2D objects in SketchUp and exported to Odeon. A referencevalue for a pair of measurement setups was obtained by using the ratio of thetotal reverberation time (octave bands 125 Hz to 4 kHz) of the rooms. Thisratio was used as a target in Odeon for the same simulated pair of room setups.The scattering factor was adjusted in increments of the specific furniture usedin the setup until an optimized fit was reached. These steps were carried outfor each combination of setups. Afterwards the simulations were comparedto the measured and calculated reverberation times using Sabine’s formulaand Arau-Puchades formula. It was possible to establish specific scatteringfactors for the furniture types within Odeon although their accuracy was hardto determine. The resulting reverberation times from the Odeon simulationsof the large and small spaces were not closer to the measured reverberationtime than the calculated ones to any distinct degree. It’s worth noting that allof these calculations are dependant on ideally diffuse circumstances which theactual rooms rarely are. This is why they tend to underestimate the room’sreverberation time. The main goal of this thesis is to an extent fulfilled,although maybe not with as great future utility as hoped. The scattering factorscorrespond to each other independently of what room was being modelled. Inan isolated framework of Odeon, the attained scattering factors may be of use,especially if the measured room can be assumed ideally diffuse.</p>

corrected abstract:
o<p>Accurate estimations of the reverberation time of furnished office spaces is essential as an acoustic consultant. A time efficient way to predict the furniture’s effect is to investigate it by software modelling. The room acoustic software Odeon is suitable for doing this. In today’s geometrical room acoustic modellers (such as Odeon) a parameter called the scattering factor was used which was of large importance in this thesis. This thesis set out to investigate how well Odeon predicts the reverberation time in smaller and larger office spaces given that the correct scattering factor for each type of furniture could be established. The method for the investigation was to perform field measurements in two office spaces of different sizes and geometry. Then use different setups of furniture to examine the furniture’s effect on the reverberation time connected to their sound scattering properties. The model was designed with 2D objects in SketchUp and exported to Odeon. A reference value for a pair of measurement setups was obtained by using the ratio of the total reverberation time (octave bands 125 Hz to 4 kHz) of the rooms. This ratio was used as a target in Odeon for the same simulated pair of room setups. The scattering factor was adjusted in increments of the specific furniture used in the setup until an optimized fit was reached. These steps were carried out for each combination of setups. Afterwards the simulations were compared to the measured and calculated reverberation times using Sabine’s formula and Arau-Puchades formula. It was possible to establish specific scattering factors for the furniture types within Odeon although their accuracy was hard to determine. The resulting reverberation times from the Odeon simulations of the large and small spaces were not closer to the measured reverberation time than the calculated ones to any distinct degree. It’s worth noting that all of these calculations are dependant on ideally diffuse circumstances which the actual rooms rarely are. This is why they tend to underestimate the room’s reverberation time. The main goal of this thesis is to an extent fulfilled, although maybe not with as great future utility as hoped. The scattering factors correspond to each other independently of what room was being modelled. In an isolated framework of Odeon, the attained scattering factors may be of use, especially if the measured room can be assumed ideally diffuse.</p>
----------------------------------------------------------------------
In diva2:1644922 abstract is:
<p>This thesis studies the feasibility of integrating the novelStructural Battery (SB)[1] into the airframe of a UnmannedAerial Vehicle (UAV). The potential advantages in terms ofmass, range and endurance are studied.The aircraft performance is analysed using conventionalflight mechanics, modelled in Matlab and Xfoil. The structureis designed and analysed using composite laminate theoryand beam theory in conjunction with verification in AnsysMechanical. An iterative procedure was used to arriveat a design that satisfied the set structural- and flight requirements.The currently demonstrated structural battery has a specificenergy density of 23.8Wh/kg, an elastic modulus of25GPa and tensile strength of at least 300MPa.[1]The laminae properties used in this master thesis were estimatedusing the Reuss and Voigt model combined with theRule of Mixtures (RoM). A quasi isotropic SB laminate wasmodelled according to the previous structural requirementsand assumed material properties. It yielded an elastic modulusof 54GPa. In order to simplify the analysis the energyand stiffness were decoupled. The SB was assigned a specificenergy of 23.8Wh/kg and 60.6Wh/kg according to thevalues measured and estimated previously[1].A SB with a tensile modulus of 54GPa and specific energyof 24Wh/kg was shown not to be beneficial to integrate intothe primary aircraft structure. The designed SB yieldeda reduction in flight range of 5.8%. This was shown bycomparing the designed SB with a reference aircraft configuration.The reference configuration uses a conventionalbattery that has a specific energy density of 160Wh/kg andconventional Carbon Fibre Composite (CFC) with an elasticmodulus of 71GPa.It was shown that the integration of the SB modelled wouldbecome beneficial compared to the reference aircraft configurationwhen the SB specific energy exceeds 33Wh/kg.The integration of a structural battery with a specific energyof 60.6Wh/kg yielded a flight range improvement of16.9% compared to the reference aircraft.</p>

corrected abstract:
<p>This thesis studies the feasibility of integrating the novel Structural Battery (SB)[1] into the airframe of a Unmanned Aerial Vehicle (UAV). The potential advantages in terms of mass, range and endurance are studied. The aircraft performance is analysed using conventional flight mechanics, modelled in Matlab and Xfoil. The structure is designed and analysed using composite laminate theory and beam theory in conjunction with verification in Ansys Mechanical. An iterative procedure was used to arrive at a design that satisfied the set structural- and flight requirements.</p><p>The currently demonstrated structural battery has a specific energy density of 23.8 Wh/kg, an elastic modulus of 25GPa and tensile strength of at least 300 MPa.[1]</p><p>The laminae properties used in this master thesis were estimated using the Reuss and Voigt model combined with the Rule of Mixtures (RoM). A quasi isotropic SB laminate was modelled according to the previous structural requirements and assumed material properties. It yielded an elastic modulus of 54 GPa. In order to simplify the analysis the energy and stiffness were decoupled. The SB was assigned a specific energy of 23.8 Wh/kg and 60.6 Wh/kg according to the values measured and estimated previously[1].</p><p>A SB with a tensile modulus of 54 GPa and specific energy of 24 Wh/kg was shown not to be beneficial to integrate into the primary aircraft structure. The designed SB yielded a reduction in flight range of 5.8%. This was shown by comparing the designed SB with a reference aircraft configuration. The reference configuration uses a conventional battery that has a specific energy density of 160 Wh/kg and conventional Carbon Fibre Composite (CFC) with an elastic modulus of 71 GPa.i</p><p>It was shown that the integration of the SB modelled would become beneficial compared to the reference aircraft configuration when the SB specific energy exceeds 33 Wh/kg. The integration of a structural battery with a specific energy of 60.6 Wh/kg yielded a flight range improvement of 16.9% compared to the reference aircraft.</p>
----------------------------------------------------------------------
In diva2:1541213 abstract is:
<p>The objectives of the present project were to set up, optimise and characterise a digitalholographic microscopy (DHM) laboratory set-up designed for the study of eyetissue and to implement and optimise digital data processing and noise reductionroutines. This work is part of a collaborative project aiming to provide quantitativemethods for the in vitro and in vivo characterisation of human corneal transparency.The laboratory set-up is based on a commercial laboratory microscope with zoomfunction (a “macroscope”). In continuation of previous work, we completed and optimised,and extended a software for holographic signal processing and numericalpropagation of the wavefront.To characterise the set-up and quantify its performances for standard operationand in its DHM configuration, we compare the magnification and resolution to theoreticalvalues for a given set of parameters. We determined the magnification factorand the rotation angle between the object and camera planes. With a laser wavelengthof 532 nm, a x1 objective and a zoom setting of x2.9 (which corresponds to aplane sample wavefront), we measured a magnification of 1.68. With the same parameters,we measure a holographic resolution of about 11 m. The wavefront phasecould be determined with a precision of a fraction of the wavelength.We subsequently performed analysis of the relative contribution of coherent noiseand implemented and evaluated several noise reduction routines. While the impactof coherent noise remained visible in the amplitude image, interferometric precisionwas obtained for the phase of the wavefront and the set-up was considered qualifiedfor its intended use for corneal characterisation.A first test measurement was performed on primate cornea.Subsequent work will address the further quantitative characterisation of the setupfor the full set of parameters (objectives, zoom positions, wavelengths), test measurementson samples with known transmission and light scattering properties (e.g.solutions of PMMA beads) and the comparison of the results with the predictions ofa theoretical model, and measurements on animal and human tissue.</p>


corrected abstract:
<p>The objectives of the present project were to set up, optimise and characterise a digital holographic microscopy (DHM) laboratory set-up designed for the study of eye tissue and to implement and optimise digital data processing and noise reduction routines. This work is part of a collaborative project aiming to provide quantitative methods for the in vitro and in vivo characterisation of human corneal transparency.</p><p>The laboratory set-up is based on a commercial laboratory microscope with zoom function (a “macroscope”). In continuation of previous work, we completed and optimised, and extended a software for holographic signal processing and numerical propagation of the wavefront.</p><p>To characterise the set-up and quantify its performances for standard operation and in its DHM configuration, we compare the magnification and resolution to theoretical values for a given set of parameters. We determined the magnification factor and the rotation angle between the object and camera planes. With a laser wavelength of 532 nm, a x1 objective and a zoom setting of x2.9 (which corresponds to a plane sample wavefront), we measured a magnification of 1.68. With the same parameters, we measure a holographic resolution of about 11 µm. The wavefront phase could be determined with a precision of a fraction of the wavelength.</p><p>We subsequently performed analysis of the relative contribution of coherent noise and implemented and evaluated several noise reduction routines. While the impact of coherent noise remained visible in the amplitude image, interferometric precision was obtained for the phase of the wavefront and the set-up was considered qualified for its intended use for corneal characterisation.</p><p>A first test measurement was performed on primate cornea.</p><p>Subsequent work will address the further quantitative characterisation of the setup for the full set of parameters (objectives, zoom positions, wavelengths), test measurements on samples with known transmission and light scattering properties (e.g. solutions of PMMA beads) and the comparison of the results with the predictions of a theoretical model, and measurements on animal and human tissue.</p>
----------------------------------------------------------------------
In diva2:1334020 abstract is:
<p>In order to keep up with the increasing demand of fuel-efficiency in the transportationindustry, the interest of making the vehicles as lightweight as possible is steadilyincreasing. One of the ways of reducing the weight is to introduce an anisotropicmaterial as Short Fibre Reinforced Polymers (SFRP) as a replacement for structuralparts made out of metals. To meet the modern vehicle design process which strivestowards a more simulation driven workflow, the need for accurate simulations offibre reinforced composites is of importance.This thesis aims to evaluate and find a working process for fatigue analysis of injectionmoulded SFRP components. To evaluate the fatigue analysis procedure anexisting SFRP component has been studied. The component is the front bracket thatmounts the roof air deflector to the roof on Scania trucks. To correlate the fatigue lifeestimation from the fatigue analysis, experiments were performed at ÅF Test Centerin Borlänge.The anisotropic behaviour is modelled using the commercial software Digimat togetherwith an injection simulation provided by Scania, to estimate the fibre orientationand thereby the material behaviour of the SFRP component. The fatigue analysiswas conducted by performing a coupled structural analysis between Digimat-Abaqus and then import the resulting stress- and strain-fields into the fatigue postprocessornCode DesignLife. The stress is then cyclic tested towards experimentallydetermined S-N curves determined in Digimat.Due to restriction of available fatigue data for the plastic in the front bracket, a fatiguematerial model for a plastic containing the same fibres and matrix but witha different fibre amount was implemented. The fatigue data were scaled using theUTS method to get a good characterisation of the real-life material behaviour of theplastic of the front bracket component.From the correlation between the fatigue analysis and performed experiments, itwas shown that the simulated fatigue life was conservative compared to the fatiguelife determined from the experiments. However, the correlation between the fatigueanalysis and experiments is not fully captured but gives a better estimation of thefatigue life compared to performing the fatigue analysis using an isotropic materialmodel.</p>

corrected abstract:
<p>In order to keep up with the increasing demand of fuel-efficiency in the transportation industry, the interest of making the vehicles as lightweight as possible is steadily increasing. One of the ways of reducing the weight is to introduce an anisotropic material as Short Fibre Reinforced Polymers (SFRP) as a replacement for structural parts made out of metals. To meet the modern vehicle design process which strives towards a more simulation driven workflow, the need for accurate simulations of fibre reinforced composites is of importance.</p><p>This thesis aims to evaluate and find a working process for fatigue analysis of injection moulded SFRP components. To evaluate the fatigue analysis procedure an existing SFRP component has been studied. The component is the front bracket that mounts the roof air deflector to the roof on Scania trucks. To correlate the fatigue life estimation from the fatigue analysis, experiments were performed at ÅF Test Center in Borlänge.</p><p>The anisotropic behaviour is modelled using the commercial software Digimat together with an injection simulation provided by Scania, to estimate the fibre orientation and thereby the material behaviour of the SFRP component. The fatigue analysis was conducted by performing a coupled structural analysis between Digimat-Abaqus and then import the resulting stress- and strain-fields into the fatigue post-processor nCode DesignLife. The stress is then cyclic tested towards experimentally determined S-N curves determined in Digimat.</p><p>Due to restriction of available fatigue data for the plastic in the front bracket, a fatigue material model for a plastic containing the same fibres and matrix but with a different fibre amount was implemented. The fatigue data were scaled using the UTS method to get a good characterisation of the real-life material behaviour of the plastic of the front bracket component.</p><p>From the correlation between the fatigue analysis and performed experiments, it was shown that the simulated fatigue life was conservative compared to the fatigue life determined from the experiments. However, the correlation between the fatigue analysis and experiments is not fully captured but gives a better estimation of the fatigue life compared to performing the fatigue analysis using an isotropic material model.</p>
----------------------------------------------------------------------
In diva2:1110758 abstract is:
<p>In the frame of the improvement of the performances for Ariane 5, an analysis iscarried out to explain the pressure drop observed in the ascent phase of some flights inthe liquid hydrogen (LH2) tank of the upper stage. This stage is mainly idle until therocket is out of the atmosphere but is submitted to important excitation throughoutthe ascent phase in the atmosphere. Due to excitation, the liquid contained in thetank moves and breaks the thermodynamic equilibrium. This movement, sloshing isidentified as the most likely cause of the pressure drop observed. It is investigated inthis thesis to understand how exactly it impacts the thermodynamic equilibrium inthe tank.The pressure drop called creux PGRH can be explained by the mixing of the topof the liquid with the liquid bulk, colder than the top, when the liquid is sloshing.This movement changes the saturation conditions and yields pressure and temperatureevolutions in the ullage volume of the tank. Observations on Ariane 5 flightsshowed that the first asymmetric mode was mainly excited during this first phase ofascension. Simple models such as the pendulum model are used to simulate the dynamicbehaviour of this mode. Its stability is also investigated through lateral andlongitudinal excitations.The thermodynamics of the system in the tank can be modelled by a one-dimensionalmodel. Based on an experiment with liquid nitrogen in a cylindrical tank, the heatfluxes are calculated and plugged in the model. The pressurisation phase is first simulatedthrough self and active pressurisation to estimate the importance of the thicknessof the thermal boundary layer. Sloshing is included in the model thermodynamicallyby considering a more important conductive coefficient in the sloshing layer. The amplitudeof sloshing can be linked to the new conduction term thanks to a literaturerelation but it underestimates the actual magnitude of the pressure drop. The modelis extended to a two-dimensional model to take into account the sloshing mechanically,knowing the velocities from the pendulum model. It is found to be not accurate mostlydue to the turbulence of the sloshing layer not considered in the model.The models give in any case important results regarding the influence of some tankparameters such as the ullage volume, the importance of the pressurisation phase anda necessary distinction between chaotic and stable sloshing. From this, the data ofAriane 5 flights is analysed. The flights are divided in three families according to themagnitude of the pressure drop measured. A fault tree analysis is performed to ruleout possible influences and put forward a theory on the creux eventually.</p>

corrected abstract:
<p>In the frame of the improvement of the performances for Ariane 5, an analysis is carried out to explain the pressure drop observed in the ascent phase of some flights in the liquid hydrogen (LH<sub>2</sub>) tank of the upper stage. This stage is mainly idle until the rocket is out of the atmosphere but is submitted to important excitation throughout the ascent phase in the atmosphere. Due to excitation, the liquid contained in the tank moves and breaks the thermodynamic equilibrium. This movement, sloshing is identified as the most likely cause of the pressure drop observed. It is investigated in this thesis to understand how exactly it impacts the thermodynamic equilibrium in the tank.</p><p>The pressure drop called <em>creux PGRH</em> can be explained by the mixing of the top of the liquid with the liquid bulk, colder than the top, when the liquid is sloshing. This movement changes the saturation conditions and yields pressure and temperature evolutions in the ullage volume of the tank. Observations on Ariane 5 flights showed that the first asymmetric mode was mainly excited during this first phase of ascension. Simple models such as the pendulum model are used to simulate the dynamic behaviour of this mode. Its stability is also investigated through lateral and longitudinal excitations.</p><p>The thermodynamics of the system in the tank can be modelled by a one-dimensional model. Based on an experiment with liquid nitrogen in a cylindrical tank, the heat fluxes are calculated and plugged in the model. The pressurisation phase is first simulated through self and active pressurisation to estimate the importance of the thickness of the thermal boundary layer. Sloshing is included in the model thermodynamically by considering a more important conductive coefficient in the sloshing layer. The amplitude of sloshing can be linked to the new conduction term thanks to a literature relation but it underestimates the actual magnitude of the pressure drop. The model is extended to a two-dimensional model to take into account the sloshing mechanically, knowing the velocities from the pendulum model. It is found to be not accurate mostly due to the turbulence of the sloshing layer not considered in the model.</p><p>The models give in any case important results regarding the influence of some tank parameters such as the ullage volume, the importance of the pressurisation phase and a necessary distinction between chaotic and stable sloshing. From this, the data of Ariane 5 flights is analysed. The flights are divided in three families according to the magnitude of the pressure drop measured. A fault tree analysis is performed to rule out possible influences and put forward a theory on the creux eventually.</p>
----------------------------------------------------------------------
In diva2:783982 - spaces missing in title:
"Evaluating the effectiveness of collisionavoidance functions using state-of-the-artsimulation tools for vehicle dynamics"
==>
"Evaluating the effectiveness of collision avoidance functions using state-of-the-art simulation tools for vehicle dynamics"

abstract is:
<p>The main goal of this work is to gain knowledge of how and to what extent state-of-the-artsimulation tools can be used in a conceptual development phase for vehicle dynamics control atVolvo Car Corporation (VCC).The first part of the thesis deals with an evaluation of vehicle dynamics simulation tools and theiruses. The three simulation tools selected for the study, namely Mechanical Simulation CarSim 8.2.1,IPG CarMaker 4.0.5, and VI-Grade CarRealTime V14, are briefly described and discussed. In order toevaluate and compare these tools with respect to application for vehicle dynamics control, a criterialist is developed covering aspects such as tool requirements and intended usage. Based on thecriteria list and certain identified drawbacks, a ranking of the tools is made possible. Furthermore,the process of developing vehicle models for the different tools is discussed in detail, along with theprocedure of validating the vehicle models.In the second part, the concept of Collision Avoidance Driver Assistance (CADA) function isintroduced and possible approaches for developing CADA functions are discussed in brief. It isimportant to note that the CADA functions in this work are based on cornering the vehicle i.e.maneuvering around the threat, rather than solely reducing vehicle speed. A number ofimplementations of the functions are developed in Simulink. A frequency analysis of a simplifiedlinear vehicle model is performed to investigate the influence of steering, differential braking, andtheir combination on the resultant lateral displacement of the vehicle during an evasive maneuver.The developed CADA functions are then simulated using the vehicle simulation tools. Two specificmetrics - Lateral Displacement gain and DeltaX - are formulated to evaluate the effectiveness of theCADA functions. Based on these metrics, the assistance obtained due to the functions for a specificevasive maneuver is compared.From the evaluation process of the three tools, two were considered suitable for the purpose ofsimulating collision avoidance functions. The evaluation of the CADA functions demonstrates thatcombined assistive steering with differential braking provides considerable assistance in order toavoid collisions. The simulation results also present interesting trends which provide a usefuldirection regarding the conditions for intervention by such collision avoidance functions during anevasive maneuver. The use of simulation tools makes it possible to observe these trends and utilizethem in the development process of the functions.</p>

corrected abstract:
<p>The main goal of this work is to gain knowledge of how and to what extent state-of-the-art simulation tools can be used in a conceptual development phase for vehicle dynamics control at Volvo Car Corporation (VCC).</p><p>The first part of the thesis deals with an evaluation of vehicle dynamics simulation tools and their uses. The three simulation tools selected for the study, namely Mechanical Simulation CarSim 8.2.1, IPG CarMaker 4.0.5, and VI-Grade CarRealTime V14, are briefly described and discussed. In order to evaluate and compare these tools with respect to application for vehicle dynamics control, a criteria list is developed covering aspects such as tool requirements and intended usage. Based on the criteria list and certain identified drawbacks, a ranking of the tools is made possible. Furthermore, the process of developing vehicle models for the different tools is discussed in detail, along with the procedure of validating the vehicle models.</p><p>In the second part, the concept of Collision Avoidance Driver Assistance (CADA) function is introduced and possible approaches for developing CADA functions are discussed in brief. It is important to note that the CADA functions in this work are based on cornering the vehicle i.e. maneuvering around the threat, rather than solely reducing vehicle speed. A number of implementations of the functions are developed in Simulink. A frequency analysis of a simplified linear vehicle model is performed to investigate the influence of steering, differential braking, and their combination on the resultant lateral displacement of the vehicle during an evasive maneuver. The developed CADA functions are then simulated using the vehicle simulation tools. Two specific metrics - Lateral Displacement gain and DeltaX - are formulated to evaluate the effectiveness of the CADA functions. Based on these metrics, the assistance obtained due to the functions for a specific evasive maneuver is compared.</p><p>From the evaluation process of the three tools, two were considered suitable for the purpose of simulating collision avoidance functions. The evaluation of the CADA functions demonstrates that combined assistive steering with differential braking provides considerable assistance in order to avoid collisions. The simulation results also present interesting trends which provide a useful direction regarding the conditions for intervention by such collision avoidance functions during an evasive maneuver. The use of simulation tools makes it possible to observe these trends and utilize them in the development process of the functions.</p>
----------------------------------------------------------------------
In diva2:1527803 abstract is:
<p>Multi-body simulations are given more emphasis over physical tests owing toenvironmental, financial, and time requirements in the competitive automotive industry. Thus,it is imperative to develop models to accurately predict and analyse the system's behaviour.This thesis focuses on developing an air suspension model with Electronic Level Control thathas the ability to communicate with other air springs in a pneumatic circuit thus replicating thepneumatic connection in actual truck and regulate the ride height of the vehicle.To accomplish this, a comprehensive literature study is performed to identify an effectivecontrol variable to manipulate the air springs. This is done by understanding the working andthermodynamic principles of air suspension, understanding various Scania pneumaticconfigurations, and decrypting the working of the Electronic Level Control.Different methods for implementing the model through the identified control variable arediscussed. A brief explanation of the necessary physical tests performed to validate the modelis given. An extensive description of implementation of the static and dynamic model inADAMS through command batch script coding is provided.The developed static model is validated by comparing the results from simulations and the testdata. The axle weights have an error of maximum 6% and the pressure in the air springs havean error of maximum 9% which can be owed to neglection of hysteresis in the air springcharacteristics and using mean values to compare the data. The dynamic model is validated byaltering the ride height level and observing the response of the model. The results obtainedindicate the developed Electronic Level Control is able to regulate the ride height at the desiredlevel.The robustness of the model is validated by modifying the developed model for longitudinalpneumatic connection and for a truck with trailer model. The results indicate the developedmodel is capable to perform satisfactorily and conform to the Scania tolerance limits.Thus, an appropriate control variable for the air springs model is identified. Static and dynamicmodel to identify the suitable pressure in the air springs and thus, the force in the air springs isdeveloped which helped in drastically reducing the manual iterative work that was required.</p>

corrected abstract:
<p>Multi-body simulations are given more emphasis over physical tests owing to environmental, financial, and time requirements in the competitive automotive industry. Thus, it is imperative to develop models to accurately predict and analyse the system's behaviour.</p><p>This thesis focuses on developing an air suspension model with Electronic Level Control that has the ability to communicate with other air springs in a pneumatic circuit thus replicating the pneumatic connection in actual truck and regulate the ride height of the vehicle.</p><p>To accomplish this, a comprehensive literature study is performed to identify an effective control variable to manipulate the air springs. This is done by understanding the working and thermodynamic principles of air suspension, understanding various Scania pneumatic configurations, and decrypting the working of the Electronic Level Control.</p><p>Different methods for implementing the model through the identified control variable are discussed. A brief explanation of the necessary physical tests performed to validate the model is given. An extensive description of implementation of the static and dynamic model in ADAMS through command batch script coding is provided.</p><p>The developed static model is validated by comparing the results from simulations and the test data. The axle weights have an error of maximum 6% and the pressure in the air springs have an error of maximum 9% which can be owed to neglection of hysteresis in the air spring characteristics and using mean values to compare the data. The dynamic model is validated by altering the ride height level and observing the response of the model. The results obtained indicate the developed Electronic Level Control is able to regulate the ride height at the desired level.</p><p>The robustness of the model is validated by modifying the developed model for longitudinal pneumatic connection and for a truck with trailer model. The results indicate the developed model is capable to perform satisfactorily and conform to the Scania tolerance limits.</p><p>Thus, an appropriate control variable for the air springs model is identified. Static and dynamic model to identify the suitable pressure in the air springs and thus, the force in the air springs is developed which helped in drastically reducing the manual iterative work that was required.</p>
----------------------------------------------------------------------
In diva2:1465517 - missing space in the tttle:
"Multi-Objective Optimization of Torque Distribution inHybrid Vehicles"
==>
"Multi-Objective Optimization of Torque Distribution in Hybrid Vehicles"

abstract is:
<p>Electrification is one of the mega-trends in the transportation and automotive industry today. Boththe alarming environmental conditions and the ever decreasing fuel reserves are driving the shifttowards hybrid, all electric and alternative fuel source vehicles. This thesis work is another smallstep towards studying, addressing and handling this issue while also laying the groundwork for developingand moving towards more efficient and commercially viable vehicles.This thesis work aims at investigating the trade-off offered by optimal control techniques betweenenergy consumption and reference tracking for torque allocation to the various actuators available topropel a hybrid electric vehicle. The particular vehicle under consideration has two electric motorsat the rear wheels and an internal combustion engine along with an integrator starter generatordriving the front wheels. The torque allocation problem is originally solved by proposing a one stageoptimization strategy (OSOS) that takes into account actuator limits, losses, and objectives throughconstraints. The performance of this formulation is presented over two simulated test tracks on apareto front where the advantage on relaxing complete reference tracking becomes visible. Next,two new formulations each as a two stage optimization strategies (TSOS) are proposed, the mainobjective being to split the original formulation into two parts. One addressing energy optimalityand the other addressing reference tracking of total wheel torque and yaw moment request fulfilment.These formulations are then similarly investigated and presented in comparison with the originalformulation. In developing the formulations, an assumption about the loss models is made andthe problem size of the second stage quadratic program is significantly reduced. The problems areappropriately scaled and made mathematically robust to handle the constraints and inputs in theoperating range. As reference tracking for the vehicle is split into lateral and longitudinal torquerequests from the vehicle, this becomes a multi-objective optimization problem. To further studythe behaviour of these formulations, they are given constant inputs and simulated over a single timestep. The effect of changing hybridization level, i.e, the amount of electrical energy used comparedto fuel energy on the behaviour of these formulations is also explored. One of the effects of the twostage formulations was the confinement of solutions within a reasonable error for the majority ofchosen weights due to the energy considerations in the first stage. The proposed formulations wereable to generate results close but not equal to the original formulation on the pareto front. Anotherfinding was that due to the implementation of two actuators at the rear of the vehicle, a desired yawrate could be achieved at no additional energy cost because of regenerative and propulsive torquesgenerated respectively on either side of rear axle for torque vectoring. Furthermore with a dedicatedsolver, the TSOS could present an interesting alternative to enhance independent development invehicle dynamics control and energy management of the vehicle.</p>


corrected abstract:
<p>Electrification is one of the mega-trends in the transportation and automotive industry today. Both the alarming environmental conditions and the ever decreasing fuel reserves are driving the shift towards hybrid, all electric and alternative fuel source vehicles. This thesis work is another small step towards studying, addressing and handling this issue while also laying the groundwork for developing and moving towards more efficient and commercially viable vehicles.</p><p>This thesis work aims at investigating the trade-off offered by optimal control techniques between energy consumption and reference tracking for torque allocation to the various actuators available to propel a hybrid electric vehicle. The particular vehicle under consideration has two electric motors at the rear wheels and an internal combustion engine along with an integrator starter generator driving the front wheels. The torque allocation problem is originally solved by proposing a one stage optimization strategy (OSOS) that takes into account actuator limits, losses, and objectives through constraints. The performance of this formulation is presented over two simulated test tracks on a pareto front where the advantage on relaxing complete reference tracking becomes visible. Next, two new formulations each as a two stage optimization strategies (TSOS) are proposed, the main objective being to split the original formulation into two parts. One addressing energy optimality and the other addressing reference tracking of total wheel torque and yaw moment request fulfilment.</p><p>These formulations are then similarly investigated and presented in comparison with the original formulation. In developing the formulations, an assumption about the loss models is made and the problem size of the second stage quadratic program is significantly reduced. The problems are appropriately scaled and made mathematically robust to handle the constraints and inputs in the operating range. As reference tracking for the vehicle is split into lateral and longitudinal torque requests from the vehicle, this becomes a multi-objective optimization problem. To further study the behaviour of these formulations, they are given constant inputs and simulated over a single time step. The effect of changing hybridization level, i.e, the amount of electrical energy used compared to fuel energy on the behaviour of these formulations is also explored. One of the effects of the two stage formulations was the confinement of solutions within a reasonable error for the majority of chosen weights due to the energy considerations in the first stage. The proposed formulations were able to generate results close but not equal to the original formulation on the pareto front. Another finding was that due to the implementation of two actuators at the rear of the vehicle, a desired yaw rate could be achieved at no additional energy cost because of regenerative and propulsive torques generated respectively on either side of rear axle for torque vectoring. Furthermore with a dedicated solver, the TSOS could present an interesting alternative to enhance independent development in vehicle dynamics control and energy management of the vehicle.</p>
----------------------------------------------------------------------
In diva2:1327792 - note: no full text in DiVA

abstract is:
<p>Capacitive deionization is an emerging environmentally friendly technique for waterdesalination that has been getting increasing attention in recent years. In thistechnique, water passes through a cell with nanostructured porous carbon electrodeswhich have a high surface area. When a potential is applied to these electrodes, theelectrodes adsorb the salt ion in the water stream, which results in the production offresh water. While the technique is promising, it still needs to be developed further tosee more widespread use, and modeling can be an essential tool during investigationsto expedite these developments. To make modeling more accessible, it is crucial thatmodels are developed that can predict the process performance transparently andstraightforwardly. This master thesis encompasses three submitted papers. A model,termed the Dynamic Langmuir model, is developed based on a few fundamentalmacroscopic principles. The model is more straightforward and more transparent thanprevious models yet could accurately describe key concepts including ion adsorptionand charge efficiency, in both equilibrium and dynamic settings and for variouselectrode materials and cell structures. The model is shown to accurately predictperformance over crucial parameters including the applied voltage, flow rate of thewater through the cell, inlet concentration, mixtures of ions in the water, varyingelectrode asymmetry and electrode pre-charging. The usefulness of the model isfurther demonstrated by using it to optimize the time spent absorbing salt versuscleaning the cell. This improved ion-removal efficiency by 31 % compared to doing fullsaturation/regeneration. To further aid the goal of making modeling more accessible,a software program has been developed and provided as open source that can bedirectly used to implement the model, without requiring extensive knowledge aboutthe theory, or lots of experiments to set up. Also, an automated experimental setup forcontinuous and stable CDI operation was developed that could provide data for futuremodeling. Finally, this thesis additionally includes an extensive theory section to givea comprehensive introduction to the capacitive-deionization field. In conclusion, asimple and transparent model has been developed, able to accurately describe howcritical concepts in capacitive deionization vary over a wide range of operationalparameters. It is hoped that this work can make the modeling of capacitivedeionization more accessible.</p>

corrected abstract:
<p>Capacitive deionization is an emerging environmentally friendly technique for water desalination that has been getting increasing attention in recent years. In this technique, water passes through a cell with nanostructured porous carbon electrodes which have a high surface area. When a potential is applied to these electrodes, the electrodes adsorb the salt ion in the water stream, which results in the production of fresh water. While the technique is promising, it still needs to be developed further to see more widespread use, and modeling can be an essential tool during investigations to expedite these developments. To make modeling more accessible, it is crucial that models are developed that can predict the process performance transparently and straightforwardly. This master thesis encompasses three submitted papers. A model, termed the Dynamic Langmuir model, is developed based on a few fundamental macroscopic principles. The model is more straightforward and more transparent than previous models yet could accurately describe key concepts including ion adsorption and charge efficiency, in both equilibrium and dynamic settings and for various electrode materials and cell structures. The model is shown to accurately predict performance over crucial parameters including the applied voltage, flow rate of the water through the cell, inlet concentration, mixtures of ions in the water, varying electrode asymmetry and electrode pre-charging. The usefulness of the model is further demonstrated by using it to optimize the time spent absorbing salt versus cleaning the cell. This improved ion-removal efficiency by 31 % compared to doing fullsaturation/regeneration. To further aid the goal of making modeling more accessible, a software program has been developed and provided as open source that can be directly used to implement the model, without requiring extensive knowledge about the theory, or lots of experiments to set up. Also, an automated experimental setup for continuous and stable CDI operation was developed that could provide data for future modeling. Finally, this thesis additionally includes an extensive theory section to give a comprehensive introduction to the capacitive-deionization field. In conclusion, a simple and transparent model has been developed, able to accurately describe how critical concepts in capacitive deionization vary over a wide range of operational parameters. It is hoped that this work can make the modeling of capacitive deionization more accessible.</p>
----------------------------------------------------------------------
In diva2:412700 abstract is:
<p>This diploma thesis captures the three-dimensional implementation of noise-reducing high-liftsystems. A parametric CAD model is developed for the FNG aircraft and different high-lift configurationsare built up. In the course of research, these configurations are designed based onformerly obtained two-dimensional results of DLR’s LEISA project featuring the design of a verylong chord slat (VLCS), whose slat shape resulted in a favourable aeroacoustic behaviour at noiserelevantapproach conditions. The high-lift systems derived in this thesis differ in the spanwisevariation of the slat geometry planform as well as in the applied high-lift settings described bygap, overlap and deflection angle.The aerodynamic performance is computed via CFD RANS simulations and the results arecompared to a reference high-lift system of the FNG aircraft, which has been designed in previousstudies. The observed CFD results are further evaluated in the reference wing section of the FNGaircraft in order to display the agreement between the implemented 3D high-lift configurationsand the 2D LEISA reference data. Besides the aerodynamic performance, aeroacoustic aspects arealso considered in this diploma thesis. By means of the obtained CFD results, indirect statementsabout the success of the 3D low-noise implementation approach are made.The geometrical concordance of the derived reference wing section of the 3D CAD model isfound in general to be very high in comparison to the 2D-optimized LEISA design wing section.With regard to the observed pressure distributions of the initial four designed high-lift systemshowever, small geometry deviations are noticed to affect the obtained pressure distributions in asignificantly unintended way. The requirements of a low-noise high-lift system are thus not metfor these high-lift configurations. In the 3D implementation, the 2D-optimized slat settings haveto be modified in order to maintain the favourable aeroacoustic behaviour of the 2D considerations.Based on a reduced slat deflection angle, a further derived 3D VLCS high-lift system isobtained to match the 2D-optimized pressure distributions in the reference wing section more accurately.A significant pressure increase at the VLCS trailing edge is noticed for this configuration,which shows the noise-reducing potential of the derived VLCS device. However, the aerodynamicdegradations obtained for the designed low-noise high-lift system are found to be too high in orderto still provide improved aeroacoustic behaviour during conditions of increased approachspeed. A 3D noise-reducing high-lift system is therefore not achieved, although the 2D-optimizedLEISA pressure distributions are well captured in the reference wing section of the implemented3D high-lift system featuring modified high-lift setting parameters.</p>


corrected abstract:
<p>This diploma thesis captures the three-dimensional implementation of noise-reducing high-lift systems. A parametric CAD model is developed for the FNG aircraft and different high-lift configurations are built up. In the course of research, these configurations are designed based on formerly obtained two-dimensional results of DLR’s LEISA project featuring the design of a very long chord slat (VLCS), whose slat shape resulted in a favourable aeroacoustic behaviour at noise relevant approach conditions. The high-lift systems derived in this thesis differ in the spanwise variation of the slat geometry plan form as well as in the applied high-lift settings described by gap, overlap and deflection angle.</p><p>The aerodynamic performance is computed via CFD RANS simulations and the results are compared to a reference high-lift system of the FNG aircraft, which has been designed in previous studies. The observed CFD results are further evaluated in the reference wing section of the FNG aircraft in order to display the agreement between the implemented 3D high-lift configurations and the 2D LEISA reference data. Besides the aerodynamic performance, aeroacoustic aspects are also considered in this diploma thesis. By means of the obtained CFD results, indirect statements about the success of the 3D low-noise implementation approach are made.</p><p>The geometrical concordance of the derived reference wing section of the 3D CAD model is found in general to be very high in comparison to the 2D-optimized LEISA design wing section. With regard to the observed pressure distributions of the initial four designed high-lift systems however, small geometry deviations are noticed to affect the obtained pressure distributions in a significantly unintended way. The requirements of a low-noise high-lift system are thus not met for these high-lift configurations. In the 3D implementation, the 2D-optimized slat settings have to be modified in order to maintain the favourable aeroacoustic behaviour of the 2D considerations. Based on a reduced slat deflection angle, a further derived 3D VLCS high-lift system is obtained to match the 2D-optimized pressure distributions in the reference wing section more accurately. A significant pressure increase at the VLCS trailing edge is noticed for this configuration, which shows the noise-reducing potential of the derived VLCS device. However, the aerodynamic degradations obtained for the designed low-noise high-lift system are found to be too high in order to still provide improved aeroacoustic behaviour during conditions of increased approach speed. A 3D noise-reducing high-lift system is therefore not achieved, although the 2D-optimized LEISA pressure distributions are well captured in the reference wing section of the implemented 3D high-lift system featuring modified high-lift setting parameters.</p>
----------------------------------------------------------------------
In diva2:408831 abstract is:
<p>This thesis is about teachers who share knowledge through lektion.se. Lektion.se is anInternet site and started in 2003 by three teachers. It gives its members an opportunity toshare ideas for lessons, discuss interesting topics and give advice to other members. Thethesis focuses on how the teachers define and use lection.se for sharing knowledge and howthe site can be improved concerning knowledge sharing. To investigate this, seven interviewshave been conducted with teachers in and around Stockholm and a questionnaire has beenpublished on lektion.se. I have also done an observation on how the members act on the site’sforum and the lesson database.The teachers in the study define the site depending on which functions they use and how theyuse them. The teachers who only use the lessons database or forum to get access to otherteacher’s ideas, often see the site as a collection of tips. The teachers who use the lessondatabase or forum for both getting other teacher’s ideas and sharing their own ideas often seethe site as a community of practice for teachers.The use of the functions on the site varies. It is mainly the lesson database and the forum thatis used. Only 1 % of the members share their knowledge with other members. But almostevery participant in the study has used some other teacher’s advice or lesson plan. A numberof barriers and motives for sharing knowledge on the site have been identified in the study.The barriers are both personal and organizational. Cowardice, fear and a feeling ofinadequate ideas are some of the personal barriers identified in the study. A lack of time andtechnical problems are examples of organizational barriers. The knowledge sharing amongthe teachers are motivated by own needs and altruism. Through knowledge sharing, they getfeedback on their ideas and facilitate other teacher’s work.To maintain and, hopefully, increase the sharing of knowledge, the number of barriers forsharing knowledge must be minimized. It can be done by creating more opportunities forcollaboration and that the functions in the site will encourage sharing knowledge. It is alsorequired that the teachers create a reflecting culture where knowledge sharing are consideredto be something natural.</p>


corrected abstract:
<p>This thesis is about teachers who share knowledge through lektion.se. Lektion.se is an Internet site and started in 2003 by three teachers. It gives its members an opportunity to share ideas for lessons, discuss interesting topics and give advice to other members. The thesis focuses on how the teachers define and use lection.se for sharing knowledge and how the site can be improved concerning knowledge sharing. To investigate this, seven interviews have been conducted with teachers in and around Stockholm and a questionnaire has been published on lektion.se. I have also done an observation on how the members act on the site’s forum and the lesson database.</p><p>The teachers in the study define the site depending on which functions they use and how they use them. The teachers who only use the lessons database or forum to get access to other teacher’s ideas, often see the site as a collection of tips. The teachers who use the lesson database or forum for both getting other teacher’s ideas and sharing their own ideas often see the site as a community of practice for teachers.</p><p>The use of the functions on the site varies. It is mainly the lesson database and the forum that is used. Only 1 % of the members share their knowledge with other members. But almost every participant in the study has used some other teacher’s advice or lesson plan. A number of barriers and motives for sharing knowledge on the site have been identified in the study. The barriers are both personal and organizational. Cowardice, fear and a feeling of inadequate ideas are some of the personal barriers identified in the study. A lack of time and technical problems are examples of organizational barriers. The knowledge sharing among the teachers are motivated by own needs and altruism. Through knowledge sharing, they get feedback on their ideas and facilitate other teacher’s work.</p><p>To maintain and, hopefully, increase the sharing of knowledge, the number of barriers for sharing knowledge must be minimized. It can be done by creating more opportunities for collaboration and that the functions in the site will encourage sharing knowledge. It is also required that the teachers create a reflecting culture where knowledge sharing are considered to be something natural.</p>
----------------------------------------------------------------------
In diva2:1804716 abstract is:
<p>The thesis examines the prospects of using the superconductor NbN as the gatemetal for an InP HEMT. A HEMT or High Electron Mobility Transistor is aheterostructure transistor engineered to reach very high electron mobility. InPHEMTs are used as cryogenic Low Noise Amplifiers (LNAs), which have increasedin demand as quantum computing is scaling up. A superconducting NbN gate isof interest as it has the potential to decrease the amount of noise generated by theHEMT LNAs.A gate width dependence for both the transconductance (gm) and the large-signal HEMT channel resistance (RON ) of the NbN HEMTs at room temperaturehas been observed, and the first goal pf the thesis is to determine the originof the dependence. Moreover, the measured RF characteristics of the NbNdevices tend to deviate from the norm of a standard HEMT, and the secondgoal is to understand why. The third goal is to determine if the NbN gate stayssuperconducting at cryogenic temperatures or if self-heating from the channelduring DC operations will break superconductivity.In the thesis, it was possible to recreate the observed gate width dependence withnew devices, and additionally, a gate width dependence in the threshold voltageis observed. The origin of width dependence is most likely related to the straincreated by the NbN gate. At DC, extremely high peaks in the transconductanceare observed, which is most likely related to impact ionization and a subsequentincrease in hole trapping caused by the introduction of the NbN gate.Using simulations, it was possible to accurately recreate the observed deviantbehaviour, likely associated with the NbN gate’s high capacitance, inductance andresistance at room temperature. The high capacitance is likely partly related tosome NbN gates of the HEMTs being broken. Finally, the HEMT can operatein DC at 2 K with VG = 0.3 V and a maximum VD = 0.1 V before self-heatingfrom the channel will break the NbN superconductivity of the gate. This is oneof the critical conclusions of the work because it shows that a superconductinggate electrode can be implemented and functional in a high-performance HEMTdevice structure and under realistic operating bias conditions. As long as it can bedemonstrated that the superconductivity does not break when operating in RF, aNbN gate is a promising avenue to increase the noise performance of the cryogenicHEMT.</p>


corrected abstract:
<p>The thesis examines the prospects of using the superconductor NbN as the gate metal for an InP HEMT. A HEMT or High Electron Mobility Transistor is a heterostructure transistor engineered to reach very high electron mobility. InP HEMTs are used as cryogenic Low Noise Amplifiers (LNAs), which have increased in demand as quantum computing is scaling up. A superconducting NbN gate is of interest as it has the potential to decrease the amount of noise generated by the HEMT LNAs.</p><p>A gate width dependence for both the transconductance (<em>g<sub>m</sub></em>) and the large-signal HEMT channel resistance (<em>R<sub>ON</sub></em>) of the NbN HEMTs at room temperature has been observed, and the first goal pf the thesis is to determine the origin of the dependence. Moreover, the measured RF characteristics of the NbN devices tend to deviate from the norm of a standard HEMT, and the second goal is to understand why. The third goal is to determine if the NbN gate stays superconducting at cryogenic temperatures or if self-heating from the channel during DC operations will break superconductivity.</p><p>In the thesis, it was possible to recreate the observed gate width dependence with new devices, and additionally, a gate width dependence in the threshold voltage is observed. The origin of width dependence is most likely related to the strain created by the NbN gate. At DC, extremely high peaks in the transconductance are observed, which is most likely related to impact ionization and a subsequent increase in hole trapping caused by the introduction of the NbN gate.</p><p>Using simulations, it was possible to accurately recreate the observed deviant behaviour, likely associated with the NbN gate’s high capacitance, inductance and resistance at room temperature. The high capacitance is likely partly related to some NbN gates of the HEMTs being broken. Finally, the HEMT can operate in DC at 2 K with <em>V<sub>G</sub> = 0.3</em> V and a maximum <em>V<sub>D</sub> = 0.1</em> V before self-heating from the channel will break the NbN superconductivity of the gate. This is one of the critical conclusions of the work because it shows that a superconducting gate electrode can be implemented and functional in a high-performance HEMT device structure and under realistic operating bias conditions. As long as it can be demonstrated that the superconductivity does not break when operating in RF, a NbN gate is a promising avenue to increase the noise performance of the cryogenic HEMT.</p>
----------------------------------------------------------------------
In diva2:1698151 abstract is:
<p>An extensive technological shift is currently taking place to mitigate climate changeand this trend is particularly noticeable in the transport sector. This is interestingfrom an acoustical perspective, since it changes the noise environment in society.For example, a study in Gothenburg has shown that a complete electrification ofthe road traffic would reduce the noise levels by between 2 and 5 dB(A). In Sweden,noise emissions are calculated with a calculation model from 1996, called the Nordiskberäkningsmodell (Nordiska). Given the age of the model it is reasonable to investigate whether Sweden should change completely to the EU-common calculationmodel Common NOise aSSessment methOdS (CNOSSOS), since it is mandatory touse for national noise mapping.</p><p>This master thesis has performed a computation analysis to compare and discussdifferences between CNOSSOS and Nordiska, to contribute to answering the question whether Sweden should change to CNOSSOS (or perhaps another model). Theresults show that CNOSSOS overall computes higher noise levels than Nordiska andthat the differences between them increase linearly with distance. Farthest from thenoise source the differences are up to 5 dB(A) for the road case and 9 dB(A) forthe railway case. In other words, the differences are larger for the railway trafficmodels than they are for the road traffic models, which is thought to be a result ofthe complexity of the CNOSSOS railway model. Another interesting phenomenonis that the differences behind buildings between the models are different for roadand railway traffic, which can be explained by the fact that the screening effects inNordiska’s road and railway models are different.</p><p>My conclusion is that CNOSSOS is unsuitable for domestic calculations of noiseemissions. The model does not align with Swedish legislation and there is uncertaintydue to the fact that the differences between the CNOSSOS and Nordiska road andrailway models are different in size. Moreover, CNOSSOS railway model requires alot of computational power, which can delay and increase the costs of noise mappingor reduce the accuracy of the results. However, additional work is needed in whicheach calculation model is compared with measurements in situ to see which modelbest describes reality. If the conclusion thereafter is that CNOSSOS still is not asuitable option, it could be examined whether it is possible to create an updatedversion of Nord2000 (another Nordic calculation model used e.g. in Denmark) toobtain a calculation model that is more suitable for future traffic conditions.</p>


corrected abstract:
<p>An extensive technological shift is currently taking place to mitigate climate change and this trend is particularly noticeable in the transport sector. This is interesting from an acoustical perspective, since it changes the noise environment in society. For example, a study in Gothenburg has shown that a complete electrification of the road traffic would reduce the noise levels by between 2 and 5 dB(A). In Sweden, noise emissions are calculated with a calculation model from 1996, called the <em lang="sv">Nordisk beräkningsmodell</em> (Nordiska). Given the age of the model it is reasonable to investigate whether Sweden should change completely to the EU-common calculation model <em>Common NOise aSSessment methOdS</em> (CNOSSOS), since it is mandatory to use for national noise mapping.</p><p>This master thesis has performed a computation analysis to compare and discuss differences between CNOSSOS and Nordiska, to contribute to answering the question whether Sweden should change to CNOSSOS (or perhaps another model). The results show that CNOSSOS overall computes higher noise levels than Nordiska and that the differences between them increase linearly with distance. Farthest from the noise source the differences are up to 5 dB(A) for the road case and 9 dB(A) for the railway case. In other words, the differences are larger for the railway traffic models than they are for the road traffic models, which is thought to be a result of the complexity of the CNOSSOS railway model. Another interesting phenomenon is that the differences behind buildings between the models are different for road and railway traffic, which can be explained by the fact that the screening effects in Nordiska’s road and railway models are different.</p><p>My conclusion is that CNOSSOS is unsuitable for domestic calculations of noise emissions. The model does not align with Swedish legislation and there is uncertainty due to the fact that the differences between the CNOSSOS and Nordiska road and railway models are different in size. Moreover, CNOSSOS railway model requires a lot of computational power, which can delay and increase the costs of noise mapping or reduce the accuracy of the results. However, additional work is needed in which each calculation model is compared with measurements in situ to see which model best describes reality. If the conclusion thereafter is that CNOSSOS still is not a suitable option, it could be examined whether it is possible to create an updated version of Nord2000 (another Nordic calculation model used e.g. in Denmark) to obtain a calculation model that is more suitable for future traffic conditions.</p>
----------------------------------------------------------------------
In diva2:1465539 abstract is:
<p>Throughout the history of motor vehicles, the tyres have always been consideredas one of the most important components of the vehicle due to their interactionwith the road. One important aspect is the wheel alignment, with the purposeto adjust the static wheel angles that are essential for many reasons, such assafety and fuel consumption for instance. Despite the numerous methods forwheel angle measurements, there seems to be no existing technical solutionbased on computer vision, that is suitable for residential use, regarding bothcost and size of the equipment. The study aims to investigate the feasibility ofsuch a system.The proposed system is based on planar fiducial markers called ArUco.From images or video frames of the marker, the pose of the marker can beestimated. Thus, by placing such markers on the ground, on the wheel andon the vehicle, the estimated pose of the markers can be used to measure andcalculate the wheel alignment parameters. Only toe and camber angles aremeasured within the scope of this thesis, even if the system has the potential tomeasure other wheel alignment parameters as well.After camera calibration, simplified ArUco marker tests were done by measuringthe known displacement and inclination of a marker with respect to areference marker. The mean absolute error was 030400 and 0:024mm for theinclination angle and displacement, respectively. Furthermore, the toe and camberangles of a vehicle were measured and compared to reference measurementsperformed with a commercial wheel alignment system, giving mean absoluteerrors of 0520 and 0280 for the camber and toe angles, respectively. Despitethe relatively large errors for the toe and camber angle measurements, theresults from the initial inclination and displacement tests show the potential ofthe system. In addition, several error sources and suggestions for improvementcan be identified.As a conclusion, the proposed system can be considered a working firstprototype, which after improvement and optimisation has the potential tobecome a feasible alternative, especially for residential use and for mobileworkshops due to the low cost, size and usability of the system.</p>


corrected abstract:
<p>Throughout the history of motor vehicles, the tyres have always been considered as one of the most important components of the vehicle due to their interaction with the road. One important aspect is the wheel alignment, with the purpose to adjust the static wheel angles that are essential for many reasons, such as safety and fuel consumption for instance. Despite the numerous methods for wheel angle measurements, there seems to be no existing technical solution based on computer vision, that is suitable for residential use, regarding both cost and size of the equipment. The study aims to investigate the feasibility of such a system.</p><p>The proposed system is based on planar fiducial markers called ArUco. From images or video frames of the marker, the pose of the marker can be estimated. Thus, by placing such markers on the ground, on the wheel and on the vehicle, the estimated pose of the markers can be used to measure and calculate the wheel alignment parameters. Only toe and camber angles are measured within the scope of this thesis, even if the system has the potential to measure other wheel alignment parameters as well.</p><p>After camera calibration, simplified ArUco marker tests were done by measuring the known displacement and inclination of a marker with respect to a reference marker. The mean absolute error was 0&deg;3&prime;4&Prime; and 0.024 mm for the inclination angle and displacement, respectively. Furthermore, the toe and camber angles of a vehicle were measured and compared to reference measurements performed with a commercial wheel alignment system, giving mean absolute errors of 0&deg;52&prime; and 0&deg;28&prime; for the camber and toe angles, respectively. Despite the relatively large errors for the toe and camber angle measurements, the results from the initial inclination and displacement tests show the potential of the system. In addition, several error sources and suggestions for improvement can be identified.</p><p>As a conclusion, the proposed system can be considered a working first prototype, which after improvement and optimisation has the potential to become a feasible alternative, especially for residential use and for mobile workshops due to the low cost, size and usability of the system.</p>
----------------------------------------------------------------------
In diva2:1189528 abstract is:
<p>Road accidents have been a persistent cause of death worldwide, and claim millions of lives everyyear. Recent developments in the active safety systems like Electronic Stability Control (ESC) havehelped in reducing these numbers quite signicantly over the years. However, a major challenge forthese systems is to know the friction coecient between the tire and the road, as this value limits theamount of force the tires can generate. Knowledge of the coecient of friction can be used to adaptthe driving style, thereby avoiding interventions by stability control at the limit, making vehiclessafer. However, it is a major challenge within the automotive industry to estimate the coecientof friction accurately, and with sucient availability, as that requires high levels of tire utilization,such that the tire is forced to reach the non-linear range of operation. Such events are very rarein everyday driving, and requires a system induced active excitation of the tires. One such methodthat has been proposed earlier, to carry out an active tire excitation, is by using a simultaneouspropulsive and brake force on front and the rear the axles. However, applying an equal magnitudeof propulsive and brake force results in a force neutral situation at the vehicle level, which forcesthe velocity to be constant, overriding driver acceleration requests. Thus, an active tire excitationmethod was proposed by Volvo Cars, which is able to apply an unequal propulsive and brake forceto the front and the rear axle, such that the driver's acceleration demand can be met, during frictionestimation. However, such an excitation can be dangerous to carry out, if it leads to instability ofthe vehicle.Several methods have been developed to analyze and quantify stability of a vehicle, but detailedanalysis about the stability under forced excitation, for friction estimation, is very rare. This thesiswork investigates the lateral stability of a vehicle undergoing an active tire excitation for frictionestimation. The objective is to understand which vehicle and tire models can be used to quantifythe lateral stability of a vehicle under forced excitation, and how phase portrait methods can beused to develop a stability monitor that is able to indicate the lateral stability of the vehicle undera forced excitation.The results of using a stability monitor during active tire excitation clearly show that it is able toindicate when the vehicle becomes unstable and looses control. It also shows that for slow speedsteady-state maneuvers and straight line maneuvers, the stability monitor does not indicate instability.A comparison between phase portrait based and conventional side-slip based stability monitorsshow the eectiveness and generality of the phase portrait based monitor, which is able to detectinstability earlier than the conventional side-slip based method.</p>

corrected abstract:
<p>Road accidents have been a persistent cause of death worldwide, and claim millions of lives every year. Recent developments in the active safety systems like Electronic Stability Control (ESC) have helped in reducing these numbers quite significantly over the years. However, a major challenge for these systems is to know the friction coefficient between the tire and the road, as this value limits the amount of force the tires can generate. Knowledge of the coefficient of friction can be used to adapt the driving style, thereby avoiding interventions by stability control at the limit, making vehicles safer. However, it is a major challenge within the automotive industry to estimate the coefficient of friction accurately, and with sufficient availability, as that requires high levels of tire utilization, such that the tire is forced to reach the non-linear range of operation. Such events are very rare in everyday driving, and requires a system induced active excitation of the tires. One such method that has been proposed earlier, to carry out an active tire excitation, is by using a simultaneous propulsive and brake force on front and the rear the axles. However, applying an equal magnitude of propulsive and brake force results in a force neutral situation at the vehicle level, which forces the velocity to be constant, overriding driver acceleration requests. Thus, an active tire excitation method was proposed by Volvo Cars, which is able to apply an unequal propulsive and brake force to the front and the rear axle, such that the driver's acceleration demand can be met, during friction estimation. However, such an excitation can be dangerous to carry out, if it leads to instability of the vehicle.</p><p>Several methods have been developed to analyze and quantify stability of a vehicle, but detailed analysis about the stability under forced excitation, for friction estimation, is very rare. This thesis work investigates the lateral stability of a vehicle undergoing an active tire excitation for friction estimation. The objective is to understand which vehicle and tire models can be used to quantify the lateral stability of a vehicle under forced excitation, and how phase portrait methods can be used to develop a stability monitor that is able to indicate the lateral stability of the vehicle under a forced excitation.</p><p>The results of using a stability monitor during active tire excitation clearly show that it is able to indicate when the vehicle becomes unstable and looses control. It also shows that for slow speed steady-state maneuvers and straight line maneuvers, the stability monitor does not indicate instability. A comparison between phase portrait based and conventional side-slip based stability monitors show the effectiveness and generality of the phase portrait based monitor, which is able to detect instability earlier than the conventional side-slip based method.</p>
----------------------------------------------------------------------
In diva2:1087251 - space missing in title:
"Numerical study on hydraulic verticallift gate during shutdown process"
==>
"Numerical study on hydraulic vertical lift gate during shutdown process"

abstract is:
<p>China is undergoing a rapid increase in their development of hydropower.Due to this rapid increase, China has become one of theleading countries in technological solutions regarding the constructionof the hydropower plant. The hydro resources in China are extensivebut building a new power plant is laborious and costly. Upgrading anexisting power plant is therefore of interest. Increasing the volume flowis one way, but this can bring problems to the hydraulic structures.The design of hydraulic gates is crucial for operating a hydropowerplant safely. An emergency gate is especially important as it protectsthe turbine situated downstream of the gate. In this study, a numericalsimulation of the shutdown process of a hydraulic vertical lift gatewas conducted. The simulation was done in two dimensions using theReynolds Navier Stokes Equations (RANS), together with the RNGk ≠ ‘ turbulence model and the Volume of Fluid method (VOF). Thegoal was to extract the pressure distribution around the gate, subsequently,attaining the hydrodynamic forces and also to observe andanalyze the flow surrounding the gate. The simulation was comparedwith existing experimental data, from a 1/18 scale model, for validation.Once the model was validated, eight different cases were tested toimprove the operating conditions. The closing speed of the gate andthe gate bottom angle was altered in order to reduce the down-pullforce and undesirable flow phenomena. It was found that lowering thegate speed to 8.1 m/min would have positive effect. As the gate closesrelatively fast with reduced forces compared to a faster speed, and withless induced vibrations than with a slower speed. Changing the gatebottom angle from 9¶ to 30¶, would also have a considerable positiveinfluence of the lowered gate vibrations. However changing the bottomangle needs to be more thoroughly studied concerning structuraleffects.</p>

corrected abstract:
<p>China is undergoing a rapid increase in their development of hydropower. Due to this rapid increase, China has become one of the leading countries in technological solutions regarding the construction of the hydropower plant. The hydro resources in China are extensive but building a new power plant is laborious and costly. Upgrading an existing power plant is therefore of interest. Increasing the volume flow is one way, but this can bring problems to the hydraulic structures. The design of hydraulic gates is crucial for operating a hydropower plant safely. An emergency gate is especially important as it protects the turbine situated downstream of the gate. In this study, a numerical simulation of the shutdown process of a hydraulic vertical lift gate was conducted. The simulation was done in two dimensions using the Reynolds Navier Stokes Equations (RANS), together with the RNG <em>k - &epsilon;</em> turbulence model and the Volume of Fluid method (VOF). The goal was to extract the pressure distribution around the gate, subsequently, attaining the hydrodynamic forces and also to observe and analyze the flow surrounding the gate. The simulation was compared with existing experimental data, from a 1/18 scale model, for validation. Once the model was validated, eight different cases were tested to improve the operating conditions. The closing speed of the gate and the gate bottom angle was altered in order to reduce the down-pull force and undesirable flow phenomena. It was found that lowering the gate speed to 8.1 m/min would have positive effect. As the gate closes relatively fast with reduced forces compared to a faster speed, and with less induced vibrations than with a slower speed. Changing the gate bottom angle from 9&deg; to 30&deg;, would also have a considerable positive influence of the lowered gate vibrations. However changing the bottom angle needs to be more thoroughly studied concerning structural effects.</p>
----------------------------------------------------------------------
In diva2:1078078 - missing space and ligature in title:
"An Experimental Study on Global TurbineArray Eects in Large Wind Turbine Clusters"
==>
"An Experimental Study on Global Turbine Array Effects in Large Wind Turbine Clusters"

abstract is:
<p>It is well known that the layout of a large wind turbine cluster aects the energyoutput of the wind farm. The individual placement and distances betweenturbines will in uence the wake spreading and the wind velocity decit. Manyanalytical models and simulations have been made trying to calculate this, butstill there is a lack of experimental data to conrm the models. This thesis isdescribing the preparations and the execution of an experiment that has beenconducted using about 250 small rotating turbine models in a wind tunnel. Theturbine models were developed before the experiment and the characteristicswere investigated. The main focus was laid on special eects occurring in largewind turbine clusters, which were named Global Turbine Array Eects.It was shown that the upstream wind was little aected by a large windfarm downstream, even though there existed a small dierence in wind speedbetween the undisturbed free stream and the wind that arrived to the rstturbines in the wind farm. The dierence in wind speed was shown to beunder 1% of the undisturbed free stream. It was also shown that the densityof the wind farm was related to the reduced wind velocity, with a more densefarm the reduction could get up to 2.5% of the undisturbed free stream at theupstream center turbine. Less velocity decit was observed at the upstreamcorner turbines in the wind farm.When using small rotating turbine models some scaling requirements hadto be considered to make the experiment adaptable to reality. It was concludedthat the thrust coecient of the turbine models was the most important parameterwhen analysing the eects. One problem discussed was the low Reynoldsnumber, an eect always present in wind tunnel studies on small wind turbinemodels.A preliminary investigation of a photo measuring technique was also performed,but the technique was not fully developed. The idea was to take oneor a few photos instantaneously and then calculate the individual rotationalspeed of all the turbine models. It was dicult to apply the technique becauseof uctuations in rotational speed during the experiment, therefore thecalculated values could not represent the mean value over a longer time period.</p>

corrected abstract:
<p>It is well known that the layout of a large wind turbine cluster affects the energy output of the wind farm. The individual placement and distances between turbines will influence the wake spreading and the wind velocity deficit. Many analytical models and simulations have been made trying to calculate this, but still there is a lack of experimental data to confirm the models. This thesis is describing the preparations and the execution of an experiment that has been conducted using about 250 small rotating turbine models in a wind tunnel. The turbine models were developed before the experiment and the characteristics were investigated. The main focus was laid on special effects occurring in large wind turbine clusters, which were named Global Turbine Array Effects.</p><p>It was shown that the upstream wind was little affected by a large wind farm downstream, even though there existed a small difference in wind speed between the undisturbed free stream and the wind that arrived to the first turbines in the wind farm. The difference in wind speed was shown to be under 1% of the undisturbed free stream. It was also shown that the density of the wind farm was related to the reduced wind velocity, with a more dense farm the reduction could get up to 2.5% of the undisturbed free stream at the upstream center turbine. Less velocity deficit was observed at the upstream corner turbines in the wind farm.</p><p>When using small rotating turbine models some scaling requirements had to be considered to make the experiment adaptable to reality. It was concluded that the thrust coefficient of the turbine models was the most important parameter when analysing the effects. One problem discussed was the low Reynolds number, an effect always present in wind tunnel studies on small wind turbine models.</p><p>A preliminary investigation of a photo measuring technique was also performed, but the technique was not fully developed. The idea was to take one or a few photos instantaneously and then calculate the individual rotational speed of all the turbine models. It was difficult to apply the technique because of fluctuations in rotational speed during the experiment, therefore the calculated values could not represent the mean value over a longer time period.</p>
----------------------------------------------------------------------
In diva2:783994 abstract is:
<p>The performance and sound emission of a fan is strongly influenced by the installationeffect, which can be defined as the difference between the performance of a fan in ainstallation and the ideal configuration of the same fan. The factors that one should keepin mind while designing a fan system are many, but if some ground rules are followed thenoise can be drastically reduced. The choice of location for the equipment in the buildingis a critical decision and a less ideal location can result in expensive reconstructions and,or that spaces around the fan room can not be used for its initial purpose. A large fan withlower rotation speed will have lower sound emissions then a smaller fan with a higherrotation speed, for the same air flow. The sound and vibration emissions, as well as theenergy consumption of the fan will be at its lowest values when it is at its point ofmaximum efficiency. The outlet configuration of the duct from the fan should be straightand without dampers or ducts silencers that can create turbulence or a higher staticpressure close to the fan, which will decrease the fans performance drastically.The vibration isolation of the fan should be created and specified for the specificinstallation and not solely the fan characteristics. Proposals to predict and measure thestructure-borne sound pressure and transmissions in buildings have recently beenreleased. With a standard over the structure borne sound, the manufactures can declarethe source data for the fans under different operations. This brings that more accuratepredictions and calculations of the structure borne sound from installations can be done.Earlier calculation methods show big deviances between measured and calculated soundpressure in several cases. Above all the spread of the results is large, which makes themethod somewhat unreliable when sound rating spaces, regarding fan room noise.Calculations and predictions of the sound pressure in a fan room can, after proposals ofchange, be done with a deviation of 10 dBfor all frequencies between 63 and 4000 Hz.The method shows a tendency to overrate the sound pressure with a relatively smallspread of the results. It also shows signs to be able to predict the sound pressure in fanrooms with smaller fans then big fan units.Calculations of the increase of sound pressure that occur in the cavity between the floorstructure and the fan unit show big deviations it if is done for specific frequencies.However results show that calculations of the total sound pressure can be done with abetter accuracy.</p>


corrected abstract:
<p>The performance and sound emission of a fan is strongly influenced by the installation effect, which can be defined as the difference between the performance of a fan in a installation and the ideal configuration of the same fan. The factors that one should keep in mind while designing a fan system are many, but if some ground rules are followed the noise can be drastically reduced. The choice of location for the equipment in the building is a critical decision and a less ideal location can result in expensive reconstructions and, or that spaces around the fan room can not be used for its initial purpose. A large fan with lower rotation speed will have lower sound emissions then a smaller fan with a higher rotation speed, for the same air flow. The sound and vibration emissions, as well as the energy consumption of the fan will be at its lowest values when it is at its point of maximum efficiency. The outlet configuration of the duct from the fan should be straight and without dampers or ducts silencers that can create turbulence or a higher static pressure close to the fan, which will decrease the fans performance drastically.</p><p>The vibration isolation of the fan should be created and specified for the specific installation and not solely the fan characteristics. Proposals to predict and measure the structure-borne sound pressure and transmissions in buildings have recently been released. With a standard over the structure borne sound, the manufactures can declare the source data for the fans under different operations. This brings that more accurate predictions and calculations of the structure borne sound from installations can be done.</p><p>Earlier calculation methods show big deviances between measured and calculated sound pressure in several cases. Above all the spread of the results is large, which makes the method somewhat unreliable when sound rating spaces, regarding fan room noise.</p><p>Calculations and predictions of the sound pressure in a fan room can, after proposals of change, be done with a deviation of ±10 dB for all frequencies between 63 and 4000 Hz. The method shows a tendency to overrate the sound pressure with a relatively small spread of the results. It also shows signs to be able to predict the sound pressure in fan rooms with smaller fans then big fan units. Calculations of the increase of sound pressure that occur in the cavity between the floor structure and the fan unit show big deviations it if is done for specific frequencies. However results show that calculations of the total sound pressure can be done with abetter accuracy.</p>
----------------------------------------------------------------------
In diva2:618588 error in title
"Modeling And Analysis Of Fault Conditions In Avehicle With Four In-Wheel Motors"
==>
"Modeling And Analysis Of Fault Conditions In A Vehicle With Four In-Wheel Motors"

abstract is:
<p>A vast expansion is found in the field of automotive electronic systems. The expansion iscoupled with a related increase in the demands of power and design. Now, this is goodarena of engineering opportunities and challenges. One of the challenges faced, isdeveloping fault tolerant systems, which increases the overall automotive and passengersafety. The development in the field of automotive electronics has led to the innovationof some very sophisticated technology. However, with increasing sophistication intechnology also rises the requirement to develop fault tolerant solutions.As one of many steps towards developing a fault tolerant system, this thesis presents anexhaustive fault analysis. The modeling and fault analysis is carried out for a vehicle withfour in-wheel motors. The primary goal is to collect as many of the possible failuremodes that could occur in a vehicle. A database of possible failure modes is retrievedfrom the Vehicle Dynamics research group at KTH. Now with further inputs to thisdatabase the individual faults are factored with respect to change in parameters of vehicleperformance. The factored faults are grouped with respect to similar outputcharacterization.The fault groups are modeled and integrated into a vehicle model developed earlier inMatlab/Simulink. All the fault groups are simulated under specific conditions and theresults are obtained. The dynamic behavior of the vehicle under such fault conditions isanalyzed. Further, in particular the behavior of the vehicle with electronic stabilitycontrol (ESC) under the fault conditions is tested. The deviation in the vital vehicleperformance parameters from nominal is computed.Finally based on the results obtained, a ranking system termed Severity Ranking System(SeRS) is presented. The severity ranking is presented based on three essential vehicleperformance parameters, such as longitudinal acceleration ( ), lateral acceleration ( )and yaw rate ( ̇ ). The ranking of the faults are classified as low severity S1, mediumseverity S2, high severity S3 and very high severity S4. A fault tolerant system must beable to successfully detect the fault condition, isolate the fault and provide correctiveaction. Hence, this database would serve as an effective input in developing fault tolerantsystems.</p>

corrected abstract:
<p>A vast expansion is found in the field of automotive electronic systems. The expansion is coupled with a related increase in the demands of power and design. Now, this is good arena of engineering opportunities and challenges. One of the challenges faced, is developing fault tolerant systems, which increases the overall automotive and passenger safety. The development in the field of automotive electronics has led to the innovation of some very sophisticated technology. However, with increasing sophistication in technology also rises the requirement to develop fault tolerant solutions.</p><p>As one of many steps towards developing a fault tolerant system, this thesis presents an exhaustive fault analysis. The modeling and fault analysis is carried out for a vehicle with four in-wheel motors. The primary goal is to collect as many of the possible failure modes that could occur in a vehicle. A database of possible failure modes is retrieved from the Vehicle Dynamics research group at KTH. Now with further inputs to this database the individual faults are factored with respect to change in parameters of vehicle performance. The factored faults are grouped with respect to similar output characterization.</p><p>The fault groups are modeled and integrated into a vehicle model developed earlier in Matlab/Simulink. All the fault groups are simulated under specific conditions and the results are obtained. The dynamic behavior of the vehicle under such fault conditions is analyzed. Further, in particular the behavior of the vehicle with electronic stability control (ESC) under the fault conditions is tested. The deviation in the vital vehicle performance parameters from nominal is computed.</p><p>Finally based on the results obtained, a ranking system termed Severity Ranking System (SeRS) is presented. The severity ranking is presented based on three essential vehicle performance parameters, such as longitudinal acceleration (<strong>a<sub>x</sub></strong>), lateral acceleration (<strong>a<sub>y</sub></strong>) and yaw rate (<strong>&psi;&#x307;</strong>). The ranking of the faults are classified as low severity S1, medium severity S2, high severity S3 and very high severity S4. A fault tolerant system must be able to successfully detect the fault condition, isolate the fault and provide corrective action. Hence, this database would serve as an effective input in developing fault tolerant systems.</p>
----------------------------------------------------------------------
In diva2:1670946 - error in title:
"Mechanical Design,Analysis, andManufacturing of Wind Tunnel Modeland support structure"
==>
"Mechanical Design, Analysis, and Manufacturing of Wind Tunnel Model and Support Structure"

abstract is:
<p>This volume covers the phases from design to manufacturing of a wind tunnel testsupport structure for a conceptual blended wingbodyUAV designed by KTH GreenRaven Project students. The innovative aircraft design demonstrates sustainabilitywithin aviation by utilizing a hybrid electricfuelcell propulsion system. The windtunnel test to be conducted at Bristol University will produce data to evaluate theaerodynamic properties of the model for design verification. The wind tunnel modelis a smallscaled1.5mspanmodel supported by struts that change the pitch andyaw angles during testing. An external force balance provided by Bristol Universitymeasures the loads and moments experienced by the model. The main requirementsfor the structure are to withstand the aerodynamic loads imposed by the model andto change the model’s orientation while maintaining wind speed during the test. Themaximum aerodynamic loads were provided in a matrix, the largest of which was usedas the load condition for the support equating to a 512N lift at 14◦ AOA. Trade studieswere conducted to determine the mechanisms to satisfy the requirements while stayingwithin budget. The chosen design for the support structure includes a circular baseplate constrained by a locking ring with positioning pins to change the yaw angle. Themain strut is mounted at the the center of the circular base plate. A hinge bracketat the top of the strut interfaces with another hinge bracket within the model viaa clevis pin. An electric linear actuator mounted downstream of the main strut isused to vary the pitch angle, with the center of rotation at the clevis pin. Once thedesign was finalized, finite element analysis was done to verify the structural stabilityof the design. The FEA results were compared to EulerBernoulliapproximations fordeflection. Manufacturing of the components was outsourcedwhile assembly andprogramming of the actuator was done inhouse.</p>


corrected abstract:
<p>This volume covers the phases from design to manufacturing of a wind tunnel test support structure for a conceptual blended wing-body UAV designed by KTH Green Raven Project students. The innovative aircraft design demonstrates sustainability within aviation by utilizing a hybrid electric-fuel cell propulsion system. The wind tunnel test to be conducted at Bristol University will produce data to evaluate the aerodynamic properties of the model for design verification. The wind tunnel model is a small-scaled 1.5m-span model supported by struts that change the pitch and yaw angles during testing. An external force balance provided by Bristol University measures the loads and moments experienced by the model. The main requirements for the structure are to withstand the aerodynamic loads imposed by the model and to change the model’s orientation while maintaining wind speed during the test. The maximum aerodynamic loads were provided in a matrix, the largest of which was used as the load condition for the support equating to a 512N lift at 14&deg; AOA. Trade studies were conducted to determine the mechanisms to satisfy the requirements while staying within budget. The chosen design for the support structure includes a circular base plate constrained by a locking ring with positioning pins to change the yaw angle. The main strut is mounted at the the center of the circular base plate. A hinge bracket at the top of the strut interfaces with another hinge bracket within the model via a clevis pin. An electric linear actuator mounted downstream of the main strut is used to vary the pitch angle, with the center of rotation at the clevis pin. Once the design was finalized, finite element analysis was done to verify the structural stability of the design. The FEA results were compared to Euler-Bernoulli approximations for deflection. Manufacturing of the components was out-sourced while assembly and programming of the actuator was done in-house.</p>
----------------------------------------------------------------------
In diva2:1547559 abstract is:
<p>A hydraulic damper plays an important role in tuning the handling and comfort characteristicsof a vehicle. Tuning and selecting a damper based on subjective evaluation, by considering theopinions of various users, would be an inefficient method since the comfort requirements of usersvary a lot. Instead, mathematical models of damper and simulation of these models in variousoperating conditions are preferred to standardize the tuning procedure, quantify the comfortlevels and reduce cost of testing. This would require a model, which is good enough to capture thebehaviour of damper in various operating and extreme conditions.The Force-Velocity (FV) curve is one of the most widely used model of a damper. This curve isimplemented either as an equation or as a look-up table. It is a plot between the maximum forceat each peak velocity point. There are certain dynamic phenomena like hysteresis and dependencyon the displacement of damper, which cannot be captured with a FV curve model, but are requiredfor better understanding of the vehicle behaviour.This thesis was conducted in cooperation with Volvo Cars with an aim to improve the existingdamper model which is a Force-Velocity curve. This work focuses on developing a damper model,which is complex enough to capture the phenomena discussed above and simple enough to beimplemented in real time simulations. Also, the thesis aims to establish a standard method toparameterise the damper model and generate the Force-Velocity curve from the tests performedon the damper test rig. A test matrix which includes the standard tests for parameterising andthe extreme test cases for the validation of the developed model will be developed. The final focusis to implement the damper model in a multi body simulation (MBS) software.The master thesis starts with an introduction, where the background for the project is described and then the thesis goals are set. It is followed by a literature review in which fewadvanced damper models are discussed in brief. Then, a step-by-step process of developing thedamper model is discussed along with few more possible options. Later, the construction of a testmatrix is discussed in detail followed by the parameter identification process. Next, the validationof the developed damper model is discussed using the test data from Volvo Hällered ProvingGround (HPG). After validation, implementation of the model in VI CarRealTime and Adams Caralong with the results are presented. Finally the thesis is concluded and the recommendations forfuture work are made on further improving the model.</p>


corrected abstract:
<p>A hydraulic damper plays an important role in tuning the handling and comfort characteristics of a vehicle. Tuning and selecting a damper based on subjective evaluation, by considering the opinions of various users, would be an inefficient method since the comfort requirements of users vary a lot. Instead, mathematical models of damper and simulation of these models in various operating conditions are preferred to standardize the tuning procedure, quantify the comfort levels and reduce cost of testing. This would require a model, which is good enough to capture the behaviour of damper in various operating and extreme conditions.</p><p>The Force-Velocity (FV) curve is one of the most widely used model of a damper. This curve is implemented either as an equation or as a look-up table. It is a plot between the maximum force at each peak velocity point. There are certain dynamic phenomena like hysteresis and dependency on the displacement of damper, which cannot be captured with a FV curve model, but are required for better understanding of the vehicle behaviour.</p><p>This thesis was conducted in cooperation with Volvo Cars with an aim to improve the existing damper model which is a Force-Velocity curve. This work focuses on developing a damper model, which is complex enough to capture the phenomena discussed above and simple enough to be implemented in real time simulations. Also, the thesis aims to establish a standard method to parameterise the damper model and generate the Force-Velocity curve from the tests performed on the damper test rig. A test matrix which includes the standard tests for parameterising and the extreme test cases for the validation of the developed model will be developed. The final focus is to implement the damper model in a multi body simulation (MBS) software.</p><p>The master thesis starts with an introduction, where the background for the project is described and then the thesis goals are set. It is followed by a literature review in which few advanced damper models are discussed in brief. Then, a step-by-step process of developing the damper model is discussed along with few more possible options. Later, the construction of a test matrix is discussed in detail followed by the parameter identification process. Next, the validation of the developed damper model is discussed using the test data from Volvo Hällered Proving Ground (HPG). After validation, implementation of the model in VI CarRealTime and Adams Car along with the results are presented. Finally the thesis is concluded and the recommendations for future work are made on further improving the model.</p>
----------------------------------------------------------------------
In diva2:1465540 abstract is:
<p>There are many applications where 3D models of landscapes can be used, suchas determining volume of objects, inspecting buildings and planning of infrastructure.One common way of creating 3D models of a geographical area is totake overlapping geotagged photos with a drone and then perform an aerotriangulation(AT). The aerotriangulating software find common key points in theimages and create a 3D surface model of the area. This process requires the use ofground control points (GCPs), which are used to map the 3D model onto a globalcoordinate system. These GCPs consume a lot of time to place at the location,measure accurately with total station and manually pinpoint in several photos.The purpose of this study is to compare models created by images taken with differentGNSS-based drone positioning systems and investigate for example howmany GCPs are needed, how the GCPs should be placed, and how the accuracyof models vary between the drone positioning systems considering both relativeand absolute accuracy.Two data acquisition sessions were done where images from two differentlocations were collected. In the first session the use of regular GPS and the use ofa local reference station are used as positioning system for the drone, and in thesecond session network real time kinematics (RTK) is also used as a third kind ofpositioning system.From the produced 3D models there is no significant difference between modelsusing a local reference station and network RTK, but when only using GPS thevertical accuracy drops significantly which means that more GCPs are required inorder for the model to be accurate. The standard deviation for points in createdmodels is calculated in easting, northing and vertical for the coordinate differencesfor the three positioning methods. When no GCPs are used the absoluteaccuracy is drastically lowered to meter level accuracy. In conclusion, by usingnetwork RTK or a local reference station the same accuracy for the 3D model canbe acquired with much fewer GCPs than if stand-alone GPS is used. None ofthe positioning systems can fully replace GCPs when a high absolute accuracy isneeded. With a relative accuracy requirement of 10 cm or more, both networkRTK and use of a local reference station has the potential to provide such qualitythat a 3D model would not need GCPs.</p>


corrected abstract:
<p>There are many applications where 3D models of landscapes can be used, such as determining volume of objects, inspecting buildings and planning of infrastructure. One common way of creating 3D models of a geographical area is to take overlapping geotagged photos with a drone and then perform an aerotriangulation (AT). The aerotriangulating software find common key points in the images and create a 3D surface model of the area. This process requires the use of ground control points (GCPs), which are used to map the 3D model onto a global coordinate system. These GCPs consume a lot of time to place at the location, measure accurately with total station and manually pinpoint in several photos. The purpose of this study is to compare models created by images taken with different GNSS-based drone positioning systems and investigate for example how many GCPs are needed, how the GCPs should be placed, and how the accuracy of models vary between the drone positioning systems considering both relative and absolute accuracy.</p><p>Two data acquisition sessions were done where images from two different locations were collected. In the first session the use of regular GPS and the use of a local reference station are used as positioning system for the drone, and in the second session network real time kinematics (RTK) is also used as a third kind of positioning system.</p><p>From the produced 3D models there is no significant difference between models using a local reference station and network RTK, but when only using GPS the vertical accuracy drops significantly which means that more GCPs are required in order for the model to be accurate. The standard deviation for points in created models is calculated in easting, northing and vertical for the coordinate differences for the three positioning methods. When no GCPs are used the absolute accuracy is drastically lowered to meter level accuracy. In conclusion, by using network RTK or a local reference station the same accuracy for the 3D model can be acquired with much fewer GCPs than if stand-alone GPS is used. None of the positioning systems can fully replace GCPs when a high absolute accuracy is needed. With a relative accuracy requirement of 10 cm or more, both network RTK and use of a local reference station has the potential to provide such quality that a 3D model would not need GCPs.</p>
----------------------------------------------------------------------
In diva2:1142969 abstract is:
<p>Standardized information and mathematicalmodels, which model the characteristics of the power generationand power transmission systems, are requirements for futuredevelopment and maintenance of different applications tooperate the electrical grid. Available databases such as Nordpoolprovides large amounts of data for power supply and demand [1].The typical misconception with open availability of data is thatexisting power system software tools can interact and process thisdata. Difficulties occur mainly because of two reasons. The firston is the amount of data produced. When the topology of theelectrical grid changes e.g. when a switch opens or closes, the flowof electrical power changes. This event produce changes ingeneration, transmission and distribution of the energy anddifferent data sets are produced. The second problem is therepresentation of information [2]. There are a limited number ofsoftware tools that can analyze this data, but each software toolrequires a specific data format structure to run. Dealing withthese difficulties requires an effective way to transform theprovided data representation into new data structures that canbe used in different execution platforms. This project aims tocreate a generic Model-to-Text (M2T) transformation capable oftransforming standardized power system information modelsinto input files executable by the Power System Analysis Tool(PSAT). During this project, a working M2T transformation wasnever achieved. However, missing functionality in someprograms connected to sub processes resulted in unexpectedproblems. This led to a new task of updating the informationmodel interpreter PyCIM. This task is partially completed andcan load basic power system information models.</p>

corrected abstract:
<p>Standardized information and mathematical models, which model the characteristics of the power generation and power transmission systems, are requirements for future development and maintenance of different applications to operate the electrical grid. Available databases such as Nordpool provides large amounts of data for power supply and demand [1]. The typical misconception with open availability of data is that existing power system software tools can interact and process this data. Difficulties occur mainly because of two reasons. The first on is the amount of data produced. When the topology of the electrical grid changes e.g. when a switch opens or closes, the flow of electrical power changes. This event produce changes in generation, transmission and distribution of the energy and different data sets are produced. The second problem is the representation of information [2]. There are a limited number of software tools that can analyze this data, but each software tool requires a specific data format structure to run. Dealing with these difficulties requires an effective way to transform the provided data representation into new data structures that can be used in different execution platforms. This project aims to create a generic Model-to-Text (M2T) transformation capable of transforming standardized power system information models into input files executable by the Power System Analysis Tool (PSAT). During this project, a working M2T transformation was never achieved. However, missing functionality in some programs connected to sub processes resulted in unexpected problems. This led to a new task of updating the information model interpreter PyCIM. This task is partially completed and can load basic power system information models.</p>
----------------------------------------------------------------------
In diva2:405988 abstract is:
<p>Recently, Volvo Construction Equipment AB has developed a weld class system forimperfections in welded joints, which contains demands for the toe radii, cold laps, undercutsetc. and where root defects are treated as requirements on the drawing. In this master thesis, thetoe radius has been studied more carefully along with the selection of reliable measurementsystems which are able to measure the toe radius along the weld. A computerized vision systemhas been evaluated by performing a measurement system analysis. FE-simulations anddestructive fatigue testing has also been carried out to determine which radial geometry beingcritical to the fatigue life.The results show that the currently used methods and gauges do not provide the requiredaccuracy when measuring the toe radius. The gauges are handled differently by differentoperators – even when using the vision system – which makes the methods subjective andtherefore unreliable. There are measuring systems that can gather surface data along the weldwith high accuracy, but there is no reliable method to assess the data. Therefore, the authors havedeveloped an algorithm – named STELIN – that assess the gathered surface data andautomatically identifies and calculates the toe radius and the toe angle along the weld. Using thatinformation an opportunity to improve the process control when welding is possible.The performed FE-calculations show that the surface roughness in the weld toe probably has aninfluence at the fatigue life of the joint. A more precise separate study should be made todetermine the impact of the surface roughness on the fatigue life. Those results should serve as abase when reviewing the theory used when predicting the fatigue life. Currently, stress averagingapproach is used in the notches of the root and the weld toe. In the future though, there might beanother stress condition to be taken into account, if the goal of reducing weight of the finishedproduct shall be achieved. Regarding measuring the surface roughness in the weld toe, theevaluated vision system has enough accuracy to deliver reliable data.More work remains with the STELIN-algorithm. The method used when assessing the calculatedtoe radii should be based on the conclusions from the performed FE-calculations. Integrating theSTELIN-algorithm in a fast feedback measurement system – for instance, on a laser – willprobably provide good opportunities for a better process control in order to achieve higherfatigue life of the welded joint.</p>

corrected abstract:
<p>Recently, Volvo Construction Equipment AB has developed a weld class system for imperfections in welded joints, which contains demands for the toe radii, cold laps, undercuts etc. and where root defects are treated as requirements on the drawing. In this master thesis, the toe radius has been studied more carefully along with the selection of reliable measurement systems which are able to measure the toe radius along the weld. A computerized vision system has been evaluated by performing a measurement system analysis. FE-simulations and destructive fatigue testing has also been carried out to determine which radial geometry being critical to the fatigue life.</p><p>The results show that the currently used methods and gauges do not provide the required accuracy when measuring the toe radius. The gauges are handled differently by different operators – even when using the vision system – which makes the methods subjective and therefore unreliable. There are measuring systems that can gather surface data along the weld with high accuracy, but there is no reliable method to assess the data. Therefore, the authors have developed an algorithm – named STELIN – that assess the gathered surface data and automatically identifies and calculates the toe radius and the toe angle along the weld. Using that information an opportunity to improve the process control when welding is possible.</p><p>The performed FE-calculations show that the surface roughness in the weld toe probably has an influence at the fatigue life of the joint. A more precise separate study should be made to determine the impact of the surface roughness on the fatigue life. Those results should serve as a base when reviewing the theory used when predicting the fatigue life. Currently, stress averaging approach is used in the notches of the root and the weld toe. In the future though, there might be another stress condition to be taken into account, if the goal of reducing weight of the finished product shall be achieved. Regarding measuring the surface roughness in the weld toe, the evaluated vision system has enough accuracy to deliver reliable data.</p><p>More work remains with the STELIN-algorithm. The method used when assessing the calculated toe radii should be based on the conclusions from the performed FE-calculations. Integrating the STELIN-algorithm in a fast feedback measurement system – for instance, on a laser – will probably provide good opportunities for a better process control in order to achieve higher fatigue life of the welded joint.</p>
----------------------------------------------------------------------
In diva2:1800339 abstract is:
<p>Coil supports are integral load-bearing components employed in generators andmotors. They serve the purpose of preventing excessive deformation and maintaininga stable position of the coils responsible for generating power and magnetic fieldswhen rotating. However, a problem with these coil supports is that they block theairflow aimed to cool the coils. Thus, this master thesis aimed to conduct a topologyoptimization to develop a cooling-air permeable coil support and select a suitablematerial. The new design was required to withstand 30,000 operational cycles andan overspeed test running at 120% speed without plastic deformation or failure.</p><p>The material selection process was initiated and based on mechanical and physicalproperties requirements. One of these was that the material should be non-magnetic.Utilizing Ansys Granta EduPack, two materials were suggested, the reference materialcurrently used for the coil support, and a titanium alloy, Ti-6Al-4V. The subsequentstep was to create a CAD model of the original design based on technical drawingsprovided by ABB. With the generated design, finite element analysis (FEA) simulationand the topology optimization could be performed. The generated topology optimizedmodel was modified and two new models were created, one with smaller central cutoutsand one with larger central cutouts and a top surface cutout. Furthermore, a thirdmodel was created based on the fundamentals of fluid mechanics, the Rounded originalmodel. Computational Fluid Dynamics (CFD) simulations of the four models wereexecuted.</p><p>The findings indicate that the design with larger central cutouts exhibited the mostsubstantial increase in airflow through and in between the coil supports, achieving a122 % improvement compared to the original design. The model satisfied the fatiguerequirement and successfully passed the overspeed test. Both the current referencematerial and the Ti-6Al4V alloy are suitable to use for coil support. However, theutilization of a titanium alloy might be deemed excessive in terms of its mechanicalproperties and cost.</p>

corrected abstract:
<p>Coil supports are integral load-bearing components employed in generators and motors. They serve the purpose of preventing excessive deformation and maintaining a stable position of the coils responsible for generating power and magnetic fields when rotating. However, a problem with these coil supports is that they block the airflow aimed to cool the coils. Thus, this master thesis aimed to conduct a topology optimization to develop a cooling-air permeable coil support and select a suitable material. The new design was required to withstand 30,000 operational cycles and an overspeed test running at 120% speed without plastic deformation or failure.</p><p>The material selection process was initiated and based on mechanical and physical properties requirements. One of these was that the material should be non-magnetic. Utilizing Ansys Granta EduPack, two materials were suggested, the reference material currently used for the coil support, and a titanium alloy, Ti-6Al-<sub>4</sub>V. The subsequent step was to create a CAD model of the original design based on technical drawings provided by ABB. With the generated design, finite element analysis (FEA) simulation and the topology optimization could be performed. The generated topology optimized model was modified and two new models were created, one with smaller central cutouts and one with larger central cutouts and a top surface cutout. Furthermore, a third model was created based on the fundamentals of fluid mechanics, the Rounded original model. Computational Fluid Dynamics (CFD) simulations of the four models were executed.</p><p>The findings indicate that the design with larger central cutouts exhibited the most substantial increase in airflow through and in between the coil supports, achieving a 122 % improvement compared to the original design. The model satisfied the fatigue requirement and successfully passed the overspeed test. Both the current reference material and the Ti-6Al<sub>4</sub>V alloy are suitable to use for coil support. However, the utilization of a titanium alloy might be deemed excessive in terms of its mechanical properties and cost.</p>
----------------------------------------------------------------------
In diva2:1701306 abstract is:
<p>A big focus of the Automotive Industry’s work is now on the development ofAutonomous Vehicles (AVs). In order to be able to release them on the market,they need to be tested and validated in a safe and efficient way. That is whycompanies working on the development of AVs use simulation to test the workthey are completing. Before putting an Autonomous Vehicle on any road, itwould be ideal to make sure, it will be able to navigate through the given roadsafely and react to everything that has ever happened on this road. What ismore, for the Autonomous Vehicle to be safely on the roads, it should also beable to react to uncommon situations, not seen exactly in the data it was trainedon before. In this thesis, the focus is on trajectories and their variations. Theaim of this work is to develop a framework, which would allow, having discretedata of traffic participants from chosen locations, to model the trajectories ofthose vehicles and the variations of those trajectories. This is to help withthe testing of Autonomous Vehicles in a simulation environment. The data,which is used to develop this method are from an intersection in Denmark,however, it is believed the method can be applied to data from anywhere,as long as it contains information about x and y coordinates of the vehiclesand the corresponding times of the vehicles being at those positions. In thiswork, only trajectories of cars are considered, but again other vehicles can betaken into account in the future. First, vehicle trajectories from given data aremodelled with the use of B-splines. The routine is set up as a constrainedoptimization problem with seven different constraints developed for a car.The constraints are highly nonlinear and therefore a constrained nonlinearoptimization problem is solved. The chosen method for this is the interior-pointmethod. After obtaining the approximation of the trajectory in the Bsplineform, a variation of it is achieved through the change of the speed of thevehicle and its initial position. A projection of the required velocity change onthe derivative of B-spline basis space is calculated and then a new variation ofthe original approximated trajectory in B-spline form is obtained. The methodwas implemented in Matlab and successfully used to approximate and varytrajectories from a dataset from an intersection in Denmark, Aalborg.</p>

corrected abstract:
<p>A big focus of the Automotive Industry’s work is now on the development of Autonomous Vehicles (AVs). In order to be able to release them on the market, they need to be tested and validated in a safe and efficient way. That is why companies working on the development of AVs use simulation to test the work they are completing. Before putting an Autonomous Vehicle on any road, it would be ideal to make sure, it will be able to navigate through the given road safely and react to everything that has ever happened on this road. What is more, for the Autonomous Vehicle to be safely on the roads, it should also be able to react to uncommon situations, not seen exactly in the data it was trained on before. In this thesis, the focus is on trajectories and their variations. The aim of this work is to develop a framework, which would allow, having discrete data of traffic participants from chosen locations, to model the trajectories of those vehicles and the variations of those trajectories. This is to help with the testing of Autonomous Vehicles in a simulation environment. The data, which is used to develop this method are from an intersection in Denmark, however, it is believed the method can be applied to data from anywhere, as long as it contains information about x and y coordinates of the vehicles and the corresponding times of the vehicles being at those positions. In this work, only trajectories of cars are considered, but again other vehicles can be taken into account in the future. First, vehicle trajectories from given data are modelled with the use of B-splines. The routine is set up as a constrained optimization problem with seven different constraints developed for a car. The constraints are highly nonlinear and therefore a constrained nonlinear optimization problem is solved. The chosen method for this is the interior-point method. After obtaining the approximation of the trajectory in the B-spline form, a variation of it is achieved through the change of the speed of the vehicle and its initial position. A projection of the required velocity change on the derivative of B-spline basis space is calculated and then a new variation of the original approximated trajectory in B-spline form is obtained. The method was implemented in Matlab and successfully used to approximate and vary trajectories from a dataset from an intersection in Denmark, Aalborg.</p>
----------------------------------------------------------------------
In diva2:1350191 abstract is:
<p>Bending stiness is one of the most important mechanical properties in paperboard making,giving rigidity to panels and boxes. This property is currently only possible to measure bydestructive measure o the production line. The current quality control method is decient byassuming a non-realistic consistency of the paperboard properties along the machine direction.The objective of this thesis is to predict the thickness and bending stiness of the nal boardsfrom process data.Two modelling approaches are used: the rst model calculates the bending stiness from acalculated thickness, while the other one uses the measured baseboard thickness. Both modelsuse common inputs such as material properties and grammage measurement. The grammage istaken from the online baseboard measurement. The material properties come from laboratorymeasurements and assumptions. It is assumed that the density ratio between the outer andmiddle plies is constant for all product lines, at all times. The TSI of each ply is dened fromtensile testing experiments and nominal bending stiness. It is also assumed that the coatingdoes not contribute to bending stiness. The two models use equations based on laminatetheory assuming orthotropic layers and neglecting the interlaminar shear forces. The modelsuse data of two dierent natures: i.e. laboratory data and online data. Laboratory data is usedas a comparative to evaluate the models' performance of calculated values from online data.The results show various levels of prediction accuracy for dierent paperboard grades. Theaverage thickness predictions are all underestimations within a 5% error while the bendingstiness estimations vary much more from product to product; varying from 9% underestimationto 32% overestimation. The bending stiness prediction for CD is consistently higher thanfor MD for both models. Most product lines have better results with the calculated thickness,approach 1. The calculated thickness is always underestimated and bending stiness is overestimated,hence the better results with the rst approach.The most important conclusion from the models' results is the spread of laboratory measurements,when compared to the predicted values. The large variation most likely comes fromproduction, implying inconsistencies in the manufacturing process that are not accounted forby the models. These modelling approaches have failed to capture the production variationsbecause of the lack of input parameters.</p>

corrected abstract:
<p>Bending stiffness is one of the most important mechanical properties in paperboard making, giving rigidity to panels and boxes. This property is currently only possible to measure by destructive measure off the production line. The current quality control method is deficient by assuming a non-realistic consistency of the paperboard properties along the machine direction. The objective of this thesis is to predict the thickness and bending stiffness of the final boards from process data.</p><p>Two modelling approaches are used: the first model calculates the bending stiffness from a calculated thickness, while the other one uses the measured baseboard thickness. Both models use common inputs such as material properties and grammage measurement. The grammage is taken from the online baseboard measurement. The material properties come from laboratory measurements and assumptions. It is assumed that the density ratio between the outer and middle plies is constant for all product lines, at all times. The TSI of each ply is defined from tensile testing experiments and nominal bending stiffness. It is also assumed that the coating does not contribute to bending stiffness. The two models use equations based on laminate theory assuming orthotropic layers and neglecting the interlaminar shear forces. The models use data of two different natures: i.e. laboratory data and online data. Laboratory data is used as a comparative to evaluate the models' performance of calculated values from online data.</p><p>The results show various levels of prediction accuracy for different paperboard grades. The average thickness predictions are all underestimations within a 5% error while the bending stiffness estimations vary much more from product to product; varying from 9% underestimation to 32% overestimation. The bending stiffness prediction for CD is consistently higher than for MD for both models. Most product lines have better results with the calculated thickness, approach 1. The calculated thickness is always underestimated and bending stiffness is overestimated, hence the better results with the first approach.</p><p>The most important conclusion from the models' results is the spread of laboratory measurements, when compared to the predicted values. The large variation most likely comes from production, implying inconsistencies in the manufacturing process that are not accounted for by the models. These modelling approaches have failed to capture the production variations because of the lack of input parameters.</p>
----------------------------------------------------------------------
In diva2:1183391 abstract is:
<p>Transportation underlines the vehicle industry's critical role in a country's economic future.The amount of goods moved, specically by trucks, is only expected to increase inthe near future. This work attempts to tackle the problem of optimizing fuel consumptionin Volvo trucks, when there are hard constraints on the delivery time and speed limits.Knowledge of the truck such as position, state, conguration etc., along with the completeroute information of the transport mission is used for fuel optimization.Advancements in computation, storage, and communication on cloud based systems, hasmade it possible to easily incorporate such systems in assisting modern eet. In this work,an algorithm is developed in a cloud based system to compute a speed plan for the completemission for achieving fuel minimization. This computation is decoupled from thelocal control operations on the truck such as prediction control, safety, cruise control, etc.;and serves as a guide to the truck driver to reach the destination on time by consumingminimum fuel.To achieve fuel minimization under hard constraints on delivery (or arrival) time andspeed limits, a non-linear optimization problem is formulated for the high delity modelestimated from real-time drive cycles. This optimization problem is solved using a Nonlinearprogramming solver in Matlab.The optimal policy was tested on two drive cycles provided by Volvo. The policy wascompared with two dierent scenarios, where the mission demands hard constraints ontravel time and the speed limits in addition to no trac uncertainties (deterministic). with a cruise controller running at a constant set speed throughout the mission. Itis observed that there is no signicant fuel savings. with maximum possible fuel consumption; achieved without the help of optimalspeed plan (worst case). It is seen that there is a notable improvement in fuelsaving.In a real world scenario, a transport mission is interrupted by uncertainties such as trac ow, road blocks, re-routing, etc. To this end, a stochastic optimization algorithm is proposedto deal with the uncertainties modeled using historical trac ow data. Possiblesolution methodologies are suggested to tackle this stochastic optimization problem.</p>


corrected abstract:
<p>Transportation underlines the vehicle industry's critical role in a country's economic future. The amount of goods moved, specifically by trucks, is only expected to increase in the near future. This work attempts to tackle the problem of optimizing fuel consumption in Volvo trucks, when there are hard constraints on the delivery time and speed limits. Knowledge of the truck such as position, state, configuration etc., along with the complete route information of the transport mission is used for fuel optimization.</p><p>Advancements in computation, storage, and communication on cloud based systems, has made it possible to easily incorporate such systems in assisting modern fleet. In this work, an algorithm is developed in a cloud based system to compute a speed plan for the complete mission for achieving fuel minimization. This computation is decoupled from the local control operations on the truck such as prediction control, safety, cruise control, etc.; and serves as a guide to the truck driver to reach the destination on time by consuming minimum fuel.</p><p>To achieve fuel minimization under hard constraints on delivery (or arrival) time and speed limits, a non-linear optimization problem is formulated for the high fidelity model estimated from real-time drive cycles. This optimization problem is solved using a Nonlinear programming solver in Matlab.</p><p>The optimal policy was tested on two drive cycles provided by Volvo. The policy was compared with two different scenarios, where the mission demands hard constraints on travel time and the speed limits in addition to no traffic uncertainties (deterministic).</p><ul><li>with a cruise controller running at a constant set speed throughout the mission. It is observed that there is no significant fuel savings.</li><li>with maximum possible fuel consumption; achieved without the help of optimal speed plan (worst case). It is seen that there is a notable improvement in fuel saving.</li></ul><p>In a real world scenario, a transport mission is interrupted by uncertainties such as traffic flow, road blocks, re-routing, etc. To this end, a stochastic optimization algorithm is proposed to deal with the uncertainties modeled using historical traffic flow data. Possible solution methodologies are suggested to tackle this stochastic optimization problem.</p>
----------------------------------------------------------------------
In diva2:1142952 abstract is:
<p>Sending bacteria to space is a further step withinthe framework of transporting humans to distant locations inspace. This can build a knowledge platform of how the bacteriabehaves in the space environments, in order to be able to functionin the long term as a LLS (long term life support system), i.ea mini ecology for the space station that handles waste (gas,liquid and solid) and transforms it into food, water and oxygen.By constructing a bacterial experiment (MoreBac) in a smallsatellite and thermally simulating it in space environment, itcan aid future projects performed in similar but larger scales.To visualize the experiment in presentations, a CAD-model ofthe experiment will be designed and constructed in SIEMENSSolid Edge. The thermal analysis is made in Airbus SYSTEMAThermica and will help show on the critical problem, which isto maintain suitable temperature conditions on the microfluidicchip inside the experiment. By performing the simulations, onecan assure that the design is suitable and that the heat gradientis in required intervals for different components. The CADmodelwas designed in a sandwich layout and consist of twoprinted circuit boards, one microfluidic chip and one reservoir.Not specified components of the experiment was not used in theCAD- model since they where still in early development. Thethermal analysis of the experiment was studied in a steady stateenvironment, with boundary conditions of 5˝C in the cold caseand 30˝C in the hot case, which means that the time variablewas not considered. Three configurations of heat dissipation weremade; 16 nodes at the illumination board with 0,05 W each, 16nodes at the detection board with 0,05 W each and finally 36nodes on both PCBs together with 0,025 W each. In the hot case,the microfluidic chip reaches temperatures between 34, 16˝C and42, 15˝C when 0,8 W is equally divided to both PCBs. In thecold case, the microfluidic chip reaches temperatures between13, 82˝C and 22, 32˝C with the same heat distribution as thehot case.</p>

corrected abstract:
<p>Sending bacteria to space is a further step within the framework of transporting humans to distant locations in space. This can build a knowledge platform of how the bacteria behaves in the space environments, in order to be able to function in the long term as a LLS (long term life support system), i.e a mini ecology for the space station that handles waste (gas, liquid and solid) and transforms it into food, water and oxygen. By constructing a bacterial experiment (MoreBac) in a small satellite and thermally simulating it in space environment, it can aid future projects performed in similar but larger scales. To visualize the experiment in presentations, a CAD-model of the experiment will be designed and constructed in SIEMENS Solid Edge. The thermal analysis is made in Airbus SYSTEMA Thermica and will help show on the critical problem, which is to maintain suitable temperature conditions on the microfluidic chip inside the experiment. By performing the simulations, one can assure that the design is suitable and that the heat gradient is in required intervals for different components. The CAD-model was designed in a sandwich layout and consist of two printed circuit boards, one microfluidic chip and one reservoir. Not specified components of the experiment was not used in the CAD- model since they where still in early development. The thermal analysis of the experiment was studied in a steady state environment, with boundary conditions of 5˝C in the cold case and 30˝C in the hot case, which means that the time variable was not considered. Three configurations of heat dissipation were made; 16 nodes at the illumination board with 0,05 W each, 16 nodes at the detection board with 0,05 W each and finally 36 nodes on both PCBs together with 0,025 W each. In the hot case, the microfluidic chip reaches temperatures between 34,16˝C and 42,15˝C when 0,8 W is equally divided to both PCBs. In the cold case, the microfluidic chip reaches temperatures between 13,82˝C and 22,32˝C with the same heat distribution as the hot case.</p>
----------------------------------------------------------------------
In diva2:1083778 error in title:
"Pressure measurements in pulsatingflows"
==>
"Pressure measurements in pulsating flows"

abstract is:
<p>Due to confidentiality several axis in the figures and large parts of the specifics of the resultsand of the experimental setups have been replaced by symbols. Also one section of the report,concerning a prototype sensor, has been removed completely due to the sensitive nature of theresults.Measuring the exhaust gas pressure and the boost pressure at the air intake manifold isconsidered a standard procedure in modern cars and trucks. Although how to measure thepressure accurately for steady flows is well known, the pressure measurements in pulsatingflows is not a trivial task. This theses shows, experimentally, how well the characteristics of apressure measurement systems, using different dimensions of straight pneumatic tubing, canbe predicted using the Helmholtz resonator model. Also how much this resonance influencethe pressure measurements for different pressure transducers used in trucks today. This thesisalso demonstrates the effects that the sampling frequency and the averaging time has on theaccuracy of measuring an average pressure in pulsating gas flows and how clogging of thepneumatic tubes influence the measurements. This was done using two types of experiments;a step response experiment to properly show the characteristics of the measuring system and apulse rig experiment that shows the impact, of the tubing, on the measurements for typicalfrequencies found in medium sized trucks. The experiments shows that the response time andresonance frequency of a measurement system can be predicted with an accuracy of 𝜇! % fortubes longer than 725 mm. It also that the average absolute pressure measurement keeps anaccuracy of 𝜇! % for all tube dimensions, including clogging of the tube with a decrease ofdiameter up to 𝜇! %. It does however show that if the sensor has some internal resonance that4matches the Helmholtz resonance the measurement can be overestimated by over 𝜇! %. Testsof the sampling frequency shows that if the sampling frequency is chosen as a divisor or amultiple of the pulse frequency the error due to averaging is increased by one order ofmagnitude. Using the information given in this thesis it is possible to avoid unnecessary errorswhen performing pressure measurements in a pulsating flow.</p>


corrected abstract:
<p>Due to confidentiality several axis in the figures and large parts of the specifics of the results and of the experimental setups have been replaced by symbols. Also one section of the report, concerning a prototype sensor, has been removed completely due to the sensitive nature of the results.</p><p>Measuring the exhaust gas pressure and the boost pressure at the air intake manifold is considered a standard procedure in modern cars and trucks. Although how to measure the pressure accurately for steady flows is well known, the pressure measurements in pulsating flows is not a trivial task. This theses shows, experimentally, how well the characteristics of a pressure measurement systems, using different dimensions of straight pneumatic tubing, can be predicted using the Helmholtz resonator model. Also how much this resonance influence the pressure measurements for different pressure transducers used in trucks today. This thesis also demonstrates the effects that the sampling frequency and the averaging time has on the accuracy of measuring an average pressure in pulsating gas flows and how clogging of the pneumatic tubes influence the measurements. This was done using two types of experiments; a step response experiment to properly show the characteristics of the measuring system and a pulse rig experiment that shows the impact, of the tubing, on the measurements for typical frequencies found in medium sized trucks. The experiments shows that the response time and oresonance frequency of a measurement system can be predicted with an accuracy of µ<sub>1</sub> % for tubes longer than 725 mm. It also that the average absolute pressure measurement keeps an accuracy of µ<sub>2</sub> % for all tube dimensions, including clogging of the tube with a decrease of diameter up to µ<sub>3</sub> %. It does however show that if the sensor has some internal resonance that matches the Helmholtz resonance the measurement can be overestimated by over µ<sub>4</sub> %. Tests of the sampling frequency shows that if the sampling frequency is chosen as a divisor or a multiple of the pulse frequency the error due to averaging is increased by one order of magnitude. Using the information given in this thesis it is possible to avoid unnecessary errors when performing pressure measurements in a pulsating flow.</p>
----------------------------------------------------------------------
In diva2:860546 abstract is:
<p>Ankylosing Spondylitis (AS), or Bechterew’s disease, is an inflammatory rheumaticdisease that through the formation of additional bone tissue in the spine eventuallyleads to the complete fusion of the vertebrae, in effect turning the spine into one longbone. Due to the reduced flexibility of the spine with the long lever arms, spinalfractures in AS-patients are relatively common even after minor trauma.</p><p>The aim of this thesis was to use an existing finite element model of a healthy spineand adapt it to the conditions of AS, thus gaining some insight into the effects ofsurgical stabilization of cervical fractures, using posterior screws and rods. Althoughthis type of surgery is often performed, it has not been previously investigated in abiomechanical model. This thesis should be considered as a starting point for how afinite element model of the spine could be used to investigate the effect of spinalimplants in the case of a fracture in the ankylosed spine.</p><p>An existing FE-model was modified to some of the conditions of AS: The vertebraewere fused by adding ossifications at the intervertebral discs (with the Head-C1 andC1-C2 joints left mobile). A fracture was simulated at the C6C7 disc level. Fourdifferent implant configurations were tested: Short instrumentation C6C7, mediuminstrumentation C5toT1, long instrumentation C3toT3, and a long instrumentationC3C6C7T3 with skipped intermediate levels. Three loads (1.5g, 3.0g, 4.5g) wereapplied according to a specific load curve. Kinematic data such as the gap distance inthe fracture site were obtained. Furthermore the stresses in the ossified parts of thediscs were evaluated.</p><p>It was shown that the chosen methods of adapting the model to the AS conditions, andmodeling the fracture and implant, changed the kinematics so that less movementoccurred between the vertebra, which is typical for AS. Measured as fracture gap, alltested implant configurations were equally good at stabilizing the fracture, althoughthey all allowed more movement than the non-fractured AS-model did. All implantconfigurations were also able to stabilize the fracture in terms of the horizontal translation in the fracture. The disc ossifications were somewhat shielded from stress for those ossifications that were within the range of the implant. This was so for all implant configurations. No increased stress was observed in the ossifications immediately outside the range for the implants, relative the non-fractured AS-model.</p><p>For the C6C7 and C5toT1 implant configurations as well as the non-fractured ASmodel,the stresses were highest at the T1T2 level. Stresses in the ossifications in the thoracic spine were generally low, apart from the T1T2 level. The results show that the chosen AS-adaptations and the modeled implant seem reasonable for testing some of the considerations of cervical fractures in the ankylosed spine as well as for some implant configurations. The results also make it possible to speculate about the optimal type of implant. The effects of screw placement and anchoring, osteoporosis, muscle activation and possible spinal deformity on the implant stability were not investigated, and should be a matter for further studies.</p>

corrected abstract:
<p>Ankylosing Spondylitis (AS), or Bechterew’s disease, is an inflammatory rheumatic disease that through the formation of additional bone tissue in the spine eventually leads to the complete fusion of the vertebrae, in effect turning the spine into one long bone. Due to the reduced flexibility of the spine with the long lever arms, spinal fractures in AS-patients are relatively common even after minor trauma.</p><p>The aim of this thesis was to use an existing finite element model of a healthy spine and adapt it to the conditions of AS, thus gaining some insight into the effects of surgical stabilization of cervical fractures, using posterior screws and rods. Although this type of surgery is often performed, it has not been previously investigated in a biomechanical model. This thesis should be considered as a starting point for how a finite element model of the spine could be used to investigate the effect of spinal implants in the case of a fracture in the ankylosed spine.</p><p>An existing FE-model was modified to some of the conditions of AS: The vertebrae were fused by adding ossifications at the intervertebral discs (with the Head-C1 and C1-C2 joints left mobile). A fracture was simulated at the C6C7 disc level. Four different implant configurations were tested: Short instrumentation C6C7, medium instrumentation C5toT1, long instrumentation C3 to T3, and a long instrumentation C3C6C7T3 with skipped intermediate levels. Three loads (1.5g, 3.0g, 4.5g) were applied according to a specific load curve. Kinematic data such as the gap distance in the fracture site were obtained. Furthermore the stresses in the ossified parts of the discs were evaluated.</p><p>It was shown that the chosen methods of adapting the model to the AS conditions, and modeling the fracture and implant, changed the kinematics so that less movement occurred between the vertebra, which is typical for AS. Measured as fracture gap, all tested implant configurations were equally good at stabilizing the fracture, although they all allowed more movement than the non-fractured AS-model did. All implant configurations were also able to stabilize the fracture in terms of the horizontal translation in the fracture. The disc ossifications were somewhat shielded from stress for those ossifications that were within the range of the implant. This was so for all implant configurations. No increased stress was observed in the ossifications immediately outside the range for the implants, relative the non-fractured AS-model.</p><p>For the C6C7 and C5toT1 implant configurations as well as the non-fractured AS-model, the stresses were highest at the T1T2 level. Stresses in the ossifications in the thoracic spine were generally low, apart from the T1T2 level.</p><p>The results show that the chosen AS-adaptations and the modeled implant seem reasonable for testing some of the considerations of cervical fractures in the ankylosed spine as well as for some implant configurations. The results also make it possible to speculate about the optimal type of implant. The effects of screw placement and anchoring, osteoporosis, muscle activation and possible spinal deformity on the implant stability were not investigated, and should be a matter for further studies.</p>
----------------------------------------------------------------------
In diva2:643818 abstract is:
<p>This master thesis, which has been carried out in collaboration with Fairtrade,investigates how sustainable development can be integrated in the mathematicaldiscipline in the Swedish upper secondary school. The study includedfindings on design and disposition, with sustainability themes relatedto Fairtrade. This since Fairtrade is going to use the study as a basis forthe development of an educational material. The results of the study areof interest to teachers and organizations that want to develop educationalmaterial that integrate sustainable development in mathematics education.The main focus of the study has been the teachers’ preferences on dispositionand design.Semi-structured interviews were conducted with seven mathematics teachersin the Stockholm area. A workshop was also conducted with three mathematicsstudent teachers at the end of their training, and two people workingat Fairtrade. The result of the study shows that it is important that theeducational material corresponds to the achievement goals of the mathematicscourses. It also shows the importance of making the material differsfrom ordinary learning milieu, as this would inspire teachers to vary theirteaching. The results also indicate that mathematics education in the exerciseparadigm is not suitable for integrating sustainable development inmathematics education. Instead an investigative approach and working withtasks referring to real-life situations where the students are encouraged totake a position on questions of sustainable development is preferred. For instance,child labor and poverty can be investigated using mathematics andby linking those problems to consumptions the students can question theirown role as consumers.</p>

corrected abstract:
<p>This master thesis, which has been carried out in collaboration with Fairtrade, investigates how sustainable development can be integrated in the mathematical discipline in the Swedish upper secondary school. The study included findings on design and disposition, with sustainability themes related to Fairtrade. This since Fairtrade is going to use the study as a basis for the development of an educational material. The results of the study are of interest to teachers and organizations that want to develop educational material that integrate sustainable development in mathematics education. The main focus of the study has been the teachers’ preferences on disposition and design.</p><p>Semi-structured interviews were conducted with seven mathematics teachers in the Stockholm area. A workshop was also conducted with three mathematics student teachers at the end of their training, and two people working at Fairtrade. The result of the study shows that it is important that the educational material corresponds to the achievement goals of the mathematics courses. It also shows the importance of making the material differs from ordinary learning milieu, as this would inspire teachers to vary their teaching. The results also indicate that mathematics education in the exercise paradigm is not suitable for integrating sustainable development in mathematics education. Instead an investigative approach and working with tasks referring to real-life situations where the students are encouraged to take a position on questions of sustainable development is preferred. For instance, child labor and poverty can be investigated using mathematics and by linking those problems to consumptions the students can question their own role as consumers.</p>
----------------------------------------------------------------------
In diva2:405926 - missing space in title:
"Development of a dynamic calculation tool forsimulation of ditching"
==>
"Development of a dynamic calculation tool for simulation of ditching"

abstract is:
<p>The present document is the final master thesis report written by Marc PILORGET,student at SUPAERO (home institution) and KTH (Royal Institute of Technology,Exchange University). This six months internship was done at DASSAULT AVIATION(Airframe engineering department) based in Saint-Cloud, France. It spanned from the 5thof July to the 23rd of December. The thesis work aims at developing an SPH (SmoothParticle Hydrodynamics) calculation method for ditching and implementing it in the finiteelement software ELFINI® developed by DASSAULT. Ditching corresponds to a phasewhen the aeroplane is touching the water. The problematic of ditching has always beenan area of interest for DASSAULT and the whole aeronautical industry. So far, only testsand simple analytical calculations have been performed. Most of the work was carried bythe NACA (National Advisory Committee for Aeronautics) in the late 70's. However in thepast decade, a new method for fluid-structure coupling problems has been developed. Itis called SPH. The basic principle is the following: the domain is represented by means ofparticles and each particle of fluid is treated separately and submitted to the Navier-Stokes equations. The particle is influenced by the neighbouring particles with a weightfunction depending on the distance between the two particles. Particles are also placed atthe interface solid-fluid: they are called limit particles. The final purpose of this SPHmethod is to access to the structural response of an aircraft when ditching. The crucialinterest of such a method compared to methods used so far is the absence of mesh. Theanalysis of large deformation problems by the finite element method may require thecontinuous remeshing of the domain to avoid the breakdown of the calculation due toexcessive mesh distortion. When considering ditching or other large deformationsproblems, the mesh generation is a far more time-consuming task than the constructionand solution of a discrete set of equations. For DASSAULT-AVIATION, the long termobjective is to get a numerical tool able to model ditching. The SPH method is used tosolve the equations for the fluid and is coupled with a finite element method for thestructure. So far, the compressible solver for 2D geometries has been implemented.Tests are going to be performed to ensure the program’s robustness. Then theincompressible solver for 2D geometries will be studied both theoretically andnumerically.</p>

corrected abstract:
<p>The present document is the final master thesis report written by Marc PILORGET, student at SUPAERO (home institution) and KTH (Royal Institute of Technology, Exchange University). This six months internship was done at DASSAULT AVIATION (Airframe engineering department) based in Saint-Cloud, France. It spanned from the 5<sup>th</sup> of July to the 23<sup>rd</sup> of December. The thesis work aims at developing an SPH (Smooth Particle Hydrodynamics) calculation method for ditching and implementing it in the finite element software ELFINI® developed by DASSAULT. Ditching corresponds to a phase when the aeroplane is touching the water. The problematic of ditching has always been an area of interest for DASSAULT and the whole aeronautical industry. So far, only tests and simple analytical calculations have been performed. Most of the work was carried by the NACA (National Advisory Committee for Aeronautics) in the late 70's. However in the past decade, a new method for fluid-structure coupling problems has been developed. It is called SPH. The basic principle is the following: the domain is represented by means of particles and each particle of fluid is treated separately and submitted to the Navier-Stokes equations. The particle is influenced by the neighbouring particles with a weight function depending on the distance between the two particles. Particles are also placed at the interface solid-fluid: they are called limit particles. The final purpose of this SPH method is to access to the structural response of an aircraft when ditching. The crucial interest of such a method compared to methods used so far is the absence of mesh. The analysis of large deformation problems by the finite element method may require the continuous remeshing of the domain to avoid the breakdown of the calculation due to excessive mesh distortion. When considering ditching or other large deformations problems, the mesh generation is a far more time-consuming task than the construction and solution of a discrete set of equations. For DASSAULT-AVIATION, the long term objective is to get a numerical tool able to model ditching. The SPH method is used to solve the equations for the fluid and is coupled with a finite element method for the structure. So far, the compressible solver for 2D geometries has been implemented. Tests are going to be performed to ensure the program’s robustness. Then the incompressible solver for 2D geometries will be studied both theoretically and numerically.
----------------------------------------------------------------------
In diva2:1707348 abstract is:
<p>This thesis studies the implementation of an Explicit Algebraic Reynolds-Stress Model(EARSM) for Atmospheric Boundary Layer (ABL) in an open source ComputationalFluid Dynamics (CFD) software, OpenFOAM, following the guidance provided by thewind company ENERCON that aims to make use of this novel model to improvesites’ wind-field predictions. After carefully implementing the model in OpenFOAM,the EARSM implementation is verified and validated by testing it with a stratifiedCouette flow case. The former was done by feeding mean flow properties, takenfrom OpenFOAM, in a python tool containing the full EARSM system of equationsand constants, and comparing the resulting flux profiles with the ones extracted bythe OpenFOAM simulations. Subsequently, the latter was done by comparing theprofiles of the two universal functions used by Monin-Obukhov Similarity Theory(MOST) for mean velocity and temperature to the results obtained by Želi et al. intheir study of the EARSM applied to a single column ABL, in “Modelling of stably-stratified, convective and transitional atmospheric boundary layers using the explicitalgebraic Reynolds-stress model” (2021). The verification of the model showed minordifferences between the flux profiles from the python tool and OpenFOAM thus, themodel’s implementation was deemed verified, while the validation step showed nodifference in the unstable and neutral stratification cases, but a significant discrepancyfor stably stratified flow. Nonetheless, the reason behind the inconsistency is believedto be related to the choice of boundary conditions thus, the model’s implementationitself is considered validated.</p><p>Finally, the comparison between the EARSM and the k − ε model showed thatthe former is able to capture the physics of the flow properties where the latter failsto. In particular, the diagonal momentum fluxes resulting from the EARSM reflectthe observed behaviour of being different from each other, becoming isotropic withaltitude in the case of unstable stratification, and having magnitude u′u′ &gt; v′v′ &gt; w′w′ for stably stratified flows. On the other hand, the eddy viscosity assumption used bythe k − ε model computes the diagonal momentum fluxes as being equal to each other.Moreover, the EARSM captures more than one non-zero heat flux component in theCouette flow case, which has been observed to be the case in literature, while the eddydiffusivity assumption used by the k − ε model only accounts for one non-zero heat fluxcomponent.</p><p> </p>

corrected abstract:
<p>This thesis studies the implementation of an Explicit Algebraic Reynolds-Stress Model (EARSM) for Atmospheric Boundary Layer (ABL) in an open source Computational Fluid Dynamics (CFD) software, OpenFOAM, following the guidance provided by the wind company ENERCON that aims to make use of this novel model to improve sites’ wind-field predictions. After carefully implementing the model in OpenFOAM, the EARSM implementation is verified and validated by testing it with a stratified Couette flow case. The former was done by feeding mean flow properties, taken from OpenFOAM, in a python tool containing the full EARSM system of equations and constants, and comparing the resulting flux profiles with the ones extracted by the OpenFOAM simulations. Subsequently, the latter was done by comparing the profiles of the two universal functions used by Monin-Obukhov Similarity Theory (MOST) for mean velocity and temperature to the results obtained by Želi et al. in their study of the EARSM applied to a single column ABL, in “Modelling of stably-stratified, convective and transitional atmospheric boundary layers using the explicit algebraic Reynolds-stress model” (2021). The verification of the model showed minor differences between the flux profiles from the python tool and OpenFOAM thus, the model’s implementation was deemed verified, while the validation step showed no difference in the unstable and neutral stratification cases, but a significant discrepancy for stably stratified flow. Nonetheless, the reason behind the inconsistency is believed to be related to the choice of boundary conditions thus, the model’s implementation itself is considered validated.</p><p>Finally, the comparison between the EARSM and the <em>k − ϵ</em> model showed that the former is able to capture the physics of the flow properties where the latter fails to. In particular, the diagonal momentum fluxes resulting from the EARSM reflect the observed behaviour of being different from each other, becoming isotropic with altitude in the case of unstable stratification, and having magnitude <em><span style="border-top: 1px solid black; padding: 0.2rem;">u&prime;u&prime;</span></em> &gt; <em><span style="border-top: 1px solid black; padding: 0.2rem;">v&prime;v&prime;</span></em> &gt; <em><span style="border-top: 1px solid black; padding: 0.2rem;">w&prime;w&prime;</span></em> for stably stratified flows. On the other hand, the eddy viscosity assumption used by the <em>k − ϵ</em> model computes the diagonal momentum fluxes as being equal to each other. Moreover, the EARSM captures more than one non-zero heat flux component in the Couette flow case, which has been observed to be the case in literature, while the eddy diffusivity assumption used by the <em>k − ϵ</em> model only accounts for one non-zero heat flux component.</p>


Note: In the corrected abstract I could not use an overline, as the primes punctured the overline; therefore, I used the border-top echanism and raised by adding more padding.
----------------------------------------------------------------------
In diva2:1228966 abstract is:
<p>Much research is done today on how to make vehicles autonomous. But the main focuslies in how to make the techniques and safety sufficient. This means that the comfort forthe passengers has fallen behind. Studies show that approximately 25 % of the users ofautonomous vehicles would experience motion sickness.The purpose of this thesis is to use existing hypotheses about what causes motion sicknessto analyse different technical solutions that could decrease motion sickness inautonomous vehicles.To investigate this a literature study is done. Only existing research and experiments areused.The report is based on the theories about the sensor conflict and postural instability. Thetheory about the sensor conflict means that a person gets symptoms of motion sicknesswhen the visual impression doesn’t match with the ones from the balance organs. Thetheory about postural instability says that the motion sickness is caused when the braindoesn’t have control over the posture of the body.The different solutions analysed were active suspension, installing a screen on which thepassengers do all the activities and a Virtual Reality headset where the passengers gets apreview of the road to make their body prepare for the movement of the cars so that theycan make a countermovement. The conclusions are that the VR-headset can’t be usedbecause it limits the user possibilities to do other activities while traveling. The screen inthe middle of the passenger’s view can work for the activities that can be done using ascreen but doesn’t work for other activities. The solution with active suspension is apromising solution but perhaps a bit expensive.Finally a new solution to the problem is presented by the authors. The new solution isactive suspension of the car seats and is a mix of the two analysed solutions VirtualRealityheadset and active suspension.</p>


corrected abstract:
<p>Much research is done today on how to make vehicles autonomous. But the main focus lies in how to make the techniques and safety sufficient. This means that the comfort for the passengers has fallen behind. Studies show that approximately 25 % of the users of autonomous vehicles would experience motion sickness.</p><p>The purpose of this thesis is to use existing hypotheses about what causes motion sickness to analyse different technical solutions that could decrease motion sickness in autonomous vehicles.</p><p>To investigate this a literature study is done. Only existing research and experiments are used.</p><p>The report is based on the theories about the sensor conflict and postural instability. The theory about the sensor conflict means that a person gets symptoms of motion sickness when the visual impression doesn’t match with the ones from the balance organs. The theory about postural instability says that the motion sickness is caused when the brain doesn’t have control over the posture of the body.</p><p>The different solutions analysed were active suspension, installing a screen on which the passengers do all the activities and a Virtual Reality headset where the passengers gets a preview of the road to make their body prepare for the movement of the cars so that they can make a countermovement. The conclusions are that the VR-headset can’t be used because it limits the user possibilities to do other activities while traveling. The screen in the middle of the passenger’s view can work for the activities that can be done using a screen but doesn’t work for other activities. The solution with active suspension is a promising solution but perhaps a bit expensive.</p><p>Finally a new solution to the problem is presented by the authors. The new solution is active suspension of the car seats and is a mix of the two analysed solutions Virtual Reality headset and active suspension.</p>
----------------------------------------------------------------------
In diva2:408836 abstract is:
<p>This study illuminates how the science center as a concept can be developed and how asociocultural perspective on learning influences the design of an interactive exhibit. The aimof the study is to propose ideas on how a science center can be designed and developed withthe purpose of creating good conditions for learning.The work was divided in three parts. In the first part literature was studied with theaim of highlighting aspects important for learning from a sociocultural perspective. In thesecond part an educational model was formulated based on the result from the literature study,interviews and study visits. The educational model was then used to guide the design of aninteractive exhibit on hydro power. The third part consists of an evaluation of the exhibitbased on observation of the visitors’ interaction with the exhibit.In this thesis the work and the result of the three parts are presented leading to a finaldiscussion about the key question of the study: How can learning possibilities be createdthrough the design of an interactive exhibition?Creating possibilities for learning is about creating possibilities for activities thatmakes learning possible. Through the design and in the choice of content of an exhibition it ispossible to create more or less good conditions for learning. Therefore it is important to havea clear picture of what type of activities it is desirable that an exhibition invite the visitor to,for example cooperation and conversation and the visitors’ possibility to influence the resultof the activity.The traditional science center is often criticized for presenting science and technologyas something static and finished. To create interest and engagement for the subject area it isinstead needed to be presented from a wide range of perspectives. Therefore throughout thework of developing an interactive exhibition or a science center it is important to discuss howthe subject area can be presented to fulfil this aim.Creating and developing an exhibition is a work that involves a number of people withdifferent backgrounds, knowledge and ideas. Just as in any other project that involves manypeople a clear ambition with clear goals is needed and makes a shared vision possible that canlead and steer all parts of the work.</p>

corrected abstract:
<p>This study illuminates how the science center as a concept can be developed and how a sociocultural perspective on learning influences the design of an interactive exhibit. The aim of the study is to propose ideas on how a science center can be designed and developed with the purpose of creating good conditions for learning.</p><p>The work was divided in three parts. In the first part literature was studied with the aim of highlighting aspects important for learning from a sociocultural perspective. In the second part an educational model was formulated based on the result from the literature study, interviews and study visits. The educational model was then used to guide the design of an interactive exhibit on hydro power. The third part consists of an evaluation of the exhibit based on observation of the visitors’ interaction with the exhibit.</p><p>In this thesis the work and the result of the three parts are presented leading to a final discussion about the key question of the study: How can learning possibilities be created through the design of an interactive exhibition?</p><p>Creating possibilities for learning is about creating possibilities for activities that makes learning possible. Through the design and in the choice of content of an exhibition it is possible to create more or less good conditions for learning. Therefore it is important to have a clear picture of what type of activities it is desirable that an exhibition invite the visitor to, for example cooperation and conversation and the visitors’ possibility to influence the result of the activity.</p><p>The traditional science center is often criticized for presenting science and technology as something static and finished. To create interest and engagement for the subject area it is instead needed to be presented from a wide range of perspectives. Therefore throughout the work of developing an interactive exhibition or a science center it is important to discuss how the subject area can be presented to fulfil this aim.</p><p>Creating and developing an exhibition is a work that involves a number of people with different backgrounds, knowledge and ideas. Just as in any other project that involves many people a clear ambition with clear goals is needed and makes a shared vision possible that can lead and steer all parts of the work.</p>
----------------------------------------------------------------------
In diva2:401129 abstract is:
<p>CFD use has increased signi cantly in airplane conception, and the industry demands more andmore precise and reliable tools. This was the goal of the SimSAC project. The result is CEASIOM,a computerized environment made of several modules for the design and prediction of the aircraft'scharacteristics. It constructs aerodynamic tables used in the prediction of the characteristics of anaircraft. In simple ight conditions, simple computation methods are used, whereas in complex ightconditions,involving turbulences, more advanced methods are used. This reduces the computationalcost, but the tables resulting from di erent delity sources must be fused to obtain a coherent tablecovering the whole ight envelope.The goal of this project was to realize the fusion. Additionally, a lter and a custom-made mapping toenhance the accuracy of the results from the fusion were required. The addition of helpful visualizationtools was suggested. The whole should be integrated in the CEASIOM interface as a Fusion module.For this, 6 functions were coded. The rst one loads the data sets. The second, myplot, allows theengineer by plotting the data in a coherent way, to spot any big mistakes or incompatibility in thedata sets. The third, myvisual, displays the elements spotted as outliers or potentially out of pattern.This is used by the next function, my ltermap, to lter out the erroneous data. This function alsorealizes the custom-made mapping.The fth function, myfusion, fuses the data and saves it in a .xmlCEASIOM formatted structure to be used by the next CEASIOM module. The sixth function ltersout, in the same way as my ltermap, the outliers from the fused data, and saves the ltered fused dataset in a .xml CEASIOM formatted structure. Finally, a Matlab GUI was implemented and integratedinto the main CEASIOM interface.The module works perfectly, except for the mapping part, that needs a few readjustments.</p>


corrected abstract:
<p>CFD use has increased significantly in airplane conception, and the industry demands more and more precise and reliable tools. This was the goal of the SimSAC project. The result is CEASIOM, a computerized environment made of several modules for the design and prediction of the aircraft's characteristics. It constructs aerodynamic tables used in the prediction of the characteristics of an aircraft. In simple flight conditions, simple computation methods are used, whereas in complex flight conditions, involving turbulences, more advanced methods are used. This reduces the computational cost, but the tables resulting from different fidelity sources must be fused to obtain a coherent table covering the whole flight envelope.</p><p>The goal of this project was to realize the fusion. Additionally, a filter and a custom-made mapping to enhance the accuracy of the results from the fusion were required. The addition of helpful visualization tools was suggested. The whole should be integrated in the CEASIOM interface as a Fusion module. For this, 6 functions were coded. The first one loads the data sets. The second, myplot, allows the engineer by plotting the data in a coherent way, to spot any big mistakes or incompatibility in the data sets. The third, myvisual, displays the elements spotted as outliers or potentially out of pattern. This is used by the next function, myfiltermap, to filter out the erroneous data. This function also realizes the custom-made mapping. The fifth function, myfusion, fuses the data and saves it in a .xml CEASIOM formatted structure to be used by the next CEASIOM module. The sixth function filters out, in the same way as myfiltermap, the outliers from the fused data, and saves the filtered fused data set in a .xml CEASIOM formatted structure. Finally, a Matlab GUI was implemented and integrated into the main CEASIOM interface.</p><p>The module works perfectly, except for the mapping part, that needs a few readjustments.</p>
----------------------------------------------------------------------
In diva2:1818051 abstract is:
<p>The oceans are a key element in our society, economy and environmental system.They cover over 70% of the worlds surface and contribute substantially to ecosystemservices such as climate management as well as to economic sectors such as foodproduction and tourism. While the importance of the oceans for climate changeand the society is generally acknowledged in science and literature, it is often notreflected in policy. Integrated Assessment Models (IAMs) which are used to advicepolicy on carbon prices often systematically omit process and damages related tothe ocean such as ocean acidification, loss of biodiversity and changes in oceancurrents.The aim of this study is to give a more detailed perspective on ocean related processesand their role and importance for the economy under climate change and to testassumptions made in the development of IAMs - and more precisely the DynamicIntegrated Climate-Economy model also referred to as the DICE model. The initialresults of the DICE model resulted in a optimal temperature trajectory with amaximum of 4 ◦C contradicting the goals set with the Paris Agreement.This thesis is the first of its kind attempt in reviewing the most recentbiophysical evidence on climate change impacts with a focus on marine systemsand incorporating these damages to market and non-market sectors into the DICEmodel. The impacts from climate change are implemented into the DICE modelthrough economic valuation of the damages and an update of the damage function.The analysis is based on the damage function used in the original DICE2016R2model as well as the suggested update presented by Hänsel et al. (2020)The results show, that incorporating marine damages into the model yields in amajor increase in economic damages particularly in the temperature range up to 2◦C.These increased damages influence the results of the optimal temperature trajectoryand give a clear indication for a more stringent climate policy, drastically limitingthe maximum temperature increase compared to the original DICE model.</p>


corrected abstract:
<p>The oceans are a key element in our society, economy and environmental system. They cover over 70% of the worlds surface and contribute substantially to ecosystem services such as climate management as well as to economic sectors such as food production and tourism. While the importance of the oceans for climate change and the society is generally acknowledged in science and literature, it is often not reflected in policy. Integrated Assessment Models (IAMs) which are used to advice policy on carbon prices often systematically omit process and damages related to the ocean such as ocean acidification, loss of biodiversity and changes in ocean currents.</p><p>The aim of this study is to give a more detailed perspective on ocean related processes and their role and importance for the economy under climate change and to test assumptions made in the development of IAMs - and more precisely the Dynamic Integrated Climate-Economy model also referred to as the DICE model. The initial results of the DICE model resulted in a optimal temperature trajectory with a maximum of 4 ˚C contradicting the goals set with the Paris Agreement.</p><p>This thesis is the first of its kind attempt in reviewing the most recent biophysical evidence on climate change impacts with a focus on marine systems and incorporating these damages to market and non-market sectors into the DICE model. The impacts from climate change are implemented into the DICE model through economic valuation of the damages and an update of the damage function. The analysis is based on the damage function used in the original DICE2016R2 model as well as the suggested update presented by Hänsel et al. (2020)</p><p>The results show, that incorporating marine damages into the model yields in a major increase in economic damages particularly in the temperature range up to 2˚C. These increased damages influence the results of the optimal temperature trajectory and give a clear indication for a more stringent climate policy, drastically limiting the maximum temperature increase compared to the original DICE model.</p>
----------------------------------------------------------------------
In diva2:1800176 abstract is:
<p>Bolted joints are important due to their energy dissipation property in structures,but the damping mechanism is also highly nonlinear and localized. The goal ofthis thesis is to develop an accurate method for modeling bolted joint dampingin large structures using finite element (FE) software. To model bolted jointdamping, the first step is to study the mechanism and define the terms like slip,micro-slip, and macro-slip. An extensive literature review identified the necessarymethods: detailed contact model, thin-layer elements, and connector elements.These methods are compared based on parameters such as computation time,modeling time, etc. The thin-layer method was used for modeling bolted jointdamping in large structures.</p><p>To evaluate parameters for thin-layer element modeling, a local joint model wasbuilt using contact formulation of an engine housing and ladder frame assembly.The computed parameters include normal stiffness, tangential stiffness, and lossfactor. Analysis reveals that the loss factor depends on pre-load and amplitudeload. The micro-slip is the region of interest where the loss factor was computed. Using curve-fitting, a range of amplitude-dependent loss factors was calculated.</p><p>Finally, the thin-layer elements are used in the engine housing and ladder frameassembly to model bolted joint damping. The parameters estimated using the localjoint model are used to define the properties of the thin-layer elements such thatthe elements are a phenomenological representation of bolted joint. A mode-basedsteady-state analysis has been performed to estimate the loss factor on a systemlevel. The frequency response of such an analysis accurately captures the frequencyresponse curves of structures with bolted joints. The two important behaviors thathave been captured are the shifting of the resonance peak to a lower value and thewidening of the frequency response curve as the applied load increases. However,the resonance frequency shifting to a lower frequency (softening) has not beencaptured due to modeling limitations in the FE software. A substructure couplingmodel using the Craig-Bampton formulation of the engine housing and ladderframe assembly has been analyzed using a constant loss factor. The frequencyresponse of such a system appears to give an approximate behavior of a structurewith bolted joint damping.</p>

corrected abstract:
<p>Bolted joints are important due to their energy dissipation property in structures, but the damping mechanism is also highly nonlinear and localized. The goal of this thesis is to develop an accurate method for modeling bolted joint damping in large structures using finite element (FE) software. To model bolted joint damping, the first step is to study the mechanism and define the terms like slip, micro-slip, and macro-slip. An extensive literature review identified the necessary methods: detailed contact model, thin-layer elements, and connector elements. These methods are compared based on parameters such as computation time, modeling time, etc. The thin-layer method was used for modeling bolted joint damping in large structures.</p><p>To evaluate parameters for thin-layer element modeling, a local joint model was built using contact formulation of an engine housing and ladder frame assembly. The computed parameters include normal stiffness, tangential stiffness, and loss factor. Analysis reveals that the loss factor depends on pre-load and amplitude load. The micro-slip is the region of interest where the loss factor was computed. Using curve-fitting, a range of amplitude-dependent loss factors was calculated.</p><p>Finally, the thin-layer elements are used in the engine housing and ladder frame assembly to model bolted joint damping. The parameters estimated using the local joint model are used to define the properties of the thin-layer elements such that the elements are a phenomenological representation of bolted joint. A mode-based steady-state analysis has been performed to estimate the loss factor on a system level. The frequency response of such an analysis accurately captures the frequency response curves of structures with bolted joints. The two important behaviors that have been captured are the shifting of the resonance peak to a lower value and the widening of the frequency response curve as the applied load increases. However, the resonance frequency shifting to a lower frequency (softening) has not been captured due to modeling limitations in the FE software. A substructure coupling model using the Craig-Bampton formulation of the engine housing and ladder frame assembly has been analyzed using a constant loss factor. The frequency response of such a system appears to give an approximate behavior of a structure with bolted joint damping.</p>
----------------------------------------------------------------------
In diva2:1698414 abstract is:
<p>The impact of air travel on the climate, along with its increasing share in CO2 emissions haveraised the demand for sustainable air travel solutions. The current aircraft technologies haveseen significant improvement throughout the years. Although, the rate at which new aircrafttechnologies are developed can not keep up with the increased demand for air travel. Hence, adifferent approach to reduce the aviation’s impact on climate can be achieved by optimizing thevertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming.Upon departure, an optimization of the vertical flight path is initiated and an optimal flight planis suggested to the flight crew.</p><p>The fuel saving produced by the optimal flight plan is a potential saving that can only be fullyachieved if the flight crew chose to fly according to the optimized flight path. However, restrictionsfrom the Air Traffic Control, as well as the flight crew’s willingness to follow theoptimized flight path can affect the achieved saving. Hence, a tool is developed in order tocompute trip fuel consumption from post-flight data obtained from the Automatic DependentSurveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start andend positions of cruise segments is successfully implemented. Two methods of calculating thefuel are implemented and compared. The first method is based on simulating the actual flight,which uses the same performance model as for the simulation of the operational flight plantrip and optimized trip. The second method is based on utilizing the ADS-B data to obtain theaircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft,hence the trip is not simulated. The results reveals that the simulation method produces flighttrajectories that are comparable to the operational and optimized flight plans since they use thesame model structure. However, using ADS-B data to obtain fuel consumption represents theactual flight trajectory more accurately.</p><p>Furthermore, an optimization algorithm based on the on-board Flight Management Computeris implemented. According to the results, the FMC optimization offers a sufficient optimizationof the cruise phase, when compared to the OFP trip, however performs worse than the dynamicprogramming, which provides a global optimal solution</p>


corrected abstract:
<p>The impact of air travel on the climate, along with its increasing share in CO2 emissions have raised the demand for sustainable air travel solutions. The current aircraft technologies have seen significant improvement throughout the years. Although, the rate at which new aircraft technologies are developed can not keep up with the increased demand for air travel. Hence, a different approach to reduce the aviation’s impact on climate can be achieved by optimizing the vertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming. Upon departure, an optimization of the vertical flight path is initiated and an optimal flight plan is suggested to the flight crew.</p><p>The fuel saving produced by the optimal flight plan is a potential saving that can only be fully achieved if the flight crew chose to fly according to the optimized flight path. However, restrictions from the Air Traffic Control, as well as the flight crew’s willingness to follow the optimized flight path can affect the achieved saving. Hence, a tool is developed in order to compute trip fuel consumption from post-flight data obtained from the Automatic Dependent Surveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start and end positions of cruise segments is successfully implemented. Two methods of calculating the fuel are implemented and compared. The first method is based on simulating the actual flight, which uses the same performance model as for the simulation of the operational flight plan trip and optimized trip. The second method is based on utilizing the ADS-B data to obtain the aircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft, hence the trip is not simulated. The results reveals that the simulation method produces flight trajectories that are comparable to the operational and optimized flight plans since they use the same model structure. However, using ADS-B data to obtain fuel consumption represents the actual flight trajectory more accurately.</p><p>Furthermore, an optimization algorithm based on the on-board Flight Management Computer is implemented. According to the results, the FMC optimization offers a sufficient optimization of the cruise phase, when compared to the OFP trip, however performs worse than the dynamic programming, which provides a global optimal solution</p>
----------------------------------------------------------------------
In diva2:1216784 missing space in title:
"Mobile Network trafficprediction: Based on machine learning"
==>
"Mobile Network traffic prediction: Based on machine learning"

The following abstract is in DiVA - but it belongs to another thesis. Compare to the Swedish abstract!

abstract is:
<p>The investing market can be a cold ruthless placefor the layman. In order to get the chance of making money inthis business one must place countless hours on research, withmany different parameters to handle in order to reach success.To reduce the risk, one must look to many different companiesoperating in multiple fields and industries. In other words, it canbe a hard task to manage this feat.With modern technology, there is now lots of potential tohandle this tedious analysis autonomously using machine learningand clever algorithms. With this approach, the amount ofanalyzes is only limited by the capacity of the computer. Resultingin a number far greater than if done by hand.This study aims at exploring the possibilities to modify andimplement efficient algorithms in the field of finance. The studyutilizes the power of kernel methods in order to algorithmicallyanalyze the patterns found in financial data efficiently. Bycombining the powerful tools of change point detection andnonlinear regression the computer can classify the differenttrends and moods in the market.The study culminates to a tool for analyzing data from thestock market in a way that minimizes the influence from shortspikes and drops, and instead is influenced by the underlying pattern.But also, an additional tool for predicting future movementsin the price.</p>

corrected abstract:
<p>The amount of data traffic sent through mobile networks varies throughout the day and week. Thus, the network experiences varying demand and therefore, the load on all the back end systems in the core network is far from constant. By being able to predict the load, the back end system capacity can be optimized during the day, reducing maintenance costs and energy consumption, affecting the environment positively. The predictions may also be used for network planning.</p><p>The aim of this project was to predict the mobile network data traffic based on two weeks of data aggregated into five minute intervals. The data was treated as a time series and time series forecasting methods were used, the ARIMA model using external regressors based on a polynomial model and a Fourier series as well as the TBATS model. Also, a recurrent neural network based on a method called Long Short Term Memory was used.</p><p>The results show that the seasonal components of the time series are modelled well using simple methods such as a polynomial model or Fourier series. However, modelling the dynamics of the stationary time series is very difficult and the ARIMA model did not perform well in this situation due to the long time predictions made. Neither did the neural network or TBATS model manage to model the stationary dynamics and were only able to capture the seasonal components.</p>
----------------------------------------------------------------------
In diva2:1799891 abstract is:
<p>This thesis examines water mixing and exchange in a drinking water reservoir operated by themunicipal association Norrvatten. Recent water samples from the reservoir’s outgoing waterhave shown an increase in culturable bacteria during late summer and fall. This thesis utilizesComputational Fluid Dynamics (CFD) modeling and analysis in OpenFOAM to simulatereservoir inflow and outflow, analyzing mixing processes and their relationship to operationalstrategies. The objective is to understand the correlation between the residence time of waterand microbial growth and propose operational improvements to increase the exchange of waterin order to achieve improved water quality. A trace element was implemented in the CFDmodel to simulate the residence time of water. Initial simulations were based on the reservoir’shistorical operational data, utilizing temperature and water level measurements providedby Norrvatten. After the initial simulations, four alternative simulations were performed,comparing different operational strategies by modifying inflow parameters. Inflow parametersthat were changed were the volumetric inflow rate, water level variation, and the temperatureof the inflowing water. The post­processing in ParaView focused on the thermal stratificationand residence time distribution near the outlet during each mixing process. The study revealeda complex relationship between flow conditions and microbial growth, making it challengingto identify a clear pattern. However, based on the simulations with the alternative operationalstrategies it was concluded that the set of operational strategies called ”Strategy 1” generated themost optimal flow conditions. This strategy involves a three times larger volumetric inflow rate(an increase from 0.05 to 0.15 m^3/s) and a water level that is kept at the same values comparedto the original simulation. Strategy 1 resulted in a 3.6 % higher water exchange compared to theoriginal simulation. In comparison to the other simulated strategies, Strategy 1 generates thehighest water exchange, with a 63.6 % increase compared to the worst­-case scenario involvingcolder inflow. The conclusion that could be drawn is that the most favorable operationalstrategies involve higher volumetric inflow rates, lower water levels, and an incoming watertemperature that is higher than the initial reservoir temperature.</p><p> </p>

corrected abstract:
<p>This thesis examines water mixing and exchange in a drinking water reservoir operated by the municipal association Norrvatten. Recent water samples from the reservoir’s outgoing water have shown an increase in culturable bacteria during late summer and fall. This thesis utilizes Computational Fluid Dynamics (CFD) modeling and analysis in OpenFOAM to simulate reservoir inflow and outflow, analyzing mixing processes and their relationship to operational strategies. The objective is to understand the correlation between the residence time of water and microbial growth and propose operational improvements to increase the exchange of water in order to achieve improved water quality. A trace element was implemented in the CFD model to simulate the residence time of water. Initial simulations were based on the reservoir’s historical operational data, utilizing temperature and water level measurements provided by Norrvatten. After the initial simulations, four alternative simulations were performed, comparing different operational strategies by modifying inflow parameters. Inflow parameters that were changed were the volumetric inflow rate, water level variation, and the temperature of the inflowing water. The post­processing in ParaView focused on the thermal stratification and residence time distribution near the outlet during each mixing process. The study revealed a complex relationship between flow conditions and microbial growth, making it challenging to identify a clear pattern. However, based on the simulations with the alternative operational strategies it was concluded that the set of operational strategies called ”Strategy 1” generated the most optimal flow conditions. This strategy involves a three times larger volumetric inflow rate (an increase from 0.05 to 0.15 m<sup>3</sup>/s) and a water level that is kept at the same values compared to the original simulation. Strategy 1 resulted in a 3.6 % higher water exchange compared to the original simulation. In comparison to the other simulated strategies, Strategy 1 generates the highest water exchange, with a 63.6 % increase compared to the worst-case scenario involving colder inflow. The conclusion that could be drawn is that the most favorable operational strategies involve higher volumetric inflow rates, lower water levels, and an incoming water temperature which is higher than the initial reservoir temperature.</p>
----------------------------------------------------------------------
In diva2:1740181 abstract is:
<p>This project is about the design process of a resonancemitigating algorithm for a large solar sail. The solar sail isa structure made up by four long booms in a cross patternwith suspended sails between the edges of the booms. Thespacecraft in this report is controlled using smaller rotatingsails at each tip of the booms. Since the booms are longthey will experience significant bending moment even thoughthe actual force from the control sails will be small. Thisbending has a high risk of exciting the spacecraft’s resonancemodes, which will in turn complicate the control of theentire spacecraft. Because of this it is necessary to designan algorithm to mitigate resonance excitation. To design thisalgorithm three main steps were taken, an approximation ofspacecraft resonance modes, a valid mathematical model ofthe system and robustness building to handle model error.The resonance modes were approximated through analyticalformulas. The boom model was determined by combiningdifferent approaches from previous similar works. Finally,the controller chosen was a simple PID controller with abuilt-in saturation limiter to make sure the controller stayswithin the spacecraft’s operating bounds. To ensure robustness,multiple test simulations were made on systems with differentresonance modes compared to the true system. The chosencontroller passed these tests. Ultimately the chosen controllerreduced the settling time of the resonance oscillations by 75%.</p>

corrected abstract:
<p>This project is about the design process of a resonance mitigating algorithm for a large solar sail. The solar sail is a structure made up by four long booms in a cross pattern with suspended sails between the edges of the booms. The spacecraft in this report is controlled using smaller rotating sails at each tip of the booms. Since the booms are long they will experience significant bending moment even though the actual force from the control sails will be small. This bending has a high risk of exciting the spacecraft’s resonance modes, which will in turn complicate the control of the entire spacecraft. Because of this it is necessary to design an algorithm to mitigate resonance excitation. To design this algorithm three main steps were taken, an approximation of spacecraft resonance modes, a valid mathematical model of the system and robustness building to handle model error. The resonance modes were approximated through analytical formulas. The boom model was determined by combining different approaches from previous similar works. Finally, the controller chosen was a simple PID controller with a built-in saturation limiter to make sure the controller stays within the spacecraft’s operating bounds. To ensure robustness, multiple test simulations were made on systems with different resonance modes compared to the true system. The chosen controller passed these tests. Ultimately the chosen controller reduced the settling time of the resonance oscillations by 75%.</p>
----------------------------------------------------------------------
In diva2:1571119 abstract is:
<p>Shaped charges (SC) have been used as a means of explosives in military andcivilian use for decades. Thus, there is a substantial amount of research behindthis area. However, as this is a sensitive subject much of this research is notpublicly available.</p><p>This thesis will look at how one can use asymmetries in SC’s to velocity compensatethe jet formation. Velocity compensation is required when the SC is perpendicularto the projectile direction, hence, leading to an angled jet which decreases thepenetration potential.</p><p>The asymmetries that were investigated are• off-­center detonation• angled liner• displaced wave shaper• displaced wave shaper &amp; angled liner.</p><p>The 3D explosive simulation was conducted in IMPETUS AFEA solver and tocompare the performance of these asymmetries the position and velocity of thejet were measured. To create a baseline a simulation without any asymmetrieswas used.</p><p>The off­-center detonation showed some velocity compensating characteristicsat the tip of the jet. However, as the jet progressed it converged towards thereference.</p><p>Angled liner simulations were conducted with an angle of 0.5 degrees and 1 degreeand these asymmetries behaved vastly differently. Angled Liner 0.5 degrees hada greater jet angle but a greater quantity of the jet particles were concentratedaround one point increasing the penetration potential. A general characteristicthat angled liner displaced was the fact that it had desirable velocity compensatingtraits all through the jet.</p><p>Displaced Wave Shaper, like off­-center detonation, showed promising velocitycompensating attributes at the tip of the jet, however, it too converged towardsthe reference on the later part of the jet.</p><p>When combining the displaced wave shaper and angled liner asymmetries thedesire was to also combine their velocity compensating traits, i.e achievingthe displaced wave shaper’s tip compensation with the angled liner’s totalcompensation. Unfortunately, this was not achieved. The tip, again, showedpromising velocity compensating attributes but the rest of the jet convergedtowards the reference.</p><p>Conclusively, angled liner shows the highest potential for compensating thevelocity and allowed the most amount of jet particles to be concentrated aroundone point increasing the penetration potential.</p><p> </p>

corrected abstract:
<p>Shaped charges (SC) have been used as a means of explosives in military and civilian use for decades. Thus, there is a substantial amount of research behind this area. However, as this is a sensitive subject much of this research is not publicly available.</p><p>This thesis will look at how one can use asymmetries in SC’s to velocity compensate the jet formation. Velocity compensation is required when the SC is perpendicular to the projectile direction, hence, leading to an angled jet which decreases the penetration potential.</p><p>The asymmetries that were investigated are<ul><li>off-center detonation</li><li>angled liner</li><li>displaced wave shaper</li><li>displaced wave shaper &amp; angled liner.</li></ul></p><p>The 3D explosive simulation was conducted in IMPETUS AFEA solver and to compare the performance of these asymmetries the position and velocity of the jet were measured. To create a baseline a simulation without any asymmetries was used.</p><p>The off-center detonation showed some velocity compensating characteristics at the tip of the jet. However, as the jet progressed it converged towards the reference.</p><p>Angled liner simulations were conducted with an angle of 0.5 degrees and 1 degree and these asymmetries behaved vastly differently. Angled Liner 0.5 degrees had a greater jet angle but a greater quantity of the jet particles were concentrated around one point increasing the penetration potential. A general characteristic that angled liner displaced was the fact that it had desirable velocity compensating traits all through the jet.</p><p>Displaced Wave Shaper, like off-center detonation, showed promising velocity compensating attributes at the tip of the jet, however, it too converged towards the reference on the later part of the jet.</p><p>When combining the displaced wave shaper and angled liner asymmetries the desire was to also combine their velocity compensating traits, i.e achieving the displaced wave shaper’s tip compensation with the angled liner’s total compensation. Unfortunately, this was not achieved. The tip, again, showed promising velocity compensating attributes but the rest of the jet converged towards the reference.</p><p>Conclusively, angled liner shows the highest potential for compensating the velocity and allowed the most amount of jet particles to be concentrated around one point increasing the penetration potential.</p>
----------------------------------------------------------------------
In diva2:1527916 - missing space in title:
"Connected Tyres: Real-time Tyre Monitoring System for Fleet& Autonomous Vehicles with Tyre WearEstimation through Sensor Fusion"
==>
"Connected Tyres: Real-time Tyre Monitoring System for Fleet & Autonomous Vehicles with Tyre Wear Estimation through Sensor Fusion"

abstract is:
<p>Tyres are one crucial part for vehicles, as they are the only contact pointbetween the vehicle and the road. Intelligent tyres are a trending new subjectin the tyre industry. They are designed to monitor various tyre states and sendthis information to both drivers and remote servers. The master thesis focuseson the proposal of a real-time tyre monitoring system for fleet and autonomousvehicles. It includes developing a tyre wear model and analysis of the currenttyre pressure monitoring functionality by leveraging the connectivity of fleetvehicles equipped with a Volvo web cloud service. The tyre wear model indirectlymonitors the tread depth of the vehicles all four tyres by identifyingcharacteristics between worn and fresh tyres. The two characteristics are identifiedby monitoring and analyzing vehicle speed and braking signals. The twocharacteristics is input to a voting scheme which decides when a worn tyre isdetected. The test vehicle was a Volvo XC40 with three types of tyres: wintertyres, summer tyres and worn summer tyres. The wear model gives 90 %accuracy to 10 set of test data, randomly selected from all dataset at HälleredProving Ground (Sweden). The connectivity realizes the data transmissionfrom the raw data of onCAN and FlexRay signals stored in a Volvo web cloudservice to the tyre monitoring fleet system. The signals are filtered and resampled,leaving the required signals of the tyre pressure monitor system andthe tyre wear model. Two signals, Calibration Status and iTPMS Status, areused to perform a statistical analysis on tyre pressure by categorizing the calibrationstatus and the tyre pressure conditions.The project outcome is an interfacebuilt on MATLAB GUI for demonstration of vehicle identification andtyre health conditions, with the embedded tyre wear model and connectivity.</p>

corrected abstract:
<p>Tyres are one crucial part for vehicles, as they are the only contact point between the vehicle and the road. Intelligent tyres are a trending new subject in the tyre industry. They are designed to monitor various tyre states and send this information to both drivers and remote servers. The master thesis focuses on the proposal of a real-time tyre monitoring system for fleet and autonomous vehicles. It includes developing a tyre wear model and analysis of the current tyre pressure monitoring functionality by leveraging the connectivity of fleet vehicles equipped with a Volvo web cloud service. The tyre wear model indirectly monitors the tread depth of the vehicles all four tyres by identifying characteristics between worn and fresh tyres. The two characteristics are identified by monitoring and analyzing vehicle speed and braking signals. The two characteristics is input to a voting scheme which decides when a worn tyre is detected. The test vehicle was a Volvo XC40 with three types of tyres: winter tyres, summer tyres and worn summer tyres. The wear model gives 90 % accuracy to 10 set of test data, randomly selected from all dataset at Hällered Proving Ground (Sweden). The connectivity realizes the data transmission from the raw data of on CAN and FlexRay signals stored in a Volvo web cloud service to the tyre monitoring fleet system. The signals are filtered and resampled, leaving the required signals of the tyre pressure monitor system and the tyre wear model. Two signals, Calibration Status and iTPMS Status, are used to perform a statistical analysis on tyre pressure by categorizing the calibration status and the tyre pressure conditions. The project outcome is an interface built on MATLAB GUI for demonstration of vehicle identification and tyre health conditions, with the embedded tyre wear model and connectivity.</p>
----------------------------------------------------------------------
In diva2:1142785 - possible duplicate 'diva2:1120402

abstract is:
<p>Due to demographic changes, the transportationdemand is predicted to increase significantly in the next decades.Considering the transport sector’s impact on society and theenvironment, the development of a sustainable transport systemis of great importance. Two possible building blocks in such asystem are connectivity and automation, and this project aims tostudy a way of combining these two.The purpose of this project is to investigate how the introductionof autonomous minibuses to a pre-existing bus systemwould affect its operational cost and environmental impact. Thisis done using a linear programming model that finds the optimalcombination of conventional buses and autonomous minibuseswith respect to cost. The model is implemented in the modellingsystem GAMS for bus lines 1–4 in Stockholm using data ontravel demand. Two scenarios are analysed; the first allowingan arbitrary number of minibuses, and the second being morerealistic and restricting the number of minibuses. The solutionsare then compared to the corresponding solutions using onlyconventional buses.In both cases, the results indicate that considerable savingscan be obtained while maintaining or even improving availability.From this, we draw the conclusion that when such technologyis truly available, it would be advisable to investigate if thesesavings can weigh up the costs related to necessary investments.</p>

corrected abstract:
<p>Due to demographic changes, the transportation demand is predicted to increase significantly in the next decades. Considering the transport sector’s impact on society and the environment, the development of a sustainable transport system is of great importance. Two possible building blocks in such a system are connectivity and automation, and this project aims to study a way of combining these two.</p><p>The purpose of this project is to investigate how the introduction of autonomous minibuses to a pre-existing bus system would affect its operational cost and environmental impact. This is done using a linear programming model that finds the optimal combination of conventional buses and autonomous minibuses with respect to cost. The model is implemented in the modelling system GAMS for bus lines 1–4 in Stockholm using data on travel demand. Two scenarios are analysed; the first allowing an arbitrary number of minibuses, and the second being more realistic and restricting the number of minibuses. The solutions are then compared to the corresponding solutions using only conventional buses.</p><p>In both cases, the results indicate that considerable savings can be obtained while maintaining or even improving availability. From this, we draw the conclusion that when such technology is truly available, it would be advisable to investigate if these savings can weigh up the costs related to necessary investments.</p>
----------------------------------------------------------------------
In diva2:1110767 abstract is:
<p>The present work reports the first systematic results obtained in wind tunnel and at full scale with theexperimental apparatus developed within the joint project among Politecnico di Milano, North Sails andCSEM. The steady state upwind aerodynamics of sailing yachts are investigated through the contemporarymeasurement of global forces, distributed pressures and sail flying shapes, and with numerical simulationsbased on Potential Theory and Navier-Stokes equations.The wind tunnel of Politecnico di Milano and the Sailing Yacht Lab (SYL), used for full scale investigations,are described together with the measurement systems adopted for recording forces, pressures and sailshapes. The aerodynamic loads are obtained through dedicated arrangements of load cells; the pressuredistributions on sail sections are evaluated with integrated systems of customized local measurementsolutions and MEMS sensors; the sail flying shapes are detected thanks to two laser scanners based of theTime of Flight technology. The experimental procedures adopted during tests are presented and discussedin relation with the aim of the work.Numerical simulations of selected wind tunnel cases are performed in order to assess the capabilities of theempirical techniques to validate Computational Fluid Dynamics (CFD) codes and to propose solid numericalset-ups for investigating sailing yacht aerodynamics. A Vortex Lattice Method (VLM) code, written inMatlab, is used for quick preliminary analyses of the global forces developed by the sail plan, whereas theopen source environment OpenFOAM is adopted to perform 3D simulations for investigating in details thelocal flow patterns.The experimental apparatus, both at model and full scale, proved to be extremely well suited for thepurposes of the study, giving remarkable results regarding, in particular, pressures and sail shapes. Theexpected distributions are obtained during wind tunnel tests and interesting considerations arise from thecomparison with the full scale outcomes. Numerically, the results of simulations meet the preliminaryintuitions with the VLM code capable of accurately predicting global forces up to certain wind angles andthe RANS-based computations providing notable agreement with the measured local pressures.</p>

corrected abstract:
<p>The present work reports the first systematic results obtained in wind tunnel and at full scale with the experimental apparatus developed within the joint project among Politecnico di Milano, North Sails and CSEM. The steady state upwind aerodynamics of sailing yachts are investigated through the contemporary measurement of global forces, distributed pressures and sail flying shapes, and with numerical simulations based on Potential Theory and Navier-Stokes equations.</p><p>The wind tunnel of Politecnico di Milano and the Sailing Yacht Lab (SYL), used for full scale investigations, are described together with the measurement systems adopted for recording forces, pressures and sail shapes. The aerodynamic loads are obtained through dedicated arrangements of load cells; the pressure distributions on sail sections are evaluated with integrated systems of customized local measurement solutions and MEMS sensors; the sail flying shapes are detected thanks to two laser scanners based of the Time of Flight technology. The experimental procedures adopted during tests are presented and discussed in relation with the aim of the work.</p><p>Numerical simulations of selected wind tunnel cases are performed in order to assess the capabilities of the empirical techniques to validate Computational Fluid Dynamics (CFD) codes and to propose solid numerical set-ups for investigating sailing yacht aerodynamics. A Vortex Lattice Method (VLM) code, written in Matlab, is used for quick preliminary analyses of the global forces developed by the sail plan, whereas the open source environment OpenFOAM is adopted to perform 3D simulations for investigating in details the local flow patterns.</p><p>The experimental apparatus, both at model and full scale, proved to be extremely well suited for the purposes of the study, giving remarkable results regarding, in particular, pressures and sail shapes. The expected distributions are obtained during wind tunnel tests and interesting considerations arise from the comparison with the full scale outcomes. Numerically, the results of simulations meet the preliminary intuitions with the VLM code capable of accurately predicting global forces up to certain wind angles and the RANS-based computations providing notable agreement with the measured local pressures.</p>
----------------------------------------------------------------------
In diva2:1083779 abstract is:
<p>Carbonated sparkling water has been widely used from ancient age [1]. The original ideacame from natural sparkling water and people believed that taking baths at carbonatedhot springs was good for health and healed their sicknesses. This fact led people to startthinking that sparkling water could have more effective uses. Joseph Priestley success-fully produced artificial carbonated water in 1767 and sparkling water quickly becamewidely spread because it gives people refreshing feeling. The bottled and canned beverageindustry has grown from the 19th century and has become one of the biggest markets inthe world. According to Bloomberg Intelligence and Euromonitor, the global market ofthe carbonated beverages is around 350 billion dollar. One main drawback was that itwas not possible to re-cork the bottle to save the carbonation so that once it was opened,fizz was kept only for a short time. In 1813, the method to dispense a portion of carbon-ated water was invented by Charles Plinth[2]. This was the origin of the Soda Syphon.As the demand of sparkling water increased, the machine with which people could makesparkling water by themselves was introduced. Recently, it has become a very popularhome appliance, especially in Europe and North America. The most common way tocarbonate water is by injecting high-pressure CO2 into a water bottle. However, currentsystems waste a lot of CO2 during this carbonating process. In this thesis, the flow insidethe bottle during the injection of CO2 into water was studied in order to determine the pa-rameters that had most influence on the carbonation process. CFD (Computational FluidDynamics) simulations were performed in STAR-CCM+ of an axisymmetric 2D modeland a 3D model that was a 30 degree wedge of the real bottle shape. The Volume of Fluidmethod was used to solve the multiphase flow of gas and liquid. The RANS approachwas used with k 􀀀ϵ model and implicit time marching. To validate the simulations, axialpropagation of the volume fraction of CO2 was compared with the experimental visual-ization of the CO2 and H2O distribution. At the beginning of the phenomena, the gaspropagation was reasonably predicted and the results capture the features of the bubbleshape. However the results did not perfectly match with the experimental visualization.To seek the reason for the unrealistic results, the grid sensitivity study was performedand to consider the 3D effect the results with the 2D and the 3D model were compared.In addition, the bubble breakup process was deeply investigated.</p>

corrected abstract:
<p>Carbonated sparkling water has been widely used from ancient age [1]. The original idea came from natural sparkling water and people believed that taking baths at carbonated hot springs was good for health and healed their sicknesses. This fact led people to start thinking that sparkling water could have more effective uses. Joseph Priestley successfully produced artificial carbonated water in 1767 and sparkling water quickly became widely spread because it gives people refreshing feeling. The bottled and canned beverage industry has grown from the 19th century and has become one of the biggest markets in the world. According to Bloomberg Intelligence and Euromonitor, the global market of the carbonated beverages is around 350 billion dollar. One main drawback was that it was not possible to re-cork the bottle to save the carbonation so that once it was opened, fizz was kept only for a short time. In 1813, the method to dispense a portion of carbonated water was invented by Charles Plinth[2]. This was the origin of the Soda Syphon. As the demand of sparkling water increased, the machine with which people could make sparkling water by themselves was introduced. Recently, it has become a very popular home appliance, especially in Europe and North America. The most common way to carbonate water is by injecting high-pressure CO<sub>2</sub> into a water bottle. However, current systems waste a lot of CO<sub>2</sub> during this carbonating process. In this thesis, the flow inside the bottle during the injection of CO<sub>2</sub> into water was studied in order to determine the parameters that had most influence on the carbonation process. CFD (Computational Fluid Dynamics) simulations were performed in STAR-CCM+ of an axisymmetric 2D model and a 3D model that was a 30 degree wedge of the real bottle shape. The Volume of Fluid method was used to solve the multiphase flow of gas and liquid. The RANS approach was used with <em>k - ϵ</em> model and implicit time marching. To validate the simulations, axial propagation of the volume fraction of CO<sub>2</sub> was compared with the experimental visualization of the CO<sub>2</sub> and H<sub>2</sub>O distribution. At the beginning of the phenomena, the gas propagation was reasonably predicted and the results capture the features of the bubble shape. However the results did not perfectly match with the experimental visualization. To seek the reason for the unrealistic results, the grid sensitivity study was performed and to consider the 3D effect the results with the 2D and the 3D model were compared. In addition, the bubble breakup process was deeply investigated.</p>
----------------------------------------------------------------------
In diva2:1081137 - missing space in title:
"Revisiting Hot-Wire AnemometryClose to Solid Walls"
==>
"Revisiting Hot-Wire Anemometry Close to Solid Walls"

abstract is:
<p>A well-known problem of hot-wire anemometry (HWA), is the “wall effect”, namely theoverestimation of the measured velocity near a wall. The overestimation occurs due toadditional heat loss from the heated wire-sensor to the wall. The extra heat loss dependson parameters such as the heat conductivity of the wall material, the overheat ratio ofthe wire, and the sensor geometry. This problem has been studied for quite some timeand there are several suggestions with regard to the effect of these parameters for meanflow corrections, however the effect on measurements of turbulent fluctuation has notbeen investigated. The present work aims at providing further insight on this topic, byelucidating how these parameters affect measurements of both the mean and fluctuatingvelocity. Furthermore, the present study proposes a theoretical model on the total heattransfer from hot-wire sensor to explain the phenomenon.In the experimental part of the study, the measurements under both no flow and flowconditions are carried out to consider natural convection and forced convection separately.The results showed that the effect of the parameters is consistent with what is agreedwidely: higher wall conductivity, higher overheat ratio, and larger wire exposed area leadto higher output from an anemometer. On the other hand, it is observed that the conduc-tion under natural convection can be scaled with the overheat ratio. Velocity fluctuationsare found to decrease by employing higher overheat ratio and for walls with higher heatconductivity.In the numerical part of the study, a two-dimensional steady calculation using Open-FOAM is performed and the parameter dependency with respect to the overheat ratio andwall heat conductivity is investigated. The results qualitatively agree with the experi-mental results. Moreover, the inner scaling commonly employed in wall-turbulence isfound to be inadequate to resolve the wall effect of HWA when various sensor heights areconcerned.Lastly, a theoretical model on the total heat transfer from the wire close to solid wallsis established based on a superposition of the convection and the conduction contributions.The proposed model with the empirically determined coefficients is found to be capableof capturing the qualitative behaviours found in the experiment and numerical analysis, howewer for more practical use it leaves several issues to be further analysed.</p>

corrected abstract:
<p>A well-known problem of hot-wire anemometry (HWA), is the “wall effect”, namely the overestimation of the measured velocity near a wall. The overestimation occurs due to additional heat loss from the heated wire-sensor to the wall. The extra heat loss depends on parameters such as the heat conductivity of the wall material, the overheat ratio of the wire, and the sensor geometry. This problem has been studied for quite some time and there are several suggestions with regard to the effect of these parameters for mean flow corrections, however the effect on measurements of turbulent fluctuation has not been investigated. The present work aims at providing further insight on this topic, by elucidating how these parameters affect measurements of both the mean and fluctuating velocity. Furthermore, the present study proposes a theoretical model on the total heat transfer from hot-wire sensor to explain the phenomenon.</p><p>In the experimental part of the study, the measurements under both no flow and flow conditions are carried out to consider natural convection and forced convection separately. The results showed that the effect of the parameters is consistent with what is agreed widely: higher wall conductivity, higher overheat ratio, and larger wire exposed area lead to higher output from an anemometer. On the other hand, it is observed that the conduction under natural convection can be scaled with the overheat ratio. Velocity fluctuations are found to decrease by employing higher overheat ratio and for walls with higher heat conductivity.</p><p>In the numerical part of the study, a two-dimensional steady calculation using OpenFOAM is performed and the parameter dependency with respect to the overheat ratio and wall heat conductivity is investigated. The results qualitatively agree with the experimental results. Moreover, the inner scaling commonly employed in wall-turbulence is found to be inadequate to resolve the wall effect of HWA when various sensor heights are concerned.</p><p>Lastly, a theoretical model on the total heat transfer from the wire close to solid walls is established based on a superposition of the convection and the conduction contributions. The proposed model with the empirically determined coefficients is found to be capable of capturing the qualitative behaviours found in the experiment and numerical analysis, however for more practical use it leaves several issues to be further analysed.</p>
----------------------------------------------------------------------
In diva2:1894689 abstract is:
<p>This thesis evaluates the performance of four equity funds managed by SEBInvestment Management using the Fama-French three-factor model whichconsiders market risk, size (SMB), and value (HML) factors in addition to thegeneral market movement. The study aims to understand how these factorscontribute to the fund’s returns and to examine potential investment biasesthat could influence the funds performance.The analysis revealed varying degrees of sensitivity to these factors amongthe funds, overall reflecting their distinct investment strategies. For instance,the small cap fund displayed a significant positive relationship with SMB,indicating a tendency to benefit from investments in smaller companies.Conversely, the value fund showed a positive HML coeﬀicient, suggestinga preference for value stocks. However, several of the funds saw correlationwith development of small cap stocks.Results indicate that while market risk remains a dominant factor, size andvalue significantly contribute to fund performance. This is offering insightsbeyond the traditional CAPM model. The study’s findings are significant,suggesting that actively managed funds can exhibit distinct behavioral patternswhich could be systematically explored to enhance investment strategies anddecision making.The thesis concludes with recommendations for extending this research toinclude additional factors like momentum and liquidity which could providea deeper understanding of the influences affecting fund performance. Theadoption of multi factor models may offer a more comprehensive frameworkfor predicting stock returns and assisting portfolio management.</p>

corrected abstract:
<p>This thesis evaluates the performance of four equity funds managed by SEB Investment Management using the Fama-French three-factor model which considers market risk, size (SMB), and value (HML) factors in addition to the general market movement. The study aims to understand how these factors contribute to the fund’s returns and to examine potential investment biases that could influence the funds performance.</p><p>The analysis revealed varying degrees of sensitivity to these factors among the funds, overall reflecting their distinct investment strategies. For instance, the small cap fund displayed a significant positive relationship with SMB, indicating a tendency to benefit from investments in smaller companies. Conversely, the value fund showed a positive HML coefficient, suggesting a preference for value stocks. However, several of the funds saw correlation with development of small cap stocks.</p><p>Results indicate that while market risk remains a dominant factor, size and value significantly contribute to fund performance. This is offering insights beyond the traditional CAPM model. The study’s findings are significant, suggesting that actively managed funds can exhibit distinct behavioral patterns which could be systematically explored to enhance investment strategies and decision making.</p><p>The thesis concludes with recommendations for extending this research to include additional factors like momentum and liquidity which could provide a deeper understanding of the influences affecting fund performance. The adoption of multi factor models may offer a more comprehensive framework for predicting stock returns and assisting portfolio management.</p>
----------------------------------------------------------------------
In diva2:1881360 abstract is:
<p>ArtEmis is an EU project that today consists of 14 differentinstitutions collectively working towards the final goal of buildinga system that can make trustworthy earthquake predictions withthe help of radon gas. The purpose of this report is to analyse theconcentration of radon measured by four out of six prototype sensorsinstalled by the ArtEmis project.</p><p>In the occurrence of an earthquake, tectonic plates slide together,causing stress levels to rise within the Earth’s crust and microcracksbegin to form. When these microcracks form, specific elements suchas radon gas can ascend toward the surface and reach groundwater.Once in groundwater, the concentration of radon can be measuredby analysing the amount of γ rays at certain energies using γ-rayspectroscopy.</p><p>With energy spectra measured by these sensors, an energy intervalcorresponding to the presence of the isotope 222Rn could be identified,namely the interval around 609 keV. This stems from a daughterisotope of 222Rn. Further analysis on the activity over time, andcomparisons to instances when earthquakes occurred, could thenbe done. These measurements were also tested against a statisticalmodel based on Gaussian distribution, showing correlation in severalcases.</p><p>One sensor location had an extra interesting find. Here it wasable to see, on two different occasions, a distinct increase in 222Rnconcentration roughly 10 days prior to an earthquake.Considering that these sensors are active for the very first timeduring the time span of this report, unintended behaviour occurredon several occasions. A large focus of the project currently lies onfixing these issues. This leads to limited conclusions being able tobe drawn from such a short time span, but could give indication ofpositive results moving forward.</p>

corrected abstract:
<p>ArtEmis is an EU project that today consists of 14 different institutions collectively working towards the final goal of building a system that can make trustworthy earthquake predictions with the help of radon gas. The purpose of this report is to analyse the concentration of radon measured by four out of six prototype sensors installed by the ArtEmis project.</p><p>In the occurrence of an earthquake, tectonic plates slide together, causing stress levels to rise within the Earth’s crust and microcracks begin to form. When these microcracks form, specific elements such as radon gas can ascend toward the surface and reach groundwater. Once in groundwater, the concentration of radon can be measured by analysing the amount of γ rays at certain energies using γ-ray spectroscopy.</p><p>With energy spectra measured by these sensors, an energy interval corresponding to the presence of the isotope <sup>222</sup><em>Rn</em> could be identified, namely the interval around 609 keV. This stems from a daughter isotope of <sup>222</sup><em>Rn</em>. Further analysis on the activity over time, and comparisons to instances when earthquakes occurred, could then be done. These measurements were also tested against a statistical model based on Gaussian distribution, showing correlation in several cases.</p><p>One sensor location had an extra interesting find. Here it was able to see, on two different occasions, a distinct increase in <sup>222</sup><em>Rn</em> concentration roughly 10 days prior to an earthquake.</p><p>Considering that these sensors are active for the very first time during the time span of this report, unintended behaviour occurred on several occasions. A large focus of the project currently lies on fixing these issues. This leads to limited conclusions being able to be drawn from such a short time span, but could give indication of positive results moving forward.</p>
----------------------------------------------------------------------
In diva2:1878576 abstract is:
<p>Hematology analyzers can be used for screening patients for blood abnor-malities. The techniques used in a hematology analyzer include impedanceanalysis, flow cytometry and spectroscopy, which allow for measuring of forexample absolute count, sizes and concentration of different cells in a patient’sblood sample. Hyperlipidemia, which refers to elevated blood lipid levels, isthe primary cause of heart-related illness and fatalities in today’s developedor developing countries. Currently, blood lipid levels are not measured as aparameter with hematology analyzers. Since hematology analyzers allow for arapid general screening of blood parameters, an area of interest is therefore tobe able to measure blood lipids with a hematology analyzer. Thus, this studyaims to investigate the possibility of detecting and measuring blood lipids witha hematology analyzer, using flow cytometry and/or spectrophotometry.</p><p>In order to investigate this possibility, two simulating methods were conductedwhere in the first method Intralipid 20% was mixed with saline into sampleswith different lipid concentrations. In the second method, diluent wasused instead of saline. Lastly a Correlation study was performed whereIntralipid 20% was mixed with donor blood to prepare samples with differentlipid concentrations. All samples were then analyzed in a hematologyanalyser and scatter plots from flow cytometry and light absorption datafrom spectrophotometry measurements were obtained. The methods showedthat there is a strong correlation between number of detected pulse countsfrom the scatter plots and lipid concentration. Same applies to lightabsorption compared to the lipid concentration of the samples, measured withspectrophotometry.</p><p>The results from this study show that it is in fact possible to detect andmeasure blood lipid levels with a hematology analyser using flow cytometryand spectrophotometry. Further development within this area could thereforeenable simple screening of this additional parameter and early detection ofindications of hyperlipidemia.</p>


corrected abstract:
<p>Hematology analyzers can be used for screening patients for blood abnormalities. The techniques used in a hematology analyzer include impedance analysis, flow cytometry and spectroscopy, which allow for measuring of for example absolute count, sizes and concentration of different cells in a patient’s blood sample. Hyperlipidemia, which refers to elevated blood lipid levels, is the primary cause of heart-related illness and fatalities in today’s developed or developing countries. Currently, blood lipid levels are not measured as a parameter with hematology analyzers. Since hematology analyzers allow for a rapid general screening of blood parameters, an area of interest is therefore to be able to measure blood lipids with a hematology analyzer. Thus, this study aims to investigate the possibility of detecting and measuring blood lipids with a hematology analyzer, using flow cytometry and/or spectrophotometry.</p><p>In order to investigate this possibility, two simulating methods were conducted where in the first method Intralipid 20% was mixed with saline into samples with different lipid concentrations. In the second method, diluent was used instead of saline. Lastly a Correlation study was performed where Intralipid 20% was mixed with donor blood to prepare samples with different lipid concentrations. All samples were then analyzed in a hematology analyser and scatter plots from flow cytometry and light absorption data from spectrophotometry measurements were obtained. The methods showed that there is a strong correlation between number of detected pulse counts from the scatter plots and lipid concentration. Same applies to light absorption compared to the lipid concentration of the samples, measured with spectrophotometry.</p><p>The results from this study show that it is in fact possible to detect and measure blood lipid levels with a hematology analyser using flow cytometry and spectrophotometry. Further development within this area could therefore enable simple screening of this additional parameter and early detection of indications of hyperlipidemia.</p>
----------------------------------------------------------------------
In diva2:1817475 abstract is:
<p>Both commuting to work and long-distance travelling with one's own bicycle have been intrend for years and a large market has emerged. While the reasons on the customer side aremainly sustainability and sporting activity, the companies offer customised products for manydifferent use-cases.In this master's thesis, a transport box for bicycles is being developed that can be taken on aplane, for example, and converted into a cargo trailer at the destination. A market researchshows that such a product is already available for certain folding bikes, whereas the goal is auniversal solution for utility bikes as well as mountain bikes.The methodological development follows a standard that divides the process into four phases.After the market research, main functions of the product are identified, which are "Transport abicycle as luggage" and "Carry goods during the bicycle ride". These are then divided intosub-functions in order to find different design variants for each function, combine them intodifferent drafts with the help of a morphological box and finally evaluate according totechnical and economic criteria, so that a final draft is determined at the end of this phase. It isa hard case providing enough space for different types of bicycles, that can be converted intoa trailer by mounting two wheels on the sides and a tow bar.In phase 3, the wheel size is set at 12 inches due to small space requirements and weight, andthe material is set to ABS-plastic for reasons of sustainability and mechanical properties.The final phase involves 3D design using Fusion360 with drawing derivation and validationof the model. The model shows that the requirements regarding geometry and weight havebeen implemented and that a practical transport option for bicycles has been found. In order topotentially launch the product on the market, further investigations such as FEM analysis ordynamic simulations are necessary.</p>

corrected abstract:
<p>Both commuting to work and long-distance travelling with one's own bicycle have been in trend for years and a large market has emerged. While the reasons on the customer side are mainly sustainability and sporting activity, the companies offer customised products for many different use-cases.</p><p>In this master's thesis, a transport box for bicycles is being developed that can be taken on a plane, for example, and converted into a cargo trailer at the destination. A market research shows that such a product is already available for certain folding bikes, whereas the goal is a universal solution for utility bikes as well as mountain bikes.</p><p>The methodological development follows a standard that divides the process into four phases. After the market research, main functions of the product are identified, which are "Transport a bicycle as luggage" and "Carry goods during the bicycle ride". These are then divided into sub-functions in order to find different design variants for each function, combine them into different drafts with the help of a morphological box and finally evaluate according to technical and economic criteria, so that a final draft is determined at the end of this phase. It is a hard case providing enough space for different types of bicycles, that can be converted into a trailer by mounting two wheels on the sides and a tow bar.</p><p>In phase 3, the wheel size is set at 12 inches due to small space requirements and weight, and the material is set to ABS-plastic for reasons of sustainability and mechanical properties. The final phase involves 3D design using Fusion360 with drawing derivation and validation of the model. The model shows that the requirements regarding geometry and weight have been implemented and that a practical transport option for bicycles has been found. In order to potentially launch the product on the market, further investigations such as FEM analysis or dynamic simulations are necessary.</p>
----------------------------------------------------------------------
In diva2:1345189 abstract is:
<p>-aminobutyric acid receptors of type A (GABAARs) are the majorinhibitory neurotransmitter receptors in the human brain, andare modulated by a vast range of exogenous molecules, such assedatives and anesthetics. In the last year, the first cryo-electronmicroscopy (cryo-EM) images of the closed and desensitized statesof the GABAAR were released, enabling fruitful research throughsimulations of these complex proteins. This report investigatesthe characteristics of the two structures. Specifically, pore hydration,radius, and hydrophobicity is compared, and a major focuslies in the general anesthetic (GA) binding pockets in the transmembranedomain, as well as the ligands propofol, etomidate,and pentobarbital. Furthermore, different models for the missingstructure of the intracellular domain (ICD) are compared. Thestructures were simulated for 1 μs using GROMACS. Using multiplesequence alignment as the basis of different models with theheptapeptid SQPARAA in the place of the ICD, resulted in stablestructures with a backbone RMSD close to 2 Å after 1 μs. Thepores are shown to exhibit significant differences between the twostates, with heavier constriction at the 9’ site of the closed state,but also suspected faulty expansion of the pore near the top in thedesensitized state after equilibration. Two of the pockets in thedesensitized state further deviates from expectation, by being tooconstricted. The other pockets were large enough to bind ligandsin the desensitized state, but not in the closed state, as expected.The binding analysis of the GAs suggests that etomidate bindswith the phenyl ring pointing towards the ICD, and that pentobarbitalbinds with the head group pointing towards the pore. Italso suggests that the GAs can bind to every GA pocket, but thatmodulatory activity is dependent on consistently low binding energies,which varies between the ligands for the different pockets.</p>

corrected abstract:
<p>γ-aminobutyric acid receptors of type A (GABA<sub>A</sub>Rs) are the major inhibitory neurotransmitter receptors in the human brain, and are modulated by a vast range of exogenous molecules, such as sedatives and anesthetics. In the last year, the first cryo-electron microscopy (cryo-EM) images of the closed and desensitized states of the GABA<sub>A</sub>R were released, enabling fruitful research through simulations of these complex proteins. This report investigates the characteristics of the two structures. Specifically, pore hydration, radius, and hydrophobicity is compared, and a major focus lies in the general anesthetic (GA) binding pockets in the transmembrane domain, as well as the ligands propofol, etomidate, and pentobarbital. Furthermore, different models for the missing structure of the intracellular domain (ICD) are compared. The structures were simulated for 1 µs using GROMACS. Using multiple sequence alignment as the basis of different models with the heptapeptid SQPARAA in the place of the ICD, resulted in stable structures with a backbone RMSD close to 2 Å after 1 µs. The pores are shown to exhibit significant differences between the two states, with heavier constriction at the 9’ site of the closed state, but also suspected faulty expansion of the pore near the top in the desensitized state after equilibration. Two of the pockets in the desensitized state further deviates from expectation, by being too constricted. The other pockets were large enough to bind ligands in the desensitized state, but not in the closed state, as expected. The binding analysis of the GAs suggests that etomidate binds with the phenyl ring pointing towards the ICD, and that pentobarbital binds with the head group pointing towards the pore. It also suggests that the GAs can bind to every GA pocket, but that modulatory activity is dependent on consistently low binding energies, which varies between the ligands for the different pockets.</p>
----------------------------------------------------------------------
In diva2:1307667 abstract is:
<p>The industrial robot is a flexible and cheap standard component that can becombined with a milling head to complete low accuracy milling tasks. Thefuture goal for researchers and industry is to increase the milling accuracy, suchthat it can be introduced to more high value added operations.The serial build up of an industrial robot bring non-linear compliance andchallenges in vibration mitigation due to the member and reducer design. WithAdditive Manufacturing (AM), the traditional cast aluminum structure couldbe revised and, therefore, milling accuracy gain could be made possible due tostructural changes.This thesis proposes the structural changes that would improve the millingaccuracy for a specific trajectory. To quantify the improvement, first the robothad to be reverse engineered and a kinematic simulation model be built. Nextthe kinematic simulation process was automated such that multiple input parameterscould be varied and a screening conducted that proposed the mostprofitable change.It was found that a mass decrease in any member did not affect the millingaccuracy and a stiffness increase in the member of the second axis would increasethe milling accuracy the most, without changing the design concept. To changethe reducer in axis 1 would reduce the mean position error by 7.5 % and themean rotation error by 4.5 % approximately, but also reduces the maximumspeed of the robot. The best structural change would be to introduce twosupport bearings for axis two and three, which decreased the mean positioningerror and rotation error by approximately 8 % and 13 % respectively.</p>

corrected abstract:
<p>The industrial robot is a flexible and cheap standard component that can be combined with a milling head to complete low accuracy milling tasks. The future goal for researchers and industry is to increase the milling accuracy, such that it can be introduced to more high value added operations.</p><p>The serial build up of an industrial robot bring non-linear compliance and challenges in vibration mitigation due to the member and reducer design. With Additive Manufacturing (AM), the traditional cast aluminum structure could be revised and, therefore, milling accuracy gain could be made possible due to structural changes.</p><p>This thesis proposes the structural changes that would improve the milling accuracy for a specific trajectory. To quantify the improvement, first the robot had to be reverse engineered and a kinematic simulation model be built. Next the kinematic simulation process was automated such that multiple input parameters could be varied and a screening conducted that proposed the most profitable change.</p><p>It was found that a mass decrease in any member did not affect the milling accuracy and a stiffness increase in the member of the second axis would increase the milling accuracy the most, without changing the design concept. To change the reducer in axis 1 would reduce the mean position error by 7.5 % and the mean rotation error by 4.5 % approximately, but also reduces the maximum speed of the robot. The best structural change would be to introduce two support bearings for axis two and three, which decreased the mean positioning error and rotation error by approximately 8 % and 13 % respectively.</p>
----------------------------------------------------------------------
In diva2:1142911 abstract is:
<p>Recommender systems can be seen everywhere today,having endless possibilities of implementation. However,operating in the background, they can easily be passed withoutnotice. Essentially, recommender systems are algorithms thatgenerate predictions by operating on a certain data set. Eachcase of recommendation is environment sensitive and dependenton the condition of the data at hand. Consequently, it is difficultto foresee which method, or combination of methods, to apply in aparticular situation for obtaining desired results. The area of recommendersystems that this thesis is delimited to is Collaborativefiltering (CF) and can be split up into three different categories,namely memory based, model based and hybrid algorithms. Thisthesis implements a CF algorithm for each of these categoriesand sets focus on comparing their prediction accuracy and theirdependency on the amount of available training data (i.e. asa function of sparsity). The results show that the model basedalgorithm clearly performs better than the memory based, bothin terms of overall accuracy and sparsity dependency. With anincreasing sparsity level, the problem of having users without anyratings is encountered, which greatly impacts the accuracy forthe memory based algorithm. A hybrid between these algorithmsresulted in a better accuracy than the model based algorithmitself but with an insignificant improvement.</p>

corrected abstract:
<p>Recommender systems can be seen everywhere today, having endless possibilities of implementation. However, operating in the background, they can easily be passed without notice. Essentially, recommender systems are algorithms that generate predictions by operating on a certain data set. Each case of recommendation is environment sensitive and dependent on the condition of the data at hand. Consequently, it is difficult to foresee which method, or combination of methods, to apply in a particular situation for obtaining desired results. The area of recommender systems that this thesis is delimited to is Collaborative filtering (CF) and can be split up into three different categories, namely memory based, model based and hybrid algorithms. This thesis implements a CF algorithm for each of these categories and sets focus on comparing their prediction accuracy and their dependency on the amount of available training data (i.e. as a function of sparsity). The results show that the model based algorithm clearly performs better than the memory based, both in terms of overall accuracy and sparsity dependency. With an increasing sparsity level, the problem of having users without any ratings is encountered, which greatly impacts the accuracy for the memory based algorithm. A hybrid between these algorithms resulted in a better accuracy than the model based algorithm itself but with an insignificant improvement.</p>
----------------------------------------------------------------------
In diva2:1078073 spaces missing in title:
"An Experimental Study of Fibre SuspensionFlows in Pipes using Nuclear MagneticResonance Imaging"
==>
"An Experimental Study of Fibre Suspension Flows in Pipes using Nuclear Magnetic Resonance Imaging"

abstract is:
<p>This study deals with fibre suspension flows through cylindrical pipes. Thepresent work aims at measurements of opaque flows, which are common inindustries. Nuclear magnetic resonance imaging (NMRI) and ultrasound velocimetryprofiling (UVP) were employed as non-invasive and optic-independenttools to measure the velocity profiles. As a first experiment, a paper-pulp suspensionflow through a sudden contraction and expansion was investigated.The results show the NMRI technique can be used to measure the stronglyunsteady flow such as separated regions though the MR signal is attenuateddue to the turbulence in the flow. The flow loop had however an insufficientinlet length which caused asymmetric profiles at the test section. As a secondexperiment, a flow loop which provided fully developed flows at the test sectionwas designed. After that, the velocity profiles of rayon-fibre and micro-spheresuspension flows were measured by the NMRI and the UVP independently.In principle, these two techniques measure the different velocities of the fibresuspensionflows, i.e. the velocity of the water and the fibre. In dilute suspensionflows, where the velocities of the two phases were assumed to be thesame, the velocity profiles were in good agreement. This shows the validityof the two measurement techniques. However, it should be pointed out thatthere is a limitation of the current UVP method for highly concentrated flows.The velocity profiles obtained by the UVP at high concentrations seems notto represent physics while the NMRI is not affected by the concentrations. Itis argued that the advances of the NMRI for the measurement of the highlyconcentrated flows.</p>

corrected abstract:
<p>This study deals with fibre suspension flows through cylindrical pipes. The present work aims at measurements of opaque flows, which are common in industries. Nuclear magnetic resonance imaging (NMRI) and ultrasound velocimetry profiling (UVP) were employed as non-invasive and optic-independent tools to measure the velocity profiles. As a first experiment, a paper-pulp suspension flow through a sudden contraction and expansion was investigated. The results show the NMRI technique can be used to measure the strongly unsteady flow such as separated regions though the MR signal is attenuated due to the turbulence in the flow. The flow loop had however an insufficient inlet length which caused asymmetric profiles at the test section. As a second experiment, a flow loop which provided fully developed flows at the test section was designed. After that, the velocity profiles of rayon-fibre and micro-sphere suspension flows were measured by the NMRI and the UVP independently. In principle, these two techniques measure the different velocities of the fibresuspension flows, i.e. the velocity of the water and the fibre. In dilute suspension flows, where the velocities of the two phases were assumed to be the same, the velocity profiles were in good agreement. This shows the validity of the two measurement techniques. However, it should be pointed out that there is a limitation of the current UVP method for highly concentrated flows. The velocity profiles obtained by the UVP at high concentrations seems not to represent physics while the NMRI is not affected by the concentrations. It is argued that the advances of the NMRI for the measurement of the highly concentrated flows.</p>
----------------------------------------------------------------------
 diva2:854657 error in title:
 "Analysis oft yre wear using the expanded brush tyre model"
 ==>
 ""Analysis of tyre wear using the expanded brush tyre model"
 
abstract is:
<p>Approximately 60 000 tonnes of tyres are produced annually in Sweden to meet thedemand in the market. It is believed that 10 000 tonnes of rubber particles contaminatesthe Swedish roads every year. Some of the elements in the emitted particles cannegatively impact the environment. These elements can lead to leaching in water thatcan cause serious problems to aquatic organisms. Furthermore worn out tyres negativelyinfluence the driving dynamics. It increases the risk of aquaplaning which can have fatalconsequences. Innovative ways of recycling tyres are constantly being developed butstill faces major challenges. It is therefore important to understand tyre wear, whatinfluences it and how to reduce it.</p><p>The aim of the project is to acquire knowledge related to tyre wear, its environmentalimpacts, use a mathematical model to simulate tyre wear and study how the differentparameters influences wear.</p><p>First a literature survey was performed to acquire knowledge related to tyre wear. It wasfound that tyre wear is mainly due to adhesive and hysteresis wear. Several factors werefound to affect tyre wear for example velocity, slip angle and the type of road surface.The environment impact was also studied and the results show the composition of theparticles emitted to the environment. Some of the emitted particles negatively affect theaquatic organism and human beings.</p><p>In the second part of the project a mathematical model based on the well-known brushtyre model was used to simulate how wear changes with different parameters. Themodel created at KTH Vehicle Dynamics is named the expanded brush tyre model(EBM). The wear model chosen for this evaluation was the Archards wear law. Thismodel was used to be able to quantify wear and study how it is influenced by differentfactors.</p><p>The result of the mathematical models shows clearly an exponential increase in thevolume of wear with increases in velocity, slip angle and vertical load. The analysis wasdone using zero camber angle.</p><p>For future work it is recommended to investigate camber angle as it is also one of themajor factors that affects wear. Temperature is also another factor that was not taken into account in the study. It can also be studied in future work.</p>

corrected abstract:
<p>Approximately 60 000 tonnes of tyres are produced annually in Sweden to meet the demand in the market. It is believed that 10 000 tonnes of rubber particles contaminates the Swedish roads every year. Some of the elements in the emitted particles can negatively impact the environment. These elements can lead to leaching in water that can cause serious problems to aquatic organisms. Furthermore worn out tyres negatively influence the driving dynamics. It increases the risk of aquaplaning which can have fatal consequences. Innovative ways of recycling tyres are constantly being developed but still faces major challenges. It is therefore important to understand tyre wear, what influences it and how to reduce it.</p><p>The aim of the project is to acquire knowledge related to tyre wear, its environmental impacts, use a mathematical model to simulate tyre wear and study how the different parameters influences wear.</p><p>First a literature survey was performed to acquire knowledge related to tyre wear. It was found that tyre wear is mainly due to adhesive and hysteresis wear. Several factors were found to affect tyre wear for example velocity, slip angle and the type of road surface. The environment impact was also studied and the results show the composition of the particles emitted to the environment. Some of the emitted particles negatively affect the aquatic organism and human beings.</p><p>In the second part of the project a mathematical model based on the well-known brushf tyre model was used to simulate how wear changes with different parameters. The model created at KTH Vehicle Dynamics is named the expanded brush tyre model (EBM). The wear model chosen for this evaluation was the Archards wear law. This model was used to be able to quantify wear and study how it is influenced by different factors.</p><p>The result of the mathematical models shows clearly an exponential increase in the volume of wear with increases in velocity, slip angle and vertical load. The analysis was done using zero camber angle.</p><p>For future work it is recommended to investigate camber angle as it is also one of the major factors that affects wear. Temperature is also another factor that was not taken in to account in the study. It can also be studied in future work.</p>
----------------------------------------------------------------------
In diva2:1890520 abstract is:
<p>The observed cases of increased radon emissions from the earth precedingheightened seismic activity have given rise to numerous papers exploringthe viability of using radon concentration levels in soil gas and groundwateras a precursor in earthquake forecasting. In this paper, these methods areexplored further through a statistical analysis of the initial data collected by thegamma detectors installed within the scope of ArtEmis, a project funded bythe European Union with the goal of producing a reliable model for earthquakeforecasting using measured radioactivity as a precursor. A Gaussian RandomWalk model is implemented using Integrated Nested Laplace Approximationto infer a set of points defining the hidden distribution from which the observeddata are drawn. The model is implemented and trained on data sets recordedby five gamma detectors. The inferences made by the model imply thatthe model is applicable to the data collected by four of the detectors. Theinferred distributions are compared to seismic data collected by 142 seismicobservatories in Greece. No significant correlations between the inferredchanges in radon levels and seismic activity were found. The impact ofchanging the sampling frequency of radon data is investigated, with theconclusion that the created model infers a distribution corresponding to the rawdata obtained with a lower sampling frequency. It is concluded that the modelshould be improved further for more advanced analyses, with some of the mostimportant developments suggested being the inclusion of meteorological dataand upgrading from a Gaussian Random Walk of order 1 to order 2. Moreseismic and radiation data is also deemed necessary for meaningful analysesof the correlation between the two. More advanced analyses of the availableseismic data are required for improved classification of the events expected togive rise to precursory phenomena observable in the radiation data collectedby the various detectors.</p>


corrected abstract:
<p>The observed cases of increased radon emissions from the earth preceding heightened seismic activity have given rise to numerous papers exploring the viability of using radon concentration levels in soil gas and ground water as a precursor in earthquake forecasting. In this paper, these methods are explored further through a statistical analysis of the initial data collected by the gamma detectors installed within the scope of ArtEmis, a project funded by the European Union with the goal of producing a reliable model for earthquake forecasting using measured radioactivity as a precursor. A Gaussian Random Walk model is implemented using Integrated Nested Laplace Approximation to infer a set of points defining the hidden distribution from which the observed data are drawn. The model is implemented and trained on data sets recorded by five gamma detectors. The inferences made by the model imply that the model is applicable to the data collected by four of the detectors. The inferred distributions are compared to seismic data collected by 142 seismic observatories in Greece. No significant correlations between the inferred changes in radon levels and seismic activity were found. The impact of changing the sampling frequency of radon data is investigated, with the conclusion that the created model infers a distribution corresponding to the raw data obtained with a lower sampling frequency. It is concluded that the model should be improved further for more advanced analyses, with some of the most important developments suggested being the inclusion of meteorological data and upgrading from a Gaussian Random Walk of order 1 to order 2. More seismic and radiation data is also deemed necessary for meaningful analyses of the correlation between the two. More advanced analyses of the available seismic data are required for improved classification of the events expected to give rise to precursory phenomena observable in the radiation data collected by the various detectors.</p>
----------------------------------------------------------------------
In diva2:1881327
abstract is:
<p>Formula Student is Europe’s most established engineering competition, with teamsall over the world. Practical problem solving in combination with applyingacademic knowledge, give students the opportunity to explore their field of study inan exciting and meaningful way.</p><p>Aerodynamic development of race cars have seen significant results in competitionsince its introduction in the 1960s. Initial designs were adaptations of aerospaceconcepts for ground vehicles. Development relied solely on track- and wind tunneltesting but despite their rudimentary designs, significant performance increaseswere made. The purpose of aerodynamic development of race cars is to balance thecar, getting it to behave as desired. As a consequence of the forces generated, thevehicle corners faster at the cost of acceleration and top speed. With more powerfulcomputers, earlier unsolvable equations started to get numerically solved andcomputational fluid dynamics was born. CFD introduced the possibility for rapiditeration and exploration of more intricate designs. This report will solely utilizeCFD as a simulation tool, recognising its limitations in accuracy and real worldcorrelation.</p><p>The aim of this study is to increase downforce on the front wing, whilst beingcautious of downstream impact. The goal set by the team is an adjustable frontwing that generates as much downforce as possible, whilst allowing for adjustmentsto shift the center of pressure by promoting more air to the side-structure. Toachieve this, an iterative design process based on literature is the chosen method.Continuous cross evaluations with other parts of the design team is of the highestimportance to avoid poor interaction between aerodynamic devices.</p><p>The (negative) lift coefficient was increased from 4.7 to 5.7 for the entire vehicle, byonly improving the front wing. This was very satisfactory as increases upstreamoften lead do degraded performance downstream. An increased lift coefficient ofover 20%, with improvements to front wheel drag and similar side-structureperformance, demonstrate the quality and effectiveness of the design.</p>

corrected abstract:
<p>Formula Student is Europe’s most established engineering competition, with teams all over the world. Practical problem solving in combination with applying academic knowledge, give students the opportunity to explore their field of study in an exciting and meaningful way.</p><p>Aerodynamic development of race cars have seen significant results in competition since its introduction in the 1960s. Initial designs were adaptations of aerospace concepts for ground vehicles. Development relied solely on track- and wind tunnel testing but despite their rudimentary designs, significant performance increases were made. The purpose of aerodynamic development of race cars is to balance the car, getting it to behave as desired. As a consequence of the forces generated, the vehicle corners faster at the cost of acceleration and top speed. With more powerful computers, earlier unsolvable equations started to get numerically solved and computational fluid dynamics was born. CFD introduced the possibility for rapid iteration and exploration of more intricate designs. This report will solely utilize CFD as a simulation tool, recognising its limitations in accuracy and real world correlation.</p><p>The aim of this study is to increase downforce on the front wing, whilst being cautious of downstream impact. The goal set by the team is an adjustable front wing that generates as much downforce as possible, whilst allowing for adjustments to shift the center of pressure by promoting more air to the side-structure. To achieve this, an iterative design process based on literature is the chosen method. Continuous cross evaluations with other parts of the design team is of the highest importance to avoid poor interaction between aerodynamic devices.</p><p>The (negative) lift coefficient was inc
reased from 4.7 to 5.7 for the entire vehicle, by only improving the front wing. This was very satisfactory as increases upstream often lead do degraded performance downstream. An increased lift coefficient of over 20%, with improvements to front wheel drag and similar side-structure performance, demonstrate the quality and effectiveness of the design.</p>
----------------------------------------------------------------------
In diva2:1880362 

abstract is:

<p>Measurements of radon in groundwater before, during and after the 1995 Kobe earth-quake in Japan indicated that there might be a correlation between levels of 222Rn ingroundwater and seismological activity. The artEmis project investigates this possibleconnection with the goal of building a network of detectors in seismically active parts ofEurope. The detectors will be placed in groundwater and measure many factors, one ofthem being the radon level by measuring gamma radiation. The original vision for thedetectors also included alpha detection. The obtained data is analyzed with artificialintelligence.</p><p>This thesis investigates a possible method for alpha detection under water. Specif-ically by seeing if it is possible for radon dissolved in water to diffuse from the water,through silicone tubes and into the air inside of the silicone tubes. There is a possibilityfor alpha detection of the radon decay if the radon gas could get into the air. This wasinvestigated by submerging an air-filled silicone construction in water with high levelsof radon. The level of radon in the water was increased by placing pieces of lightweightconcrete in the water. The construction was removed after a period of time and itsgamma-ray spectrum was measured. A statistically significant increase in radon levelscompared to the background radiation would indicate that diffusion happened.</p><p>Measurements of the silicone construction with a germanium detector resulted ingamma spectra that were analyzed with a Python program to determine the activity of222Rn over time. Short measurements, around 1 hour long, showed a significant increaseof radon compared to the background. For longer measurements however, around one ortwo days, this effect was no longer apparent. The conclusion is that radon diffused intothe silicone construction, either into the silicone material itself or into the air inside theconstruction, but it comes out again quickly. If the radon diffused into the air inside ofthe silicone, the use of alpha detection to measure radon levels in groundwater is muchless far-fetched than before. Therefore, the artEmis project might be one step closer tousing alpha detection in their detector network.</p>


corrected abstract:
<p>Measurements of radon in groundwater before, during and after the 1995 Kobe earth-quake in Japan indicated that there might be a correlation between levels of <sup>222</sup>Rn in groundwater and seismological activity. The artEmis project investigates this possible connection with the goal of building a network of detectors in seismically active parts of Europe. The detectors will be placed in groundwater and measure many factors, one of them being the radon level by measuring gamma radiation. The original vision for the detectors also included alpha detection. The obtained data is analyzed with artificial intelligence.</p><p>This thesis investigates a possible method for alpha detection under water. Specifically by seeing if it is possible for radon dissolved in water to diffuse from the water, through silicone tubes and into the air inside of the silicone tubes. There is a possibility for alpha detection of the radon decay if the radon gas could get into the air. This was investigated by submerging an air-filled silicone construction in water with high levels of radon. The level of radon in the water was increased by placing pieces of lightweight concrete in the water. The construction was removed after a period of time and its gamma-ray spectrum was measured. A statistically significant increase in radon levels compared to the background radiation would indicate that diffusion happened.</p><p>Measurements of the silicone construction with a germanium detector resulted in gamma spectra that were analyzed with a Python program to determine the activity of <sup>222</supZRn over time. Short measurements, around 1 hour long, showed a significant increase of radon compared to the background. For longer measurements however, around one or two days, this effect was no longer apparent. The conclusion is that radon diffused into the silicone construction, either into the silicone material itself or into the air inside the construction, but it comes out again quickly. If the radon diffused into the air inside of the silicone, the use of alpha detection to measure radon levels in groundwater is much less far-fetched than before. Therefore, the artEmis project might be one step closer to using alpha detection in their detector network.</p>
----------------------------------------------------------------------
In diva2:1779369 -error in title:
"Computational Fluid Dynamics of the flow in a diffuser: - like geometry"
==>
"Computational Fluid Dynamics of the flow in a diffuser - like geometry"

If you want to include the subtitle, it is:
"Computational Fluid Dynamics of the flow in a diffuser - like geometry:
A study of flow separation using Computational Fluid Dynamics"

abstract is:
<p>Simulations were performed to investigate flow separation of an asymmetricdiffuser - like geometry. The geometry used for the simulations was modeledafter an experimental setup with recorded flow data, which was compared tothe simulated data. For all simulations, steady state flow at the inlet was usedwith the assumption of a 2D flow.A grid convergence study consisting of three different grids was performed.From this study no apparent change in simulation results were observed forfiner grids. This is caused by the fact that the coarse grid had a high enoughresolution to fully capture the flow, meaning that the higher resolution gridsyielded small improvements.Additionally, two different turbulence models RN G k − ε and SST k − ωwere used for evaluating which model was best suited to model flow separation.The simulations showed that the RN G k − ε model could not capture the flowseparation and had a poor accuracy when predicting the turbulent kinetic energy(TKE). Simulation results from SST k − ω gave good results in capturing flowseparation and predicting both the velocity and TKE when compared to theexperimental data.Finally, a turbulence intensity study was made for the mid grid with theSST k − ω model. The turbulent intensity was set to 5%, 10%, 15% and 20%at the inlet. This resulted in the point of separation moving further down thegeometry to x/H ≈ [17.68, 18.71, 19.58, 20.72] for respective intensity. The pointof reattachment also moves to x/H ≈ [44.85, 43.60, 42.67, 41.67] for respectiveintensity.In summary for simulating flow separation in turbulent flows the SST k − ωmodel is optimal and an increase in turbulent intensity reduces the recirculationzone.</p>


corrected abstract:
<p>Simulations were performed to investigate flow separation of an asymmetric diffuser - like geometry. The geometry used for the simulations was modeled after an experimental setup with recorded flow data, which was compared to the simulated data. For all simulations, steady state flow at the inlet was used with the assumption of a 2D flow.</p><p>A grid convergence study consisting of three different grids was performed. From this study no apparent change in simulation results were observed for finer grids. This is caused by the fact that the coarse grid had a high enough resolution to fully capture the flow, meaning that the higher resolution grids yielded small improvements.</p><p>Additionally, two different turbulence models <em>RNG k − ε</em> and <em>SST k − ω</em>  were used for evaluating which model was best suited to model flow separation. The simulations showed that the <em>RNG k − ε</em> model could not capture the flow separation and had a poor accuracy when predicting the turbulent kinetic energy (TKE). Simulation results from <em>SST k − ω</em> gave good results in capturing flow separation and predicting both the velocity and TKE when compared to the experimental data.</p><p>Finally, a turbulence intensity study was made for the mid grid with the <em>SST k − ω</em> model. The turbulent intensity was set to 5%, 10%, 15% and 20% at the inlet. This resulted in the point of separation moving further down the geometry to <em>x/H</em> ≈ [17.68, 18.71, 19.58, 20.72] for respective intensity. The point of reattachment also moves to <em>x/H</em> ≈ [44.85, 43.60, 42.67, 41.67] for respective intensity.</p><p>In summary for simulating flow separation in turbulent flows the <em>SST k − ω</em> model is optimal and an increase in turbulent intensity reduces the recirculation zone.</p>
----------------------------------------------------------------------
In diva2:1576894 correct the title:
"Tensile strength reduction for insufficient thread engagement A FEM study of a wall-shoe assembly"
==>
"Tensile strength reduction for insufficient thread engagement: A FEM study of a wall-shoe assembly"

abstract is:
<p>The purpose of this master thesis is to determine whether or not it exist, a model that can describethe reduction in strength, due to missing threads in a bond between a bolt and nut. And how thereduction in strength might effect a wall-shoe assembly, used to connect a wall to another wall or aconcrete base plate. This is done by firstly considering the strength of the entire wall-shoe assembly andthe strength of the bond between the nut and the bolt is then considered.</p><p>The strength of the entire assembly is calculated using some simple analytical models. The strengthof the bolt is the limiting factor for the assembly given the analytical models. Four FEM-models arethen created, three to evaluate the strength of the bolt and nut assembly, for bolt sized from M6to M60 with thread engagement from one thread to seven threads. </p><p>An elastic model without a defined tensile stress limit is proposed. The maximum stress at the sharpgeometry changes (stress concentrations) are used to dimension the maximum allowed load accordingto the von Mises yield criterion. </p><p>The yield limit is implemented by introducing a elastic-plastic (without hardening) for three different materials. Where the maximum yield force is determined as the maximum reactionforce on the frictionless support boundary condition, when a displacement is applied.</p><p>Material hardening is applied according to a bi-linear material model (with hardening). Thereaction force is evaluated and the maximum force and displacement can be determined using thedefinition of property class 8.8 that propose a yield limit of 80 percent of maximum load.</p><p>The behavior of the anchor bolt when it is pulled out of the concrete was also modeled. To obtain anunderstanding of the pull-out behavior.</p><p>The FEM-models makes it possible to formulate a simple reduction model, where the bolt failure loadis reduced by the calculated reduction factors. The reduction factors are dependent on the amountof missing thread due to damages or insufficient bolt height. The reduction factors are also highlydependent on the ratio between the rise and bolt diameter. The reduction factors are significantlysmaller when a perfect plastic material model is applied.</p><p>The failure mode is dependent on the material model, when hardening is applied fewer threads areneeded to achieve bolt failure rather then thread failure, compared to when a ideal plastic materialmodel is applied.</p><p>The reduction factors are not affected by the material yield limit, the maximal load is however highlydependent on the yield limit.</p><p>The placement of the missing thread does not effect the reduction factors. They can therefor beused regardless of if threads are missing due to damage or due to partial thread engagement.</p><p>A test of an M8 bolt was performed to attempt to validate the FEM-model. Due to some inherent flawsin the test procedure no clear conclusion, about the validity of the model can be made. It is howeverclear that missing threads induces risk of failure when tightening the assembly, since the bolt or nutcan be damaged without any clear signs that the assembly is compromised, leading to catastrophicfailure. Reducing the load with the reduction factor model should be done with caution.</p>


corrected abstract:
<p>The purpose of this master thesis is to determine whether or not it exist, a model that can describe the reduction in strength, due to missing threads in a bond between a bolt and nut. And how the reduction in strength might effect a wall-shoe assembly, used to connect a wall to another wall or a concrete base plate. This is done by firstly considering the strength of the entire wall-shoe assembly and the strength of the bond between the nut and the bolt is then considered.</p><p>The strength of the entire assembly is calculated using some simple analytical models. The strength of the bolt is the limiting factor for the assembly given the analytical models. Four FEM-models are then created, three to evaluate the strength of the bolt and nut assembly, for bolt sized from M6 to M60 with thread engagement from one thread to seven threads.</p><ul><li>An elastic model without a defined tensile stress limit is proposed. The maximum stress at the sharp geometry changes (stress concentrations) are used to dimension the maximum allowed load according to the von Mises yield criterion.</li><li>The yield limit is implemented by introducing a elastic-plastic (without hardening) for three different materials. Where the maximum yield force is determined as the maximum reaction force on the frictionless support boundary condition, when a displacement is applied.</li><li><p>Material hardening is applied according to a bi-linear material model (with hardening). The reaction force is evaluated and the maximum force and displacement can be determined using the definition of property class 8.8 that propose a yield limit of 80 percent of maximum load.</p><p>The behavior of the anchor bolt when it is pulled out of the concrete was also modeled. To obtain an understanding of the pull-out behavior.</p></li></ul><p>The FEM-models makes it possible to formulate a simple reduction model, where the bolt failure load is reduced by the calculated reduction factors. The reduction factors are dependent on the amount of missing thread due to damages or insufficient bolt height. The reduction factors are also highly dependent on the ratio between the rise and bolt diameter. The reduction factors are significantly smaller when a perfect plastic material model is applied.</p><p>The failure mode is dependent on the material model, when hardening is applied fewer threads are needed to achieve bolt failure rather then thread failure, compared to when a ideal plastic material model is applied.</p><p>The reduction factors are not affected by the material yield limit, the maximal load is however highly dependent on the yield limit.</p><p>The placement of the missing thread does not effect the reduction factors. They can therefor be used regardless of if threads are missing due to damage or due to partial thread engagement.</p><p>A test of an M8 bolt was performed to attempt to validate the FEM-model. Due to some inherent flaws in the test procedure no clear conclusion, about the validity of the model can be made. It is however clear that missing threads induces risk of failure when tightening the assembly, since the bolt or nut can be damaged without any clear signs that the assembly is compromised, leading to catastrophic failure. Reducing the load with the reduction factor model should be done with caution.</p>
----------------------------------------------------------------------
In diva2:1527832 abstract is:
<p>This master’s thesis covers the structuring and implementation of a digital testbench for the air brake system of freight trains. The test bench will serveto further improve the existing brake models at Transrail Sweden AB. Theseare used for the optimised calculation of train speed profiles by the DriverAdvisory System CATO. This work is based on the research of the technicalbackground, as well as the methodical approach to physical modelling anda modular implementation of the test bench. It gives full flexibility for thesimulation of customised train configurations using the European UIC brakesystem. Train length and vehicle arrangement can be adapted to the user’sspecific needs. For example, the test bench could be used for the simulation ofa train with distributed power. The system parameters are stored in a vehiclelibrary for the convenient generation of train configurations. This vehiclelibrary is freely expandable.The simulation is based on an equivalent electric circuit model which iscompleted with nozzle flow modelling. This model involves monitoring themain pipe, brake cylinder and reservoir pressure. Linear approximation is usedto obtain braking forces for the individual wagons and for the whole train. Thedepiction of the brake system behaviour is mostly accurate in the operationalscenarios, which is validated with measurement data. Additional calibrationis required for further reduction of the simulation errors and an extension ofthe model’s domain of validity. The test bench is developed by incrementaland iterative modelling and prepared for further improvements and variations,for example the adaption to the American AAR system variant.The presented work can also be used as a basis for similar implementationssuch as driving simulators. The methods are transferable to other applicationsof modular simulation.</p>

corrected abstract:
<p>This master’s thesis covers the structuring and implementation of a digital test bench for the air brake system of freight trains. The test bench will serve to further improve the existing brake models at Transrail Sweden AB. These are used for the optimised calculation of train speed profiles by the Driver Advisory System CATO. This work is based on the research of the technical background, as well as the methodical approach to physical modelling and a modular implementation of the test bench. It gives full flexibility for the simulation of customised train configurations using the European UIC brake system. Train length and vehicle arrangement can be adapted to the user’s specific needs. For example, the test bench could be used for the simulation of a train with distributed power. The system parameters are stored in a vehicle library for the convenient generation of train configurations. This vehicle library is freely expandable.</p><p>The simulation is based on an equivalent electric circuit model which is completed with nozzle flow modelling. This model involves monitoring the main pipe, brake cylinder and reservoir pressure. Linear approximation is used to obtain braking forces for the individual wagons and for the whole train. The depiction of the brake system behaviour is mostly accurate in the operational scenarios, which is validated with measurement data. Additional calibration is required for further reduction of the simulation errors and an extension of the model’s domain of validity. The test bench is developed by incremental and iterative modelling and prepared for further improvements and variations, for example the adaption to the American AAR system variant.</p><p>The presented work can also be used as a basis for similar implementations such as driving simulators. The methods are transferable to other applications of modular simulation.</p>
----------------------------------------------------------------------
In diva2:1500046 abstract is:
<p>The reliability of a mechanical system containing electronic packages is highly affectedby the environment the system is stationed in. The difference and fluctuationsbetween the ambient temperature and the operating temperature of the electronicpackage cause accumulation of inelastic strains in the package components thusdecreasing the service life. The most common failure modes of an electronic packagehas been identified from inspection of malfunctioning machines as cracks in the solderjoint and delamination between the glue and the die. Knowledge regarding therelationships between parameters affecting these failure modes, which are importantand which are not, is of high interest when developing new and existing products. SAAB AB would like to develop a methodology using design exploration to allow forevaluation of electronic packages using nonlinear finite element methods.</p><p>A surrogate model was created and parameterized with HyperMorph to be used forthree linear static variations of design of experiments, where both the performance ofthe methods themselves and the relative importance of the parameters were ofinterest. A connectivity condition was also implemented to allow for relativemovement between components while keeping the mesh intact. The designexploration was executed using a Taguchi design, a Modified extensive latticesequence design and a fractional factorial design where the three methods werecompared as well as the parameter significance analysed. An optimization was thenperformed to find the optimal parameter settings within the allowed bounds to beused where a nominal model and an optimized model are evaluated with animplemented creep law. The fatigue life of the two models were then estimated.</p>

corrected abstract:
<p>The reliability of a mechanical system containing electronic packages is highly affected by the environment the system is stationed in. The difference and fluctuations between the ambient temperature and the operating temperature of the electronic package cause accumulation of inelastic strains in the package components thus decreasing the service life. The most common failure modes of an electronic package has been identified from inspection of malfunctioning machines as cracks in the solder joint and delamination between the glue and the die. Knowledge regarding the relationships between parameters affecting these failure modes, which are important and which are not, is of high interest when developing new and existing products. SAAB AB would like to develop a methodology using design exploration to allow for evaluation of electronic packages using nonlinear finite element methods.</p><p>A surrogate model was created and parameterized with HyperMorph to be used for three linear static variations of design of experiments, where both the performance of the methods themselves and the relative importance of the parameters were of interest. A connectivity condition was also implemented to allow for relative movement between components while keeping the mesh intact. The design exploration was executed using a Taguchi design, a Modified extensive lattice sequence design and a fractional factorial design where the three methods were compared as well as the parameter significance analysed. An optimization was then performed to find the optimal parameter settings within the allowed bounds to be used where a nominal model and an optimized model are evaluated with an implemented creep law. The fatigue life of the two models were then estimated.</p>
----------------------------------------------------------------------
In diva2:1229161 missing hyphen in title:
"Direct optimization of dose-volume histogram metrics in intensity modulated radiation therapy treatment planning"
==>
"Direct optimization of dose-volume histogram metrics in intensity-modulated radiation therapy treatment planning"

abstract is:
<p>In optimization of intensity-modulated radiation therapy treatment plans, dose-volumehistogram (DVH) functions are often used as objective functions to minimize the violationof dose-volume criteria. Neither DVH functions nor dose-volume criteria, however,are ideal for gradient-based optimization as the former are not continuously differentiableand the latter are discontinuous functions of dose, apart from both beingnonconvex. In particular, DVH functions often work poorly when used in constraintsdue to their being identically zero when feasible and having vanishing gradients on theboundary of feasibility.In this work, we present a general mathematical framework allowing for direct optimizationon all DVH-based metrics. By regarding voxel doses as sample realizations ofan auxiliary random variable and using kernel density estimation to obtain explicit formulas,one arrives at formulations of volume-at-dose and dose-at-volume which are infinitelydifferentiable functions of dose. This is extended to DVH functions and so calledvolume-based DVH functions, as well as to min/max-dose functions and mean-tail-dosefunctions. Explicit expressions for evaluation of function values and corresponding gradientsare presented. The proposed framework has the advantages of depending on onlyone smoothness parameter, of approximation errors to conventional counterparts beingnegligible for practical purposes, and of a general consistency between derived functions.Numerical tests, which were performed for illustrative purposes, show that smoothdose-at-volume works better than quadratic penalties when used in constraints and thatsmooth DVH functions in certain cases have significant advantage over conventionalsuch. The results of this work have been successfully applied to lexicographic optimizationin a fluence map optimization setting.</p>

corrected abstract:
<p>In optimization of intensity-modulated radiation therapy treatment plans, dose-volume histogram (DVH) functions are often used as objective functions to minimize the violation of dose-volume criteria. Neither DVH functions nor dose-volume criteria, however, are ideal for gradient-based optimization as the former are not continuously differentiable and the latter are discontinuous functions of dose, apart from both being nonconvex. In particular, DVH functions often work poorly when used in constraints due to their being identically zero when feasible and having vanishing gradients on the boundary of feasibility.</p><p>In this work, we present a general mathematical framework allowing for direct optimization on all DVH-based metrics. By regarding voxel doses as sample realizations of an auxiliary random variable and using kernel density estimation to obtain explicit formulas, one arrives at formulations of volume-at-dose and dose-at-volume which are infinitely differentiable functions of dose. This is extended to DVH functions and so called volume-based DVH functions, as well as to min/max-dose functions and mean-tail-dose functions. Explicit expressions for evaluation of function values and corresponding gradients are presented. The proposed framework has the advantages of depending on only one smoothness parameter, of approximation errors to conventional counterparts being negligible for practical purposes, and of a general consistency between derived functions.</p><p>Numerical tests, which were performed for illustrative purposes, show that smooth dose-at-volume works better than quadratic penalties when used in constraints and that smooth DVH functions in certain cases have significant advantage over conventional such. The results of this work have been successfully applied to lexicographic optimization in a fluence map optimization setting.</p>
----------------------------------------------------------------------
In diva2:1120314 abstract is:
<p>A better understanding of turbine performance and its sensitivity to variations in the inletboundary conditions is crucial in the quest of further improving the efficiency of aero engines.Within the research efforts to reach this goal, a high-pressure turbine test rig has been designedby Rolls-Royce Deutschland in cooperation with the Deutsches Zentrum für Luft- und Raumfahrt(DLR), the German Aerospace Center. The scope of the test rig is high-precision measurement ofaerodynamic efficiency including the effects of film cooling and secondary air flows as well as theimprovement of numerical prediction tools, especially 3D Computational Fluid Dynamics (CFD).A sensitivity analysis of the test rig based on detailed 3D CFD computations was carried outwith the aim to quantify the influence of inlet boundary condition variations occurring in the testrig on the outlet capacity of the first stage nozzle guide vane (NGV) and the turbine efficiency.The analysis considered variations of the cooling and rimseal leakage mass flow rates as well asfluctuations in the inlet distributions of total temperature and pressure. The influence of anincreased rotor tip clearance was also studied.This thesis covers the creation, calibration and validation of the steady state 3D CFD modelof the full turbine domain. All relevant geometrical details of the blades, walls and the rimsealcavities are included with the exception of the film cooling holes that are replaced by a volumesource term based cooling strip model to reduce the computational cost of the analysis. Thehigh-fidelity CFD computation is run only on a sample of parameter combinations spread overthe entire input parameter space determined using the optimal latin hypercube technique. Thesubsequent sensitivity analysis is based on a Kriging response surface model fit to the sampledata. The results are discussed with regard to the planned experimental campaign on the test rigand general conclusions concerning the impacts of the studied parameters on turbine performanceare deduced.</p>


corrected abstract:
<p>A better understanding of turbine performance and its sensitivity to variations in the inlet boundary conditions is crucial in the quest of further improving the efficiency of aero engines. Within the research efforts to reach this goal, a high-pressure turbine test rig has been designed by Rolls-Royce Deutschland in cooperation with the Deutsches Zentrum für Luft- und Raumfahrt (DLR), the German Aerospace Center. The scope of the test rig is high-precision measurement of aerodynamic efficiency including the effects of film cooling and secondary air flows as well as the improvement of numerical prediction tools, especially 3D Computational Fluid Dynamics (CFD).</p><p>A sensitivity analysis of the test rig based on detailed 3D CFD computations was carried out with the aim to quantify the influence of inlet boundary condition variations occurring in the test rig on the outlet capacity of the first stage nozzle guide vane (NGV) and the turbine efficiency. The analysis considered variations of the cooling and rimseal leakage mass flow rates as well as fluctuations in the inlet distributions of total temperature and pressure. The influence of an increased rotor tip clearance was also studied.</p><p>This thesis covers the creation, calibration and validation of the steady state 3D CFD model of the full turbine domain. All relevant geometrical details of the blades, walls and the rimseal cavities are included with the exception of the film cooling holes that are replaced by a volume source term based cooling strip model to reduce the computational cost of the analysis. The high-fidelity CFD computation is run only on a sample of parameter combinations spread over the entire input parameter space determined using the optimal latin hypercube technique. The subsequent sensitivity analysis is based on a Kriging response surface model fit to the sample data. The results are discussed with regard to the planned experimental campaign on the test rig and general conclusions concerning the impacts of the studied parameters on turbine performance are deduced.</p>
----------------------------------------------------------------------
In diva2:618595 missing space in title:
"Future fuel for worldwide tankershipping in spot market"
==>
"Future fuel for worldwide tanker shipping in spot market"

abstract is:
<p>Ship exhausts contain high levels of sulphur oxides, nitrogen oxides, carbon dioxide and particles dueto the heavy fuel oil, HFO, used for combustion and the combustion characteristics of the engine.As a result of upcoming stricter regulations for shipping pollution, as well as growing attentionto greenhouse gas emissions, air pollution and uncertainty of future petroleum oil supply, a shifttowards a cleaner burning fuel is needed.This work explores potential alternative fuels, both conventional and unconventional, and abatementtechnologies, to be used by tankers in the worldwide spot market to comply with upcomingenvironmental regulations in the near and coming future. As a reference the product tanker M/TGotland Marieann is used and recommendations for which fuel that shall be used by the referenceship in 2015 and 2020 are presented.The environmental assessment and evaluation of the fuels are done from a life cycle perspective usingresults from Life Cycle Assessment, LCA, studies.This study illustrates that, of the various alternatives, methanol appears to be the best candidatefor long-term, widespread replacement of petroleum-based fuels within tanker shipping. It does notemit any sulphur oxides nor particles and the nitrogen oxides are shown to be lower than those ofmarine gas oil, MGO. The global warming potential of the natural gas produced methanol is notlower than that of MGO, but when gradually switching to bio-methanol the greenhouse gas emissionsare decreasing and with methanol the vision of a carbon free society can be reached.For 2015 a switch towards methanol is not seen as realistic. Further research and establishment ofregulations and distribution systems are needed, however there are indications that a shift will bepossible sometime between 2015 and 2020. For 2015 a shift towards MGO is suggested as it involveslow investment costs and there is no need for infrastructure changes. As MGO is more expensivethan methanol, a shift is preferable as soon as the market, technology and infrastructure are ready.</p>

corrected abstract:
<p>Ship exhausts contain high levels of sulphur oxides, nitrogen oxides, carbon dioxide and particles due to the heavy fuel oil, HFO, used for combustion and the combustion characteristics of the engine. As a result of upcoming stricter regulations for shipping pollution, as well as growing attention to greenhouse gas emissions, air pollution and uncertainty of future petroleum oil supply, a shift towards a cleaner burning fuel is needed.</p><p>This work explores potential alternative fuels, both conventional and unconventional, and abatement technologies, to be used by tankers in the worldwide spot market to comply with upcoming environmental regulations in the near and coming future. As a reference the product tanker M/T Gotland Marieann is used and recommendations for which fuel that shall be used by the reference ship in 2015 and 2020 are presented.</p><p>The environmental assessment and evaluation of the fuels are done from a life cycle perspective using results from Life Cycle Assessment, LCA, studies.</p><p>This study illustrates that, of the various alternatives, methanol appears to be the best candidate for long-term, widespread replacement of petroleum-based fuels within tanker shipping. It does not emit any sulphur oxides nor particles and the nitrogen oxides are shown to be lower than those of marine gas oil, MGO. The global warming potential of the natural gas produced methanol is not lower than that of MGO, but when gradually switching to bio-methanol the greenhouse gas emissions are decreasing and with methanol the vision of a carbon free society can be reached.</p><p>For 2015 a switch towards methanol is not seen as realistic. Further research and establishment of regulations and distribution systems are needed, however there are indications that a shift will be possible sometime between 2015 and 2020. For 2015 a shift towards MGO is suggested as it involves low investment costs and there is no need for infrastructure changes. As MGO is more expensive than methanol, a shift is preferable as soon as the market, technology and infrastructure are ready.</p>
----------------------------------------------------------------------
In diva2:401149 abstract is:
<p> </p>
<p> </p>
<p>During the 2000s, the ship owners have become more and more concerned thattheir ships save fuel. Several projects have been undertaken to exploit the resourcesavailable on board today’s vessels to reduce fuel consumption. As a stepin this the Swedish Meteorological and Hydrological Institute (SMHI) today offera Weather Routing service to ships. By planning your route more effectivelymuch fuel can be saved.This thesis has been about developing a fuel prediction program (FPP) forhow much fuel a ship consumes in different sea conditions. The model takes intoaccount the ship’s loading condition, winds, wind waves and swell. Any othereffects are pooled in one term. This makes it possible to also consider how muchfuel the ship consumes on the various route options in the planning process.The model will also be a useful tool to retrospectively evaluate how a ship hasperformed in relation to the contract.On the ships in this report the prediction program was able to calculate thefuel consumption with an error of only 1% of the reported fuel consumption.This requires that the data about the vessel is accurate and up to date. If not,the model can still, with thoughtful assumptions, reach an error of less than10% of the reported consumption, which is better than the strategy that SMHIuses today.</p>



corrected abstract:
<p>During the 2000s, the ship owners have become more and more concerned that their ships save fuel. Several projects have been undertaken to exploit the resources available on board today’s vessels to reduce fuel consumption. As a step in this the Swedish Meteorological and Hydrological Institute (SMHI) today offer a Weather Routing service to ships. By planning your route more effectively much fuel can be saved.</p><p>This thesis has been about developing a fuel prediction program (FPP) for how much fuel a ship consumes in different sea conditions. The model takes into account the ship’s loading condition, winds, wind waves and swell. Any other effects are pooled in one term. This makes it possible to also consider how much fuel the ship consumes on the various route options in the planning process. The model will also be a useful tool to retrospectively evaluate how a ship has performed in relation to the contract.</p><p>On the ships in this report the prediction program was able to calculate the fuel consumption with an error of only 1% of the reported fuel consumption. This requires that the data about the vessel is accurate and up to date. If not, the model can still, with thoughtful assumptions, reach an error of less than 10% of the reported consumption, which is better than the strategy that SMHI uses today.</p>
----------------------------------------------------------------------
In diva2:1877617 - Note: no full text in DiVA

abstract is:
<p>This thesis investigates the optimization of GaAs-based single-line defect photoniccrystal waveguides (PCWs) for mid-infrared (MIR) gas sensing applications. Photoniccrystals (PhCs) are materials with a periodic dielectric constant variation, structuredto create photonic bandgaps for precise light propagation control. This work focuseson the design of PCWs in GaAs membranes and development of processes to fabricateair-holes with smooth and straight sidewalls, which are crucial for optical PhC deviceperformance.The project is structured into two phases. In the first phase, AnsysLumerical is used for the simulation of PCWs, which includes designing a 2D hexagonallattice structure and optimizing parameters such as radius and lattice constant toachieve desired bandstructure, light confinement, and single-mode propagation.Simulation results show that the optimal PCW configuration effectively confines light,provides evanescent fields in both lateral and vertical directions, and supports singlemodepropagation at the target wavelength of 4.26 μm, at which CO2 has a majorabsorption peak. The second phase focuses on the fabrication process, utilizingcharged colloidal lithography and Inductively Coupled Plasma Reactive Ion Etching(ICP-RIE) with Ar/Cl2 chemistry to create air holes in GaAs substrates. The aim is toexplore the effects of varying hole diameters and process conditions to achieve goodhole profiles at a depth of 600 nm — the designed thickness of the GaAs membrane foreffective vertical light confinement. Impact of various parameters on the etch rate andhole profile is studied. A feature-size dependent etching phenomenon, the lag effect,is observed, with a significant variation in etch depths for different hole diameters.The findings provide clear guidelines for optimizing conditions to achieve suitable holedepths and profiles for fabricating MIR PhC devices in GaAs membranes.</p>

corrected abstract:
<p>This thesis investigates the optimization of GaAs-based single-line defect photoniccrystal waveguides (PCWs) for mid-infrared (MIR) gas sensing applications. Photonic crystals (PhCs) are materials with a periodic dielectric constant variation, structured to create photonic bandgaps for precise light propagation control. This work focuses on the design of PCWs in GaAs membranes and development of processes to fabricate air-holes with smooth and straight sidewalls, which are crucial for optical PhC device performance. The project is structured into two phases. In the first phase, Ansys Lumerical is used for the simulation of PCWs, which includes designing a 2D hexagonal lattice structure and optimizing parameters such as radius and lattice constant to achieve desired band structure, light confinement, and single-mode propagation. Simulation results show that the optimal PCW configuration effectively confines light, provides evanescent fields in both lateral and vertical directions, and supports single mode propagation at the target wavelength of 4.26 μm, at which CO<sub>2</sub> has a major absorption peak. The second phase focuses on the fabrication process, utilizing charged colloidal lithography and Inductively Coupled Plasma Reactive Ion Etching(ICP-RIE) with Ar/Cl<<sub>2</sub> chemistry to create air holes in GaAs substrates. The aim is to explore the effects of varying hole diameters and process conditions to achieve good hole profiles at a depth of 600 nm — the designed thickness of the GaAs membrane for effective vertical light confinement. Impact of various parameters on the etch rate and hole profile is studied. A feature-size dependent etching phenomenon, the lag effect, is observed, with a significant variation in etch depths for different hole diameters. The findings provide clear guidelines for optimizing conditions to achieve suitable hole depths and profiles for fabricating MIR PhC devices in GaAs membranes.</p>
----------------------------------------------------------------------
In diva2:1876088 abstract is:
<p>A key component of biological research is cell culture technology, which allows researchersto examine the behavior and functionality of cells in controlled environments. Conventionalcell culture monitoring frequently necessitates taking the cultures out of their incubators tomake observations under a microscope. This exposes them to pollutants and changes in thesurrounding environment and may jeopardize the integrity of the experiment.This thesis presents the development of a cost-effective, miniaturized microscope designedfor imaging of cell cultures directly within incubators. By integrating simple, inexpensiveglass lenses and 3D-printed components and focusing on the ESP32-CAM module for digitalimaging, the project explores various optical setups to optimize image quality while minimizingdisruption to cell environments.Central to the research was the identification and testing of diverse optical configurations todetermine the most effective arrangement for both brightfield and fluorescence microscopy.The design features a baseplate for stability, a filter plate for fluorescence imaging, and afocus adjustment mechanism using magnets. Iterative enhancements led to a side illuminationtechnique using an economical LED, removing the need for a beamsplitter and simplifying theoptical path.The final microscope demonstrated successful brightfield imaging and weak fluorescenceimaging of Madin-Darby Canine Kidney (MDCK) II cell cultures marked with Green FluorescentProtein (GFP), using a magnification ratio of 2.5:1 through an infinity-corrected optical system.The findings illustrate the potential of developing an economical, functional microscope thatcan be readily replicated and scaled for use in cell culture technology.</p><p> </p>

corrected abstract:
<p>A key component of biological research is cell culture technology, which allows researchers to examine the behavior and functionality of cells in controlled environments. Conventional cell culture monitoring frequently necessitates taking the cultures out of their incubators to make observations under a microscope. This exposes them to pollutants and changes in the surrounding environment and may jeopardize the integrity of the experiment.</p><p>This thesis presents the development of a cost-effective, miniaturized microscope designed for imaging of cell cultures directly within incubators. By integrating simple, inexpensive glass lenses and 3D-printed components and focusing on the ESP32-CAM module for digital imaging, the project explores various optical setups to optimize image quality while minimizing disruption to cell environments.</p><p>Central to the research was the identification and testing of diverse optical configurations to determine the most effective arrangement for both brightfield and fluorescence microscopy. The design features a baseplate for stability, a filter plate for fluorescence imaging, and a focus adjustment mechanism using magnets. Iterative enhancements led to a side illumination technique using an economical LED, removing the need for a beamsplitter and simplifying the optical path.</p><p>The final microscope demonstrated successful brightfield imaging and weak fluorescence imaging of Madin-Darby Canine Kidney (MDCK) II cell cultures marked with Green Fluorescent Protein (GFP), using a magnification ratio of 2.5:1 through an infinity-corrected optical system. The findings illustrate the potential of developing an economical, functional microscope that can be readily replicated and scaled for use in cell culture technology.</p>
----------------------------------------------------------------------
In diva2:1787386 abstract is:
<p>In this thesis, an innovative coarse grid CFD approach is developed that aims toexploit the capabilities of sub-channel codes and CFD methods while overcoming theirlimitations. In the approach, a very coarse mesh is implemented in the CFD softwareOpenFOAM and a new wall treatment, based on the traditional concept of the wallfunction, is applied to the wall boundary conditions of the domain to take into accountthe low resolution of the grid which does not allow to effectively capture the effect of thesolid walls on the thermo-hydraulics of the flow. To investigate the performance of thenew approach, the method is implemented first in three simple test cases for whichthe sub-channel codes are the state-of-the-art thermo-hydraulic analysis since theyare single-phase flow problems in which there are no prevailing 3D flow conditions.An additional test case representing a 2x2 fuel bundle with three full-length rods andone half-length rod is investigated to verify the behavior of the new approach in caseswhere secondary flows are present. The results for the pressure fields are comparedwith the analytical pressure profiles for the four test cases that well represent the onesthat would be obtained with sub-channel code analysis, while the results for the wallshear stresses obtained in the four test cases are compared with the ones obtained witha more refined mesh in which the traditional wall function approach is implementedsince they should be the best estimation of the actual wall shear stresses at the walldomain. For the first two cases, the developed approach produces reasonable resultswith a good agreement to the analytical pressure profiles while the other two testcases show that the methodology has a limited applicability and, before proceedingwith the extension of the new approach to single-phase problems with 3D prevailingphenomena and two-phase problems, it is necessary to solve the issues that emerge forsome types of cases.</p>

corrected abstract:
<p>In this thesis, an innovative coarse grid CFD approach is developed that aims to exploit the capabilities of sub-channel codes and CFD methods while overcoming their limitations. In the approach, a very coarse mesh is implemented in the CFD software OpenFOAM and a new wall treatment, based on the traditional concept of the wall function, is applied to the wall boundary conditions of the domain to take into account the low resolution of the grid which does not allow to effectively capture the effect of the solid walls on the thermo-hydraulics of the flow. To investigate the performance of the new approach, the method is implemented first in three simple test cases for which the sub-channel codes are the state-of-the-art thermo-hydraulic analysis since they are single-phase flow problems in which there are no prevailing 3D flow conditions. An additional test case representing a 2x2 fuel bundle with three full-length rods and one half-length rod is investigated to verify the behavior of the new approach in cases where secondary flows are present. The results for the pressure fields are compared with the analytical pressure profiles for the four test cases that well represent the ones that would be obtained with sub-channel code analysis, while the results for the wall shear stresses obtained in the four test cases are compared with the ones obtained with a more refined mesh in which the traditional wall function approach is implemented since they should be the best estimation of the actual wall shear stresses at the wall domain. For the first two cases, the developed approach produces reasonable results with a good agreement to the analytical pressure profiles while the other two test cases show that the methodology has a limited applicability and, before proceeding with the extension of the new approach to single-phase problems with 3D prevailing phenomena and two-phase problems, it is necessary to solve the issues that emerge for some types of cases.</p>
----------------------------------------------------------------------
In diva2:1782728 abstract is:
<p>Composite monocoque frames are becoming increasingly more popular inperformance cars. Compared to their steel and aluminum counterparts theyprovide additional torsional stiffness at the cost of less weight. This thesiscovers the complex optimization process of a monocoque applied within theregulations of a Formula Student competition. It aims to give the reader a goodunderstanding of the rules and how they affect the optimization process whilegenerating an optimized design used in the competition of Formula StudentGermany -21 by KTH Formula Student.</p><p>The rules of Formula Student dictate the structural requirements on themonocoque based on a steel space frame. All materials except low carbon steelused in the structure require proof of equivalence through regulated testingmethods. However, this thesis shows that the regulated setup can severelyaffect results through a deep analysis of the testing methodology.The torsional stiffness of the monocoque is analyzed and optimized accordingto the results of a free-size optimization. Both through slight adjustmentsin chassis geometry and the laminate, resulting in a theoretical torsionalstiffness of 9.9 kNm/deg, more than five times as much as the old space frame.Weighing in at 20 kg, a significant weight reduction of about 10 kg, eventhough it was larger, with a surface area of about 4.2 m2.</p><p>This design will be the first monocoque manufactured within KTH FormulaStudent since 2010. Therefore, a lot of focus was put on analyzing the rulesand lay the ground for future development by conducting tests on optimizedpanels. These results have the potential to further reduce the weight of a futuremonocoque with a different geometry.</p>

corrected abstract:
<p>Composite monocoque frames are becoming increasingly more popular in performance cars. Compared to their steel and aluminum counterparts they provide additional torsional stiffness at the cost of less weight. This thesis covers the complex optimization process of a monocoque applied within the regulations of a Formula Student competition. It aims to give the reader a good understanding of the rules and how they affect the optimization process while generating an optimized design used in the competition of Formula Student Germany -21 by KTH Formula Student.</p><p>The rules of Formula Student dictate the structural requirements on the monocoque based on a steel space frame. All materials except low carbon steel used in the structure require proof of equivalence through regulated testing methods. However, this thesis shows that the regulated setup can severely affect results through a deep analysis of the testing methodology. The torsional stiffness of the monocoque is analyzed and optimized according to the results of a free-size optimization. Both through slight adjustments in chassis geometry and the laminate, resulting in a theoretical torsional stiffness of 9.9 kNm/deg, more than five times as much as the old space frame. Weighing in at 20 kg, a significant weight reduction of about 10 kg, even though it was larger, with a surface area of about 4.2 m<sup>2</sup>.</p><p>This design will be the first monocoque manufactured within KTH Formula Student since 2010. Therefore a lot of focus was put on analyzing the rules and lay the ground for future development by conducting tests on optimized panels. These results have the potential to further reduce the weight of a future monocoque with a different geometry.</p>
----------------------------------------------------------------------
In diva2:1780558 abstract is:
<p>To mitigate climate change a proposed space-based geoengineering solutionis to screen off solar irradiance by placing a membrane in between the Earthand the Sun. The feasibility of such a project largely depends on minimizingthe mass of the shading screen and as an extension to the Sunshade projectthis thesis investigated how such a low-mass membrane could be designed.Because of the acting forces at location in space, minimizing the mass impliesthat the material ought to have a low reflection coeﬀicient and surface densityand therefore the highly transparent material of artificial spider silk was chosenas the proposed material. The only possibility to block light is then byrefraction or diffraction and, since the presence of apertures might lower thesurface density, the structure of the suggested membrane is a grid patternof wires. Such a diffraction grating was investigated while applying twomethods. Method 1 optimized the dimensions of the structure to lower thetotal transmission on Earth when placed on the direct transmission axis ofthe membrane and method 2 tilted the membrane in order to place Earth ata diffraction minimum. This resulted in three suggested designs A, B, andC with surface densities varying from that of 0.00867 to 0.228 gm−2. Theresults were compared with two previous design proposals where the lowestareal density was 0.34g/m2, which is 3/2 to 40 times larger than the densitiesproposed in this paper. The reflectivities for A and B were 12.5 and 3.75 timeslarger than that of the smallest previously achieved reflectivity. The reflectivityof C could not be determined exactly but ought to have a reflectivity at leastas low as B at 3%, making it the most promising candidate for a membranedesign of the three.</p>

corrected abstract:
<p>To mitigate climate change a proposed space-based geoengineering solution is to screen off solar irradiance by placing a membrane in between the Earth and the Sun. The feasibility of such a project largely depends on minimizing the mass of the shading screen and as an extension to the Sunshade project this thesis investigated how such a low-mass membrane could be designed. Because of the acting forces at location in space, minimizing the mass implies that the material ought to have a low reflection coefficient and surface density and therefore the highly transparent material of artificial spider silk was chosen as the proposed material. The only possibility to block light is then by refraction or diffraction and, since the presence of apertures might lower the surface density, the structure of the suggested membrane is a grid pattern of wires. Such a diffraction grating was investigated while applying two methods. Method 1 optimized the dimensions of the structure to lower the total transmission on Earth when placed on the direct transmission axis of the membrane and method 2 tilted the membrane in order to place Earth at a diffraction minimum. This resulted in three suggested designs A, B, and C with surface densities varying from that of 0.00867 to 0.228 gm<sup>−2</sup>. The results were compared with two previous design proposals where the lowest areal density was 0.34gm<sup>-2</sup>, which is 3/2 to 40 times larger than the densities proposed in this paper. The reflectivities for A and B were 12.5 and 3.75 times larger than that of the smallest previously achieved reflectivity. The reflectivity of C could not be determined exactly but ought to have a reflectivity at least as low as B at 3%, making it the most promising candidate for a membrane design of the three.</p>
----------------------------------------------------------------------
In diva2:1761916 abstract is:
<p>Since its introduction by Max Otto Lorenz, the Lorenz curve has been utilizedin several financial contexts. By using regression analysis to approximate theclaim cost of policyholders, a vector consisting of policyholder characteristics canbe obtained. The ordered Lorenz curve can subsequently be used to understandwhat commonalities are shared between profitable policyholders. This allows forbetter management of the insurance portfolio and thus better customer relationstowards both the policyholders and the insurer, which is important for an insuranceconsultancy agency. The aim of this thesis was to investigate which attributesapproximate the policyholder claim costs and consequently obtain insight into whatattributes are shared among profitable portfolio clients. The results presented inthis thesis show that a multi-linear regression model, transformed using the Box-Cox method is insuﬀicient to approximate the claim costs in a convincing manner.The model obtained in the thesis was capable of identifying significant regressorsbut the overall result displayed uncertainties in regards to overall goodness of fit.This means that the variability explained by the regression model only represents4.95% of the variability in the claim cost data. Thus, the relativity measureintroduced in section 2.1.1 was deemed uninterruptible in a meaningful way.Consequently, the empirical distribution functions presented in section 1.1 wouldbe based on a faulty order statistic, and in turn the visualization of an orderedLorenz curve with such a relativity measure is unnecessary.</p>

corrected abstract:
<p>Since its introduction by Max Otto Lorenz, the Lorenz curve has been utilized in several financial contexts. By using regression analysis to approximate the claim cost of policyholders, a vector consisting of policyholder characteristics can be obtained. The ordered Lorenz curve can subsequently be used to understand what commonalities are shared between profitable policyholders. This allows for better management of the insurance portfolio and thus better customer relations towards both the policyholders and the insurer, which is important for an insurance consultancy agency. The aim of this thesis was to investigate which attributes approximate the policyholder claim costs and consequently obtain insight into what attributes are shared among profitable portfolio clients. The results presented inthis thesis show that a multi-linear regression model, transformed using the Box-Cox method is insufficient to approximate the claim costs in a convincing manner. The model obtained in the thesis was capable of identifying significant regressors but the overall result displayed uncertainties in regards to overall goodness of fit. This means that the variability explained by the regression model only represents 4.95% of the variability in the claim cost data. Thus, the relativity measure introduced in section 2.1.1 was deemed uninterruptible in a meaningful way. Consequently, the empirical distribution functions presented in section 1.1 would be based on a faulty order statistic, and in turn the visualization of an ordered Lorenz curve with such a relativity measure is unnecessary.</p>
----------------------------------------------------------------------
In diva2:1583521 abstract is:
<p>With the increasing number of satellites operating in orbit and the development of nanosatelliteconstellations, it has become more and more arduous for operators to keep track of every satellitestate, and perform corrective or avoidance manoeuvres. That is why CNES, the French space agency,is developing new algorithms, which aimed at making satellites more self-su cient. More especially,these algorithms are in charge of autonomous orbit control, collision risk calculations and satellitestatus monitoring. In this thesis, we present the architecture of these three algorithms and how theyinteract between them to deal with the autonomous control of a satellite. In addition, this paper studiestheir integration within the OPS-SAT nanosatellite, which is an in-orbit demonstrator developed bythe European Space Agency (ESA) and opened to worldwide experimenters. By analysing the dataused by the numerical propagators, the size of the input configuration files sent to the nanosatellitewas optimised. Thanks to this optimisation, the size of telecommands sent during each OPS-SATflyby above the ESOC ground station meets the requirements.</p><p>Due to some issues encountered with the nanosatellite’s GPS, a solution was found to update thecurrent orbit on-board, and thus allow the proper algorithms’ operation. This thesis also introduceshow the tests were carried out in order to validate these algorithms, both on flat-sat and on the realsatellite. The results demonstrate that their integration on the OPS-SAT numerical environment issuccessful, meaning that the algorithms and their dependences are correctly packaged, sent and uploaded,and that they work as expected. Their execution time are of course longer due to the limitedcalculation capacity of the on-board computer, but are still compatible with real operations, except forthe collision risk computation, which can exceed the orbital period depending on the initial conditions.Finally, the thesis presents the process of real operations for one of the three algorithms developed byCNES, the di culties encountered and the solutions considered.</p>

corrected abstract:
<p>With the increasing number of satellites operating in orbit and the development of nanosatellite constellations, it has become more and more arduous for operators to keep track of every satellite state, and perform corrective or avoidance manoeuvres. That is why CNES, the French space agency, is developing new algorithms, which aimed at making satellites more self-sufficient. More especially, these algorithms are in charge of autonomous orbit control, collision risk calculations and satellite status monitoring. In this thesis, we present the architecture of these three algorithms and how they interact between them to deal with the autonomous control of a satellite. In addition, this paper studies their integration within the OPS-SAT nanosatellite, which is an in-orbit demonstrator developed by the European Space Agency (ESA) and opened to worldwide experimenters. By analysing the data used by the numerical propagators, the size of the input configuration files sent to the nanosatellite was optimised. Thanks to this optimisation, the size of telecommands sent during each OPS-SAT flyby above the ESOC ground station meets the requirements.</p><p>Due to some issues encountered with the nanosatellite’s GPS, a solution was found to update the current orbit on-board, and thus allow the proper algorithms’ operation. This thesis also introduces how the tests were carried out in order to validate these algorithms, both on flat-sat and on the real satellite. The results demonstrate that their integration on the OPS-SAT numerical environment is successful, meaning that the algorithms and their dependences are correctly packaged, sent and uploaded, and that they work as expected. Their execution time are of course longer due to the limited calculation capacity of the on-board computer, but are still compatible with real operations, except for the collision risk computation, which can exceed the orbital period depending on the initial conditions. Finally, the thesis presents the process of real operations for one of the three algorithms developed by CNES, the difficulties encountered and the solutions considered.</p>
----------------------------------------------------------------------
In diva2:1465506 abstract is:
<p>Silencers are used in vehicles to reduce the noise in the engine system. However,silencers themselves may produce break-out noise due to the interactionwith the exhaust gas ow and structure. In this Master thesis project, thenumerical simulation of vibrational behavior of housing plates of silencers isdeveloped.The housing plate is composed of two steel plates and a brous materiallayer. Measurement results show that the brous material has good dampingeect to decrease the vibration and radiated sound of steel plates. Steel platesare connected by spot welding. Modeling of spot welds can improve themodal assurance criterion between simulation and measurements. Interfacedamping is introduced into the simulation models to simulate the contacteect between two steel plates so that the simulated amplitude can have agood agreement with measurement result.Several numerical models of brous material are investigated. The Mikimodel is not chosen for the nal result due to the limit of range of frequencies.The rigid frame model can simulate the sound absorption but is unfeasible forthe simulation of vibration. The limp frame model can simulate the vibrationof light glass wool but cannot simulate the vibration of heavy glass wool.Finally, the Biot-Allard model which is a poro-elastic model is investigatedfor the nal result. The simulation results show good agreement with themeasurement result.</p>


corrected abstract:
<p>Silencers are used in vehicles to reduce the noise in the engine system. However, silencers themselves may produce break-out noise due to the interaction with the exhaust gas flow and structure. In this Master thesis project, the numerical simulation of vibrational behavior of housing plates of silencers is developed.</p><p>The housing plate is composed of two steel plates and a fibrous material layer. Measurement results show that the fibrous material has good damping effect to decrease the vibration and radiated sound of steel plates. Steel plates are connected by spot welding. Modeling of spot welds can improve the modal assurance criterion between simulation and measurements. Interface damping is introduced into the simulation models to simulate the contact effect between two steel plates so that the simulated amplitude can have a good agreement with measurement result.</p><p>Several numerical models of fibrous material are investigated. The Miki model is not chosen for the final result due to the limit of range of frequencies. The rigid frame model can simulate the sound absorption but is unfeasible for the simulation of vibration. The limp frame model can simulate the vibration of light glass wool but cannot simulate the vibration of heavy glass wool. Finally, the Biot-Allard model which is a poro-elastic model is investigated for the final result. The simulation results show good agreement with the measurement result.</p>
----------------------------------------------------------------------
In diva2:1244326 missing space in title:
"Transonic Flutter for aGeneric Fighter Configuration"
==>
"Transonic Flutter for a Generic Fighter Configuration"

abstract is:
<p>A hazardous and not fully understood aeroelastic phenomenon is the transonic dip,the decrease in flutter dynamic pressure that occurs for most aircraft configurationsin transonic flows. The difficulty of predicting this phenomenon forces aircraft manufacturersto run long and costly flight test campaigns to demonstrate flutter-free behaviourof their aircraft at transonic Mach numbers.In this project, subsonic and transonic flutter calculations for the KTH-NASA genericfighter research model have been performed and compared to existing experimentalflutter data from wind tunnel tests performed at NASA Langley in 2016. For the fluttercalculations, industry-standard linear panel methods have been used together with afinite element model from NASTRAN.Further, an alternative approach for more accurate transonic flutter predictions usingthe full-potential solver Phi has been investigated. To predict flutter using this newmethodology a simplified structural model has been used together with aerodynamicmeshes of the main wing. The purpose of the approach was to see if it was possibleto find a method that was more accurate than panel methods in the transonic regimewhilst still being suitable for use during iterative design processes.The results of this project demonstrated that industry-standard linear panel methodssignificantly over-predict the flutter boundary in the transonic regime. It was alsoseen that the flutter predictions using Phi showed potential, being close to the linearresults for the same configuration as tested in Phi. For improved transonic accuracy inPhi, an improved transonic flow finite element formulation could possibly help .Another challenge with Phi is the requirement of an explicit wake from all liftingsurfaces in the aerodynamic mesh. Therefore, a method for meshing external storeswith blunt trailing edges needs to be developed. One concept suggested in this projectis to model external stores in "2.5D", representing external stores using airfoils withsharp trailing edges.</p>

corrected abstract:
<p>A hazardous and not fully understood aeroelastic phenomenon is the transonic dip, the decrease in flutter dynamic pressure that occurs for most aircraft configurations in transonic flows. The difficulty of predicting this phenomenon forces aircraft manufacturers to run long and costly flight test campaigns to demonstrate flutter-free behaviour of their aircraft at transonic Mach numbers.</p><p>In this project, subsonic and transonic flutter calculations for the KTH-NASA generic fighter research model have been performed and compared to existing experimental flutter data from wind tunnel tests performed at NASA Langley in 2016. For the flutter calculations, industry-standard linear panel methods have been used together with a finite element model from NASTRAN.</p><p>Further, an alternative approach for more accurate transonic flutter predictions using the full-potential solver Phi has been investigated. To predict flutter using this new methodology a simplified structural model has been used together with aerodynamic meshes of the main wing. The purpose of the approach was to see if it was possible to find a method that was more accurate than panel methods in the transonic regime whilst still being suitable for use during iterative design processes.</p><p>The results of this project demonstrated that industry-standard linear panel methods significantly over-predict the flutter boundary in the transonic regime. It was also seen that the flutter predictions using Phi showed potential, being close to the linear results for the same configuration as tested in Phi. For improved transonic accuracy in Phi, an improved transonic flow finite element formulation could possibly help.</p><p>Another challenge with Phi is the requirement of an explicit wake from all lifting surfaces in the aerodynamic mesh. Therefore, a method for meshing external stores with blunt trailing edges needs to be developed. One concept suggested in this project is to model external stores in "2.5D", representing external stores using airfoils with sharp trailing edges.</p>
----------------------------------------------------------------------
In diva2:1083220 abstract is:
<p>The current pollution policies in all European and American countries are forcing the industry to movetowards a more efficient and environmentally friendly engines. On the other hand, customers requiremaintaining the power and fuel consumption. Lowering mainly nitrous oxides (NOx) and carbon particles(Soot) is therefore a challenging task with a very strong impact on mainly the automotive andaeronautical market.The purpose of the current work is to research the pollution production of automotive diesel enginesand optimize the fuel injection and piston geometry to lower the emissions. The interaction betweenfuel and air as well as the combustion are the two main physical and chemical processes governing thepollutants formation. Converged-CFD will be the CFD tool employed during the analysis of the previousproblems.The fuel-air interaction is related to jet break up, vaporization and turbulence. The strong dependenceon the surrounding flow field of the previous processes require the equations to be solved numericallywithin a CFD code. The fuel is to be placed in a combustion chamber (piston) where the spray will affectthe surrounding flow field and ultimately the combustion process.In order to accurately represent the nature of the processes, the current work is divided into two mainchapters. Spray modelling and Combustion Modelling. The first will help to accurately model the discretephase (fuel spray) and the vapour entrainment. The second chapter, combustion modelling willretrieve the knowledge gain in the first part to accurately represent the fuel injection in the chamber aswell as the combustion process to ultimately model the pollutants emissions.Finally, a piston bowl optimization will be performed using the previous analysed models and give theindustry a measure of the potential improvement by just adjusting the fuel injection or by modifyingthe piston bowl geometry.</p>


corrected abstract:
<p>The current pollution policies in all European and American countries are forcing the industry to move towards a more efficient and environmentally friendly engines. On the other hand, customers require maintaining the power and fuel consumption. Lowering mainly nitrous oxides (NOx) and carbon particles (Soot) is therefore a challenging task with a very strong impact on mainly the automotive and aeronautical market.</p><p>The purpose of the current work is to research the pollution production of automotive diesel engines and optimize the fuel injection and piston geometry to lower the emissions. The interaction between fuel and air as well as the combustion are the two main physical and chemical processes governing the pollutants formation. Converged-CFD will be the CFD tool employed during the analysis of the previous problems.</p><p>The fuel-air interaction is related to jet break up, vaporization and turbulence. The strong dependence on the surrounding flow field of the previous processes require the equations to be solved numerically within a CFD code. The fuel is to be placed in a combustion chamber (piston) where the spray will affect the surrounding flow field and ultimately the combustion process.</p><p>In order to accurately represent the nature of the processes, the current work is divided into two main chapters. Spray modelling and Combustion Modelling. The first will help to accurately model the discrete phase (fuel spray) and the vapour entrainment. The second chapter, combustion modelling will retrieve the knowledge gain in the first part to accurately represent the fuel injection in the chamber as well as the combustion process to ultimately model the pollutants emissions.</p><p>Finally, a piston bowl optimization will be performed using the previous analysed models and give the industry a measure of the potential improvement by just adjusting the fuel injection or by modifying the piston bowl geometry.</p>
----------------------------------------------------------------------
In diva2:1078083 missing space in title:
"Wind tunnel blockage corrections forwind turbine measurements"
==>
"Wind tunnel blockage corrections for wind turbine measurements"

abstract is:
<p>Wind-tunnel measurements are an important step during the windturbinedesign process. The goal of wind-tunnel tests is to estimate theoperational performance of the wind turbine, for example by measuringthe power and thrust coecients. Depending on the sizes of both thewind turbine and the test section, the eect of blockage can be substantial.Correction schemes for the power and thrust coecients havebeen proposed in the literature, but for high blockage and highly loadedrotors these correction schemes become less accurate.A new method is proposed here to calculate the eect a cylindricalwind-tunnel test section has on the performance of the wind turbine.The wind turbine is modeled with a simplied vortex model. Usingvortices of constant circulation to model the wake vortices, the performancecharacteristics are estimated. The test section is modeled witha panel method, adapted for this specic situation. It uses irrotationalaxisymmetric source panels to enforce the solid-wall boundary condition.Combining both models in an iterative scheme allows for thesimulation of the eect of the presence of the test-section walls on windturbines performace.Based on the proposed wind-tunnel model, a more general empirical correlationscheme is proposed to estimate the performance characteristicsof a wind turbine operating under unconned conditions by correctingthe performance measured in the conned wind-tunnel conguration.The proposed correction scheme performs better than the existing correctionschemes, including cases with high blockage and highly loadedrotors.</p>


corrected abstract:
<p>Wind-tunnel measurements are an important step during the windturbine design process. The goal of wind-tunnel tests is to estimate the operational performance of the wind turbine, for example by measuring the power and thrust coefficients. Depending on the sizes of both the wind turbine and the test section, the effect of blockage can be substantial. Correction schemes for the power and thrust coefficients have been proposed in the literature, but for high blockage and highly loaded rotors these correction schemes become less accurate.</p><p>A new method is proposed here to calculate the effect a cylindrical wind-tunnel test section has on the performance of the wind turbine. The wind turbine is modeled with a simplified vortex model. Using vortices of constant circulation to model the wake vortices, the performance characteristics are estimated. The test section is modeled with a panel method, adapted for this specific situation. It uses irrotational axisymmetric source panels to enforce the solid-wall boundary condition. Combining both models in an iterative scheme allows for the simulation of the effect of the presence of the test-section walls on wind turbines performace.</p><p>Based on the proposed wind-tunnel model, a more general empirical correlation scheme is proposed to estimate the performance characteristics of a wind turbine operating under unconfined conditions by correcting the performance measured in the confined wind-tunnel configuration. The proposed correction scheme performs better than the existing correction schemes, including cases with high blockage and highly loaded rotors.</p>
----------------------------------------------------------------------
In diva2:1817116 abstract is:
<p>Composite structures are commonly joined using adhesive or mechanical joints, withmechanical joints being preferred when components need to be removable for maintenancepurposes. However, the presence of mechanical joints introduces a discontinuity in theload path, which can serve as an initiation point for failure and needs to be taken intoaccount in the design of the joint. Additionally, delaminations may occur around thefastener hole during the manufacturing and assembly processes, further impacting thestrength of the laminate under compressive loading. While some studies have assessedthe residual strength of open-hole specimens, limited information exists regarding theresidual bearing strength in delaminated composite joints. This study aims to assessthe significance of delaminations of varying sizes on the bearing strength of single-bolt,single-lap shear joints under static loading using numerical analysis methods. The effectsof countersinking and bolt size are also examined. Stress and progressive failure analysisare utilized to evaluate different parameters and account for the nonlinear behavior of thematerials. The study reveals that the presence of delamination leads to degradation ofthe bearing strength of approximately five percent when bolt pretension is applied and15 percent in the absence of pretension. Countersinking increases maximum and averagestresses on the cylindrical section of the hole, while a larger bolt size enhances bearingstrength by reducing bolt bending in single-lap shear joints.</p>

corrected abstract:
<p>Composite structures are commonly joined using adhesive or mechanical joints, with mechanical joints being preferred when components need to be removable for maintenance purposes. However, the presence of mechanical joints introduces a discontinuity in the load path, which can serve as an initiation point for failure and needs to be taken into account in the design of the joint. Additionally, delaminations may occur around the fastener hole during the manufacturing and assembly processes, further impacting the strength of the laminate under compressive loading. While some studies have assessed the residual strength of open-hole specimens, limited information exists regarding the residual bearing strength in delaminated composite joints. This study aims to assess the significance of delaminations of varying sizes on the bearing strength of single-bolt, single-lap shear joints under static loading using numerical analysis methods. The effects of countersinking and bolt size are also examined. Stress and progressive failure analysis are utilized to evaluate different parameters and account for the nonlinear behavior of the materials. The study reveals that the presence of delamination leads to degradation of the bearing strength of approximately five percent when bolt pretension is applied and 15 percent in the absence of pretension. Countersinking increases maximum and average stresses on the cylindrical section of the hole, while a larger bolt size enhances bearing strength by reducing bolt bending in single-lap shear joints.</p>
----------------------------------------------------------------------
In diva2:1817018 abstract is:
<p>Space industry has been booming in the recent times, investments have not justbeen made on satellites and launch vehicles but also on space sustainability. Spaceindustry has its users in national agencies, private commercial agencies, academia andexperimental organisations. Smallsats and cubesats are one of the most interestingdomains in space industry today. This has led to pure research and astonishinginnovations in this domain to enable lower costs, lower mass, increased orbital period,better accessibility and global impact. Development of sustainable products requiresthe system to qualify a certain standard set up in the industry. This standard ensuresthat the system safely completes its mission up in space. The problem described byGomSpace Sweden concerns one of their ongoing products, a cold gas propulsionunit which is suitable for a typical 6U cubesat called ESA6DOF. In large, the productconsists of a structure, two propellant tanks, one plenum tank, piping, electronics andsix thrusters. Qualification of this propulsion system module involves the system toundergo a random vibration test according to ECSS standards. This thesis work shallbe focused on setting up the structure needed to perform random vibration simulationsin COMSOL. This step is done to primarily iterate the design to make it robust enoughto sustain loads during flight and also to avoid any physical damages during testingcampaign. Followed by performing the actual random vibration test at a facility usingthe assembled ESA6DOF propulsion module. Finally, this ends with validating thesimulation results with that of testing.</p>

corrected abstract:
<p>Space industry has been booming in the recent times, investments have not just been made on satellites and launch vehicles but also on space sustainability. Space industry has its users in national agencies, private commercial agencies, academia and experimental organisations. Smallsats and cubesats are one of the most interesting domains in space industry today. This has led to pure research and astonishing innovations in this domain to enable lower costs, lower mass, increased orbital period, better accessibility and global impact. Development of sustainable products requires the system to qualify a certain standard set up in the industry. This standard ensures that the system safely completes its mission up in space. The problem described by GomSpace Sweden concerns one of their ongoing products, a cold gas propulsion unit which is suitable for a typical 6U cubesat called ESA6DOF. In large, the product consists of a structure, two propellant tanks, one plenum tank, piping, electronics and six thrusters. Qualification of this propulsion system module involves the system to undergo a random vibration test according to ECSS standards. This thesis work shall be focused on setting up the structure needed to perform random vibration simulations in COMSOL. This step is done to primarily iterate the design to make it robust enough to sustain loads during flight and also to avoid any physical damages during testing campaign. Followed by performing the actual random vibration test at a facility using the assembled ESA6DOF propulsion module. Finally, this ends with validating the simulation results with that of testing.</p>
----------------------------------------------------------------------
In diva2:1745587 abstract is:
<p>This project’s idea revolved around utilizing the most recent techniques in MachineLearning, Neural Networks, and Data processing to construct a model to be used asa tool to determine stability during core design work. This goal will be achieved bycollecting distribution profiles describing the core state from different steady statesin five burn-up cycles in a reactor to serve as the dataset for training the model. Anadditional cycle will be reserved as a blind testing dataset for the trained model topredict. The variables that will be the target for the predictions are the decay ratioand the frequency since they describe the core stability.The distribution profiles extracted from the core simulator POLCA7 were subjectedto many different Data processing techniques to isolate the most relevant variablesto stability. The processed input variables were merged with the decay ratio andfrequency for those cases, as calculated with POLCA-T. Two different MachineLearning models, one for each output parameter, were designed with Pytorch toanalyze those labeled datasets. The goal of the project was to predict the outputvariables with an error lower than 0.1 for decay ratio and 0.05 for frequency. Themodels were able to predict the testing data with an RMSE of 0.0767 for decay ratioand 0.0354 for frequency.Finally, the trained models were saved and tasked with predicting the outputparameters for a completely unknown cycle. The RMSE was even better forthe unknown cycle, with 0.0615 for decay ratio and 0.0257 for frequency,respectively.</p>

corrected abstract:
<p>This project’s idea revolved around utilizing the most recent techniques in Machine Learning, Neural Networks, and Data processing to construct a model to be used as a tool to determine stability during core design work. This goal will be achieved by collecting distribution profiles describing the core state from different steady states in five burn-up cycles in a reactor to serve as the dataset for training the model. An additional cycle will be reserved as a blind testing dataset for the trained model to predict. The variables that will be the target for the predictions are the decay ratio and the frequency since they describe the core stability.</p><p>The distribution profiles extracted from the core simulator POLCA7 were subjected to many different Data processing techniques to isolate the most relevant variables to stability. The processed input variables were merged with the decay ratio and frequency for those cases, as calculated with POLCA-T. Two different Machine Learning models, one for each output parameter, were designed with Pytorch to analyze those labeled datasets. The goal of the project was to predict the output variables with an error lower than 0.1 for decay ratio and 0.05 for frequency. The models were able to predict the testing data with an RMSE of 0.0767 for decay ratio and 0.0354 for frequency.</p><p>Finally, the trained models were saved and tasked with predicting the output parameters for a completely unknown cycle. The RMSE was even better for the unknown cycle, with 0.0615 for decay ratio and 0.0257 for frequency, respectively.</p>
----------------------------------------------------------------------
In diva2:1679005 abstract is:
<p>As an alternative solution to global warming, this thesis explores the possibility of aspace-based geoengineering scheme that may prove worthwhile to implement in parallel toother environmental efforts that help mitigate impact of climate change. One suggestionof a geoengineering solution is deploying a large number of sunshades in the vicinity ofthe first Lagrange point of the Sun-Earth system, and this prospective sunshade projectwould serve to shield Earth from incident solar radiation. This thesis is an extension ofa feasibility study for the implementation of this large-scale mission, and has a focus oncomparing electric thrusters to solar sailing as a means of propulsion. Background onelectric propulsion systems and spaceflight mechanics is provided. The investigation wasperformed by defining the spacecraft configurations, and then computing trajectories toa point of escape from Earth and from there to the final equilibrium point.Our results show that in order to meet the propellant demands of the electric thrusters,the launch mass would need to increase by around 15-25 % compared to the solar sailingimplementation, equating to around 1010 kg. Nevertheless, electric propulsion could stillbe a beneficial choice since it would allow shorter transfer times for each shade whichreduces the radiation exposure and subsequent degradation of the spacecraft’s systems.It was found that the transfer time with electric propulsion would be about one-half orone-fifth that of solar sailing, depending on spacecraft parameters. Additionally, electricpropulsion allows a much lower initial parking orbit, and while this would increase the ra-diation exposure it would also reduce the launch costs due to the higher payload capacityto lower altitudes. However, electric propulsion of this scale require prior advancementsin xenon or other inert propellant extraction methods and possibly a wide-scale construc-tion of air separation plants.</p>


corrected abstract:
<p>As an alternative solution to global warming, this thesis explores the possibility of a space-based geoengineering scheme that may prove worthwhile to implement in parallel to other environmental efforts that help mitigate impact of climate change. One suggestion of a geoengineering solution is deploying a large number of sunshades in the vicinity of the first Lagrange point of the Sun-Earth system, and this prospective sunshade project would serve to shield Earth from incident solar radiation. This thesis is an extension of a feasibility study for the implementation of this large-scale mission, and has a focus on comparing electric thrusters to solar sailing as a means of propulsion. Background on electric propulsion systems and spaceflight mechanics is provided. The investigation was performed by defining the spacecraft configurations, and then computing trajectories to a point of escape from Earth and from there to the final equilibrium point.</p><p>Our results show that in order to meet the propellant demands of the electric thrusters, the launch mass would need to increase by around 15-25 % compared to the solar sailing implementation, equating to around 10<sup>10</sup> kg. Nevertheless, electric propulsion could still be a beneficial choice since it would allow shorter transfer times for each shade which reduces the radiation exposure and subsequent degradation of the spacecraft’s systems. It was found that the transfer time with electric propulsion would be about one-half or one-fifth that of solar sailing, depending on spacecraft parameters. Additionally, electric propulsion allows a much lower initial parking orbit, and while this would increase the radiation exposure it would also reduce the launch costs due to the higher payload capacity to lower altitudes. However, electric propulsion of this scale require prior advancements in xenon or other inert propellant extraction methods and possibly a wide-scale construction of air separation plants.</p>
----------------------------------------------------------------------
In diva2:1583422 abstract is:
<p>Highly automated driving is approaching reality at a high speed. BMW is planningto put its first autonomous driving vehicle on the road already by 2021. The path torealising this new technology is however, full of challenges. Not only the transverseand longitudinal dynamic vehicle motion play an important role in experiencedcomfort but also the requirements and expectations of the occupants regarding thevertical dynamic vibration behaviour. Especially during long trips on the motorwaywhere the so far active driver becomes the chauffeured passenger, who reads, worksor sleeps in his newly gained time. These new use-cases create new requirements forthe future design of driving comfort which are yet to be fully discovered.This work was carried out at the BMW headquarters and had the aim to usedifferent machine learning models to investigate and identify patterns between thesubjective comfort values reported by participants in a study, on a comfort scale of 1-7 and the mechanical vibrations that they experienced, measured inm/s2. The datawas collected in a previous independent study and statistical methods were used toinsure the quality of the data. A comparison of the ISO 2631-1 comfort ratings andthe study’s findings is done to understand the need for a more sophisticated model to predict comfort in autonomous driving. The work continued by investigating different dimensionality reduction methods and their influence on the performance of the models. The process used to build, optimise and validate neural networks and other models is included in the method chapter and the results are presented. The work ends with a discussion of both the prediction results and the modelsre-usability. The machine learning models investigated in this thesis have shown great po-tential for detecting complex pattern that link feelings and thoughts to mechanical variables. The models were able to predict the correct level of comfort with up to50% precision when trying to predict 6 or 7 levels of comfort. When divided into high versus low discomfort, i.e. predicting one of two comfort levels, the models were able to achieve a precision of up to 75.4%.Excluded from this thesis is the study of differences in attentive vs inattentive state when being driven in an autonomous driving vehicle. It became clear shortly before the start of this work, that the experiment that yielded the data used for it failed to find a statistically significant difference between the two states.</p><p> </p>

corrected abstract:
<p>Highly automated driving is approaching reality at a high speed. BMW is planning to put its first autonomous driving vehicle on the road already by 2021. The path to realising this new technology is however, full of challenges. Not only the transverse and longitudinal dynamic vehicle motion play an important role in experienced comfort but also the requirements and expectations of the occupants regarding the vertical dynamic vibration behaviour. Especially during long trips on the motorway where the so far active driver becomes the chauffeured passenger, who reads, works or sleeps in his newly gained time. These new use-cases create new requirements for the future design of driving comfort which are yet to be fully discovered.</p><p>This work was carried out at the BMW headquarters and had the aim to use different machine learning models to investigate and identify patterns between the subjective comfort values reported by participants in a study, on a comfort scale of 1-7 and the mechanical vibrations that they experienced, measured in <em>m/s<sup>2</sup></em>. The data was collected in a previous independent study and statistical methods were used to insure the quality of the data. A comparison of the ISO 2631-1 comfort ratings and the study’s findings is done to understand the need for a more sophisticated model to predict comfort in autonomous driving. The work continued by investigating different dimensionality reduction methods and their influence on the performance of the models. The process used to build, optimise and validate neural networks and other models is included in the method chapter and the results are presented. The work ends with a discussion of both the prediction results and the models re-usability.</p><p>The machine learning models investigated in this thesis have shown great potential for detecting complex pattern that link feelings and thoughts to mechanical variables. The models were able to predict the correct level of comfort with up to 50% precision when trying to predict 6 or 7 levels of comfort. When divided into high versus low discomfort, i.e. predicting one of two comfort levels, the models were able to achieve a precision of up to 75.4%.</p><p>Excluded from this thesis is the study of differences in attentive vs inattentive state when being driven in a autonomous driving vehicle. It became clear shortly before the start of this work, that the experiment that yielded the data used for it failed to find a statistically significant difference between the two states.</p>
----------------------------------------------------------------------
In diva2:1579558 - Note: no full text in DiVA
abstract is:
<p>Ion channels are essential for numerous functions in the human body e.g.they control electrical signaling, absorb salts and regulate the vital osmoticgradient. They are embedded in cell membranes and the interaction thatoccurs between molecules of the membranes and the channel proteins playsa role in the regulation of the protein structure and function. KcsA is aprototypical K+-channel whose inactivation is suppressed when the bindingto anionic lipids is inhibited. However, the biophysical phenomenon that liesbehind this effect on channel inactivation is not yet discovered. Here, wehave trained several random forest classifiers trained with MD simulation datato distinguish between datasets with lipids bound to the protein and datasetwithout lipids bound. When KcsA is in the Open state, there is a significantlipid impact on the intracellular end of the TM2 helix. We show that thereis a potential favouring of the Fully Open state when the KcsA channel isbound to the anionic DOPG lipid. This was seen in all random forest modelset ups. Our results suggest that a potential acceleration of the opening of theKcsA activation gate, which allosterically controls the selectivity filter, mightexplain why the binding to anionic lipids is necessary for efficient inactivation.We believe that using machine learning together with further biophysical,biochemical analysis of MD simulations is a promising method for findingnew interaction patterns between proteins and other molecules or stimuli.</p>


corrected abstract:
<p>Ion channels are essential for numerous functions in the human body e.g. they control electrical signaling, absorb salts and regulate the vital osmotic gradient. They are embedded in cell membranes and the interaction that occurs between molecules of the membranes and the channel proteins plays a role in the regulation of the protein structure and function. KcsA is a prototypical K+-channel whose inactivation is suppressed when the binding to anionic lipids is inhibited. However, the biophysical phenomenon that lies behind this effect on channel inactivation is not yet discovered. Here, we have trained several random forest classifiers trained with MD simulation data to distinguish between datasets with lipids bound to the protein and dataset without lipids bound. When KcsA is in the Open state, there is a significant lipid impact on the intracellular end of the TM2 helix. We show that there is a potential favouring of the Fully Open state when the KcsA channel is bound to the anionic DOPG lipid. This was seen in all random forest model set ups. Our results suggest that a potential acceleration of the opening of the KcsA activation gate, which allosterically controls the selectivity filter, might explain why the binding to anionic lipids is necessary for efficient inactivation. We believe that using machine learning together with further biophysical, biochemical analysis of MD simulations is a promising method for finding new interaction patterns between proteins and other molecules or stimuli.</p>
----------------------------------------------------------------------
In diva2:1334283 abstract is:
<p>The development of chisels is currently mainly based on experiments and empirical researcheswithin the company. Recently it was decided to develop a simulation tool aiming at predictingthe performances and the capabilities of its chisels. The simulation of concrete during thechiselling process is based on a Cohesive Particle Model for concrete developed in partnershipwith external universities and implemented in the open-source software YADE using the DiscreteElement Method.The goal of this thesis is first to continue the development of the simulation tool by improvingthe calibration method of the material parameters in order to better describe the concretebehaviour under static loading. Afterward a validation phase, aiming at evaluating the real capabilitiesof the simulation tool to predict the demolition process, is performed by comparingsimulations and experiments results. The last objective is to define a simulation method in orderto evaluate the clamping phenomenon during chiselling in an acceptable amount of time.The new calibration algorithm has produced significant improvements in the determinationof the material parameters. Moreover, it was discovered that the ratio between the ultimatetensile and compressive strengths as well as the concrete brittleness are key parameters forthe material calibration accuracy. On the other hand, the validation phase was performed byevaluating the influence of seven parameters, such as the impact energy, the chisel length, onthe demolition process for both experiments and simulations. The procedure and the key resultsare presented in the thesis report. Concerning the clamping phenomenon, it was discovered thatthere is a relation between the pull-out force and the contact force during chiselling. This resultoffers a new and quick possibility for the evaluation of chisels sticking behaviour.</p>

corrected abstract:
<p>The development of chisels is currently mainly based on experiments and empirical researches within the company. Recently it was decided to develop a simulation tool aiming at predicting the performances and the capabilities of its chisels. The simulation of concrete during the chiselling process is based on a Cohesive Particle Model for concrete developed in partnership with external universities and implemented in the open-source software YADE using the Discrete Element Method. The goal of this thesis is first to continue the development of the simulation tool by improving the calibration method of the material parameters in order to better describe the concrete behaviour under static loading. Afterward a validation phase, aiming at evaluating the real capabilities of the simulation tool to predict the demolition process, is performed by comparing simulations and experiments results. The last objective is to define a simulation method in orderto evaluate the clamping phenomenon during chiselling in an acceptable amount of time. The new calibration algorithm has produced significant improvements in the determination of the material parameters. Moreover, it was discovered that the ratio between the ultimate tensile and compressive strengths as well as the concrete brittleness are key parameters for the material calibration accuracy. On the other hand, the validation phase was performed by evaluating the influence of seven parameters, such as the impact energy, the chisel length, onthe demolition process for both experiments and simulations. The procedure and the key results are presented in the thesis report. Concerning the clamping phenomenon, it was discovered that there is a relation between the pull-out force and the contact force during chiselling. This result offers a new and quick possibility for the evaluation of chisels sticking behaviour.</p>
----------------------------------------------------------------------
In diva2:1321182 abstract is:
<p>A method to evaluate technical solutions to handle external loads on subsea wellheadshas been developed. The solutions, or concepts, are compared with respect to load relief,cost and operation. As a basis, a Pugh matrix was used. It is well proven and commonlyused amongst engineers to evaluate concepts. However, it has some major cons due toits simplicity.Two more layers were added trying to solve or minimize these cons. This made up a totalof three layers.I. Evaluation - gather concept data, answer questions with valuesII. Transformation - transforms gathered values to a [1-5] scalingIII. Comparison - scaled values are presented in a Pugh matrixIn layer I, questions are to be answered by analyses and expert knowledge, carried outby developers.As for layer II, a value-scaling relationship should be set by developers and decisionmakers. They decide what is a good difference compared with a reference, and what isnot. The values for layer I can then be translated to layer III.Lastly, in layer III the performances of concepts with respect to different criteria are statedin a Pugh matrix. A scaling [1-5] is used for this. The decision maker decides what criteriaare most important by weighting them.Besides that, everything could be made automated. So when the method was carried outon two concepts, a winner could be decided immediately in layer III when the questionsin layer I had been answered.A simple and straightforward method to compare concepts have been done. Visualizingthe concept evaluation process and the connection between developers and decisionmakers. Making it easier for them to understand one another.The method can continuously be improved over time and might have the potential to makethe development process in many companies leaner.</p>

corrected abstract:
<p>A method to evaluate technical solutions to handle external loads on subsea wellheads has been developed. The solutions, or concepts, are compared with respect to load relief, cost and operation. As a basis, a Pugh matrix was used. It is well proven and commonly used amongst engineers to evaluate concepts. However, it has some major cons due to its simplicity.</p><p>Two more layers were added trying to solve or minimize these cons. This made up a total of three layers.</p><ol type="I"><li>Evaluation - gather concept data, answer questions with values</li><li>Transformation - transforms gathered values to a [1-5] scaling</li><li>Comparison - scaled values are presented in a Pugh matrix</li></ol><p>In layer I, questions are to be answered by analyses and expert knowledge, carried out by developers.</p><p>As for layer II, a value-scaling relationship should be set by developers and decision makers. They decide what is a good difference compared with a reference, and what is not. The values for layer I can then be translated to layer III. Lastly, in layer III the performances of concepts with respect to different criteria are stated in a Pugh matrix. A scaling [1-5] is used for this. The decision maker decides what criteria are most important by weighting them.</p><p>Besides that, everything could be made automated. So when the method was carried out on two concepts, a winner could be decided immediately in layer III when the questions in layer I had been answered.</p><p>A simple and straightforward method to compare concepts have been done. Visualizing the concept evaluation process and the connection between developers and decision makers. Making it easier for them to understand one another.</p><p>The method can continuously be improved over time and might have the potential to make the development process in many companies leaner.</p>
----------------------------------------------------------------------
In diva2:1142912 abstract is:
<p>The purpose of this report is to examine the combinationof an Extreme Learning Machine (ELM) with the KernelMethod. Kernels lies at the core of Support Vector Machines successin classifying non-linearly separable datasets. The hypothesisis that by combining ELM with a kernel we will utilize featuresin the ELM-space otherwise unused. The report is intended asa proof of concept for the idea of using kernel methods in anELM setting. This will be done by running the new algorithmagainst five image datasets for a classification accuracy and timecomplexity analysis.Results show that our extended ELM algorithm, which we havenamed Extreme Kernel Machine (EKM), improve classificationaccuracy for some datasets compared to the regularised ELM,in the best scenarios around three percentage points. We foundthat the choice of kernel type and parameter values had greateffect on the classification performance. The implementation ofthe kernel does however add computational complexity, but wherethat is not a concern EKM does have an advantage. This tradeoffmight give EKM a place between other neural networks andregular ELMs.</p>

corrected abstract:
<p>The purpose of this report is to examine the combination of an <em>Extreme Learning Machine</em> (ELM) with the <em>Kernel Method</em>. Kernels lies at the core of <em>Support Vector Machines</em> success in classifying non-linearly separable datasets. The hypothesis is that by combining ELM with a kernel we will utilize features in the ELM-space otherwise unused. The report is intended as a proof of concept for the idea of using kernel methods in an ELM setting. This will be done by running the new algorithm against five image datasets for a classification accuracy and time complexity analysis.</p><p>Results show that our extended ELM algorithm, which we have named <em>Extreme Kernel Machine</e> (EKM), improve classification accuracy for some datasets compared to the regularised ELM, in the best scenarios around three percentage points. We found that the choice of kernel type and parameter values had great effect on the classification performance. The implementation of the kernel does however add computational complexity, but where that is not a concern EKM does have an advantage. This tradeoff might give EKM a place between other neural networks and regular ELMs.</p>
----------------------------------------------------------------------
In diva2:1139499 
Note: There was soe text missing in the DiVA abstract.

abstract is:
<p>The entropy noise in modern engines is mainly originating from two types of mechanisms.First, chemical reactions in the combustion chamber lead to unsteady heat releasewhich is responsible of the direct combustion noise. Second, hot and cold blobsof air coming from the combustion chamber are advected and accelerated throughturbine stages, giving rise to the so-called entropy noise (or indirect combustionnoise). In the present work, numerical characterization of indirect combustion noiseof a Nozzle Guide Vane passage was assessed using three-dimensional Large EddySimulations. The study was conducted on a simplified topology of a real turbinestator passage, for which experimental data were available in transonic operatingconditions. First, a baseline case was reproduced to validate a numerical finite volumesolver against the experimental measurements. Then, the same solver is used toreproduce the effects of incoming entropy waves from the combustion chamber andto characterize the additional generated acoustic power. Periodic temperature fluctuationsare imposed at the inlet, permitting to simulate hot and cold packets of aircoming from the unsteady combustion. The incoming waves are characterized bytheir characteristic wavelength; therefore, a parametric study has been conductedvarying the inlet temperature of the passage, generating entropy waves of greaterwavelengths. The study proves that the generated indirect combustion noise canbe significant. Moreover, the generated indirect combustion noise increases as thewavelength of the incoming disturbances increases. Finally, the present work suggeststhat, in transonic conditions, there might be flow features which enhance theindirect combustion noise generation mechanism.</p>

corrected abstract:
<p>The entropy noise in modern engines is mainly originating from two types of mechanisms. First, chemical reactions in the combustion chamber lead to unsteady heat release which is responsible of the direct combustion noise. Second, hot and cold blobs of air coming from the combustion chamber are advected and accelerated through turbine stages, giving rise to the so-called entropy noise (or indirect combustion noise). In the present work, numerical characterization of indirect combustion noise of a Nozzle Guide Vane passage was assessed using three-dimensional Large Eddy Simulations. The study was conducted on a simplified topology of a real turbine stator passage, for which experimental data were available in transonic operating conditions. First, a baseline case was reproduced to validate a numerical finite volume solver against the experimental measurements. Then, the same solver is used to reproduce the effects of incoming entropy waves from the combustion chamber and to characterize the additional generated acoustic power. Periodic temperature fluctuations are imposed at the inlet, permitting to simulate hot and cold packets of air coming from the unsteady combustion. The incoming waves are characterized by their characteristic wavelength; therefore, a parametric study has been conducted varying the inlet temperature of the passage, generating entropy waves of greater wavelengths. The study proves that the generated indirect combustion noise can be significant for transonic operating conditions. Moreover, the generated indirect combustion noise increases as the wavelength of the incoming disturbances increases. Finally, the present work suggests that, in transonic conditions, there might be flow features which enhance the indirect combustion noise generation mechanism.</p>
----------------------------------------------------------------------
In diva2:623694 abstract is:
<p>Companiesissuing insurance cover, in return for insurance premiums, face the payments ofclaims occurring according to a loss distribution. Hence, capital must be heldby the companies so that they can guarantee the fulfilment of the claims ofeach line of insurance. The increased incidence of insurance insolvencymotivates the birth of new legislations as the European Solvency II Directive.Companies have to determine the required amount of capital and the optimalcapital allocation across the different lines of insurance in order to keep therisk of insolvency at an adequate level. The capital allocation problem may betreated in different ways, starting from the insurance company balance sheet.Here, the running process and efficiency of four methods are evaluated andcompared so as to point out the characteristics of each of the methods. TheValue-at-Risk technique is straightforward and can be easily generated for anyloss distribution. The insolvency put option principle is easily implementableand is sensitive to the degree of default. The capital asset pricing model isone of the oldest reliable methods and still provides very helpful intermediateresults. The Myers and Read marginal capital allocation approach encouragesdiversification and introduces the concept of default value. Applications ofthe four methods to some fictive and real insurance companies are provided. Thethesis further analyses the sensitivity of those methods to changes in the economiccontext and comments how insurance companies can anticipate those changes.</p>


corrected abstract:
<p>Companies issuing insurance cover, in return for insurance premiums, face the payments of claims occurring according to a loss distribution. Hence, capital must be held by the companies so that they can guarantee the fulfilment of the claims of each line of insurance. The increased incidence of insurance insolvency motivates the birth of new legislations as the European Solvency II Directive. Companies have to determine the required amount of capital and the optimal capital allocation across the different lines of insurance in order to keep the risk of insolvency at an adequate level. The capital allocation problem may be treated in different ways, starting from the insurance company balance sheet. Here, the running process and efficiency of four methods are evaluated and compared so as to point out the characteristics of each of the methods. The Value-at-Risk technique is straightforward and can be easily generated for any loss distribution. The insolvency put option principle is easily implementable and is sensitive to the degree of default. The capital asset pricing model is one of the oldest reliable methods and still provides very helpful intermediate results. The Myers and Read marginal capital allocation approach encourages diversification and introduces the concept of default value. Applications of the four methods to some fictive and real insurance companies are provided. The thesis further analyses the sensitivity of those methods to changes in the economic context and comments how insurance companies can anticipate those changes.</p>
----------------------------------------------------------------------
In diva2:618335 - missing spaces in title:
"Structural strength of work boatsand high speed crafts with floatingframes"
==>
"Structural strength of work boats and high speed crafts with floating frames"

abstract is:
<p>This thesis investigates the usage of floating frames in boats. A floating frame is a transverse frame fittedto the longitudinal stiffener flanges without contact with the shell plating, as opposed to the traditionalfixed frame which is welded to the shell plating with the stiffeners most commonly fitted through cut outsin the frame.To study the floating frame structure in a bigger perspective a finite element analysis is performed on amid ship compartment of an existing 60 m high speed catamaran ferry. The analysis is performed on amodel with scantlings as the original craft but with introduced floating frames. Stresses are analysed withrespect to maximum allowable stress as given in the DNV-rules for HAZ.High stresses are found in the bottom of the frames due to the reduced bending stiffness without effectiveflange from the shell plating. A large deformation in the shell plating relative the transverse frames isfound, creating high stresses in the stiffener webs. This deformation is induced by a large verticaldeformation of the frames.It is concluded that the transverse frames requires an increased stiffness to achieve acceptable stress levels.Possible solutions to increase stiffness are discussed, further studies are required to achieve an acceptablestructure.A design criterion for stiffeners in floating frame constructions is evaluated. The criterion considers theinteraction between a concentrated contact force and a bending moment with the purpose of simplifyingthe design process of stiffeners. The criterion is a combination of design methods from DNV HSLC andEurocode 9.The design criterion is found to give conservative results, although not unreasonably conservative. Thecriterion is suitable for the design of smaller work boats where the scantlings traditionally are not veryoptimized.</p>

corrected abstract:
<p>This thesis investigates the usage of floating frames in boats. A floating frame is a transverse frame fitted to the longitudinal stiffener flanges without contact with the shell plating, as opposed to the traditional fixed frame which is welded to the shell plating with the stiffeners most commonly fitted through cut outs in the frame.</p><p>To study the floating frame structure in a bigger perspective a finite element analysis is performed on a mid ship compartment of an existing 60 m high speed catamaran ferry. The analysis is performed on a model with scantlings as the original craft but with introduced floating frames. Stresses are analysed with respect to maximum allowable stress as given in the DNV-rules for HAZ.</p><p>High stresses are found in the bottom of the frames due to the reduced bending stiffness without effective flange from the shell plating. A large deformation in the shell plating relative the transverse frames is found, creating high stresses in the stiffener webs. This deformation is induced by a large vertical deformation of the frames.</p><p>It is concluded that the transverse frames requires an increased stiffness to achieve acceptable stress levels. Possible solutions to increase stiffness are discussed, further studies are required to achieve an acceptable structure.</p><p>A design criterion for stiffeners in floating frame constructions is evaluated. The criterion considers the interaction between a concentrated contact force and a bending moment with the purpose of simplifying the design process of stiffeners. The criterion is a combination of design methods from DNV HSLC and Eurocode 9.</p><p>The design criterion is found to give conservative results, although not unreasonably conservative. The criterion is suitable for the design of smaller work boats where the scantlings traditionally are not very optimized.</p>
----------------------------------------------------------------------
In diva2:618217 - title is missing spaces:
"Driver Assistance Systemswith focus onAutomatic Emergency Brake"
==>
"Driver Assistance Systems with focus on Automatic Emergency Brake"

abstract is:
<p>This thesis work aims at performing a survey of those technologies generally called DriverAssistance Systems (DAS). This thesis work focuses on gathering information in terms ofaccident statistics, sensors and functions and analyzing this information and shall thruaccessible information match functions with accidents, functions with sensors etc.This analysis, based on accidents in United States and Sweden during the period 1998 – 2002and two truck accident studies, shows that of all accidents with fatalities or sever injuriesinvolving a heavy truck almost half are the result of a frontal impact. About one fourth of theaccidents are caused by side impact, whereas single vehicle and rear impact collisions causesaround 14 % each. Of these, about one fourth is collision with unprotected (motorcycles,mopeds, bicycles, and pedestrians) whereas around 60 % are collision with other vehicles.More than 90 % of all accidents are partly the result of driver error and about 75 % aredirectly the result of driver error. Hence there exist a great opportunity to reduce the numberof accidents by introducing DAS.In this work, an analysis of DAS shows that six of the systems discussed today have thepotential to prevent 40 – 50 % of these accidents, whereas 20 – 40 % are estimated to actuallyhaving the chance to be prevented.One of these DAS, automatic emergency brake (AEB), has been analyzed in more detail.Decision models for an emergency brake capable to mitigate rear-end accidents has beendesigned and evaluated. The results show that this model has high capabilities to mitigatecollisions.</p>

corrected abstract:
<p>This thesis work aims at performing a survey of those technologies generally called <em>Driver Assistance Systems (DAS)</em>. This thesis work focuses on gathering information in terms of accident statistics, sensors and functions and analyzing this information and shall thru accessible information match functions with accidents, functions with sensors etc.</p><p>This analysis, based on accidents in United States and Sweden during the period 1998 – 2002 and two truck accident studies, shows that of all accidents with fatalities or sever injuries involving a heavy truck almost half are the result of a frontal impact. About one fourth of the accidents are caused by side impact, whereas single vehicle and rear impact collisions causes around 14 % each. Of these, about one fourth is collision with unprotected (motorcycles, mopeds, bicycles, and pedestrians) whereas around 60 % are collision with other vehicles.</p><p>More than 90 % of all accidents are partly the result of driver error and about 75 % are directly the result of driver error. Hence there exist a great opportunity to reduce the number of accidents by introducing <em>DAS</em>.</p><p>In this work, an analysis of <em>DAS</em> shows that six of the systems discussed today have the potential to prevent 40 – 50 % of these accidents, whereas 20 – 40 % are estimated to actually having the chance to be prevented.</p><p>One of these <em>DAS</em>, <em>automatic emergency brake (AEB)</em>, has been analyzed in more detail. Decision models for an emergency brake capable to mitigate rear-end accidents has been designed and evaluated. The results show that this model has high capabilities to mitigate collisions.</p>
----------------------------------------------------------------------
In diva2:503940 abstract is:
<p>The wheel corner module represents a new technology for controlling the motion of avehicle. It is based on a modular design around the geometric boundaries of a conventionalwheel. The typical WCM consists of a wheel containing an electrical in-wheel propulsion motor, a friction brake, a steering system and a suspension system. Generally, the braking,steering and suspension systems are controlled by means of electrical actuators. The WCMis designed to easily, by means of bolted connections and a power connector, attach toa vehicle platform constructed for the specic purpose. All functions are controlled viaan electrical system, connecting the steering column to the module. A WCM vehicle cancontain two or four wheel corner modules.The purpose of this thesis is to serve as an introduction to wheel corner module technology.The technology itself, as well as advantages and disadvantages related to wheelcorner modules are discussed. An analysis of a variety of wheel corner module concepts iscarried out. In addition, simulations are conducted in order to estimate how an increasedunsprung mass aects the ride comfort and handling performance of a vehicle.Longitudinal translation over two types of road disturbance proles, a curb and a bump,is simulated. A quarter car model as well as a full car model is utilized. The obtainedresults indicate that handling performance is deteriorated in connection to an increase dunsprung mass. The RMS value of the tire force uctuation increases with up to 18%,when 20 kg is added to each of the rear wheels of the full car model. Ride comfort is deteriorated or enhanced in connection to an increased unsprung mass, depending on the disturbance frequency of the road. When subjected to a road disturbance frequency below the eigenfrequency of the unsprung mass, ride comfort deterioration is indicated. The RMS vertical acceleration of the sprung mass increases with up to 6%, in terms of the full car model. When subjected to a road disturbance frequency above the eigenfrequency ofthe unsprung mass, decreased RMS vertical acceleration of up to 25% is noted, indicatinga signicantly enhanced ride comfort. Implementation of wheel corner module technology enables improved handling performance,safety and ride comfort compared to conventional vehicle technology. Further development, e.g. in terms of in-wheel motors and alternative power sources, is however required. In addition, major investments related to manufacturing equipment andtechnology is regarded as a signicant obstacle in terms of serial production.</p>

corrected abstract:
<p>The wheel corner module represents a new technology for controlling the motion of a vehicle. It is based on a modular design around the geometric boundaries of a conventional wheel. The typical WCM consists of a wheel containing an electrical in-wheel propulsion motor, a friction brake, a steering system and a suspension system. Generally, the braking, steering and suspension systems are controlled by means of electrical actuators. The WCM is designed to easily, by means of bolted connections and a power connector, attach to a vehicle platform constructed for the specific purpose. All functions are controlled via an electrical system, connecting the steering column to the module. A WCM vehicle can contain two or four wheel corner modules.</p><p>The purpose of this thesis is to serve as an introduction to wheel corner module technology. The technology itself, as well as advantages and disadvantages related to wheel corner modules are discussed. An analysis of a variety of wheel corner module concepts is carried out. In addition, simulations are conducted in order to estimate how an increased unsprung mass affects the ride comfort and handling performance of a vehicle.</p><p>Longitudinal translation over two types of road disturbance profiles, a curb and a bump, is simulated. A quarter car model as well as a full car model is utilized. The obtained results indicate that handling performance is deteriorated in connection to an increased unsprung mass. The RMS value of the tire force fluctuation increases with up to 18%, when 20 kg is added to each of the rear wheels of the full car model. Ride comfort is deteriorated or enhanced in connection to an increased unsprung mass, depending on the disturbance frequency of the road. When subjected to a road disturbance frequency below the eigenfrequency of the unsprung mass, ride comfort deterioration is indicated. The RMS vertical acceleration of the sprung mass increases with up to 6%, in terms of the full car model. When subjected to a road disturbance frequency above the eigenfrequency of the unsprung mass, decreased RMS vertical acceleration of up to 25% is noted, indicating a significantly enhanced ride comfort.</p><p>Implementation of wheel corner module technology enables improved handling performance, safety and ride comfort compared to conventional vehicle technology. Further development, e.g. in terms of in-wheel motors and alternative power sources, is however required. In addition, major investments related to manufacturing equipment and technology is regarded as a significant obstacle in terms of serial production.</p>
----------------------------------------------------------------------
In diva2:411684 abstract is:
<p>The aim of this master thesis is to characterize the fluid forces applied to a fuel assembly inthe core of a nuclear power plant in case of seism. The forces are studied with a simplifiedtwo-dimensional model constituted of an array of 3 by 3 infinite cylinders oscillating in aclosed box. The axial flow of water, which convects the heat in the core of a nuclear powerplant, is also taken into account. The velocity of the axial flow reaches 4m/s in the middle ofthe assembly and modifies the forces features when the cylinders move laterally.The seism is modeled as a lateral displacement with high amplitude (several cylinderdiameters) and low frequencies (below 20 Hz). In order to study the effects of the amplitudeand of the frequency of the displacement, the displacement taken is a sine function withboth controlled amplitude and frequency. Four degrees of freedom of the system will bestudied: the amplitude of the displacement, its frequency, the axial velocity amplitude andthe confinement (due to the closed box).The fluid forces exerted on the cylinders can be seen as a combination of three terms: anadded mass, related to the acceleration of cylinders, a drift force, related to the damping ofthe fluid and a force due to the interaction of the cylinder with residual vortices. The firsttwo components will be characterized through the Morison expansion, and their evolutionwith the variation of the degree of freedom of the system will be quantified. The effect ofthe interaction with the residual vortices will be observed in the plots of the forces vs. timebut also in the velocity and vorticity map of the fluid.The fluid forces are calculated with the CFD code Code_Saturne, which uses a second orderaccurate finite volume method. Unsteady Reynolds Averaged Navier Stokes simulations arerealized with a k-epsilon turbulence model. The Arbitrary Lagrange Euler model is used todescribe the structure displacement. The domain is meshed with hexahedra with thesoftware gmsh [1] and the flow is visualized with Paraview [2]. The modeling techniquesused for the simulations are described in the first part of this master thesis.</p>

corrected abstract:
<p>The aim of this master thesis is to characterize the fluid forces applied to a fuel assembly in the core of a nuclear power plant in case of seism. The forces are studied with a simplified two-dimensional model constituted of an array of 3 by 3 infinite cylinders oscillating in a closed box. The axial flow of water, which convects the heat in the core of a nuclear power plant, is also taken into account. The velocity of the axial flow reaches 4m/s in the middle of the assembly and modifies the forces features when the cylinders move laterally.</p><p>The seism is modeled as a lateral displacement with high amplitude (several cylinder diameters) and low frequencies (below 20 Hz). In order to study the effects of the amplitude and of the frequency of the displacement, the displacement taken is a sine function with both controlled amplitude and frequency. Four degrees of freedom of the system will be studied: the amplitude of the displacement, its frequency, the axial velocity amplitude and the confinement (due to the closed box).</p><p>The fluid forces exerted on the cylinders can be seen as a combination of three terms: an added mass, related to the acceleration of cylinders, a drift force, related to the damping of the fluid and a force due to the interaction of the cylinder with residual vortices. The first two components will be characterized through the Morison expansion, and their evolution with the variation of the degree of freedom of the system will be quantified. The effect of the interaction with the residual vortices will be observed in the plots of the forces vs. time but also in the velocity and vorticity map of the fluid.</p><p>The fluid forces are calculated with the CFD code Code_Saturne, which uses a second order accurate finite volume method. Unsteady Reynolds Averaged Navier Stokes simulations are realized with a k-epsilon turbulence model. The Arbitrary Lagrange Euler model is used to describe the structure displacement. The domain is meshed with hexahedra with the software gmsh [1] and the flow is visualized with Paraview [2]. The modeling techniques used for the simulations are described in the first part of this master thesis.</p>
----------------------------------------------------------------------
In diva2:1808437 - Note: The French and ENglish abstracts are at the end of the PDF file. The French abstracts is not in DiVA.


abstract is:
<p>The PTR system allows the EPR2 fuel pool to be cooled. The evacuation of the residual power fromthe pool is ensured by several heat exchangers and pumps, which have to be dimensioned in order to meetdifferent requirements.In order to dimension them, the worst-case scenario of the components must first be determined.Sensitivity to external conditions and efficiency studies enable to propose a heat exchanger design tomeet the requirements. A parametric study then allows to study more precisely the influence of thegeometry of the exchanger on the heat transfer. This allows to guide the conception of a CFD study ofthe design on the Comsol software in order to validate it. The proposed design can then be integratedinto the PTR cooling train. The train is modeled with FloMaster, in order to compute the head losses inthe hydraulic system and to propose a pump altimetry preventing cavitation.The dimensioning case of the exchangers corresponds to the operating case of the PTR trains duringunit shutdown, while the scenario that facilitates cavitation corresponds to the boiling of the fuel pool.The temperature of the cold source RRI is a sensitive data for the operation of the exchangers. In addition,the placement of the baffles and the space between the tubes play a determining role in the heat removal.It was difficult to construct the desired exchanger geometry in CFD. A compromise model was thusidentified and studied in CFD. The FloMaster study showed that the pressure drop in the PTR network isabout 15.5 mCE at the considered flow rate. Cavitation in a main train is not a problem if the pumps arelowered by at least 1.8 meters from the pool suction point.The sizing study therefore allowed us to propose a heat exchanger design close to the specifications,but this could not be precisely studied in CFD. The pressure drop study allowed to propose a pumpaltimetry preventing cavitation.</p>

corrected abstract:
<p>The PTR system allows the EPR2 fuel pool to be cooled. The evacuation of the residual power from the pool is ensured by several heat exchangers and pumps, which have to be dimensioned in order to meet different requirements.</p><p>In order to dimension them, the worst-case scenario of the components must first be determined. Sensitivity to external conditions and efficiency studies enable to propose a heat exchanger design to meet the requirements. A parametric study then allows to study more precisely the influence of the geometry of the exchanger on the heat transfer. This allows to guide the conception of a CFD study of the design on the Comsol software in order to validate it. The proposed design can then be integrated into the PTR cooling train. The train is modeled with FloMaster, in order to compute the head losses in the hydraulic system and to propose a pump altimetry preventing cavitation.</p><p>The dimensioning case of the exchangers corresponds to the operating case of the PTR trains during unit shutdown, while the scenario that facilitates cavitation corresponds to the boiling of the fuel pool. The temperature of the cold source RRI is a sensitive data for the operation of the exchangers. In addition, the placement of the baffles and the space between the tubes play a determining role in the heat removal. It was difficult to construct the desired exchanger geometry in CFD. A compromise model was thus identified and studied in CFD. The FloMaster study showed that the pressure drop in the PTR network is about 15.5 mCE at the considered flow rate. Cavitation in a main train is not a problem if the pumps are lowered by at least 1.8 meters from the pool suction point.</p><p>The sizing study therefore allowed us to propose a heat exchanger design close to the specifications, but this could not be precisely studied in CFD. The pressure drop study allowed to propose a pump altimetry preventing cavitation.</p>

French abstract:

<p>Le système PTR permet le refroidissement de la piscine combustible de l’EPR2. L’évacuation de la puissance résiduelle de la piscine est assurée par plusieurs échangeurs de chaleur et pompes, qui doivent être dimensionnés afin de respecter plusieurs critères.</p><p>Pour les dimensionner, les conditions de fonctionnement les plus défavorable des composants doivent d’abord être déterminées. Des études de sensibilité au conditions extérieures, ainsi que d’efficacité permettent de proposer un type d’échangeur pour répondre aux exigences. Une étude paramétrique permet ensuite d’étudier plus précisément l’influence de la géométrie de l’échangeur sur le transfert de chaleur. Celle-ci permet d’orienter la conception d’une étude CFD du design sur le logiciel Comsol permettant de le valider. Le design proposé peut être intégré au train de refroidissement PTR. Le train est modélisé sous FloMaster, afin de calculer les pertes de charges du réseau et proposer une altimétrie de pompe prévenant la cavitation.</p><p>Le cas dimensionnant des échangeurs correspond au cas de fonctionnement des trains PTR en arrêt de tranche, tandis que celui favorisant la cavitation correspond au scenario d’ébullition de la piscine combustible. La température de la source froide RRI est une donnée sensible pour le fonctionnement des échangeurs. De plus, le positionnement de chicanes et l’espace entre les tubes jouent un rôle déterminant dans l’évacuation de la chaleur. Il a été difficile de construire la géométrie d’échangeur souhaitée en CFD. Un modèle compromis a été trouvé et étudié en CFD. L’étude <em>FloMaster</em> a montré que les pertes de charge dans le réseau PTR sont de l’ordre de 15.5 mCE au débit considéré. La cavitation dans un train principal n’est pas un problème en abaissant de 2 mètres minimum les pompes par rapport au point d’aspiration piscine.</p><p>L’étude de dimensionnement a donc permis de proposer un design d’échangeur de chaleur se rapprochant du cahier des charges, mais celui-ci n’a pas pu être précisément étudié en CFD. L’étude de perte de charge a permis de proposer une altimétrie de pompe prévenant la cavitation.</p>
----------------------------------------------------------------------
In diva2:1795177 abstract is:
<p>Since the global financial crisis of 2008, regulatory bodies worldwide have implementedincreasingly stringent requirements for measuring and pricing default risk in financialderivatives. Counterparty Credit Risk (CCR) serves as the measure for default risk infinancial derivatives, and Credit Valuation Adjustment (CVA) is the pricing method used toincorporate this default risk into derivatives prices. To calculate the CVA, one needs the risk-neutral Probability of Default (PD) for the counterparty, which is the centre in this type ofderivative.The traditional method for calculating risk-neutral probabilities of default involves constructingcredit curves, calibrated using the credit derivative Credit Default Swap (CDS). However,liquidity issues in CDS trading present a major challenge, as the majority of counterpartieslack liquid CDS spreads. This poses the difficult question of how to model risk-neutral PDwithout liquid CDS spreads.The current method for generating proxy credit curves, introduced by the Japanese BankNomura in 2013, involves a cross-sectional linear regression model. Although this model issufficient in most cases, it often generates credit curves unsuitable for larger counterpartiesin more volatile times. In this thesis, we introduce two Long Short-Term Memory (LSTM)models trained on similar entities, which use CDS spreads as input. Our introduced modelsshow some improvement in generating proxy credit curves compared to the Nomura model,especially during times of higher volatility. While the result were more in line with the tradedCDS-market, there remains room for improvement in the model structure by using a moreextensive dataset.</p>

corrected abstract:
<p>Since the global financial crisis of 2008, regulatory bodies worldwide have implemented increasingly stringent requirements for measuring and pricing default risk in financial derivatives. Counterparty Credit Risk (CCR) serves as the measure for default risk in financial derivatives, and Credit Valuation Adjustment (CVA) is the pricing method used to incorporate this default risk into derivatives prices. To calculate the CVA, one needs the risk-neutral Probability of Default (PD) for the counterparty, which is the centre in this type of derivative.</p><p>The traditional method for calculating risk-neutral probabilities of default involves constructing credit curves, calibrated using the credit derivative Credit Default Swap (CDS). However, liquidity issues in CDS trading present a major challenge, as the majority of counterparties lack liquid CDS spreads. This poses the difficult question of how to model risk-neutral PD without liquid CDS spreads.</p><p>The current method for generating proxy credit curves, introduced by the Japanese bank Nomura in 2013, involves a cross-sectional linear regression model. Although this model is sufficient in most cases, it often generates credit curves unsuitable for larger counterparties in more volatile times.</p><p>In this thesis, we introduce two Long Short-Term Memory (LSTM) models trained on similar entities, which use CDS spreads as input. Our introduced models show some improvement in generating proxy credit curves compared to the Nomura model, especially during times of higher volatility. While the result were more in line with the traded CDS-market, there remains room for improvement in the model structure by using a more extensive dataset.</p>
----------------------------------------------------------------------
In diva2:1781520 - missing space in title:
"Extraction and optimization for modeling ofdesalination by capacitive deionization"
==>
"Extraction and optimization for modeling of desalination by capacitive deionization"

Note there were many wording differences between the abstract in the thesis and the abstract in DiVA. It seems that someone tried to clean up the grammer.

abstract is:
<p>Water scarcity is set to become a big challenge in the 21st century and more efficient desalinationtechnologies will be needed in the future. In this project, one desalination method called capacitivedeionization (CDI) is explored and we used a model called the ELC model to simulate CDI withComsol. The goal of this project focuses on evaluating the performance of CDI and how changingdifferent operational parameters of the process affects other aspects of desalination. Some examplesare power consumption, desalination rate and water usage. With the gathered information, the process of CDI can be optimized in some way. Even though our project simulates a specific model ofCDI, the hope is to have come to general conclusions regarding CDI so that the results can be usedfor other models. If the correlations between parameters are known, it will be easier to calibrate anysetup of CDI. The gathered data is exported, stored, processed, and plotted using Matlab functionsintegrated with Comsol. The results consist of two sets, the first for constant voltage and the secondfor constant current. Both have results on how desalination rate and energy efficiency are related toparameters such as internal voltage intervals controlling how long the desalination cycle is running,external voltage, and inflow salt concentration in the water. The key conclusions drawn are as thefollowing for constant voltage. High external voltages are effective in increasing both desalinationrate and energy efficiency but will degrade the CDI electrodes. The internal voltage span should bepretty long with high max internal voltage and the minimum internal voltage the same as the external voltage. The energy efficiency increase with lower salt concentrations in the inflow water up toa point. The best setup for the desalination rate is at quite a high maximum internal voltage withvaried low minimum internal voltage. For constant current, low current is generally efficient, whilethe maximum external voltage depends on the current. Avoid a high current with a low externalvoltage. By relating all these parameters, we get more insights into what an energy-efficient and fastadsorbing CDI setup looks like.</p>


corrected abstract:
<p>Water scarcity is set to become a significant challenge in the 21st century and more efficient desalination technologies will be needed in the future. In this project, a desalination method called capacitive deionization (CDI) is explored and we used the ELC model to simulate CDI with Comsol. The goal of this project focuses on evaluating the performance of CDI and how changing different operational parameters of the process affects other aspects of desalination. Some examples are power consumption, desalination rate and water usage. With the gathered information, the process of CDI can be optimized in some way. Even though our project simulates a specific model of CDI, the hope is to have come to general conclusions regarding CDI so that the results can be used for other models. If the correlations between parameters are known, it will be easier to calibrate any setup of CDI. The gathered data is exported, stored, processed, and plotted using Matlab functions integrated with Comsol. The results consist of two sets, the first for constant voltage and the second for constant current. Both show how desalination rate and energy efficiency are related to parameters such as internal voltage intervals controlling how big the desalination cycle is running, external voltage, and salinity of inlet water. The key conclusions drawn are as the following for constant voltage. High external voltages effectively increase both desalination rate and energy efficiency but will degrade the CDI electrodes. The internal voltage span should be as big as possible with high max internal voltage and a minimum internal voltage the same as the external voltage. The energy efficiency increase with lower salt concentrations in the inflow water up to a point. The best setup for the desalination rate is at a high maximum internal voltage with varied low minimum internal voltage. For constant current, low current is generally efficient, while the maximum external voltage depends on the current. Avoid a high current with a low external voltage. By relating all these parameters, we get more insights into what an energy-efficient and fast-adsorbing CDI configuration should be set up.</p>
----------------------------------------------------------------------
In diva2:1781504 abstract is:
<p>Computed x-ray tomography is one of the most common medical imaging modalities andas such ways of improving the images are of high relevance. Applying deep learningmethods to denoise CT images has been of particular interest in recent years. In thisstudy, rather than using traditional denoising metrics such as MSE or PSNR for evaluation, we use a radiomic approach combined with 3D printed phantoms as a "groundtruth" to compare with. Our approach of having a ground truth ensures that we withabsolute certainty can say what a scanned tumor is supposed to look like and compareour results to a true value. This performance metric is better suited for evaluation thanMSE since we want to maintain structures and edges in tumors and MSE-evaluationrewards over-smoothing.</p><p>Here we apply U-Net networks to images of 3D printed tumors. The 4 tumors and alung phantom were printed with PLA filament and 80% fill rate with a gyroidal patternto mimic soft tissue in a CT-scan while maintaining isotropicity. CT images of the 3Dprinted phantom and tumors were taken with a GE revolution DE scanner at KarolinskaUniversity Hospital. The networks were trained on the 2016 NIH-AAPM-Mayo ClinicLow Dose CT Grand Challenge dataset, mapping Low Dose CT images to Normal DoseCT images using three different loss functions, l1, vgg16, and vgg16_l1.</p><p>Evaluating the networks on RadiomicsShape features from SlicerRadiomics® we findcompetitive performance with TrueFidelityTM Deep Learning Image Reconstruction (DLIR)by GE HealthCareTM. With one of our networks (UNet_alt, vgg16_l1 loss function with32 features, and batch size 16 in training.) outperforming TrueFidelity in 63% of caseswhen evaluated by counting if a radiomic feature has a lower relative error comparedto ground truth after our own denoising for four different kind of tumors. The samenetwork outperformed FBP in 84% of cases which in combination with the majority ofour networks performing substantially better against FBP than TrueFidelity shows theviability of DLIR compared to older methods such as FBP.</p><p> </p>

corrected abstract:
<p>Computed x-ray tomography is one of the most common medical imaging modalities and as such ways of improving the images are of high relevance. Applying deep learning methods to denoise CT images has been of particular interest in recent years. In this study, rather than using traditional denoising metrics such as MSE or PSNR for evaluation, we use a radiomic approach combined with 3D printed phantoms as a "ground truth" to compare with. Our approach of having a ground truth ensures that we with absolute certainty can say what a scanned tumor is supposed to look like and compare our results to a true value. This performance metric is better suited for evaluation than MSE since we want to maintain structures and edges in tumors and MSE-evaluation rewards over-smoothing.</p><p>Here we apply U-Net networks to images of 3D printed tumors. The 4 tumors and a lung phantom were printed with PLA filament and 80% fill rate with a gyroidal pattern to mimic soft tissue in a CT-scan while maintaining isotropicity. CT images of the 3D printed phantom and tumors were taken with a GE revolution DE scanner at Karolinska University Hospital. The networks were trained on the 2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge dataset, mapping Low Dose CT images to Normal Dose CT images using three different loss functions, l1, vgg16, and vgg16_l1.</p><p>Evaluating the networks on RadiomicsShape features from SlicerRadiomics® we find competitive performance with TrueFidelity™ Deep Learning Image Reconstruction (DLIR) by GE HealthCare™. With one of our networks (UNet_alt, vgg16_l1 loss function with 32 features, and batch size 16 in training.) outperforming TrueFidelity in 63% of cases when evaluated by counting if a radiomic feature has a lower relative error compared to ground truth after our own denoising for four different kind of tumors. The same network outperformed FBP in 84% of cases which in combination with the majority of our networks performing substantially better against FBP than TrueFidelity shows the viability of DLIR compared to older methods such as FBP.</p>
----------------------------------------------------------------------
In diva2:1698135 abstract is:
<p>Wear condition modelling is a research topic which lately has increased in popularitydue to its significance to the railway industry. Large sums are spent on maintenanceof the network every year, which has generated a demand for understanding and predictingwear mechanisms in a multitude of components within both railway vehiclesand infrastructure. To acquire this knowledge, using a simulation software to predictwear and maintenance needs, is a cheap and dependable option. This thesis sets outto create a wear modelling software for the catenary-pantograph interaction, which intheory could enlighten railway operators of the condition of their infrastructure.</p><p>This research contains an extensive literature review on the subject of wear modelling,analysing different strategies and theories of the wear mechanism and desired modelproperties. Thereafter, an attempt to recreate a model for a representative Swedishcase was conducted; implementing parameters of a typical Swedish rail vehicle to awear rate formula, recreating a passage of a pantograph sliding along the catenary.The output was a wear rate as a function of time, a determination of life expectancyfor the infrastructure and a worn profile of the catenary wire after one year of operation.</p><p>The model, named AWEAR, was deemed functional for determining wear mechanismtendencies and behaviour for a few cases of alternating parameters. However, sinceno validation could be performed due to a lack of resources, the validity of the outputvalues could not be confirmed - thus leaving calibration of the model to future work.In conclusion, AWEAR needs to be calibrated for future research but does contain amultitude of enhancement opportunities proposed in this thesis. While not completelyfunctional, the software was deemed a useful foundation for future projects and mightresult in a product that aids operators maintaining their infrastructure.Keywords: Wear modelling, catenary pantograph interaction, mechanism,</p>

corrected abstract:
<p>Wear condition modelling is a research topic which lately has increased in popularity due to its significance to the railway industry. Large sums are spent on maintenance of the network every year, which has generated a demand for understanding and predicting wear mechanisms in a multitude of components within both railway vehicles and infrastructure. To acquire this knowledge, using a simulation software to predict wear and maintenance needs, is a cheap and dependable option. This thesis sets out to create a wear modelling software for the catenary-pantograph interaction, which in theory could enlighten railway operators of the condition of their infrastructure.</p><p>This research contains an extensive literature review on the subject of wear modelling, analysing different strategies and theories of the wear mechanism and desired model properties. Thereafter, an attempt to recreate a model for a representative Swedish case was conducted; implementing parameters of a typical Swedish rail vehicle to a wear rate formula, recreating a passage of a pantograph sliding along the catenary. The output was a wear rate as a function of time, a determination of life expectancy for the infrastructure and a worn profile of the catenary wire after one year of operation.</p><p>The model, named AWEAR, was deemed functional for determining wear mechanism tendencies and behaviour for a few cases of alternating parameters. However, since no validation could be performed due to a lack of resources, the validity of the output values could not be confirmed - thus leaving calibration of the model to future work.  In conclusion, AWEAR needs to be calibrated for future research but does contain a multitude of enhancement opportunities proposed in this thesis. While not completely functional, the software was deemed a useful foundation for future projects and might result in a product that aids operators maintaining their infrastructure.</p>
----------------------------------------------------------------------
In diva2:1595652 abstract is:
<p>A comparative study is carried out to investigate the most promising route towardsthe lightweight construction of a retractable mast for a sailing cargo vessel.Four design families are developed and compared. The primary criteria forjudgment are the structural mass, strength, and stiffness in relation to a providedbenchmark design. Additional evaluation criteria are the capital costsfor raw materials and manufacturing.The design space includes isotropic materials as well as fiber-reinforced polymer(FRP) solutions and is navigated by employing analytical evaluation methodssupported by finite element analysis (FEA). Restrictions to the designspace are given by a general arrangement of the benchmark design. This includesthe limitation to the ULS loads and the overall mast geometry.A review of relevant Det Norske Veritas (DNV) rules for classification is performedand the guidelines for wind turbine blades and wind-powered units(WPU) are judged most suitable to the design challenge. Relevant design principlesare implemented in the structural analysis.It is concluded that pure metal constructions imply an unreasonably large weightpenalty. Local buckling is found to disqualify FRP single-skin solutions as successfulcandidates. Secondary to that, strength concerns are the major driversfor the structural mass.The report presents two designs that are judged fit for the purpose, one is ahybrid truss structure from high strength low alloy steel (HSLA steel) and carbonfiber-reinforced polymer (CFRP). The second design is a sandwich constructionwith CFRP face sheets, a PVC foam core, and additional stiffeningmembers in steel.</p>

corrected abstract:
<p>A comparative study is carried out to investigate the most promising route towards lightweight construction of a retractable mast for a sailing cargo vessel. Four design families are developed and compared. The primary criteria for judgement are the structural mass, strength and stiffness in relation to a provided benchmark design. Additional evaluation criteria are the capital costs for raw materials and manufacturing.</p><p>The design space includes isotropic materials as well as fiber reinforced polymer (FRP) solutions and is navigated by employing analytical evaluation methods supported by finite element analysis (FEA). Restrictions to the design space are given by a general arrangement of the benchmark design. This includes the limitation to the ULS loads and the overall mast geometry.</p><p>A review of relevant Det Norske Veritas (DNV) rules for classification is performed and the guidelines for wind turbine blades and wind powered units (WPU) are judged most suitable to the design challenge. Relevant design principles are implemented in the structural analysis.</p><p>It is concluded that pure metal constructions imply an unreasonably large weight penalty. Local buckling is found to disqualify FRP single skin solutions as successful candidates. Secondary to that, strength concerns are the major drivers for the structural mass.</p><p>The report presents two designs that are judged fit for the purpose, one is a hybrid truss structure from high strength low alloy steel (HSLA steel) and carbon fiber reinforced polymer (CFRP). The second design is a sandwich construction with CFRP face sheets, a PVC foam core and additional stiffening members in steel.</p>
----------------------------------------------------------------------
In diva2:1527800 abstract is:
<p>With the advent of hybrids and electric vehicles, the need for lightweight and highperformancematerials is growing. Sheet molding compound (SMC) is a compositemade of short and randomized  bers that o ers a substantial weight reduction andgood mechanical properties while meeting the demand for large volume production.This thesis aims to develop a constitutive FE model of the SMC used in the bodyin black of an autonomous vehicle.To extract its properties, several physical tests were performed on specimens madeof the above-mentioned material. Both the tensile and three point bending testsresults show that the material is not homogeneous and that its properties vary fordi erent directions. The damping ratio extracted from the vibration test is muchlower than in conventional structural materials like aluminum and steel.In the FE analysis, the material was modeled both as isotropic and orthotropic.After adjusting the Young's modulus, the isotropic model shows accurate resultsuntil 1200 Hz. On the other hand, without knowing in which directions the propertiesoccur, the orthotropic model is very limited.In conclusion, even though the properties were tailored speci cally for the specimen,the model might not correctly represent the material's behavior, being itsproperties not the same for di erent components. Therefore, it is more reasonableto use average data instead.</p>

corrected abstract:
<p>With the advent of hybrids and electric vehicles, the need for lightweight and high performance materials is growing. Sheet molding compound (SMC) is a composite made of short and randomized fibers that offers a substantial weight reduction and good mechanical properties while meeting the demand for large volume production. This thesis aims to develop a constitutive FE model of the SMC used in the body in black of an autonomous vehicle.</p><p>To extract its properties, several physical tests were performed on specimens made of the above-mentioned material. Both the tensile and three point bending tests results show that the material is not homogeneous and that its properties vary for different directions. The damping ratio extracted from the vibration test is much lower than in conventional structural materials like aluminum and steel.</p><p>In the FE analysis, the material was modeled both as isotropic and orthotropic. After adjusting the Young's modulus, the isotropic model shows accurate results until 1200 Hz. On the other hand, without knowing in which directions the properties occur, the orthotropic model is very limited.</p><p>In conclusion, even though the properties were tailored specifically for the specimen, the model might not correctly represent the material's behavior, being its properties not the same for different components. Therefore, it is more reasonable to use average data instead.</p>
----------------------------------------------------------------------
In diva2:1285510 - missing space and hyphen in title:
"Method for rebuilding gaspoweredtrucks"
==>
"Method for rebuilding gas-powered trucks"

abstract is:
<p>This master thesis investigates the possibility to rebuild heavy-duty trucks poweredby liquefied natural gas (LNG) to other fuel options such as compressednatural gas (CNG), diesel or ethanol. The background is that the second handvalue for a LNG truck is lower than a similar diesel truck due to an undevelopedinfrastructure with few refuelling stations. This results in a small market for secondhand LNG trucks.Both tractor and rigid trucks are evaluated from a technical perspective to determinewhich components that need to be changed when switching from LNG toanother type of fuel. When that is completed each fuel alternative is evaluatedbased on cost and market interest. Also certification and other legislations areinvestigated to determine if they will affect the rebuilding process.The result shows that the rebuild faces different technical complexity dependingon the target fuel alternative. It is concluded that a rebuild to diesel or ethanolis expensive due to many changes needed for the engine and aftertreatment andtherefore these alternatives are not a good choice for a rebuild. A rebuild to CNGis still expensive but can be of interest for rigid trucks, but not for tractor truckssince they usually have a demand for longer range. In order to get the final cost ofthe rebuild to CNG a commercial assessment has to be made and the rebuild willdepend on in which country the rebuild is performed due to different legislationsfor re-registration which may be a an obstacle.</p>

corrected abstract:
<p>This master thesis investigates the possibility to rebuild heavy-duty trucks powered by liquefied natural gas (LNG) to other fuel options such as compressed natural gas (CNG), diesel or ethanol. The background is that the second hand value for a LNG truck is lower than a similar diesel truck due to an undeveloped infrastructure with few refuelling stations. This results in a small market for second hand LNG trucks.</p><p>Both tractor and rigid trucks are evaluated from a technical perspective to determine which components that need to be changed when switching from LNG to another type of fuel. When that is completed each fuel alternative is evaluated based on cost and market interest. Also certification and other legislations are investigated to determine if they will affect the rebuilding process.</p><p>The result shows that the rebuild faces different technical complexity depending on the target fuel alternative. It is concluded that a rebuild to diesel or ethanol is expensive due to many changes needed for the engine and aftertreatment and therefore these alternatives are not a good choice for a rebuild. A rebuild to CNG is still expensive but can be of interest for rigid trucks, but not for tractor trucks since they usually have a demand for longer range. In order to get the final cost of the rebuild to CNG a commercial assessment has to be made and the rebuild will depend on in which country the rebuild is performed due to different legislations for re-registration which may be a an obstacle.</p>
----------------------------------------------------------------------
In diva2:1184068

abstract is:
<p>In this project, the design of a transverse leaf spring for an automotive vehicle isinvestigated. A transverse leaf spring is a concept for implementing the traditionalcoil spring for the vehicle, into a spring operating through beam bending. There aredifferent constructions and layouts of said leaf spring developed previously. Onesolution is where the spring is spanning from one side to the other of the vehicle,making it a transverse leaf spring. This solution has an extra gain; it is also providingan anti-roll bar action to the ride characteristics of the vehicle.The design of the transverse leaf spring is made for an automotive research vehicle atRoyal Institute of Technology (KTH). This vehicle is designed to represent a smallcity vehicle, weighing approximately 600 𝑘𝑔. The design of the original suspensionsystem is of the type Double Wishbone with push rod and coil springs with damper.The system is modular and exactly the same for the front and rear of the vehicle.Original mounting positions on the vehicle are to be kept intact. The design of thetransverse leaf spring is made in order to mimic the exact characteristics of theoriginal suspension system.First analytical optimizations are made in order to find an initial solution. This designis then implemented in FEM-software in order to further investigate thecharacteristics and design. A final design is found that is fulfilling the requirementsand a full scale version of the transverse leaf spring is built and examined withregards to its fulfilment of requirements.</p>

corrected abstract:
<p>In this project, the design of a transverse leaf spring for an automotive vehicle is investigated. A transverse leaf spring is a concept for implementing the traditional coil spring for the vehicle, into a spring operating through beam bending. There are different constructions and layouts of said leaf spring developed previously. One solution is where the spring is spanning from one side to the other of the vehicle, making it a transverse leaf spring. This solution has an extra gain; it is also providing an anti-roll bar action to the ride characteristics of the vehicle.</p><p>The design of the transverse leaf spring is made for an automotive research vehicle at <em>Royal Institute of Technology</em> (KTH). This vehicle is designed to represent a small city vehicle, weighing approximately 600 <em>kg</em>. The design of the original suspension system is of the type <em>Double Wishbone</em> with push rod and coil springs with damper. The system is modular and exactly the same for the front and rear of the vehicle. Original mounting positions on the vehicle are to be kept intact. The design of the transverse leaf spring is made in order to mimic the exact characteristics of the original suspension system.</p><p>First analytical optimizations are made in order to find an initial solution. This design is then implemented in FEM-software in order to further investigate the characteristics and design. A final design is found that is fulfilling the requirements and a full scale version of the transverse leaf spring is built and examined with regards to its fulfilment of requirements.</p>
----------------------------------------------------------------------
In diva2:1141679 - title missing spaces:
"Swing check valvecharacterization: 3D CFDvalidation of one dimensionalmodels used in RELAP5"
==>
"Swing check valve characterization: 3D CFD validation of one dimensional models used in RELAP5"

abstract is:
<p>In a previous thesis work a swing check valve was studied with CFD analysisin order to nd correlations that could provide a good input for a onedimensionalmodel of the same. In this document, starting from the previousthesis results and using the model by Li and Liou as the reference work, a checkvalve is investigated and the hydraulic torque coecients identied. In this wayit becomes possible to analyze the behavior of the same valve with a 1D codecalled RELAP5.The rst part of the work was dedicated to understanding the dynamics lyingbehind the movement of swing check vale, and to the construction of a suitable3D CFD model being able to nd the required coecients. The results weresubsequently elaborated and implemented in the RELAP5 code, in order to runthe 1D simulations. In the second part of the job, several transient simulations ofdierent pipelines were conducted with the 1D model, monitoring in particularthe closure of the valve over time.In the end, the data obtained in RELAP5 were compared to those fromequivalent 3D CFD analysis and from an alternative 1D approach by Adamkowski.Although not completely matching, the results showed that the 1D model byLi and Liou has a good ability to accurately simulate the valve. Further workcould be surely done on particular topics in order to improve and tune thismodel, making it a consistent and cheaper alternative to 3D simulations, ableto accurately simulate a wide range of cases.</p>

corrected abstract:
<p>In a previous thesis work a swing check valve was studied with CFD analysis in order to find correlations that could provide a good input for a one-dimensional model of the same. In this document, starting from the previous thesis results and using the model by Li and Liou as the reference work, a check valve is investigated and the hydraulic torque coefficients identified. In this way it becomes possible to analyze the behavior of the same valve with a 1D code called RELAP5.</p><p>The first part of the work was dedicated to understanding the dynamics lying behind the movement of swing check vale, and to the construction of a suitable 3D CFD model being able to find the required coefficients. The results were subsequently elaborated and implemented in the RELAP5 code, in order to run the 1D simulations. In the second part of the job, several transient simulations of different pipelines were conducted with the 1D model, monitoring in particular the closure of the valve over time.</p><p>In the end, the data obtained in RELAP5 were compared to those from equivalent 3D CFD analysis and from an alternative 1D approach by Adamkowski. Although not completely matching, the results showed that the 1D model by Li and Liou has a good ability to accurately simulate the valve. Further work could be surely done on particular topics in order to improve and tune this model, making it a consistent and cheaper alternative to 3D simulations, able to accurately simulate a wide range of cases.</p>
----------------------------------------------------------------------
In diva2:1120505 abstract is:
<p>This paper presents a locomotion system suitable for interactive usethat can plan realistic paths for small numbers of bipedal charactersin virtual worlds. Earlier approaches are extended by allowing animationsto be arbitrarily blended to increase the range of motions thatthe character can produce and our system also achieves greater performancecompared to the earlier approaches. The system uses a graphof valid footprints in the world in which is searched for a path thatthe character should traverse. The resulting sequence of footprints aresmoothed and refined to make them more similar to the character’soriginal animations. To make the motion smoother the curvature andother parameters of the path are estimated and those estimates areused to interpolate between different sets of similar animation clips. Asthe system is based on footprints it allows characters to navigate evenacross regions which are not directly connected, for example by jumpingover the gaps between disconnected regions. We have implementedthe system in C# using the Unity Game Engine and we evaluate it bymaking the character perform various actions such as walking, runningand jumping and study the visual result.Accompanying material can be found at http://arongranberg.com/research/thesis2017.</p>


corrected abstract:
<p>This paper presents a locomotion system suitable for interactive use that can plan realistic paths for small numbers of bipedal characters in virtual worlds. Earlier approaches are extended by allowing animations to be arbitrarily blended to increase the range of motions that the character can produce and our system also achieves greater performance compared to the earlier approaches. The system uses a graph of valid footprints in the world in which is searched for a path that the character should traverse. The resulting sequence of footprints are smoothed and refined to make them more similar to the character's original animations. To make the motion smoother the curvature and other parameters of the path are estimated and those estimates are used to interpolate between different sets of similar animation clips. As the system is based on footprints it allows characters to navigate even across regions which are not directly connected, for example by jumping over the gaps between disconnected regions. We have implemented the system in C# using the Unity Game Engine and we evaluate it by making the character perform various actions such as walking, running and jumping and study the visual result. Accompanying material can be found at http://arongranberg.com/research/thesis2017.</p>
----------------------------------------------------------------------
In diva2:1120039 abstract is:
<p>Recommender systems can be seen everywheretoday, having endless possibilities of implementation. However, operating inthe background, they can easily be passed without notice. Essentially, recommendersystems are algorithms that generate predictions by operating on a certain dataset. Each case of recommendation is environment sensitive and dependent on thecondition of the data at hand. Consequently, it is difficult to foresee whichmethod, or combination of methods, to apply in a particular situation forobtaining desired results. The area of recommender systems that this thesis isdelimited to is Collaborative filtering (CF) and can be split up into threedifferent categories, namely memory based, model based and hybrid algorithms.This thesis implements a CF algorithm for each of these categories and setsfocus on comparing their prediction accuracy and their dependency on the amountof available training data (i.e. as a function of sparsity). The results showthat the model based algorithm clearly performs better than the memory based,both in terms of overall accuracy and sparsity dependency. With an increasingsparsity level, the problem of having users without any ratings is encountered,which greatly impacts the accuracy for the memory based algorithm. A hybridbetween these algorithms resulted in a better accuracy than the model basedalgorithm itself but with an insignificant improvement.</p>

corrected abstract:
<p>Recommender systems can be seen everywhere today, having endless possibilities of implementation. However, operating in the background, they can easily be passed without notice. Essentially, recommender systems are algorithms that generate predictions by operating on a certain data set. Each case of recommendation is environment sensitive and dependent on the condition of the data at hand. Consequently, it is difficult to foresee which method, or combination of methods, to apply in a particular situation for obtaining desired results. The area of recommender systems that this thesis is delimited to is Collaborative filtering (CF) and can be split up into three different categories, namely memory based, model based and hybrid algorithms. This thesis implements a CF algorithm for each of these categories and sets focus on comparing their prediction accuracy and their dependency on the amount of available training data (i.e. as a function of sparsity). The results show that the model based algorithm clearly performs better than the memory based, both in terms of overall accuracy and sparsity dependency. With an increasing sparsity level, the problem of having users without any ratings is encountered, which greatly impacts the accuracy for the memory based algorithm. A hybrid between these algorithms resulted in a better accuracy than the model based algorithm itself but with an insignificant improvement.</p>
----------------------------------------------------------------------
In diva2:783984 abstract is:
<p>With the increasing global demand for energy and environmental awareness, the interestin sustainable energy solutions has grown over the last years including wave energy.In this thesis there is a literature study on Wave Energy Converters (WEC) and a theorychapter on the power in ocean waves. The thesis work was done in in collaboration withCorPower Ocean (CPO), an innovative company developing a WEC. Two buoy shapes,both with two dierent weights were investigated and a comparison made on the eectof latching on power absorption. The work can be separated into two main parts, anumerical simulation and experimental tests.A numerical model incorporated with a mathematical description of CPO Power Take-o(PTO) physics was used to simulate and obtain numerical results on the buoy behaviourin select sea states. The benet of latching was obtained by comparing passively heavingbuoys to latch controlled buoys. The simulation model was used for various analysis ofthe system.Experiments were performed at 1:30 scale on the same buoys in a tank facility. Informationabout the hydrodynamics of the buoy, motion and power absorption was obtainedand the eect of latching on the power absorption found.Results on natural period and radiation damping were obtained and a drag coecientwas estimated.The results show that phase control by latching can substantially increase the powerabsorption of a point absorber and broaden the range of waves it can operate in. Agreementwas found in the numerical model and the experiments when investigating thebenet of latching compared to passively heaving.</p>


corrected abstract:
<p>With the increasing global demand for energy and environmental awareness, the interest in sustainable energy solutions has grown over the last years including wave energy. In this thesis there is a literature study on Wave Energy Converters (WEC) and a theory chapter on the power in ocean waves. The thesis work was done in in collaboration with CorPower Ocean (CPO), an innovative company developing a WEC. Two buoy shapes, both with two different weights were investigated and a comparison made on the effect of latching on power absorption. The work can be separated into two main parts, a numerical simulation and experimental tests.</p><p>A numerical model incorporated with a mathematical description of CPO Power Take-off (PTO) physics was used to simulate and obtain numerical results on the buoy behaviour in select sea states. The benefit of latching was obtained by comparing passively heaving buoys to latch controlled buoys. The simulation model was used for various analysis of the system.</p><p>Experiments were performed at 1:30 scale on the same buoys in a tank facility. Information about the hydrodynamics of the buoy, motion and power absorption was obtained and the effect of latching on the power absorption found. Results on natural period and radiation damping were obtained and a drag coefficient was estimated.</p><p>The results show that phase control by latching can substantially increase the power absorption of a point absorber and broaden the range of waves it can operate in. Agreement was found in the numerical model and the experiments when investigating the benefit of latching compared to passively heaving.</p>
----------------------------------------------------------------------
In diva2:515592 abstract is:
<p>This study examines how formal mathematics can be taught in the Swedishsecondary school with its new curriculum for mathematics. The study examineswhat a teaching material in formal mathematics corresponding to the initialcontent of the course Mathematics 1c could look like, and whether formalmathematics can be taught to high school students.The survey was conducted with second year students from the science programme.The majority of these students studied the course Mathematics D.The students described themselves as not being motivated towards mathematics.The results show that the content of the curriculum can be presented withformal mathematics. This both in terms of requirements for content and studentsbeing able to comprehend this content. The curriculum also requires thatthis type of mathematics is introduced in the course Mathematics 1c.The results also show that students are open towards and want more formalmathematics in their ordinary education. They initially felt it was strangebecause they had never encountered this type of mathematics before, but somestudents found the formal mathematics to be easier than the mathematicsordinarily presented in class.The study finds no reason to postpone the meeting with the formal mathematicsto university level. Students’ commitment to proof and their comprehentionof content suggests that formal mathematics can be introduced inhigh school courses. This study thus concludes that the new secondary schoolcourse Mathematics 1c can be formalised and therefore makes possible a renewedmathematics education.</p>

corrected abstract:
<p>This study examines how formal mathematics can be taught in the Swedish secondary school with its new curriculum for mathematics. The study examines what a teaching material in formal mathematics corresponding to the initial content of the course Mathematics 1c could look like, and whether formal mathematics can be taught to high school students.</p><p>The survey was conducted with second year students from the science programme. The majority of these students studied the course Mathematics D. The students described themselves as not being motivated towards mathematics.</p><p>The results show that the content of the curriculum can be presented with formal mathematics. This both in terms of requirements for content and students being able to comprehend this content. The curriculum also requires that this type of mathematics is introduced in the course Mathematics 1c.</p><p>The results also show that students are open towards and want more formal mathematics in their ordinary education. They initially felt it was strange because they had never encountered this type of mathematics before, but some students found the formal mathematics to be easier than the mathematics ordinarily presented in class.</p><p>The study finds no reason to postpone the meeting with the formal mathematics to university level. Students’ commitment to proof and their comprehention of content suggests that formal mathematics can be introduced in high school courses. This study thus concludes that the new secondary school course Mathematics 1c can be formalised and therefore makes possible a renewed mathematics education.</p>
----------------------------------------------------------------------
In diva2:515494 title missing space:
"User Centred Design for adolescents withCerebral Palsy: Designing an eye controlled software to enhance mathematical activities"
==>
"User Centred Design for adolescents with Cerebral Palsy: Designing an eye controlled software to enhance mathematical activities"


abstract is:
<p>This study aims to find an answer to what prerequisites that needs to betaken into consideration when designing an eye controlled software formathematical activities carried out by children that suffers from cerebralpalsy. A user centred study was conducted at three habilitation centresaround Stockholm. This resulted in a high-fi prototype for columnarcalculation, in which findings from the study were incorporated.These findings included the need to be able to adjust colour, sizeand shape of interface elements, as the target group suffered from visualimpairments. The interface should have a simple and clean design, astoo appealing elements may draw the attention away from the task. Furthermore,it shouldn’t be too childish, despite the fact that the softwarecovers basic mathematics.The tasks should have various kinds of representation, such as readoutinstructions and visualizations. It is also theorized that by designingthe interface to have non-selectable elements, the user doesn’t needworry about clicking on buttons that affects the interface. Thus, thefocus can be on solving the task. The user should be encourage to solvetasks by getting feedback when a task is solved. This feedback shouldonly be given once per task, and should be customizable and optional.</p>

corrected abstract:
<p>This study aims to find an answer to what prerequisites that needs to be taken into consideration when designing an eye controlled software for mathematical activities carried out by children that suffers from cerebral palsy. A user centred study was conducted at three habilitation centres around Stockholm. This resulted in a high-fi prototype for columnar calculation, in which findings from the study were incorporated.</p><p>These findings included the need to be able to adjust colour, size and shape of interface elements, as the target group suffered from visual impairments. The interface should have a simple and clean design, as too appealing elements may draw the attention away from the task. Furthermore, it shouldn’t be too childish, despite the fact that the software covers basic mathematics.</p><p>The tasks should have various kinds of representation, such as readout instructions and visualizations. It is also theorized that by designing the interface to have non-selectable elements, the user doesn’t need worry about clicking on buttons that affects the interface. Thus, the focus can be on solving the task. The user should be encourage to solve tasks by getting feedback when a task is solved. This feedback should only be given once per task, and should be customizable and optional.</p>
----------------------------------------------------------------------
In diva2:1816888 abstract is:
<p>Accurately modeling aerodynamic forces and moments are crucial for understanding thebehavior of an aircraft when performing various maneuvers at different flight conditions.However, this task is challenging due to complex nonlinear dependencies on manydifferent parameters. Currently, Computational Fluid Dynamics (CFD), wind tunnel,and flight tests are the most common methods used to gather information about thecoefficients, which are both costly and time–consuming. Consequently, great efforts aremade to find alternative methods such as machine learning.</p><p>This thesis focus on finding machine learning models that can model the static and thedynamic aerodynamics coefficients for lift, drag, and pitching moment. Seven machinelearning models for static estimation were trained on data from CFD simulations.The main focus was on dynamic aerodynamics since these are more difficult toestimate. Here two machine learning models were implemented, Long Short–TermMemory (LSTM) and Gaussian Process Regression (GPR), as well as the ordinaryleast squares. These models were trained on data generated from simulated flighttrajectories of longitudinal movements.</p><p>The results of the study showed that it was possible to model the static coefficients withlimited data and still get high accuracy. There was no machine learning model thatperformed best for all three coefficients or with respect to the size of the training data.The Support vector regression was the best for the drag coefficients, while there wasno clear best model for the lift and moment. For the dynamic coefficients, the ordinaryleast squares performed better than expected and even better than LSTM and GPR forsome flight trajectories. The Gaussian process regression produced better results whenestimating a known trajectory, while the LSTM was better when predicting values ofa flight trajectory not used to train the models.</p>

corrected abstract:
<p>Accurately modeling aerodynamic forces and moments are crucial for understanding the behavior of an aircraft when performing various maneuvers at different flight conditions. However, this task is challenging due to complex nonlinear dependencies on many different parameters. Currently, Computational Fluid Dynamics (CFD), wind tunnel, and flight tests are the most common methods used to gather information about the coefficients, which are both costly and time–consuming. Consequently, great efforts are made to find alternative methods such as machine learning.</p><p>This thesis focus on finding machine learning models that can model the static and the dynamic aerodynamics coefficients for lift, drag, and pitching moment. Seven machine learning models for static estimation were trained on data from CFD simulations. The main focus was on dynamic aerodynamics since these are more difficult to estimate. Here two machine learning models were implemented, Long Short–Term Memory (LSTM) and Gaussian Process Regression (GPR), as well as the ordinary least squares. These models were trained on data generated from simulated flight trajectories of longitudinal movements.</p><p>The results of the study showed that it was possible to model the static coefficients with limited data and still get high accuracy. There was no machine learning model that performed best for all three coefficients or with respect to the size of the training data. The Support vector regression was the best for the drag coefficients, while there was no clear best model for the lift and moment. For the dynamic coefficients, the ordinary least squares performed better than expected and even better than LSTM and GPR for some flight trajectories. The Gaussian process regression produced better results when estimating a known trajectory, while the LSTM was better when predicting values of a flight trajectory not used to train the models.</p>
----------------------------------------------------------------------
In diva2:1757049 abstract is:
<p>This thesis is a case study in collaboration with the company Getswish AB. GetswishAB provides the mobile application and payment service Swish with the purpose ofdelivering smooth money transfers for individuals and companies in Sweden. About80 percent of the Swedish population are connected to Swish, and the majority seethe service as an apparent part of everyday life. This work studies a small part of alltransactions that take place daily between individuals and companies. Specifically, thispaper examines which factors affect the Swish transaction amount (TA) to companieswithin five different industries. The five industries studied are: Sports, leisure,and entertainment activities; Restaurant, catering, and bar activities; Retail trade,except for motor vehicles and motorcycles; Trade and repair of motor vehicles andmotorcycles; and Telecommunications. In combination with descriptive analysis andseasonality studies, a multiple linear regression model is used to evaluate patternsin the amount transferred to companies within the various industries. The responsevariable is the daily aggregated TA and the seven responding regressors examined are:i) The number of employees of the company, ii) The revenue of the company, iii) Thedate for registration to Swish service for companies, iv) The age of the customers, v) Thegender of the customers, vi) The number of transactions, and vii) The transaction date.The estimated parameters for each regressor are studied to evaluate correlations withthe TA. This thesis states that it is possible to construct a model from the regressorsanalyzed, which can predict the amount with an explanation degree of above 85% forfour of the five industries. The model constructed for the motor vehicle industry nevergives satisfactory results and must be further investigated to conclude.</p>

corrected abstract:
<p>This thesis is a case study in collaboration with the company Getswish AB (Swish). The company provides a mobile application and payment service Swish with the purpose to delivering smooth money transfers for individuals and companies in Sweden. About 80 percent of the Swedish population are connected to Swish, and the majority see the service as an apparent part of everyday life. This work studies a small part of all transactions that take place daily between individuals and companies. Specifically, this paper examines which factors affect the Swish transaction amount (TA) to companies within five different industries. The five industries studied are: Sports, leisure, and entertainment activities; Restaurant, catering, and bar activities; Retail trade, except for motor vehicles and motorcycles; Trade and repair of motor vehicles and motorcycles; and Telecommunications. In combination with descriptive analysis and seasonality studies, a multiple linear regression model is used to evaluate patterns in the amount transferred to companies within the various industries. The response variable is the daily aggregated TA and the seven responding regressors examined are: i) The number of employees of the company, ii) The revenue of the company, iii) The date for registration to Swish service for companies, iv) The age of the customers, v) The gender of the customers, vi) The number of transactions, and vii) The transaction date. The estimated parameters for each regressor are studied to evaluate correlations with the TA. This thesis states that it is possible to construct a model from the regressors analyzed, which can predict the amount with an explanation degree of above 85% for four of the five industries. The model constructed for the motor vehicle industry never gives satisfactory results and must be further investigated to conclude.</p>
----------------------------------------------------------------------
In diva2:1720701 abstract is:
<p>Transformer-based models are frequently used in natural language processing. These models are oftenlarge and pre-trained for general language understanding and then fine-tuned for a specific task. Becausethese models are large, they have a high memory requirement and have high inference time. Severalmodel compression techniques have been developed in order to reduce the mentioned disadvantageswithout significantly reducing the inference performance of the models. This thesis studies unstructuredpruning method, which are pruning methods that do not follow a predetermined pattern when removingparameters, to understand which parameters can be removed from language models and the impact ofremoving a significant portion of a model's parameters. Specifically, magnitude pruning, movementpruning, soft movement pruning, and $L_0$ regularization were applied to the pre-trained languagemodels BERT and M-BERT. The pre-trained models in turn were fine-tuned for sentiment classificationtasks, which refers to the task of classifying a given sentence to predetermined labels, such as positive ornegative. Magnitude pruning worked the best when pruning the models to a ratio of 15\% of the models'original parameters, while soft movement pruning worked the best for the weight ratio of 3\%. Formovement pruning, we were not able to achieve satisfying results for binary sentiment classification.From investigating the pruning patterns derived from soft movement pruning and $L_0$ regularization, itwas found that a large portion of the parameters from the last transformer blocks in the model architecturecould be removed without significantly reducing the model performance. An example of interestingfurther work is to remove the last transformer blocks altogether and investigate if an increase in inferencespeed is attained without significantly reducing the performance.</p>

corrected abstract:
<p>Transformer-based models are frequently used in natural language processing. These models are often large and pre-trained for general language understanding and then fine-tuned for a specific task. Because these models are large, they have a high memory requirement and have high inference time. Several model compression techniques have been developed in order to reduce the mentioned disadvantages without significantly reducing the inference performance of the models. This thesis studies unstructured pruning method, which are pruning methods that do not follow a predetermined pattern when removing parameters, to understand which parameters can be removed from language models and the impact of removing a significant portion of a model's parameters. Specifically, magnitude pruning, movement pruning, soft movement pruning, and <em>L<sub>0</sub></em> regularization were applied to the pre-trained language models BERT and M-BERT. The pre-trained models in turn were fine-tuned for sentiment classification tasks, which refers to the task of classifying a given sentence to predetermined labels, such as positive or negative. Magnitude pruning worked the best when pruning the models to a ratio of 15% of the models' original parameters, while soft movement pruning worked the best for the weight ratio of 3%. For movement pruning, we were not able to achieve satisfying results for binary sentiment classification. From investigating the pruning patterns derived from soft movement pruning and <em>L<sub>0</sub></em> regularization, it was found that a large portion of the parameters from the last transformer blocks in the model architecture could be removed without significantly reducing the model performance. An example of interesting further work is to remove the last transformer blocks altogether and investigate if an increase in inference speed is attained without significantly reducing the performance.</p>
----------------------------------------------------------------------
In diva2:1720668 abstract is:
<p>As Machine Learning (ML) methods have gained traction in recent years, someproblems regarding the construction of such methods have arisen. One such problem isthe collection and labeling of data sets. Specifically when it comes to many applicationsof Computer Vision (CV), one needs a set of images, labeled as either being of someclass or not. Creating such data sets can be very time consuming. This project setsout to tackle this problem by constructing an end-to-end system for searching forobjects in images (i.e. an Object Based Image Retrieval (OBIR) method) using an objectdetection framework (You Only Look Once (YOLO) [16]). The goal of the project wasto create a method that; given an image of an object of interest q, search for that sameor similar objects in a set of other images S. The core concept of the idea is to passthe image q through an object detection model (in this case YOLOv5 [16]), create a”fingerprint” (can be seen as a sort of identity for an object) from a set of feature mapsextracted from the YOLOv5 [16] model and look for corresponding similar parts of aset of feature maps extracted from other images. An investigation regarding whichvalues to select for a few different parameters was conducted, including a comparisonof performance for a couple of different similarity metrics. In the table below,the parameter combination which resulted in the highest F_Top_300-score (a measureindicating the amount of relevant images retrieved among the top 300 recommendedimages) in the parameter selection phase is presented.</p><p>Layer: 23Pool Methd: maxSim. Mtrc: eucFP Kern. Sz: 4</p><p>Evaluation of the method resulted in F_Top_300-scores as can be seen in the table below.</p><p>Mouse: 0.820Duck: 0.640Coin: 0.770Jet ski: 0.443Handgun: 0.807Average: 0.696</p>


corrected abstract:
<p>As Machine Learning (ML) methods have gained traction in recent years, some problems regarding the construction of such methods have arisen. One such problem is the collection and labeling of data sets. Specifically when it comes to many applications of Computer Vision (CV), one needs a set of images, labeled as either being of some class or not. Creating such data sets can be very time consuming. This project sets out to tackle this problem by constructing an end-to-end system for searching for objects in images (i.e. an Object Based Image Retrieval (OBIR) method) using an object detection framework (You Only Look Once (YOLO) [16]). The goal of the project was to create a method that; given an image of an object of interest <em>q</em>, search for that same or similar objects in a set of other images S. The core concept of the idea is to pass the image <em>q</em> through an object detection model (in this case YOLOv5 [16]), create a ”fingerprint” (can be seen as a sort of identity for an object) from a set of feature maps extracted from the YOLOv5 [16] model and look for corresponding similar parts of a set of feature maps extracted from other images. An investigation regarding which values to select for a few different parameters was conducted, including a comparison of performance for a couple of different similarity metrics. In the table below, the parameter combination which resulted in the highest <em>F<sub>Top300</sub></em>-score (a measure indicating the amount of relevant images retrieved among the top 300 recommended images) in the parameter selection phase is presented.</p>


<table style="border-collapse: collapse; width: 50%;" border="1">
    <tbody>
        <tr>
            <td style="width: 25%;"><span style="color: #ff3232;">Layer Pool.</span></td>
            <td style="width: 25%;"><span style="color: #0000da;">Methd Sim.</span></td>
            <td style="width: 25%;"><span style="color: #008080;">Mtrc FP</span></td>
            <td style="width: 25%;"><span style="color: #912c91;">Kern. Sz</span></td>
        </tr>
        <tr>
            <td style="width: 25%;"><span style="color: #ff3232;">23</span></td>
            <td style="width: 25%;"><span style="color: #0000da;">max</span></td>
            <td style="width: 25%;"><span style="color: #008080;">euc</span></td>
            <td style="width: 25%;"><span style="color: #912c91;">4</span></td>
        </tr>
    </tbody>
</table>

Layer Pool. Methd Sim. Mtrc FP Kern. Sz
23 max euc 4
<p>”Pool. Methd” denotes ”Pooling Method”, ”Sim. Mtrc” denotes ”Similarity Metric” and ”FP Kern. Sz”
denotes ”Fingerprintization Kernel Size” (for image fingerprintization)</p>

<p>Evaluation of the method resulted in <em>F<sub>Top300</sub></em>-scores as can be seen in the table below.</p>
<table style="border-collapse: collapse; width: 100%;" border="1">
    <tbody>
        <tr>
            <td style="width: 10%;"><span style="color: #ff3232;">Layer Pool.</span></td>
            <td style="width: 10%;"><span style="color: #0000da;">Methd Sim.</span></td>
            <td style="width: 10%;"><span style="color: #008080;">Mtrc FP</span></td>
            <td style="width: 10%;"><span style="color: #912c91;">Kern. Sz</span></td>
            <td style="width: 10%;"><span style="color: black;">Mouse</span></td>
            <td style="width: 10%;"><span style="color: black;">Duck</span></td>
            <td style="width: 10%;"><span style="color: black;">Coin</span></td>
            <td style="width: 10%;"><span style="color: black;">Jet ski</span></td>
            <td style="width: 10%;"><span style="color: black;">Handgun</span></td>
            <td style="width: 10%;"><span style="color: black;">Average</span></td>
        </tr>
        <tr>
            <td style="width: 10%;"><span style="color: #ff3232;">23</span></td>
            <td style="width: 10%;"><span style="color: #0000da;">max</span></td>
            <td style="width: 10%;"><span style="color: #008080;">euc</span></td>
            <td style="width: 10%;"><span style="color: #912c91;">4</span></td>
            <td style="width: 10%;"><span style="color: black;">0.820</span></td>
            <td style="width: 10%;"><span style="color: black;">0.640</span></td>
            <td style="width: 10%;"><span style="color: black;">0.770</span></td>
            <td style="width: 10%;"><span style="color: black;">0.443</span></td>
            <td style="width: 10%;"><span style="color: black;">0.807</span></td>
            <td style="width: 10%;"><span style="color: black;">0.696</span></td>
        </tr>
    </tbody>
</table>
<p>Resulting <em>F<sub>Top300</sub></em> scores for the five classes (as well as the average) of the evaluation.</p>
----------------------------------------------------------------------
In diva2:1682170 abstract is:
<p>We manipulate two superconducting qubits using digital microwave electronics. Starting fromtheir characterization, we develop a real-time reset scheme and implement the iSwap gate. Thequbits’ parameters are obtained using standard single-qubit characterization techniques, such asRabi and Ramsey oscillations and frequency sweep of the resonators. We also characterized theexperimental setup, including finding the working point of a Josephson Parametric Amplifierand the coupler between the two qubits. We solve the linear differential equations that modelthe resonator, in order to design a high-fidelity, single-shot qubit-measurement pulse shape,which actively empties the cavity. Using this pulse, we achieve a readout assignment fidelity of99.9%. The readout is formed in real-time using template matching. In addition, we implementa conditional reset of the qubit’s state in 1.4 μs, which resets the excited state population from5.4% to 0.5%. We simulate the cavity using QuTip to further optimize the readout pulse.Furthermore, we characterize the third energy level of the qubit to implement a qutrit readoutand observe a second excited state population of 0.3%, in accordance with theory. Finally,we implement the iSwap gate that, together with single-qubit gates, constitute a set of universalquantum gates, where we swap the 95.4% of the quantum state between the qubits in 690 ns. Allexperiments, including the pulse events and synchronization of the readout and feedback, wereperformed using a digital microwave platform based on a radio-frequency-on-a-chip system,and implemented using a Python interface.</p>


corrected abstract:
<p>We manipulate two superconducting qubits using digital microwave electronics. Starting from their characterization, we develop a real-time reset scheme and implement the iSwap gate. The qubits’ parameters are obtained using standard single-qubit characterization techniques, such as Rabi and Ramsey oscillations and frequency sweep of the resonators. We also characterized the experimental setup, including finding the working point of a Josephson Parametric Amplifier and the coupler between the two qubits. We solve the linear differential equations that model the resonator, in order to design a high-fidelity, single-shot qubit-measurement pulse shape, which actively empties the cavity. Using this pulse, we achieve a readout assignment fidelity of 99.9%. The readout is formed in real-time using template matching. In addition, we implement a conditional reset of the qubit’s state in 1.4 µs, which resets the excited state population from 5.4% to 0.5%. We simulate the cavity using QuTip to further optimize the readout pulse. Furthermore, we characterize the third energy level of the qubit to implement a qutrit readout and observe a second excited state population of 0.3%, in accordance with theory. Finally, we implement the iSwap gate that, together with single-qubit gates, constitute a set of universal quantum gates, where we swap the 95.4% of the quantum state between the qubits in 690 ns. All experiments, including the pulse events and synchronization of the readout and feedback, were performed using a digital microwave platform based on a radio-frequency-on-a-chip system, and implemented using a Python interface.</p>
----------------------------------------------------------------------
In diva2:1673915 abstract is:
<p>There are constant natural disasters and conflicts around the world. In these areas, thereis a great demand to temporary housing. Better Shelter is a Swedish organization thatoffers temporary housing units for aid organizations. The product they invest in the mostis a metal frame on which roofs and walls must be complemented with local materials.The metal structure has a good strength characteristic but poses a challenge on howevertries to attach roofs and walls reliably. Therefore, the assigner wants to create a fasteningelement that can be sent with the rest of the structure, and which not only facilitatesattachment of the roof, but also handles the loads that the home is assumed to endure.This work includes the design process of a fastening element that facilitates the fasteningof a corrugated galvanized metal sheet roof (CGI) as well as a linear analysis in ANSYS ofthe load capacity of the roof and the fastening. The fastening element that was decided toproceed with is the so-called J-connector which is sized with standardized bolt materialsand dimensions in mind.Since the linear analysis can only consider the material within the linear elastic limit, themodel could not make the accurate prediction results as the stresses surpassed the yieldlimit locally. Mesh refining around the stress concentrations combined with including thenon-linear response of the material is required. The change in the contact status need tobe included at places where plasticizing and buckling can occur. Since the yield limitwas surpassed close to connection points and it could be tolerated in reality, the suggesteddevelopment is constructing the physical prototype base on the proposed fastening elementand performing the experimental analysis.</p>

corrected abstract:
<p>There are constant natural disasters and conflicts around the world. In these areas, there is a great demand to temporary housing. Better Shelter is a Swedish organization that offers temporary housing units for aid organizations. The product they invest in the most is a metal frame on which roofs and walls must be complemented with local materials. The metal structure has a good strength characteristic but poses a challenge on however tries to attach roofs and walls reliably. Therefore, the assigner wants to create a fastening element that can be sent with the rest of the structure, and which not only facilitates attachment of the roof, but also handles the loads that the home is assumed to endure. This work includes the design process of a fastening element that facilitates the fastening of a corrugated galvanized metal sheet roof (CGI) as well as a linear analysis in ANSYS of the load capacity of the roof and the fastening. The fastening element that was decided to proceed with is the so-called J-connector which is sized with standardized bolt materials and dimensions in mind.</p><p>Since the linear analysis can only consider the material within the linear elastic limit, the model could not make the accurate prediction results as the stresses surpassed the yield limit locally. Mesh refining around the stress concentrations combined with including the non-linear response of the material is required. The change in the contact status need to be included at places where plasticizing and buckling can occur. Since the yield limit was surpassed close to connection points and it could be tolerated in reality, the suggested development is constructing the physical prototype base on the proposed fastening element and performing the experimental analysis.</p>
----------------------------------------------------------------------
In diva2:1602688 abstract is:
<p>Metamaterials are artificially designed materials with desired electromagneticresponses for advanced wave manipulation. Their key constituent is often somenoble metal, thanks to its well localized plasmonic effects with highextinction cross section. In this project, a metamaterial based onmetal-insulator-metal (MIM) structure is investigated to create a compactplanar reflector which mimics the function of a parabolic mirror. In such ametamaterial, each MIM unit is essentially a sub-wavelength resonator whichexhibits magnetic-dipole resonance. To achieve focusing effect, phase shift onreflected wave by each MIM unit upon a plane-wave incidence is calculatedrigorously through finite-element method. By carefully selecting unitgeometries and thereby introducing a phase gradient along the reflector plane,one can control propagation direction of reflected wave at each reflectorposition. The principle can be explained in terms of either ray-optics theoryor generalized Snell’s law. As a particular demonstration, we have designed inthe thesis a planar reflector consisting of eleven MIM units with a totaldevice width of 5.5 µm. FEM simulation showed that the reflector focuses lightat 1.2 µm wavelength with a nominal focus length of 6 µm. Such compactmetamaterial devices can be potentially fabricated on chips for sensing andtelecom applications, circumventing many inconveniences of includingconventional lenses in an optical system.</p>

corrected abstract:
<p>Metamaterials are artificially designed materials with desired electromagnetic responses for advanced wave manipulation. Their key constituent is often some noble metal, thanks to its well localized plasmonic effects with high extinction cross section. In this project, a metamaterial based on metal-insulator-metal (MIM) structure is investigated to create a compact planar reflector which mimics the function of a parabolic mirror. In such a metamaterial, each MIM unit is essentially a sub-wavelength resonator which exhibits magnetic-dipole resonance. To achieve focusing effect, phase shift on reflected wave by each MIM unit upon a plane-wave incidence is calculated rigorously through finite-element method. By carefully selecting unit geometries and thereby introducing a phase gradient along the reflector plane, one can control propagation direction of reflected wave at each reflector position. The principle can be explained in terms of either ray-optics theory or generalized Snell’s law. As a particular demonstration, we have designed in the thesis a planar reflector consisting of eleven MIM units with a total device width of 5.5 µm. FEM simulation showed that the reflector focuses light at 1.2 µm wavelength with a nominal focus length of 6 µm. Such compact metamaterial devices can be potentially fabricated on chips for sensing and telecom applications, circumventing many inconveniences of including conventional lenses in an optical system.</p>
----------------------------------------------------------------------
In diva2:1595609 Note: no full text in DiVA

abstract is:
<p>Electric solutions for leisure craft propulsion are gaining popularity as global environmentalawareness increases and it also enables the experience of silent boating. The most criticalproblem with electric boats is the low range due to the limited energy density of batteries.Most of the energy is lost due to the resistance of the boat hull. Conventional electricpropulsion systems require mechanical gears and rudders which result in energy losses.To maximise the efficiency and thus the operating range/endurance of such displacementvessels, different propulsion systems are investigated by marine propulsor suppliers. Thisthesis investigates different aspects of pod propulsion systems for displacement boats. Amethod is presented to estimate the added resistance of the pod body and the strut based onrecommended procedures from ITTC and resistance coefficients from Hoerner. The coolingperformance of the system is investigated by a method considering approximated convectiveheat transfer from the housing to the seawater. A concept of a steerable pod is presentedaccording to the evaluated dimensions of a 50 kW electric motor by fellow thesis workerA. Bagaskara. General arrangement, attachment to an example hull and load carryingcapability of the structure were evaluated. The pod unit consists of different metallic parts,therefore it is subjected to galvanic corrosion. The possibilities of cathodic protection areconsidered. A comparison is carried out between podded and L-drive arrangements, and it isconcluded that pods are more beneficial in terms of space requirement, cooling, maintenanceand overall efficiency at low speeds. The building costs are estimated to increase mostlydue to the need for a custom made electric motor.</p>

corrected abstract:
<p>Electric solutions for leisure craft propulsion are gaining popularity as global environmentalawareness increases and it also enables the experience of silent boating. The most critical problem with electric boats is the low range due to the limited energy density of batteries. Most of the energy is lost due to the resistance of the boat hull. Conventional electricpropulsion systems require mechanical gears and rudders which result in energy losses. To maximise the efficiency and thus the operating range/endurance of such displacement vessels, different propulsion systems are investigated by marine propulsor suppliers. This thesis investigates different aspects of pod propulsion systems for displacement boats. Amethod is presented to estimate the added resistance of the pod body and the strut based on recommended procedures from ITTC and resistance coefficients from Hoerner. The cooling performance of the system is investigated by a method considering approximated convective heat transfer from the housing to the seawater. A concept of a steerable pod is presented according to the evaluated dimensions of a 50 kW electric motor by fellow thesis worker A. Bagaskara. General arrangement, attachment to an example hull and load carrying capability of the structure were evaluated. The pod unit consists of different metallic parts, therefore it is subjected to galvanic corrosion. The possibilities of cathodic protection are considered. A comparison is carried out between podded and L-drive arrangements, and it is concluded that pods are more beneficial in terms of space requirement, cooling, maintenance and overall efficiency at low speeds. The building costs are estimated to increase mostly due to the need for a custom made electric motor.</p>
----------------------------------------------------------------------
In diva2:1528136 abstract is:
<p>Optimization methods are commonly used to develop new products and are also an importantstep in more incremental design improvements. In the maritime industry, these methodsare often used to create more ecient vessels and to ful ll the environmental requirementsimposed by the IMO. In recent years, the adjoint method have been used more frequently.This method can be used to predict the inuence of some input parameters on a quantityin a Computational Fluid Dynamics (CFD) simulation.In this project, the adjoint method has been investigated and applied on a relevant case;how it can be used to reduce the drag of a twisted rudder by changing the twist angles.STAR-CCM+ has been used to perform the CFD and adjoint simulations. These resultshave been imported to CAESES, a CAD-modeler, which connects the adjoint results to thedesign parameters. The adjoint results indicate a possible change of the design parameter,the twist angle is modi ed based on these results and a new geometry of the rudder is constructedin CAESES. Furthermore, the numerical results indicates that the method can beused to reduce the drag on the rudder. One of the cases in the project achieved a reductionof the rudder drag by 3.35 % and the total drag decreased with 0.18 %. However, the othertwo cases did not achieve a reduction of the drag and hence further investigations needs tobe done.The adjoint method have the possibility to be a good optimization alternative for developmentof new products or in engineering-to-order processes. The option of connecting theadjoint results to design parameters is a great advantage. On the other hand, the method inthis project is not reliable and the reason for the contradictory results needs to be studiedfurther.</p>

corrected abstract:
<p>Optimization methods are commonly used to develop new products and are also an important step in more incremental design improvements. In the maritime industry, these methods are often used to create more efficient vessels and to fulfill the environmental requirements imposed by the IMO. In recent years, the adjoint method have been used more frequently. This method can be used to predict the influence of some input parameters on a quantity in a Computational Fluid Dynamics (CFD) simulation.</p><p>In this project, the adjoint method has been investigated and applied on a relevant case; how it can be used to reduce the drag of a twisted rudder by changing the twist angles. STAR-CCM+ has been used to perform the CFD and adjoint simulations. These results have been imported to CAESES, a CAD-modeler, which connects the adjoint results to the design parameters. The adjoint results indicate a possible change of the design parameter, the twist angle is modified based on these results and a new geometry of the rudder is constructed in CAESES. Furthermore, the numerical results indicates that the method can be used to reduce the drag on the rudder. One of the cases in the project achieved a reduction of the rudder drag by 3.35 % and the total drag decreased with 0.18 %. However, the other two cases did not achieve a reduction of the drag and hence further investigations needs to be done.</p><p>The adjoint method have the possibility to be a good optimization alternative for development of new products or in engineering-to-order processes. The option of connecting the adjoint results to design parameters is a great advantage. On the other hand, the method in this project is not reliable and the reason for the contradictory results needs to be studied further.</p>
----------------------------------------------------------------------
In diva2:1528127 abstract is:
<p>Knowledge of vessel responses to waves is of the utmost importance for vessel operations.The responses affect which routes a vessel can take, what cargo it can carry, theconditions it’s crew will experience and much more. This can pose a problem for performanceoptimisation companies such as GreenSteam, partners in this project, for whomthe vessel transfer functions are generally not available.This project aims to use bayesian machine learning methods to infer transfer functionsand predict vessel responses. Publically available directional wave spectra are combinedwith highfrequencymotion measurements from a vessel to train a model to create thetransfer functions, which can be integrated to get the motions. If successful, this would bea relatively inexpensive method for computing transfer functions on any vessel for whichthe required measurements are available. Though not many vessels measure this datacurrently, the industry is moving towards more data collection, so that number is likely torise.The results identify a number of issues in the available data which must be overcome toproduce usable results from these methods. Though results are not optimal, they show apromising start and a route is proposed for future research in this area.</p>

corrected abstract:
<p>Knowledge of vessel responses to waves is of the utmost importance for vessel operations. The responses affect which routes a vessel can take, what cargo it can carry, the conditions it’s crew will experience and much more. This can pose a problem for performance optimisation companies such as GreenSteam, partners in this project, for whom the vessel transfer functions are generally not available.</p><p>This project aims to use bayesian machine learning methods to infer transfer functions and predict vessel responses. Publically available directional wave spectra are combined with high-frequency motion measurements from a vessel to train a model to create the transfer functions, which can be integrated to get the motions. If successful, this would be a relatively inexpensive method for computing transfer functions on any vessel for which the required measurements are available. Though not many vessels measure this data currently, the industry is moving towards more data collection, so that number is likely to rise.</p><p>The results identify a number of issues in the available data which must be overcome to produce usable results from these methods. Though results are not optimal, they show a promising start and a route is proposed for future research in this area.</p>
----------------------------------------------------------------------
In diva2:1319886 abstract is:
<p>The interest in modeling non-maturing deposits has skyrocketed ever since thefinancial crisis 2008. Not only from a regulatory and legislative perspective,but also from an investment and funding perspective.Modeling of non-maturing deposits is a very broad subject. In this thesis someof the topics within the subject are investigated, where the greatest focus inon the modeling of the deposit volumes. The main objective is to providethe bank with an analysis of the majority of the topics that needs to be cov-ered when modeling non-maturing deposits. This includes short-rate model-ing using Vasicek’s model, deposit rate modeling using a regression approachand a method proposed by Jarrow and Van Deventer, volume modeling usingSARIMA, SARIMAX and a general additive model, a static replicating port-folio based on Maes and Timmerman’s to model the behaviour of the depositaccounts and finally a liquidity risk model that was suggested by Kalkbrenerand Willing. All of these models have been applied on three different accounttypes: private transaction accounts, savings accounts and corporate savingsaccounts.The results are that, due to the current market, the static replicating portfoliodoes not achieve the desired results. Furthermore, the best volume model forthe data provided is a SARIMA model, meaning the effect of the exogenousvariables are seemingly already embedded in the lagged volume. Finally, theliquidity risk results are plausible and thus deemed satisfactory.</p>


corrected abstract:
<p>The interest in modeling non-maturing deposits has skyrocketed ever since the financial crisis 2008. Not only from a regulatory and legislative perspective, but also from an investment and funding perspective.</p><p>Modeling of non-maturing deposits is a very broad subject. In this thesis some of the topics within the subject are investigated, where the greatest focus in on the modeling of the deposit volumes. The main objective is to provide the bank with an analysis of the majority of the topics that needs to be covered when modeling non-maturing deposits. This includes short-rate modeling using Vasicek’s model, deposit rate modeling using a regression approach and a method proposed by Jarrow and Van Deventer, volume modeling using SARIMA, SARIMAX and a general additive model, a static replicating portfolio based on Maes and Timmerman’s to model the behaviour of the deposit accounts and finally a liquidity risk model that was suggested by Kalkbrener and Willing. All of these models have been applied on three different account types: private transaction accounts, savings accounts and corporate savings accounts.</p><p>The results are that, due to the current market, the static replicating portfolio does not achieve the desired results. Furthermore, the best volume model for the data provided is a SARIMA model, meaning the effect of the exogenous variables are seemingly already embedded in the lagged volume. Finally, the liquidity risk results are plausible and thus deemed satisfactory.</p>
----------------------------------------------------------------------
In diva2:1293627 Note: no full text in DiVA

abstract is:
<p>Numerous civil and military applications are based on the detection of light in the infraredspectrum : night-vision goggle, thermography or spatial imaging. These complex and specializedapplications require a reliable and high performing technology of photodetectors. Acurrent trend is to increase throughput and detector size. One particular step in the process,is an annealing under saturating mercury pressure of the wafers. The experimental knowledgeof the annealing is insufficient, but it is also a crucial step : during the process, the samplesneed to remain inside their stability domain at all time, so they do not deteriorate.The goal of this thesis was to investigate numerically the annealing process under saturatingmercury pressure and extract information that were not accessible with experimentalmeasurements.We incrementally developed a model and refined it until it fitted well the experimentalmeasurements. We were then able to extract meaningful data : we discovered that the experimentalmeasurements were biased, that the cold point regulating the mercury pressure wasnot maintained at the right spot, and we brought to light that there were inhomogeneitiesbetween and along the substrates. Even though we found out these problems, we checked thatthe samples remained inside the stability diagram during the whole process.Finally, these results were transmitted to help establish the specifications of new and largerfurnaces.</p>

corrected abstract:
<p>Numerous civil and military applications are based on the detection of light in the infrared spectrum : night-vision goggle, thermography or spatial imaging. These complex and specialized applications require a reliable and high performing technology of photo detectors. A current trend is to increase throughput and detector size. One particular step in the process, is an annealing under saturating mercury pressure of the wafers. The experimental knowledge of the annealing is insufficient, but it is also a crucial step : during the process, the samples need to remain inside their stability domain at all time, so they do not deteriorate. The goal of this thesis was to investigate numerically the annealing process under saturating mercury pressure and extract information that were not accessible with experimental measurements.</p><p>We incrementally developed a model and refined it until it fitted well the experimental measurements. We were then able to extract meaningful data : we discovered that the experimental measurements were biased, that the cold point regulating the mercury pressure was not maintained at the right spot, and we brought to light that there were inhomogeneities between and along the substrates. Even though we found out these problems, we checked that the samples remained inside the stability diagram during the whole process. Finally, these results were transmitted to help establish the specifications of new and larger furnaces.</p>
----------------------------------------------------------------------
In diva2:1212110 abstract is:
<p>To maintain solvency intimes of severe economic downturns banks and financialinstitutions keep capital cushions that reflect the risks in the balance sheet.Broadly,how much capital that is being held is a combination of external requirementsfromregulators and internal assessments of credit risk. We discuss alternatives totheBasel Pillar II capital add-on based on multi-factor models for held capitaland howthese can be applied so that only concentration (or sector) risk affects theoutcome,even in a portfolio with prominent idiosyncratic risk. Further, the stabilityandreliability of these models are evaluated. We found that this idiosyncraticrisk canefficiently be removed both on a sector and a portfolio level and that themulti-factormodels tested converge.We introduce two new indices based on Risk Weighted Assets (RI) and EconomicCapital (EI). Both show the desired effect of an intuitive dependence on the PDand LGD. Moreover, EI shows a dependence on the inter-sector correlation. Inthesample portfolio, we show that the high concentration in one sector could be(better)justified by these methods when the low average LGD and PD of this sector weretaken into consideration.</p>

corrected abstract:
<p>To maintain solvency in times of severe economic downturns banks and financial institutions keep capital cushions that reflect the risks in the balance sheet. Broadly, how much capital that is being held is a combination of external requirements from regulators and internal assessments of credit risk. We discuss alternatives to the Basel Pillar II capital add-on based on multi-factor models for held capital and how these can be applied so that only concentration (or sector) risk affects the outcome, even in a portfolio with prominent idiosyncratic risk. Further, the stability and reliability of these models are evaluated. We found that this idiosyncratic risk can efficiently be removed both on a sector and a portfolio level and that the multi-factor models tested converge.</p><p>We introduce two new indices based on Risk Weighted Assets (RI) and Economic Capital (EI). Both show the desired effect of an intuitive dependence on the PD and LGD. Moreover, EI shows a dependence on the inter-sector correlation. In the sample portfolio, we show that the high concentration in one sector could be (better) justified by these methods when the low average LGD and PD of this sector were taken into consideration.</p>
----------------------------------------------------------------------
In diva2:1133508 abstract is:
<p>GKN Aerospace Engine Systems specializes in large load carrying static structuresfor aero engines and is, as part of a light weight strategy, developing design andmanufacturing technology to be able to complement the current metallic product offerwith composite fan guide vane structures. Fan vanes in modern engines are structuraland to meet the requirements for low weight in the aircraft industry, it is necessary todesign the vane as a sandwich structure. The objective with this thesis is to investigateand model the heating effect in the polymeric sandwich core during cyclic loadingand to assess the impact on fatigue life from heating both during structural testing aswell as in service.To model the heating in a FEM model, the damping in the material is measured withDynamic Mechanical Thermal Analysis (DMTA) and is used together with the cyclicamplitude and frequency to calculate the heating term for each element in the FEmodel. In order to validate the thermal analysis and see the effect of heating on thefatigue properties of the core material, fatigue tests are done at normal testingfrequencies and elevated frequencies with temperature elevations monitored as afunction of time.The predicted heating effect is shown to correlate well with the experimental datawhereas the maximum loading frequency of 40 Hz that could be applied was too lowto give any effect on the fatigue life. It is also shown that the effect of loadingfrequencies in the order of 300 Hz requires the fatigue amplitude to be lower than thefatigue strength to avoid excessive heating effects. For in service loadings, the highcycle fatigue is intermittent and as the heating is slow enough, the conclusion is thatfatigue strength is not affected by heating effects</p>

corrected abstract:
<p>GKN Aerospace Engine Systems specializes in large load carrying static structures for aero engines and is, as part of a light weight strategy, developing design and manufacturing technology to be able to complement the current metallic product offer with composite fan guide vane structures. Fan vanes in modern engines are structural and to meet the requirements for low weight in the aircraft industry, it is necessary to design the vane as a sandwich structure. The objective with this thesis is to investigate and model the heating effect in the polymeric sandwich core during cyclic loading and to assess the impact on fatigue life from heating both during structural testing as well as in service.</p><p>To model the heating in a FEM model, the damping in the material is measured with Dynamic Mechanical Thermal Analysis (DMTA) and is used together with the cyclic amplitude and frequency to calculate the heating term for each element in the FE model. In order to validate the thermal analysis and see the effect of heating on the fatigue properties of the core material, fatigue tests are done at normal testing frequencies and elevated frequencies with temperature elevations monitored as a function of time.</p><p>The predicted heating effect is shown to correlate well with the experimental data whereas the maximum loading frequency of 40 Hz that could be applied was too low to give any effect on the fatigue life. It is also shown that the effect of loading frequencies in the order of 300 Hz requires the fatigue amplitude to be lower than the fatigue strength to avoid excessive heating effects. For in service loadings, the high cycle fatigue is intermittent and as the heating is slow enough, the conclusion is that fatigue strength is not affected by heating effects</p>
----------------------------------------------------------------------
In diva2:1120605 abstract is:
<p>In this project a simulation of multiple quadcopters is created to solvethe problem of exploring and mapping an unknown environment. Thetwo major challengers are allocation of the route of the quadcopters tooptimize the mapping, as well as the merging of the three-dimensionalsub maps to get a final result. Occupancy grid maps will be used forthe allocation task to calculate the probability that a cell is occupiedby an obstacle. A cost and an utility function will also be calculatedfor every cell next to the unexplored areas to make a decisions for theroutes of the quadcopters. The cost makes the quadcopter travel tothe closest unexplored cell that is not occupied with some probabilitywhile the utility function works in a way that several quadcopters willnot travel to the same area. The combination of these two calculatesthe optimal path. The optimal merging of the sub maps is then to findan optimal rigid transformation that optimally aligns two sets of pointsin R3 in a least square sent. To do this an optimal translation andan optimal rotation using Singular Value Decomposition is calculated.Finally, algorithms which can be implemented in any suitable softwarefor simulation and demonstration of how this things can be handledin an practical environment, is described.</p>


corrected abstract:
<p>In this project a simulation of multiple quadcopters is created to solve the problem of exploring and mapping an unknown environment. The two major challengers are allocation of the route of the quadcopters to optimize the mapping, as well as the merging of the three-dimensional sub maps to get a final result. Occupancy grid maps will be used for the allocation task to calculate the probability that a cell is occupied by an obstacle. A cost and an utility function will also be calculated for every cell next to the unexplored areas to make a decisions for the routes of the quadcopters. The cost makes the quadcopter travel to the closest unexplored cell that is not occupied with some probability while the utility function works in a way that several quadcopters will not travel to the same area. The combination of these two calculates the optimal path. The optimal merging of the sub maps is then to find an optimal rigid transformation that optimally aligns two sets of points in ℝ<sup>3</sup> in a least square sent. To do this an optimal translation and an optimal rotation using Singular Value Decomposition is calculated. Finally, algorithms which can be implemented in any suitable software for simulation and demonstration of how this things can be handled in an practical environment, is described.</p>
----------------------------------------------------------------------
In diva2:1110830 - missing ligature in title:
"CFD modelling of ski-jump spillway in Stornnforsen"
==>
"CFD modelling of ski-jump spillway in Storfinnforsen"


abstract is:
<p>Traditionally when designing dams and spillways, experiments in physical scale models are conductedin order to determine whether or not the design fulls it purpose, and to identify and avoid undesiredproblems, such as unfavourable ow patterns and unwanted water splatter. Physical models can oftenbe expensive and time consuming to build, and often suer from scale eects that in uence the results.Uniper and Vattenfall have recently done experiments in a physical 1:50 scale model of the dam Stornnforsen,in order to test new solutions for the energy dissipation from the spillways. One of the testedsolutions is a ip bucket at the bottom of the right surface spillway.In this project the same solution is numerically modelled, using the CFD software ANSYS® Fluent®,and the results are compared to those from the experiments. The CFD simulations are done both in fullscale and model scale, in order to identify potential scale eects. The aspects that are compared are theheight and length of the jet from the ip bucket.In addition to the CFD simulations, the height and length are also calculated semi-empirically, usingtwo dierent methods.Altogether the results correspond quite well to the experimental values. Some possible scale eects areobserved, where the jet from the full scale simulation is more dispersed than the jet from the model scalesimulation. The jet trajectory from the full scale simulation is also a bit lower than the jet from theexperiments and model scale simulations.The grid independence for the simulations is not quite satisfactory, and the grid should be rened to getmore reliable results. Due to lack of time and computational power any further grid renement is notdone in this project.</p>

corrected abstract:
<p>Traditionally when designing dams and spillways, experiments in physical scale models are conducted in order to determine whether or not the design fulfils it purpose, and to identify and avoid undesired problems, such as unfavourable flow patterns and unwanted water splatter. Physical models can often be expensive and time consuming to build, and often suffer from scale effects that influence the results.</p><p>Uniper and Vattenfall have recently done experiments in a physical 1:50 scale model of the dam Storfinnforsen, in order to test new solutions for the energy dissipation from the spillways. One of the tested solutions is a flip bucket at the bottom of the right surface spillway.</p><p>In this project the same solution is numerically modelled, using the CFD software ANSYS® Fluent®, and the results are compared to those from the experiments. The CFD simulations are done both in full scale and model scale, in order to identify potential scale effects. The aspects that are compared are the height and length of the jet from the flip bucket.</p><p>In addition to the CFD simulations, the height and length are also calculated semi-empirically, using two different methods.</p><p>Altogether the results correspond quite well to the experimental values. Some possible scale effects are observed, where the jet from the full scale simulation is more dispersed than the jet from the model scale simulation. The jet trajectory from the full scale simulation is also a bit lower than the jet from the experiments and model scale simulations.</p><p>The grid independence for the simulations is not quite satisfactory, and the grid should be refined to get more reliable results. Due to lack of time and computational power any further grid refinement is not done in this project.</p>
----------------------------------------------------------------------
In diva2:1110762 abstract is:
<p>This paper presents a study on spacecraftRendezvous and Docking (RvD) through a comprehensiveliterature review, in addition to investigate the main phases andpossible control methods during the space rendezvous and docking.It presents a study of different energy efficient far rangerendezvous (e.g. Hoffman, bi-elliptic, phasing maneuver etc.). Aset of formation flying models (i.e. relative navigation) for twospacecraft operating in close proximity are examined. Oneapproach to depict the relative orbit’s dynamics model for closeproximity operations is to mathematically express it by thenonlinear equations of relative motion (NERM), that present thehighest accuracy and are valid for all types of orbit eccentricitiesand separations. Another approach is the Hill-Clohessy-Wiltshire(HCW). This method is only valid for two conditions, when thetarget satellite orbit is near circular and the distance between thechaser and target is small. The dynamics models in this paperdescribe the spacecraft formation flying in both unperturbed andperturbed environment, where only the Earth oblatenessperturbations are being treated.Furthermore, this paper presents a design of a control, guidance,and navigation (GNC) based on the aforementioned dynamicmodel. This will enable the chaser satellite to autonomouslyapproach towards the target satellite during the close proximitynavigation using some control techniques such Linear QuadraticRegulation (LQR) and Linear Quadratic Gaussian (LQG). Thesecontrol techniques aim to reduce both the duration and the ΔVcost of the entire mission.</p>


corrected abstract:
<p>This paper presents a study on spacecraft Rendezvous and Docking (RvD) through a comprehensive literature review, in addition to investigate the main phases and possible control methods during the space rendezvous and docking. It presents a study of different energy efficient far range rendezvous (e.g. Hoffman, bi-elliptic, phasing maneuver etc.). A set of formation flying models (i.e. relative navigation) for two spacecraft operating in close proximity are examined. One approach to depict the relative orbit’s dynamics model for close proximity operations is to mathematically express it by the nonlinear equations of relative motion (NERM), that present the highest accuracy and are valid for all types of orbit eccentricities and separations. Another approach is the Hill-Clohessy-Wiltshire (HCW). This method is only valid for two conditions, when the target satellite orbit is near circular and the distance between the chaser and target is small. The dynamics models in this paper describe the spacecraft formation flying in both unperturbed and perturbed environment, where only the Earth oblateness perturbations are being treated.</p><p>Furthermore, this paper presents a design of a control, guidance, and navigation (GNC) based on the aforementioned dynamic model. This will enable the chaser satellite to autonomously approach towards the target satellite during the close proximity navigation using some control techniques such Linear Quadratic Regulation (LQR) and Linear Quadratic Gaussian (LQG). These control techniques aim to reduce both the duration and the ΔV cost of the entire mission.</p>
----------------------------------------------------------------------
In diva2:937846 abstract is:
<p>Proton radiation therapy allows for delivering a high dose to a well-confinedregion of interest due to the characteristic proton dose deposition. Due to protonrange straggling, anatomic variations in patients and small patient setup errors,treatment plans needs to account for proton range uncertainties of up to 3.5% invivo.Therefore, it is highly desirable to measure the proton range on-line in orderto minimize margins in the treatment plan. Initially, the feasibility of on-linerange monitoring through prompt gamma imaging and Positron EmissionTomography (PET) at different proton energies is evaluated using GEANT4Application for Tomographic Emission (GATE) Monte Carlo (MC) simulations.In the second phase, the performance of a lead knife-edge slit system for promptgamma imaging was evaluated with MC simulations. Results from simulationsindicate that prompt gamma emission and PET isotope production is correlatedwith proton range, with discrete prompt gamma emission lines from Carbon (4.4MeV) showing good correlation. The evaluated system was able to image thepeak gamma emission location at three different slit positions with promisingprecision ± 1 mm, ± 0.7 mm and ± 1.3 mm, and average shifts of -2 mm, -3 mmand -4 mm, respectively. The proton range was resolved with mean profile shiftsof -12 ± 1 mm, -13 ± 0.7 mm and -14 ± 1.3 mm, following prompt gamma crosssectionbehavior with peak emission- and threshold energies. The results providean indication of the potential of the knife-edge slit system and future work willinclude more extensive MC simulations and experimental measurements at the Skandion clinic to determine its clinical validity.</p>

corrected abstract:
<p>Proton radiation therapy allows for delivering a high dose to a well-confined region of interest due to the characteristic proton dose deposition. Due to proton range straggling, anatomic variations in patients and small patient setup errors, treatment plans needs to account for proton range uncertainties of up to 3.5% <em>in vivo</em>. Therefore, it is highly desirable to measure the proton range on-line in order to minimize margins in the treatment plan. Initially, the feasibility of on-line range monitoring through prompt gamma imaging and Positron Emission Tomography (PET) at different proton energies is evaluated using GEANT4 Application for Tomographic Emission (GATE) Monte Carlo (MC) simulations. In the second phase, the performance of a lead knife-edge slit system for prompt gamma imaging was evaluated with MC simulations. Results from simulations indicate that prompt gamma emission and PET isotope production is correlated with proton range, with discrete prompt gamma emission lines from Carbon (4.4 MeV) showing good correlation. The evaluated system was able to image the peak gamma emission location at three different slit positions with promising precision ± 1 mm, ± 0.7 mm and ± 1.3 mm, and average shifts of -2 mm, -3 mm and -4 mm, respectively. The proton range was resolved with mean profile shifts of -12 ± 1 mm, -13 ± 0.7 mm and -14 ± 1.3 mm, following prompt gamma crosssection behavior with peak emission- and threshold energies. The results provide an indication of the potential of the knife-edge slit system and future work will include more extensive MC simulations and experimental measurements at the Skandion clinic to determine its clinical validity.</p>
----------------------------------------------------------------------
In diva2:624028 - missing space in title:
"Pricing With Uncertainty: The impact of uncertainty in the valuation models ofDupire and Black&Scholes"
==>
"Pricing With Uncertainty: The impact of uncertainty in the valuation models of Dupire and Black&Scholes"

abstract is:
<p>Theaim of this master-thesis is to study the impact of uncertainty in the local-and implied volatility surfaces when pricing certain structured products suchas capital protected notes and autocalls. Due to their long maturities, limitedavailability of data and liquidity issue, the uncertainty may have a crucialimpact on the choice of valuation model. The degree of sensitivity andreliability of two different valuation models are studied. The valuation models chosen for this thesis are the local volatility model of Dupire and the implied volatility model of Black&amp;Scholes. The two models are stress tested with varying volatilities within an uncertainty interval chosen to be the volatilities obtained from Bid and Ask market prices. The volatility surface of the Mid market prices is set as the relative reference and then successively scaled up and down to measure the uncertainty.The results indicates that the uncertainty in the chosen interval for theDupire model is of higher order than in the Black&amp;Scholes model, i.e. thelocal volatility model is more sensitive to volatility changes. Also, the pricederived in the Black&amp;Scholes modelis closer to the market price of the issued CPN and the Dupire price is closer tothe issued Autocall. This might be an indication of uncertainty in thecalibration method, the size of the chosen uncertainty interval or the constantextrapolation assumption.A further notice is that the prices derived from the Black&amp;Scholes model areoverall higher than the prices from the Dupire model. Another observation ofinterest is that the uncertainty between the models is significantly greaterthan within each model itself.</p>

corrected abstract:
<p>The aim of this master-thesis is to study the impact of uncertainty in the local- and implied volatility surfaces when pricing certain structured products such as capital protected notes and autocalls. Due to their long maturities, limited availability of data and liquidity issue, the uncertainty may have a crucial impact on the choice of valuation model. The degree of sensitivity and reliability of two different valuation models are studied.</p><p>The valuation models chosen for this thesis are the local volatility model of Dupire and the implied volatility model of Black&amp;Scholes. The two models are stress tested with varying volatilities within an uncertainty interval chosen to be the volatilities obtained from Bid and Ask market prices. The volatility surface of the Mid market prices is set as the relative reference and then successively scaled up and down to measure the uncertainty.</p><p>The results indicates that the uncertainty in the chosen interval for the Dupire model is of higher order than in the BlackScholes model, i.e. the local volatility model is more sensitive to volatility changes. Also, the price derived in the BlackScholes model is closer to the market price of the issued CPN and the Dupire price is closer to the issued Autocall. This might be an indication of uncertainty in the calibration method, the size of the chosen uncertainty interval or the constant extrapolation assumption.</p><p>A further notice is that the prices derived from the Black&amp;Scholes model are overall higher than the prices from the Dupire model. Another observation of interest is that the uncertainty between the models is significantly greater than within each model itself.</p>
----------------------------------------------------------------------
In diva2:408834 - missing space in title:
"Learning collaborators: A study in making experiencebased knowledge conscious"
==>
"Learning collaborators: A study in making experience based knowledge conscious"

abstract is:
<p>The Swedish steel manufacturer Oxelösund AB is a member of the SSAB SwedishSteel Group and is the largest Nordic manufacturer of heavy steel plate. This thesis isabout how selected employees at SSAB Oxelösund AB acquire and consolidate theirknowledge, when it is supposed that they, in the future, will pass their knowledge onwithin the company. The thesis focuses on how employees perceive and gain theirlearning.The employees expresses that their experience transforms into knowledge when theyreflect upon recently perceived events. They describe that learning is a continuousprocess that is achieved over time and that they learn in their meeting with other people.Those employees were consolidating their knowledge when they both formulated andexpressed themselves in words or when they practiced their skills. Thus they haveconfirmed and have made their knowledge visible, primarily for themselves but also fortheir surroundings.For SSAB Oxelösund AB, to gradually will be able to transfer the employees experienceand knowledge to other employees, it is essential to give them time to reflect upon theirown knowledge. They must become aware of both the knowledge they possess andwhat skills are to be shared, to succeed in sharing their knowledge.</p>

corrected abstract:
<p>The Swedish steel manufacturer Oxelösund AB is a member of the SSAB Swedish Steel Group and is the largest Nordic manufacturer of heavy steel plate. This thesis is about how selected employees at SSAB Oxelösund AB acquire and consolidate their knowledge, when it is supposed that they, in the future, will pass their knowledge on within the company. The thesis focuses on how employees perceive and gain their learning.</p><p>The employees expresses that their experience transforms into knowledge when they reflect upon recently perceived events. They describe that learning is a continuous process that is achieved over time and that they learn in their meeting with other people. Those employees were consolidating their knowledge when they both formulated and expressed themselves in words or when they practiced their skills. Thus they have confirmed and have made their knowledge visible, primarily for themselves but also for their surroundings.</p><p>For SSAB Oxelösund AB, to gradually will be able to transfer the employees experience and knowledge to other employees, it is essential to give them time to reflect upon their own knowledge. They must become aware of both the knowledge they possess and what skills are to be shared, to succeed in sharing their knowledge.</p>
----------------------------------------------------------------------
In diva2:408832 abstract is:
<p>In an ever more competitive environment Sandvik Materials Technology,SMT, runs a large-scale program to improve their processes for product development.The purpose with the program is to increase the company’s profitability by increasingthe number of new products in their product portfolio. The strategies SMT is usingfor reaching their goals are to change the way they work with product development byreducing lead time and learning how to prioritize products with the highestprofitability. In the R &amp; D Department the work of improvement pursue by adaptingLean R &amp; D to their way of working. Lean R &amp; D encourages working withcontinuous improvements and eliminating waste.The main purpose for this Master’s Thesis is to develop didactic support to make iteasier to implement Lean in SMT R &amp; D department. With use of interviews,observations and taking part in education at SMT and at Sandholm Associates has thisMaster’s Thesis resulted in modifying of the internal education material at Sandvik anddevelopment of a concept of Stickers for the computer screen. The main product ofthis Master’s Thesis project is a Lean game adapted for Sandvik SMT R &amp; Ddepartment.This report contains suggestion for Sandvik how to improve the communication ofknowledge between their departments with purpose to reduce external consultationsand to achieve a more pleasant working environment.</p>

corrected abstract:
<p>In an ever more competitive environment Sandvik Materials Technology, SMT, runs a large-scale program to improve their processes for product development. The purpose with the program is to increase the company’s profitability by increasing the number of new products in their product portfolio. The strategies SMT is using for reaching their goals are to change the way they work with product development by reducing lead time and learning how to prioritize products with the highest profitability. In the R &amp; D Department the work of improvement pursue by adapting Lean R &amp; D to their way of working. Lean R &amp; D encourages working with continuous improvements and eliminating waste.</p><p>The main purpose for this Master’s Thesis is to develop didactic support to make it easier to implement Lean in SMT R &amp; D department. With use of interviews, observations and taking part in education at SMT and at Sandholm Associates has this Master’s Thesis resulted in modifying of the internal education material at Sandvik and development of a concept of Stickers for the computer screen. The main product of this Master’s Thesis project is a Lean game adapted for Sandvik SMT R &amp; D department.</p><p>This report contains suggestion for Sandvik how to improve the communication of knowledge between their departments with purpose to reduce external consultations and to achieve a more pleasant working environment.</p>
----------------------------------------------------------------------
In diva2:1892290 abstract is:
<p>Characterizing the optical scattering properties of materials is usefulfor many purposes. There are also multiple ways to describe scatteringfrom matter. One way, from a macroscopic point of view, is to measurethe bidirectional scatter distribution function (BSDF) with an opticalscattering measurement device, such as a scatterometer. On the otherend, mathematical descriptions such as Mie theory provide an exact de-scription of scattering from well-defined objects. In this work we considerboth theoretical and practical aspects of optical scattering measurementsin the development of an optical scattering measurement device. Firstwe study electromagnetic scattering theory, especially Mie theory. Thenwe study optical models, from a geometrical-optics point of view, of thedetector system of an optical scattering measurement device. We developand configure a detector system and also a BSDF mockup device withwhich we perform some BSDF measurements on a sample of milk. Wecompare the experimental result with the result from a simulation of anoptical scattering model for milk based on Mie theory. The results seemto stay within the same order of magnitude but the uncertainties of themeasurements and the experimental setup are too high to find any clearcorrespondence between the results.</p>

corrected abstract:
<p>Characterizing the optical scattering properties of materials is useful for many purposes. There are also multiple ways to describe scattering from matter. One way, from a macroscopic point of view, is to measure the bidirectional scatter distribution function (BSDF) with an optical scattering measurement device, such as a scatterometer. On the other end, mathematical descriptions such as Mie theory provide an exact description of scattering from well-defined objects. In this work we consider both theoretical and practical aspects of optical scattering measurements in the development of an optical scattering measurement device. First we study electromagnetic scattering theory, especially Mie theory. Then we study optical models, from a geometrical-optics point of view, of the detector system of an optical scattering measurement device. We develop and configure a detector system and also a BSDF mockup device with which we perform some BSDF measurements on a sample of milk. We compare the experimental result with the result from a simulation of an optical scattering model for milk based on Mie theory. The results seem to stay within the same order of magnitude but the uncertainties of the measurements and the experimental setup are too high to find any clear correspondence between the results.</p>
----------------------------------------------------------------------
In diva2:1880821 abstract is:
<p>This thesis investigates an aerodynamic optimization of front lower wishbones on a FormulaStudent car using CFD simulations and windtunnel testing. The aim of the study is to de-termine weather a geometric optimization of wishbones is feasable and could enhance theaerodynamic performance of the vehicle.Siemens Star-CCM+ is utilized for the CFD simulations with focus on iterative design im-provments to optimize the initial generic airfoil shape. The CFD model is based on the modelprovided by KTH Formula Student. The model was also developed further in the initial stagesof the study. Turbulance transition model was investigated as well as the turbulance model toaccurately capture the near wall flow dynamics.In order to validate the results from the CFD model a set of windtunnel tests where doneinvolving a 13 scale model of the vehicle at various yaw angles.Some key findings from the analysis indicate that significant changes in the aerodynamicalcharacteristics can be made by making relatively small changes to the geometry of the vehicle.An increase of 2,1% was achieved which closely correlate to the accuired data which indic-ated a 2,2% increase.The optimization process highlighted the importance of developing CFD optimization tech-niques as they can make large contributions to the aerodynamic characteristics of vehicles.This can be used in order to increase the performance of of vehicles in motorsport or reducedrag in everyday vehicles to reduce the energy consumtion.</p>

corrected abstract:
<p>This thesis investigates an aerodynamic optimization of front lower wishbones on a Formula Student car using CFD simulations and wind tunnel testing. The aim of the study is to determine weather a geometric optimization of wishbones is feasable and could enhance the aerodynamic performance of the vehicle.</p><p>Siemens Star-CCM+ is utilized for the CFD simulations with focus on iterative design improvments to optimize the initial generic airfoil shape. The CFD model is based on the model provided by KTH Formula Student. The model was also developed further in the initial stages of the study. Turbulance transition model was investigated as well as the turbulance model to accurately capture the near wall flow dynamics.</p><p>In order to validate the results from the CFD model a set of wind tunnel tests where done involving a &frac13; scale model of the vehicle at various yaw angles.</p><p>Some key findings from the analysis indicate that significant changes in the aerodynamical characteristics can be made by making relatively small changes to the geometry of the vehicle. An increase of 2,1% was achieved which closely correlate to the accuired data which indicated a 2,2% increase.</p><p>The optimization process highlighted the importance of developing CFD optimization techniques as they can make large contributions to the aerodynamic characteristics of vehicles. This can be used in order to increase the performance of of vehicles in motorsport or reduce drag in everyday vehicles to reduce the energy consumtion.</p>

Note: I've used a vulgar fraction /13 rather than 1 over 3.


<math xmlns="http://www.w3.org/1998/Math/MathML" alttext="\frac{1}{3}" display="inline"><mfrac><mn>1</mn><mn>3</mn></mfrac></math>

corrected abstract with MathML:
<p>This thesis investigates an aerodynamic optimization of front lower wishbones on a Formula Student car using CFD simulations and wind tunnel testing. The aim of the study is to determine weather a geometric optimization of wishbones is feasable and could enhance the aerodynamic performance of the vehicle.</p><p>Siemens Star-CCM+ is utilized for the CFD simulations with focus on iterative design improvments to optimize the initial generic airfoil shape. The CFD model is based on the model provided by KTH Formula Student. The model was also developed further in the initial stages of the study. Turbulance transition model was investigated as well as the turbulance model to accurately capture the near wall flow dynamics.</p><p>In order to validate the results from the CFD model a set of wind tunnel tests where done involving a <math xmlns="http://www.w3.org/1998/Math/MathML" alttext="\frac{1}{3}" display="inline"><mfrac><mn>1</mn><mn>3</mn></mfrac></math> scale model of the vehicle at various yaw angles.</p><p>Some key findings from the analysis indicate that significant changes in the aerodynamical characteristics can be made by making relatively small changes to the geometry of the vehicle. An increase of 2,1% was achieved which closely correlate to the accuired data which indicated a 2,2% increase.</p><p>The optimization process highlighted the importance of developing CFD optimization techniques as they can make large contributions to the aerodynamic characteristics of vehicles. This can be used in order to increase the performance of of vehicles in motorsport or reduce drag in everyday vehicles to reduce the energy consumtion.</p>


----------------------------------------------------------------------
In diva2:1843030 abstract is:
<p>In this thesis, the main focus is to formulate and test a suitable model forexogenous fault detection in swarms containing unmanned aerial vehicles(UAVs), which are aerial autonomous systems. FOI Swedish DefenseResearch Agency provided the thesis project and research question. Inspiredby previous work, the implementation use behavioral feature vectors (BFVs)to simulate the movements of the UAVs and to identify anomalies in theirbehaviors.</p><p>The chosen algorithm for fault detection is the density-based cluster analysismethod known as the Local Outlier Factor (LOF). This method is built on thek-Nearest Neighbor(kNN) algorithm and employs densities to detect outliers.In this thesis, it is implemented to detect faulty agents within the swarm basedon their behavior. A confusion matrix and some associated equations are usedto evaluate the accuracy of the method.</p><p>Six features are selected for examination in the LOF algorithm. The firsttwo features assess the number of neighbors in a circle around the agent,while the others consider traversed distance, height, velocity, and rotation.Three different fault types are implemented and induced in one of the agentswithin the swarm. The first two faults are motor failures, and the last oneis a sensor failure. The algorithm is successfully implemented, and theevaluation of the faults is conducted using three different metrics. Several setsof experiments are performed to assess the optimal value for the LOF thresholdand to understand the model’s performance. The thesis work results in a strongLOF value which yields an acceptable F1 score, signifying the accuracy of theimplementation is at a satisfactory level.</p>

corrected abstract:
<p>In this thesis, the main focus is to formulate and test a suitable model for exogenous fault detection in swarms containing unmanned aerial vehicles (UAVs), which are aerial autonomous systems. FOI Swedish Defense Research Agency provided the thesis project and research question. Inspired by previous work, the implementation use behavioral feature vectors (BFVs) to simulate the movements of the UAVs and to identify anomalies in their behaviors.</p><p>The chosen algorithm for fault detection is the density-based cluster analysis method known as the Local Outlier Factor (LOF). This method is built on the k-Nearest Neighbor(kNN) algorithm and employs densities to detect outliers. In this thesis, it is implemented to detect faulty agents within the swarm based on their behavior. A confusion matrix and some associated equations are used to evaluate the accuracy of the method.</p><p>Six features are selected for examination in the LOF algorithm. The first two features assess the number of neighbors in a circle around the agent, while the others consider traversed distance, height, velocity, and rotation. Three different fault types are implemented and induced in one of the agents within the swarm. The first two faults are motor failures, and the last one is a sensor failure. The algorithm is successfully implemented, and the evaluation of the faults is conducted using three different metrics. Several sets of experiments are performed to assess the optimal value for the LOF threshold and to understand the model’s performance. The thesis work results in a strong LOF value which yields an acceptable F1 score, signifying the accuracy of the implementation is at a satisfactory level.</p>
----------------------------------------------------------------------
In diva2:1823869 abstract is:
<p>The purpose of this study was twofold: The first part was about analyzing possiblecorrelations between various weather factors and the demand for bike-sharing.The aim of the study was to investigate how the relationship between theseexplanatory variables and the response variable looks like. To investigate this,regression analysis was used where a multivariate linear model was built basedon real data. The study was based on the city of Seoul, where the city’s bike-sharing and weather data were used. What the study found was that all weatherfactors were statistically significant with the demand for bike-sharing and therelationship between them was exponential. The relationship was positive forthe weather factors temperature and visibility while the remaining, i.e. weatherfactors, humidity, wind speed, solar radiation, rain and snowfall, had a negativerelationship. The strongest relationship were temperature, humidity and rainwhile the weakest were wind speed, snowfall and visibility.</p><p>In the second part of the study a market analysis for bike-sharing was carried out.Also in this part, the city of Seoul was in focus. Within this market analysis, amongother things, the market situation, competing services, strengths and weaknessesswere analyzed. The conclusion based on this analysis is that the market for bike-sharing is complex and includes many factors. It has also shown that a city likeSeoul has succeeded in introducing bike-sharing despite many challenges. Finally,the study has shown that bike-sharing is a growing market that will become evenmore relevant in the future.</p>

corrected abstract:
<p>The purpose of this study was twofold: The first part was about analyzing possible correlations between various weather factors and the demand for bike-sharing. The aim of the study was to investigate how the relationship between these explanatory variables and the response variable looks like. To investigate this, regression analysis was used where a multivariate linear model was built based on real data. The study was based on the city of Seoul, where the city’s bike-sharing and weather data were used. What the study found was that all weather factors were statistically significant with the demand for bike-sharing and the relationship between them was exponential. The relationship was positive for the weather factors temperature and visibility while the remaining, i.e. weather factors, humidity, wind speed, solar radiation, rain and snowfall, had a negative relationship. The strongest relationship were temperature, humidity and rain while the weakest were wind speed, snowfall and visibility.</p><p>In the second part of the study a market analysis for bike-sharing was carried out. Also in this part, the city of Seoul was in focus. Within this market analysis, among other things, the market situation, competing services, strengths and weaknessess were analyzed. The conclusion based on this analysis is that the market for bike-sharing is complex and includes many factors. It has also shown that a city like Seoul has succeeded in introducing bike-sharing despite many challenges. Finally, the study has shown that bike-sharing is a growing market that will become even more relevant in the future.</p>
----------------------------------------------------------------------
In diva2:1816745 abstract is:
<p>The use of continuously reinforced thermoplastics have increased in recent years andoffer some significant advantages over their thermoset counterparts. The utilizationof these materials is however still limited due to the labour intensive processingand manufacturing. In the first part of this thesis, the aim is to investigate howcontinuously reinforced thermoplastics can be used in high volume manufacturingof complex components, specifically when combined with injection moulding. Thesecond part will attempt to develop a conceptual manufacturing process and designof a demonstrator part and perform a structural analysis of the component.To answer the first question, an extensive literature study on continuous fibre materialsand thermoplastics was conducted along with research on established and newlydeveloped manufacturing methods such as automated tape laying and fibre placement,tailored fibre placement and 3D printing combined with injection overmoulding. Theconceptual manufacturing process of the demonstrator part was then based on thisresearch and the design was modeled in using finite element analysis.The results shows that continuously reinforced thermoplastics can be used forhigh volume manufacturing of complex components when combined with injectionmoulding. While some of the processing methods are still in an early developmentstage, the techniques have been tested and implemented. The structural analysis of thedemonstrator part shows that the design can withstand the maximum external loadsthus and provide proof of concept.</p>

corrected abstract:
<p>The use of continuously reinforced thermoplastics have increased in recent years and offer some significant advantages over their thermoset counterparts. The utilization of these materials is however still limited due to the labour intensive processing and manufacturing. In the first part of this thesis, the aim is to investigate how continuously reinforced thermoplastics can be used in high volume manufacturing of complex components, specifically when combined with injection moulding. The second part will attempt to develop a conceptual manufacturing process and design of a demonstrator part and perform a structural analysis of the component.</p><p>To answer the first question, an extensive literature study on continuous fibre materials and thermoplastics was conducted along with research on established and newly developed manufacturing methods such as automated tape laying and fibre placement, tailored fibre placement and 3D printing combined with injection overmoulding. The conceptual manufacturing process of the demonstrator part was then based on this research and the design was modeled in using finite element analysis.</p><p>The results shows that continuously reinforced thermoplastics can be used for high volume manufacturing of complex components when combined with injection moulding. While some of the processing methods are still in an early development stage, the techniques have been tested and implemented. The structural analysis of the demonstrator part shows that the design can withstand the maximum external loads thus and provide proof of concept.</p>
----------------------------------------------------------------------
In diva2:1799888 abstract is:
<p>Self propulsion modelling is important in order to accurately simulate ships and submarinesusing Computational Fluid Dynamics (CFD). However, fully resolved simulations of hull andpropeller geometries are computationally heavy and time consuming. As such there is a greatinterest in lower order CFD models of propellers. This work investigates three lower ordermodels of a non-cavitating generic submarine propeller (INSEAN E1619) in OpenFOAM. Themodels investigated are Actuator Disk (AD). Rotor Disk (RD) and Actuator Line Model (ALM).The AD model applies a momentum change based on propeller performance coefficients overa disc cell set. The RD uses Blade Element Method (BEM) to calculate a more realistic thrustdistribution over the disk. Finally the ALM applies BEM over seven rotating lines within the cellset disc. The source code to the RD model was modified according to suggestions provided fromearlier studies on the model. The ALM used was originally designed for turbines which wasrectified by changing the force projection vectors in the source code to model propellers instead.There was not enough published data to directly utilize BEM on the E1619 propeller, thus thedata was generated by conducting 2D simulations on every element. The simulations were setup to replicate results provided in earlier works with higher order models in order to compareboth quantitative and qualitative results. It was found the ALM matched the reference databest out of the models tested in this work. The RD was qualitatively similar to the time averageof the ALM fields but numerically inaccurate. The AD results were poor, both quantitativelyand qualitatively.</p><p> </p>

corrected abstract:
<p>Self propulsion modelling is important in order to accurately simulate ships and submarines using Computational Fluid Dynamics (CFD). However, fully resolved simulations of hull and propeller geometries are computationally heavy and time consuming. As such there is a great interest in lower order CFD models of propellers. This work investigates three lower order models of a non-cavitating generic submarine propeller (INSEAN E1619) in OpenFOAM. The models investigated are Actuator Disk (AD). Rotor Disk (RD) and Actuator Line Model (ALM). The AD model applies a momentum change based on propeller performance coefficients over a disc cell set. The RD uses Blade Element Method (BEM) to calculate a more realistic thrust distribution over the disk. Finally the ALM applies BEM over seven rotating lines within the cell set disc. The source code to the RD model was modified according to suggestions provided from earlier studies on the model. The ALM used was originally designed for turbines which was rectified by changing the force projection vectors in the source code to model propellers instead. There was not enough published data to directly utilize BEM on the E1619 propeller, thus the data was generated by conducting 2D simulations on every element. The simulations were set up to replicate results provided in earlier works with higher order models in order to compare both quantitative and qualitative results. It was found the ALM matched the reference data best out of the models tested in this work. The RD was qualitatively similar to the time average of the ALM fields but numerically inaccurate. The AD results were poor, both quantitatively and qualitatively.</p>
----------------------------------------------------------------------
In diva2:1774376 abstract is:
<p>Predicting wall heat flux accurately in wall-bounded turbulent flows is critical fora variety of engineering applications, including thermal management systems andenergy-efficient designs. Traditional methods, which rely on expensive numericalsimulations, are hampered by increasing complexity and extremly high computationcost. Recent advances in deep neural networks (DNNs), however, offer an effectivesolution by predicting wall heat flux using non-intrusive measurements derivedfrom off-wall quantities. This study introduces a novel approach, the convolution-compacted vision transformer (ViT), which integrates convolutional neural networks(CNNs) and ViT to predict instantaneous fields of wall heat flux accurately based onoff-wall quantities including velocity components at three directions and temperature.Our method is applied to an existing database of wall-bounded turbulent flowsobtained from direct numerical simulations (DNS). We first conduct an ablationstudy to examine the effects of incorporating convolution-based modules into ViTarchitectures and report on the impact of different modules. Subsequently, we utilizefully-convolutional neural networks (FCNs) with various architectures to identify thedistinctions between FCN models and the convolution-compacted ViT. Our optimizedViT model surpasses the FCN models in terms of instantaneous field predictions,learning turbulence statistics, and accurately capturing energy spectra. Finally, weundertake a sensitivity analysis using a gradient map to enhance the understandingof the nonlinear relationship established by DNN models, thus augmenting theinterpretability of these models</p>

corrected abstract:
<p>Predicting wall heat flux accurately in wall-bounded turbulent flows is critical for a variety of engineering applications, including thermal management systems and energy-efficient designs. Traditional methods, which rely on expensive numerical simulations, are hampered by increasing complexity and extremly high computation cost. Recent advances in deep neural networks (DNNs), however, offer an effective solution by predicting wall heat flux using non-intrusive measurements derived from off-wall quantities. This study introduces a novel approach, the convolution-compacted vision transformer (ViT), which integrates convolutional neural networks (CNNs) and ViT to predict instantaneous fields of wall heat flux accurately based on off-wall quantities including velocity components at three directions and temperature. Our method is applied to an existing database of wall-bounded turbulent flows obtained from direct numerical simulations (DNS). We first conduct an ablation study to examine the effects of incorporating convolution-based modules into ViT architectures and report on the impact of different modules. Subsequently, we utilize fully-convolutional neural networks (FCNs) with various architectures to identify the distinctions between FCN models and the convolution-compacted ViT. Our optimized ViT model surpasses the FCN models in terms of instantaneous field predictions, learning turbulence statistics, and accurately capturing energy spectra. Finally, we undertake a sensitivity analysis using a gradient map to enhance the understanding of the nonlinear relationship established by DNN models, thus augmenting the interpretability of these models.</p>
----------------------------------------------------------------------
In diva2:1766585 abstract is:
<p>Cytokines are small, secreted proteins that are important for cell signalling in theimmune system. Interferon gamma (IFN-γ) is one of the most potent cytokines thatnatural killer (NK) cells of the innate immune system secrete with both antiviral,antibacterial, and antitumoral activity. Analysis of NK cells, such as that of secretionof IFN-γ, is important for studying the immune response to cancer and for developingeffective immunotherapies. In this master thesis project, a method was developedfor determining the amount of IFN-γ secreted by NK cells when being confinedwith cancer cells in deep microwells. Antibody-coated microbeads was used tocapture secreted IFN-γ, which was fluorescently labeled and detected by imaging usingfluorescence microscopy. Microbead seeding into small microwells for single cellassays and into large microwells for embedding of beads into 3D tumor spheroidswas investigated. An analytical model based on experimental standard curves wasdeveloped for straightforward quantification of the amount of bound IFN-γ, with ademonstrated detection down to 2.10−18 moles per bead. The detection of IFN-γ wasevaluated for primary NK cells stimulated by PMA/ionomycin for different incubationtimes. The secretion rate of IFN-γ by IL-2 activated NK cells under PMA/ionomycinstimulation was estimated at 184 molecules per second. IFN-γ detection was alsoevaluated in cell cytotoxicity assays where NK cells were confined over time togetherwith cancer cells in microwells. Both assays showed a successful detection of IFN-γ secretion, demonstrating the potential of the developed method for immune cellanalysis.</p>

corrected abstract:
<p>Cytokines are small, secreted proteins that are important for cell signalling in the immune system. Interferon gamma (IFN-γ) is one of the most potent cytokines that natural killer (NK) cells of the innate immune system secrete with both antiviral, antibacterial, and antitumoral activity. Analysis of NK cells, such as that of secretion of IFN-γ, is important for studying the immune response to cancer and for developing effective immunotherapies. In this master thesis project, a method was developed for determining the amount of IFN-γ secreted by NK cells when being confined with cancer cells in deep microwells. Antibody-coated microbeads was used to capture secreted IFN-γ, which was fluorescently labeled and detected by imaging using fluorescence microscopy. Microbead seeding into small microwells for single cell assays and into large microwells for embedding of beads into 3D tumor spheroids was investigated. An analytical model based on experimental standard curves was developed for straightforward quantification of the amount of bound IFN-γ, with a demonstrated detection down to 2.10<sup>−18</sup> moles per bead. The detection of IFN-γ was evaluated for primary NK cells stimulated by PMA/ionomycin for different incubation times. The secretion rate of IFN-<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&gamma;</mi></math> by IL-2 activated NK cells under PMA/ionomycin stimulation was estimated at 113 molecules per second. IFN-<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>&gamma;</mi></math> detection was also evaluated in cell cytotoxicity assays where NK cells were confined over time together with cancer cells in microwells. Both assays showed a successful detection of IFN-γ secretion, demonstrating the potential of the developed method for immune cell analysis.</p>

Note that two instance of gamma (γ) use math mode - to match the use of the CMMI font used for these two instances in the original manuscript. Note that the same two instances occur in the Swedish version of the abstract. This same math style is used in the list of acronyms. If one looks at lines 2 and 3 on page 8, you can ee both forms being used! The first with the math gamma, is referring to the protean, while the second is referring to the interferon gamma cytokine. I've sent a note to Prof. Björn Önfelt - who was the examiner for this thesis to ask about this.
----------------------------------------------------------------------
In diva2:1707770 abstract is:
<p>Lithium-ion batteries (LiBs) are the most popular choice in the shift towardselectrification due to their high volumetric energy and power density. An importantaspect to study is the effect of swelling on the mechanical performance of LiBsas it plays an important role in determining the forces in the battery module.During charge/discharge a battery cell swells/shrinks and over the lifetime of thebattery, swelling becomes permanent. The swelling increases with cycling that causesincreasing forces in the module. Excessive pressure generated due to cycling in themodule may electrically short the cells and/or cause mechanical damage to the cells.Compression pads placed between cells in the battery module absorb the swelling. Thematerial properties and size of the compression pads used influence the module forcesat End of Life (EoL).In this study, a 1D phenomenological model is built to predict the swelling forces. Themodel differs from others in literature in a way that the swelling forces are predictedwith cycling rather than State of Charge (SoC) and a stress-strain based constitutivemodel is used rather than a spring model. A process to eliminate the need for multipletests is also proposed in the thesis to predict swelling forces for different compressionpads and preloads.The proposed model is relatively simple and can improve existing battery managementsystems by predicting the swelling and the magnitude of swelling forces for differentcompression pads and preloads during the operational life of the battery.</p>

corrected abstract:
<p>Lithium-ion batteries (LiBs) are the most popular choice in the shift towards electrification due to their high volumetric energy and power density. An important aspect to study is the effect of swelling on the mechanical performance of LiBs as it plays an important role in determining the forces in the battery module. During charge/discharge, a battery cell swells/shrinks and over the lifetime of the battery, swelling becomes permanent. The swelling increases with cycling that causes increasing forces in the module. Excessive pressure generated due to cycling in the module may electrically short the cells and/or cause mechanical damage to the cells. Compression pads placed between cells in the battery module absorb the swelling. The material properties and size of the compression pads used influence the module forces at End of Life (EoL).</p><p>In this study, a 1D phenomenological model is built to predict the swelling forces. The model differs from others in literature in a way that the swelling forces are predicted with cycling rather than State of Charge (SoC) and a stress-strain based constitutive model is used rather than a spring model. A process to eliminate the need for multiple tests is also proposed in the thesis to predict swelling forces for different compression pads and preloads.</p><p>The proposed model is relatively simple and can improve existing battery management systems by predicting the swelling and the magnitude of swelling forces for different compression pads and preloads during the operational life of the battery.</p>
----------------------------------------------------------------------
In diva2:1698347 abstract is:
<p>An aerodynamic shape optimization procedure was performed on a low observable engineintake duct. The intake duct was fixed in its throat and aerodynamic interface plane (AIP)sections, while leaving up to 7 design parameters free to deformation in the centroid curveand mid section profile. The optimization setup consisted of an optimizer block implementedin MATLAB, where the NSGA-II optimization algorithm was implemented, and a simulationblock using computational fluid dynamics (CFD). The objective functions for the optimizationprocess were the pressure recovery and the DC60 distortion coefficient in the AIP section.In total, four optimizations with gradually increasing degrees of deformation were conducted.The first optimization process was a validation case, performed on a test duct design, whilethe remaining optimizations were performed using a duct designed by the Swedish DefenceResearch Agency (FOI) as a starting point, for cruise and take-off conditions.</p><p>The connection of NSGA-II and the CFD setup proved useful, as the distortion was decreasedby up to 52.8% relative the original value while keeping the pressure recovery within 0.06% ofthe original duct. The algorithm was successful in finding an improvement for both consideredoperating conditions, with the largest improvement for the cruise case. In total 975 duct designswere evaluated in the four processes, using a uniform inflow boundary condition on a boundaryextruded one meter from the throat of the intake duct.</p><p>The importance of the handling of non-converged solutions in the automated optimizationprocess was also pointed out, as an oscillating solution affected the third optimization, therebyrendering that solution useless.</p>

corrected abstract:
<p>An aerodynamic shape optimization procedure was performed on a low observable engine intake duct. The intake duct was fixed in its throat and aerodynamic interface plane (AIP) sections, while leaving up to 7 design parameters free to deformation in the centroid curve and mid section profile. The optimization setup consisted of an optimizer block implemented in MATLAB, where the NSGA-II optimization algorithm was implemented, and a simulation block using computational fluid dynamics (CFD). The objective functions for the optimization process were the pressure recovery and the DC60 distortion coefficient in the AIP section. In total, four optimizations with gradually increasing degrees of deformation were conducted. The first optimization process was a validation case, performed on a test duct design, while the remaining optimizations were performed using a duct designed by the Swedish Defence Research Agency (FOI) as a starting point, for cruise and take-off conditions.</p><p>The connection of NSGA-II and the CFD setup proved useful, as the distortion was decreased by up to 52.8% relative the original value while keeping the pressure recovery within 0.06% of the original duct. The algorithm was successful in finding an improvement for both considered operating conditions, with the largest improvement for the cruise case. In total 975 duct designs were evaluated in the four processes, using a uniform inflow boundary condition on a boundary extruded one meter from the throat of the intake duct.</p><p>The importance of the handling of non-converged solutions in the automated optimization process was also pointed out, as an oscillating solution affected the third optimization, thereby rendering that solution useless.</p>
----------------------------------------------------------------------
In diva2:1698159  - Note: no full text in DiVA

abstract is:
<p>Improving the performance of aircraft is a challenging topic and a burning issue in a societywho strives to reduce as much as possible CO2 emissions. A better performance could leadto a reduction of the fuel burnt during flight and thus to a more sustainable aviation. It istherefore crucial to have a clear understanding of all the parameters that have an impact onaircraft performance.This project aims at estimating two unknown parameters for aircraft manufacturers: theCost Index and the take-off weight. These two parameters contain sensitive information forairlines, which prevents them from sharing the parameters with manufacturers. However,these parameters have a strong impact on aircraft performance and also need to be estimated.The performance parameters of in-service aircraft have been collected for several flights andstudied. Reverse engineering is used on each high speed phase of the flight (climb, cruise,descent) to deduce the Cost Index and the weight. The whole methodology has been testedon simulated flights where the Cost Index and weight were known.The Cost Index and take-off weight have been estimated for simulated flights. Regardingreal flights, it has been possible to either deduce both parameters for most of the flights, orone of them or otherwise a relationship between both parameters has been found.</p>

corrected abstract:
<p>Improving the performance of aircraft is a challenging topic and a burning issue in a society who strives to reduce as much as possible CO<sub>2</sub> emissions. A better performance could lead to a reduction of the fuel burnt during flight and thus to a more sustainable aviation. It is therefore crucial to have a clear understanding of all the parameters that have an impact on aircraft performance. This project aims at estimating two unknown parameters for aircraft manufacturers: the Cost Index and the take-off weight. These two parameters contain sensitive information for airlines, which prevents them from sharing the parameters with manufacturers. However, these parameters have a strong impact on aircraft performance and also need to be estimated. The performance parameters of in-service aircraft have been collected for several flights and studied. Reverse engineering is used on each high speed phase of the flight (climb, cruise, descent) to deduce the Cost Index and the weight. The whole methodology has been tested on simulated flights where the Cost Index and weight were known. The Cost Index and take-off weight have been estimated for simulated flights. Regarding real flights, it has been possible to either deduce both parameters for most of the flights, or one of them or otherwise a relationship between both parameters has been found.</p>
----------------------------------------------------------------------
In diva2:1673875 - unnecessary hyphen in title:
"Fluid flow features in swirl injectors for ethanol fueled rocket: - Analysis using computational fluid dynamics"
===>
"Fluid flow features in swirl injectors for ethanol fueled rocket: Analysis using computational fluid dynamics"

abstract is:
<p>A swirl injector for a rocket engine being developed by \emph{AESIR} (Association of EngineeringStudents in Rocketry) was simulated with different geometric parameters. The swirl injector is usedto atomize the ethanol used as fuel and to create a spray that mixes well with the oxidizer withinthe combustion chamber. Inlet slot angle (90, 75, 60 and 45 degrees), swirl chamber length (15, 20and 25 mm) and outlet orifice diameter (3, 6 and 9 mm) were examined.Previous studies in swirl injectors show that CFD can be used to analyze the flow in such aninjector, furthermore theoretical models exist that can predict some of the general characteristicsof the flow. Previous studies have also simulated transient behavior and flow features effectingbreakup of fuel flowing through a swirl injector.A steady state simulation using Volume of Fluid (VOF) multiphase modeling and $k$-$\omega$ \emph{SST}turbulence modeling was used to simulate the swirl injector intended for the rocket engine. It wasfound that a wider outlet orifice would give a wider cone angle of spray. This is desirable in thecurrent rocket engine design as it will promote greater mixing of fuel and oxidizer higher up in thecombustion chamber. No large variances was observed when different inlet slot angles was simulated. Ashorter swirl chamber length reduced the amount of losses in energy due to viscous forces. The flowafter the outlet orifice was not simulated so the effect of turbulence kinetic energy and energylosses outside of the swirl injector have not been analyzed, previous studies have indicated thatturbulent kinetic energy does have an effect on the breakup and atomization of the fuel.It was concluded that using a wider outlet orifice of 9 mm gave the best results out of the differentgeometric parameters analyzed and the swirl chamber length should be a short as possible.</p><p> </p>

corrected abstract:
<p>A swirl injector for a rocket engine being developed by <em>AESIR</em> (Association of Engineering Students in Rocketry) was simulated with different geometric parameters. The swirl injector is used to atomize the ethanol used as fuel and to create a spray that mixes well with the oxidizer within the combustion chamber. Inlet slot angle (90, 75, 60 and 45 degrees), swirl chamber length (15, 20 and 25 mm) and outlet orifice diameter (3, 6 and 9 mm) were examined.</p><p>Previous studies in swirl injectors show that CFD can be used to analyze the flow in such an injector, furthermore theoretical models exist that can predict some of the general characteristics of the flow. Previous studies have also simulated transient behavior and flow features effecting breakup of fuel flowing through a swirl injector.</p><p>A steady state simulation using Volume of Fluid (VOF) multiphase modeling and <em>k-&omega;</em> <em>SST</em> turbulence modeling was used to simulate the swirl injector intended for the rocket engine. It was found that a wider outlet orifice would give a wider cone angle of spray. This is desirable in the current rocket engine design as it will promote greater mixing of fuel and oxidizer higher up in the combustion chamber. No large variances was observed when different inlet slot angles was simulated. A shorter swirl chamber length reduced the amount of losses in energy due to viscous forces. The flow after the outlet orifice was not simulated so the effect of turbulence kinetic energy and energy losses outside of the swirl injector have not been analyzed, previous studies have indicated that turbulent kinetic energy does have an effect on the breakup and atomization of the fuel.</p><p>It was concluded that using a wider outlet orifice of 9 mm gave the best results out of the different geometric parameters analyzed and the swirl chamber length should be a short as possible.</p>
----------------------------------------------------------------------
In diva2:1671528 - Note: no full text in DiVA
abstract is:
<p>Studies estimate that as much as 95% of people would benefit from eyeglasses.This means having eye tracking work well with eyeglasses is key to making thetechnology universal. Unfortunately, eyeglasses make eye tracking challenging.One common issue is the large reflections caused by anti-reflective coatedeyeglasses. Though they may have good performance in the visible spectrum,anti-reflective coatings are highly reflective under near-infrared illumination.In this thesis, we investigate using circularly polarized illumination to combatthe size and intensity of the glasses reflections.</p><p>Polarization is one of the only properties of light that we cannot perceivewith the naked eye. Thus, we have built an imaging polarimeter that allows usto easily visualize polarization state. We then use a well-known optical system,the optical isolator, to separate the glasses reflections. We also looked at howincidence angle and specific coating affected our ability to reduce reflections.</p><p>Our results show that we can reduce the intensity of glasses reflectionsby 99%. However, in general, it is difficult to reduce the glasses reflectionswithout sacrificing the some of the integrity of the glint. The various coatingsalso behaved differently under circularly polarized illumination. There arespecific cases where glasses reflections were reduced, while the glint wasonly minimally affected. This thesis also serves as a jumping-off point usepolarization in future iterations of eye trackers.</p>

corrected abstract:
<p>Studies estimate that as much as 95% of people would benefit from eyeglasses. This means having eye tracking work well with eyeglasses is key to making the technology universal. Unfortunately, eyeglasses make eye tracking challenging. One common issue is the large reflections caused by anti-reflective coated eyeglasses. Though they may have good performance in the visible spectrum, anti-reflective coatings are highly reflective under near-infrared illumination. In this thesis, we investigate using circularly polarized illumination to combat the size and intensity of the glasses reflections.</p><p>Polarization is one of the only properties of light that we cannot perceive with the naked eye. Thus, we have built an imaging polarimeter that allows us to easily visualize polarization state. We then use a well-known optical system, the optical isolator, to separate the glasses reflections. We also looked at how incidence angle and specific coating affected our ability to reduce reflections.</p><p>Our results show that we can reduce the intensity of glasses reflections by 99%. However, in general, it is difficult to reduce the glasses reflections without sacrificing the some of the integrity of the glint. The various coatings also behaved differently under circularly polarized illumination. There are specific cases where glasses reflections were reduced, while the glint was only minimally affected. This thesis also serves as a jumping-off point use polarization in future iterations of eye trackers.</p>
----------------------------------------------------------------------
In diva2:1648276 - Note: no full text in DiVA
abstract is:
<p>In This Master thesis report, three-dimensional Direct Numerical Simulations are usedto characterize natural convection in a Rayleigh-Bénard cavity with roughness on thebottom hot plate made of square based blocks. The cavity is filled with water and DNSwere conducted for different Rayleigh numbers from 1.105 up to 1.1010.As stated in the literature, three different regimes of heat transfer are identified in thiscavity. The regime II present a particularly intense heat transfer efficiency compared toregime III. We highlight all along different approaches, differences between the two platesthat might explain such difference in heat transfer regimes. A particular attention will beput on plumes as they carry heat.To do so, a first analysis is performed to find typical regions of plumes formation alongthe z-axis of the cavity . An extended database is hence computed for a finer vision ofthe inner and the outer mixing zone. A first approach of the identification of plumes isperformed temporally, regime II is thus characterized with the heat advected by plumes,and explain the difference in heat transfer regimes.A proof of concept is then done to extract plumes spatially using Topology DataAnalysis for a finer identification and finer statistics.Several results allowed us to understand the differences in heat transfer from the tworegimes, like the scaling with the convective velocity and temperature. In an other hand,the plumes analysis highlighted how heat was exchanged between the two plates.</p>


corrected abstract:
<p>In This Master thesis report, three-dimensional Direct Numerical Simulations are used to characterize natural convection in a Rayleigh-Bénard cavity with roughness on the bottom hot plate made of square based blocks. The cavity is filled with water and DNS were conducted for different Rayleigh numbers from 1.105 up to 1.1010.As stated in the literature, three different regimes of heat transfer are identified in this cavity. The regime II present a particularly intense heat transfer efficiency compared to regime III. We highlight all along different approaches, differences between the two plates that might explain such difference in heat transfer regimes. A particular attention will be put on plumes as they carry heat. To do so, a first analysis is performed to find typical regions of plumes formation along the z-axis of the cavity . An extended database is hence computed for a finer vision of the inner and the outer mixing zone. A first approach of the identification of plumes is performed temporally, regime II is thus characterized with the heat advected by plumes, and explain the difference in heat transfer regimes. A proof of concept is then done to extract plumes spatially using Topology DataAnalysis for a finer identification and finer statistics. Several results allowed us to understand the differences in heat transfer from the two regimes, like the scaling with the convective velocity and temperature. In an other hand, the plumes analysis highlighted how heat was exchanged between the two plates.</p>
----------------------------------------------------------------------
In diva2:1640036 - Note: no full text in DiVA
abstract is:
<p>The focus of this thesis is the effect of planetary curvature on the heat transfer efficiencyaccross latitudes in planetary atmospheres and oceans. We investigate both theoreticallyand numerically a physically-based parametrization of baroclinic turbulence inthe Boussinesq Eady model to include variations of the Coriolis parameter with latitude.In this model, a rapidly rotating density-stratified fluid is subjected to a meridional temperaturegradient in thermal wind balance with a uniform vertically sheared zonal flowand the effect of planetary curvature is captured by the parameter β.A normal mode projection of the Eady model with β was inconclusive to properlydescribe meridional heat transfers and the zonal structures usually observed in planetaryflows. However, the DNS solver CORAL allows us to perform 3D high-Reynolds numericalsimulations to seek an extension of the ’vortex-gas’ scaling theory for baroclinic turbulence.Planetary curvature reduces heat transfer between latitudes through the emergenceof coherent zonal structures while the flow remain mainly quasi-geostrophic. The meridionalbuoyancy flux displays the same functional dependence on the control parametersthan for the two-layer model within the framework of quasi-geostrophy.With similar arguments than for the Eady problem, it is shown that in a perturbativefashion for small β, the vertical profiles of meridional buoyancy flux are no longer depthinvariant.The flux decreases at least exponentially with height. The buoyancy transportis shown to be along mean isopycnals, whereas potential vorticity is transported onlyalong instantaneous isopycnals. Overall, the vortex-gas theory and its extension to theβ-plane lead to good predictions for heat transfer in the quasi-geostrophy limit for 3Dflows and weak β. The theory becomes less precise as we increase β.</p>

corrected abstract:
<p>The focus of this thesis is the effect of planetary curvature on the heat transfer efficiency across latitudes in planetary atmospheres and oceans. We investigate both theoretically and numerically a physically-based parametrization of baroclinic turbulence inthe Boussinesq Eady model to include variations of the Coriolis parameter with latitude. In this model, a rapidly rotating density-stratified fluid is subjected to a meridional temperature gradient in thermal wind balance with a uniform vertically sheared zonal flowand the effect of planetary curvature is captured by the parameter β.A normal mode projection of the Eady model with β was inconclusive to properly describe meridional heat transfers and the zonal structures usually observed in planetary flows. However, the DNS solver CORAL allows us to perform 3D high-Reynolds numerical simulations to seek an extension of the ’vortex-gas’ scaling theory for baroclinic turbulence. Planetary curvature reduces heat transfer between latitudes through the emergence of coherent zonal structures while the flow remain mainly quasi-geostrophic. The meridional buoyancy flux displays the same functional dependence on the control parameters than for the two-layer model within the framework of quasi-geostrophy. With similar arguments than for the Eady problem, it is shown that in a perturbative fashion for small β, the vertical profiles of meridional buoyancy flux are no longer depth invariant. The flux decreases at least exponentially with height. The buoyancy transport is shown to be along mean isopycnals, whereas potential vorticity is transported only along instantaneous isopycnals. Overall, the vortex-gas theory and its extension to theβ-plane lead to good predictions for heat transfer in the quasi-geostrophy limit for 3Dflows and weak β. The theory becomes less precise as we increase β.</p>
----------------------------------------------------------------------
In diva2:1511011 - Note: no full text in DiVA
abstract is:
<p>X-ray phase-contrast imaging is a powerful technique, which, due to its high sensitivity to density alterations, allows carrying out high-contrast imaging of soft tissue. The lungs are especially suitable for being examined with phasecontrast due to their compartments being largely air-filled. Thus, phasecontrastimaging allows to visualize structures down to the alveolar level, providing microscopic information about lung anatomy and functionality. Asthe lungs are moving when conducting in-vivo experiments, the aim of thisthesis is to develop and implement respiration-gating into an already existinglaboratory small animal X-ray imaging setup to improve the image resolutionby compensating for respiratory motion. Additionally, the radiation dose formice is aimed to be suitable for longitudinal in-vivo imaging.</p><p>A prospective gating approach was implemented into the imaging setup by allowing the different hardware components (e.g., camera, rotation stage) to communicate with each other. A pressure sensor was used to register respiratorymotion and to trigger a microcontroller when exceeding a certainrespiratory threshold. The micro-controller then transmits pulses to the devicesin the imaging setup, triggering them sequentially and thus allowingto perform a tomographic scan. In addition to programming the microcontroller,a graphical user interface (GUI) was designed. The system was then tested, optimized and evaluated using two different phantoms developed inthe course of this thesis, whereupon in-vivo experiments on free breathingmice were conducted.</p><p>The evaluation of the gating implementation on the phantoms indicates a significantimprovement of the image quality and thus a successful compensation for respiratory motion. This effective execution could subsequently be appliedto free breathing mice, allowing to identify airway features in the murine lungdown to the 50-100 μm range. By only allowing X-ray exposure during imageacquisition, the dose could be reduced to a level where repeated imaging ofthe same mouse becomes possible.</p>

corrected abstract:
<p>X-ray phase-contrast imaging is a powerful technique, which, due to its high sensitivity to density alterations, allows carrying out high-contrast imaging of soft tissue. The lungs are especially suitable for being examined with phasecontrast due to their compartments being largely air-filled. Thus, phasecontrastimaging allows to visualize structures down to the alveolar level, providing microscopic information about lung anatomy and functionality. Asthe lungs are moving when conducting in-vivo experiments, the aim of this thesis is to develop and implement respiration-gating into an already existing laboratory small animal X-ray imaging setup to improve the image resolution by compensating for respiratory motion. Additionally, the radiation dose for mice is aimed to be suitable for longitudinal in-vivo imaging.</p><p>A prospective gating approach was implemented into the imaging setup by allowing the different hardware components (e.g., camera, rotation stage) to communicate with each other. A pressure sensor was used to register respiratory motion and to trigger a microcontroller when exceeding a certain respiratory threshold. The micro-controller then transmits pulses to the devices in the imaging setup, triggering them sequentially and thus allowing to perform a tomographic scan. In addition to programming the microcontroller, a graphical user interface (GUI) was designed. The system was then tested, optimized and evaluated using two different phantoms developed inthe course of this thesis, where upon in-vivo experiments on free breathing mice were conducted.</p><p>The evaluation of the gating implement ation on the phantoms indicates a significant improvement of the image quality and thus a successful compensation for respiratory motion. This effective execution could subsequently be applied to free breathing mice, allowing to identify airway features in the murine lung down to the 50-100 μm range. By only allowing X-ray exposure during image acquisition, the dose could be reduced to a level where repeated imaging of the same mouse becomes possible.</p>
----------------------------------------------------------------------
In diva2:1509441 - Note: no full text in DiVA
abstract is:
<p>development of innovative Adaptive Optics (AO) systems for the next generation of gianttelescopes and instruments poses new instrumental questions essential to achieving this astrophysicalgoal. This type of observation indeed involves the use of dedicated instruments,combining a large telescope, an extreme adaptive optics (XAO) system, a coronograph physicallyremoving the photons from the star, optimized instrumentation, but also an efficientdata processing methodology.A new wavefront measurement method, based on a self-referenced Mach-Zehner interfermoter,a Mach-Zehnder interferometer that creates its own reference wave using a spatialfilter referred to as pinehole, is being studied at CRAL. In order to study its behavior andhelp predict the results that could be achieved, a simulation of this interferometer has beenrealized. Developing an analytical model, we can argue that this method is supposed to beworking for small values of phase and pinehole radius. The first part of the simulation isto verify whether or not this statement is true. To simulate the atmospheric aberrations,Zernike polynomials are used (only the ones optically interpreted as tip/tilt and defocus).Then, in a second time, a process of modulation is implemented to try and enhance theperformance of the simulation towards the high phase values.The results obtained with this simulation are analyzed both qualitatively, with visual comparisonat each step of the process, and quantitatively by computing the Strehl ratio, ratiobetween the maximum intensity of the measured PSF and the PSF as if it were perfect withoutaberrations. Hence, the evolution of the Strehl ratio with respect to both the variationof the pinehole radius and the variation of the amplitude of the aberration can be plotted.</p>

corrected abstract:
<p>Development of innovative Adaptive Optics (AO) systems for the next generation of giant telescopes and instruments poses new instrumental questions essential to achieving this astrophysical goal. This type of observation indeed involves the use of dedicated instruments, combining a large telescope, an extreme adaptive optics (XAO) system, a coronograph physically removing the photons from the star, optimized instrumentation, but also an efficient data processing methodology. A new wavefront measurement method, based on a self-referenced Mach-Zehner interferometer, a Mach-Zehnder interferometer that creates its own reference wave using a spatial filter referred to as pinhole, is being studied at CRAL. In order to study its behavior and help predict the results that could be achieved, a simulation of this interferometer has been realized. Developing an analytical model, we can argue that this method is supposed to be working for small values of phase and pinhole radius. The first part of the simulation is to verify whether or not this statement is true. To simulate the atmospheric aberrations, Zernike polynomials are used (only the ones optically interpreted as tip/tilt and defocus). Then, in a second time, a process of modulation is implemented to try and enhance theperformance of the simulation towards the high phase values. The results obtained with this simulation are analyzed both qualitatively, with visual comparison at each step of the process, and quantitatively by computing the Strehl ratio, ratio between the maximum intensity of the measured PSF and the PSF as if it were perfect without aberrations. Hence, the evolution of the Strehl ratio with respect to both the variation of the pinhole radius and the variation of the amplitude of the aberration can be plotted.</p>
----------------------------------------------------------------------
In diva2:1285492 abstract is:
<p>There is a need for renewable energy sources that can replace the non-renewable energy sourcesthat we use today. This is on the agenda as one of the United Nations sustainable developmentgoals. Embracing new technologies is addressed as one of the ways of achieving aordable,reliable, sustainable and modern energy for all. Oshore wind power has great potential as anenergy source, and development of the oating solutions is of special importance.In this report, key design parameters of oating oshore wind turbines are identied based ona literature study on research projects as well as on-going test turbines and wind farms. Thekey design parameters should be used for determining the type of technology suitable for aproject, as well as for guidance in the design phase.Based on the key design parameters, a conceptual design of a semisubmersible substructurehas been made for the DTU 10 MW reference wind turbine for a site outside the island ofBarra, west of Scotland. The substructure is a three column semisubmersible connected witha closed shape pontoon and no bracing. The wind turbine is placed on top of one of the threecolumns to reduce number of columns and utilize more of the structure.Variation of the column diameter and distance between the columns has been studied to ndsuitable main dimensions. Mass estimations has been made and the required amount of ballasthas been calculated for a set of combinations to select a conguration for further analysis.Hydrostatic and hydrodynamic analysis has been performed on the design to understand itscharacteristics in the ocean environment. Intact stability is considered in the hydrostatic analysis,and the hydrodynamic analysis includes a study of the motions, loads and accelerationsof the structure.</p>

corrected abstract:
<p>There is a need for renewable energy sources that can replace the non-renewable energy sources that we use today. This is on the agenda as one of the United Nations sustainable development goals. Embracing new technologies is addressed as one of the ways of achieving affordable, reliable, sustainable and modern energy for all. Offshore wind power has great potential as an energy source, and development of the floating solutions is of special importance.</p><p>In this report, key design parameters of floating offshore wind turbines are identified based on a literature study on research projects as well as on-going test turbines and wind farms. The key design parameters should be used for determining the type of technology suitable for a project, as well as for guidance in the design phase.</p><p>Based on the key design parameters, a conceptual design of a semisubmersible substructure has been made for the DTU 10 MW reference wind turbine for a site outside the island of Barra, west of Scotland. The substructure is a three column semisubmersible connected with a closed shape pontoon and no bracing. The wind turbine is placed on top of one of the three columns to reduce number of columns and utilize more of the structure.</p><p>Variation of the column diameter and distance between the columns has been studied to find suitable main dimensions. Mass estimations has been made and the required amount of ballast has been calculated for a set of combinations to select a configuration for further analysis. Hydrostatic and hydrodynamic analysis has been performed on the design to understand its characteristics in the ocean environment. Intact stability is considered in the hydrostatic analysis, and the hydrodynamic analysis includes a study of the motions, loads and accelerations of the structure.</p>
----------------------------------------------------------------------
In diva2:1249681 abstract is:
<p>This master thesis uses applied mathematicalstatistics to analyse purchase behaviour based on customer data of the Swedishbrand Indiska. The aim of the study is to build a model that can helppredicting the sales quantities of different product classes and identify whichfactors are the most significant in the different models and furthermore, tocreate an algorithm that can provide suggested product combinations in thepurchasing process. Generalized linear models with a Negative binomial distributionare applied to retrieve the predicted sales quantity. Moreover, conditionalprobability is used in the algorithm which results in a product recommendationengine based on the calculated conditional probability that the suggestedcombinations are purchased.From the findings, it can be concluded that all variables considered in themodels; original price, purchase month, colour, cluster, purchase country andchannel are significant for the predicted outcome of the sales quantity foreach product class. Furthermore, by using conditional probability andhistorical sales data, an algorithm can be constructed which createsrecommendations of product combinations of either one or two products that canbe bought together with an initial product that a customer shows interest in.  </p>

corrected abstract:
<p>This master thesis uses applied mathematical statistics to analyse purchase behaviour based on customer data of the Swedish brand Indiska. The aim of the study is to build a model that can help predicting the sales quantities of different product classes and identify which factors are the most significant in the different models and furthermore, to create an algorithm that can provide suggested product combinations in the purchasing process. Generalized linear models with a Negative binomial distribution are applied to retrieve the predicted sales quantity. Moreover, conditional probability is used in the algorithm which results in a product recommendation engine based on the calculated conditional probability that the suggested combinations are purchased.</p><p>From the findings, it can be concluded that all variables considered in the models; original price, purchase month, colour, cluster, purchase country and channel are significant for the predicted outcome of the sales quantity for each product class. Furthermore, by using conditional probability and historical sales data, an algorithm can be constructed which creates recommendations of product combinations of either one or two products that can be bought together with an initial product that a customer shows interest in.</p>
----------------------------------------------------------------------
In diva2:1142918 abstract is:
<p>A video streaming service faces several difficultiesoperating. Hardware is expensive and it is crucial to prioritizecustomers in a way that will make them content with the serviceprovided. That is, deliver a sufficient frame rate and neverallocate too much, essentially waste, resources on a client. Thisallocation has to be done several times per second so readingdata from the client is out of the question, because the systemwould be adapting too slow. This raises the question whether it ispossible to predict the frame rate of a client using only variablesmeasured on the server and if it can be done efficiently. Which itcan [1]. To further build on the work of Yanggratoke et al [1], weevaluated several different machine learning methods on a dataset in terms of performance, training time and dependence on thesize of the data set. Neural networks, having the best adaptingcapabilities, resulted in the best performance but training is moretime consuming than for the linear model. Using neural networksis a good idea when the relationship between input and outputis not linear.</p>

corrected abstract:
<p>A video streaming service faces several difficulties operating. Hardware is expensive and it is crucial to prioritize customers in a way that will make them content with the service provided. That is, deliver a sufficient frame rate and never allocate too much, essentially waste, resources on a client. This allocation has to be done several times per second so reading data from the client is out of the question, because the system would be adapting too slow. This raises the question whether it is possible to predict the frame rate of a client using only variables measured on the server and if it can be done efficiently. Which it can [1]. To further build on the work of Yanggratoke et al [1], we evaluated several different machine learning methods on a data set in terms of performance, training time and dependence on the size of the data set. Neural networks, having the best adapting capabilities, resulted in the best performance but training is more time consuming than for the linear model. Using neural networks is a good idea when the relationship between input and output is not linear.</p>
----------------------------------------------------------------------
In diva2:1142914 abstract is:
<p>Machine learning and neural networks haverecently become hot topics in many research areas. They havealready proved to be useful in the fields of medicine andbiotechnology. In these areas, they can be used to facilitatecomplicated and time consuming analysis processes. Animportant application is image recognition of cells, tumours etc.,which also is the focus of this paper.Our project was to construct both Fully Connected NeuralNetworks and Convolutional Neural Networks with the ability torecognize pictures of muscular stem cells (MuSCs). We wanted toinvestigate if the intensity values in each pixel of the images weresufficient to use as indata for classification.By optimizing the structure of our networks, we obtained goodresults. Using only the pixel values as input, the pictures werecorrectly classified with up to 95.1% accuracy. If the image sizewas added to the indata, the accuracy was as best 97.9 %.The conclusion was that it is sensible and practical to use pixelintensity values as indata to classification programs. Importantrelationships exist and by adding some other easily accessiblecharacteristics, the success rate can be compared to a human’sability to classify these cells.</p>

corrected abstract:
<p>Machine learning and neural networks have recently become hot topics in many research areas. They have already proved to be useful in the fields of medicine and biotechnology. In these areas, they can be used to facilitate complicated and time consuming analysis processes. An important application is image recognition of cells, tumours etc., which also is the focus of this paper.</p><p>Our project was to construct both Fully Connected Neural Networks and Convolutional Neural Networks with the ability to recognize pictures of muscular stem cells (MuSCs). We wanted to investigate if the intensity values in each pixel of the images were sufficient to use as indata for classification.</p><p>By optimizing the structure of our networks, we obtained good results. Using only the pixel values as input, the pictures were correctly classified with up to 95.1% accuracy. If the image size was added to the indata, the accuracy was as best 97.9 %.</p><p>The conclusion was that it is sensible and practical to use pixel intensity values as indata to classification programs. Important relationships exist and by adding some other easily accessible characteristics, the success rate can be compared to a human’s ability to classify these cells.</p>
----------------------------------------------------------------------
In diva2:1088402 abstract is:
<p>This masters thesis has two aims. The first is to assemble literature regarding the calculation of crackwidths and crack control in reinforced concrete structures. The second aim is to develop a calculationmethodology to design required reinforcement from data produced by the calculation programBRIGADE.There are several reasons why cracks appear in reinforced concrete structures. Except for externalloads, they appear as a result of attack from frost, seawater or chemicals. Also shrinkage or changesin temperature as the concrete hardens can cause cracks in the structure. Crack control in structuresis desirable for a range of reasons including aesthetics, life-span and the ability to provide water orgas proofing. Crack widths can be controlled by providing sufficient reinforcement in a structure andby arranging the reinforcement in the way it has most effect. Additionally, while pouring theconcrete, there are ways to affect the number of cracks in a structure. Treating the concrete in thecorrect way after pouring and also the use of chemicals can reduce cracking.This thesis considers the calculation of crack widths according to British Standard, Eurocode, ACI andBBK 04. Through calculation of crack widths according to these four standards, where one parameterat a time is varied, general conclusions can be drawn. Reducing the reinforcement spacing makes amajor difference to the calculated crack width. Also larger bar diameters give better results, while asmaller cover within the boundaries that the standards permit results in no significant effect.</p>

corrected abstract:
<p>This masters thesis has two aims. The first is to assemble literature regarding the calculation of crack widths and crack control in reinforced concrete structures. The second aim is to develop a calculation methodology to design required reinforcement from data produced by the calculation program BRIGADE.</p><p>There are several reasons why cracks appear in reinforced concrete structures. Except for external loads, they appear as a result of attack from frost, seawater or chemicals. Also shrinkage or changes in temperature as the concrete hardens can cause cracks in the structure. Crack control in structures is desirable for a range of reasons including aesthetics, life-span and the ability to provide water or gas proofing. Crack widths can be controlled by providing sufficient reinforcement in a structure and by arranging the reinforcement in the way it has most effect. Additionally, while pouring the concrete, there are ways to affect the number of cracks in a structure. Treating the concrete in the correct way after pouring and also the use of chemicals can reduce cracking.</p><p>This thesis considers the calculation of crack widths according to British Standard, Eurocode, ACI and BBK 04. Through calculation of crack widths according to these four standards, where one parameter at a time is varied, general conclusions can be drawn. Reducing the reinforcement spacing makes a major difference to the calculated crack width. Also larger bar diameters give better results, while a smaller cover within the boundaries that the standards permit results in no significant effect.</p>
----------------------------------------------------------------------
In diva2:1083784 abstract is:
<p>Different aspects of numerical simulations of turbulent flows are assessed by consideringa fully-developed turbulent channel flow that is rotating in the spanwise direction.Differences between differential and explicit algebraic Reynolds-stress models(RSMs) are investigated theoretically and numerically. Simulation results are comparedwith existing DNS-data. Both families of RSMs are demonstrated to achievegood qualitative agreement with the DNS. The results constitutes a demonstration ofthe validity of the so called extended weak-equilibrium assumption for systems witha superimposed solid body rotation. An original derivation, based on sound physicalgrounds, of the extended weak-equilibrium assumption is presented.It is further examined if the roll-cell vortex pattern, that constitutes a secondaryflow field, has an influence on the averaged solutions obtained by application of theReynolds-Averaged Navier-Stokes equations. This is assessed by comparison of resultsobtained by either considering the secondary plane as homogeneous in the spanwisedirection or by accounting for a fully three-dimensional flow field. Simulationsdemonstrate that existence of roll-cells in the latter case yields results that are in closeragreement with DNS-data compared with if they are suppressed as for the former case.Aspects of numerical treatment of explicit source terms are also assessed in theframework of finite volume methods for collocated grids.</p>

corrected abstract:
<p>Different aspects of numerical simulations of turbulent flows are assessed by considering a fully-developed turbulent channel flow that is rotating in the spanwise direction. Differences between differential and explicit algebraic Reynolds-stress models (RSMs) are investigated theoretically and numerically. Simulation results are compared with existing DNS-data. Both families of RSMs are demonstrated to achieve good qualitative agreement with the DNS. The results constitutes a demonstration of the validity of the so called <em>extended</em> weak-equilibrium assumption for systems with a superimposed solid body rotation. An original derivation, based on sound physical grounds, of the <em>extended</em> weak-equilibrium assumption is presented.</p><p>It is further examined if the roll-cell vortex pattern, that constitutes a secondary flow field, has an influence on the averaged solutions obtained by application of the Reynolds-Averaged Navier-Stokes equations. This is assessed by comparison of results obtained by either considering the secondary plane as homogeneous in the spanwise direction or by accounting for a fully three-dimensional flow field. Simulations demonstrate that existence of roll-cells in the latter case yields results that are in closer agreement with DNS-data compared with if they are suppressed as for the former case.</p><p>Aspects of numerical treatment of explicit source terms are also assessed in the framework of finite volume methods for collocated grids.</p>
----------------------------------------------------------------------
In diva2:503959 - Note: no full text in DiVA
abstract is:
<p>In this Master’s thesis a method to automatically classify log files containing recorded vehicle data from Scania trucks is investigated.The files were recordings of data from the on-board controller area network(CAN)-buses and were gathered from field test vehicles during normal operating conditions. All the recordings were triggered by the embedded collision warning system. The test vehicles were equipped with a forward looking radar giving information about the surrounding traffic. Seven classes of traffic situations were proposed as classes to try to detect automatically. These classes were Queue, Vehicle ahead turns left,Vehicle ahead turns right, Roundabout, Catch up, Overtake and Overtakebicycle.Using Matlab, an evaluation tool was made to handle and analyse the log files. The tool converts the log files in all necessary steps in orderto analyse and classify the contents of each file. The results is presented in plots and also stored in a file for later review.To classify the files automatically, three methods have been tested and two of the methods have been used for this task. The first method uses a set of conditions on the signals to determine the class, the secondmethod uses a decision tree to differentiate the classes. A cluster analysisof the log files showed not to be an effective method of classifying thefiles and was therefore not fully developed.The result showed that the classification rules detected over 85%of the 192 available situations compared to the “decision tree”-methodwhich detected a maximum of 75% of the situations. The drawback ofthe first method was that the more correct detections also meant morefalse detections. The specificity was used as a measure of how well the method was able to leave the non-existing situations undetected. Thespecificity for the “classification rules”-method was approximately 85%and for the decision tree the specificity was approximately 95%.The conclusion is that both the “classification rules”- and the “decisiontree”-method can be used to classify the log files, however thefirst method produces more detections but less specific, and vice versafor the latter. The methods can be further developed using more of theavailable signals or by working with the specific conditions used by themethods.</p>

corrected abstract:
<p>In this Master’s thesis a method to automatically classify log files containing recorded vehicle data from Scania trucks is investigated. The files were recordings of data from the on-board controller area network (CAN)-buses and were gathered from field test vehicles during normal operating conditions. All the recordings were triggered by the embedded collision warning system. The test vehicles were equipped with a forward looking radar giving information about the surrounding traffic. Seven classes of traffic situations were proposed as classes to try to detect automatically. These classes were Queue, Vehicle ahead turns left, Vehicle ahead turns right, Roundabout, Catch up, Overtake and Overtake bicycle. Using Matlab, an evaluation tool was made to handle and analyse the log files. The tool converts the log files in all necessary steps in order to analyse and classify the contents of each file. The results is presented in plots and also stored in a file for later review. To classify the files automatically, three methods have been tested and two of the methods have been used for this task. The first method uses a set of conditions on the signals to determine the class, the second method uses a decision tree to differentiate the classes. A cluster analysis of the log files showed not to be an effective method of classifying the files and was therefore not fully developed. The result showed that the classification rules detected over 85% of the 192 available situations compared to the “decision tree”-method which detected a maximum of 75% of the situations. The drawback of the first method was that the more correct detections also meant more false detections. The specificity was used as a measure of how well the method was able to leave the non-existing situations undetected. The specificity for the “classification rules”-method was approximately 85% and for the decision tree the specificity was approximately 95%. The conclusion is that both the “classification rules” - and the “decision tree” - method can be used to classify the log files, however the first method produces more detections but less specific, and vice versa for the latter. The methods can be further developed using more of the available signals or by working with the specific conditions used by themethods.</p>
----------------------------------------------------------------------
In diva2:488441 abstract is:
<p>Optical Coherence Tomography (OCT) is a non-invasive high-resolutionmethod for measuring the reectance of scattering media in 1/2/3D, e.g.skin. The method has been used in a number of dierent medical elds andfor measurement of tissue optical properties.The software developed in this thesis is able to display features hidden ina shadowed volume by adding multiple OCT measurements taken at obliqueangles, a technique here called Multiple-Angle Oblique Optical CoherenceTomography (MAO-OCT).Three dierent objects with were measured at 5 to 9 angles. The measurementswere automatically and manually aligned in the software. They werealso tested with 6 dierent high pass intensity lters (HPIF) and reduced insize using 4 dierent methods to speed up calculations.The software's automatic alignment was tested with one tilted computergenerated test at 9 angles and with 5 dierent shadow strengths.With MAO-OCT it is possible to remove some eects of shadows in OCT,though it comes with a cost of reduced sharpness. The errors depend muchon the dierences in index of refraction in the sample.The software managed to automatically align 90% of the articial measurements,and 60% of the OCT measurements. The shadow strength andthe resize method had no noticeable eect on the automatic alignment of themeasurements.</p>

corrected abstract:
<p>Optical Coherence Tomography (OCT) is a non-invasive high-resolution method for measuring the reflectance of scattering media in 1/2/3D, e.g. skin. The method has been used in a number of different medical fields and for measurement of tissue optical properties.</p><p>The software developed in this thesis is able to display features hidden in a shadowed volume by adding multiple OCT measurements taken at oblique angles, a technique here called Multiple-Angle Oblique Optical Coherence Tomography (MAO-OCT).</p><p>Three different objects with were measured at 5 to 9 angles. The measurements were automatically and manually aligned in the software. They were also tested with 6 different high pass intensity filters (HPIF) and reduced in size using 4 different methods to speed up calculations.</p><p>The software's automatic alignment was tested with one tilted computer generated test at 9 angles and with 5 different shadow strengths.</p><p>With MAO-OCT it is possible to remove some effects of shadows in OCT, though it comes with a cost of reduced sharpness. The errors depend much on the differences in index of refraction in the sample.</p><p>The software managed to automatically align 90% of the artificial measurements, and 60% of the OCT measurements. The shadow strength and the resize method had no noticeable effect on the automatic alignment of the measurements.</p>
----------------------------------------------------------------------
In diva2:408838 - possible duplicate of diva2:408837 - one student is common to both, but the duplicate only lists one of the two students.

abstract is:
<p>This thesis is organized in three different parts. In the first part Ericsson’s methods fordeveloping and deploying the existing knowledge are analyzed. In the second part we analyzethe competence build-up for consultants within a technical domain that is constantly evolving.The third part is an evaluation, on an overview level, of a new organizational concept thatEricsson launched. The concept is a way to globally manage knowledge and competencewithin different technical domains. The concept is called Global Competence Center.The method applied was interviews, as a first step to learn about the subject and also forourselves to get to know the organization in which we conducted our research. Interviewswere also used in order to answer our research questions. The employees gave their view onwhat makes learning more efficient. Examples are hands-on, reality based and problemoriented tasks. This was combined with studies of literature and our own experiences oflearning.Our study shows that a combination of different methods for developing and deployingknowledge and building competence seems to be most suitable. This is based on theinterviewees’ experiences and the principles of a theory about adult learning called andragogy.From the interviews, we also received suggestions about work improvements for theconsultants’ work roles. A competence program was developed for the building ofcompetence amongst consultants. The program uses case studies and mentorship as two of themethods, which both apply to the theory of andragogy.</p>

corrected abstract:
<p>This thesis is organized in three different parts. In the first part Ericsson’s methods for developing and deploying the existing knowledge are analyzed. In the second part we analyze the competence build-up for consultants within a technical domain that is constantly evolving. The third part is an evaluation, on an overview level, of a new organizational concept that Ericsson launched. The concept is a way to globally manage knowledge and competence within different technical domains. The concept is called Global Competence Center.</p><p>The method applied was interviews, as a first step to learn about the subject and also for ourselves to get to know the organization in which we conducted our research. Interviews were also used in order to answer our research questions. The employees gave their view on what makes learning more efficient. Examples are hands-on, reality based and problem oriented tasks. This was combined with studies of literature and our own experiences of learning.</p><p>Our study shows that a combination of different methods for developing and deploying knowledge and building competence seems to be most suitable. This is based on the interviewees’ experiences and the principles of a theory about adult learning called andragogy. From the interviews, we also received suggestions about work improvements for the consultants’ work roles. A competence program was developed for the building of competence amongst consultants. The program uses case studies and mentorship as two of the methods, which both apply to the theory of andragogy.</p>
----------------------------------------------------------------------
In diva2:1880984 abstract is:
<p>The study of human locomotion, known as gait analysis, has for a long time been performed withexpensive equipment in laboratory settings. However, the emergence of machine learning sparkedinterest in integrating this technology in gait analysis, thus simplifying the process. This study’saim is to substitute the pressure insoles used during gait cycle analysis of a walking subject, with amachine learning model.To achieve this, a model based on Long-Short Term Memory networks that predicts vertical groundreaction force based on data from inertial measurement unit sensors was used. This serves as asubstitution for pressure insoles or pressure plates. The model was trained with time series datasetscontaining inertial measurement unit data and corresponding pressure insole data. Subsequently, itwas tested for intersubjective, out-of-sample data.The model was able to capture the periodicity of the gait cycle as well as predict the general shapeof the vertical ground reaction force curves, where the accuracy was quantified using normalisedroot mean squared error. The error was in a range between 17.8% and 13.4% and had an average of15.2%, when tested intersubjectively and out-of-sample. The most significant factor contributing tothe error was the model’s amplitude inaccuracies which was, most likely, due to information beinglost during the processing of the data, as well as simply having an insufficient amount of data.</p>

corrected abstract:
<p>The study of human locomotion, known as gait analysis, has for a long time been performed with expensive equipment in laboratory settings. However, the emergence of machine learning sparked interest in integrating this technology in gait analysis, thus simplifying the process. This study’s aim is to substitute the pressure insoles used during gait cycle analysis of a walking subject, with a machine learning model.</p><p>To achieve this, a model based on Long-Short Term Memory networks that predicts vertical ground reaction force based on data from inertial measurement unit sensors was used. This serves as a substitution for pressure insoles or pressure plates. The model was trained with time series datasets containing inertial measurement unit data and corresponding pressure insole data. Subsequently, it was tested for intersubjective, out-of-sample data.</p><p>The model was able to capture the periodicity of the gait cycle as well as predict the general shape of the vertical ground reaction force curves, where the accuracy was quantified using normalised root mean squared error. The error was in a range between 17.8% and 13.4% and had an average of 15.2%, when tested intersubjectively and out-of-sample. The most significant factor contributing to the error was the model’s amplitude inaccuracies which was, most likely, due to information being lost during the processing of the data, as well as simply having an insufficient amount of data.</p>
----------------------------------------------------------------------
In diva2:1877759 abstract is:
<p>Mobile cellular networks are widely integrated in today’s infrastructure. These networks are constantly evolving and continuously expanding, especially with the introduction of fifth-generation (5G). It is important to ensure the effectiveness of these expansions.Mobile networks consist of a set of radio nodes that are distributed in a geographicalregion to provide connectivity services. Each radio node is served by a set of cells. Thehandover relations between cells is determined by Software features such as AutomaticNeighbor Relations (ANR). The handover relations, also refereed as edges, betweenradio nodes in the mobile network graph are created through historical interactions between User Equipment (UE) and radio nodes. The method has the limitation of not being able to set the edges before the physical hardware is integrated. In this work, we usegraph-based deep learning methods to determine mobility relations (edges), trained onradio node configuration data and a set of reliable relations of ANR in stable networks.The report focuses on measuring the accuracy and precision of different graph baseddeep learning approaches applied to real-world mobile networks. The report considers four models. Our comprehensive experiments on Telecom datasets obtained fromoperational Telecom Networks demonstrate that graph neural network model and multilayer perceptron trained with Binary Cross Entropy (BCE) loss outperform all othermodels. The four models evaluation showed that considering graph structure improveresults. Additionally, the model investigates the use of heuristics to reduce the trainingtime based on distance between radio node to eliminate irrelevant cases. The use ofthese heuristics improved precision and accuracy.</p><p> </p>

corrected abstract:
<p>Mobile cellular networks are widely integrated in today’s infrastructure. These networks are constantly evolving and continuously expanding, especially with the introduction of fifth-generation (5G). It is important to ensure the effectiveness of these expansions.</p><p>Mobile networks consist of a set of radio nodes that are distributed in a geographical region to provide connectivity services. Each radio node is served by a set of cells. The handover relations between cells is determined by Software features such as Automatic Neighbor Relations (ANR). The handover relations, also refereed as edges, between radio nodes in the mobile network graph are created through historical interactions between User Equipment (UE) and radio nodes. The method has the limitation of not being able to set the edges before the physical hardware is integrated. In this work, we use graph-based deep learning methods to determine mobility relations (edges), trained on radio node configuration data and a set of reliable relations of ANR in stable networks. The report focuses on measuring the accuracy and precision of different graph based deep learning approaches applied to real-world mobile networks. The report considers four models. Our comprehensive experiments on Telecom datasets obtained from operational Telecom Networks demonstrate that graph neural network model and multilayer perceptron trained with Binary Cross Entropy (BCE) loss outperform all other models. The four models evaluation showed that considering graph structure improve results. Additionally, the model investigates the use of heuristics to reduce the training time based on distance between radio node to eliminate irrelevant cases. The use of these heuristics improved precision and accuracy.</p>
----------------------------------------------------------------------
In diva2:1832656 abstract is:
<p>This thesis aims to investigate the feasibility of using a Markovian approach toforecast short-term stock market movements. To assist traders in making soundtrading decisions, this study proposes a Markovian model using a selection ofthe latest closing prices. Assuming that each time step in the one-minute timeframe of the stock market is stochastically independent, the model eliminates theimpact of fundamental analysis and creates a feasible Markov model. The modeltreats the stock price’s movement as entirely randomly generated, which allowsfor a more simplified model that can be implemented with ease. The modelis intended to serve as a starting ground for more advanced technical tradingstrategies and act as useful guidance for a short-term trader when combinedwith other resources. The creation of the model involves Laplace smoothing toensure there are no zero-probabilities and calculating the steady-state probabilityvector of the smoothed matrix to determine the predicted direction of the nexttime step. The model will reset daily, reducing the impact of fundamental factorsoccurring outside trading hours and reducing the risk of carrying over bias fromprevious trading day. Any open positions will hence be closed at the end of theday. The study’s purpose is to research and test if a simple forecasting modelbased on Markov chains can serve as a useful tool for forecasting stock prices atshort time intervals. The result of the study shows that a Markov-based tradingstrategy is more profitable than a simple buy-and-hold strategy and that theprediction accuracy of the Markov model is relatively high.</p>

corrected abstract:
<p>This thesis aims to investigate the feasibility of using a Markovian approach to forecast short-term stock market movements. To assist traders in making sound trading decisions, this study proposes a Markovian model using a selection of the latest closing prices. Assuming that each time step in the one-minute time frame of the stock market is stochastically independent, the model eliminates the impact of fundamental analysis and creates a feasible Markov model. The model treats the stock price’s movement as entirely randomly generated, which allows for a more simplified model that can be implemented with ease. The model is intended to serve as a starting ground for more advanced technical trading strategies and act as useful guidance for a short-term trader when combined with other resources. The creation of the model involves Laplace smoothing to ensure there are no zero-probabilities and calculating the steady-state probability vector of the smoothed matrix to determine the predicted direction of the next time step. The model will reset daily, reducing the impact of fundamental factors occurring outside trading hours and reducing the risk of carrying over bias from previous trading day. Any open positions will hence be closed at the end of the day. The study’s purpose is to research and test if a simple forecasting model based on Markov chains can serve as a useful tool for forecasting stock prices at short time intervals. The result of the study shows that a Markov-based trading strategy is more profitable than a simple buy-and-hold strategy and that the prediction accuracy of the Markov model is relatively high.</p>
----------------------------------------------------------------------
In diva2:1781270 abstract is:
<p>According to the paradigm of lambda-CDM cosmology, the stellar halo ofour Galaxy has been built-up over time through the accretion of other galaxiesand star clusters. The remnants of some of these are still observable today asstellar streams, but are typically very faint and difficult to resolve amidst the farmore numerous foreground Milky Way stars. The VelHel-4 stream, discoveredby Helmi et al. [2017], consists of seven members selected based on their energiesand angular momenta. Further studies of these stars has shown evidence ofglobular cluster (GC) abundance patterns, suggesting that the stream originatedfrom a GC progenitor, but a larger sample is needed to verify this signature. Theobjective of this thesis is to find new candidate members of the VelHel-4 stellarstream in order to better characterize its properties and to confirm a possibleGC origin.The preliminary selection of stars was done kinematically, by computing theorbital actions and energies using astrometric data and radial velocities for abright subset of the Gaia DR3 database, and then analyzing the clustering ofstream members in different combinations of action space. The selected samplewas then cleaned by analyzing the positions of these stars in a colour-magnitudediagram. In total, 34 stars were included in the final selection. Follow-up high-resolution spectroscopy of these candidates is needed to study their stellar abun-dances and confirm the possible GC origin of this stream.</p>

corrected abstract:
<p>According to the paradigm of lambda-CDM cosmology, the stellar halo of our Galaxy has been built-up over time through the accretion of other galaxies and star clusters. The remnants of some of these are still observable today as stellar streams, but are typically very faint and difficult to resolve amidst the far more numerous foreground Milky Way stars. The VelHel-4 stream, discovered by Helmi et al. [2017], consists of seven members selected based on their energies and angular momenta. Further studies of these stars has shown evidence of globular cluster (GC) abundance patterns, suggesting that the stream originated from a GC progenitor, but a larger sample is needed to verify this signature. The objective of this thesis is to find new candidate members of the VelHel-4 stellar stream in order to better characterize its properties and to confirm a possible GC origin.</p><p>The preliminary selection of stars was done kinematically, by computing the orbital actions and energies using astrometric data and radial velocities for a bright subset of the Gaia DR3 database, and then analyzing the clustering of stream members in different combinations of action space. The selected sample was then cleaned by analyzing the positions of these stars in a colour-magnitude diagram. In total, 34 stars were included in the final selection. Follow-up high-resolution spectroscopy of these candidates is needed to study their stellar abundances and confirm the possible GC origin of this stream.</p>
----------------------------------------------------------------------
In diva2:1756983 - Note: no full text in DiVA
abstract is:
<p>Companies that are publicly traded can be valued in numerous ways. Thefactors affecting the price of a stock are also numerous. One factor that is ofparticular interest is analysts’ judgements of how a stock is expected to perform,also known as revisions. This report aims to explore if analyst’s revisions havean effect on the price of a stock, and if so, to what degree it does. The reportis confined to the AxJ-market, that is, the Asian financial market excludingJapan.Mathematically, the analysis was performed by creating a regression model.The regressors were 8 separate revision measures, and the response variablewas the change in stock price over a time-period of one year, expressed as apercentage.The results from the report indicate that analysts’ revision indeed have aneffect on the price movement of a stock. For the regressors that were incorpo-rated into the model, the contribution that each regressor had to the model fora given year, varied over the time.As a cautionary note, it should be mentioned that it is extremely difficult tomake correct financial decisions that solely are based on analysts’ revisions, asthere are many other factors that may affect the price movements of a stock.</p>

corrected abstract:
<p>Companies that are publicly traded can be valued in numerous ways. The factors affecting the price of a stock are also numerous. One factor that is of particular interest is analysts’ judgements of how a stock is expected to perform, also known as revisions. This report aims to explore if analyst’s revisions have an effect on the price of a stock, and if so, to what degree it does. The report is confined to the AxJ-market, that is, the Asian financial market excluding Japan. Mathematically, the analysis was performed by creating a regression model. The regressors were 8 separate revision measures, and the response variable was the change in stock price over a time-period of one year, expressed as a percentage. The results from the report indicate that analysts’ revision indeed have an effect on the price movement of a stock. For the regressors that were incorpo-rated into the model, the contribution that each regressor had to the model fora given year, varied over the time. As a cautionary note, it should be mentioned that it is extremely difficult to make correct financial decisions that solely are based on analysts’ revisions, as there are many other factors that may affect the price movements of a stock.</p>
----------------------------------------------------------------------
In diva2:1742205 - Note: no full text in DiVA

abstract is:
<p>Spontaneous parametric down-conversion generation (SPDC) is a resource to producetwin photons, exhibiting two-mode quantum correlations and quantum entanglement,two aspects routinely exploited in many quantum information protocols.Spontaneous parametric generation is a nonlinear phenomenon that requires materialswith high order dielectric susceptibilities to generate efficiently twin photons.In my project, I have studied two integrated optical devices intended to generate twinphotons through SPDC: SiN-based micro-ring resonators and periodically poled lithiumniobate (PPLN) waveguides.In the SiN micro-ring resonators, four-wave mixing (FWM), a third order nonlinearinteraction in SiN is used to produce twin-photons. During my project I characterizedexperimentally the micro-rings by studying their resonances and quantifying their qualityfactors Q. Indeed, high Q resonators are necessary to enhance the efficiency of FWM andproduce high-rate twin photons.In the PPLN waveguides, SPDC is based on a second order nonlinear interaction,where a pump photon is converted into twin photons. During my degree project, Ihave investigated Second Harmonic Generation (SHG) to determine the phase matchingwavelengths of different PPLN waveguides with different lengths and poling periods.Finally, I have characterized the SPDC spectral emission by pumping the PPLN with atunable laser.</p>

corrected abstract:
<p>Spontaneous parametric down-conversion generation (SPDC) is a resource to produce twin photons, exhibiting two-mode quantum correlations and quantum entanglement, two aspects routinely exploited in many quantum information protocols. Spontaneous parametric generation is a nonlinear phenomenon that requires materials with high order dielectric susceptibilities to generate efficiently twin photons. In my project, I have studied two integrated optical devices intended to generate twin photons through SPDC: SiN-based micro-ring resonators and periodically poled lithiumniobate (PPLN) waveguides. In the SiN micro-ring resonators, four-wave mixing (FWM), a third order nonlinear interaction in SiN is used to produce twin-photons. During my project I characterized experimentally the micro-rings by studying their resonances and quantifying their quality factors Q. Indeed, high Q resonators are necessary to enhance the efficiency of FWM and produce high-rate twin photons. In the PPLN waveguides, SPDC is based on a second order nonlinear interaction, where a pump photon is converted into twin photons. During my degree project, I have investigated Second Harmonic Generation (SHG) to determine the phase matching wavelengths of different PPLN waveguides with different lengths and poling periods. Finally, I have characterized the SPDC spectral emission by pumping the PPLN with a tunable laser.</p>
----------------------------------------------------------------------
In diva2:1698340 abstract is:
<p>Noise generation from underwater activities propagates into the marine environment.For marine vessels the propulsion system generates the most noise during itsoperations. Naval vessels that want to operate without being detected want to controlthe sound generating properties of the vessel. To control the sound generatingproperties this project has been looking into the existing propeller of the submergedcraft Carrier Seal that is produced by James Fisher Defense. Then a new and bespokenpropeller has been developed with theories applied to minimize its noise generatingproperties. The properties of the propeller that have been altered is the number ofblades, blade area ratio, pitch and skew angle. These properties have been alteredwith aid of the open-source software for Matlab named Openprop. From the finalpropeller design a prototype was later produced, tested and compared to the existingpropeller of the Seal Carrier. To test and compare these two propellers a test procedurewith inspiration from NATO and the Swedish Defense and Research Agency (FOI) wasdeveloped. The results from the comparison show that the sound pressure level fromthe propeller spectrum could be lowered with 3 dB re 1 μP a for the vessels design speedand several blade tones could be eliminated entirely. Simultaneously the efficiency ofthe vessel is increased throughout its speed range.In conclusions the recommendation to JFD is to change their existing propeller tothis bespoken propeller as it has proven itself to better in every way during thesetrials.</p>

corrected abstract:
<p>Noise generation from underwater activities propagates into the marine environment. For marine vessels the propulsion system generates the most noise during its operations. Naval vessels that want to operate without being detected want to control the sound generating properties of the vessel. To control the sound generating properties this project has been looking into the existing propeller of the submerged craft Carrier Seal that is produced by James Fisher Defense. Then a new and bespoken propeller has been developed with theories applied to minimize its noise generating properties. The properties of the propeller that have been altered is the number of blades, blade area ratio, pitch and skew angle. These properties have been altered with aid of the open-source software for Matlab named Openprop. From the final propeller design a prototype was later produced, tested and compared to the existing propeller of the Seal Carrier. To test and compare these two propellers a test procedure with inspiration from NATO and the Swedish Defense and Research Agency (FOI) was developed. The results from the comparison show that the sound pressure level from the propeller spectrum could be lowered with 3 dB re 1 μP a for the vessels design speed and several blade tones could be eliminated entirely. Simultaneously the efficiency of the vessel is increased throughout its speed range.</p><p>In conclusions the recommendation to JFD is to change their existing propeller to this bespoken propeller as it has proven itself to better in every way during these trials.</p>
----------------------------------------------------------------------
In diva2:1680240 abstract is:
<p>Historically, the earth’s fluctuation between interglacial and glacial climates has been observedto have a period of 105 years [1]. However, simulations of the global average temperature didn’tmanage to reproduce this cycle period until 1982, when Benzi et al. [2] introduced the combinationof long-term variations in incoming solar radiation and stochastic noise in an energy balancemodel. Using an energy balance model means that the change in global average temperature isset as proportional to the difference in ingoing and outgoing energy. The result of the simulationsdemonstrated so-called stochastic resonance, where small stochastic perturbations amplified thepattern of the variation in insolation, causing a pattern of large changes in the global averagetemperature, i.e. changes in the climate. The stochastic perturbations model unpredictable shorttime scale phenomena like the weather. Our study aimed to reproduce the result of Benzi et al.[2] and to investigate the model and its parameters. The presence of a 105-year climatic cycle insimulated data was found. The combination of both noise and varying incoming solar radiationwas necessary to observe the 105-year cycle. The characteristics of the climate cycle pattern did,however, vary greatly depending on the values of constants in the model, illustrating how themodel and constants were imprecise. Therefore, no conclusions can be drawn from this studyabout the earth’s current or future climate. However, the study still confirms that stochasticnoise is an important part of modeling the climate, and manages to simulate the earth’s observed105-year climate cycle.</p>

corrected abstract:
<p>Historically, the earth’s fluctuation between interglacial and glacial climates has been observed to have a period of 10<sup>5</sup> years [1]. However, simulations of the global average temperature didn’t manage to reproduce this cycle period until 1982, when Benzi et al. [2] introduced the combination of long-term variations in incoming solar radiation and stochastic noise in an energy balance model. Using an energy balance model means that the change in global average temperature is set as proportional to the difference in ingoing and outgoing energy. The result of the simulations demonstrated so-called stochastic resonance, where small stochastic perturbations amplified the pattern of the variation in insolation, causing a pattern of large changes in the global average temperature, i.e. changes in the climate. The stochastic perturbations model unpredictable short time scale phenomena like the weather. Our study aimed to reproduce the result of Benzi et al.[2] and to investigate the model and its parameters. The presence of a 10<sup>5</sup>-year climatic cycle in simulated data was found. The combination of both noise and varying incoming solar radiation was necessary to observe the 10<sup>5</sup>-year cycle. The characteristics of the climate cycle pattern did, however, vary greatly depending on the values of constants in the model, illustrating how the model and constants were imprecise. Therefore, no conclusions can be drawn from this study about the earth’s current or future climate. However, the study still confirms that stochastic noise is an important part of modeling the climate, and manages to simulate the earth’s observed 10<sup>5</sup>-year climate cycle.</p>
----------------------------------------------------------------------
In diva2:1679305 abstract is:
<p>The passive particle separation method of elasto-inertial microfluidics have greatpotential in the field of physics, biology and chemistry. The objective of thisdegree project was to understand particle behavior in curved microchannels fornon-Newtonian fluids. This in order to optimize the separation of 1 µm and 2 µmparticles where the end goal is to create an efficient sample preparation method fordiagnosing sepsis. Fluorescent beads were spiked into PEO solutions of differentconcentrations and used in microfluidic PDMS-glass chips with various radii toexamine the influence of curvature and elasticity as well as the flow rate. Theresult indicated an independence of both curvature and elasticity. Reynoldsnumber and Dean number are dependent on the flow rate which results in atrade-off between a high and low flow rate. A low Reynolds number is not enoughto create Dean vortices that can be used to separate particles while a highReynolds number creates strong Dean vortices that can obstruct the focusing.</p><p>Later, microfluidic silicon-glass chips were used to separate 1 µm and 2 µm beads.The 2 µm particles were able to focus in two different PEO concentrations whereasthe 1 µm particles did not have time to focus entirely. This makes it possible toseparate 2 µm particles along with some 1 µm particles towards one outlet whileleaving another outlet with only 1 µm particles. This is a promising start butfurther optimization is required before being applied to real bacteria separation.</p>

corrected abstract:
<p>The passive particle separation method of elasto-inertial microfluidics have great potential in the field of physics, biology and chemistry. The objective of this degree project was to understand particle behavior in curved microchannels for non-Newtonian fluids. This in order to optimize the separation of 1 µm and 2 µm particles where the end goal is to create an efficient sample preparation method for diagnosing sepsis. Fluorescent beads were spiked into PEO solutions of different concentrations and used in microfluidic PDMS-glass chips with various radii to examine the influence of curvature and elasticity as well as the flow rate. The result indicated an independence of both curvature and elasticity. Reynolds number and Dean number are dependent on the flow rate which results in a trade-off between a high and low flow rate. A low Reynolds number is not enough to create Dean vortices that can be used to separate particles while a high Reynolds number creates strong Dean vortices that can obstruct the focusing.</p><p>Later, microfluidic silicon-glass chips were used to separate 1 µm and 2 µm beads. The 2 µm particles were able to focus in two different PEO concentrations where as the 1 µm particles did not have time to focus entirely. This makes it possible to separate 2 µm particles along with some 1 µm particles towards one outlet while leaving another outlet with only 1 µm particles. This is a promising start but further optimization is required before being applied to real bacteria separation.</p>
----------------------------------------------------------------------
In diva2:1546793 abstract is:
<p>The automotive environment is quickly evolving due to increasingly stringent environmentalstandards and the gradual reduction of the volume of diesel motorized vehicles. The volumes of electrifiedvehicles are thus constantly growing and are brought to be more and more present in our streets. Thiselectrification of vehicles involves new specifics issues compared to conventional vehicles, depending on thedifferent levels of electrification, which includes notably Hybrid Electric Vehicles.Hybridization in cars is characterized by the addition of a electrical traction and/or electrical generation systemin addition to the conventional thermal engine. However, if the complexity of vehicles with thermal tractioncomes essentially from the internal combustion engine and its efficiency which are sometimes complex tooptimize, the complexity of electric traction is expressed on the other hand at the level of the battery whichsupplies high voltage electricity. Indeed, while an electric machine offers high efficiency and an easy control,the high voltage battery contains many issues linked to a complex chemistry which must be controlled, andcan be subject to overheating.This overheating phenomenon is particularly an issue on HEV, which have smaller batteries than BEV and PHEVapplications for a comparable power demand. The conception of an efficient high-voltage battery coolingsystem is therefore essential in order to avoid any danger of damaging the system or potential fires linked tothe overheating of the battery. The air cooling solution is the most common, but this could change with thenew standard of the Electric Vehicle Safety Global Technical Regulation (EVS-GTR) applicable in 2021prohibiting the rejection of the cooling air that was in contact with the battery cells inside the passengercompartment. Is this solution able to adapt in order to remain competitive with the water or air conditioningcooling solutions? This study's purpose is to bring an answer to this issue.</p>

corrected abstract:
<p>The automotive environment is quickly evolving due to increasingly stringent environmental standards and the gradual reduction of the volume of diesel motorized vehicles. The volumes of electrified vehicles are thus constantly growing and are brought to be more and more present in our streets. This electrification of vehicles involves new specifics issues compared to conventional vehicles, depending on the different levels of electrification, which includes notably Hybrid Electric Vehicles.</p><p>Hybridization in cars is characterized by the addition of a electrical traction and/or electrical generation system in addition to the conventional thermal engine. However, if the complexity of vehicles with thermal traction comes essentially from the internal combustion engine and its efficiency which are sometimes complex to optimize, the complexity of electric traction is expressed on the other hand at the level of the battery which supplies high voltage electricity. Indeed, while an electric machine offers high efficiency and an easy control, the high voltage battery contains many issues linked to a complex chemistry which must be controlled, and can be subject to overheating.</p><p>This overheating phenomenon is particularly an issue on HEV, which have smaller batteries than BEV and PHEV applications for a comparable power demand. The conception of an efficient high-voltage battery cooling system is therefore essential in order to avoid any danger of damaging the system or potential fires linked to the overheating of the battery. The air cooling solution is the most common, but this could change with the new standard of the Electric Vehicle Safety Global Technical Regulation (EVS-GTR) applicable in 2021 prohibiting the rejection of the cooling air that was in contact with the battery cells inside the passenger compartment. Is this solution able to adapt in order to remain competitive with the water or air conditioning cooling solutions? This study's purpose is to bring an answer to this issue.</p>
----------------------------------------------------------------------
In diva2:1528126 abstract is:
<p>The modern marine fuel system have a vital part in preparing the fuel before it can enter theengines. Hard particles and water are removed and the viscosity needs to meet limitationsof the engines, to prevent damage. The cost for operating the fuel system and wear on theengine varies depending on how the fuel system are operated and on surrounding parameters.This thesis explains the cost impact of di erent parameters on the total cost for operating afuel system and aims to create a total cost of ownership model that also considers the costsfor systems connected to the fuel system.The total cost of ownership model includes capital expenses for the equipment and the operationalexpenses in form of; energy, service, sludge storage, water production, risk and alsoincreased costs on connected systems.The result of this total cost of ownership research shows that even if there are an increasedcost for operating the fuel system itself for highest performance will the total cost of ownershipwhen considering connected systems be lower. The user are therefore always recommendedto aim for maximum separation of hard particles to decrease the cost for enginewear and to lower the total cost of risk for a breakdown. The total energy consumption forthe fuel system can be decreased by up to 15% when variable ow control are used on thesupply pumps and meanwhile increase the separation eciency.</p>


corrected abstract:
<p>The modern marine fuel system have a vital part in preparing the fuel before it can enter the engines. Hard particles and water are removed and the viscosity needs to meet limitations of the engines, to prevent damage. The cost for operating the fuel system and wear on the engine varies depending on how the fuel system are operated and on surrounding parameters. This thesis explains the cost impact of different parameters on the total cost for operating a fuel system and aims to create a total cost of ownership model that also considers the costs for systems connected to the fuel system.</p><p>The total cost of ownership model includes capital expenses for the equipment and the operational expenses in form of; energy, service, sludge storage, water production, risk and also increased costs on connected systems.</p><p>The result of this total cost of ownership research shows that even if there are an increased cost for operating the fuel system itself for highest performance will the total cost of ownership when considering connected systems be lower. The user are therefore always recommended to aim for maximum separation of hard particles to decrease the cost for engine wear and to lower the total cost of risk for a breakdown. The total energy consumption for the fuel system can be decreased by up to 15% when variable flow control are used on the supply pumps and meanwhile increase the separation efficiency.</p>
----------------------------------------------------------------------
In diva2:1465543 abstract is:
<p>In early design phase for new hybrid electric vehicle (HEV) powertrains, simulation isused for the estimation of vehicle fuel consumption. For hybrid electric powertrains,fuel consumption is highly related to powertrain efficiency. While powertrainefficiency of hybrid electric powertrain is not a linear product of efficiencies ofcomponents, it has to be analysed as a sequence of energy conversions includingcomponent losses and energy interaction among components.This thesis is aimed at studying the energy losses and flows and present them in theform of Sankey diagram, later, an adaptive energy management system is developedbased on current rule-based control strategy. The first part involves developing energycalculation block in GT-SUITE corresponding to the vehicle model, calculating allthe energy losses and flows and presenting them in Sankey diagram. The secondpart involves optimizing energy management system control parameters according todifferent representative driving cycles. The third part involves developing adaptiveenergy management system by deploying optimal control parameter based on drivingpattern recognition with the help of SVM (support vector machine).In conclusion, a sturctured way to generate the Sankey diagram has been successfullygenerated and it turns out to be an effective tool to study HEV powertrain efficiencyand fuel economy. In addition, the combination of driving pattern recognition andoptimized control parameters also show a significant potential improvement in fuelconsumption.</p>

corrected abstract:
<p>In early design phase for new hybrid electric vehicle (HEV) powertrains, simulation is used for the estimation of vehicle fuel consumption. For hybrid electric powertrains, fuel consumption is highly related to powertrain efficiency. While powertrain efficiency of hybrid electric powertrain is not a linear product of efficiencies of components, it has to be analysed as a sequence of energy conversions including component losses and energy interaction among components.</p><p>This thesis is aimed at studying the energy losses and flows and present them in the form of Sankey diagram, later, an adaptive energy management system is developed based on current rule-based control strategy. The first part involves developing energy calculation block in GT-SUITE corresponding to the vehicle model, calculating all the energy losses and flows and presenting them in Sankey diagram. The second part involves optimizing energy management system control parameters according to different representative driving cycles. The third part involves developing adaptive energy management system by deploying optimal control parameter based on driving pattern recognition with the help of SVM (support vector machine).</p><p>In conclusion, a sturctured way to generate the Sankey diagram has been successfully generated and it turns out to be an effective tool to study HEV powertrain efficiency and fuel economy. In addition, the combination of driving pattern recognition and optimized control parameters also show a significant potential improvement in fuel consumption.</p>
----------------------------------------------------------------------
In diva2:1293442 abstract is:
<p>Harry Markowitz work in the 50’s spring-boarded modernportfolio theory. It gives investors quantitative tools to compose and assessasset portfolios in a systematic fashion. The main idea of the Mean-Varianceframework is that composing an optimal portfolio is equivalent to solving aquadratic optimization problem.In this project we employ the Maximally Predictable Portfolio (MPP) frameworkproposed by Lo and MacKinlay, as an alternative to Markowitz’s approach, inorder to construct investment portfolios. One of the benefits of using theformer method is that it accounts for forecasting estimation errors. Ourinvestment strategy is to buy and hold these portfolios during a time periodand assess their performance. We show that it is indeed possible to constructportfolios with high rate of return and coefficient of determination based onhistorical data. However, despite their many promising features, the success ofMPP portfolios is short lived. Based on our assessment we conclude thatinvesting in the stock market solely on the basis of the optimization resultsis not a lucrative strategy</p>

corrected abstract:
<p>Harry Markowitz work in the 50’s spring-boarded modern portfolio theory. It gives investors quantitative tools to compose and assess asset portfolios in a systematic fashion. The main idea of the Mean-Variance framework is that composing an optimal portfolio is equivalent to solving a quadratic optimization problem.</p><p>In this project we employ the Maximally Predictable Portfolio (MPP) framework proposed by Lo and MacKinlay, as an alternative to Markowitz’s approach, in order to construct investment portfolios. One of the benefits of using the former method is that it accounts for forecasting estimation errors. Our investment strategy is to buy and hold these portfolios during a time period and assess their performance. We show that it is indeed possible to construct portfolios with high rate of return and coefficient of determination based on historical data. However, despite their many promising features, the success of MPP portfolios is short lived. Based on our assessment we conclude that investing in the stock market solely on the basis of the optimization results is not a lucrative strategy.</p>
----------------------------------------------------------------------
In diva2:1206952 - missing space in title:
"Test Method Optimization ofSemi-Automatic ParkingFunction"
==>
"Test Method Optimization of Semi-Automatic Parking Function"

abstract is:
<p>This is a Master of Science project performed at Volvo Cars in Gothenburg and at The RoyalInstitute of Technology KTH in Stockholm. The project is about optimization of the testmethod for semi-automatic parking. The current test method to verify the parking functions aredescribed in a document called design verification method, DVM. The test method in DVMconsiders each function’s parameter separately which takes a lot of test time and all of thefunctions cannot be tested because of time shortage. The aim of this project is to develop anoptimized test method which can solves this issue and can replace the current test method.There are also some other issues that the project need to deal with, such as the sensor’smeasuring error and the optimization of the distance from vehicle to the curb.The current test methods are based on principle of one factor at a time method, which is verytime consuming. Several other test method such as Factorial Design, Taguchi Design andPlacket Burman Design which are based on the principle of factorial design are thereforestudied. Amongst these the factorial design is chosen since it is an adequate design in term ofreduction of test time and other properties which are beneficial for the aim of this project.The proposed test method is evaluated by first performing a test version with a number ofrelevant inputs parameters for which the process is described in Chapter 3 and the evaluationof the method is described in Chapter 4. In Chapter 5 the process on how the proposed methodcan replace the current method is described.The result of this thesis work is a proposed and verified system verification test method forparking assistance which can also be used for other systems as well on some levels.</p>

corrected abstract:
<p>This is a Master of Science project performed at Volvo Cars in Gothenburg and at The Royal Institute of Technology KTH in Stockholm. The project is about optimization of the test method for semi-automatic parking. The current test method to verify the parking functions are described in a document called design verification method, DVM. The test method in DVM considers each function’s parameter separately which takes a lot of test time and all of the functions cannot be tested because of time shortage. The aim of this project is to develop an optimized test method which can solves this issue and can replace the current test method. There are also some other issues that the project need to deal with, such as the sensor’s measuring error and the optimization of the distance from vehicle to the curb.</p><p>The current test methods are based on principle of one factor at a time method, which is very time consuming. Several other test method such as Factorial Design, Taguchi Design and Placket Burman Design which are based on the principle of factorial design are therefore studied. Amongst these the factorial design is chosen since it is an adequate design in term of reduction of test time and other properties which are beneficial for the aim of this project.</p><p>The proposed test method is evaluated by first performing a test version with a number of relevant inputs parameters for which the process is described in Chapter 3 and the evaluation of the method is described in Chapter 4. In Chapter 5 the process on how the proposed method can replace the current method is described.</p><p>The result of this thesis work is a proposed and verified system verification test method for parking assistance which can also be used for other systems as well on some levels.</p>
----------------------------------------------------------------------
In diva2:1136781 - missing space in title:
"CFD Analysis of Cold Stage Centrifugal Pump for Cooling of Hot IsostaticPress with Validation Case Study"
==>
"CFD Analysis of Cold Stage Centrifugal Pump for Cooling of Hot Isostatic Press with Validation Case Study"

abstract is:
<p>Hot isostatic pressing (HIPing) has been a growing material treatment process for performance part manufacturingfor over 50 years. This process of using an inert gas at high temperature and pressure to densifymaterials leads to vastly improved material properties by removing pores and other micro- aws. Interest forHIP treatment has greatly increased in recent years due to the development of metal 3D printing technology.HIP treatment is very well suited for treating 3D printed and cast parts due to their relatively poor materialproperties.An important part of any HIP cycle is the cooling phase. New uniform and rapid cooling technology hasvastly reduced HIP cycle times, but room for further improvement exists. This study aims to accurately andtrustfully evaluate the performance of one of a pair of centrifugal pumps used in a Quintus Technologies ABHIP cooling system. Computational uid dynamics (CFD) software and techniques are used to achieve this.This paper is split into two main parts; the rst of which is a validation case study, and the second is theperformance analysis of a Quintus HIP cold gas pump. The validation case study is conducted to supportthe accuracy and reliability of results obtained in the Quintus cold gas pump performance analysis.The validation case study results show good agreement with experimental data and supports the accuracy ofCFD in the analysis of centrifugal pumps. Both detailed ow and macro ow characteristics are shown to beaccurately predicted. The pump curve generated for the Quintus Cold gas pump quanties its performanceover a range of rotational speeds and mass ow rates. The work done here lays the groundwork for furtheranalysis and improvement of Quintus HIP cooling systems.</p>

corrected abstract:
<p>Hot isostatic pressing (HIPing) has been a growing material treatment process for performance part manufacturing for over 50 years. This process of using an inert gas at high temperature and pressure to densify materials leads to vastly improved material properties by removing pores and other micro-flaws. Interest for HIP treatment has greatly increased in recent years due to the development of metal 3D printing technology. HIP treatment is very well suited for treating 3D printed and cast parts due to their relatively poor material properties.</p><p>An important part of any HIP cycle is the cooling phase. New uniform and rapid cooling technology has vastly reduced HIP cycle times, but room for further improvement exists. This study aims to accurately and trustfully evaluate the performance of one of a pair of centrifugal pumps used in a Quintus Technologies AB HIP cooling system. Computational fluid dynamics (CFD) software and techniques are used to achieve this. This paper is split into two main parts; the first of which is a validation case study, and the second is the performance analysis of a Quintus HIP cold gas pump. The validation case study is conducted to support the accuracy and reliability of results obtained in the Quintus cold gas pump performance analysis.</p><p>The validation case study results show good agreement with experimental data and supports the accuracy of CFD in the analysis of centrifugal pumps. Both detailed flow and macro flow characteristics are shown to be accurately predicted. The pump curve generated for the Quintus Cold gas pump quantifies its performance over a range of rotational speeds and mass flow rates. The work done here lays the groundwork for further analysis and improvement of Quintus HIP cooling systems.</p>
----------------------------------------------------------------------
In diva2:1120459 abstract is:
<p>Collaboration between humans and robots is becoming an increasingly commonoccurrence in both industry and homes, more so with every forthcomingtechnological advance. This paper examines the possibilities of performinghuman hand movement predictions on the fly, e.g. by only using informationup to the specific moment in time of which the prediction is carried out.Specifically, data will be collected using a Kinect (v.1).The model used for the predictor developed is the Minimum Jerk model,which states that certain multi-joint reaching movements are planned in sucha way that the hand is to follow a straight path while maximizing smoothness.Extent, direction and duration of the motion are main objectives for thepredictor to determine, with a Kalman filter and curve fitting as the mainconstituents. Another assumption in this work is that a reliable start detectoris available. An experiment where five volunteers were to perform differentreaching movements was conducted.This study shows that the approach is feasible in some cases, namelyusable predictions is acquired for long movements. In the case of shortmovements the alternative of not doing any prediction was by all meansbetter.</p>

corrected abstract:
<p>Collaboration between humans and robots is becoming an increasingly common occurrence in both industry and homes, more so with every forthcoming technological advance. This paper examines the possibilities of performing human hand movement predictions on the fly, e.g. by only using information up to the specific moment in time of which the prediction is carried out. Specifically, data will be collected using a Kinect (v.1).</p><p>The model used for the predictor developed is the Minimum Jerk model, which states that certain multi-joint reaching movements are planned in such a way that the hand is to follow a straight path while maximizing smoothness. Extent, direction and duration of the motion are main objectives for the predictor to determine, with a Kalman filter and curve fitting as the main constituents. Another assumption in this work is that a reliable start detector is available. An experiment where five volunteers were to perform different reaching movements was conducted.</p><p>This study shows that the approach is feasible in some cases, namely usable predictions is acquired for long movements. In the case of short movements the alternative of not doing any prediction was by all means better.</p>
----------------------------------------------------------------------
In diva2:1114153 abstract is:
<p>The study aims to investigate which factors that could potentially affectthe yearly card transaction volume for smaller or medium sized businesses, inthis study entitled merchants, not listed on the stock exchange. Thesemerchants are characterized by a low number of employees, and in many casessmaller businesses without any noted ambition to grow in size. They are alldirected towards consumers, and are therefore heavily reliable of peopleactually visiting their business in order to make revenues. These merchants donot offer any public financial information like previous yearly revenues,planned future investments or any debts the company might have, and hence thestudy is focused on factors an external actor could take part of. Byinvestigating non-financial factors, this investigation further contributeswith research on how a company can ensure growth and revenues. It alsoemphasizes specific industries or size of companies that generate larger cardtransaction volumes than others.</p><p>By mapping both possible quantitative andqualitative factors, that could potentially have an impact on the yearly cardtransaction volume of a merchant, a mathematical analysis has been conducted.The mathematical part of the study is based on multiple linear regressionanalysis, where the result is a model that can predict the card transactionvolume for a company. The study has been conducted on actors operating on theDanish market, but could with smaller adjustments also be applied on other geographicalmarkets</p>

corrected abstract:
<p>The study aims to investigate which factors that could potentially affect the yearly card transaction volume for smaller or medium sized businesses, in this study entitled merchants, not listed on the stock exchange. These merchants are characterized by a low number of employees, and in many cases smaller businesses without any noted ambition to grow in size. They are all directed towards consumers, and are therefore heavily reliable of people actually visiting their business in order to make revenues. These merchants do not offer any public financial information like previous yearly revenues, planned future investments or any debts the company might have, and hence the study is focused on factors an external actor could take part of. By investigating non-financial factors, this investigation further contributes with research on how a company can ensure growth and revenues. It also emphasizes specific industries or size of companies that generate larger card transaction volumes than others.</p><p>By mapping both possible quantitative and qualitative factors, that could potentially have an impact on the yearly card transaction volume of a merchant, a mathematical analysis has been conducted. The mathematical part of the study is based on multiple linear regression analysis, where the result is a model that can predict the card transaction volume for a company. The study has been conducted on actors operating on the Danish market, but could with smaller adjustments also be applied on other geographical markets.</p>
----------------------------------------------------------------------
In diva2:692743 abstract is:
<p>This thesis is intended to give an overview of creditvaluation adjustment (CVA) and adjacent concepts. Firstly, the historicalevents that preceded the initiative to reform the Basel regulations and tointroduce CVA as a core component of counterparty credit risk are illustrated.After some conceptual background material, a journey is taken through theregulatory aspects of CVA. The three most commonly used methods for calculatingthe regulatory CVA capital charge are explained in detail and potentialchallenges with the methods are addressed. Further, the document analyses ingreater depth two of the methods; the internal model method (IMM) and thecurrent exposure method (CEM). The differences between these two methods areexplained mathematically and analysed. This comparison is supported bysimulations of portfolios containing interest rate swap contracts with differenttime to maturity and of counterparties with varying credit ratings. Oneconcluding observations is that credit valuation adjustment is a measure of centralimportance within counterparty credit risk. Further, it is shown that IMM has someimportant advantages over CEM, especially when it comes to model connection withreality. Finally, some possible future work to be done within the topic area is suggested.</p>


corrected abstract:
<p>This thesis is intended to give an overview of credit valuation adjustment (CVA) and adjacent concepts. Firstly, the historical events that preceded the initiative to reform the Basel regulations and to introduce CVA as a core component of counterparty credit risk are illustrated. After some conceptual background material, a journey is taken through the regulatory aspects of CVA. The three most commonly used methods for calculating the regulatory CVA capital charge are explained in detail and potential challenges with the methods are addressed. Further, the document analyses in greater depth two of the methods; the internal model method (IMM) and the current exposure method (CEM). The differences between these two methods are explained mathematically and analysed. This comparison is supported by simulations of portfolios containing interest rate swap contracts with different time to maturity and of counterparties with varying credit ratings. One concluding observations is that credit valuation adjustment is a measure of central importance within counterparty credit risk. Further, it is shown that IMM has some important advantages over CEM, especially when it comes to model connection with reality. Finally, some possible future work to be done within the topic area is suggested.</p>
----------------------------------------------------------------------
In diva2:618592 - Note: no full text in DiVA

abstract is:
<p>In the quest of reducing emissions of passenger cars a trend in the car industry has recently been tointroduce lightweight composite materials to replace steel which has been the otherwise main materialto use. In this report the weight-optimising of a structural underbody for a passenger car using twodifferent manufacturing methods is described. The two methods are Advanced Sheet MouldingCompound (A-SMC) and Resin Transfer Moulding (RTM). A-SMC is characterised by a low cycle time andfast layup of the material resulting in lower cost. RTM is slower and thus more expensive but has bettermaterial properties which results in lower weight. The novel approach is to use A-SMC to construct thecomplete underbody as one piece which has not been done before. The geometry is FEM -optimised forminimum material thickness under a standard load case for torsional stiffness.The simulations showed that the underbody was not possible to be manufactured using A-SMC in itsoriginal shape without being reinforced. When reinforcing the structure it met the design constraintsand the weight was optimised to 53.9 kg with the possibility for further improvements. The RTMmethod resulted in 25.83 kg without any reinforcements but showed potential for further weightreduction by changing the geometrical design. A final analysis of the underbody combined the twomanufacturing methods and the weight was here optimised to 25.27 kg without any reinforcements.</p>

corrected abstract:
<p>In the quest of reducing emissions of passenger cars a trend in the car industry has recently been to introduce lightweight composite materials to replace steel which has been the otherwise main material to use. In this report the weight-optimising of a structural under body for a passenger car using twodifferent manufacturing methods is described. The two methods are Advanced Sheet Moulding Compound (A-SMC) and Resin Transfer Moulding (RTM). A-SMC is characterised by a low cycle time and fast layup of the material resulting in lower cost. RTM is slower and thus more expensive but has better material properties which results in lower weight. The novel approach is to use A-SMC to construct the complete under body as one piece which has not been done before. The geometry is FEM -optimised for minimum material thickness under a standard load case for torsional stiffness. The simulations showed that the under body was not possible to be manufactured using A-SMC in its original shape without being reinforced. When reinforcing the structure it met the design constraints and the weight was optimised to 53.9 kg with the possibility for further improvements. The RTM method resulted in 25.83 kg without any reinforcements but showed potential for further weight reduction by changing the geometrical design. A final analysis of the under body combined the two manufacturing methods and the weight was here optimised to 25.27 kg without any reinforcements.</p>
----------------------------------------------------------------------
In diva2:408828 abstract is:
<p>The digital development may be the biggest challenge that the school has faced since theinvention of the printing press. Several of the teachers conditions have already changed andtheir role will in the future change fundamentally. To be able to operate in the school of thefuture teachers need a digital literacy. How to develop teacher’s digital literacy and how to findthe factors that affect this competence is the subject of this master thesis. In the thesis adevelopment model for teachers' digital literacy is used. The model has been used together withphysics teachers. The results are based on literature studies and a field study.The field study shows that it is possible to develop a teacher’s digital literacy with simplemeans. There are three main factors affecting the development of teachers' digital literacy:access to computers, access to software customized for schools and training. The educationalfoundation for both students and teachers must be the focus of attention and it is important thatthis is not overshadowed by a focus on the technology. When teachers no longer have amonopoly on knowledge, the traditional teacher role will change. The teacher will become aguide with the function to support the students learning. A computer-aided school is positivelywelcomed by students and cautiously welcomed by teachers. A professional production ofsoftware for use in schools has to begin. All teachers must be trained so that they have anadequate digital literacy to be sufficient in the school of tomorrow.</p>

corrected abstract:
<p>The digital development may be the biggest challenge that the school has faced since the invention of the printing press. Several of the teachers conditions have already changed and their role will in the future change fundamentally. To be able to operate in the school of the future teachers need a digital literacy. How to develop teacher’s digital literacy and how to find the factors that affect this competence is the subject of this master thesis. In the thesis a development model for teachers' digital literacy is used. The model has been used together with physics teachers. The results are based on literature studies and a field study.</p><p>The field study shows that it is possible to develop a teacher’s digital literacy with simple means. There are three main factors affecting the development of teachers' digital literacy: access to computers, access to software customized for schools and training. The educational foundation for both students and teachers must be the focus of attention and it is important that this is not overshadowed by a focus on the technology. When teachers no longer have a monopoly on knowledge, the traditional teacher role will change. The teacher will become a guide with the function to support the students learning. A computer-aided school is positively welcomed by students and cautiously welcomed by teachers. A professional production of software for use in schools has to begin. All teachers must be trained so that they have an adequate digital literacy to be sufficient in the school of tomorrow.</p>
----------------------------------------------------------------------
In diva2:401124 - missing space in title:
"Flight Dynamics Modelling and Application to SatelliteDeorbitation Strategy"
==>
"Flight Dynamics Modelling and Application to Satellite Deorbitation Strategy"

abstract is:
<p>In this paper a study to determine the best strategic choice to deorbit satellites incase of system failures is conducted. An orbital mechanic analysis of the deorbitationis done and a matlab code based on Lagrange’s interplanetary equations is developed.The effects of the atmospheric drag and the solar activity are investigated and the fueland delta velocity consumptions are assessed for different deorbitation strategies. Thepaper also investigates the probability to hit or be hit by a piece of debris. A briefoverview of the acquisition and safe hold modes Bdot and Bspin is given. Then thepossibility to deorbit a satellite working in these attitude and orbit control modes fromlow Earth’s orbits is studied. An attitude and orbit simulator is used to analyze theevolution of the attitude during a manoeuvre, in particular the effect of the thrusters’misalignment is investigated. A strategy which combines manoeuvres and aerobrakingis chosen and its implementation on different types of orbits is studied. The paperconcludes that the necessary manoeuvres for a 25 years deorbiting time can be achieveif the spacecraft has a functional acquisition and safe hold mode and an operationalpropulsion system. In the end an alternate mode based on star trackers and reactionwheels is developed to avoid most of the activations of the acquisition and safe holdmode and to eventually perform the end of life disposal of a spacecraft.</p>

corrected abstract:
<p>In this paper a study to determine the best strategic choice to deorbit satellites in case of system failures is conducted. An orbital mechanic analysis of the deorbitation is done and a matlab code based on Lagrange’s interplanetary equations is developed. The effects of the atmospheric drag and the solar activity are investigated and the fuel and delta velocity consumptions are assessed for different deorbitation strategies. The paper also investigates the probability to hit or be hit by a piece of debris. A brief overview of the acquisition and safe hold modes <em>Bdot</em> and <em>Bspin</em> is given. Then the possibility to deorbit a satellite working in these attitude and orbit control modes from low Earth’s orbits is studied. An attitude and orbit simulator is used to analyze the evolution of the attitude during a manoeuvre, in particular the effect of the thrusters’ misalignment is investigated. A strategy which combines manoeuvres and aerobraking is chosen and its implementation on different types of orbits is studied. The paper concludes that the necessary manoeuvres for a 25 years deorbiting time can be achieve if the spacecraft has a functional acquisition and safe hold mode and an operational propulsion system. In the end an alternate mode based on star trackers and reaction wheels is developed to avoid most of the activations of the acquisition and safe hold mode and to eventually perform the end of life disposal of a spacecraft.</p>
----------------------------------------------------------------------
In diva2:1890962 abstract is:
<p>The objective of this research is to investigate bound-state beta decay (BSBD) and its influence on the half-life oflong-lived fission products (LLFPs). We reviewed the existing BSBD model by Takahashi and Yokoi, which isbased on plasma states at very high temperatures. Following this, we devised a systematic BSBD model assumingbare ions in a vacuum. Our BSBD study was then performed using this bare ion model. The research methodicallyexplores how BSBD impacts LLFP half-lives, by comparing theoretical calculations with experimental data tovalidate the model’s predictive accuracy. To calculate the half-life of LLFPs, we first computed the radialwavefunctions of electrons in the K and L orbitals, where electron creation is more likely, by solving the Diracequation. Subsequently, we used FLYCHK software to determine the bound-state decay energy and obtain thelepton phase volume. Additionally, we calculated the decay rate function by implementing finite size correctionsand the partial half-life for each LLFP, resulting in the total decay rate of the nuclide. The study investigated thehalf-life of fully stripped LLFPs compared to their neutral states for known transitions, and also examined variousnuclear-level couplings and their influence on the decay transitions. The study suggests that these unknowntransitions can lead to different types of allowed transitions, significantly shortening half-lives, with additionalcontributions from the continuum channel. Furthermore, the research reveals variations in branching patternsbetween neutral and bare atoms, including instances of branching flip observed in specific LLFP isotopes</p>

corrected abstract:
<p>The objective of this research is to investigate bound-state beta decay (BSBD) and its influence on the half-life of long-lived fission products (LLFPs). We reviewed the existing BSBD model by Takahashi and Yokoi, which is based on plasma states at very high temperatures. Following this, we devised a systematic BSBD model assuming bare ions in a vacuum. Our BSBD study was then performed using this bare ion model. The research methodically explores how BSBD impacts LLFP half-lives, by comparing theoretical calculations with experimental data to validate the model’s predictive accuracy. To calculate the half-life of LLFPs, we first computed the radial wavefunctions of electrons in the K and L orbitals, where electron creation is more likely, by solving the Dirac equation. Subsequently, we used FLYCHK software to determine the bound-state decay energy and obtain the lepton phase volume. Additionally, we calculated the decay rate function by implementing finite size corrections and the partial half-life for each LLFP, resulting in the total decay rate of the nuclide. The study investigated the half-life of fully stripped LLFPs compared to their neutral states for known transitions, and also examined various nuclear-level couplings and their influence on the decay transitions. The study suggests that these unknown transitions can lead to different types of allowed transitions, significantly shortening half-lives, with additional contributions from the continuum channel. Furthermore, the research reveals variations in branching patterns between neutral and bare atoms, including instances of branching flip observed in specific LLFP isotopes</p>
----------------------------------------------------------------------
In diva2:1879630 abstract is:
<p>This thesis concerns itself with word classes and their application to language modelling.Considering a purely statistical Markov model trained on sequences of word classes in theSwedish language different problems in language engineering are examined. Problemsconsidered are part-of-speech tagging, evaluating text modifiers such as translators withthe help of probability measurements and matrix norms, and lastly detecting differenttypes of text using the Fourier transform of cross entropy sequences of word classes.The results show that the word class language model is quite weak by itself but that itis able to improve part-of-speech tagging for 1 and 2 letter models. There are indicationsthat a stronger word class model could aid 3-letter and potentially even stronger models.For evaluating modifiers the model is often able to distinguish between shuffled andsometimes translated text as well as to assign a score as to how much a text has beenmodified. Future work on this should however take better care to ensure large enoughtest data. The results from the Fourier approach indicate that a Fourier analysis of thecross entropy sequence between word classes may allow the model to distinguish betweenA.I. generated text as well as translated text from human written text. Future work onmachine learning word class models could be carried out to get further insights into therole of word class models in modern applications. The results could also give interestinginsights in linguistic research regarding word classes.</p>

corrected abstract:
<p>This thesis concerns itself with word classes and their application to language modelling. Considering a purely statistical Markov model trained on sequences of word classes in the Swedish language different problems in language engineering are examined. Problems considered are part-of-speech tagging, evaluating text modifiers such as translators with the help of probability measurements and matrix norms, and lastly detecting different types of text using the Fourier transform of cross entropy sequences of word classes.  The results show that the word class language model is quite weak by itself but that it is able to improve part-of-speech tagging for 1 and 2 letter models. There are indications that a stronger word class model could aid 3-letter and potentially even stronger models. For evaluating modifiers the model is often able to distinguish between shuffled and sometimes translated text as well as to assign a score as to how much a text has been modified. Future work on this should however take better care to ensure large enough test data. The results from the Fourier approach indicate that a Fourier analysis of the cross entropy sequence between word classes may allow the model to distinguish between A.I. generated text as well as translated text from human written text. Future work on machine learning word class models could be carried out to get further insights into the role of word class models in modern applications. The results could also give interesting insights in linguistic research regarding word classes.</p>
----------------------------------------------------------------------
In diva2:1876262 - Note: no full text in DiVA
abstract is:
<p>This thesis presents a comprehensive methodology for modelling asymmetric silicon anodes within the COMSOL Multiphysics software, with the aim to enhance the design andevaluation of lithium-ion batteries. The research addresses the critical issue of swelling inamorphous silicon anodes, which can undergo up to 400% volumetric expansion duringcharging cycles. This effect can potentially lead to the degradation of battery performanceand shorten lifespan.The thesis provides in detail the implementation of Finite Element Method (FEM) simulations to investigate the mechanical and electrochemical responses of silicon anodes underoperational conditions. The modelling process is elaborated, starting from the theoreticalbasis of lithium diffusion and plastic deformation and then proceeding with the practical setup of simulation parameters and boundary conditions in COMSOL Multiphysics.Different anode geometries, including double-walled nanotube structures, have been modelled to study the ability to mitigate the effects of expansion.The results from the simulations provide insights into the stress distribution and deformation patterns within the anodes and highlight the effectiveness of certain geometricalconfigurations in reducing mechanical stresses and maintaining the integrity of the solidelectrolyte interface (SEI).The work concludes with suggestions for further enhancements in the simulation modeland proposes future research directions to explore alternative materials and configurations.Through detailed modelling and analysis, this thesis contributes to the ongoing efforts toimprove the performance and reliability of next-generation lithium-ion batteries.</p><p> </p>

corrected abstract:
<p>This thesis presents a comprehensive methodology for modelling asymmetric silicon anodes within the COMSOL Multiphysics software, with the aim to enhance the design and evaluation of lithium-ion batteries. The research addresses the critical issue of swelling in amorphous silicon anodes, which can undergo up to 400% volumetric expansion during charging cycles. This effect can potentially lead to the degradation of battery performance and shorten lifespan. The thesis provides in detail the implementation of Finite Element Method (FEM) simulations to investigate the mechanical and electrochemical responses of silicon anodes under operational conditions. The modelling process is elaborated, starting from the theoretical basis of lithium diffusion and plastic deformation and then proceeding with the practical setup of simulation parameters and boundary conditions in COMSOL Multiphysics. Different anode geometries, including double-walled nanotube structures, have been modelled to study the ability to mitigate the effects of expansion. The results from the simulations provide insights into the stress distribution and deformation patterns within the anodes and highlight the effectiveness of certain geometrical configurations in reducing mechanical stresses and maintaining the integrity of the solid electrolyte interface (SEI). The work concludes with suggestions for further enhancements in the simulation model and proposes future research directions to explore alternative materials and configurations. Through detailed modelling and analysis, this thesis contributes to the ongoing efforts toimprove the performance and reliability of next-generation lithium-ion batteries.</p>
----------------------------------------------------------------------
In diva2:1873390 - Note: The PDF file has a problem and cannot be openned.

abstract is:
<p>Metallic fuels were produced through arc-melting. As-cast phases, microstructuresand selected mechanical properties were investigated for UZr,U-Th, and U-Th-Zr systems. For each system, two compositions wereinvestigated, with approximately 5 at. % and 20 at. % solute material, for atotal of six alloys. As-cast alloy microstructures were assessed in the contextof their equilibrium systems and compared to relevant published works whereapplicable. Mechanical testing revealed increased hardness with increasingsolute concentration, compared to the reference materials. The results supportthe conclusion that solid solution strengthening is the primary mechanismenabling this change in each binary system.Additionally, (U,Zr)N fuel was synthesized. This work exemplified aprocess to produce fuel with a homogeneous distribution of zirconium in thefuel matrix, thus representing a simulated burn-up distribution of zirconium.Refinements can be made to further improve this process in future work. Thesefindings will support a broader separate effects testing campaign underway bythe SUNRISE centre</p>

corrected abstract:
<p>Metallic fuels were produced through arc-melting. As-cast phases, microstructures and selected mechanical properties were investigated for UZr,U-Th, and U-Th-Zr systems. For each system, two compositions were investigated, with approximately 5 at. % and 20 at. % solute material, for a total of six alloys. As-cast alloy microstructures were assessed in the context of their equilibrium systems and compared to relevant published works where applicable. Mechanical testing revealed increased hardness with increasing solute concentration, compared to the reference materials. The results support the conclusion that solid solution strengthening is the primary mechanism enabling this change in each binary system. Additionally, (U,Zr)N fuel was synthe sized. This work exemplified a process to produce fuel with a homogeneous distribution of zirconium in the fuel matrix, thus representing a simulated burn-up distribution of zirconium. Refinements can be made to further improve this process in future work. These findings will support a broader separate effects testing campaign underway by the SUNRISE centre</p>
----------------------------------------------------------------------
In diva2:1831429 - Note: no full text in DiVA

abstract is:
<p>This bachelor thesis is an attempt to apply mathematical optimization to the orderingprocedure of a small business selling a perishable product. This is carried out througha collaboration with an organization selling Christmas trees each winter in a small partof Stockholm. In order to optimize the order quantity the aim is to find a practicalway to apply the newsvendor problem when the distribution of demand is unknown.Two different approaches are identified and both applied as models to solve thisproblem.The first model predicts a trend line based on historical data and assumes a normallydistributed demand. The other model uses Sample Average Approximation. Theresults from both models are analyzed and compared to the actual demand and orderdecisions made historically by the organization. The result shows that the modelwith normal distribution is more accurate than the model using Sample AverageApproximation because of the trend line. In general, both models can be useful andhave advantages in different situations. In this specific case the first model producesimproved decisions some years while others not, indicating that there are other factorsthat the model does not include. In order to produce better results than the decisionsmade by the organization improvements to the model are required.</p>

corrected abstract:
<p>This bachelor thesis is an attempt to apply mathematical optimization to the ordering procedure of a small business selling a perishable product. This is carried out through a collaboration with an organization selling Christmas trees each winter in a small partof Stockholm. In order to optimize the order quantity the aim is to find a practical way to apply the news vendor problem when the distribution of demand is unknown. Two different approaches are identified and both applied as models to solve this problem. The first model predicts a trend line based on historical data and assumes a normally distributed demand. The other model uses Sample Average Approximation. The results from both models are analyzed and compared to the actual demand and order decisions made historically by the organization. The result shows that the model with normal distribution is more accurate than the model using Sample Average Approximation because of the trend line. In general, both models can be useful and have advantages in different situations. In this specific case the first model produces improved decisions some years while others not, indicating that there are other factors that the model does not include. In order to produce better results than the decisions made by the organization improvements to the model are required.</p>
----------------------------------------------------------------------
In diva2:1817012 abstract is:
<p>Paydrive is a pioneer in the Swedish auto insurance market. Being able to influence your insurancepremium through your driving is a concept that is still in its early stages. Throughout this thesis,an attempt to consolidate the vast amounts of data gathered while driving with neural networkshas been made, together with comparisons to the currently existing generalized linear models. Inthe end, a full analysis of the data yielded four distinct groupings of customer behavior but becauseof how the data is structured the results from the modeling became sub-optimal. Insurance datais typically very skewed and zero-heavy due to the absence of accidents. The original researchquestion is whether it is possible to use two neural networks, calculating the probability of anaccident, r, and the size of a potential claim, s respectively. These two factors could be multipliedto determine a final insurance premium as c = r · s.</p><p>Using statistical standards and tools such as the Gini-coefficient, R2 values, MSE, and MAE themodels were evaluated both individually and pairwise. However, previous research in the fieldshows there haven’t been big enough advancements in this area yet. This thesis comes to the sameconclusion that due to the volatile nature of neural networks and the skewness of the data, it isincredibly difficult to get good results. Future work in the field could result in fairer prices forcustomers on their insurance premiums.</p>

corrected abstract:
<p>Paydrive is a pioneer in the Swedish auto insurance market. Being able to influence your insurance premium through your driving is a concept that is still in its early stages. Throughout this thesis, an attempt to consolidate the vast amounts of data gathered while driving with neural networks has been made, together with comparisons to the currently existing generalized linear models. In the end, a full analysis of the data yielded four distinct groupings of customer behavior but because of how the data is structured the results from the modeling became sub-optimal. Insurance data is typically very skewed and zero-heavy due to the absence of accidents. The original research question is whether it is possible to use two neural networks, calculating the probability of an accident, <em>r</e>, and the size of a potential claim, <em>s</em> respectively. These two factors could be multiplied to determine a final insurance premium as <em>c = r · s</em>.</p><p>Using statistical standards and tools such as the Gini-coefficient, <em>R<sup>2</sup></em> values, MSE, and MAE the models were evaluated both individually and pairwise. However, previous research in the field shows there haven’t been big enough advancements in this area yet. This thesis comes to the same conclusion that due to the volatile nature of neural networks and the skewness of the data, it is incredibly difficult to get good results. Future work in the field could result in fairer prices for customers on their insurance premiums.</p>
----------------------------------------------------------------------
In diva2:1764265 abstract is:
<p>The effect of proton (H+) irradiation on uranium mononitride (UN) and UN compositefuel with 10 at.% ZrN (UN10at%ZrN) was examined. Protons of 2 MeV with fluences of1E17, 1E18, 1E19 and 1E20 ions/cm2 were accelerated towards the fabricated samples in orderto investigate the evolution of the micro-structure. Stopping and Range of Ions in Matter(SRIM) calculations were performed to determine the displacements per atom associatedwith the depth of the highest damage, for each fluence.X-Ray diffraction (XRD) was used in both samples to identify the chemical composition ofeach pellet, which revealed the low presence of oxygen. Based on scanning electron microscopy(SEM), deterioration of the samples surface was observed, as the proton fluence increased.The applied stress due to the irradiation, led to the cracking of the pellets at the highestfluences. Blisters and craters appear to surround the cracked region, which might originatefrom the significant levels of hydrogen implantation within the samples.From Electron backscatter diffraction (EBSD) analysis, the grain size of the UN10at%ZrNcomposite was found to be smaller than in UN, due to the nano-particle nature of the ZrNpowder. The latter technique was also used to observe the elevated irradiated regions, whichwere further investigated by atomic force microscopy (AFM). Nano-indentation detectedirradiation hardening for both samples in the irradiated regions. Focused ion beam (FIB)milling was applied to remove lamellas from the cracked regions in both UN and compositesamples in order to be analyzed by transmission electron microscopy (TEM). The latter mightreveals the formation of dislocation loops in the irradiated areas.</p>

corrected abstract:
<p>The effect of proton (H<sup>+</sup>) irradiation on uranium mononitride (UN) and UN composite fuel with 10 at.% ZrN (UN10at%ZrN) was examined. Protons of 2 MeV with fluences of 10<sup>17</sup>, 10<sup>18</sup>, 10<sup>19</sup> and 10<sup>20</sup> ions/cm<sup>2</sup> were accelerated towards the fabricated samples in order to investigate the evolution of the micro-structure. Stopping and Range of Ions in Matter (SRIM) calculations were performed to determine the displacements per atom associated with the depth of the highest damage, for each fluence.</p><p>X-Ray diffraction (XRD) was used in both samples to identify the chemical composition of each pellet, which revealed the low presence of oxygen. Based on scanning electron microscopy (SEM), deterioration of the samples surface was observed, as the proton fluence increased. The applied stress due to the irradiation, led to the cracking of the pellets at the highest fluences. Blisters and craters appear to surround the cracked region, which might originate from the significant levels of hydrogen implantation within the samples.</p><p>From Electron backscatter diffraction (EBSD) analysis, the grain size of the UN10at%ZrN composite was found to be smaller than in UN, due to the nano-particle nature of the ZrN powder. The latter technique was also used to observe the elevated irradiated regions, which were further investigated by atomic force microscopy (AFM). Nano-indentation detected irradiation hardening for both samples in the irradiated regions. Focused ion beam (FIB) milling was applied to remove lamellas from the cracked regions in both UN and composite samples in order to be analyzed by transmission electron microscopy (TEM). The latter might reveals the formation of dislocation loops in the irradiated areas.</p>
----------------------------------------------------------------------
In diva2:1745694 - Note: no full text in DiVA
abstract is:
<p>A nuclear power plant is composed of a set of systems whose objective is to producecarbon-free, controllable and safe electricity. Safety is assured by essential elementslike the ventilation of the premises. This makes it possible to regulate the temperatureof certain equipmens and to provide pleasant climatic conditions inside the enclosure.The loss of ventilation can have serious repercussions on the operation of the plant.In this study the consequences of a loss of electrical room ventilation (DVL) followinga fire in the electrical room for the 900MW pressurised water reactors (PWR) isdetermined. France has 32 units of this type of reactor. The electrical architectureof the DVL system is studied in details.Since the Three Miles Island accident, a new safety analysis methodology has beendeveloped to complement the deterministic approach: the probabilistic approach. Ananalysis in the form of a probabilistic safety study makes it possible to quantify therisk of core damage due to the loss of ventilation. The study carries out with RiskSpectrum software shows, first of all, a significant evolution of the risk resulting fromfire in certain electrical rooms. The accidental analysis allows the identification of themost important accidental scenarios. The approach is then to identify certaincountermeasures to limit the serious phenomena resulting from this loss of ventilation.At the end, the overall risk is limited thanks to the valuation of some human actions.</p>

corrected abstract:
<p>A nuclear power plant is composed of a set of systems whose objective is to produce carbon-free, controllable and safe electricity. Safety is assured by essential elements like the ventilation of the premises. This makes it possible to regulate the temperature of certain equipments and to provide pleasant climatic conditions inside the enclosure. The loss of ventilation can have serious repercussions on the operation of the plant. In this study the consequences of a loss of electrical room ventilation (DVL) following a fire in the electrical room for the 900MW pressurised water reactors (PWR) is determined. France has 32 units of this type of reactor. The electrical architecture of the DVL system is studied in details. Since the Three Miles Island accident, a new safety analysis methodology has been developed to complement the deterministic approach: the probabilistic approach. An analysis in the form of a probabilistic safety study makes it possible to quantify the risk of core damage due to the loss of ventilation. The study carries out with RiskSpectrum software shows, first of all, a significant evolution of the risk resulting from fire in certain electrical rooms. The accidental analysis allows the identification of the most important accidental scenarios. The approach is then to identify certain countermeasures to limit the serious phenomena resulting from this loss of ventilation. At the end, the overall risk is limited thanks to the valuation of some human actions.</p>
----------------------------------------------------------------------
In diva2:1739365 abstract is:
<p>This master thesis aims in first instance to provide insights about the groundoperations that are carried out by different space players in order to exploit theirreusable space launch vehicles. The CNES is currently developing the CALLISTO flightdemonstrator with the objective of demonstrating the recovery and reuse capabilityof a vertical take-off and landing vehicle, for the benefit of future operational systemdevelopments. This paper also presents the work achieved in support of the design ofCALLISTO ground operations, namely via the use of 3D representation.The methodology adopted to perform the search and participate to the definition ofoperations is first described. Lessons are learned from the process of benchmark,specifically in the area of operations. Moreover, a comparison of the duration ofthe operational timelines of several vehicles is performed. Concerning some specificvehicles especially the DC-X, the RVT and the Falcon 9, significant information relatedto the content of the main operation phases is gathered and shown. An analysisof SpaceX actions to achieve reusability is performed. Focus is also posed on themaintenance operations in the aeronautics field. Regarding the 3D representation ofoperations, the method and the benefits are underlined with support from pictures ofthe final deliverables.</p>

corrected abstract:
<p>This master thesis aims in first instance to provide insights about the ground operations that are carried out by different space players in order to exploit their reusable space launch vehicles. The CNES is currently developing the CALLISTO flight demonstrator with the objective of demonstrating the recovery and reuse capability of a vertical take-off and landing vehicle, for the benefit of future operational system developments. This paper also presents the work achieved in support of the design of CALLISTO ground operations, namely via the use of 3D representation.</p><p>The methodology adopted to perform the search and participate to the definition of operations is first described. Lessons are learned from the process of benchmark, specifically in the area of operations. Moreover, a comparison of the duration of the operational timelines of several vehicles is performed. Concerning some specific vehicles especially the DC-X, the RVT and the Falcon 9, significant information related to the content of the main operation phases is gathered and shown. An analysis of SpaceX actions to achieve reusability is performed. Focus is also posed on the maintenance operations in the aeronautics field. Regarding the 3D representation of operations, the method and the benefits are underlined with support from pictures of the final deliverables.</p>
----------------------------------------------------------------------
In diva2:1695944 abstract is:
<p>Neutron stars are stellar objects of extreme properties. The dense core enables usto study nuclear matter beyond saturation density. The exact composition of matterat such densities is not yet established, but the thermodynamic states of the matteris theoreticized by the Equation of State (EOS). The EOS cannot be derived analyt-ically and is dependent on constraints from neutron stars and nuclear experiments inlaboratories on earth. Recent advances in astrophysical experiments have probed newconstraints on the EOS by studying properties such as mass, radius and tidal deformabil-ity of neutron stars. Especially the possibility to detect gravitational waves from mergingbinary systems by the LIGO/VIRGO collaboration and the mass-radius measurementsby NICER have contributed a great deal. Constraints from terrestrial experiments havebeen derived by studying matter at supra saturation density in Heavy Ion Collisions andby determining the neutron skin thickness. In this work, an overview of neutron stars,dense matter and the EOS is presented. Further, results of studies aiming to determineand constrain the EOS are reviewed. Even though there is consensus about some neutronstar properties among different research groups, there are still major uncertainties as allresult depend on a relatively small set of observational data. Therefore, the EOS can stillbe considered to be far from precise and the knowledge of the true neutron star matterremains undisclosed.</p>

corrected abstract:
<p>Neutron stars are stellar objects of extreme properties. The dense core enables us to study nuclear matter beyond saturation density. The exact composition of matter at such densities is not yet established, but the thermodynamic states of the matter is theoreticized by the Equation of State (EOS). The EOS cannot be derived analytically and is dependent on constraints from neutron stars and nuclear experiments in laboratories on earth. Recent advances in astrophysical experiments have probed new constraints on the EOS by studying properties such as mass, radius and tidal deformability of neutron stars. Especially the possibility to detect gravitational waves from merging binary systems by the LIGO/VIRGO collaboration and the mass-radius measurements by NICER have contributed a great deal. Constraints from terrestrial experiments have been derived by studying matter at supra saturation density in Heavy Ion Collisions and by determining the neutron skin thickness. In this work, an overview of neutron stars, dense matter and the EOS is presented. Further, results of studies aiming to determine and constrain the EOS are reviewed. Even though there is consensus about some neutron star properties among different research groups, there are still major uncertainties as all result depend on a relatively small set of observational data. Therefore, the EOS can still be considered to be far from precise and the knowledge of the true neutron star matter remains undisclosed.</p>
----------------------------------------------------------------------
In diva2:1695518 abstract is: <p>In this paper, a periodic maintenance model is formulated assumingcontinuous monitoring, imperfect preventive maintenance (PM) and perfect correctivemaintenance (CM) using three decision variables, (I, N, Z). The model is derived in aninfinite horizon context where the mean cost per unit time is modelled. PM actionsare performed N − 1 times at time instants iT for i = 1, ..., N − 1, where T = ∆T · Iand ∆T is a fixed positive number representing the minimum time allowed betweenPM actions and I is a time interval multiple representing the decision of how oftenPM actions should be performed. The N:th maintenance activity is either a plannedreplacement (if Z = 0) or a corrective replacement from letting the component runto failure (if Z = 1). Imperfect PM is modelled using age reductions, either using aconstant r or a factor γ. Previous research on assumptions of these types has beenlimited as the assumptions yield models of high complexity which are not analyticallytractable. However, assumptions of this type are considered more realistic than othermore thoroughly researched assumptions, using e.g. minimal CM. Therefore, twocomplimentary optimisation methods are proposed and evaluated, namely, completeenumeration and a specially derived genetic algorithm which can be used for differentproblem sizes respectively. Carefully determined solution bounds enabled completeenumeration to be applicable for many input parameter values which is a great strengthof the proposed model.</p>


corrected abstract:
<p>In this paper, a periodic maintenance model is formulated assuming continuous monitoring, imperfect preventive maintenance (PM) and perfect corrective maintenance (CM) using three decision variables, (<em>I</em>, <em>N</em>, <em>Z</em>). The model is derived in an infinite horizon context where the mean cost per unit time is modelled. PM actions are performed <em>N − 1</em> times at time instants <em>iT<em> for <em>i = 1, &mldr;, N − 1</em>, where <em>T = ∆T · I</em> and <em>∆T</em> is a fixed positive number representing the minimum time allowed between PM actions and <em>I</em> is a time interval multiple representing the decision of how often PM actions should be performed. The <em>N</em>:th maintenance activity is either a planned replacement (if <em>Z = 0</em>) or a corrective replacement from letting the component run to failure (if <em>Z = 1</em>). Imperfect PM is modelled using age reductions, either using a constant <em>r</em> or a factor <em>γ</em>. Previous research on assumptions of these types has been limited as the assumptions yield models of high complexity which are not analytically tractable. However, assumptions of this type are considered more realistic than other more thoroughly researched assumptions, using e.g. minimal CM. Therefore, two complimentary optimisation methods are proposed and evaluated, namely, complete enumeration and a specially derived genetic algorithm which can be used for different problem sizes respectively. Carefully determined solution bounds enabled complete enumeration to be applicable for many input parameter values which is a great strength of the proposed model.</p>
----------------------------------------------------------------------
In diva2:1682167 abstract is:
<p>Parametric amplifiers are essential for analyzing and measuring the weak signals generated byquantum circuits at cryogenic temperatures. This project aims to realize a low noise travelingwave parametric amplifier (TWPA) by exploiting the nonlinear current dependence of thekinetic inductance of superconducting NbTiN nanowires. We fabricate an inductor in theform of a compact meandering nanostructure on small chips. We describe the microwavecircuit design, the simulations performed and the fabrication recipes. We present the resultsfrom the initial measurements at low-temperature (4.2 K - 0.3 K) performed in a 3He dipstickcryostat.We analyzed two different structures in this thesis. The first design implements a co-planarwaveguide structure that operates as a multi-modal cavity with resonances that can be modifiedby adjusting geometrical parameters. In contrast, the second design attempts to eliminate theseresonances by matching the impedance of the device with that of the input and output signallines. For this reason, we adopted a microstrip structure with a top-layered ground plane.In addition, the second design allows for phase matching of the signal and idler frequenciesinvolved in parametric amplification through dispersion engineering. Finally, we determineimportant parameters like the temperature dependence of the kinetic inductance, phase velocity,and characteristic impedance of the devices at cryogenic temperatures.</p>

corrected abstract:
<p>Parametric amplifiers are essential for analyzing and measuring the weak signals generated by quantum circuits at cryogenic temperatures. This project aims to realize a low noise traveling wave parametric amplifier (TWPA) by exploiting the nonlinear current dependence of the kinetic inductance of superconducting NbTiN nanowires. We fabricate an inductor in the form of a compact meandering nanostructure on small chips. We describe the microwave circuit design, the simulations performed and the fabrication recipes. We present the results from the initial measurements at low-temperature (4.2 K - 0.3 K) performed in a <sup>3</sup>He dipstick cryostat.</p><p>We analyzed two different structures in this thesis. The first design implements a co-planar waveguide structure that operates as a multi-modal cavity with resonances that can be modified by adjusting geometrical parameters. In contrast, the second design attempts to eliminate these resonances by matching the impedance of the device with that of the input and output signal lines. For this reason, we adopted a microstrip structure with a top-layered ground plane. In addition, the second design allows for phase matching of the signal and idler frequencies involved in parametric amplification through dispersion engineering. Finally, we determine important parameters like the temperature dependence of the kinetic inductance, phase velocity, and characteristic impedance of the devices at cryogenic temperatures.</p>
----------------------------------------------------------------------
In diva2:1673657 abstract is:
<p>The evolution of electronics in the vehicle industry has introduced the possibility for more X-by-wire systems in future vehicles. However, the use of steer-by-wire systems has not yet been widelyimplemented. This opens up an opportunity to explore strategies around the potential use of such asystem.The purpose of the project was to evaluate how to design a variable steering ratio which would give asuitable ratio in all speeds. This would, in turn, make it possible to reduce the need for large steeringwheel angles. Additionally, steering wheel designs which can be implemented with a steer-by-wiresystems are discussed and what possibilities there are to move certain interfaces to the steering wheel.The evaluation process consisted of driving a real car with a constant steering ratio in normal trafficand later modelling a variable steering ratio and testing it in a simulator. This was to get data on howlarge steering wheel angles that are needed in different driving scenarios to then be able to design asuitable variable steering ratio. The tests conducted on normal roads in a real car has shown that thedriver utilises the whole steering range (full lock-lock distance) at speeds below 30 km/h and about±10° on the steering wheel at high speeds. The tests conducted in the simulator show that the variablesteering ratio presented in this report decreases the workload on the driver most of all at speeds below30 km/h.The variable steering ratio presented has been compared with a fixed steering ratio in the simulatorand the tests show that the variable steering ratio works similar to the fixed steering ratio in the samescenarios. The variable steering ratio also decreases the need for a steering wheel angle greater than±180°.</p><p> </p>

corrected abstract:
<p>The evolution of electronics in the vehicle industry has introduced the possibility for more X-by-wire systems in future vehicles. However, the use of steer-by-wire systems has not yet been widely implemented. This opens up an opportunity to explore strategies around the potential use of such a system.</p><p>The purpose of the project was to evaluate how to design a variable steering ratio which would give a suitable ratio in all speeds. This would, in turn, make it possible to reduce the need for large steering wheel angles. Additionally, steering wheel designs which can be implemented with a steer-by-wire systems are discussed and what possibilities there are to move certain interfaces to the steering wheel.</p><p>The evaluation process consisted of driving a real car with a constant steering ratio in normal traffic and later modelling a variable steering ratio and testing it in a simulator. This was to get data on how large steering wheel angles that are needed in different driving scenarios to then be able to design a suitable variable steering ratio. The tests conducted on normal roads in a real car has shown that the driver utilises the whole steering range (full lock-lock distance) at speeds below 30 km/h and about ±10° on the steering wheel at high speeds. The tests conducted in the simulator show that the variable steering ratio presented in this report decreases the workload on the driver most of all at speeds below 30 km/h.</p><p>The variable steering ratio presented has been compared with a fixed steering ratio in the simulator and the tests show that the variable steering ratio works similar to the fixed steering ratio in the same scenarios. The variable steering ratio also decreases the need for a steering wheel angle greater than ±180°.</p>
----------------------------------------------------------------------
In diva2:1656357 abstract is:
<p>With the rise of artificial intelligence, applications for machine learning can be found in nearly everyaspect of modern life, from healthcare and transportation to software services like recommendationsystems. Consequently, there are now more developers engaged in the field than ever - with the numberof implementations rapidly increasing by the day. In order to meet the new demands, it would be usefulto provide services that allow for an easy orchestration of a large number of repositories. Enabling usersto easily share, access and search for source code would be beneficial for both research and industryalike. A first step towards this is to find methods for clustering source code by functionality.</p><p>The problem of clustering source code has previously been studied in the literature. However, theproposed methods have so far not leveraged the capabilities of deep neural networks (DNN). In thiswork, we investigate the possibility of using DNNs to learn embeddings of source code for the purpose ofclustering by functionality. In particular, we evaluate embeddings from Code2Vec and cuBERT modelsfor this specific purpose.</p><p>From the results of our work we conclude that both Code2Vec and cuBERT are capable of learningsuch embeddings. Among the different frameworks that we used to fine-tune cuBERT, we found thebest performance for this task when fine-tuning the model under the triplet loss criterion. With thisframework, the model was capable of learning embeddings that yielded the most compact and well-separated clusters. We found that a majority of the cluster assignments were semantically coherent withrespect to the functionalities implemented by the methods. With these results, we have found evidenceindicating that it is possible to learn embeddings of source code that encode the functional similaritiesamong the methods. Future research could therefore aim to further investigate the possible applicationsof the embeddings learned by the different frameworks.</p>

corrected abstract:
<p>With the rise of artificial intelligence, applications for machine learning can be found in nearly every aspect of modern life, from healthcare and transportation to software services like recommendation systems. Consequently, there are now more developers engaged in the field than ever - with the number of implementations rapidly increasing by the day. In order to meet the new demands, it would be useful to provide services that allow for an easy orchestration of a large number of repositories. Enabling users to easily share, access and search for source code would be beneficial for both research and industry alike. A first step towards this is to find methods for clustering source code by functionality.</p><p>The problem of clustering source code has previously been studied in the literature. However, the proposed methods have so far not leveraged the capabilities of deep neural networks (DNN). In this work, we investigate the possibility of using DNNs to learn embeddings of source code for the purpose of clustering by functionality. In particular, we evaluate embeddings from Code2Vec and cuBERT models for this specific purpose.</p><p>From the results of our work we conclude that both Code2Vec and cuBERT are capable of learning such embeddings. Among the different frameworks that we used to fine-tune cuBERT, we found the best performance for this task when fine-tuning the model under the triplet loss criterion. With this framework, the model was capable of learning embeddings that yielded the most compact and well-separated clusters. We found that a majority of the cluster assignments were semantically coherent with respect to the functionalities implemented by the methods. With these results, we have found evidence indicating that it is possible to learn embeddings of source code that encode the functional similarities among the methods. Future research could therefore aim to further investigate the possible applications of the embeddings learned by the different frameworks.</p>
----------------------------------------------------------------------
In diva2:1655431 abstract is:
<p>In recent years the number of Special Purpose Acquisition Companies that have entered financialmarkets has increased tremendously, especially in the United States. An important query to this ishow they are valued and priced on stock exchanges. Using multiple regression analysis, the thesisexamines how different factors in a Special Purpose Acquisition Company relates to the stock price atdifferent phases of the companies lifetime. Further, it analyzes if these factors can help determining thestock price of similar companies in the future by proposing a model. Our result presents two modelswhich describes the stock price at target announced date and merger complete date respectively. Thetarget announced model is dependent of the initial term of the company, how many days until a targetcompany is specified and also if the company is specified to acquire in tech, energy or healthcare.However, the presented model for the target announced stock price shows weak accuracy and shouldin our proposition not be used in practice — and if at all — with great caution. The merger completemodel that is presented shows that the merger complete stock price is highly dependent on the targetannounced stock price, and although this model is somewhat more accurate, its practical purposes isalso limited due to inaccuracy. In conclusion, despite some relationship between the stock price andexamined factors being detected, our models are not appropriate to be used for practical purposes.Although, the target announced stock price can be used as a indicator of in which direction the mergercomplete stock price is heading.</p>

corrected abstract:
<p>In recent years the number of Special Purpose Acquisition Companies that have entered financial markets has increased tremendously, especially in the United States. An important query to this is how they are valued and priced on stock exchanges. Using multiple regression analysis, the thesis examines how different factors in a Special Purpose Acquisition Company relates to the stock price at different phases of the companies lifetime. Further, it analyzes if these factors can help determining the stock price of similar companies in the future by proposing a model. Our result presents two models which describes the stock price at target announced date and merger complete date respectively. The target announced model is dependent of the initial term of the company, how many days until a target company is specified and also if the company is specified to acquire in tech, energy or healthcare. However, the presented model for the target announced stock price shows weak accuracy and should in our proposition not be used in practice — and if at all — with great caution. The merger complete model that is presented shows that the merger complete stock price is highly dependent on the target announced stock price, and although this model is somewhat more accurate, its practical purposes is also limited due to inaccuracy. In conclusion, despite some relationship between the stock price and examined factors being detected, our models are not appropriate to be used for practical purposes. Although, the target announced stock price can be used as a indicator of in which direction the merger complete stock price is heading.</p>
----------------------------------------------------------------------
In diva2:1613485 abstract is:
<p>Using autonomous underwater vehicles is a popular method of collecting samplesand conducting surveys, but the transportation of this information is not always easy.The underwater vehicle might be unable to transmit the information wirelessly, andsamples may be required to be transported a long distance. A possible solution tothese problems is a hybrid unmanned aerial vehicle, accompanying said underwatervehicle. After a submerged deployment, this vehicle could transport the informationover long distances, or conduct other operations at different locations in air or water.While quadcopters are an increasingly popular type of vehicle, conventional fixed­wingplanes are still superior when it comes to range. This thesis designs, builds and testssuch a vehicle, with the goal of a submerged deployment, performing vertical takeoff,and then transitioning to fixed­wing flight. To minimize the drone’s impact on thevehicle which it accompanies, it is nearly buoyancy neutral by flooding the hull withwater, which enters and exits the vehicle rapidly during dive and egress. To managethe pressure at the underwater vehicle’s operating depth, it utilizes a bladder ratherthan having a heavy rigid compartment. It floats as a tailsitter at the surface, usingtwo motors in a tractor configuration to pull itself out of the water. The vehicle builtproved capable of being submerged and taking off vertically, however there were nofixed­wing flights attempted.</p>


corrected abstract:
<p>Using autonomous underwater vehicles is a popular method of collecting samples and conducting surveys, but the transportation of this information is not always easy. The underwater vehicle might be unable to transmit the information wirelessly, and samples may be required to be transported a long distance. A possible solution to these problems is a hybrid unmanned aerial vehicle, accompanying said underwater vehicle. After a submerged deployment, this vehicle could transport the information over long distances, or conduct other operations at different locations in air or water. While quadcopters are an increasingly popular type of vehicle, conventional fixed-wing planes are still superior when it comes to range. This thesis designs, builds and tests such a vehicle, with the goal of a submerged deployment, performing vertical takeoff, and then transitioning to fixed­wing flight. To minimize the drone’s impact on the vehicle which it accompanies, it is nearly buoyancy neutral by flooding the hull with water, which enters and exits the vehicle rapidly during dive and egress. To manage the pressure at the underwater vehicle’s operating depth, it utilizes a bladder rather than having a heavy rigid compartment. It floats as a tailsitter at the surface, using two motors in a tractor configuration to pull itself out of the water. The vehicle built proved capable of being submerged and taking off vertically, however there were no fixed­wing flights attempted.</p>
----------------------------------------------------------------------
In diva2:1528147 abstract is:
<p>This paper presents the work from a Master thesisat Bauhaus Luftfahrt (Munich, Germany) about subsystemdesign and analysis for electric commercial aircraft, particularlythe assessment of a Solid Oxide Fuel Cell (SOFC) powering a fullelectric subsystem architecture. The different components of theSOFC system architecture are modelled and assessed with theOpenMDAO framework. They are then assembled together toassess the performance of the whole SOFC system architecture,the main goal being to replace the conventional Auxiliary PowerUnit (APU) on the ground and to provide energy to all thesubsystems (e.g. flight controls, air conditioning, ice and rainprotection among others) of the aircraft during flight andground operations. The mass of the different components of theSOFC system is calculated, and a 2% operational empty massincrease is assumed for subsystem electrification. The resultsshow a kerosene block fuel reduction of 2.1% compared to theconventional baseline aircraft.</p>

corrected abstract:
<p>This paper presents the work from a Master thesis at Bauhaus Luftfahrt (Munich, Germany) about subsystem design and analysis for electric commercial aircraft, particularly the assessment of a Solid Oxide Fuel Cell (SOFC) powering a full electric subsystem architecture. The different components of the SOFC system architecture are modelled and assessed with the OpenMDAO framework. They are then assembled together to assess the performance of the whole SOFC system architecture, the main goal being to replace the conventional Auxiliary Power Unit (APU) on the ground and to provide energy to all the subsystems (e.g. flight controls, air conditioning, ice and rain protection among others) of the aircraft during flight and ground operations. The mass of the different components of the SOFC system is calculated, and a 2% operational empty mass increase is assumed for subsystem electrification. The results show a kerosene block fuel reduction of 2.1% compared to the conventional baseline aircraft.</p>
----------------------------------------------------------------------
In diva2:1528135 - missing spaces in title:
"Development of an underwaterruler using an AUV-deployedbeacon and Matched filter CFARdetector"
==>
"Development of an underwater ruler using an AUV-deployed beacon and Matched filter CFAR detector"

abstract is:
<p>Our ocean covers more than 70% of our planet’s surface. It is a huge reservoir continuously supplying us with enormousvaluable resources. The autonomous aquaculture, deep sea mining, subsea oil / gas exploitation and marine biology etc.,are the increasingly important driving forces for the development of underwater robotics. For all underwater robotics,navigation and positioning system often endures a challenging problem due to the high attenuation of radio-frequencysignals and the lack of Global Positioning System (GPS). Normally, acoustic navigation is the only way for theunderwater robotics to have the accurate navigation. One core element for the acoustic localisation is the rangeestimation. To provide an accurate range estimate from the underwater vehicles to the fixed reference point, an dropbeaconneeds to be established. In the underwater environment, the acoustic signals suffer from multi-path effects,ambient noises etc., which together with its simplified hardware can impose restrictions on the development of a perfectsystem. This project employs a very simplified hardware to deal with the range estimation problem from the beacon tothe transmitter subjected to the above mentioned issues. The algorithm consists of three main components includingdownsampling, matched filtering and CFAR detection. The downsampling is comprised of three steps such asbasebanding, lowpass filtering and resampling with lower sampling rate. By simulating four different signal typesincluding sinusoidal signal, frequency-modulated signals and M-sequence signal, the sinusoidal waveform is selected tosuit the system’s objective both for simplicity and robustness. A series of tests including tube test, water tank test, nearfieldopen water tests as well as LoLo integration and far-field tests verified and validated the system’s capability toestimate the accurate range (error ≤ 1m) in near field cases (≤ 20 meters). For far-field tests, it proved some furtherimprovements need to be accomplished before it is able to carry out long range missions.</p>

corrected abstract:
<p>Our ocean covers more than 70% of our planet’s surface. It is a huge reservoir continuously supplying us with enormous valuable resources. The autonomous aquaculture, deep sea mining, subsea oil / gas exploitation and marine biology etc., are the increasingly important driving forces for the development of underwater robotics. For all underwater robotics, navigation and positioning system often endures a challenging problem due to the high attenuation of radio-frequency signals and the lack of Global Positioning System (GPS). Normally, acoustic navigation is the only way for the underwater robotics to have the accurate navigation. One core element for the acoustic localisation is the range estimation. To provide an accurate range estimate from the underwater vehicles to the fixed reference point, an drop-beacon needs to be established. In the underwater environment, the acoustic signals suffer from multi-path effects, ambient noises etc., which together with its simplified hardware can impose restrictions on the development of a perfect system. This project employs a very simplified hardware to deal with the range estimation problem from the beacon to the transmitter subjected to the above mentioned issues. The algorithm consists of three main components including downsampling, matched filtering and CFAR detection. The downsampling is comprised of three steps such as basebanding, lowpass filtering and resampling with lower sampling rate. By simulating four different signal types including sinusoidal signal, frequency-modulated signals and M-sequence signal, the sinusoidal waveform is selected to suit the system’s objective both for simplicity and robustness. A series of tests including tube test, water tank test, nearfield open water tests as well as LoLo integration and far-field tests verified and validated the system’s capability to estimate the accurate range (error ≤ 1m) in near field cases (≤ 20 meters). For far-field tests, it proved some further improvements need to be accomplished before it is able to carry out long range missions.</p>
----------------------------------------------------------------------
In diva2:1216693 - missing space in title:
Autonomous Overtaking Using Reachability Analysisand MPC
==>
"Autonomous Overtaking Using Reachability Analysis and MPC"

abstract is:
<p>The era of autonomous cars is on the rise. Asdrivers lose control of the steering wheel, it is crucial that thecars themselves can guarantee safety for all traffic participants.This study aims to design a complete control system that cansafely perform an overtaking maneuver. To guarantee safety ofthe maneuver, reachability calculations will be carried out andanalyzed. The overtaking will be planned by using the modelpredictive control, MPC, framework. To complete the controlsystem a modified proportional controller will be designed totrack the planned path. The control system is implemented inMATLAB and the entire overtaking maneuver is simulated. Theresults show that the designed control framework successfullyperforms the overtaking on a straight two-lane highway in asafe manner.</p>

corrected abstract:
<p>The era of autonomous cars is on the rise. As drivers lose control of the steering wheel, it is crucial that the cars themselves can guarantee safety for all traffic participants. This study aims to design a complete control system that can safely perform an overtaking maneuver. To guarantee safety of the maneuver, reachability calculations will be carried out and analyzed. The overtaking will be planned by using the model predictive control, MPC, framework. To complete the control system a modified proportional controller will be designed to track the planned path. The control system is implemented in MATLAB and the entire overtaking maneuver is simulated. The results show that the designed control framework successfully performs the overtaking on a straight two-lane highway in a safe manner.</p>
----------------------------------------------------------------------
In diva2:1210175
abstract is:
<p>This thesis investigates the use of Artificial Neural Networks (ANNs)for calculating present values, Value-at-Risk and Expected Shortfall ofoptions, both European call options and more complex rainbow options. Theperformance of the ANN is evaluated by comparing it to a second-order Taylorpolynomial using pre-calculated sensitivities to certain risk-factors. Amultilayer perceptron approach is chosen based on previous literature andapplied to both types of options. The data is generated from a financial risk-managementsoftware for both call options and rainbow options along with the relatedTaylor approximations. The study shows that while the ANN outperforms theTaylor approximation in calculating present values and risk measures forcertain movements in the underlying risk-factors, the general conclusion isthat an ANN trained and evaluated in accordance with the method in this studydoes not outperform a Taylor approximation even if it is theoretically possiblefor the ANN to do so. The important conclusion of the study is that the ANNseems to be able to learn to calculate present values that otherwise requireMonte Carlo simulation. Thus, the study is a proof of concept that requiresfurther development for implementation.</p>

corrected abstract:
<p>This thesis investigates the use of Artificial Neural Networks (ANNs) for calculating present values, Value-at-Risk and Expected Shortfall of options, both European call options and more complex rainbow options. The performance of the ANN is evaluated by comparing it to a second-order Taylor polynomial using pre-calculated sensitivities to certain risk-factors. A multilayer perceptron approach is chosen based on previous literature and applied to both types of options. The data is generated from a financial risk-management software for both call options and rainbow options along with the related Taylor approximations. The study shows that while the ANN outperforms the Taylor approximation in calculating present values and risk measures for certain movements in the underlying risk-factors, the general conclusion is that an ANN trained and evaluated in accordance with the method in this study does not outperform a Taylor approximation even if it is theoretically possible for the ANN to do so. The important conclusion of the study is that the ANN seems to be able to learn to calculate present values that otherwise require Monte Carlo simulation. Thus, the study is a proof of concept that requires further development for implementation.</p>
----------------------------------------------------------------------
In diva2:1156678 - Note: no full text in DiVA
abstract is:
<p>Many industries are currently making the transition to high strength steel due to advantages relatedto the high strength to weight ratio of high strength steel. The fatigue properties in high strength steelshow a large dependency of the weld quality in structural applications. Recent studies show a weakrelation between quality classifications and fatigue properties in the current international standard forweld quality assurance, ISO 5817. The objective of this project is to develop and assess a method forpredicting fatigue life in welded joints based on measured weld geometry and applied load.Two different materials (S355 and S960) and two different material thicknesses (2mm and 8mm) wereused in this study. Experiments on cruciform joints were conducted to evaluate the fatigueperformance for different types of weld geometries. A computational model based on FEM and linearelastic fracture mechanics was developed and adapted to fit the experimental results usingoptimization and surrogate models.A comparison between the fatigue behavior according to the standards and the fatigue behavior givenby the computational model was thereafter made. The conclusions that can be made are that thegeneral fatigue behavior differs for the different materials for the same variation in geometry, thefatigue performance depends on a combination of geometrical parameters, the developed computational model cannot be used for the 2mm specimens in this particular case, the use of FAT-curves accordingto the weld quality systems is insufficient to describe fatigue properties for welds in thin high strengthsteel, and different geometries within different weld quality classes can give the same fatigue behavior.</p>

corrected abstract:
<p>Many industries are currently making the transition to high strength steel due to advantages related  to the high strength to weight ratio of high strength steel. The fatigue properties in high strength steel show a large dependency of the weld quality in structural applications. Recent studies show a weak relation between quality classifications and fatigue properties in the current international standard for weld quality assurance, ISO 5817. The objective of this project is to develop and assess a method for predicting fatigue life in welded joints based on measured weld geometry and applied load. Two different materials (S355 and S960) and two different material thicknesses (2mm and 8mm) were used in this study. Experiments on cruciform joints were conducted to evaluate the fatigue performance for different types of weld geometries. A computational model based on FEM and linearelastic fracture mechanics was developed and adapted to fit the experimental results using optimization and surrogate models. A comparison between the fatigue behavior according to the standards and the fatigue behavior given by the computational model was thereafter made. The conclusions that can be made are that the general fatigue behavior differs for the different materials for the same variation in geometry, the fatigue performance depends on a combination of geometrical parameters, the developed computational model cannot be used for the 2mm specimens in this particular case, the use of FAT-curves according to the weld quality systems is insufficient to describe fatigue properties for welds in thin high strength steel, and different geometries within different weld quality classes can give the same fatigue behavior.</p>
----------------------------------------------------------------------
In diva2:1120538 missing spaces in title:
"Effective Sampling and Windowingfor an Artificial Neural Network Model Used in Currency Exchange Rate Forecasting"
==>
"Effective Sampling and Windowing for an Artificial Neural Network Model Used in Currency Exchange Rate Forecasting"

abstract is: <p>Financial forecasting is a field of great interest in academia and economy. The subfield of exchangerate prediction is of considerable value to practically every entity operating within the financialmarket. Ranging from private hedgers, speculators or arbitrageurs to entire financial institutions suchas international banks or insurance companies, the ability to predict exchange rate movementsprovides major benefits for organizations in contact with these. A great multitude of research has beenconducted to construct methods to aid firms and investors to better anticipate on potentialdevelopments in the foreign exchange market. Much of the research has been focusing on a promisingprediction model within computational intelligence developed in recent decades, namely the ArtificialNeural Network (ANN). However, a review of existing literature suggests that the time step, predictionhorizon and window size have not been of central essence. Hence, this paper attempts to provide amore formal analysis of the actual impact of the three mentioned parameters on the prediction resultsof ANNs. Through literature studies, modeling and experimentation it is found that no specificcombination of time step, prediction horizon and window size results in more exact forecasts, but thatcertain combinations of the three parameters generally result in superior performance.</p>


corrected abstract:
<p>Financial forecasting is a field of great interest in academia and economy. The subfield of exchange rate prediction is of considerable value to practically every entity operating within the financial market. Ranging from private hedgers, speculators or arbitrageurs to entire financial institutions such as international banks or insurance companies, the ability to predict exchange rate movements provides major benefits for organizations in contact with these. A great multitude of research has been conducted to construct methods to aid firms and investors to better anticipate on potential developments in the foreign exchange market. Much of the research has been focusing on a promising prediction model within computational intelligence developed in recent decades, namely the Artificial Neural Network (ANN). However, a review of existing literature suggests that the time step, prediction horizon and window size have not been of central essence. Hence, this paper attempts to provide a more formal analysis of the actual impact of the three mentioned parameters on the prediction results of ANNs. Through literature studies, modeling and experimentation it is found that no specific combination of time step, prediction horizon and window size results in more exact forecasts, but that certain combinations of the three parameters generally result in superior performance.</p>
----------------------------------------------------------------------
In diva2:1078070 abstract is:
<p>The preliminary design of Guidance, Navigation, and Control (GNC) algorithms andAttitude and Orbital Control Systems (AOCS) for spacecraft plays an important rolein the planning of a space mission. Many missions require accurate positioning andattitude control to be able to full the mission objectives. In an early phase of conceptualstudies, trade-os have to be made to the GNC/AOCS subsystem designand compromises with respect to other subsystem designs have to be taken intoaccount. This demands for the possibility of rapid prototyping where design parameters,such as the choice of sensors and actuators, can be changed easily to assessthe compliance to the mission requirements. This thesis presents the modelling ofGNC/AOCS components for a toolbox created in the MATLAB/Simulink environment.The resulting toolbox is a user-friendly tool, which simplies the creationof GNC/AOCS system simulations for conceptual studies. A number of completesimulations were constructed to demonstrate the capabilities of the toolbox.</p>

corrected abstract:
<p>The preliminary design of Guidance, Navigation, and Control (GNC) algorithms and Attitude and Orbital Control Systems (AOCS) for spacecraft plays an important role in the planning of a space mission. Many missions require accurate positioning and attitude control to be able to fulfil the mission objectives. In an early phase of conceptual studies, trade-offs have to be made to the GNC/AOCS subsystem design and compromises with respect to other subsystem designs have to be taken into account. This demands for the possibility of rapid prototyping where design parameters, such as the choice of sensors and actuators, can be changed easily to assess the compliance to the mission requirements. This thesis presents the modelling of GNC/AOCS components for a toolbox created in the MATLAB/Simulink environment. The resulting toolbox is a user-friendly tool, which simplifies the creation of GNC/AOCS system simulations for conceptual studies. A number of complete simulations were constructed to demonstrate the capabilities of the toolbox.</p>
----------------------------------------------------------------------
In diva2:941181 - missing space in title:
"MicroCT system software updateand implementation of a beamhardening correction method"
==>
"MicroCT system software update and implementation of a beam hardening correction method"

abstract is:
<p>The School of Technology and Health, STH, has been developing a micro-CTscanner for pre-clinical imaging. The scanner consists of an X-ray tube and aflat panel sensor fixated on a gantry rotating around the object to be imaged. Acomputer located on the gantry runs an acquisition software for communicationbetween the devices on the gantry as well as for controlling them. For this thesisthe acquisition software has been significantly improved in terms of functionality,performance, usability and user-adaptivity.Projection images acquired by the microCT are created by measuring theX-ray beam attenuation as a function of spatial location. Using all projectionimages, a 3-dimensional image can be reconstructed giving a map of the attenuationinside the object, thus providing information about its internal structure.A common artifact for CT-scanner images is the <em>cupping arti fact</em> which the attenuation in the middle of an object is underestimated due to beam hardening.For the second part of this thesis, a method for correcting for this artifact hasbeen implemented and tested. The cupping artifact was successfully removedin most cases, although it was shown that for some situation it is not applicable</p>

corrected abstract:
<p>The School of Technology and Health, STH, has been developing a micro-CT scanner for pre-clinical imaging. The scanner consists of an X-ray tube and a flat panel sensor fixated on a gantry rotating around the object to be imaged. A computer located on the gantry runs an acquisition software for communication between the devices on the gantry as well as for controlling them. For this thesis the acquisition software has been significantly improved in terms of functionality, performance, usability and user-adaptivity.</p><p>Projection images acquired by the microCT are created by measuring the X-ray beam attenuation as a function of spatial location. Using all projection images, a 3-dimensional image can be reconstructed giving a map of the attenuation inside the object, thus providing information about its internal structure. A common artifact for CT-scanner images is the <em>cupping&mdash;artifact</em>, in which the attenuation in the middle of an object is underestimated due to beam hardening. For the second part of this thesis, a method for correcting for this artifact has been implemented and tested. The cupping artifact was successfully removed in most cases, although it was shown that for some situation it is not applicable</p>

Note: The original abstract does not end with a period.
----------------------------------------------------------------------
In diva2:852543 abstract is:
<p>This first level thesis focuses on the design of a human poweredaircraft which theoretically can complete the Kremer InternationalMarathon prize. The competition awards the firstheavier-than-air aircraft human-power-driven aircraft that completesa circuit of 42 195 meter in less than one hour with £50000.</p><p>The paper first introduces the reader to the basics of aircraftmechanics. It continues with the presentation of the design processwith examples of evaluated airfoils and key aspects.</p><p>Our conclusion of the study is that the Kremer InternationalMarathon prize is fully feasible to complete with the technologyof today. However, the project is not economically viabledue to high prizes of selected composite materials and the relativesmall size of the prize money. An increment of the prizewould certainly increase the research into human powered highperformance aircraft, which could lead to a successful KremerInternational Marathon prize attempt.</p><p>The limitations of this thesis are to strictly stay within the aerodynamicfield of study. Therefore, solid mechanics has not beenexactly calculated, but has been taken into account when dimensioningthe aircraft in CAD, and neither has any calculatedturning performance due to academic level.</p>

corrected abstract:
<p>This first level thesis focuses on the design of a human powered aircraft which theoretically can complete the Kremer International Marathon prize. The competition awards the first heavier-than-air aircraft human-power-driven aircraft that completes a circuit of 42 195 meter in less than one hour with £50000.</p><p>The paper first introduces the reader to the basics of aircraft mechanics. It continues with the presentation of the design process with examples of evaluated airfoils and key aspects.</p><p>Our conclusion of the study is that the Kremer International Marathon prize is fully feasible to complete with the technology of today. However, the project is not economically viable due to high prizes of selected composite materials and the relative small size of the prize money. An increment of the prize would certainly increase the research into human powered highperformance aircraft, which could lead to a successful Kremer International Marathon prize attempt.</p><p>The limitations of this thesis are to strictly stay within the aerodynamic field of study. Therefore, solid mechanics has not been exactly calculated, but has been taken into account when dimensioning the aircraft in CAD, and neither has any calculated turning performance due to academic level.</p>
----------------------------------------------------------------------
In diva2:515563 abstract is:
<p>Recent studies show that the knowledge in and understanding of science, technology andmathematics of Swedish junior high school and high school students is decreasing. As a resultof this the Swedish government has initialized several studies, research and reforms. One ofthe theories for teaching that today is seen as effective is the socio-cultural perspective and itsdescendants, including scientific inquiry. Using the pedagogy of the socio-cultural perspectivefour assignments regarding the human body was constructed for junior high school studentsattending summer research school in the summer of 2008, hosted by AstraZeneca. Theassignments follow a structure (goal of assignment, preparations, experiment and follow-up)inspired by the concept of NTA – Naturvetenskap och Teknik för Alla (Science andTechnology for everyone). The assignments can be found (in Swedish) as an appendix to thispaper. The assignments were evaluated and assessed through the observations made by theteacher and two questionnaires separated by six months. Results showed that three of the fourassignments had fulfilled the aim of making the students interested and engaged in scientificquestions. The forth had only partially achieved this, mostly due to a silent, built-in structureexpecting to generate certain knowledge in the students. This contradicts the socio-culturalperspective where new knowledge should be derived from what students already know.Seeing as the socio-cultural perspective was effective when using the other three assignmentsthis study shows that assignments constructed with a socio-cultural perspective are usefulwhen teaching science in junior high school.Nyckelord</p>

corrected abstract:
<p>Recent studies show that the knowledge in and understanding of science, technology and mathematics of Swedish junior high school and high school students is decreasing. As a result of this the Swedish government has initialized several studies, research and reforms. One of the theories for teaching that today is seen as effective is the socio-cultural perspective and its descendants, including scientific inquiry. Using the pedagogy of the socio-cultural perspective four assignments regarding the human body was constructed for junior high school students attending summer research school in the summer of 2008, hosted by AstraZeneca. The assignments follow a structure (goal of assignment, preparations, experiment and follow-up) inspired by the concept of NTA – <span lang="sv">Naturvetenskap och Teknik för Alla</span> (Science and Technology for everyone). The assignments can be found (in Swedish) as an appendix to this paper. The assignments were evaluated and assessed through the observations made by the teacher and two questionnaires separated by six months. Results showed that three of the four assignments had fulfilled the aim of making the students interested and engaged in scientific questions. The forth had only partially achieved this, mostly due to a silent, built-in structure expecting to generate certain knowledge in the students. This contradicts the socio-cultural perspective where new knowledge should be derived from what students already know. Seeing as the socio-cultural perspective was effective when using the other three assignments this study shows that assignments constructed with a socio-cultural perspective are useful when teaching science in junior high school.</p>
----------------------------------------------------------------------
In diva2:515459 - missing space, unnecessary hyphen, and unnecessary period in (sub)title:
"Environmental training for municipality officers inBosnia and Herzegovina: - A needs analysis performed before the start of a postgraduate programme for environmentalofficers in municipalities in Bosnia and Herzegovina."
==>
"Environmental training for municipality officers in Bosnia and Herzegovina: - A needs analysis performed before the start of a postgraduate programme for environmental officers in municipalities in Bosnia and Herzegovina"

abstract is:
<p>This is a study that aims to help create a better world. As big as it sounds, it is the truth. Everyday municipality officers in Bosnia and Herzegovina work for a better environment, a betterworld. Despite their efforts the work they do is not as efficient as it could be and the supportavailable for them is meagre. For this reason a programme called Municipality EnvironmentalInfrastructure is under development in cooperation between the University of Sarajevo (Bosniaand Herzegovina) and the Royal Institute of Technology (Sweden) with financing from theSwedish International Development Cooperation Agency (Sida). This thesis is a part of the workto make the programme successful.Competence needs for municipality officers will be identified through a needs analysis based oninterviews with different stakeholders. The answers provided will create a picture of the needsthat is both univocal and diverse with competences in identifying and handling environmentalthreats as well as managing infrastructure projects.The thesis will also look at what pedagogical methods the teachers at the programme plan to useand how this affects the programme. Since the programme is held in a formal setting but intendspractical use of the knowledge this leads to high demands on the pedagogical methods. Theprogramme syllabus will be fount to not entirely encompass all competence needs butsuggestions will be made as to how to include the identified needs.</p>

corrected abstract:
<p>This is a study that aims to help create a better world. As big as it sounds, it is the truth. Every day municipality officers in Bosnia and Herzegovina work for a better environment, a better world. Despite their efforts the work they do is not as efficient as it could be and the support available for them is meagre. For this reason a programme called Municipality Environmental Infrastructure is under development in cooperation between the University of Sarajevo (Bosnia and Herzegovina) and the Royal Institute of Technology (Sweden) with financing from the Swedish International Development Cooperation Agency (Sida). This thesis is a part of the work to make the programme successful.</p><p>Competence needs for municipality officers will be identified through a needs analysis based on interviews with different stakeholders. The answers provided will create a picture of the needs that is both univocal and diverse with competences in identifying and handling environmental threats as well as managing infrastructure projects.</p><p>The thesis will also look at what pedagogical methods the teachers at the programme plan to use and how this affects the programme. Since the programme is held in a formal setting but intends practical use of the knowledge this leads to high demands on the pedagogical methods. The programme syllabus will be fount to not entirely encompass all competence needs but suggestions will be made as to how to include the identified needs.</p>
----------------------------------------------------------------------
In diva2:492864 abstract is:
<p>This thesis project focuses on the verification method of a safety function called PICthat stands for Post-Impact Control which controls the vehicle motion of passengercars after being exposed to external disturbances produced by a 1st impact, aiming atavoiding or mitigating secondary events.The main objective was to select a promising method, among several candidates, todevelop further for testing the function and the interaction with the driver. To do thisis was first necessary to map the real destabilized states of motion that are targeted bythe function. These states are referred as Post-Impact problem space and are acombination of variables that describes the host vehicles motion at the instant thedestabilizing force has ceased. Knowing which states are requested by the solutioncandidates, it is possible to grade the rig candidates based on the capability ofcovering the problem space. Then, simulating the proposed rig solutions withMatlab/Simulink models to investigate which candidate fulfils best the problem space.The result of the simulations and other criteria is that a moving base simulator(Simulator SIM4) is most fitted to research verification. The second mostadvantageous solution is the rig alternative called Built-in Actuators.</p>

corrected abstract:
<p>This thesis project focuses on the verification method of a safety function called PIC that stands for Post-Impact Control which controls the vehicle motion of passenger cars after being exposed to external disturbances produced by a 1st impact, aiming at avoiding or mitigating secondary events.</p><p>The main objective was to select a promising method, among several candidates, to develop further for testing the function and the interaction with the driver. To do this is was first necessary to map the real destabilized states of motion that are targeted by the function. These states are referred as Post-Impact problem space and are a combination of variables that describes the host vehicles motion at the instant the destabilizing force has ceased. Knowing which states are requested by the solution candidates, it is possible to grade the rig candidates based on the capability of covering the problem space. Then, simulating the proposed rig solutions with Matlab/Simulink models to investigate which candidate fulfils best the problem space.</p><p>The result of the simulations and other criteria is that a moving base simulator (Simulator SIM4) is most fitted to research verification. The second most advantageous solution is the rig alternative called Built-in Actuators.</p>
----------------------------------------------------------------------
In diva2:435361 abstract is:
<p>In one of the largest renewable energy sectors, the Wind, research on maintenance on windturbines is surprisingly nearly nonexistent. No meaningful work has been made on optimizingthe scheduled maintenance process. Filling this gap this thesis stems.Unplanned maintenance is commonly synonym to large energy loss since the wind turbine mustbe stopped nearly throughout the whole duration of the maintenance procedure. All partiesevolved in the sector are hindered by this fact. It is therefore not only in all the sectors’ playersbut also the general publics’ interest to optimize this process for a more sustainable world.Responsible for all the calculations is a model, which was fully developed for this thesis and ispart of it. Having considered several programming languages the choice was Excel (VBA); beingthis software spread worldwide it encourages the models’ global implementation. Easy to use,versatility and accurate and prompt results were the guidelines for its developments. A weatherforecast is the fundamental input.Running the model on two different wind farms gave conclusive results. The energy loss wasreduced up to 71%. Also in some cases the time frame was cut up to 62%.Even with these promising figures energy loss must be significant in order to have a realeconomical impact. Nevertheless the model unveils the most convenient schedule formaintenance and its implementation is exclusively beneficial.</p>

corrected abstract:
<p>In one of the largest renewable energy sectors, the Wind, research on maintenance on wind turbines is surprisingly nearly nonexistent. No meaningful work has been made on optimizing the scheduled maintenance process. Filling this gap this thesis stems.</p><p>Unplanned maintenance is commonly synonym to large energy loss since the wind turbine must be stopped nearly throughout the whole duration of the maintenance procedure. All parties evolved in the sector are hindered by this fact. It is therefore not only in all the sectors’ players but also the general publics’ interest to optimize this process for a more sustainable world. Responsible for all the calculations is a model, which was fully developed for this thesis and is part of it. Having considered several programming languages the choice was Excel (VBA); being this software spread worldwide it encourages the models’ global implementation. Easy to use, versatility and accurate and prompt results were the guidelines for its developments. A weather forecast is the fundamental input.</p><p>Running the model on two different wind farms gave conclusive results. The energy loss was reduced up to 71%. Also in some cases the time frame was cut up to 62%. Even with these promising figures energy loss must be significant in order to have a real economical impact. Nevertheless the model unveils the most convenient schedule for maintenance and its implementation is exclusively beneficial.</p>
----------------------------------------------------------------------
In diva2:1827845 abstract is:
<p>Modern investors are considering investments in sustainable companies more than ever and oneof the main metrics that are of use is ESG performance. This criterion provides a comprehensiveoverview over an organization’s ability to generate value for all stakeholders, including employees,customers and shareholders. As the use of ESG criterion increases, so does the amount ofstudies regarding ESG performance and a company’s profitability. Most of these studies showa non negative correlation between the two. This study will however aim to examine how ESGperformance impact the efficiency and productivity level of a company. This will be done bystudying the sales generated by each employee. Moreover, the project will focus on two industries,tech and industrial and will analyse their differences. The results will be of help to business ownersand decision makers with questions regarding if investing in the company’s ESG performancecan be a way of increasing the company’s internal performance. The results of this projectsuggest that there exist a statistically significant correlation between them for both industries.The project also presented an insight to which categories of ESG are most significant in theregression model in the two different industries. The adjusted R2 for the regression model forindustrials was higher than the tech industry, which means that it is a more extensive correlationbetween the regressors and the response variable for the industrial industry.</p>

corrected abstract:
<p>Modern investors are considering investments in sustainable companies more than ever and one of the main metrics that are of use is ESG performance. This criterion provides a comprehensive overview over an organization’s ability to generate value for all stakeholders, including employees, customers and shareholders. As the use of ESG criterion increases, so does the amount of studies regarding ESG performance and a company’s profitability. Most of these studies show a non negative correlation between the two. This study will however aim to examine how ESG performance impact the efficiency and productivity level of a company. This will be done by studying the sales generated by each employee. Moreover, the project will focus on two industries, tech and industrial and will analyse their differences. The results will be of help to business owners and decision makers with questions regarding if investing in the company’s ESG performance can be a way of increasing the company’s internal performance. The results of this project suggest that there exist a statistically significant correlation between them for both industries. The project also presented an insight to which categories of ESG are most significant in the regression model in the two different industries. The adjusted R<sup>2</sup> for the regression model for industrials was higher than the tech industry, which means that it is a more extensive correlation between the regressors and the response variable for the industrial industry.</p>
----------------------------------------------------------------------
In diva2:1820859 abstract is:
<p>Anderson localization occurs when an otherwise conductive solid becomes insulatingdue to a sufficiently large degree of disorder in the medium. The electron band energy(as a function of disorder) at which this transition between extended and localizedelectron states occur is called the mobility edge (ME) and is energy-dependent only in3-dimensional systems. In lower dimensional systems, energy-independent ME (allstates localized or all extended) has been demonstrated by replacing disorder withquasi-periodic potential. However, recent theoretical findings indicate that neitherdisorder nor quasi-periodic potential is necessary for a material to exhibit electronlocalization and existence of energy-dependent pseudo ME at finite system size.In this thesis work, we use light in coupled silicon nitride waveguides to simulatesingle-particle transport of a solid-state medium and investigate the coexistence ofdelocalized and localized states in disorder-free photonic lattices of finite systemsize. This was achieved by implementing a simulated linearly increasing electricpotential on even-numbered sites by varying the refractive index of the wave guide(ch. 3). Through our experimental setup, we successfully achieved a coexistence oflocalized and delocalized states, where the degree of localization varies depending onthe strength of the applied electric field.The findings have implications for the field of quantum technology, whereunderstanding and controlling quantum states is crucial. The ability to achievelocalization in the absence of disorder opens new possibilities for designing andengineering photonic devices for quantum information processing tasks.</p>

corrected abstract:
<p>Anderson localization occurs when an otherwise conductive solid becomes insulating due to a sufficiently large degree of disorder in the medium. The electron band energy (as a function of disorder) at which this transition between extended and localized electron states occur is called the mobility edge (ME) and is energy-dependent only in 3-dimensional systems. In lower dimensional systems, energy-independent ME (all states localized or all extended) has been demonstrated by replacing disorder with quasi-periodic potential. However, recent theoretical findings indicate that neither disorder nor quasi-periodic potential is necessary for a material to exhibit electron localization and existence of energy-dependent pseudo ME at finite system size.</p><p>In this thesis work, we use light in coupled silicon nitride waveguides to simulate single-particle transport of a solid-state medium and investigate the coexistence of delocalized and localized states in disorder-free photonic lattices of finite system size. This was achieved by implementing a simulated linearly increasing electric potential on even-numbered sites by varying the refractive index of the wave guide (ch. 3). Through our experimental setup, we successfully achieved a coexistence of localized and delocalized states, where the degree of localization varies depending on the strength of the applied electric field.</p><p>The findings have implications for the field of quantum technology, where understanding and controlling quantum states is crucial. The ability to achieve localization in the absence of disorder opens new possibilities for designing and engineering photonic devices for quantum information processing tasks.</p>
----------------------------------------------------------------------
In diva2:1794337 abstract is:
<p>Patents are a fundamental part of scientific and engineering work, ensuringprotection of inventions owned by individuals or organizations. Patents areusually made public 18 months after being filed to a patent office, whichmeans that current publicly available patent data only provides informationabout the past. Regression models applied on discrete time series can be usedas a prediction tool to counteract this, building a 18 month long bridge intothe future and beyond. While linear models are popular for their simplicity,Bayesian networks have statistical properties that can produce high forecastingquality. Improvements is also made by using signal processing as patentdata is naturally stochastic. This thesis implements wavelet-based signalprocessing and P CA to increase stability and reduce overfitting. A multiplelinear regression model and a Bayesian network model is then designed andapplied to the transformed data. When evaluated on each data set, the Bayesianmodel both performs better and exhibits greater stability and consistency inits predictions. As expected, the linear model is both smaller and faster toevaluate and train. Despite an increase in complexity and slower evaluationtimes, the Bayesian model is conclusively superior to the linear model. Futurework should focus on the signal processing method and additional layers inthe Bayesian network.</p>

corrected abstract:
<p>Patents are a fundamental part of scientific and engineering work, ensuring protection of inventions owned by individuals or organizations. Patent families are usually made public 18 months after being filed to a patent office, which means that current publicly available patent data only provides information about the past. Regression models applied on discrete time series can be used as a prediction tool to counteract this, building a 18 month long bridge into the future and beyond. While linear models are popular for their simplicity, Bayesian networks have statistical properties that can produce high forecasting quality. Improvements is also made by using signal processing as patent data is naturally stochastic. This thesis implements wavelet-based signal processing and <em>PCA</em> to increase stability and reduce overfitting. A multiple linear regression model and a Bayesian network model is then designed and applied to the transformed data. When evaluated on each data set, the Bayesian model both performs better and exhibits greater stability and consistency in its predictions. As expected, the linear model is both smaller and faster to evaluate and train. Despite an increase in complexity and slower evaluation times, the Bayesian model is conclusively superior to the linear model. Future work should focus on the signal processing method and additional layers in the Bayesian network.</p>
----------------------------------------------------------------------
In diva2:1768905 abstract is:
<p>In this thesis the effect of an active suspension system versus a passive suspensionsystem utilized on a main battle tank/combat vehicle is studied. The effect oncomfort, mobility, accuracy and power draw is discussed. This is done with thegoal to study and understand if an improvement can be made using active sys-tems. This is done mainly by simulation of a main battle tank/combat vehicle inSimulink/Matlab.</p><p>The active suspension system chosen was the skyhook suspension system whichwas compared to a regular passive suspension. The results show that the activesuspension system has positive effects on comfort, mobility and accuracy. Thisthesis discusses the viability of such a system. This is done through analysing thepower draw of the active system and through reasoning about use in field situations.Besides comfort, mobility and accuracy, the results also show that in some cases theactive suspension would need more power than the engine can generate. In additionan active suspension system would be very sensitive due to the complexity of thecomponents needed. If it is possible to control the power draw and the sensitivityof an active suspension system it would vastly improve the comfort, mobility andaccuracy of main battle tanks/combat vehicles.</p>

corrected abstract:
<p>In this thesis the effect of an active suspension system versus a passive suspension system utilized on a main battle tank/combat vehicle is studied. The effect on comfort, mobility, accuracy and power draw is discussed. This is done with the goal to study and understand if an improvement can be made using active systems. This is done mainly by simulation of a main battle tank/combat vehicle in Simulink/Matlab.</p><p>The active suspension system chosen was the skyhook suspension system which was compared to a regular passive suspension. The results show that the active suspension system has positive effects on comfort, mobility and accuracy. This thesis discusses the viability of such a system. This is done through analysing the power draw of the active system and through reasoning about use in field situations. Besides comfort, mobility and accuracy, the results also show that in some cases the active suspension would need more power than the engine can generate. In addition an active suspension system would be very sensitive due to the complexity of the components needed. If it is possible to control the power draw and the sensitivity of an active suspension system it would vastly improve the comfort, mobility and accuracy of main battle tanks/combat vehicles.</p>
----------------------------------------------------------------------
In diva2:1740119 abstract is:
<p>As emissions of polluting substances due to shipping are growing, newsolutions for ship propulsion are emerging. One of them is the suctionwing, a vertical wing designed to generate lift forward, and made even moreefficient than classical wings by the suction of its boundary layer. Thisproject allowed for the participation, within a team of mechanical engineers,in the early design of a pilot model of such a system, ultimately intendedto be industrialised. The design process was followed in order to define thesolutions to match the performances of each function of the wing, basedon a set of specifications, and to verify the sustainability of those solutions.Also, a particular interest in the furniture and/or manufacturing of thosesolutions was given. For those instances, the suction system was defined withthe collaboration of manufacturers. Also, the wing orientation system wasdefined and its integration was verified with respect to the deformation ofthe components at its interfaces. Then, The flap was designed in order todefine solutions to allow mobility transmission, taking into account its stressconditions. The solution for the sealing of the system was also worked upon.Finally, the main mast of the wing was designed so to maintain its integrity,with respect to its stress charges and its strong stress concentrations.</p>

corrected abstract:
<p>As emissions of polluting substances due to shipping are growing, new solutions for ship propulsion are emerging. One of them is the suction wing, a vertical wing designed to generate lift forward, and made even more efficient than classical wings by the suction of its boundary layer. This project allowed for the participation, within a team of mechanical engineers, in the early design of a pilot model of such a system, ultimately intended to be industrialised. The design process was followed in order to define the solutions to match the performances of each function of the wing, based on a set of specifications, and to verify the sustainability of those solutions. Also, a particular interest in the furniture and/or manufacturing of those solutions was given. For those instances, the suction system was defined with the collaboration of manufacturers. Also, the wing orientation system was defined and its integration was verified with respect to the deformation of the components at its interfaces. Then, The flap was designed in order to define solutions to allow mobility transmission, taking into account its stress conditions. The solution for the sealing of the system was also worked upon. Finally, the main mast of the wing was designed so to maintain its integrity, with respect to its stress charges and its strong stress concentrations.</p>
----------------------------------------------------------------------
In diva2:1737092 abstract is:
<p>Nanoparticles (NPs) have been part of our daily lives whether we are aware of it ornot and are continuously shaping our world of tomorrow. They are employed in awide range of applications such as in sunscreens, breaking down microplastics inthe ocean and scaling down beyond Moore’s law. Superparamagnetic iron oxidenanoparticles (SPIONs) are one such NPs which have shown promising resultsin diverse fields, from magnetic resonance imagining (MRI) to drug delivery andmagnetic separation. In this work, SPIONs were synthesised following the conceptof green chemistry and self-assembled into nanoclusters (NCs) with a one-potsynthesis via microwave-assisted (MW) or conventional solvothermal (ST) route.The NC surface was engineered with several capping agents granting monodispersityand a flexible platform for bioconjugation. Furthermore, as-synthesised NCswere subsequently coated with a 55 nm thin silica shell leading to a core-shellarchitecture, which would prevent the probable iron dissolution in the biologicalenvironment as well as enhance their biocompatibility. After optimizing the NCsize and shape, a detailed characterization of their properties, behaviour, and potentialapplications was performed.</p>

corrected abstract:
<p>Nanoparticles (NPs) have been part of our daily lives whether we are aware of it or not and are continuously shaping our world of tomorrow. They are employed in a wide range of applications such as in sunscreens, breaking down microplastics in the ocean and scaling down beyond Moore’s law. Superparamagnetic iron oxide nanoparticles (SPIONs) are one such NPs which have shown promising results in diverse fields, from magnetic resonance imagining (MRI) to drug delivery and magnetic separation. In this work, SPIONs were synthesised following the concept of green chemistry and self-assembled into nanoclusters (NCs) with a one-pot synthesis <em>via</em> microwave-assisted (MW) or conventional solvothermal (ST) route. The NC surface was engineered with several capping agents granting monodispersity and a flexible platform for bioconjugation. Furthermore, as-synthesised NCs were subsequently coated with a 55 nm thin silica shell leading to a core-shell architecture, which would prevent the probable iron dissolution in the biological environment as well as enhance their biocompatibility. After optimizing the NC size and shape, a detailed characterization of their properties, behaviour, and potential applications was performed.</p>
----------------------------------------------------------------------
In diva2:1698025 abstract is:
<p>In a nuclear reactor, in case of a severe accident occurs, backup circuits are usedlike SIS and CSS, flooding the reactor with water. This might lead to many liquidleakages outside the reactor containment building. These leakages contain a lot offission products and especially iodine that is dangerous in terms of radiotoxicity oncereleased into the environment. Therefore, it is necessary to model correctly the liquidleakage in a reactor to size their impact properly. Their re-injection with a newpump implemented as part of the safety review for the extension lifetime of reactorsbeyond 40 years constitutes also a safety improvement to reduce the source term inthe environment.Modelling the transfer of fission products through liquid leakage and the re-injectionof said leakages is performed at IRSN with the ASTEC code. This thesis focuses onthe optimization of this model, and the consequences of modification in the code forfission products. Indeed, the model of pump used for leakages and re-injection hadsome issues that had to be circumvented.Also, the efficiency of re-injection will be tested with the new model, knowing thatthis implementation has already been proven to be effective in reducing releases tothe environment. Besides, this thesis studies the failure of the reinjection of liquidleaks that can lead to an accumulation of fission products in the auxiliary safeguardingbuilding. Other pH sensitivities in different areas are also studied.</p>

corrected abstract:
<p>In a nuclear reactor, in case of a severe accident occurs, backup circuits are used like SIS and CSS, flooding the reactor with water. This might lead to many liquid leakages outside the reactor containment building. These leakages contain a lot of fission products and especially iodine that is dangerous in terms of radiotoxicity once released into the environment. Therefore, it is necessary to model correctly the liquid leakage in a reactor to size their impact properly. Their re-injection with a new pump implemented as part of the safety review for the extension lifetime of reactors beyond 40 years constitutes also a safety improvement to reduce the source term in the environment.</p><p>Modelling the transfer of fission products through liquid leakage and the re-injection of said leakages is performed at IRSN with the ASTEC code. This thesis focuses on the optimization of this model, and the consequences of modification in the code for fission products. Indeed, the model of pump used for leakages and re-injection had some issues that had to be circumvented.</p><p>Also, the efficiency of re-injection will be tested with the new model, knowing that this implementation has already been proven to be effective in reducing releases to the environment. Besides, this thesis studies the failure of the reinjection of liquid leaks that can lead to an accumulation of fission products in the auxiliary safeguarding building. Other pH sensitivities in different areas are also studied.</p>
----------------------------------------------------------------------
In diva2:1640018 abstract is:
<p>The lattice Boltzmann method (LBM) is widely studied andused in the last years to replace the conventional numerical solvers forthe Navier-Stokes equations. In this work, a general introduction tofluid dynamics equations and the changes when the flow is a reactivemulti-species one will be given first. The lattice Boltzmann method andalgorithm will then be explained in details with the kinetic theorybehind, tested and validated for canonical test cases of doubly shearlayer flow in reactive and non-reactive flows. Finally the method willbe applied to simulate a non-reacting propane jet and the results willbe validated using experimental data.</p><p>The objectives of this thesis are mainly: first a better understandingof the LBM, the combustion reactions, the jets and how they work, secondthe use of this method to produce a simple code that works for a basictest case, third validate this code with more developed methods, andfinally apply this method to simulate a more complex configuration whichis the non-reacting propane jet flame into co-flowing air</p>

corrected abstract:
<p>The lattice Boltzmann method (LBM) is widely studied and used in the last years to replace the conventional numerical solvers for the Navier-Stokes equations. In this work, a general introduction to fluid dynamics equations and the changes when the flow is a reactive multi-species one will be given first. The lattice Boltzmann method and algorithm will then be explained in details with the kinetic theory behind, tested and validated for canonical test cases of doubly shear layer flow in reactive and non-reactive flows. Finally the method will be applied to simulate a non-reacting propane jet and the results will be validated using experimental data.</p><p>The objectives of this thesis are mainly: first a better understanding of the LBM, the combustion reactions, the jets and how they work, second the use of this method to produce a simple code that works for a basic test case, third validate this code with more developed methods, and finally apply this method to simulate a more complex configuration which is the non-reacting propane jet flame into co-flowing air.</p>
----------------------------------------------------------------------
In diva2:1528107 abstract is:
<p>Carbon Fibre Reinforced Plastic (CFRP) is a material with high specific propertiesand good fatigue and vibration dampening characteristics, and can potentiallybe used instead of steel and aluminium in heavy duty vehicles. This work focuseson testing methodology and the fatigue properties of a unidirectional (UD)material in the 0° and 90°orientation, reproducing and validating the method developedby Wanner[1]. While conducting a fatigue test of a CFRP composite intension-tension fatigue, in-situ strain measurements were performed to examinethe gradual elongation of the specimen (as this relates to stiffness loss, i.e. fatiguedamage). An imaging methodology capturing the specimen at peak loading hasbeen developed, including a trigger mechanism that activates the camera at the desiredtime and cycle count, as well as a method of extracting the photograph ofthe specimen at maximum displacement, allowing for peak-to-peak comparison.A method improving specimen production output and consistency has been developed.SN-curves have been produced for both 0° and 90° fibre orientations.Micrography of sectioned specimen has been conducted. The study finds the fatiguelimit of the 0° specimen to be as high as 80% of the material tensile failurestrength, while results from the 90° study indicate a lower but inconclusivevalue. An attempt at qualitatively determining the factors causing the material behaviourhas been made and is deliberated upon.</p>

corrected abstract:
<p>Carbon Fibre Reinforced Plastic (CFRP) is a material with high specific properties and good fatigue and vibration dampening characteristics, and can potentially be used instead of steel and aluminium in heavy duty vehicles. This work focuses on testing methodology and the fatigue properties of a unidirectional (UD) material in the 0° and 90° orientation, reproducing and validating the method developed by Wanner[1]. While conducting a fatigue test of a CFRP composite in tension-tension fatigue, in-situ strain measurements were performed to examine the gradual elongation of the specimen (as this relates to stiffness loss, i.e. fatigue damage). An imaging methodology capturing the specimen at peak loading has been developed, including a trigger mechanism that activates the camera at the desired time and cycle count, as well as a method of extracting the photograph of the specimen at maximum displacement, allowing for peak-to-peak comparison. A method improving specimen production output and consistency has been developed. SN-curves have been produced for both 0° and 90° fibre orientations. Micrography of sectioned specimen has been conducted. The study finds the fatigue limit of the 0° specimen to be as high as 80% of the material tensile failure strength, while results from the 90° study indicate a lower but inconclusive value. An attempt at qualitatively determining the factors causing the material behaviour has been made and is deliberated upon.</p>
----------------------------------------------------------------------
In diva2:1357359 abstract is:
<p>During the last decade, a novel methodology for wheel wear simulation has been developed in Sweden. The practical objective of this simulation procedure is to provide an integratedengineering tool to support rail vehicle design with respect to wheel wear performance and detailed understanding of wheel-rail interaction. The tool is integrated in a vehicle dynamicssimulation environment.The wear calculation is based on a set of dynamic simulations, representing the vehicle, the network, and the operating conditions. The wheel profile evolution is simulated in an iterativeprocess by adding the contribution from each simulation case and updating the profile geometry.The method is being validated against measurements by selected pilot applications. To strengthen the confidence in simulation results the scope of application should be as wide aspossible in terms of vehicle classes. The purpose of this thesis work has been to try to extend the scope of validation of this method into the light rail area, simulating the light rail vehicleA32 operating in Stockholm commuter service on the line Tvärbanan.An exhaustive study of the wear theory and previous work on wear prediction has been necessary to understand the wear prediction method proposed by KTH. The dynamicbehaviour of rail vehicles has also been deeply studied in order to understand the factors affecting wear in the wheel-rail contact.The vehicle model has been validated against previous studies of this vehicle. Furthermore new elements have been included in the model in order to better simulate the real conditionsof the vehicle.Numerous tests have been carried out in order to calibrate the wear tool and find the settings which better match the real conditions of the vehicle.Wheel and rail wear as well as profile evolution measurements were available before this work and they are compared with those results obtained from the simulations carried out.The simulated wear at the tread and flange parts of the wheel match quite well the measurements. However, the results are not so good for the middle part, since themeasurements show quite evenly distributed wear along the profile while the results from simulations show higher difference between extremes and middle part. More tests would benecessary to obtain an optimal solution.</p>

corrected abstract:
<p>During the last decade, a novel methodology for wheel wear simulation has been developed in Sweden. The practical objective of this simulation procedure is to provide an integrated engineering tool to support rail vehicle design with respect to wheel wear performance and detailed understanding of wheel-rail interaction. The tool is integrated in a vehicle dynamics simulation environment.</p><p>The wear calculation is based on a set of dynamic simulations, representing the vehicle, the network, and the operating conditions. The wheel profile evolution is simulated in an iterative process by adding the contribution from each simulation case and updating the profile geometry.</p><p>The method is being validated against measurements by selected pilot applications. To strengthen the confidence in simulation results the scope of application should be as wide as possible in terms of vehicle classes. The purpose of this thesis work has been to try to extend the scope of validation of this method into the light rail area, simulating the light rail vehicle A32 operating in Stockholm commuter service on the line Tvärbanan. An exhaustive study of the wear theory and previous work on wear prediction has been necessary to understand the wear prediction method proposed by KTH. The dynamic behaviour of rail vehicles has also been deeply studied in order to understand the factors affecting wear in the wheel-rail contact.</p><p>The vehicle model has been validated against previous studies of this vehicle. Furthermore new elements have been included in the model in order to better simulate the real conditions of the vehicle.</p><p>Numerous tests have been carried out in order to calibrate the wear tool and find the settings which better match the real conditions of the vehicle.</p><p>Wheel and rail wear as well as profile evolution measurements were available before this work and they are compared with those results obtained from the simulations carried out.</p><p>The simulated wear at the tread and flange parts of the wheel match quite well the measurements. However, the results are not so good for the middle part, since the measurements show quite evenly distributed wear along the profile while the results from simulations show higher difference between extremes and middle part. More tests would be necessary to obtain an optimal solution.</p>
----------------------------------------------------------------------
In diva2:1285502 abstract is:
<p>Vehicle pull is an issue that occurs when the driver has to exert a discerniblesteering torque (pull) for the vehicle to run straight, otherwisea lateral drift takes place. This thesis deals with the straight motion ofroad vehicles, with particular focus on the role played by tire characteristics,road cross slope and interactions between tires and vehicle.A thorough theoretical approach has been adopted, adjusting thePacejka’s formulation for effective axle characteristics and extendingthe linear handling diagram theory. This has allowed to obtain innovativeanalytical expressions, describing the straight-driving slip anglesand steering torque offsets.The analytical expressions have been validated, together with asingle-track model, by means of quasi-static and dynamic simulationsof a full-vehicle model. Moreover, a relationship between tire characteristicsand on-center handling has been described, that relates objectivemetrics with subjective feedback.The obtained analytical expressions can be used by vehicle OriginalEquipment Manufacturers (OEMs) or Tire Suppliers for productdevelopment.</p>

corrected abstract:
<p>Vehicle pull is an issue that occurs when the driver has to exert a discernible steering torque (pull) for the vehicle to run straight, otherwise a lateral drift takes place. This thesis deals with the straight motion of road vehicles, with particular focus on the role played by tire characteristics, road cross slope and interactions between tires and vehicle.</p><p>A thorough theoretical approach has been adopted, adjusting the Pacejka’s formulation for effective axle characteristics and extending the linear handling diagram theory. This has allowed to obtain innovative analytical expressions, describing the straight-driving slip angles and steering torque offsets.</p><p>The analytical expressions have been validated, together with a single-track model, by means of quasi-static and dynamic simulations of a full-vehicle model. Moreover, a relationship between tire characteristics and on-center handling has been described, that relates objective metrics with subjective feedback.</p><p>The obtained analytical expressions can be used by vehicle Original Equipment Manufacturers (OEMs) or Tire Suppliers for product development.</p>
----------------------------------------------------------------------
In diva2:1188292 - Note: no full text in DiVA
abstract is:
<p>Flexible hoses are used in the exhaust systems of trucks to decouple the vibrations betweenthe engine and chassis mounted components. These stainless steel parts have tosustain high thermal loads from the exhaust gas and vibrations from the engine and road.The combination of complex geometry and thermo-mechanical loading makes the fatiguelife prediction difficult in flexible hoses. This thesis focuses on developing a method for assessing the durability in these components. The method includes how to measure theload history in six degrees of freedom using the hexapod method, how to model the heattransfer and assess the thermal effects on the fatigue life using a parametric study, andhow to calculate the fatigue life from the measured load history by using the strain-basedlife approach. In addition, equivalent loads were defined for all degrees of freedom fromwhich the fatigue life was calculated and compared to the previous results. This approach saves the computation time considerably, while its accuracy is under questions.The exhaust gas temperature was observed to have the largest effect on the fatiguelife of the flexible hose. In complex stripwound hoses, plastic deformation occurs dueto a high thermal gradient through the wall thickness. This plastic deformation can contributeto stiffening of stripwound hoses. It was not possible to include the thermal stress in the calculations of the fatigue life from the load histories due to the linearity assumption,while they could be included when the equivalent loads were used. The calculated damage from the load histories was not comparable with the damage calculated fromthe equivalent loads. This is caused mainly by neglecting the effects of multiaxial loading in the equivalent loads. Further physical tests are required to validate and improvethe presented method herein.</p>

corrected abstract:
<p>Flexible hoses are used in the exhaust systems of trucks to decouple the vibrations between the engine and chassis mounted components. These stainless steel parts have to sustain high thermal loads from the exhaust gas and vibrations from the engine and road. The combination of complex geometry and thermo-mechanical loading makes the fatigue life prediction difficult in flexible hoses. This thesis focuses on developing a method for assessing the durability in these components. The method includes how to measure the load history in six degrees of freedom using the hexapod method, how to model the heattransfer and assess the thermal effects on the fatigue life using a parametric study, and how to calculate the fatigue life from the measured load history by using the strain-based life approach. In addition, equivalent loads were defined for all degrees of freedom from which the fatigue life was calculated and compared to the previous results. This approach saves the computation time considerably, while its accuracy is under questions. The exhaust gas temperature was observed to have the largest effect on the fatigue life of the flexible hose. In complex strip wound hoses, plastic deformation occurs due to a high thermal gradient through the wall thickness. This plastic deformation can contribute to stiffening of strip wound hoses. It was not possible to include the thermal stress in the calculations of the fatigue life from the load histories due to the linearity assumption, while they could be included when the equivalent loads were used. The calculated damage from the load histories was not comparable with the damage calculated from the equivalent loads. This is caused mainly by neglecting the effects of multiaxial loading in the equivalent loads. Further physical tests are required to validate and improve the presented method herein.</p>
----------------------------------------------------------------------
In diva2:1142942 - is 'diva2:1120520' a duplicate?

abstract is:
<p>SDR (Software Defined Radio) is a radio communicationsystem that has been of great interest and developmentover the last 20 years. It decreases communication costs significantlyas it replaces expensive analogue system components withcheap and flexible digital ones. In this article we describe anSDR implementation for communication with the SEAM (SmallExplorer for Advances Missions) satellite, a CubeSat satellitethat will perform high quality magnetic measurements in theEarth orbit. The project result consists of carefully chosen SDRtransceiver and software tools, integrated with parts of thecurrent satellite communication system. The implementation hasrequired studies within the field of digital processing, SDR andspecifications of the SEAM satellite communication system, andhas been developed and tested using a radio identical to an onboard satellite radio and coaxial cables as transmission media.The system is able to interpret incoming messages from theradio as hexadecimal data and will serve as a prototype whenimplementing SDR for another, more complex and expensivesystem that is used for communication with the SEAM satellite.</p>

corrected abstract:
<p>SDR (Software Defined Radio) is a radio communication system that has been of great interest and development over the last 20 years. It decreases communication costs significantly as it replaces expensive analogue system components with cheap and flexible digital ones. In this article we describe an SDR implementation for communication with the SEAM (Small Explorer for Advances Missions) satellite, a CubeSat satellite that will perform high quality magnetic measurements in the Earth orbit. The project result consists of carefully chosen SDR transceiver and software tools, integrated with parts of the current satellite communication system. The implementation has required studies within the field of digital processing, SDR and specifications of the SEAM satellite communication system, and has been developed and tested using a radio identical to an on board satellite radio and coaxial cables as transmission media. The system is able to interpret incoming messages from the radio as hexadecimal data and will serve as a prototype when implementing SDR for another, more complex and expensive system that is used for communication with the SEAM satellite.</p>
----------------------------------------------------------------------
In diva2:1142795 - is 'diva2:1120491' (with only one author) a duplicate?
abstract is:
<p>Electrohydrodynamic (EHD) thrusters hold promiseto provide more efficient thrust than propeller driven systems forsmall drones. However, the fact is that there is incomplete analysisin the area and that no work has yet studied the capacities ofbattery driven EHD thrusters. This study shows how a batterysystem from a commercial Arc-Lighter can produce a suitablevoltage for an EHD thruster, with 7.1 kV. Using the batterysystem to drive a single EHD thruster, the EHD thruster couldonly lift 13.3 ppm of the total weight of a battery system andEHD thruster. However, the low value is due to a low-qualitythruster. If the battery system would be used with suitable highqualityEHD thrusters from literature, approximately 5% of thetotal weight could be reached when at least 20% is needed forairplane drones to lift. In conclusion, the battery system is notsufficient for lifting the total weight. Still, in future, with morework on both EHD thruster and a battery system providing ahigher voltage, there might be possible to have small airplanedrones with battery driven EHD thrusters.</p>

corrected abstract:
<p>Electrohydrodynamic (EHD) thrusters hold promise to provide more efficient thrust than propeller driven systems for small drones. However, the fact is that there is incomplete analysis in the area and that no work has yet studied the capacities of battery driven EHD thrusters. This study shows how a battery system from a commercial Arc-Lighter can produce a suitable voltage for an EHD thruster, with 7.1 kV. Using the battery system to drive a single EHD thruster, the EHD thruster could only lift 13.3 ppm of the total weight of a battery system and EHD thruster. However, the low value is due to a low-quality thruster. If the battery system would be used with suitable high-quality EHD thrusters from literature, approximately 5% of the total weight could be reached when at least 20% is needed for airplane drones to lift. In conclusion, the battery system is not sufficient for lifting the total weight. Still, in future, with more work on both EHD thruster and a battery system providing a higher voltage, there might be possible to have small airplane drones with battery driven EHD thrusters.</p>
----------------------------------------------------------------------
In diva2:1127576 abstract is:
<p>This degree project is focused on the study of the tumble and the swirlmotions, which develop during the intake stroke, inside a cylinder of a Dieselengine.Nowadays, the reduction of fuel consumption and emissions is a primaryaspect for automotive companies, including Scania. Then, an efficient combustionprocess is required, and a fundamental role is played by tumble andswirl motion.These motions have been studied by means of the Particle Image Velocimetry(PIV) technique. In particular, two different cylinder head designshave been investigated, focusing on the structures present in the flow andtheir evolutions inside the cylinder. Finally, these results have been comparedwith LES results, in order to validate the latest. Analysing the swirl motion, it has been possible to identify three mainregions, along the cylinder, characterized by different vortex structures. Inaddition, the velocity field into the entire cylinder volume has been extractedby means of a three-dimensional three-component reconstruction.</p>

corrected abstract:
<p>This degree project is focused on the study of the tumble and the swirl motions, which develop during the intake stroke, inside a cylinder of a Diesel engine.</p><p>Nowadays, the reduction of fuel consumption and emissions is a primary aspect for automotive companies, including Scania. Then, an efficient combustion process is required, and a fundamental role is played by tumble and swirl motion.</p><p>These motions have been studied by means of the Particle Image Velocimetry (PIV) technique. In particular, two different cylinder head designs have been investigated, focusing on the structures present in the flow and their evolutions inside the cylinder. Finally, these results have been compared with LES results, in order to validate the latest.</p><p>Analysing the swirl motion, it has been possible to identify three main regions, along the cylinder, characterized by different vortex structures. In addition, the velocity field into the entire cylinder volume has been extracted by means of a three-dimensional three-component reconstruction.</p>
----------------------------------------------------------------------
In diva2:1078061 - missing space in title:
"PIV measurments and analysis of steady in-cylinderflow in tumble plane"
==>
"PIV measurments and analysis of steady in-cylinder flow in tumble plane"

abstract is:
<p>In this thesis the flow that is generated after passage through channels and inlet valveswithin a diesel engine cylinder is examined with the means of Particle Image Velocimetry(PIV). Experiments are carried out by le􀄴ing a thin laser sheet enter into the cylinder frombelow, and with short time intervals take pictures of smoke particles introduced into theair flowing through a given geometric channel shape, via inlet valves and down throughthe cylinder. Three valve lifts were tested and for all valve lifts six sample planes wereused, three parallel to the inlet valves and three orthogonal. Each case were measuredby taking 1500 PIV image pairs of the flow in the cylinder.After pre processing the PIV images they are evaluated by a cross-correlation algorithmwhich evaluates the displacements of the particles within a predetermined interrogationarea in order to find the velocity vector field. For each image pair an instantaneousvelocity field is obtained, and these are then used to calculate the mean velocity fieldof the stationary flow. The measured results are analyzed to find flow structures withinthe flow as well as turbulence properties. An identification scheme to detect tumble ismade in order to find large scale tumble motions as well as a Proper Orthogonal Decomposition(POD) analysis of the center plane for the three valve lifts. The velocity fieldsare found to be sensitive to the valve lift, which is most clearly seen in the difference ofswirl whereas no significant difference can be seen for the tumble motion.</p>

corrected abstract:
<p>In this thesis the flow that is generated after passage through channels and inlet valves within a diesel engine cylinder is examined with the means of Particle Image Velocimetry (PIV). Experiments are carried out by letting a thin laser sheet enter into the cylinder from below, and with short time intervals take pictures of smoke particles introduced into the air flowing through a given geometric channel shape, via inlet valves and down through the cylinder. Three valve lifts were tested and for all valve lifts six sample planes were used, three parallel to the inlet valves and three orthogonal. Each case were measured by taking 1500 PIV image pairs of the flow in the cylinder.</p><p>After pre processing the PIV images they are evaluated by a cross-correlation algorithm which evaluates the displacements of the particles within a predetermined interrogation area in order to find the velocity vector field. For each image pair an instantaneous velocity field is obtained, and these are then used to calculate the mean velocity field of the stationary flow. The measured results are analyzed to find flow structures within the flow as well as turbulence properties. An identification scheme to detect tumble is made in order to find large scale tumble motions as well as a Proper Orthogonal Decomposition (POD) analysis of the center plane for the three valve lifts. The velocity fields are found to be sensitive to the valve lift, which is most clearly seen in the difference of swirl whereas no significant difference can be seen for the tumble motion.</p>
----------------------------------------------------------------------
In diva2:726182 abstract is:
<p>Thispaper presents a mathematical optimization model concerning Prefab supplier selectionfor Spinactor AB with respect to the purchase price, volume discounts and transportationcost. Furthermore, it includes a qualitative study regarding supplier selectioncriteria. We identified the most important supplier criteria for Spinactor and developeda supplier selection model, based on the framework “total cost of ownership”(TCO), where those criteria were quantified. It was concluded that the threemost important supplier criteria for Spinactor AB, in the current situation, isdelivery performance, customer service and communication, and economicstability. The solution of the optimization problem was implemented using twodifferent methods, linear programming and integer programming, where linearprogramming was dominant in computational time. The optimization results showedthat the transportation cost is an important element in the total cost since anoptimal allocation suggests that five out of six potential suppliers should becontracted. Thus, our recommendation to Spinactor is to contract several localsuppliers for the initial purchasing of Prefab.</p>

corrected abstract:
<p>This paper presents a mathematical optimization model concerning Prefab supplier selection for Spinactor AB with respect to the purchase price, volume discounts and transportation cost. Furthermore, it includes a qualitative study regarding supplier selection criteria. We identified the most important supplier criteria for Spinactor and developed a supplier selection model, based on the framework “total cost of ownership” (TCO), where those criteria were quantified. It was concluded that the three most important supplier criteria for Spinactor AB, in the current situation, is delivery performance, customer service and communication, and economic stability.</p><p>The solution of the optimization problem was implemented using two different methods, linear programming and integer programming, where linear programming was dominant in computational time. The optimization results showed that the transportation cost is an important element in the total cost since an optimal allocation suggests that five out of six potential suppliers should be contracted. Thus, our recommendation to Spinactor is to contract several local suppliers for the initial purchasing of Prefab.</p>
----------------------------------------------------------------------
In diva2:618223 - Note: no full text in DiVA
abstract is:
<p>The aerospace industry becomes more and more accustomed with automated processing.Current manufacturing techniques are under development for higher automation. Within thiswork the behavior of prepreg material during forming is investigated, conducting simulationson previously performed experiments at SAAB AB. The aim is to implement materialbehavior in the AniForm Virtual Forming Tool simulation. The results of various experimentshave been combined to generate the basic material properties to assemble the materialbehavior and the experiment. With the use of investigated simplifications, the possibilities ofthe AniForm Virtual Forming Tool are used to compare two experiments conducted. Byrelating the simulations of wrinkled and non-wrinkled trials, the analysis of stresses andstrains within the simulation output is conducted. With the outcomes of these simulations,previously taken assumptions on fiber direction behavior are investigated. With the resultsshowing no geometrical wrinkling of the prepreg material itself, indicators for wrinkling areexamined on the output of the results. It can be seen that looking at various parametersexclusively does not necessarily explain the wrinkling of the prepreg material. Multiplefactors need to be taken into account at the same time. The results indicate that alignment ofstress- and strain behavior next to the fiber angle deviations play important parts in futureinvestigations regarding wrinkle development.</p>

corrected abstract:
<p>The aerospace industry becomes more and more accustomed with automated processing. Current manufacturing techniques are under development for higher automation. Within this work the behavior of prepreg material during forming is investigated, conducting simulations on previously performed experiments at SAAB AB. The aim is to implement material behavior in the AniForm Virtual Forming Tool simulation. The results of various experiments have been combined to generate the basic material properties to assemble the material behavior and the experiment. With the use of investigated simplifications, the possibilities of the AniForm Virtual Forming Tool are used to compare two experiments conducted. By relating the simulations of wrinkled and non-wrinkled trials, the analysis of stresses and strains within the simulation output is conducted. With the outcomes of these simulations, previously taken assumptions on fiber direction behavior are investigated. With the results showing no geometrical wrinkling of the prepreg material itself, indicators for wrinkling are examined on the output of the results. It can be seen that looking at various parameters exclusively does not necessarily explain the wrinkling of the prepreg material. Multiplefactors need to be taken into account at the same time. The results indicate that alignment of stress- and strain behavior next to the fiber angle deviations play important parts in future investigations regarding wrinkle development.</p>
----------------------------------------------------------------------
In diva2:558921 abstract is:
<p>During an emergency in a building complex, an effective evacuation is essentialto avoid crowd disasters. The evacuation efficiency could be enhanced both bychanging the layout of the building, and by changing the route guiding given tothe evacuating pedestrians. This thesis considers how to guide the evacuatingpedestrians so that the evacuation time is minimised.In this thesis, a dynamic network model, namely the point queue model, isused to form a linear programming problem whose solution is used to create anevacuation plan. By continuously updating the initial data in this model andsolving the problem with this new data, a feedback based control law is derivedbased on Model Predictive Control.The control law is tested on a simulation of the social force model for abuilding with five rooms and one respectively two exits. The result shows thatthe control law manages to efficiently guide the pedestrians out of the building,taking the varying distribution of pedestrians into account. The control lawfurther manages to handle minor errors in the layout information.                       Keywords. Evacuation modelling, pedestrian dynamics, optimal control</p>

corrected abstract:
<p>During an emergency in a building complex, an effective evacuation is essential to avoid crowd disasters. The evacuation efficiency could be enhanced both by changing the layout of the building, and by changing the route guiding given to the evacuating pedestrians. This thesis considers how to guide the evacuating pedestrians so that the evacuation time is minimised.</p><p>In this thesis, a dynamic network model, namely the point queue model, is used to form a linear programming problem whose solution is used to create an evacuation plan. By continuously updating the initial data in this model and solving the problem with this new data, a feedback based control law is derived based on Model Predictive Control.</p><p>The control law is tested on a simulation of the social force model for a building with five rooms and one respectively two exits. The result shows that the control law manages to efficiently guide the pedestrians out of the building, taking the varying distribution of pedestrians into account. The control law further manages to handle minor errors in the layout information.</p>
----------------------------------------------------------------------
In diva2:515496 abstract is:
<p>Lean Product Development is a knowledge-based business concept in order to maintain high quality, meetcustomer requirements and to make product development more efficient. An important part of the processis to add a lot of resources at an early stage and execute the development as an iterative process betweendepartments exploring many alternatives thoroughly. The work focuses on how Lean ProductDevelopment is carried out and explores how to manage interaction between different departments andexpertise with regard to cross-functional collaboration and knowledge sharing i.e. learning.The information for the studies was gathered at a major Swedish company from two projects. The projectswere carried out as cross-functional and possible key factors for cross-functional collaboration wasidentified.The results have been correlated with theories of Lean, Product development, Lean product developmentand Learning. The analysis shows that to carry out activities according to Lean product development willrequire more than to follow the concept’s framework for successful implementation. Factors that areidentified as important in cross-functional collaboration is also recognised in the Lean productdevelopment theory. However the theory does not indicate how the factors is implemented and carried outbut how it should be implemented. This will create opportunities and problems for companies that want towork and implement Lean product development.</p>

corrected abstract:
<p>Lean Product Development is a knowledge-based business concept in order to maintain high quality, meet customer requirements and to make product development more efficient. An important part of the process is to add a lot of resources at an early stage and execute the development as an iterative process between departments exploring many alternatives thoroughly. The work focuses on how Lean Product Development is carried out and explores how to manage interaction between different departments and expertise with regard to cross-functional collaboration and knowledge sharing i.e. learning.</p><p>The information for the studies was gathered at a major Swedish company from two projects. The projects were carried out as cross-functional and possible key factors for cross-functional collaboration was identified.</p><p>The results have been correlated with theories of Lean, Product development, Lean product development and Learning. The analysis shows that to carry out activities according to Lean product development will require more than to follow the concept’s framework for successful implementation. Factors that are identified as important in cross-functional collaboration is also recognised in the Lean product development theory. However the theory does not indicate how the factors is implemented and carried out but how it should be implemented. This will create opportunities and problems for companies that want to work and implement Lean product development.</p>
----------------------------------------------------------------------
In diva2:857656 abstract is:
<p>The collapse-behaviour of pipes was to be studied by use of Finite Element modelling.Existing analytical expressions for collapse were evaluated and especially the one used inDNV-OS-F101 was decided to be studied in comparison with FE-model results.Parameters that may influence the collapse capacity and are not included in the analyticalexpressions –flattening, peaking, eccentricity, local wall thickness variation, materialstress-strain curve, residual stresses - were defined and explained. A model was built inthe Finite Element software package Abaqus v6.9.1 and several articles on collapsetesting used to verify it. The aforementioned parameters were studied by use ofsensitivity studies and the results shown and discussed. Effective thickness definitions foruse in the DNV-formula and the DNV-yield stress criterion were discussed in the contextof the results. The results seemed to indicate that the transition between the elastic andplastic range of the material stress-strain curve was of great importance. The results werediscussed in the context of the different collapse-related parameters defined beforehandand some concluding remarks were made on possible further work related to thesefindings.</p>

corrected abstract:
<p>The collapse-behaviour of pipes was to be studied by use of Finite Element modelling. Existing analytical expressions for collapse were evaluated and especially the one used in DNV-OS-F101 was decided to be studied in comparison with FE-model results. Parameters that may influence the collapse capacity and are not included in the analytical expressions –flattening, peaking, eccentricity, local wall thickness variation, material stress-strain curve, residual stresses - were defined and explained. A model was built in the Finite Element software package Abaqus v6.9.1 and several articles on collapse testing used to verify it. The aforementioned parameters were studied by use of sensitivity studies and the results shown and discussed. Effective thickness definitions for use in the DNV-formula and the DNV-yield stress criterion were discussed in the context of the results. The results seemed to indicate that the transition between the elastic and plastic range of the material stress-strain curve was of great importance. The results were discussed in the context of the different collapse-related parameters defined beforehand and some concluding remarks were made on possible further work related to these findings.</p>
----------------------------------------------------------------------
In diva2:515487 abstract is:
<p>The teaching of mathematics has always interested me, especially after studies in engineeringand teaching. This genuine interest, together with the fact that Swedish students achieveworse grades in mathematics than for many years, is the reason why I wanted to immersemyself in this area. I hope, with this project, to make a contribution to the development of abetter education in mathematics.Living in an online world where youths spend lots of time with computer games andcommunication and having knowledge of the importance of involving users into the process ofdeveloping new systems, I found it interesting to integrate these natural driving forces intoeducation. This project has briefly compiled the biggest competitions in mathematics but alsocontemporary research, relevant to my subject, in system development. According to thesestudies, I have also designed a digital competition in mathematics and problem solving. Themain methods have been studies of different literature, communication with informedindividuals and two bigger surveys.My proposal to a competition has only been a pilot and is therefore hard to evaluate deeply.The fact that only two students participated is of course a failure but despite this I foundcomments and results from mainly the surveys very useful when developing biggercompetitions and new material to the teaching of mathematics.</p>

corrected abstract:
<p>The teaching of mathematics has always interested me, especially after studies in engineering and teaching. This genuine interest, together with the fact that Swedish students achieve worse grades in mathematics than for many years, is the reason why I wanted to immerse myself in this area. I hope, with this project, to make a contribution to the development of a better education in mathematics.</p><p>Living in an online world where youths spend lots of time with computer games and communication and having knowledge of the importance of involving users into the process of developing new systems, I found it interesting to integrate these natural driving forces into education. This project has briefly compiled the biggest competitions in mathematics but also contemporary research, relevant to my subject, in system development. According to these studies, I have also designed a digital competition in mathematics and problem solving. The main methods have been studies of different literature, communication with informed individuals and two bigger surveys.</p><p>My proposal to a competition has only been a pilot and is therefore hard to evaluate deeply. The fact that only two students participated is of course a failure but despite this I found comments and results from mainly the surveys very useful when developing bigger competitions and new material to the teaching of mathematics.</p>
----------------------------------------------------------------------
In diva2:1879348 abstract is:
<p>This project tries to solve higher-order nonlinear equations by using neural networks.When trying to solve the standard pairing problem, one is faced with higher-order nonlinear equations, these can be solved already but it takes a lot of computing power.With the help of machine learning, it might ease the computational needs to solve theseequations or to at least aid in finding good initial values to existing solvers.</p><p>The first implementation of neural networks was a model that solves higher-orderstandard polynomials with good results, from this the aim was to continue to developa model that could compute more complex equations. The development of this modelwas not a linear process due to the behavior of the equation, for smaller systems (fewerparticles) the model is often successful in finding the correct values. When trying to solvefor larger systems the model becomes more unstable and is more likely to converge toa local minimum instead of the global minimum. One might try to use more complexmodels when solving larger systems, however this was not done in this study.</p><p>The resultsare mildly promising and a model that can solve complex equations might be a possibility,even for larger systems.</p><p> </p>

corrected abstract:
<p>This project tries to solve higher-order nonlinear equations by using neural networks. When trying to solve the standard pairing problem, one is faced with higher-order nonlinear equations, these can be solved already but it takes a lot of computing power. With the help of machine learning, it might ease the computational needs to solve these equations or to at least aid in finding good initial values to existing solvers.</p><p>The first implementation of neural networks was a model that solves higher-order standard polynomials with good results, from this the aim was to continue to develop a model that could compute more complex equations. The development of this model was not a linear process due to the behavior of the equation, for smaller systems (fewer particles) the model is often successful in finding the correct values. When trying to solve for larger systems the model becomes more unstable and is more likely to converge to a local minimum instead of the global minimum. One might try to use more complex models when solving larger systems, however this was not done in this study. The results are mildly promising and a model that can solve complex equations might be a possibility, even for larger systems.</p>
----------------------------------------------------------------------
In diva2:1862229 abstract is:
<p>This report investigates a controlled reduction process (CRP) of triuraniumoctoxide (U3O8) powder into a hyperstoichiometric uranium dioxide (UO2+x)powder that could be mixed with the fresh uranium dioxide (UO2) powderwithout decreasing its sinterability. Before the CRP could be performed, scrapUO2 pellets were oxidized into U3O8 powder which was then reduced. Fiveversions of the CRP process were performed with different parameters. Thecharacteristics of the resulting CRP powders were investigated through X-raydiffraction (XRD), scanning electron microscope (SEM), and thermogravimetricanalysis (TGA). The powders were mixed with fresh UO2 powder, pressed andsintered to pellets. The density of the pellets was investigated, as was theirsurface through SEM.The results indicate that CRP powders with x below 0.25 had higher finalpellet density, suggesting that there is a benefit of reducing the U3O8 powder tothe fluorite crystal structure before mixing with the fresh UO2 powder. Furtherinvestigation and optimisation of the process is necessary, however its futureimplementation could lead to an increase in the weight fraction of recycledscrap material above the current 9 wt% maximum, among other benefits.</p>

corrected abstract:
<p>This report investigates a controlled reduction process (CRP) of triuranium octoxide (U<sub>3</sub>O<sub>8</sub>) powder into a hyperstoichiometric uranium dioxide (UO<sub>2+x</sub>) powder that could be mixed with the fresh uranium dioxide (UO<sub>2</sub>) powder without decreasing its sinterability. Before the CRP could be performed, scrap UO<sub>2</sub> pellets were oxidized into U<sub>3</sub>O<sub>8</sub> powder which was then reduced. Five versions of the CRP process were performed with different parameters. The characteristics of the resulting CRP powders were investigated through X-ray diffraction (XRD), scanning electron microscope (SEM), and thermogravimetric analysis (TGA). The powders were mixed with fresh UO<sub>2</sub> powder, pressed and sintered to pellets. The density of the pellets was investigated, as was their surface through SEM.</p><p>The results indicate that CRP powders with x below 0.25 had higher final pellet density, suggesting that there is a benefit of reducing the U<sub>3</sub>O<sub>8</sub> powder to the fluorite crystal structure before mixing with the fresh UO<sub>2</sub> powder. Further investigation and optimisation of the process is necessary, however its future implementation could lead to an increase in the weight fraction of recycled scrap material above the current 9 wt% maximum, among other benefits.</p>
----------------------------------------------------------------------
In diva2:1827778 abstract is:
<p>The purpose of this study was to identify factors contributing to graduates from thevocational education provider Nackademin, getting employed in sectors relevantto their field of study. This was done through logistic regression and the factorsexamined were: Field of study, Delivery mode of education, The graduatesrecommendation, Degree of final diploma, Amount of registered students andAmount of course points. The data consisted of surveys collected by Nackademinfrom 2015 throuh 2021. The final model was obtained through an exhaustivevariable selection on all possible models and then based on AIC. The finalmodel included the predictors: Design, Health, Education, Recommendationand Highest degree of final diploma. The model was then evaluated throughresidual analysis and a ROC curve. The ROC prediction test yielded an AUC of0.71 suggesting the final model has an acceptable predictive ability.</p>

corrected abstract:
<p>The purpose of this study was to identify factors contributing to graduates from the vocational education provider Nackademin, getting employed in sectors relevant to their field of study. This was done through logistic regression and the factors examined were: Field of study, Delivery mode of education, The graduates recommendation, Degree of final diploma, Amount of registered students and Amount of course points. The data consisted of surveys collected by Nackademin from 2015 through 2021. The final model was obtained through an exhaustive variable selection on all possible models and then based on AIC. The final model included the predictors: Design, Health, Education, Recommendation and Highest degree of final diploma. The model was then evaluated through residual analysis and a ROC curve. The ROC prediction test yielded an AUC of 0.71 suggesting the final model has an acceptable predictive ability.</p>
----------------------------------------------------------------------
In diva2:1817787 abstract is:
<p>Positron Emission Tomography (PET) is pivotal in medical imaging but is prone to artifactsfrom physiological movements, notably respiration. These motion artifacts both degradeimage quality and compromise precise attenuation correction. To counteract this, gatingstrategies partition PET data in synchronization with respiratory cycles, ensuring each gatenearly represents a static phase. Additionally, a 3D deep learning image registration modelcan be used for inter-gate motion correction, maximizing the use of the full acquired data. Thisstudy aimed to implement and evaluate two gating strategies: an external device-based approachand a data-driven centroid-of-distribution (COD) trace algorithm, and assess their impact on theperformance of the registration model. Analysis of clinical data from four subjects indicated thatthe external device approach outperformed its data-driven counterpart, which faced challengesin real-patient settings. Post motion compensation, both methods achieved results comparableto state-of-the-art reconstructions, suggesting the deep learning model addressed some data-driven method limitations. However, the motion corrected outputs did not exhibit significantimprovements in image quality over state-of-the-art standards.</p>

corrected abstract:
<p>Positron Emission Tomography (PET) is pivotal in medical imaging but is prone to artifacts from physiological movements, notably respiration. These motion artifacts both degrade image quality and compromise precise attenuation correction. To counteract this, gating strategies partition PET data in synchronization with respiratory cycles, ensuring each gate nearly represents a static phase. Additionally, a 3D deep learning image registration model can be used for inter-gate motion correction, maximizing the use of the full acquired data. This study aimed to implement and evaluate two gating strategies: an external device-based approach and a data-driven centroid-of-distribution (COD) trace algorithm, and assess their impact on the performance of the registration model. Analysis of clinical data from four subjects indicated that the external device approach outperformed its data-driven counterpart, which faced challenges in real-patient settings. Post motion compensation, both methods achieved results comparable to state-of-the-art reconstructions, suggesting the deep learning model addressed some data-driven method limitations. However, the motion corrected outputs did not exhibit significant improvements in image quality over state-of-the-art standards.</p>
----------------------------------------------------------------------
In diva2:1816881 abstract is:
<p>Gas turbines can experience various changes that affect their performance.Compressor fouling is one of the leading causes that deteriorate the gas turbineperformance. This research aims to investigate the impact of compressorfouling on the performance of gas turbines and the rotodynamic behaviorof gas turbines. Fouling was simulated as a reduction of mass flow and areduction of compressor isentropic efficiency by using Turbomatch software.A rotor–bearing model was created to analyze the vibration behavior dueto compressor fouling by using MADYN 2000 software and that particledeposition leads to rotor imbalance. The results show that the main variationsfor performance are power output, pressure ratio and EGT. For the rotodynamicmodel, the result illustrates an increase in vibration level for the first andsecond bearings and a decrease for the third bearing. The results also predictedthat parameters mass flow, compressor discharge temperature or specific fuelconsumption show a similar trend compared to the increase in vibrations. Thisresult can be used in conjunction with GPA analysis to predict the foulingcondition and help in identifying the severity of the fouling condition.</p>

corrected abstract:
<p>Gas turbines can experience various changes that affect their performance. Compressor fouling is one of the leading causes that deteriorate the gas turbine performance. This research aims to investigate the impact of compressor fouling on the performance of gas turbines and the rotodynamic behavior of gas turbines. Fouling was simulated as a reduction of mass flow and a reduction of compressor isentropic efficiency by using Turbomatch software. A rotor–bearing model was created to analyze the vibration behavior due to compressor fouling by using MADYN 2000 software and that particle deposition leads to rotor imbalance. The results show that the main variations for performance are power output, pressure ratio and EGT. For the rotodynamic model, the result illustrates an increase in vibration level for the first and second bearings and a decrease for the third bearing. The results also predicted that parameters mass flow, compressor discharge temperature or specific fuelconsumption show a similar trend compared to the increase in vibrations. This result can be used in conjunction with GPA analysis to predict the fouling condition and help in identifying the severity of the fouling condition.</p>
----------------------------------------------------------------------
In diva2:1800206 abstract is:
<p>The aim of this master thesis was to investigate the mechanical properties of ExpandedPolystyrene (EPS) through experimental testing. The strain was measured on thespecimen’s surface with Digital Image Correlation (DIC), in order to investigate howthe first principal strain at fracture initiation depends on the loading of the material.To impose different loading conditions various tests were performed, such as biaxialtension-compression, uniaxial tension and bending tests. The study was done withfour densities of EPS, 35, 50, 75 and 100 g/L. To investigate how strain and stressat fracture initiation depend on temperature, the bending tests were performed atthree different temperatures, -20, 20 and 50 °C. All tests were performed in the SolidMechanics lab at KTH, Stockholm. For every experiment, five repetitions were madeto reduce the influence of the inhomogeneous microstructure of EPS and the stochasticbehaviour of fracture mechanics. The results indicate that the fracture initiation straindecreases with increasing density of EPS, while the fracture strength increases withincreasing density of EPS. Further, the results indicate that the loading conditions havean influence on the fracture initiation strain.</p>

corrected abstract:
<p>The aim of this master thesis was to investigate the mechanical properties of Expanded Polystyrene (EPS) through experimental testing. The strain was measured on the specimen’s surface with Digital Image Correlation (DIC), in order to investigate how the first principal strain at fracture initiation depends on the loading of the material. To impose different loading conditions various tests were performed, such as biaxial tension-compression, uniaxial tension and bending tests. The study was done with four densities of EPS, 35, 50, 75 and 100 g/L. To investigate how strain and stress at fracture initiation depend on temperature, the bending tests were performed at three different temperatures, -20, 20 and 50 °C. All tests were performed in the Solid Mechanics lab at KTH, Stockholm. For every experiment, five repetitions were made to reduce the influence of the inhomogeneous microstructure of EPS and the stochastic behaviour of fracture mechanics. The results indicate that the fracture initiation strain decreases with increasing density of EPS, while the fracture strength increases with increasing density of EPS. Further, the results indicate that the loading conditions have an influence on the fracture initiation strain.</p>
----------------------------------------------------------------------
In diva2:1781251 abstract is:
<p>In order to increase the safety of all air travel, technologies that continueto augment the pilot's ability to avoid collisions and stay clear of danger areneeded. But, before these can be certified and deployed, their performance andpotential failure cases have to be understood. This requires evaluating a modelof the system on simulated encounters, consisting of different trajectoriesthat should replicate the real world.</p><p>This is commonly done using a statistical encounter model, which produces largeamounts of data but relies on the accuracy of the statistical model, thuslimited in its ability to produce realistic data. The goal with this project isto create an encounter dataset of real trajectories that would provide analternative to encounter models.</p><p>This is done using an ADS-B dataset from The OpenSky Network (provided byDaedalean AI), consisting of 226 billion air traffic data points from 2019.First, a solution to efficiently query and reconstruct trajectories from thedataset is designed and implemented. Using it, a NMAC (Near Mid-Air Collision)dataset is created to demonstrate the viability of ADS-B as a source forcreating an encounter dataset, and to prove the capabilities of the designedsolution.</p>

corrected abstract:
<p>In order to increase the safety of all air travel, technologies that continue to augment the pilot's ability to avoid collisions and stay clear of danger are needed. But, before these can be certified and deployed, their performance and potential failure cases have to be understood. This requires evaluating a model of the system on simulated encounters, consisting of different trajectories that should replicate the real world.</p><p>This is commonly done using a statistical encounter model, which produces large amounts of data but relies on the accuracy of the statistical model, thus limited in its ability to produce realistic data. The goal with this project is to create an encounter dataset of real trajectories that would provide an alternative to encounter models.</p><p>This is done using an ADS-B dataset from The OpenSky Network (provided by Daedalean AI), consisting of 226 billion air traffic data points from 2019. First, a solution to efficiently query and reconstruct trajectories from the dataset is designed and implemented. Using it, a NMAC (Near Mid-Air Collision) dataset is created to demonstrate the viability of ADS-B as a source for creating an encounter dataset, and to prove the capabilities of the designed solution.</p>
----------------------------------------------------------------------
In diva2:1780228 abstract is:
<p>DNA microscopy is a group of emerging technologies that can visualize subcellularstructures or spatial localization of biomolecules (RNA transcripts, proteins). Themethods do not rely on light, but instead rely on different ways to propagate DNAstrands through PCR followed by some kind of backtracing of their origin to recoverspatial information. The present work aims to improve on the annealing phase of aPCR simulation script previously written by the supervisor of this project. The hybridstructure between interacting DNA strands that are about to undergo replication isimportant as it determines the extension product, or whether is it possible to beelongated at all. This complex prediction is currently done with a bioinformaticalignment which is sufficiently realistic for optmimally orthogonally designed primersand adaptor regions that can be assumed to always anneal linearly and where we expectit to. NUPACK, a DNA/RNA complex prection package, was used to replace the currentalignment based approach. Comparisons of the two methods on randomly generatedDNA strings showed large differences in the predicted complexes which are interpretedas improvements.</p>

corrected abstract:
<p>DNA microscopy is a group of emerging technologies that can visualize subcellular structures or spatial localization of biomolecules (RNA transcripts, proteins). The methods do not rely on light, but instead rely on different ways to propagate DNA strands through PCR followed by some kind of backtracing of their origin to recover spatial information. The present work aims to improve on the annealing phase of a PCR simulation script previously written by the supervisor of this project. The hybrid structure between interacting DNA strands that are about to undergo replication is important as it determines the extension product, or whether is it possible to be elongated at all. This complex prediction is currently done with a bioinformatic alignment which is sufficiently realistic for optimally orthogonally designed primers and adaptor regions that can be assumed to always anneal linearly and where we expect it to. NUPACK, a DNA/RNA complex prediction package, was used to replace the current alignment based approach. Comparisons of the two methods on randomly generated DNA strings showed large differences in the predicted complexes which are interpreted as improvements.</p>
----------------------------------------------------------------------
In diva2:1757006 abstract is:
<p>The task of predicting start-up growth has been an item of institutional as wellas widespread individual research and acclaim of those successful. This workis an attempt to distill the alleged factors of prediction in the large body ofwork that has already been documented, as well as investigating reasonable butyet untested variables. Conclusions are built with a multiple regression model,exploring 7 regressors with data spanning 2014-2019 to avoid the potentiallyabnormal impact of the Covid-19 crisis.Due to the choice of non-predictive regressors, the final result is an explanatorymodel, highlighting the importance of rigorousness in the process of model-building and outlining of data collection in regression analysis. Most regres-sors had a non-significant or weak relationship with the response variable, butconcludes an explanatory degree of 51%. Even if it can not be utilised as apredictive model, it may provide some interesting insight. In the final model,every regressor except one had an unexpected beta value, contradicting earlierresearch.</p>

corrected abstract:
<p>The task of predicting start-up growth has been an item of institutional as well as widespread individual research and acclaim of those successful. This work is an attempt to distill the alleged factors of prediction in the large body of work that has already been documented, as well as investigating reasonable but yet untested variables. Conclusions are built with a multiple regression model, exploring 7 regressors with data spanning 2014-2019 to avoid the potentially abnormal impact of the Covid-19 crisis.</p><p>Due to the choice of non-predictive regressors, the final result is an explanatory model, highlighting the importance of rigorousness in the process of model-building and outlining of data collection in regression analysis. Most regressors had a non-significant or weak relationship with the response variable, but concludes an explanatory degree of 51%. Even if it can not be utilised as a predictive model, it may provide some interesting insight. In the final model, every regressor except one had an unexpected beta value, contradicting earlier research.</p>
----------------------------------------------------------------------
In diva2:1737438 abstract is:
<p>Electrification of vehicles is now one of the top priorities of automotive manufacturers in orderto comply with the sustainability goals and standards set by the United Nations. One of thedisadvantages faced by electric vehicles is a longer time required for charging the vehicles ascompared to refuelling a conventional vehicle. To speed up the charging, it is necessary toimprove the power rating to charge the vehicle at charging stations. Developing an efficient OnBoard Charger (OBC) that converts Alternating Current (AC) power to Direct Current (DC)power to charge the battery is one of the contributing factors for faster charging.</p><p>In order to protect the electricity grid from power failures, a bidirectional OBC can beimplemented in the vehicle. This bidirectional OBC is able to transfer power to the electricitygrid as well by discharging the battery. This technique helps to stabilise the grid during highpower demand. Having an efficient OBC ensures minimal losses during power conversion fromthe charging station to the battery in the vehicle.</p><p>Therefore, this thesis deals with the development of an OBC model to be incorporated ina Hardware in Loop (HIL) simulation. This model is only an emulation of the actual OBChardware for HIL test and is therefore limited. The model is mainly developed in Simulinkand consists of control logic and power conversion stages like Precharge and Power FactorCorrection that successfully imitate the actual hardware.</p><p>Further, in order to validate the model, it was run in Model in Loop (MIL) simulationwherein the model was tested under scenarios similar to the ones in the HIL setup. A successfulconversion of AC power to DC power was performed and a demonstration of the flexibility andthe adaptability of the model to different test cases is documented in the report. Some of theoutputs of the model are DC current for charging, model stateflow and AC current drawn fromthe grid.</p>

corrected abstract:
<p>Electrification of vehicles is now one of the top priorities of automotive manufacturers in order to comply with the sustainability goals and standards set by the United Nations. One of the disadvantages faced by electric vehicles is a longer time required for charging the vehicles as compared to refuelling a conventional vehicle. To speed up the charging, it is necessary to improve the power rating to charge the vehicle at charging stations. Developing an efficient On Board Charger (OBC) that converts Alternating Current (AC) power to Direct Current (DC) power to charge the battery is one of the contributing factors for faster charging.</p><p>In order to protect the electricity grid from power failures, a bidirectional OBC can be implemented in the vehicle. This bidirectional OBC is able to transfer power to the electricity grid as well by discharging the battery. This technique helps to stabilise the grid during high power demand. Having an efficient OBC ensures minimal losses during power conversion from the charging station to the battery in the vehicle.</p><p>Therefore, this thesis deals with the development of an OBC model to be incorporated in a Hardware in Loop (HIL) simulation. This model is only an emulation of the actual OBC hardware for HIL test and is therefore limited. The model is mainly developed in Simulink and consists of control logic and power conversion stages like Precharge and Power Factor Correction that successfully imitate the actual hardware.</p><p>Further, in order to validate the model, it was run in Model in Loop (MIL) simulation wherein the model was tested under scenarios similar to the ones in the HIL setup. A successful conversion of AC power to DC power was performed and a demonstration of the flexibility and the adaptability of the model to different test cases is documented in the report. Some of the outputs of the model are DC current for charging, model stateflow and AC current drawn from the grid.</p>
----------------------------------------------------------------------
In diva2:1698044 abstract is:
<p>The drone industry is growing and the need for increased autonomy will be required if large fleetof drones will be able to fly without a single pilot per drone. A useful part of automating the flighten-route can be achieved with the upcoming standard of Direct Remote Id (DRI), which signalspositional data for drones and can be used as the perceptive part in a collision avoidance systembetween drones with the advantage of limited weight penalties and minimal financial cost.Simulations were carried out to understand different kinds of evasive maneuvers and develop asimple yet effective algorithm for avoiding obstacles and continue towards the next waypoint ona mission. Positional data can be retrieved with an ESP-32 board from a flight computer withMavlink protocol, which can then be broadcasted and received to an ESP-32 board using DirectRemote Id. The distances between the nearest drones can be computed, along with the shortest al-lowable distance and closest positions of the drones, if they were to continue on a straight course. Ifthe closest passing distance turned out closer than a set safety distance, an evasive maneuver is cal-culated and executed, with preliminary work focusing on evasion maneuvers on an horizontal plane.Flight tests showed that an evasive position could be calculated, and the drone successfully di-verted to it, while continuing with the mission after the evasion was completed. These resultsshowed the potential of using Direct Remote Id as a simple close proximity detection for use withcollision avoidance</p>

corrected abstract:
<p>The drone industry is growing and the need for increased autonomy will be required if large fleet of drones will be able to fly without a single pilot per drone. A useful part of automating the flight en-route can be achieved with the upcoming standard of Direct Remote Id (DRI), which signals positional data for drones and can be used as the perceptive part in a collision avoidance system between drones with the advantage of limited weight penalties and minimal financial cost.</p><p>Simulations were carried out to understand different kinds of evasive maneuvers and develop a simple yet effective algorithm for avoiding obstacles and continue towards the next waypoint on a mission. Positional data can be retrieved with an ESP-32 board from a flight computer with Mavlink protocol, which can then be broadcasted and received to an ESP-32 board using Direct Remote Id. The distances between the nearest drones can be computed, along with the shortest allowable distance and closest positions of the drones, if they were to continue on a straight course. If the closest passing distance turned out closer than a set safety distance, an evasive maneuver is calculated and executed, with preliminary work focusing on evasion maneuvers on an horizontal plane.</p><p>Flight tests showed that an evasive position could be calculated, and the drone successfully diverted to it, while continuing with the mission after the evasion was completed. These results showed the potential of using Direct Remote Id as a simple close proximity detection for use with collision avoidance</p>
----------------------------------------------------------------------
In diva2:1681378 abstract is:
<p>This project has investigated and formed a basis for the thermal- and flow-induceddynamic loads in the lower parts of a preheater situated in a nuclear power plant.Special attention has been given to the bottom plate that separates the heated andnon-heated secondary steam. Using computational fluid dynamics (CFD), steady RANSsimulations were first used to investigate how the flow into the preheater was affectedby the inlet boundary conditions. From there a steady RANS conjugate heat transfer(CHT) analysis was conducted as to obtain the temperature field within the preheaterand its solid components. This also investigated different approaches in modelling theheat transfer between the primary and secondary steam. Lastly, a scale-resolving LESwas conducted as to obtain the flow-induced dynamic loads on the bottom plate.</p><p>The results show that the modelling used in previous works gives a less uniformtemperature distribution as compared to when appropriate heat transfer coefficient(HTC) correlations are applied. Regardless of how the heat source is modelled, hot spotswith significantly larger temperatures are present in the bottom plate near the outletsof the bottom tube sections. The root mean square value, amplitude and frequencycontent of the fluctuating force acting on the bottom plate have also been obtained.The results of the analysis provide a good starting point for future work examining ifthe loads on their own or in combination may risk damaging the structure.</p>

corrected abstract:
<p>This project has investigated and formed a basis for the thermal- and flow-induced dynamic loads in the lower parts of a preheater situated in a nuclear power plant. Special attention has been given to the bottom plate that separates the heated and non-heated secondary steam. Using computational fluid dynamics (CFD), steady RANS simulations were first used to investigate how the flow into the preheater was affected by the inlet boundary conditions. From there a steady RANS conjugate heat transfer (CHT) analysis was conducted as to obtain the temperature field within the preheater and its solid components. This also investigated different approaches in modelling the heat transfer between the primary and secondary steam. Lastly, a scale-resolving LES was conducted as to obtain the flow-induced dynamic loads on the bottom plate.</p><p>The results show that the modelling used in previous works gives a less uniform temperature distribution as compared to when appropriate heat transfer coefficient (HTC) correlations are applied. Regardless of how the heat source is modelled, hot spots with significantly larger temperatures are present in the bottom plate near the outlets of the bottom tube sections. The root mean square value, amplitude and frequency content of the fluctuating force acting on the bottom plate have also been obtained. The results of the analysis provide a good starting point for future work examining if the loads on their own or in combination may risk damaging the structure.</p>
----------------------------------------------------------------------
In diva2:1588547 abstract is:
<p>Some physical systems exhibit topological properties in the form of topological invariants— features of the system that remain constant unless the system undergoessignificant changes i.e. changes that require closing the energy gap of the Hamiltonian.This work studies one example of a system with topological properties — a Kitaevchain. Here, this model is studied when it is coupled to an environment. We studythe effect of the coupling on the topology of the system and attempt to find signaturesof topological phases in the dynamics of the system. By using the Lindblad equationdefined in the formalism of third quantization, we study the time evolution of thesystem numerically by using the Euler method. We find that the dynamics of theentanglement spectrum of half of the chain is different in the topological and trivialphases: if the system undergoes a quench from trivial to topological phase, the entanglementspectrum exhibits crossings as the system evolves in time. We also studythe topological phases when disorder is added to the system. We test the stabilityof the topological phases of the system against disorder and find that the topologicalphases are not affected by a weak disorder. Moreover, by studying the statistics of theminimum entanglement spectrum gap, we find that, in general, a stronger disordermakes the crossings less likely to appear in the topological phase and more likely toappear in the trivial phase.</p>

corrected abstract:
<p>Some physical systems exhibit topological properties in the form of topological invariants—features of the system that remain constant unless the system undergoes significant changes i.e. changes that require closing the energy gap of the Hamiltonian. This work studies one example of a system with topological properties — a Kitaev chain. Here, this model is studied when it is coupled to an environment. We study the effect of the coupling on the topology of the system and attempt to find signatures of topological phases in the dynamics of the system. By using the Lindblad equation defined in the formalism of third quantization, we study the time evolution of the system numerically by using the Euler method. We find that the dynamics of the entanglement spectrum of half of the chain is different in the topological and trivial phases: if the system undergoes a quench from trivial to topological phase, the entanglement spectrum exhibits crossings as the system evolves in time. We also study the topological phases when disorder is added to the system. We test the stability of the topological phases of the system against disorder and find that the topological phases are not affected by a weak disorder. Moreover, by studying the statistics of the minimum entanglement spectrum gap, we find that, in general, a stronger disorder makes the crossings less likely to appear in the topological phase and more likely to appear in the trivial phase.</p>
----------------------------------------------------------------------
In diva2:1528157 abstract is:
<p>Typical propulsion systems for space transportation involve the ejectionof mass for momentum gain. Solar sails remove the requirement forpropellant mass by obtaining their momentum from solar photons, whichrequires large surface area and very low mass. In this way solar sailcraftgenerate constant accelerations, in contrast with the impulsive thrust ofchemical rockets. This enables new families of orbits and presents a newchallenge for optimization and control. This study presents a summary ofproven solar sail technology and investigates minimum-time trajectoriesto and from Mars. This optimization is carried out in two phases, usingan energy rate-maximizing algorithm for planetary escape and sparse nonlinearprogramming for the interplanetary segment. The results provideupper bounds for minimum-time transfers and are then compared to possiblesail sizes and sailcraft masses. This in turn may inform the designand selection of future missions for materials exchange during explorationor settlement efforts.</p>

corrected abstract:
<p>Typical propulsion systems for space transportation involve the ejection of mass for momentum gain. Solar sails remove the requirement for propellant mass by obtaining their momentum from solar photons, which requires large surface area and very low mass. In this way solar sailcraft generate constant accelerations, in contrast with the impulsive thrust of chemical rockets. This enables new families of orbits and presents a new challenge for optimization and control. This study presents a summary of proven solar sail technology and investigates minimum-time trajectories to and from Mars. This optimization is carried out in two phases, using an energy rate-maximizing algorithm for planetary escape and sparse nonlinear programming for the interplanetary segment. The results provide upper bounds for minimum-time transfers and are then compared to possible sail sizes and sailcraft masses. This in turn may inform the design and selection of future missions for materials exchange during exploration or settlement efforts.</p>
----------------------------------------------------------------------
In diva2:1499290 abstract is:
<p>Three different void content calculation techniques using optical microscopy werecompared in multiple-user trials. The three methods studied comprised of a selection,thresholding, and semi-automatic machine learning method. The techniques wereapplied to micrographs of three carbon fiber-epoxy composite plates manufacturedin-house, where one plate had reduced void content by means of debulking priorto curing. The users performed the techniques on the sets of micrographs and thestandard deviation between the users void content results were measured.The advantages of the three methods were discussed and their practical applications wereproposed.</p><p>The trials showed agreement between users on what are voids and not as well asshowing that uncertainties in void content are specimen-specific and not attributed todifferent users or methods applied. All three methods showed satisfying precision incalculating void content compared to void content quality levels provided by literature.It was found that thresholding, which is the current standard method of void contentcalculation using microscopy, inhabits an unscientific bias which compromises the legitimacyof the method. The study formulates a manual selection-based method usingedge-detection selection tools intended to benchmark void content in images, as wellas proposing a route to the automation of void content analysis using microscopy.</p>

corrected abstract:
<p>Three different void content calculation techniques using optical microscopy were compared in multiple-user trials. The three methods studied comprised of a selection, thresholding, and semi-automatic machine learning method. The techniques were applied to micrographs of three carbon fiber-epoxy composite plates manufactured in-house, where one plate had reduced void content by means of debulking prior to curing. The users performed the techniques on the sets of micrographs and the standard deviation between the users void content results were measured. The advantages of the three methods were discussed and their practical applications were proposed.</p><p>The trials showed agreement between users on what are voids and not as well as showing that uncertainties in void content are specimen-specific and not attributed to different users or methods applied. All three methods showed satisfying precision in calculating void content compared to void content quality levels provided by literature. It was found that thresholding, which is the current standard method of void content calculation using microscopy, inhabits an unscientific bias which compromises the legitimacy of the method. The study formulates a manual selection-based method using edge-detection selection tools intended to benchmark void content in images, as well as proposing a route to the automation of void content analysis using microscopy.</p>
----------------------------------------------------------------------
In diva2:1244644 abstract is:
<p>Holms Industri AB currently manufactures a snow plow called Holms PD Plus, where thedisc is made of a 4 mm thick steelplate. The disc weighs 131 kg and since the mass of thesnow plow is closely related to the fuel consumption of the carrying vehicle it is of interest toreduce the mass of the plow. Therefore, the aim of this report is to investigate if it is possibleto manufacture the disc out of plastic instead, namely high density polyethylene (HDPE). Achange of material also causes the need to design new mountings since the current weldingmethod no longer would be an alternative.The analyses are based on eight different load cases that occur during snow clearance.However, only three of these are used to dimension the plow disc since the others are not ascritical. The current snow plow was analysed and used as a reference in the development ofalternative snow plows. All analyses were performed in the FEM-programme AnsysWorkbench 17.1.The results show that it is possible to use a plastic plow disc if it is 10 mm thick. In detail, thetop domed part should be 10 mm HDPE and the lower straight part should be 8 mm HDPEsupported on the back by a 2 mm thick steel plate. The suggested disc weighs 73 kg, meaninga reduction by 58 kg compared to the original. The attachments should be a combinationof welds (metal to metal) and bolted joints (plastic to metal). Further work should concernfatigue analyses of the plow disc and more in depth assessments of the mountings.</p>

corrected abstract:
<p>Holms Industri AB currently manufactures a snow plow called Holms PD Plus, where the disc is made of a 4 mm thick steelplate. The disc weighs 131 kg and since the mass of the snow plow is closely related to the fuel consumption of the carrying vehicle it is of interest to reduce the mass of the plow. Therefore, the aim of this report is to investigate if it is possible to manufacture the disc out of plastic instead, namely high density polyethylene (HDPE). A change of material also causes the need to design new mountings since the current welding method no longer would be an alternative.</p><p>The analyses are based on eight different load cases that occur during snow clearance. However, only three of these are used to dimension the plow disc since the others are not as critical. The current snow plow was analysed and used as a reference in the development of alternative snow plows. All analyses were performed in the FEM-programme Ansys Workbench 17.1.</p><p>The results show that it is possible to use a plastic plow disc if it is 10 mm thick. In detail, the top domed part should be 10 mm HDPE and the lower straight part should be 8 mm HDPE supported on the back by a 2 mm thick steel plate. The suggested disc weighs 73 kg, meaning a reduction by 58 kg compared to the original. The attachments should be a combination of welds (metal to metal) and bolted joints (plastic to metal). Further work should concern fatigue analyses of the plow disc and more in depth assessments of the mountings.</p>
----------------------------------------------------------------------
In diva2:1216849 abstract is:
<p>Competitive gaming or E-Sport is more popular than ever, this has resulted inan increase in the number of players and tournament prize pools. In traditionalsports demographic factors have been shown to have high predictive power whenit comes to determining a country's success in the Olympic Games. Similarresults have been found when it comes to E-sport which is why it is interestingto investigate whether there are any dierences between regions in Dota 2 aswell. The goal is to analyze factors that contribute to the success of a Dota 2team by building a multiple-regression model. All data is collected from opensources and contains 55 active Dota 2 teams that have been playing between2011 - 2018. The factors in the nal model is the sum of the individual playersestimated skill, the skill dierence between the highest and lowest rated playerson the team, the number of games the team has played and organization region.The result gives an insight in what a person or organization would want to lookat when researching a team as well as a model that can be used to predict howgood a team will perform.</p>

corrected abstract:
<p>Competitive gaming or E-Sport is more popular than ever, this has resulted in an increase in the number of players and tournament prize pools. In traditional sports demographic factors have been shown to have high predictive power when it comes to determining a country's success in the Olympic Games. Similar results have been found when it comes to E-sport which is why it is interesting to investigate whether there are any differences between regions in Dota 2 as well. The goal is to analyze factors that contribute to the success of a Dota 2 team by building a multiple-regression model. All data is collected from open sources and contains 55 active Dota 2 teams that have been playing between 2011 - 2018. The factors in the final model is the sum of the individual players estimated skill, the skill difference between the highest and lowest rated players on the team, the number of games the team has played and organization region. The result gives an insight in what a person or organization would want to look at when researching a team as well as a model that can be used to predict how good a team will perform.</p>
----------------------------------------------------------------------
In diva2:1142776 abstract is:
<p>With the increasing advances in the field of autonomousvehicles it is alluring to ask if a possible vehicularparadigm shift is in the near future. Maximizing road capacitywith Intelligent Traffic Intersections that communicate withautonomous vehicles could become a reality, where the needfor traffic lights and stop signs is excluded. In this paper, anAutonomous Intersection Management system is introduced thatutilizes trajectory-based prioritization and motion planning techniquesto manage traffic in an orthogonal single lane four-wayintersection. The developed system reduces the need for vehiclesto slow down or even stop before intersections, contrariwise, itlets all vehicles enter the intersection at the highest allowed speed.The proposed solution is shown to increase the capacity of intersectionscompared with contemporary intersections managedwith traffic lights.</p>

corrected abstract:
<p>With the increasing advances in the field of autonomous vehicles it is alluring to ask if a possible vehicular paradigm shift is in the near future. Maximizing road capacity with Intelligent Traffic Intersections that communicate with autonomous vehicles could become a reality, where the need for traffic lights and stop signs is excluded. In this paper, an Autonomous Intersection Management system is introduced that utilizes trajectory-based prioritization and motion planning techniques to manage traffic in an orthogonal single lane four-way intersection. The developed system reduces the need for vehicles to slow down or even stop before intersections, contrariwise, it lets all vehicles enter the intersection at the highest allowed speed. The proposed solution is shown to increase the capacity of intersections compared with contemporary intersections managed with traffic lights.</p>
----------------------------------------------------------------------
In diva2:1142764 abstract is:
<p>A formation of vehicles with close inter-vehicledistances is referred to as a vehicle platoon. Platooning is madepossible by implementing controllers that regulates vehicles’velocities based on information from the rest of the platoon,independently from human interaction. This report studies howthe stability of the system is affected by the choice of availableinformation for each vehicle. Vehicle models are implemented asblock diagrams in Simulink and the feedback gains are calculatedusing MATLAB. The vehicles’ behaviors are simulated using apredefined acceleration profile for the leader. The results showthat the system reacts differently to changes in the leader’sacceleration depending on which states each vehicle determinesits control signal on. Stability is best achieved by having thevehicles communicating with the vehicle in front and the leadingvehicle. It is also shown that time delays negatively affects thestability of the system, but stability is still best achieved whenvehicles communicate with the vehicle in front and the leadingvehicle.</p>

corrected abstract:
<p>A formation of vehicles with close inter-vehicle distances is referred to as a vehicle platoon. Platooning is made possible by implementing controllers that regulates vehicles’ velocities based on information from the rest of the platoon, independently from human interaction. This report studies how the stability of the system is affected by the choice of available information for each vehicle. Vehicle models are implemented as block diagrams in Simulink and the feedback gains are calculated using MATLAB. The vehicles’ behaviors are simulated using a predefined acceleration profile for the leader. The results show that the system reacts differently to changes in the leader’s acceleration depending on which states each vehicle determines its control signal on. Stability is best achieved by having the vehicles communicating with the vehicle in front and the leading vehicle. It is also shown that time delays negatively affects the stability of the system, but stability is still best achieved when vehicles communicate with the vehicle in front and the leading vehicle.</p>
----------------------------------------------------------------------
In diva2:1120852 - duplicate of In diva2:1142776?
abstract is:
<p>With the increasing advances in the field of autonomousvehicles it is alluring to ask if a possible vehicularparadigm shift is in the near future. Maximizing road capacitywith Intelligent Traffic Intersections that communicate withautonomous vehicles could become a reality, where the needfor traffic lights and stop signs is excluded. In this paper, anAutonomous Intersection Management system is introduced thatutilizes trajectory-based prioritization and motion planning techniquesto manage traffic in an orthogonal single lane four-wayintersection. The developed system reduces the need for vehiclesto slow down or even stop before intersections, contrariwise, itlets all vehicles enter the intersection at the highest allowed speed.The proposed solution is shown to increase the capacity of intersectionscompared with contemporary intersections managedwith traffic lights.</p>

corrected abstract:
<p>With the increasing advances in the field of autonomous vehicles it is alluring to ask if a possible vehicular paradigm shift is in the near future. Maximizing road capacity with Intelligent Traffic Intersections that communicate with autonomous vehicles could become a reality, where the need for traffic lights and stop signs is excluded. In this paper, an Autonomous Intersection Management system is introduced that utilizes trajectory-based prioritization and motion planning techniques to manage traffic in an orthogonal single lane four-way intersection. The developed system reduces the need for vehicles to slow down or even stop before intersections, contrariwise, it lets all vehicles enter the intersection at the highest allowed speed. The proposed solution is shown to increase the capacity of intersections compared with contemporary intersections managed with traffic lights.</p>
----------------------------------------------------------------------
In diva2:1120393 abstract is:
<p>This work explores the possibility of using a photon counting silicon detector developedfor Computer Tomography (CT) in Single Photon Emission Computed Tomography(SPECT) applications. This would allow for a more versatile and a much cheapersystem. The main focus is on determining the efficiency and resolution of such a system.This is done initially via an geometric model evaluating the solid angle of the collimator,showing promising results versus a standard Low Energy High Resolution (LEHR)collimator. Secondly a Monte-Carlo simulation is used for a more in depth analysis ofthe detector response by using two different radionuclides. The performance is measuredwith reference to efficiency and scatter to primary ratio (s/p). The energy thresholdsfor binning is evaluated with a Signal-Difference-to-Noise Ratio (SDNR). A Point SpreadFunction (PSF) is simulated with and without the impact from a human-like phantom.The work concludes that an implementation would not likely to be able to compete withspecialised myocardium SPECT due too the high noise from the detector response whena high efficiency threshold is set. Further investigations in general SPECT applicationsis recommended.Sammanfattning</p>

corrected abstract:
<p>This work explores the possibility of using a photon counting silicon detector developed for Computer Tomography (CT) in Single Photon Emission Computed Tomography (SPECT) applications. This would allow for a more versatile and a much cheaper system. The main focus is on determining the efficiency and resolution of such a system. This is done initially via an geometric model evaluating the solid angle of the collimator, showing promising results versus a standard Low Energy High Resolution (LEHR) collimator. Secondly a Monte-Carlo simulation is used for a more in depth analysis of the detector response by using two different radionuclides. The performance is measured with reference to efficiency and scatter to primary ratio (s/p). The energy thresholds for binning is evaluated with a Signal-Difference-to-Noise Ratio (SDNR). A Point Spread Function (PSF) is simulated with and without the impact from a human-like phantom. The work concludes that an implementation would not likely to be able to compete with specialised myocardium SPECT due too the high noise from the detector response when a high efficiency threshold is set. Further investigations in general SPECT applications is recommended.</p>
----------------------------------------------------------------------
In diva2:1110826 abstract is:
<p>As surprising as it may seem, accurate north finding, with an error of only several milli-radian, is still a very difficulttask and has been achieved only with very expensive systems. On the contrary, there are very simple systems that give theazimuth with an angular error five times superior but for a price a hundred times inferior. Moreover, these systems generally arenon-autonomous (i.e. they are environment dependent and can lose their precision in many situations). This assessment leads tothe following relevant question: Is it possible to design a north finding system with good precision, for a moderated cost and thatworks in any situation?This report presents and evaluates a solution which attempts to answer this problem. This solution is based on a gyrocompassingprinciple: a gyro measures the earth’s angular velocity in order to find the azimuth. This solution can be implementedfollowing several methods, this report presents and compares two of these implementations: Maytagging and Carouseling. Thecomparison is made thanks to a theoretical study, a computer simulation and tests on a real model designed for this report.Carouseling allows us, in theory, to reach an accurate azimuth, but puts mechanical constraints on the system. Maytaggingimplementation seems adapted considering trade-off between precision and cost. Further improvements on gyros will certainlymake systems based on gyro-compassing the most efficient autonomous systems for north finding.In this report, precisions reached by the different implementations are not made explicit for confidentiality reasons.</p>

corrected abstract:
<p>As surprising as it may seem, accurate north finding, with an error of only several milli-radian, is still a very difficult task and has been achieved only with very expensive systems. On the contrary, there are very simple systems that give the azimuth with an angular error five times superior but for a price a hundred times inferior. Moreover, these systems generally are non-autonomous (i.e. they are environment dependent and can lose their precision in many situations). This assessment leads to the following relevant question: Is it possible to design a north finding system with good precision, for a moderated cost and that works in any situation?</p><p>This report presents and evaluates a solution which attempts to answer this problem. This solution is based on a gyrocompassing principle: a gyro measures the earth’s angular velocity in order to find the azimuth. This solution can be implemented following several methods, this report presents and compares two of these implementations: Maytagging and Carouseling. The comparison is made thanks to a theoretical study, a computer simulation and tests on a real model designed for this report. Carouseling allows us, in theory, to reach an accurate azimuth, but puts mechanical constraints on the system. Maytagging implementation seems adapted considering trade-off between precision and cost. Further improvements on gyros will certainly make systems based on gyro-compassing the most efficient autonomous systems for north finding.<br>In this report, precisions reached by the different implementations are not made explicit for confidentiality reasons.</p>
----------------------------------------------------------------------
In diva2:1106332 - missing space in title:
"Coefficients and zeros of mixed characteristicpolynomials"
==>
"Coefficients and zeros of mixed characteristic polynomials"

abstract is:
<p>The mixed characteristic polynomial (MCP) was introduced in the papersof Marcus, Spielman and Srivastava from 2013 on Ramanujan graphs and the Kadison-Singerconjecture. Several known results and open problems can be formulated in termsof MCPs. The proofs of Marcus, Spielman and Srivastava involve bounding theroots of certain MCPs. Gurvits’ generalization of van der Waerden’s permanentconjecture bounds the constant term of MCPs using the capacity of an underlyingpolynomial.This thesis surveys selected results for MCPs. A counterexample to theHolens-Ðoković conjecture, due to Wanless, is discussed in the context of MCPs.It is used to show how a sequence of MCP coefficients is not monotoneand how the roots of associated Laguerre polynomials do not always majorizethose of other MCPs. Finally, we prove an analogue of the root bound in theproof of the Kadison-Singer conjecture. It applies to product polynomials ofdoubly stochastic matrices through classical results in graph theory due toGodsil, Mohar, Heilmann and Lieb.</p>

corrected abstract:
<p>The mixed characteristic polynomial (MCP) was introduced in the papers of Marcus, Spielman and Srivastava from 2013 on Ramanujan graphs and the Kadison-Singer conjecture. Several known results and open problems can be formulated in terms of MCPs. The proofs of Marcus, Spielman and Srivastava involve bounding the roots of certain MCPs. Gurvits’ generalization of van der Waerden’s permanent conjecture bounds the constant term of MCPs using the capacity of an underlying polynomial.</p><p>This thesis surveys selected results for MCPs. A counterexample to the Holens-Ðoković conjecture, due to Wanless, is discussed in the context of MCPs. It is used to show how a sequence of MCP coefficients is not monotone<br>and how the roots of associated Laguerre polynomials do not always majorize those of other MCPs. Finally, we prove an analogue of the root bound in the proof of the Kadison-Singer conjecture. It applies to product polynomials of<br>doubly stochastic matrices through classical results in graph theory due to Godsil, Mohar, Heilmann and Lieb.</p>

Note there are two strange breaks - but they are in the original abstract.
----------------------------------------------------------------------
In diva2:662361 abstract is:
<p>In this thesis it isshown how to measure the annual loss expectancy of computer networks due to therisk of cyber attacks. With the development of metrics for measuring theexploitation difficulty of identified software vulnerabilities, it is possibleto make a measurement of the annual loss expectancy for computer networks usingBayesian networks. To enable the computations, computer net-work vulnerabilitydata in the form of vulnerability model descriptions, vulnerable dataconnectivity relations and intrusion detection system measurements aretransformed into vector based numerical form. This data is then used to generatea probabilistic attack graph which is a Bayesian network of an attack graph.The probabilistic attack graph forms the basis for computing the annualizedloss expectancy of a computer network. Further, it is shown how to compute anoptimized order of vulnerability patching to mitigate the annual lossexpectancy. An example of computation of the annual loss expectancy is providedfor a small invented example network</p>

corrected abstract:
<p>In this thesis it is shown how to measure the annual loss expectancy of computer networks due to the risk of cyber attacks. With the development of metrics for measuring the exploitation difficulty of identified software vulnerabilities, it is possible to make a measurement of the annual loss expectancy for computer networks using Bayesian networks. To enable the computations, computer network vulnerability data in the form of vulnerability model descriptions, vulnerable data connectivity relations and intrusion detection system measurements are transformed into vector based numerical form. This data is then used to generate a probabilistic attack graph which is a Bayesian network of an attack graph. The probabilistic attack graph forms the basis for computing the annualized loss expectancy of a computer network. Further, it is shown how to compute an optimized order of vulnerability patching to mitigate the annual loss expectancy. An example of computation of the annual loss expectancy is provided for a small invented example network</p>
----------------------------------------------------------------------
In diva2:620469 abstract is:
<p>Disabilitybenefit is a publicly funded benefit in Sweden that provides financialprotection to individuals with permanent working ability impairments due todisability, injury, or illness. The eligibility requirements for disabilitybenefit were tightened June 1, 2008 to require that the working abilityimpairment be permanent and that no other factors such as age or local labormarket conditions can affect eligibility for the benefit. The goal of thispaper is to investigate risk factors for the incidence disability benefit andthe effects of the 2008 reform. This is the first study to investigate theimpact of the 2008 reform on the demographics of those that received disabilitybenefit. A logistic regression model was used to study the effect of the 2008law change. The regression results show that the 2008 reform did have astatistically significant effect on the demographics of the individuals whowere granted disability benefit. After the reform women were lessoverrepresented, the older age groups were more overrepresented, and peoplewith short educations were more overrepresented. Although the variables for SKLregions together were jointly statistically significant, their coefficientswere small and the group of variables had the least amount of explanatory valuecompared to the variables for age, education, gender and the interactionvariables.</p>

corrected abstract:
<p>Disability benefit is a publicly funded benefit in Sweden that provides financial protection to individuals with permanent working ability impairments due to disability, injury, or illness. The eligibility requirements for disability benefit were tightened June 1, 2008 to require that the working ability impairment be permanent and that no other factors such as age or local labor market conditions can affect eligibility for the benefit. The goal of this paper is to investigate risk factors for the incidence disability benefit and the effects of the 2008 reform. This is the first study to investigate the impact of the 2008 reform on the demographics of those that received disability benefit. A logistic regression model was used to study the effect of the 2008 law change. The regression results show that the 2008 reform did have a statistically significant effect on the demographics of the individuals who were granted disability benefit. After the reform women were less overrepresented, the older age groups were more overrepresented, and people with short educations were more overrepresented. Although the variables for SKL regions together were jointly statistically significant, their coefficients were small and the group of variables had the least amount of explanatory value compared to the variables for age, education, gender and the interaction variables.</p>
----------------------------------------------------------------------
In diva2:618600 - Note: no full text in DiVA
abstract is:
<p>Due to composite materials are widely used nowadays, it is important to understand manyareas concerning their behaviour that are not completely understood yet.This work focuses on the study of the strength of GRP-laminates with multiple randomlydistributed holes. For this purpose, percolation theory has been used in order totry to predict the normalized strength of laminates as function of only the hole density(the amount of hole area/specimen area).Three dierent laminates have been studied, DBLT800, DBL600 and DBL850, withmany simulations and tensile tests, in order to obtain results with good accuracy. Forsimulations, a suitable model was developed in FE-code ABAQUS in order to get thestrength of every model. For tensile tests, plates were manufactured previously by a vacuumassisted infusion process. Then, the plates were cut, drilled and tested. Once all theresults have been obtained, simulations and experiments were compared in order to knowif the strength obtained was good.These results obtained are very encouraging, showing that percolation theory can beused for predicting the strength of perforated composite plates, and the same curve obtainedmay be used for both DBLT800 and DBL600 laminates with suciently gooddelity.</p>

corrected abstract:
<p>Due to composite materials are widely used nowadays, it is important to understand many areas concerning their behaviour that are not completely understood yet. This work focuses on the study of the strength of GRP-laminates with multiple randomly distributed holes. For this purpose, percolation theory has been used in order to try to predict the normalized strength of laminates as function of only the hole density(the amount of hole area/specimen area).Three different laminates have been studied, DBLT800, DBL600 and DBL850, with many simulations and tensile tests, in order to obtain results with good accuracy. For simulations, a suitable model was developed in FE-code ABAQUS in order to get the strength of every model. For tensile tests, plates were manufactured previously by a vacuumassisted infusion process. Then, the plates were cut, drilled and tested. Once all the results have been obtained, simulations and experiments were compared in order to know if the strength obtained was good. These results obtained are very encouraging, showing that percolation theory can be used for predicting the strength of perforated composite plates, and the same curve obtained may be used for both DBLT800 and DBL600 laminates with suciently good fidelity.</p>
----------------------------------------------------------------------
In diva2:515581 abstract is:
<p>This essay is the report of my master's dissertation for Master of Engineering and Teacher Program atKTH and SU in Stockholm. The aims of the master’s dissertation are to increase the understandingand compose a map of how different presentation models affect the interest and ability to learnwhen the subject is resilient construction element is presented on a one day seminar.I have composed maps of three different presentation models in which the presentation models’communicative approach varied between authoritative and dialogic. The presentation models thathave been composed a map of are:• Seminar designed by problem-based learning• Seminar designed by the case method• Authoritative seminar The presentations models were tested and based on the tests, the feedback from participants and myparticipation studies, the presentation were surveyed and conclusions drawn about the learning and theinterest. This study was performed in the subject resilient construction element with the target groupadults with a basic skill in mechanics.The result of this study is that the potential for learning and interest increased if the communicativeapproach is in the dialogic direction. The communicative approach should neither be veryauthoritative or very dialogic. This is because the interest tends to decrease when the communicativeapproach is too authoritative and because the participants have problem with directing the seminarif communicative approach is too dialogic. Difficulties occur for the participants to control thedialogic communicative approach if they don’t have prior knowledge. The seminar model thatincreased learning and interests the most was the case-seminar format.</p>

corrected abstract:
<p>This essay is the report of my master's dissertation for Master of Engineering and Teacher Program at KTH and SU in Stockholm. The aims of the master’s dissertation are to increase the understanding and compose a map of how different presentation models affect the interest and ability to learn when the subject is resilient construction element is presented on a one day seminar. I have composed maps of three different presentation models in which the presentation models’ communicative approach varied between authoritative and dialogic. The presentation models that have been composed a map of are:<ul><li>Seminar designed by problem-based learning</li><li>Seminar designed by the case method</li><li>Authoritative seminar</li></ul></p><p>The presentations models were tested and based on the tests, the feedback from participants and my participation studies, the presentation were surveyed and conclusions drawn about the learning and the interest. This study was performed in the subject resilient construction element with the target group adults with a basic skill in mechanics.</p><p>The result of this study is that the potential for learning and interest increased if the communicative approach is in the dialogic direction. The communicative approach should neither be very authoritative or very dialogic. This is because the interest tends to decrease when the communicative approach is too authoritative and because the participants have problem with directing the seminar if communicative approach is too dialogic. Difficulties occur for the participants to control the dialogic communicative approach if they don’t have prior knowledge. The seminar model that increased learning and interests the most was the case-seminar format.</p>
----------------------------------------------------------------------
In diva2:1905032 abstract is:
<p>Studying White Matter (WM) lesions in premature babies is a societal challenge, aspremature mortality is decreasing while neurological morbidity remains the same.This master thesis is part of the p-HCP (Premature Human Connectome Project),which aims to study these lesions using extreme magnetic field MRI (11.7T) , startingwith the construction of a typical ex vivo brain atlas at mesoscopic scale and comparingit, in a second phase, to an atlas of damaged brains. This work presents a segmentationstrategy for the fetal brain structures to characterize them at different key stages ofthe development using MRI images with unprecedented resolution. We first manuallysegmented the structures of a 20-week gestation brain using a histological atlas, inorder to explore the information contained the nineteen MRI modalities available(quantitative, weighted and diffusion images). We then segmented the fetal brainstructures, without anatomical a priori, by using data engineering and automaticclustering algorithms. We have succeeded in designing a proof of concept for automaticsegmentation for the fetal brain and extracting, from our MRI images, groups ofstructures similar in molecular composition and cytoarchitecture.</p>

corrected abstract:
<p>Studying White Matter (WM) lesions in premature babies is a societal challenge, as premature mortality is decreasing while neurological morbidity remains the same. This master thesis is part of the p-HCP (Premature Human Connectome Project), which aims to study these lesions using extreme magnetic field MRI (11.7T) , starting with the construction of a typical ex vivo brain atlas at mesoscopic scale and comparing it, in a second phase, to an atlas of damaged brains. This work presents a segmentation strategy for the fetal brain structures to characterize them at different key stages of the development using MRI images with unprecedented resolution. We first manually segmented the structures of a 20-week gestation brain using a histological atlas, in order to explore the information contained the nineteen MRI modalities available (quantitative, weighted and diffusion images). We then segmented the fetal brain structures, without anatomical a priori, by using data engineering and automatic clustering algorithms. We have succeeded in designing a proof of concept for automatic segmentation for the fetal brain and extracting, from our MRI images, groups of structures similar in molecular composition and cytoarchitecture.</p>
----------------------------------------------------------------------
In diva2:1903464 abstract is:
<p>This thesis presents the theory, process, and results of creating a computational model for creep crackgrowth calculations. The model aims to enable industry customers to better plan their productions byestimating the remaining life affected by creep crack growth. The creep crack growth is modeledusing existing theories based on fracture mechanics, such as stress intensity factors 𝐾𝐼, and the 𝐶∗parameter. Replica testing was explored as the assumed method for identifying creep defects. Thedifficulty of where to place the replicas due to creep phenomena changing the critical locationscompared to elastic analyses is also discussed.</p><p>The model was developed using Python, the model allows users to input parameters such as geometry,material properties, load data, and settings. These settings include calculating creep crack growth forvarying depth-to-length ratios, applying different stress states at different points of the crack frontand whether R6 defect assessment analysis should be performed. The model is heavily based on theSwedish handbook SSM2018:18 but also relies on the British standard BS7910 and has been partiallyvalidated by comparing its results with those from commercially available software. Additionally, thesensitivity of the input parameters is analysed and presented.</p>

corrected abstract:
<p>This thesis presents the theory, process, and results of creating a computational model for creep crack growth calculations. The model aims to enable industry customers to better plan their productions by estimating the remaining life affected by creep crack growth. The creep crack growth is modeled using existing theories based on fracture mechanics, such as stress intensity factors 𝐾<sub>𝐼</sub>, and the 𝐶<sup>∗</sup> parameter. Replica testing was explored as the assumed method for identifying creep defects. The difficulty of where to place the replicas due to creep phenomena changing the critical locations compared to elastic analyses is also discussed.</p><p>The model was developed using Python, the model allows users to input parameters such as geometry, material properties, load data, and settings. These settings include calculating creep crack growth for varying depth-to-length ratios, applying different stress states at different points of the crack front and whether R6 defect assessment analysis should be performed. The model is heavily based on the Swedish handbook SSM2018:18 but also relies on the British standard BS7910 and has been partially validated by comparing its results with those from commercially available software. Additionally, the sensitivity of the input parameters is analysed and presented.</p>

Note that this abstract uses characters from Unicode's Mathematical Alphanumeric Symbols block.
----------------------------------------------------------------------
In diva2:1900919 abstract is:
<p>The subject of this thesis is the analysis and optimisation of cable harness variantsin Scania trucks. The growth in complexity of electrical systems on vehicles hasresulted in a number of different forms and lengths of their cable harnesses. Thecable harnesses of a truck consists of a numerous cable routings and for each truckvariant there are different cable routings. Selecting these for the formation of acomplete cable harness is currently a manual task and this thesis is an effort toaddress this issue.For the purpose of solving this problem, a method and a conceptual tool werecreated in order to establish an efficient procedure for cable routing selection. Thisis based on existing cable routing layouts in Scania’s design environment of SaberHarness/ Catia V5. The method exploits the tree in a 3D environment and usesDijkstra’s algorithm which is used to find the shortest path between node points.Findings prove that the proposed tool helps to decrease the time and human effortneeded for selecting the necessary cable routings for a cable harness. This improvesthe quality of the product and also optimises the number of length variants.</p>

corrected abstract:
<p>The subject of this thesis is the analysis and optimisation of cable harness variants in Scania trucks. The growth in complexity of electrical systems on vehicles has resulted in a number of different forms and lengths of their cable harnesses. The cable harnesses of a truck consists of a numerous cable routings and for each truck variant there are different cable routings. Selecting these for the formation of a complete cable harness is currently a manual task and this thesis is an effort to address this issue.</p><p>For the purpose of solving this problem, a method and a conceptual tool were created in order to establish an efficient procedure for cable routing selection. This is based on existing cable routing layouts in Scania’s design environment of Saber Harness/ Catia V5. The method exploits the tree in a 3D environment and uses Dijkstra’s algorithm which is used to find the shortest path between node points.</p><p>Findings prove that the proposed tool helps to decrease the time and human effort needed for selecting the necessary cable routings for a cable harness. This improves the quality of the product and also optimises the number of length variants.</p>
----------------------------------------------------------------------
In diva2:1896438 abstract is:
<p>Context. Over the last two decades, space-based photometric missions such asCoRot, Kepler/K2, and TESS have provided a wealth of photometric data of solar-like stars. Several methods have been used to extract the surface rotation period,using either the Lomb-Scargle periodogram, the autocorrelation function, wavelettransforms or Gaussian processes.Aims. We propose to evaluate the efficiency for measuring rotation period of arecently proposed method, called the Gradient Power Spectrum (GPS).Methods. The GPS is calculated in two steps. First, we calculate the globalwavelet power spectrum (GWPS) from the time series. Then the gradient is cal-culated from the logarithmic derivative of the GWPS. The maximum of the GPS in-dicates the position of the GWPS inflection point. Rotation period can then be foundby multiplying the inflection period by an empirically- constrained calibration fac-tor.Results. Through our analyzes it appeared that this method was strongly depen-dent on the stellar inclination. We decided to use this feature to feed a supervisedmachine learning algorithm to predict stellar inclination from the lightcurve.Conclusion. The GPS method is not reliable alone to measure surface rotation.However for inclination our results are promising. We retrieve almost 90% of sim-ulated stars inclination at ± 20°. For Kepler stars, we currently retrieve only 57.1%of their inclination compared to asteroseismic measurements. However the sampleused was only about 56 oscillating solar-like stars and their asteroseismic uncertain-ties are significant.</p>

corrected abstract:
<p><u>Context</u>. Over the last two decades, space-based photometric missions such as CoRot, Kepler/K2, and TESS have provided a wealth of photometric data of solar-like stars. Several methods have been used to extract the surface rotation period, using either the Lomb-Scargle periodogram, the autocorrelation function, wavelet transforms or Gaussian processes.</p><p><u>Aims</u>. We propose to evaluate the efficiency for measuring rotation period of a recently proposed method, called the Gradient Power Spectrum (GPS).</p><p><u>Methods</u>. The GPS is calculated in two steps. First, we calculate the global wavelet power spectrum (GWPS) from the time series. Then the gradient is calculated from the logarithmic derivative of the GWPS. The maximum of the GPS indicates the position of the GWPS inflection point. Rotation period can then be found by multiplying the inflection period by an empirically- constrained calibration factor.</p><p><u>Results</u>. Through our analyzes it appeared that this method was strongly dependent on the stellar inclination. We decided to use this feature to feed a supervised machine learning algorithm to predict stellar inclination from the lightcurve.</p><p><u>Conclusion</u>. The GPS method is not reliable alone to measure surface rotation.<br>However for inclination our results are promising. We retrieve almost 90% of simulated stars inclination at ± 20°. For Kepler stars, we currently retrieve only 57.1% of their inclination compared to asteroseismic measurements. However the sample used was only about 56 oscillating solar-like stars and their asteroseismic uncertainties are significant.</p>
----------------------------------------------------------------------
In diva2:1896340 abstract is:
<p>Proper orthogonal decomposition (POD) has been widely used to extract modes fromflow fields based on spatial and temporal correlations and ordered by energy, and canbe used to extract coherent structures in turbulence. Each POD time coefficient formsa time series where the samples are auto-correlated. Moreover, the time series ofdifferent coefficients are cross-correlated. Based on the above mentioned correlations,the present work develops a vector auto-regression (VAR) method to model thetime coefficients and predict them. Therefore, the flow field could be predicted byreconstruction using the original spatial modes and predicted time coefficients. As anexample, turbulent boundary layer flow over a flat plate Reθ = 790 is investigated outof the motivation of inflow generation. Prior to the modelling, the correlation betweenthe POD time coefficients is studied; energy transitions among the POD modes areinvestigated; and the modelling strategy is designed to capture such correlations. Asa result, the probability density function and correlations of POD time coefficients arepreserved. Moreover, dominant Reynolds stresses and power spectral density are alsoshown to be generally consistent with the original flow. Motivated by the findings, thepossibility of applying the method to higher Reynolds-numbers is also studied.</p>

corrected abstract:
<p>Proper orthogonal decomposition (POD) has been widely used to extract modes from flow fields based on spatial and temporal correlations and ordered by energy, and can be used to extract coherent structures in turbulence. Each POD time coefficient forms a time series where the samples are auto-correlated. Moreover, the time series of different coefficients are cross-correlated. Based on the above mentioned correlations, the present work develops a vector auto-regression (VAR) method to model the time coefficients and predict them. Therefore, the flow field could be predicted by reconstruction using the original spatial modes and predicted time coefficients. As an example, turbulent boundary layer flow over a flat plate Re<sub>θ</sub> = 790 is investigated out of the motivation of inflow generation. Prior to the modelling, the correlation between the POD time coefficients is studied; energy transitions among the POD modes are investigated; and the modelling strategy is designed to capture such correlations. As a result, the probability density function and correlations of POD time coefficients are preserved. Moreover, dominant Reynolds stresses and power spectral density are also shown to be generally consistent with the original flow. Motivated by the findings, the possibility of applying the method to higher Reynolds-numbers is also studied.</p>
----------------------------------------------------------------------
In diva2:1888131 abstract is:
<p>Two-phase flows occur in many industrial applications, including light-water nuclear reactors,where accurate modeling of interactions between the liquid and vapor phases is crucial for safetyand efficiency.In this project, a simplified, one-dimensional, two-fluid model, consisting of mass, energyand momentum conservation equations for the liquid and vapor phases each, is derived andimplemented. Due to its simplicity and ease of use, the model is suitable for educationalpurposes and early-stage research, particularly for model development based on separate effectstests with simple geometries.The implemented code solves the six field equations in combination with simplified, yet easilymodifiable, constitutive equations. The current model can account for thermal non-equilibria,such as sub-cooled and post-CHF boiling, as well as interfacial momentum coupling betweenthe liquid and vapor phases. The systems code TRACE predicts comparable results for verticalupward flow, despite the numerous assumptions made in the simplified model.</p>

corrected abstract:
<p>Two-phase flows occur in many industrial applications, including light-water nuclear reactors, where accurate modeling of interactions between the liquid and vapor phases is crucial for safety and efficiency.</p><p>In this project, a simplified, one-dimensional, two-fluid model, consisting of mass, energy and momentum conservation equations for the liquid and vapor phases each, is derived and implemented. Due to its simplicity and ease of use, the model is suitable for educational purposes and early-stage research, particularly for model development based on separate effects tests with simple geometries.</p><p>The implemented code solves the six field equations in combination with simplified, yet easily modifiable, constitutive equations. The current model can account for thermal non-equilibria, such as sub-cooled and post-CHF boiling, as well as interfacial momentum coupling between the liquid and vapor phases. The systems code TRACE predicts comparable results for vertical upward flow, despite the numerous assumptions made in the simplified model.</p>
----------------------------------------------------------------------
In diva2:1880347 abstract is:
<p>By placing millions of space sunshades, of the order of 104 m2 at the sub-Lagrangian point L1',between the sun and Earth, solar radiation can be reduced enough to achieve the necessary temper-ature reduction to enable a slow down of the global warming. The vast amount of space sunshadesposes significant challenges on the communication system, as the probability of interference, whichcan distort information, increases with the number of simultaneously communicating units.This thesis aims to design a potential structure for the communication system that minimizesinterference as much as possible. To reduce the number of simultaneously communicating units, thesunshades are arranged in cell formation, where a mother is placed in the center with daughtersaround that only communicate with their specific cell mother. Direct communication betweenthe Earth and space sunshades is not possible as the interference from solar radiation can causesignificant distortion on the signals. Therefore, relay satellites are placed in orbit around thesub-Lagrangian point L1' at a sufficient distance to avoid the effects of solar radiation. Thus, thecommunication between the mothers and Earth is instead routed via the relay satellites. Sincecommunication between such a large number of entities in space has not been investigated before,this approach could provide a possible basic design framework for designing such infrastructure inthe future.</p>

corrected abstract:
<p>By placing millions of space sunshades, of the order of 104 m2 at the sub-Lagrangian point L1', between the sun and Earth, solar radiation can be reduced enough to achieve the necessary temper-ature reduction to enable a slow down of the global warming. The vast amount of space sunshades poses significant challenges on the communication system, as the probability of interference, which can distort information, increases with the number of simultaneously communicating units. This thesis aims to design a potential structure for the communication system that minimizes interference as much as possible. To reduce the number of simultaneously communicating units, the sunshades are arranged in cell formation, where a mother is placed in the center with daughters around that only communicate with their specific cell mother. Direct communication between the Earth and space sunshades is not possible as the interference from solar radiation can cause significant distortion on the signals. Therefore, relay satellites are placed in orbit around the sub-Lagrangian point L1' at a sufficient distance to avoid the effects of solar radiation. Thus, the communication between the mothers and Earth is instead routed via the relay satellites. Since communication between such a large number of entities in space has not been investigated before, this approach could provide a possible basic design framework for designing such infrastructure inthe future.</p>
----------------------------------------------------------------------
In diva2:1878787 abstract is:
<p>Reinforcement learning has emerged as a powerful paradigm in machinelearning, witnessing remarkable progress in recent years. Amongreinforcement algorithms, Q-learning stands out, enabling agents tolearn quickly from past actions. This study aims to investigate andenhance Q-learning methodologies, with a specific focus on tabularQ-learning. In particular, it addresses Q-learning with an actionspace containing actions that require different amounts of time toexecute. With such an action space the algorithm might convergeto a suboptimal solution when using a constant discount factor sincediscounting occurs per action and not per time step. We refer to thisissue as the non-temporal discounting (NTD) problem. By introducinga time-normalised discounting function, we were able to address theissue of NTD. In addition, we were able to stabilise the solutionby implementing a cost for specific actions. As a result, the modelconverged to the expected solution. Building on these results it wouldbe wise to implement time-normalised discounting in a state-of-the-artreinforcement learning model such as deep Q-learning.</p><p> </p>

corrected abstract:
<p>Reinforcement learning has emerged as a powerful paradigm in machine learning, witnessing remarkable progress in recent years. Among reinforcement algorithms, Q-learning stands out, enabling agents to learn quickly from past actions. This study aims to investigate and enhance Q-learning methodologies, with a specific focus on tabular Q-learning. In particular, it addresses Q-learning with an action space containing actions that require different amounts of time to execute. With such an action space the algorithm might converge to a suboptimal solution when using a constant discount factor since discounting occurs per action and not per time step. We refer to this issue as the non-temporal discounting (NTD) problem. By introducing a time-normalised discounting function, we were able to address the issue of NTD. In addition, we were able to stabilise the solution by implementing a cost for specific actions. As a result, the model converged to the expected solution. Building on these results it would be wise to implement time-normalised discounting in a state-of-the-art reinforcement learning model such as deep Q-learning.</p>
----------------------------------------------------------------------
In diva2:1871461 abstract is:
<p>The aerospace sector’s increasing use of composites highlights the need for accuratecharacterization of the material’s properties, particularly regarding failure parameters.This paper goes through the process of physically testing a carbon epoxy composite interms of delamination strength using the ASTM standard in three different mode mixtures.Then, by implementing a Cohesive Zone Modelling approach with a bi-linear tractionseparation law, simulating the coupons in LS-Dyna and performing an iterative correlationprocess to find the material parameters that best replicate the results obtained from the labexperiments.The results showed how the failure load at which the delamination initiates matched theexperimental ones for reasonable values of the material properties, but the displacementat which this load were obtained are noticeably off.In the end, the reason behind this behaviour is discussed and possible solutions forfuture work are presented.</p>


corrected abstract:
<p>The aerospace sector’s increasing use of composites highlights the need for accurate characterization of the material’s properties, particularly regarding failure parameters.</p><p>This paper goes through the process of physically testing a carbon epoxy composite in terms of delamination strength using the ASTM standard in three different mode mixtures. Then, by implementing a Cohesive Zone Modelling approach with a bi-linear traction separation law, simulating the coupons in LS-Dyna and performing an iterative correlation process to find the material parameters that best replicate the results obtained from the lab experiments.</p><p>The results showed how the failure load at which the delamination initiates matched the experimental ones for reasonable values of the material properties, but the displacement at which this load were obtained are noticeably off.</p><p>In the end, the reason behind this behaviour is discussed and possible solutions for future work are presented.</p>
----------------------------------------------------------------------
In diva2:1833735 abstract is:
<p>This thesis is a case study in collaboration with SJ AB, a government owned railway companyin Sweden. The employees aboard the trains are an essential part of operating thetrains efficiently. Therefore, it is vital to forecast absences well in order to avoid havingto cancel train trips or having employees work over time. The current process SJ usesdivides the total amount of absences into 11 categories representing reasons for not beingpresent. This is done three months in advance, but the model is not based on mathematics.This study is going to examine how well the forecasts compare to reality in addition toinvestigating which variables are possible to estimate using regression analysis. Furthermore,the extent to which the staff on board the trains are affected will be investigatedin terms of having to work less overtime. The financial impact of an enhanced model willbe researched. “Free” days, Vacation and Sickness all have significant regressors and canpotentially be forecast using regression analysis. Future work includes finding more potentialregressor variables that could be significant for more response variables in addition tousing the results of this thesis in an actual estimation model for the total absence.</p>

corrected abstract:
<p>This thesis is a case study in collaboration with SJ AB, a government owned railway company in Sweden. The employees aboard the trains are an essential part of operating the trains efficiently. Therefore, it is vital to forecast absences well in order to avoid having to cancel train trips or having employees work over time. The current process SJ uses divides the total amount of absences into 11 categories representing reasons for not being present. This is done three months in advance, but the model is not based on mathematics. This study is going to examine how well the forecasts compare to reality in addition to investigating which variables are possible to estimate using regression analysis. Furthermore, the extent to which the staff on board the trains are affected will be investigated in terms of having to work less overtime. The financial impact of an enhanced model will be researched. “Free” days, Vacation and Sickness all have significant regressors and can potentially be forecast using regression analysis. Future work includes finding more potential regressor variables that could be significant for more response variables in addition to using the results of this thesis in an actual estimation model for the total absence.</p>
----------------------------------------------------------------------
In diva2:1830907 abstract is:
<p>The purpose of this study is to perform computational fluid dynamics (CFD) simulationfor an incompressible flow around an airfoil and investigate the aerodynamicproperties for different Reynolds numbers (400, 000 and 1, 000, 000) based on the chordlength, at the moderate angle of attack (AoA) of 5o . For this purpose a detailedevaluation of wall-resolved and wall-modelled large-eddy simulations (LES) wereconducted for the NACA4412 wing profile. The simulations were carried out with theopen-source CFD software OpenFOAM. The meshing of the computational domainwas constructed with the commercial software (student version) Ansys Icem while forthe post-processing of the different cases, a python package turbulucid was utilized.The results show good agreement compared to data and the aerodynamic properties ofthe flow captured accurately. However, for the case of the wall-modelled LES furtherinvestigation is required and is proposed for future campaigns.</p>

corrected abstract:
<p>The purpose of this study is to perform computational fluid dynamics (CFD) simulation for an incompressible flow around an airfoil and investigate the aerodynamic properties for different Reynolds numbers (400,000 and 1,000,000) based on the chord length, at the moderate angle of attack (AoA) of 5&deg;. For this purpose a detailed evaluation of wall-resolved and wall-modelled large-eddy simulations (LES) were conducted for the NACA4412 wing profile. The simulations were carried out with the open-source CFD software OpenFOAM. The meshing of the computational domain was constructed with the commercial software (student version) Ansys Icem while for the post-processing of the different cases, a python package turbulucid was utilized. The results show good agreement compared to data and the aerodynamic properties of the flow captured accurately. However, for the case of the wall-modelled LES further investigation is required and is proposed for future campaigns.</p>
----------------------------------------------------------------------
In diva2:1817109 - Note: no full text in DiVA
abstract is:
<p>A comparison between several numerical finite element methods to model progressive failurein composite materials is performed. The focus of the study is on the development of an efficientmethod to model crack growth phenomena in large-scale thermoplastic composite laminates.The analyses are performed in the commercial software Abaqus and include three discrete crackmethods and one smeared crack method, which are applied to model coupon tensile tests. Thethree discrete crack methods involve cohesive elements, cohesive contact properties, and theextended finite element method. The smeared crack method uses the Hashin damage initiationcriterion as already implemented in the software. The numerical results were compared bothto numerical and experimental references. A method to represent multiple failure modesin a single fracture surface is used both with cohesive elements and with a cohesive contactproperty. The cohesive contact method results in being the most efficient. To achieve a moreaccurate prediction of the damage evolution and of the final failure, a method using multiplesurfaces of cohesive elements is tested. A technique to scale the mesh by reducing the initiationstrength is evaluated. Both these last two studies show that more in-depth work is required tosuccessfully apply these methods.</p>

corrected abstract:
<p>A comparison between several numerical finite element methods to model progressive failure in composite materials is performed. The focus of the study is on the development of an efficient method to model crack growth phenomena in large-scale thermoplastic composite laminates. The analyses are performed in the commercial software Abaqus and include three discrete crack methods and one smeared crack method, which are applied to model coupon tensile tests. The three discrete crack methods involve cohesive elements, cohesive contact properties, and the extended finite element method. The smeared crack method uses the Hashin damage initiation criterion as already implemented in the software. The numerical results were compared both to numerical and experimental references. A method to represent multiple failure modes in a single fracture surface is used both with cohesive elements and with a cohesive contact property. The cohesive contact method results in being the most efficient. To achieve a more accurate prediction of the damage evolution and of the final failure, a method using multiple surfaces of cohesive elements is tested. A technique to scale the mesh by reducing the initiation strength is evaluated. Both these last two studies show that more in-depth work is required to successfully apply these methods.</p>
----------------------------------------------------------------------
In diva2:1781274 abstract is:
<p>This project aimed to evaluate the effectiveness of the Auto Regressive Exogenous(ARX) model in forecasting stock prices and contribute to research on statisticalmodels in predicting stock prices. An ARX model is a type of linear regression modelused in time series analysis to forecast future values based on past values and externalinput signals. In this study, the ARX model was used to forecast the closing pricesof stocks listed on the OMX Stockholm 30 (OMXS30*) excluding Essity, Evolution,and Sinch, using historical data from 2016-01-01 to 2020-01-01 obtained from YahooFinance.</p><p>The model was trained using the least squares approach with a control signal that filtersoutliers in the data. This was done by modeling the ARX model using optimizationtheory and then solving that optimization problem using Gurobi OptimizationSoftware. Subsequently, the accuracy of the model was tested by predicting prices in aperiod based on past values and the exogenous input variable.</p><p>The results indicated that the ARX model was not suitable for predicting stock priceswhile considering short time periods.</p><p> </p>

corrected abstract:
<p>This project aimed to evaluate the effectiveness of the Auto Regressive Exogenous (ARX) model in forecasting stock prices and contribute to research on statistical models in predicting stock prices. An ARX model is a type of linear regression model used in time series analysis to forecast future values based on past values and external input signals. In this study, the ARX model was used to forecast the closing prices of stocks listed on the OMX Stockholm 30 (OMXS30*) excluding Essity, Evolution, and Sinch, using historical data from 2016-01-01 to 2020-01-01 obtained from Yahoo Finance.</p><p>The model was trained using the least squares approach with a control signal that filters outliers in the data. This was done by modeling the ARX model using optimization theory and then solving that optimization problem using Gurobi Optimization Software. Subsequently, the accuracy of the model was tested by predicting prices in a period based on past values and the exogenous input variable.</p><p>The results indicated that the ARX model was not suitable for predicting stock prices while considering short time periods.</p>
----------------------------------------------------------------------
In diva2:1779355 abstract is:
<p>The main purpose for an aggressive investor is to maximize the return in theinvestments. But in order to do so the risk should be taken into consideration.In this thesis, we utilize Markowitz portfolio theory, one of the standard modelsfor maximizing return while considering risk. The model allows the investorto balance risk tolerance and expected return on the stock market based onhistorical data. Simply put, the goal is to allocate capital to stocks in a mannerthat maximizes expected return while considering risk. The tested cases involvedmodels utilizing historical data from five and ten years ago, respectively. Theresulting allocation distribution of these portfolios depended on the varianceconstraint and, to a large extent, on how many years of historical data were used.These results emphasize the significance of data in portfolio optimization.</p>

corrected abstract:
<p>The main purpose for an aggressive investor is to maximize the return in the investments. But in order to do so the risk should be taken into consideration. In this thesis, we utilize Markowitz portfolio theory, one of the standard models for maximizing return while considering risk. The model allows the investor to balance risk tolerance and expected return on the stock market based on historical data. Simply put, the goal is to allocate capital to stocks in a manner that maximizes expected return while considering risk. The tested cases involved models utilizing historical data from five and ten years ago, respectively. The resulting allocation distribution of these portfolios depended on the variance constraint and, to a large extent, on how many years of historical data were used. These results emphasize the significance of data in portfolio optimization.</p>
----------------------------------------------------------------------
In diva2:1779326 abstract is: <p>This report discusses the accuracy of blink detection in eye tracking, using machine learningalgorithms. Blink detection is used in a wide variety of medicinal and psychological applica-tions such as a controller for motor impaired individuals. Image classification has recentlybeen used in eye tracking and blink detection applications. The blink detection is appliedon data captured from the Pupil Invisible head-mounted eye tracker. The aim is that givenan image, the classifier can accurately determine the state of which the eye is in, blink oropen.These tests will be conducted on two SVM (support vector machine) models using differenttraining data, one trained on data from controlled environments, the other model also trainedon uncontrolled environments. For this project, data was captured in infrared disturbedenvironments to see how it affects the models performance. These models are evaluatedaccording to their accuracy using multiple different metrics. This rapport will discuss theresults of both classifiers in both tests, in addition to describing training methodology withan aim to find if blink detection is viable in infrared disturbed environments.</p>

corrected abstract:
<p>This report discusses the accuracy of blink detection in eye tracking, using machine learning algorithms. Blink detection is used in a wide variety of medicinal and psychological applications such as a controller for motor impaired individuals. Image classification has recently been used in eye tracking and blink detection applications. The blink detection is applied on data captured from the Pupil Invisible head-mounted eye tracker. The aim is that given an image, the classifier can accurately determine the state of which the eye is in, blink or open.</p><p>These tests will be conducted on two SVM (support vector machine) models using different training data, one trained on data from controlled environments, the other model also trained on uncontrolled environments. For this project, data was captured in infrared disturbed environments to see how it affects the models performance. These models are evaluated according to their accuracy using multiple different metrics. This rapport will discuss the results of both classifiers in both tests, in addition to describing training methodology with an aim to find if blink detection is viable in infrared disturbed environments.</p>
----------------------------------------------------------------------
In diva2:1776549 abstract is:
<p>Computed tomography (CT) is a medical imaging technique that usesX-rays to obtain a reconstruction of an object. The term acquisition ge-ometry refers to the arrangement of imaging sensors and the X-ray sourceas well as the procedure used for data collection. The quality of the re-construction is often limited by the acquisition geometry and parametervalues. In this thesis, we present a procedure for fine-tuning acquisitiongeometry parameters in CT by minimizing the difference between theforward projection of a known phantom and measured data, i.e. data dis-crepancies. We extend the ODL library in Python and create acquisitiongeometries where different parameters have been distorted. We utilizegradient descent in an attempt recover the true parameters of the acquisi-tion geometries. Our results show that the recovery of the true geometryis successful when one or, in some cases, two parameters are perturbed.The objective function becomes very sensitive when more parameters areperturbed, requiring a low learning rate and making convergence slow.Nevertheless, we are able to minimize the objective function in the for-ward projection for all perturbations. Although our algorithm performswell in some aspects relating to parameter recovery, there is potential forfurther research by implementing other optimization methods.</p>

corrected abstract:
<p>Computed tomography (CT) is a medical imaging technique that uses X-rays to obtain a reconstruction of an object. The term acquisition geometry refers to the arrangement of imaging sensors and the X-ray source as well as the procedure used for data collection. The quality of the reconstruction is often limited by the acquisition geometry and parameter values. In this thesis, we present a procedure for fine-tuning acquisition geometry parameters in CT by minimizing the difference between the forward projection of a known phantom and measured data, i.e. data discrepancies. We extend the ODL library in Python and create acquisition geometries where different parameters have been distorted. We utilize gradient descent in an attempt recover the true parameters of the acquisition geometries. Our results show that the recovery of the true geometry is successful when one or, in some cases, two parameters are perturbed. The objective function becomes very sensitive when more parameters are perturbed, requiring a low learning rate and making convergence slow. Nevertheless, we are able to minimize the objective function in the forward projection for all perturbations. Although our algorithm performs well in some aspects relating to parameter recovery, there is potential for further research by implementing other optimization methods.</p>
----------------------------------------------------------------------
In diva2:1768469 abstract is:
<p>Does the active membrane transporter, Na+,K+-ATPase dimerize? If it does, whatis the functional benefit? Does it increase or decrease the turnover rate? Theseare still unanswered questions and current research topics. Previous studies havedemonstrated dimerizations in closely related proteins of the P-type ATPase family.For the Na+, K+-ATPase a first indication of dimerization has been shown viaFluorescence lifetime imaging microscopy (FLIM) or Fluorescence resonance energytransfer - Fluorescence correlation spectroscopy (FRET-FCS) experiments. Theprecise dimer structure, dimerization process, and its ultimate functional effecthowever, remain to be found. This master thesis approaches those questions froma co-evolutionary standpoint. It predicts a possible dimer structure by starting with amultiple sequence alignment, direct coupling analysis, and structural contact filteringalgorithm. This model would strengthen the dimerization model of a decreasedturnover rate due to a competitive behavior of two Na+, K+-ATPases for its energysource ATP.</p>

corrected abstract:
<p>Does the active membrane transporter, Na<sup>+</sup>,K<sup>+</sup>-ATPase dimerize? If it does, what is the functional benefit? Does it increase or decrease the turnover rate? These are still unanswered questions and current research topics. Previous studies have demonstrated dimerizations in closely related proteins of the P-type ATPase family. For the Na<sup>+</sup>, K<sup>+</sup>-ATPase a first indication of dimerization has been shown via Fluorescence lifetime imaging microscopy (FLIM) or Fluorescence resonance energy transfer - Fluorescence correlation spectroscopy (FRET-FCS) experiments. The precise dimer structure, dimerization process, and its ultimate functional effect however, remain to be found. This master thesis approaches those questions from a co-evolutionary standpoint. It predicts a possible dimer structure by starting with a multiple sequence alignment, direct coupling analysis, and structural contact filtering algorithm. This model would strengthen the dimerization model of a decreased turnover rate due to a competitive behavior of two Na<sup>+</sup>, K<sup>+</sup>-ATPases for its energy source ATP.</p>
----------------------------------------------------------------------
In diva2:1739683 abstract is:
<p>An aircraft is exposed for a certain risk during warhead deployment. If the distance is too short between the aircraftand warhead during burst, fragments may hit the aircraft with fatal consequences. The warhead consists not onlyof fragments created during burst but also parts to attach it to the aircraft such as mounting plates and fasteningloops. These parts have a significantly larger mass than natural fragments and may travel far during trajectory.The problem for a potential hit of fragment on a fighter jet after its warhead has detonated has been presentsince several decades. It is of interest to analyse the problem to effectively reduce the warheads arming time andthe aircraft’s altitude during warhead deployment. The complexity consists of how the mounting plates and fas-tening loops behave during trajectory, which may affect the travelled distance if they rotate or tumble. Attemptsto solve this problem for the Gripen fighter jet has been made by Staffan Harling at FOI which this thesis is asubsequent work.</p><p>This thesis treats a risk perspective analyse of the distance between the aircraft and warhead named range safetydistance. Travelled distances for fragments are calculated with variation in velocity, drag coefficient and ejectionangle to analyse the problem to a wider extent.</p><p>The conclusion states that the time from warhead deployed until it burst should be at least seven seconds. Genericdata has been used in this master thesis due to classified information concerning real cases. Focus has been onthe method and to develop a Matlab code that hopefully can be used to estimate range safety distances from aappropriate risk perspective.</p>

corrected abstract:
<p>An aircraft is exposed for a certain risk during warhead deployment. If the distance is too short between the aircraft and warhead during burst, fragments may hit the aircraft with fatal consequences. The warhead consists not only of fragments created during burst but also parts to attach it to the aircraft such as mounting plates and fastening loops. These parts have a significantly larger mass than natural fragments and may travel far during trajectory.</p><p>The problem for a potential hit of fragment on a fighter jet after its warhead has detonated has been present since several decades. It is of interest to analyse the problem to effectively reduce the warheads arming time and the aircraft’s altitude during warhead deployment. The complexity consists of how the mounting plates and fastening loops behave during trajectory, which may affect the travelled distance if they rotate or tumble. Attempts to solve this problem for the Gripen fighter jet has been made by Staffan Harling at FOI which this thesis is a subsequent work.</p><p>This thesis treats a risk perspective analyse of the distance between the aircraft and warhead named range safety distance. Travelled distances for fragments are calculated with variation in velocity, drag coefficient and ejection angle to analyse the problem to a wider extent.</p><p>The conclusion states that the time from warhead deployed until it burst should be at least seven seconds. Generic data has been used in this master thesis due to classified information concerning real cases. Focus has been on the method and to develop a Matlab code that hopefully can be used to estimate range safety distances from a appropriate risk perspective.</p>
----------------------------------------------------------------------
In diva2:1678918 abstract is:
<p>The study of the instabilities in boiling water reactors is of significant importance to the safety withwhich they can be operated, as they can cause damage to the reactor posing risks to both equipmentand personnel. The instabilities that concern this paper are progressive growths in the oscillatingpower of boiling-water reactors. As thermal power is oscillatory is important to be able to identifywhether or not the power amplitude is stable.</p><p>The main focus of this paper has been the development of a neural network estimator of these insta-bilities, fitting a non-linear model function to data by estimating it’s parameters. In doing this, theambition was to optimize the networks to the point that it can deliver near ”best-guess” estimationsof the parameters which define these instabilities, evaluating the usefulness of these networks whenapplied to problems like this.</p><p>The goal was to design both MLP(Multi-Layer Perceptron) and SVR/KRR(Support Vector Regres-sion/Kernel Rigde Regression) networks and improve them to the point that they provide reliableand useful information about the waves in question. This goal was accomplished only in part asthe SVR/KRR networks proved to have some difficulty in ascertaining the phase shift of the waves.Overall, however, these networks prove very useful in this kind of task, succeeding with a reasonabledegree of confidence to calculating the different parameters of the waves studied.</p><p> </p>

corrected abstract:
<p>The study of the instabilities in boiling water reactors is of significant importance to the safety with which they can be operated, as they can cause damage to the reactor posing risks to both equipment and personnel. The instabilities that concern this paper are progressive growths in the oscillating power of boiling-water reactors. As thermal power is oscillatory is important to be able to identify whether or not the power amplitude is stable.</p><p>The main focus of this paper has been the development of a neural network estimator of these instabilities, fitting a non-linear model function to data by estimating it’s parameters. In doing this, the ambition was to optimize the networks to the point that it can deliver near ”best-guess” estimations of the parameters which define these instabilities, evaluating the usefulness of these networks when applied to problems like this.</p><p>The goal was to design both MLP(Multi-Layer Perceptron) and SVR/KRR(Support Vector Regression/Kernel Rigde Regression) networks and improve them to the point that they provide reliable and useful information about the waves in question. This goal was accomplished only in part as the SVR/KRR networks proved to have some difficulty in ascertaining the phase shift of the waves. Overall, however, these networks prove very useful in this kind of task, succeeding with a reasonable degree of confidence to calculating the different parameters of the waves studied.</p>

Note that there are no space before the opening left parenthesis for the expanded acronyms. Note also that "Rigde" should be spelled "Ridge" (as it is in the body of the thesis); however, it is misspelled in the original thesis abstracts (both the English and Swedish abstracts). Additionally, "it's" is incorrect, and should be "its" - as the latter is the posive form; however, there error is in the original abstract.
----------------------------------------------------------------------
In diva2:1678908 abstract is:
<p>Gamma-ray bursts (GRBs) are the most luminous phenomena in the Universe, explosions whoseenergy is generated by supernovae or mergers of dense objects such as neutron stars. The GRBemission is divided into the prompt emission phase characterized by γ-ray radiation and the afterglowof lower energy radiation. The prompt emission phase is still not understood; as of now, there aretwo leading descriptions: the photospheric- and the synchrotron models. The synchrotron model hashad great success in describing GRB spectra, and specifically some of the brightest ones, although notwithout issues such as some observations being at odds with theory. On the other hand, photosphericmodels have had problems too of how to broaden the spectrum in order to explain the observeddata. One explanation for this broadening is that Radiation Mediated Shocks (RMSs) dissipate energybelow the photosphere. In this report, a time resolved spectral analysis of the prompt emission of GRB160625B – a very bright GRB known to produce synchrotron-like emission – is done. Komrad is animplementation of the Kompaneets RMS Approximation (KRA), which is a dissipative photosphericmodel. Komrad is then used to fit a photospheric model to the prompt emission of GRB 160625Bin order to explore whether photospheric models can account for synchrotron-like emission spectra.Great statistical support is found for the photospheric model in comparison to standard GRB fittingfunctions as well as a synchrotron function which is indicative of the photospheric model being able toexplain a synchrotron-like spectra.</p>

corrected abstract:
<p>Gamma-ray bursts (GRBs) are the most luminous phenomena in the Universe, explosions whose energy is generated by supernovae or mergers of dense objects such as neutron stars. The GRB emission is divided into the prompt emission phase characterized by γ-ray radiation and the afterglow of lower energy radiation. The prompt emission phase is still not understood; as of now, there are two leading descriptions: the photospheric- and the synchrotron models. The synchrotron model has had great success in describing GRB spectra, and specifically some of the brightest ones, although not without issues such as some observations being at odds with theory. On the other hand, photospheric models have had problems too of how to broaden the spectrum in order to explain the observed data. One explanation for this broadening is that Radiation Mediated Shocks (RMSs) dissipate energy below the photosphere. In this report, a time resolved spectral analysis of the prompt emission of GRB 160625B – a very bright GRB known to produce synchrotron-like emission – is done. Komrad is an implementation of the Kompaneets RMS Approximation (KRA), which is a dissipative photospheric model. <tt>Komrad</tt> is then used to fit a photospheric model to the prompt emission of GRB 160625B in order to explore whether photospheric models can account for synchrotron-like emission spectra. Great statistical support is found for the photospheric model in comparison to standard GRB fitting functions as well as a synchrotron function which is indicative of the photospheric model being able to explain a synchrotron-like spectra.</p>
----------------------------------------------------------------------
In diva2:1678462 abstract is:
<p>Cavitation has been known for a long time to cause damages in spillways athydropower plants, aerators are therefore often implemented to prevent this.The majority of the hydropower plants in Sweden are installed in the northernpart of the country. In this environment, construction details like vents areexposed to rain, snow, leaves, and other difficult nature conditions for a majorpart of the year. It is therefore of interest to see what will happen if the ventsare sealed.Similar experiments have earlier been investigated at Vattenfall AB, oneof the largest energy companies in Sweden. The investigation consists ofcase studies with multiple variables, closed and open vents, low and highwater levels. The calculations are done with help from computational fluiddynamics and the goal is to see how parameters such as pressure, spreading,and horizontal length change when the aerator is sealed compared to open.The calculations were also carried out in a way so tests in the future can bedone by Vattenfall to validate the results from the computational simulations.The geometry of the hydropower plant is taken from a plant in Skellefteälvenin northern Sweden.</p>

corrected abstract:
<p>Cavitation has been known for a long time to cause damages in spillways at hydropower plants, aerators are therefore often implemented to prevent this. The majority of the hydropower plants in Sweden are installed in the northern part of the country. In this environment, construction details like vents are exposed to rain, snow, leaves, and other difficult nature conditions for a major part of the year. It is therefore of interest to see what will happen if the vents are sealed.</p><p>Similar experiments have earlier been investigated at Vattenfall AB, one of the largest energy companies in Sweden. The investigation consists of case studies with multiple variables, closed and open vents, low and high water levels. The calculations are done with help from computational fluid dynamics and the goal is to see how parameters such as pressure, spreading, and horizontal length change when the aerator is sealed compared to open. The calculations were also carried out in a way so tests in the future can be done by Vattenfall to validate the results from the computational simulations. The geometry of the hydropower plant is taken from a plant in Skellefteälven in northern Sweden.</p>
----------------------------------------------------------------------
In diva2:1548349 abstract is:
<p>Computational Fluid Dynamics is a powerful and widely used tool for developing projectsthat concern flow motion, in very different fields. Industrial CFD solvers are continuouslydeveloped with the aim of improving accuracy and reducing the computational cost of thesimulations. Turbulent wall-flow cases are particular demanding as the presence of a solidsurfaceinterface generates steep gradients in the proximity of the wall. Resolving suchgradients can be crucial to obtain a consistent solution but also very expensive in terms ofgrid refinement, and hence computational time. Wall functions are widely used and offersignificant computational savings when it comes to near-wall flow resolution. Previous wallfunction implemented in the M-Edge solver suffered by poor performances in complex flowscharacterized by strong pressure-gradient phenomena, such as separation. A new formulationhas been developed and validated for k − omega and Spalart-Allmaras turbulence models. Testsimulations started from simple and near-ideal cases (2D zero pressure gradient flat plate)and advanced to always more complex flow cases and geometries (full 3D general fighter).Every case has been run coupling the wall-function boundary condition with three differentturbulence models: the Menter SST, the Menter BSL with an EARSM and the Spalart-Allmaras one-equation model. Overall results showed the upgraded performance of new wallfunction in flow resolution together with more agile grid requirements, faster and deeperconvergence of the residuals and a general reduction in computational time.</p>

corrected abstract:
<p>Computational Fluid Dynamics is a powerful and widely used tool for developing projects that concern flow motion, in very different fields. Industrial CFD solvers are continuously developed with the aim of improving accuracy and reducing the computational cost of the simulations. Turbulent wall-flow cases are particular demanding as the presence of a solid surface interface generates steep gradients in the proximity of the wall. Resolving such gradients can be crucial to obtain a consistent solution but also very expensive in terms of grid refinement, and hence computational time. Wall functions are widely used and offer significant computational savings when it comes to near-wall flow resolution. Previous wall function implemented in the M-Edge solver suffered by poor performances in complex flows characterized by strong pressure-gradient phenomena, such as separation. A new formulation has been developed and validated for <em>k − &omega;</em> and Spalart-Allmaras turbulence models. Test simulations started from simple and near-ideal cases (2D zero pressure gradient flat plate) and advanced to always more complex flow cases and geometries (full 3D general fighter). Every case has been run coupling the wall-function boundary condition with three different turbulence models: the Menter SST, the Menter BSL with an EARSM and the Spalart-Allmaras one-equation model. Overall results showed the upgraded performance of new wall function in flow resolution together with more agile grid requirements, faster and deeper convergence of the residuals and a general reduction in computational time.</p>
----------------------------------------------------------------------
In diva2:1546615 abstract is:
<p>This thesis work is part of a design process which aims to develop a four-seathybrid-electric aircraft at Smartflyer (Grenchen, Switzerland). In that scope,various mechanisms of the plane had to be developed, including the systemactuating the control surfaces. The objective of this thesis work is to designthe primary flight controls which will be implemented in the first prototypebuilt at Smartflyer.Firstly, the work investigates the calculation of the aerodynamic loads appliedto the control surfaces through the use of three different methods which areanalytical calculations, VLM analysis and CFD simulation. Then, the workconsists in defining the kinematic mechanisms of the flight control to handlethe deflection of the horizontal stabiliser, the ailerons and the rudder. Lastly,the calculation of the forces to which are submitted the components of theflight control is conducted. This step allows to determine the pilot controlforces and ensures to take into account the ergonomic aspect during the designphase. The results of this work highlight the limits of the different methodsused and serves as a basis for a future sizing work and detailed conception.</p>

corrected abstract:
<p>This thesis work is part of a design process which aims to develop a four-seat hybrid-electric aircraft at Smartflyer (Grenchen, Switzerland). In that scope, various mechanisms of the plane had to be developed, including the system actuating the control surfaces. The objective of this thesis work is to design the primary flight controls which will be implemented in the first prototype built at Smartflyer.</p><p>Firstly, the work investigates the calculation of the aerodynamic loads applied to the control surfaces through the use of three different methods which are analytical calculations, VLM analysis and CFD simulation. Then, the work consists in defining the kinematic mechanisms of the flight control to handle the deflection of the horizontal stabiliser, the ailerons and the rudder. Lastly, the calculation of the forces to which are submitted the components of the flight control is conducted. This step allows to determine the pilot control forces and ensures to take into account the ergonomic aspect during the design phase. The results of this work highlight the limits of the different methods used and serves as a basis for a future sizing work and detailed conception.</p>
----------------------------------------------------------------------
In diva2:1528143 abstract is:
<p>Climate change if a fact and the responsibility comes down to the footprintowner. With a possible limitation on carbon oxide emissions and climate demandsfrom customers, a change in the shipping industry is evident. Asolutionto this could be to use a renewable energy created by the nature, the wind, tomake the change over to a climate friendly shipping industry. This study aimsto find a wind powered solution for a car carriers propulsion. What is the mostsuitable rig design?Based on a review of the existing solutions for wind population this thesiswill use a conceptual design method to generate rig designs. The result of thisthesis is two concepts who stuck to be interesting throughout the thesis periodand a catalog of concepts that could be used for further work of finding theoptimal wing solution.</p>

corrected abstract:
<p>Climate change if a fact and the responsibility comes down to the footprint owner. With a possible limitation on carbon oxide emissions and climate demands from customers, a change in the shipping industry is evident. A solution to this could be to use a renewable energy created by the nature, the wind, to make the change over to a climate friendly shipping industry. This study aims to find a wind powered solution for a car carriers propulsion. What is the most suitable rig design?</p><p>Based on a review of the existing solutions for wind population this thesis will use a conceptual design method to generate rig designs. The result of this thesis is two concepts who stuck to be interesting throughout the thesis period and a catalog of concepts that could be used for further work of finding the optimal wing solution.</p>
----------------------------------------------------------------------
In diva2:1464101 abstract is:
<p>In recent years it has been discovered that the window installations on the Visby class corvette are prone to break. The aim of this report is to perform a structural analysis of three different windowinstallations. Thus, to find which window installation minimizes the risk of leaks and cracks in the attachment connecting the window glass and the hull. The three window installations consist of theexisting window installation and one of SAAB Kockums alternative solutions installed in two different ways, with and without an additional damping mass. The model that will be used in the simulationswill consist of a rectangular shaped carbon fibre composite sandwich plate that represents the side structure of the maneuver deck. Furthermore, it will include three windows where only the middlepositioned window will contain the given installation details in order to reduce the complexity of both the modelling and the simulations. Three load cases were simulated in ANSYS Workbench. Thefirst load case called "hogging" consisted of a bending moment along the vertical sides of the model. The second load case called "slamming" consisted of a vertical force pointing upwards along thebottom of the model. Lastly, a torsion load case was simulated. In all load cases the existing window installation were subjected to the largest strains along the inner edges of the attachment. In theslamming load case, the alternative solution without the double damping mass was exposed to least strains around the inner edges of attachment compared to the other window installations. For thehogging and the torsion load case the alternative solution with double damping mass produced least strains around the inner edges of the attachment. But the alternative solution without the doubledamping mass was also able to reduce the strains around the inner edges compared to the existing window installation. In conclusion, the alternative solution without an additional damping mass isin overall minimizing the strains along the inner edges of the attachment in which the most leaks and cracks have been observed. It has especially been efficient in reducing strains in the slammingload case. Even though the window installation with an additional damping mass best withstands hogging and torsion, the slamming load case is the most common scenario. Therefore, the windowinstallation that best withstands the slamming load case should be prioritized. Thus, the alternative window installation without the additional damping mass is the best alternative because it bestwithstands slamming but also reduces the strains along the inner edges compared to the existing window installation in the other load cases.</p>

corrected abstract:
<p>In recent years it has been discovered that the window installations on the Visby class corvette are prone to break. The aim of this report is to perform a structural analysis of three different window installations. Thus, to find which window installation minimizes the risk of leaks and cracks in the attachment connecting the window glass and the hull. The three window installations consist of the existing window installation and one of SAAB Kockums alternative solutions installed in two different ways, with and without an additional damping mass. The model that will be used in the simulations will consist of a rectangular shaped carbon fibre composite sandwich plate that represents the side structure of the maneuver deck. Furthermore, it will include three windows where only the middle positioned window will contain the given installation details in order to reduce the complexity of both the modelling and the simulations. Three load cases were simulated in ANSYS Workbench. The first load case called "hogging" consisted of a bending moment along the vertical sides of the model. The second load case called "slamming" consisted of a vertical force pointing upwards along the bottom of the model. Lastly, a torsion load case was simulated. In all load cases the existing window installation were subjected to the largest strains along the inner edges of the attachment. In the slamming load case, the alternative solution without the double damping mass was exposed to least strains around the inner edges of attachment compared to the other window installations. For the hogging and the torsion load case the alternative solution with double damping mass produced least strains around the inner edges of the attachment. But the alternative solution without the double damping mass was also able to reduce the strains around the inner edges compared to the existing window installation. In conclusion, the alternative solution without an additional damping mass is in overall minimizing the strains along the inner edges of the attachment in which the most leaks and cracks have been observed. It has especially been efficient in reducing strains in the slamming load case. Even though the window installation with an additional damping mass best withstands hogging and torsion, the slamming load case is the most common scenario. Therefore, the window installation that best withstands the slamming load case should be prioritized. Thus, the alternative window installation without the additional damping mass is the best alternative because it best withstands slamming but also reduces the strains along the inner edges compared to the existing window installation in the other load cases.</p>
----------------------------------------------------------------------
In diva2:1334273 abstract is:
<p>Additive Manufacturing (AM), also known as 3D-printing, is the process of joining materials layer by layerfrom a 3D-model data and has several advantages over traditional manufacturing techniques. AM is destinedto change the way products are designed and manufactured in the future. In recent years, the process hasrapidly gained interest in all industry segments due to its ability to create customized and complex geometriesfor no added costs. This study focuses on a rather unexplored area of application of AM, namely of a vehiclecomponent that traditionally is manufactured with conventional manufacturing methods. The purpose ofthis study is to investigate the potential of AM of a PTO-shaft used in Scania's trucks. With the help oftopology optimization and a developed cost estimation model, dierent design cases are compared to eachother. Three areas are investigated: design, mechanical performance, and cost. The study found that adesign with roughly 25 % weight reduction is realizable, and would today cost about 15 times the cost forseries production using traditional manufacturing methods. This have clearly suggested that the PTO-shaftis not suitable for AM. However, by forecasting the cost into the future, the study found that printing thePTO-shaft are likely to be cost eective in terms of prototype production in the future, with up to about200 e in cost savings per part.</p>

corrected abstract:
<p>Additive Manufacturing (AM), also known as 3D-printing, is the process of joining materials layer by layer from a 3D-model data and has several advantages over traditional manufacturing techniques. AM is destined to change the way products are designed and manufactured in the future. In recent years, the process has rapidly gained interest in all industry segments due to its ability to create customized and complex geometries for no added costs. This study focuses on a rather unexplored area of application of AM, namely of a vehicle component that traditionally is manufactured with conventional manufacturing methods. The purpose of this study is to investigate the potential of AM of a PTO-shaft used in Scania's trucks. With the help of topology optimization and a developed cost estimation model, different design cases are compared to each other. Three areas are investigated: design, mechanical performance, and cost. The study found that a design with roughly 25 % weight reduction is realizable, and would today cost about 15 times the cost for series production using traditional manufacturing methods. This have clearly suggested that the PTO-shaft is not suitable for AM. However, by forecasting the cost into the future, the study found that printing the PTO-shaft are likely to be cost effective in terms of prototype production in the future, with up to about 200 € in cost savings per part.</p>
----------------------------------------------------------------------
In diva2:1229892 abstract is:
<p>Lung cancer is one of the most vicious commonplace diseases around. The only wayto efficiently combat lung cancer is to detect tumors early using Computed Tomography(CT). In this procedure, a preliminary scan spots any suspicious tumors and then, aftera significant wait, a follow-up scan is taken to confirm whether the nodule is in fact atumor. Currently a new generation of CT scanners, called Spectral Photon CountingCT scanners, is being developed. These CT scanners offer a variety of improvementsincluding the possibility of reducing the pixel size in detectors. The purpose of this workis to investigate, through computational modeling, if a smaller pixel size combined witha smaller source size would improve the ability to determine lung tumor growth, and assuch reduce the wait time between visits for patients. The results showed that it waspossible to reduce the pixel size without significantly increasing the noise of the imageby using image processing. However, the smaller pixels did not improve the ability todetect tumor growth.</p>

corrected abstract:
<p>Lung cancer is one of the most vicious commonplace diseases around. The only way to efficiently combat lung cancer is to detect tumors early using Computed Tomography(CT). In this procedure, a preliminary scan spots any suspicious tumors and then, after a significant wait, a follow-up scan is taken to confirm whether the nodule is in fact a tumor. Currently a new generation of CT scanners, called Spectral Photon Counting CT scanners, is being developed. These CT scanners offer a variety of improvements including the possibility of reducing the pixel size in detectors. The purpose of this work is to investigate, through computational modeling, if a smaller pixel size combined with a smaller source size would improve the ability to determine lung tumor growth, and as such reduce the wait time between visits for patients. The results showed that it was possible to reduce the pixel size without significantly increasing the noise of the image by using image processing. However, the smaller pixels did not improve the ability to detect tumor growth.</p>
----------------------------------------------------------------------
In diva2:1229885 - Note: no full text in DiVA

abstract is:
<p>In this study, convolutional neural networks (CNNs) are used to classify simulatedimages of ultra high energy cosmic rays (UHECRs). The JEM-EUSO program aimsto place an imaging instrument on board the International Space Station that willdetect UHECRs in Earth’s atmosphere. The instrument will include software andhardware for analyzing the images. There are several different techniques for imageanalysis that could be used in the instrument.Different CNNs are implemented and tested on simulated data. The best-performingnetwork is further analyzed. Results show that CNNs can be used to identify cosmicrays in images. The accuracy is found to be highly dependent on the brightness ofthe cosmic ray tracks compared to the background photon noise. It is also shown thatthe networks are able to identify the angle of the cosmic rays relative to the imageframe. Accuracy is found to decrease with increasing angle resolution, with accuraciesranging from 100% down to pure guesses. More extensive studies with more complexsimulations could generate finer results.</p>

corrected abstract:
<p>In this study, convolutional neural networks (CNNs) are used to classify simulated images of ultra high energy cosmic rays (UHECRs). The JEM-EUSO program aims to place an imaging instrument on board the International Space Station that will detect UHECRs in Earth’s atmosphere. The instrument will include software and hardware for analyzing the images. There are several different techniques for image analysis that could be used in the instrument. Different CNNs are implemented and tested on simulated data. The best-performing network is further analyzed. Results show that CNNs can be used to identify cosmic rays in images. The accuracy is found to be highly dependent on the brightness of the cosmic ray tracks compared to the background photon noise. It is also shown that the networks are able to identify the angle of the cosmic rays relative to the image frame. Accuracy is found to decrease with increasing angle resolution, with accuracies ranging from 100% down to pure guesses. More extensive studies with more complex simulations could generate finer results.</p>
----------------------------------------------------------------------
In diva2:1229832 - Note: no full text in DiVA
abstract is:
<p>Urban transportation systems are a vital part ofeveryday life nowadays. One common, but difficult issue in thisarea is bus bunching which means that buses close in on eachother eventually ending up platooning when in operation. Thisleads to inefficiency and passenger delay. The intent of this reportis to develop a model for mitigating delays in urban transportby reducing bus bunching. Our approach is to have buses selforganizeand focus on maintaining a consistent headway betweenbuses. We propose using Markov chains in the algorithm. Byassigning control points along the bus line that have the abilityto evaluate the location of the buses. This dynamic propertyallows for a quick response to the unpredictability of urban trafficand an increase in effective use of the transportation system.Results show that self-organizing, headway-based control has thepotential to significantly increase efficiency in urban transport.</p>

corrected abstract:
<p>Urban transportation systems are a vital part of everyday life nowadays. One common, but difficult issue in this area is bus bunching which means that buses close in on each other eventually ending up platooning when in operation. This leads to inefficiency and passenger delay. The intent of this report is to develop a model for mitigating delays in urban transport by reducing bus bunching. Our approach is to have buses self organize and focus on maintaining a consistent headway between buses. We propose using Markov chains in the algorithm. By assigning control points along the bus line that have the ability to evaluate the location of the buses. This dynamic property allows for a quick response to the unpredictability of urban traffic and an increase in effective use of the transportation system. Results show that self-organizing, headway-based control has the potential to significantly increase efficiency in urban transport.</p>
----------------------------------------------------------------------
In diva2:1229169 - Note: no full text in DiVA
abstract is:
<p>Since the cost of personnel represents a big part of the budget of a call centre itis interesting to investigate how it can be minimzed via scheduling. Finding theglobal optima can be extremely hard, which make heuristic solutions an interestingalternative in order to nd a satisfactory solution relatively quickly. If the problemis linear it is appropriate to use the simplex method. This report reviews GeneticAlgorithm and the Cross-Entropy method for creating schedules that minimize thecost, and the two methods are then compared. The algorithms are reviewed forlinear problems when simplex is appropriate, and a comparison is made betweenall of the methods. The simplex method is a method where solutions is looked forin the vertex of the dened area, since the optimal solution will be found there.In the Genetic Algorithm a solution set is randomally generated, out of which thebest ones are picked out. The best solutions will in turn be combined in orderto generate a better solution set, and the process is repeated until a satisfactoryresult is reached. The Cross Entropy{method is Monte Carlo-based in which asolution set is generated from a probability density function. The solutions areevaluated and then the probability density function is tweaked in order to makebetter solutions more probable.</p>

corrected abstract:
<p>Since the cost of personnel represents a big part of the budget of a call centre itis interesting to investigate how it can be minimzed via scheduling. Finding the global optima can be extremely hard, which make heuristic solutions an interesting alternative in order to nd a satisfactory solution relatively quickly. If the problem is linear it is appropriate to use the simplex method. This report reviews GeneticAlgorithm and the Cross-Entropy method for creating schedules that minimize the cost, and the two methods are then compared. The algorithms are reviewed for linear problems when simplex is appropriate, and a comparison is made between all of the methods. The simplex method is a method where solutions is looked for in the vertex of the defined area, since the optimal solution will be found there. In the Genetic Algorithm a solution set is randomly generated, out of which the best ones are picked out. The best solutions will in turn be combined in orderto generate a better solution set, and the process is repeated until a satisfactory result is reached. The Cross-Entropy method is Monte Carlo-based in which a solution set is generated from a probability density function. The solutions are evaluated and then the probability density function is tweaked in order to make better solutions more probable.</p>
----------------------------------------------------------------------
In diva2:1216739 - missing spaces in title:
"A comparison between aconventional LSTM network and agrid LSTM network applied onspeech recognition"
==>
"A comparison between a conventional LSTM network and a grid LSTM network applied onspeech recognition"

abstract is:
<p>In this paper, a comparision between the conventional LSTM network and the one-dimensionalgrid LSTM network applied on single word speech recognition is conducted. The performanceof the networks are measured in terms of accuracy and training time. The conventional LSTMmodel is the current state of the art method to model speech recognition. However, thegrid LSTM architecture has proven to be successful in solving other emperical tasks such astranslation and handwriting recognition. When implementing the two networks in the sametraining framework with the same training data of single word audio files, the conventionalLSTM network yielded an accuracy rate of 64.8 % while the grid LSTM network yielded anaccuracy rate of 65.2 %. Statistically, there was no difference in the accuracy rate betweenthe models. In addition, the conventional LSTM network took 2 % longer to train. However,this difference in training time is considered to be of little significance when tralnslating it toabsolute time. Thus, it can be concluded that the one-dimensional grid LSTM model performsjust as well as the conventional one.</p>

corrected abstract:
<p>In this paper, a comparision between the conventional LSTM network and the one-dimensional grid LSTM network applied on single word speech recognition is conducted. The performance of the networks are measured in terms of accuracy and training time. The conventional LSTM model is the current state of the art method to model speech recognition. However, the grid LSTM architecture has proven to be successful in solving other emperical tasks such as translation and handwriting recognition. When implementing the two networks in the same training framework with the same training data of single word audio files, the conventional LSTM network yielded an accuracy rate of 64.8 % while the grid LSTM network yielded an accuracy rate of 65.2 %. Statistically, there was no difference in the accuracy rate between the models. In addition, the conventional LSTM network took 2 % longer to train. However, this difference in training time is considered to be of little significance when tralnslating it to absolute time. Thus, it can be concluded that the one-dimensional grid LSTM model performs just as well as the conventional one.</p>
----------------------------------------------------------------------
In diva2:1142922 - diva2:1120498 is also a possible duplicate (The PDF in this latter case is just the paper - rather than the 2017 book form)

abstract is:
<p>The fundamentals of secure systems can be presentedas the three cornerstones: authentication (trustworthinessof senders), integrity (inability to alter messages) and confidentiality(inability to read messages for anyone except theintended recipient). In this project, an android application hasbeen programmed by the author with the purpose of sendinggeographical data over a bluetooth connection in a secure peerto-peer manner. In the application, the three goals were metprimarily through the use of certificate validation, encryption anddigital signatures. The final application features a user-interfacewith a Google Maps interface, and query parameters that theuser can set when requesting data about a certain position.The final application would very much be considered secure inan environment where only two android devices are communicating.However, more steps would have to be taken if the applicationis to be deployed commercially. The primary questions arescalability, speed and further security complications.</p>

corrected abstract:
<p>The fundamentals of secure systems can be presented as the three cornerstones: authentication (trustworthiness of senders), integrity (inability to alter messages) and confidentiality (inability to read messages for anyone except the intended recipient). In this project, an android application has been programmed by the author with the purpose of sending geographical data over a bluetooth connection in a secure peerto-peer manner. In the application, the three goals were met primarily through the use of certificate validation, encryption and digital signatures. The final application features a user-interface with a Google Maps interface, and query parameters that the user can set when requesting data about a certain position.</p><p>The final application would very much be considered secure in an environment where only two android devices are communicating. However, more steps would have to be taken if the application is to be deployed commercially. The primary questions are scalability, speed and further security complications.</p>

Note that the capitalization of Android and Bluetooth are incorrect in the origianl abstract.
----------------------------------------------------------------------
In diva2:1140141 - mssing space in title:
"On the use of miniaturized delta wings tomodulate the Blasius boundary layer"
==>
"On the use of miniaturized delta wings to modulate the Blasius boundary layer"

abstract is:
<p>The present study represents an experimental proof-of-concept of the use offree stream vortices to modulate the Blasius boundary layer in the spanwisedirection. Miniaturized delta wings have been employed to generate pairs ofcounter rotating vortices that, through the lift-up mechanism, create velocitystreaks which modulate the boundary layer. This study has been dedicatedto the design and characterization of the experimental set up in the BL windtunnel of KTH Mechanics. A 3-component LDV system has been used tocharacterize the ow eld.By analyzing the ow eld, it has been assessed that a streaky strucutre isindeed generated inside the boundary layer though this methodology. Amongthe dierent parameters in uencing the streaks generated, the angle of attackof the delta wings has been varied. For angles of 9 and 12 streaks with higherintensities than those generated inside the boundary layer, documented in theliterature, have been generated. For an angle of 3, they have been successfullygenerated and embedded inside the boundary layer. Further development ofthis technique could lead to its use in passive ow control strategies aiming atdelaying boundary layer transition.</p>

corrected abstract:
<p>The present study represents an experimental proof-of-concept of the use of free stream vortices to modulate the Blasius boundary layer in the spanwise direction. Miniaturized delta wings have been employed to generate pairs of counter rotating vortices that, through the lift-up mechanism, create velocity streaks which modulate the boundary layer. This study has been dedicated to the design and characterization of the experimental set up in the BL wind tunnel of KTH Mechanics. A 3-component LDV system has been used to characterize the flow field.</p><p>By analyzing the flow field, it has been assessed that a streaky strucutre is indeed generated inside the boundary layer though this methodology. Among the different parameters influencing the streaks generated, the angle of attack of the delta wings has been varied. For angles of 9 and 12º streaks with higher intensities than those generated inside the boundary layer, documented in the literature, have been generated. For an angle of 3º, they have been successfully generated and embedded inside the boundary layer. Further development of this technique could lead to its use in passive flow control strategies aiming at delaying boundary layer transition.</p>
----------------------------------------------------------------------
In diva2:1110869 abstract is:
<p>With thorough literature studies as well as simulations, a way to minimize the exposure to radiationthat astronauts are at risk of encountering during a solar proton event is sought. The understanding ofwhere these particles come from, as well as the random nature of solar particle events is of importancein order to predict their occurrence. Different models used for predicting solar particle events based on aPoisson possibility distribution are presented, as well as real-time forecasts which give a warning of anapproaching event. Although the models used for real-time forecasts have a high accuracy rate, the averagewarning time is only approximately one hour. The downside with the predicted possible occurrence isthat this only gives a statistical probability of events that could possibly occur. For the real-time forecaststhe downside is that with an average warning time of only one hour, they do not give a lot of time forseeking shelter during the onset of an event. With simulations it is shown that the best way to minimizethe radiation dose obtained by astronauts is to use different materials of shielding. It is also shown that alower shielding thickness when encountering SPEs, for example when in a space suit, is useful as longas the total amount of time spent in this suit during the duration of a mission is planned thoroughly inorder to stay below the radiation dose limits. If an astronaut would be caught in an event with the samemagnitude and intensity as the solar particle event of August 1972, it is shown that the astronaut onlyhas nine minutes to seek shelter before exceeding the radiation dose limits and thereby risking radiationinduced sickness.</p>

corrected abstract:
<p>With thorough literature studies as well as simulations, a way to minimize the exposure to radiation that astronauts are at risk of encountering during a solar proton event is sought. The understanding of where these particles come from, as well as the random nature of solar particle events is of importance in order to predict their occurrence. Different models used for predicting solar particle events based on a Poisson possibility distribution are presented, as well as real-time forecasts which give a warning of an approaching event. Although the models used for real-time forecasts have a high accuracy rate, the average warning time is only approximately one hour. The downside with the predicted possible occurrence is that this only gives a statistical probability of events that could possibly occur. For the real-time forecasts the downside is that with an average warning time of only one hour, they do not give a lot of time for seeking shelter during the onset of an event. With simulations it is shown that the best way to minimize the radiation dose obtained by astronauts is to use different materials of shielding. It is also shown that a lower shielding thickness when encountering SPEs, for example when in a space suit, is useful as long as the total amount of time spent in this suit during the duration of a mission is planned thoroughly in order to stay below the radiation dose limits. If an astronaut would be caught in an event with the same magnitude and intensity as the solar particle event of August 1972, it is shown that the astronaut only has nine minutes to seek shelter before exceeding the radiation dose limits and thereby risking radiation induced sickness.</p>
----------------------------------------------------------------------
In diva2:1089907 - missing spaces in title:
"Stenotic Flows: Direct Numerical Simulation,Stability and Sensitivity to Asymmetric ShapeVariations"
==>
"Stenotic Flows: Direct Numerical Simulation, Stability and Sensitivity to Asymmetric Shape Variations"

abstract is:
<p>Flow through a sinuous stenosis with varying degrees of shape asymmetry andat Reynolds number ranging from 250 up to 800 is investigated using direct numericalsimulation (DNS), global linear stability analysis and sensitivity analysis.The shape asymmetry consists of an offset of the stenosis throat, quantifiedas the eccentricity parameter, E. At low Reynolds numbers in a symmetricgeometry, the flow is steady and symmetric. Our results show that when Reynoldsnumber is increased, the flow obtains two simultaneous linearly stablesteady states through a subcritical Pitchfork bifurcation: a symmetric stateand an asymmetric state. The critical Reynolds number for transition betweenthe states are found to be very sensitive to asymmetric shape variations, thusbifurcation can also occur with respect to eccentricity for a given Reynoldsnumber. The final state observed in the DNS can be either nearly symmetricor strongly asymmetric, depending on the initial condition. When eccentricityis increased from zero, the symmetric state becomes slightly asymmetric,flow asymmetry varying nearly linearly with eccentricity. When eccentricityis increased further, the nearly symmetric state becomes linearly unstable. Alinear global stability analysis shows that the eigenvalue sensitivity to eccentricityis of the second order, this is also confirmed by preliminary sensitivityanalysis. For higher Reynolds numbers, the asymmetric solution branch displaysregimes of periodic oscillations as well as intermittency. Comparisons aremade to earlier studies and a theory that attempts to explain and unite thedifferent numerical and experimental results within the field is presented.</p>

corrected abstract:
<p>Flow through a sinuous stenosis with varying degrees of shape asymmetry and at Reynolds number ranging from 250 up to 800 is investigated using direct numerical simulation (DNS), global linear stability analysis and sensitivity analysis. The shape asymmetry consists of an offset of the stenosis throat, quantified as the eccentricity parameter, <em>E</em>. At low Reynolds numbers in a symmetric geometry, the flow is steady and symmetric. Our results show that when Reynolds number is increased, the flow obtains two simultaneous linearly stable steady states through a subcritical Pitchfork bifurcation: a symmetric state and an asymmetric state. The critical Reynolds number for transition between the states are found to be very sensitive to asymmetric shape variations, thus bifurcation can also occur with respect to eccentricity for a given Reynolds number. The final state observed in the DNS can be either nearly symmetric or strongly asymmetric, depending on the initial condition. When eccentricity is increased from zero, the symmetric state becomes slightly asymmetric, flow asymmetry varying nearly linearly with eccentricity. When eccentricity is increased further, the nearly symmetric state becomes linearly unstable. A linear global stability analysis shows that the eigenvalue sensitivity to eccentricity is of the second order, this is also confirmed by preliminary sensitivity analysis. For higher Reynolds numbers, the asymmetric solution branch displays regimes of periodic oscillations as well as intermittency. Comparisons are made to earlier studies and a theory that attempts to explain and unite the different numerical and experimental results within the field is presented.</p>
----------------------------------------------------------------------
In diva2:1083783 abstract is:
<p>One of the governing sources of energy loss in a modern day jet engine is attributed to surfacedrag. This energy loss can be divided into friction loss and to surface geometry loss. Thefriction loss is the shear stress the fluid experience due to a no slip condition at the wall, whilethe surface geometry loss is due to pressure drop when the fuel passes an obstacle.The objective of this work is to study the drag coefficient of a plate for different types ofmilled tracks and for different kinds of flow conditions. The theories used to calculate thedrag coefficient are based on the momentum thickness theory including shear stress- andpressure integration. The computations were carried out with ANSYS CFX assuming a ShearStress Transport 𝑘 − 𝜔 turbulence model. The steady state flow conditions tested are varyingboundary layer thicknesses, milled track heights, milled track widths, Reynolds numbers overthe milled track height, Reynolds numbers over the plate length and free-stream angle ofattack. By knowing what affects the drag coefficient for different types of milled tracks, morepractical models can be developed making the prediction of surface drag inside the jet enginemore accurate.This report has resulted in a formula that predicts the drag coefficient for different types ofmilled surfaces. The formula is derived from the assumption that the CFD results on ANSYSCFX are correct. A physical test has not been made to verify those results, however this has tobe done to prove that this formula is valid.</p>

corrected abstract:
<p>One of the governing sources of energy loss in a modern day jet engine is attributed to surface drag. This energy loss can be divided into friction loss and to surface geometry loss. The friction loss is the shear stress the fluid experience due to a no slip condition at the wall, while the surface geometry loss is due to pressure drop when the fuel passes an obstacle.</p><p>The objective of this work is to study the drag coefficient of a plate for different types of milled tracks and for different kinds of flow conditions. The theories used to calculate the drag coefficient are based on the momentum thickness theory including shear stress- and pressure integration. The computations were carried out with ANSYS CFX assuming a Shear Stress Transport <em>k − &omega;</em> turbulence model. The steady state flow conditions tested are varying boundary layer thicknesses, milled track heights, milled track widths, Reynolds numbers over the milled track height, Reynolds numbers over the plate length and free-stream angle of attack. By knowing what affects the drag coefficient for different types of milled tracks, more practical models can be developed making the prediction of surface drag inside the jet engine more accurate.</p><p>This report has resulted in a formula that predicts the drag coefficient for different types of milled surfaces. The formula is derived from the assumption that the CFD results on ANSYS CFX are correct. A physical test has not been made to verify those results, however this has to be done to prove that this formula is valid.</p>
----------------------------------------------------------------------
In diva2:1083057 - missing spaces in title:
"Performance Evaluation of the EjectionSystem for the RAIN RocketExperiment"
==>
"Performance Evaluation of the Ejection System for the RAIN Rocket Experiment"

abstract is:
<p>This master's thesis work was performed at the department of Mechanics of the RoyalInstitute of Technology (KTH) in Stockholm as part of my Master of Science studiesin Aerospace Engineering at KTH. This thesis study has two major purposes: (1) toevaluate the performance of the spring-based ejection system used in the RAIN rocketexperiment and (2) to suggest improvements to reduce de-spin and tip-o of the ejectedprobes.To evaluate the performance of the ejection system two sets of data have been analyzed:on-ground tests data and ight data. Data from on-ground ejection tests have beenanalyzed by means of video analysis and inertial sensor analysis while for ight dataonly inertial sensor data were available. Moreover, simple mechanical analytical modelshave been created to model the behavior of the probes during the ejection.The results from data analysis and mechanical models are able to suggests some improvementsfor the ejection system. However, it is not possible to make any strongconclusion on what might have caused the de-spin and the tip-o of the probes.</p>

corrected abstract:
<p>This master's thesis work was performed at the department of Mechanics of the Royal Institute of Technology (KTH) in Stockholm as part of my Master of Science studies in Aerospace Engineering at KTH. This thesis study has two major purposes: (1) to evaluate the performance of the spring-based ejection system used in the RAIN rocket experiment and (2) to suggest improvements to reduce de-spin and tip-off of the ejected probes.</p><p>To evaluate the performance of the ejection system two sets of data have been analyzed: on-ground tests data and flight data. Data from on-ground ejection tests have been analyzed by means of video analysis and inertial sensor analysis while for flight data only inertial sensor data were available. Moreover, simple mechanical analytical models have been created to model the behavior of the probes during the ejection.</p><p>The results from data analysis and mechanical models are able to suggests some improvements for the ejection system. However, it is not possible to make any strong conclusion on what might have caused the de-spin and the tip-off of the probes.</p>
----------------------------------------------------------------------
The following results are based upon a comparison of two different hashes of the English abstracts

len(possible_duplicates)=24
possible_duplicates=[
{'diva2:1142785', 'diva2:1120402'}, # Bus Line Optimisation Using Autonomous Minibuses
{'diva2:1656340', 'diva2:1596326'}, # Combined Actuarial Neural Networks in Actuarial Rate Making
{'diva2:1478009', 'diva2:1487614'}, # Conceptual Design of an Air- launched Multi-stage Launch Vehicle
{'diva2:1596321', 'diva2:1610014'}, # Deep Bayesian Neural Networks for Prediction of Insurance Premiums
{'diva2:530870', 'diva2:539446'},   # Design of secondary air system and thermal models for triple spool jet engines
{'diva2:1833721', 'diva2:1781513'}, # Exploring the impact of economic and social factors on stock market performance
{'diva2:408838', 'diva2:408837'},   # Global Learning at Ericsson: how to improve knowledge managementand competence build-up
{'diva2:1120480', 'diva2:1120479'}, # Gravitational Waves and Coalescing Black Holes
{'diva2:408824', 'diva2:408825'},   # Hur förbättra utvärderingsverktyget på VUC
{'diva2:1142776', 'diva2:1120852'}, # Intelligent Traffic Intersection Management Using Motion Planning for Autonomous Vehicles
{'diva2:1867288', 'diva2:1867352'}, # Modeling of Time Series Dynamics through Nonequilibrium Statistical Mechanics
{'diva2:1083787', 'diva2:1071273'}, # Modelling coaxial jets relevat to turbofan jet engines
{'diva2:558593', 'diva2:562861'},   # Photoluminescence and AFM characterization of silicon nanocrystals prepared by low-temperature plasma enhanced chemical vapour depositon and annealing.
{'diva2:1880442', 'diva2:1879507'}, # Portable wind tunnel design​
{'diva2:1120322', 'diva2:1120317'}, # Quadcopter formation simulated in a choreographed dance to music
{'diva2:754257', 'diva2:753742'},   # Revision Of The Aircraft Engines Preliminary Design Platform Of First Level
{'diva2:1120520', 'diva2:1142942'}, # SDR Implementation for Satellite Communication
{'diva2:1120498', 'diva2:1142922'}, # Security and Privacy for Modern and Emerging Mobile Systems
{'diva2:1045047', 'diva2:1033220'}, # Simulating Group Formations Using RVO
{'diva2:865309', 'diva2:815818'},   # Stress simulation of the SEAM CubeSat structure during launch.
{'diva2:1142795', 'diva2:1120491'}, # Study of a Battery Driven Electrohydrodynamic Thruster
{'diva2:1221489', 'diva2:1219115'}, # The impact of missing data imputation on HCC survival prediction: Exploring the combination of missing data imputation with data-level methods such as clustering and oversampling
{'diva2:1142934', 'diva2:1120556'}, # Thermal Analysis and Control of MIST
{'diva2:1033208', 'diva2:1045056'} # Visualizing the Body Language of a Musical Conductor using Gaussian Process Latent Variable Models ---- Note difference in titles
]
----------------------------------------------------------------------
In diva2:853573 abstract is:
<p>This paper summarises the most important aspects of shipping today, describingstakeholders, markets and types of ships, and weighing the sustainability of this form oftransportation.</p><p>Furthermore, it follows an initial ship design project step by step: a new vessel is beingprojected to fulfill a transportation need. The vessel will safely and cost-effectively transport1.2 million cubic meters of raw logs from Port Metro in Vancouver to the Chinese harboursTianjin and Qingdao. During the initial design phase, the requirements and main limitingfactors of the ship’s specifications are identified and the system concept is created.</p><p>Also investigated in this paper is the need for a new generation of intact stability criteria forships, in light of the mechanics of a stability failure mode, parametric rolling, and cases ofparticular accidents at sea involving parametric rolling. The circumstances surroundingincidents of stability failure in cases where existing criteria are met, are explored. The existingcriteria are examined and possible motivations behind the development of new criteria areconsidered. Finally, new methods for assessing risk in the case of parametric rolling areproposed. Based on the importance of this topic for safety at sea, and the hypothesis that asecond generation of stability criteria is needed is examined critically.</p>

corrected abstract:
<p>This paper summarises the most important aspects of shipping today, describing stakeholders, markets and types of ships, and weighing the sustainability of this form of transportation.</p><p>Furthermore, it follows an initial ship design project step by step: a new vessel is being projected to fulfill a transportation need. The vessel will safely and cost-effectively transport 1.2 million cubic meters of raw logs from Port Metro in Vancouver to the Chinese harbours Tianjin and Qingdao. During the initial design phase, the requirements and main limiting factors of the ship’s specifications are identified and the system concept is created.</p><p>Also investigated in this paper is the need for a new generation of intact stability criteria for ships, in light of the mechanics of a stability failure mode, parametric rolling, and cases of particular accidents at sea involving parametric rolling. The circumstances surrounding incidents of stability failure in cases where existing criteria are met, are explored. The existing criteria are examined and possible motivations behind the development of new criteria are considered. Finally, new methods for assessing risk in the case of parametric rolling are proposed. Based on the importance of this topic for safety at sea, and the hypothesis that a second generation of stability criteria is needed is examined critically.</p>
----------------------------------------------------------------------
In diva2:852972 abstract is:
<p>The inhabitants of the earth are facing a global crisis unless the amount ofgreenhouse gases in the atmosphere is reduced to sustainable levels. In anincreasingly globalized world the aircraft industry contributes to an increasinglyamount of emissions that will only increase further if no drastic measures are taken.</p><p>This bachelor thesis aims to study the fuel reduction that in ideal cases can beobtained by utilizing the so-called distributed propulsion configuration. This meansthat a series of electrically driven propellers are placed at the trailing edge of the wingwith purpose to fill the arisen wake behind the wing. The method is well establishedin the marine technology and is referred as the Boundary Layer Ingestion. The resultin the paper is derived from mathematical models and existing data from a Cessna172 aircraft.</p><p>A possible fuel reduction of 11% is derived for a completely idealized situation whichis considered as a very good result which indicates that the BLI technology is worthstudying more closely and develop further.</p>

corrected abstract:
<p>The inhabitants of the earth are facing a global crisis unless the amount of greenhouse gases in the atmosphere is reduced to sustainable levels. In an increasingly globalized world the aircraft industry contributes to an increasingly amount of emissions that will only increase further if no drastic measures are taken.</p><p>This bachelor thesis aims to study the fuel reduction that in ideal cases can be obtained by utilizing the so-called distributed propulsion configuration. This means that a series of electrically driven propellers are placed at the trailing edge of the wing with purpose to fill the arisen wake behind the wing. The method is well established in the marine technology and is referred as the Boundary Layer Ingestion. The result in the paper is derived from mathematical models and existing data from a Cessna 172 aircraft.</p><p>A possible fuel reduction of 11% is derived for a completely idealized situation which is considered as a very good result which indicates that the BLI technology is worth studying more closely and develop further.</p>
----------------------------------------------------------------------
In diva2:784038 abstract is:
<p>A person falling of a ship can be difficult to locate since the wake behind the ship forms achaotic field, making it extremely difficult to predict the location of the victim even if thetime when they fell overboard is known. Survivability for humans immersed at sea is verydependent on the time spent in the water, and varies significantly with sea temperature; thismakes it imperative that the victim is retrieved rapidly. Our current research is aimed atreducing this time using several UAV's searching the ships wake simultaneously, as a swarm.Since the wake is chaotic, a simulation was developed to model different random motions of avictim based on a chaotic equation. Our current research is making use of an establishedsimulator environment and developing it further to investigate how different platforms mayaffect rescue time, varying on the size of the ship, the weather conditions and whether thesearch is operated during day or night. Two different search strategies were implemented inthe developed simulator; these are Expanding Square search and Parallel search. An overallconclusion based on the results obtained is that the expanding square search tends to be amore rigid and reliable search strategy. Also the results show that for any scenario, the soughtperson is detected within minutes.</p>

corrected abstract:
<p>A person falling of a ship can be difficult to locate since the wake behind the ship forms a chaotic field, making it extremely difficult to predict the location of the victim even if the time when they fell overboard is known. Survivability for humans immersed at sea is very dependent on the time spent in the water, and varies significantly with sea temperature; this makes it imperative that the victim is retrieved rapidly. Our current research is aimed at reducing this time using several UAV's searching the ships wake simultaneously, as a swarm. Since the wake is chaotic, a simulation was developed to model different random motions of a victim based on a chaotic equation. Our current research is making use of an established simulator environment and developing it further to investigate how different platforms may affect rescue time, varying on the size of the ship, the weather conditions and whether the search is operated during day or night. Two different search strategies were implemented in the developed simulator; these are Expanding Square search and Parallel search. An overall conclusion based on the results obtained is that the expanding square search tends to be a more rigid and reliable search strategy. Also the results show that for any scenario, the sought person is detected within minutes.</p>
----------------------------------------------------------------------
In diva2:784014 - Note: no full text in DiVA

missing space in title:
"Implementation of Transitional Laminar SeparationBubbles in a Navier-Stokes Code"
==>
"Implementation of Transitional Laminar Separation Bubbles in a Navier-Stokes Code"

abstract is:
<p>Laminar separation bubbles evolve on laminar wing at low Reynolds number.These bubbles have especially been observed on winglets or on wings with flapsor slats. They can stretch up to 30% chord, taking them into account is crucialsince their presence greatly modifies the forces acting on a wing. However, theestablishment of a model is extremely complex and many researches are stillundertaken on this subject. The aim of this work is to achieve automation ofdetecting and taking into account long bubbles in the Navier -Stokes code ofDassault Aviation named Aether. The key issue is the proper detection of thetransition position inside the bubble. This detection is possible by performing alinear stability calculation, but this method is not very robust in this case andvery costly in computation time, as conventional methods are not practicable inthe presence of separation. Finally, the chosen method uses a boundary layercode to predict the position of separation, then a criterion involving local integralquantities to determine the intermittent transition area. This method has beentested for various 2D airfoils and provided quite satisfactory results.</p>

corrected abstract:
<p>Laminar separation bubbles evolve on laminar wing at low Reynolds number. These bubbles have especially been observed on winglets or on wings with flapsor slats. They can stretch up to 30% chord, taking them into account is crucial since their presence greatly modifies the forces acting on a wing. However, the establishment of a model is extremely complex and many researches are still undertaken on this subject. The aim of this work is to achieve automation of detecting and taking into account long bubbles in the Navier -Stokes code of Dassault Aviation named Aether. The key issue is the proper detection of the transition position inside the bubble. This detection is possible by performing alinear stability calculation, but this method is not very robust in this case and very costly in computation time, as conventional methods are not practicable inthe presence of separation. Finally, the chosen method uses a boundary layer code to predict the position of separation, then a criterion involving local integral quantities to determine the intermittent transition area. This method has been tested for various 2D airfoils and provided quite satisfactory results.</p>
----------------------------------------------------------------------
In diva2:783979 - missing spaces in title:
"Launch and recovery systems for unmannedvehicles onboard ships. A study and initialconcepts."
==>
"Launch and recovery systems for unmanned vehicles onboard ships. A study and initial concepts."

abstract is:
<p>This master’s thesis paper is an exploratory study along with conceptual design of launch and recovery systems(LARS) for unmanned vehicles and RHIB:s, which has been conducted for ThyssenKrupp Marine System AB inKarlskrona, Sweden. Two concepts have been developed, one for aerial vehicles (UAV:s) and one for surfaceand underwater vehicles (USV, RHIB and UUV). The goal when designing the two LARS has been to meet thegrowing demand within the world navies for greater off-board capabilities. The two concepts have been designedto be an integrated solutions on a 90 m long naval ship and based on technology that will be proven in year2015-2020. To meet the goal of using technology that will be proven in year 2015-2020, existing and futurepossible solutions has been evaluated. From the evaluations one technique for each concept was chosen forfurther development.In the development of a LARS for aerial vehicles only fixed wing UAV:s have been considered. The conceptwas made for a reference UAV based on the UAV Shadow 200B, which has a weight of 170 kg. The conceptthat was developed is a parasail lifter that can both launch and recover the reference UAV effectively. In thedevelopment of a system for surface and underwater vehicles only vehicle lengths in the span 1-12 m have beenconsidered. The concept that has been developed is a stern ramp that uses a sled to launch and recover all threevehicle types. The two concepts that has been developed are in an early design state and the papers results shouldtherefore be seen as an estimation of what each system are capable of performing.</p>

corrected abstract:
<p>This master’s thesis paper is an exploratory study along with conceptual design of launch and recovery systems (LARS) for unmanned vehicles and RHIB:s, which has been conducted for <em>ThyssenKrupp Marine System AB</em> in Karlskrona, Sweden. Two concepts have been developed, one for aerial vehicles (UAV:s) and one for surface and underwater vehicles (USV, RHIB and UUV). The goal when designing the two LARS has been to meet the growing demand within the world navies for greater off-board capabilities. The two concepts have been designed to be an integrated solutions on a 90 m long naval ship and based on technology that will be proven in year 2015-2020. To meet the goal of using technology that will be proven in year 2015-2020, existing and future possible solutions has been evaluated. From the evaluations one technique for each concept was chosen for further development.</p><p>In the development of a LARS for aerial vehicles only fixed wing UAV:s have been considered. The concept was made for a reference UAV based on the UAV Shadow 200B, which has a weight of 170 kg. The concept that was developed is a parasail lifter that can both launch and recover the reference UAV effectively. In the development of a system for surface and underwater vehicles only vehicle lengths in the span 1-12 m have been considered. The concept that has been developed is a stern ramp that uses a sled to launch and recover all three vehicle types. The two concepts that has been developed are in an early design state and the papers results should therefore be seen as an estimation of what each system are capable of performing.</p>
----------------------------------------------------------------------
In diva2:650418 - Note: no full text in DiVA
abstract is:
<p>The master thesis is concerned with the investigation of void fraction distributionsacross BWR fuel assemblies without spacers in minor scale design and with theinvestigation of the influence of different axial heat flux distribution and variations ofmass flux on the void fraction using Computational Fluid Dynamic (CFD) commercialcode CFX.11. The code is validated against experimental data obtained by R.T Lahey etal. [1]. A reasonable agreement between predictions and experimental data is obtainedfor the quality. However, some discrepancies are observed for the relative mass fluxdistributions. Further investigation and appropriate modelling in CFD code would beneeded to understand these inconsistencies.</p><p>The CFD code is also tested for BWR and PWR operational conditions. In case ofBWR, CFX suggests constant average void fraction distribution at the cross-section ofrod bundle near the exit at low mass flux and this distribution disrupted at higher massflux for different axial heat flux profiles. While, in case of PWR, CFX suggests that theaverage temperature distribution at the cross-section of the rod bundle near the exit isconstant for specific mass flux, average heat flux and no influence of different axial heatflux profile is detected.</p>


corrected abstract:
<p>The master thesis is concerned with the investigation of void fraction distributions across BWR fuel assemblies without spacers in minor scale design and with the investigation of the influence of different axial heat flux distribution and variations of mass flux on the void fraction using Computational Fluid Dynamic (CFD) commercial code CFX.11. The code is validated against experimental data obtained by R.T Lahey etal. [1]. A reasonable agreement between predictions and experimental data is obtained for the quality. However, some discrepancies are observed for the relative mass flux distributions. Further investigation and appropriate modelling in CFD code would be needed to understand these inconsistencies.</p><p>The CFD code is also tested for BWR and PWR operational conditions. In case of BWR, CFX suggests constant average void fraction distribution at the cross-section of rod bundle near the exit at low mass flux and this distribution disrupted at higher mass flux for different axial heat flux profiles. While, in case of PWR, CFX suggests that the average temperature distribution at the cross-section of the rod bundle near the exit is constant for specific mass flux, average heat flux and no influence of different axial heatflux profile is detected.</p>
----------------------------------------------------------------------
In diva2:642316 abstract is:
<p>The aim of this paper is to document the development of a method for detectionand quantification of Polychlorinated biphenyls (PCB) in soil using GasChromatography (GC) connected to a Quadrupole Mass Spectrometer (MS) via aninternal standard (CB189). The method developed is performed in conjunction withthe information provided by the Swedish environmental agency (Svenskanaturvårdsverket, SNV) in regards to PCB limits for sensitive land usage. The steps ofthe method and maintenance of the GC/MS are used to create a user manual andan attempt at transformative learning is done in an effort to teach the staff atLjungaLab AB so that at the very least, independent analysis can be run and at best,new methods and application are independently developed for the GC/MS. Anevaluation of the teaching efforts is also done to assess what grade of learning isachieved.</p>

corrected abstract:
<p>The aim of this paper is to document the development of a method for detection and quantification of Polychlorinated biphenyls (PCB) in soil using Gas Chromatography (GC) connected to a Quadrupole Mass Spectrometer (MS) via an internal standard (CB189). The method developed is performed in conjunction with the information provided by the Swedish environmental agency (<span lang=sv">Svenska naturvårdsverket</span>, SNV) in regards to PCB limits for sensitive land usage. The steps of the method and maintenance of the GC/MS are used to create a user manual and an attempt at transformative learning is done in an effort to teach the staff at LjungaLab AB so that at the very least, independent analysis can be run and at best, new methods and application are independently developed for the GC/MS. An evaluation of the teaching efforts is also done to assess what grade of learning is achieved.</p>
----------------------------------------------------------------------
In diva2:618236 abstract is:
<p>High residual stresses are likely to develop in honeycomb sandwichparts after autoclave co-curing and can lead to manufacturing defects.By using finite element unit cell models, these stresses have been calculatedfor standard panels and for panels where different core blocksare joined with adhesive. Failure criteria are given for three types ofaluminum honeycombs under combined thermal and shear loads, allowingto calculate the residual strength of the cores. Residual stressvalues are also calculated for adhesive joints between different coreblocks, they being about the same order of magnitude as the strengthof the adhesive regardless of geometry or core combination. Last, it isshown that the effect of the sandwich plate chamfered edges in preventingthe core expansion during the heating cycle may cause corecrushing when high and low density honeycombs are combined.</p>

corrected abstract:
<p>High residual stresses are likely to develop in honeycomb sandwich parts after autoclave co-curing and can lead to manufacturing defects. By using finite element unit cell models, these stresses have been calculated for standard panels and for panels where different core blocks are joined with adhesive. Failure criteria are given for three types of aluminum honeycombs under combined thermal and shear loads, allowing to calculate the residual strength of the cores. Residual stress values are also calculated for adhesive joints between different core blocks, they being about the same order of magnitude as the strength of the adhesive regardless of geometry or core combination. Last, it is shown that the effect of the sandwich plate chamfered edges in preventing the core expansion during the heating cycle may cause core crushing when high and low density honeycombs are combined.</p>
----------------------------------------------------------------------
In diva2:515603 abstract is:
<p>This paper concerns my research on the culture of the IT-workforce and how it relates to otheroccupational cultures, as well as how the relation affects the communication between them. Iconducted an ethnographic study at an IT-consulting company. My focus was on theinteraction between the IT-consultants and their customers. In combination with previousresearch in the same subject area, the study shows that the IT-workforce has a distinctoccupational culture. It is influenced by the reverence of technical knowledge and how the ITworkforceis treated as a group by society in general. This reciprocal prejudice also leads tointercultural dysfunction. Since a lot of companies depend on the IT-workforce there is muchto gain if communication is improved. Some solutions are for the IT-workforce to be morepronounced and understanding in the intercultural communication and to find common goalsto strive towards. Also the lowest level of IT-knowledge among the general population mustbe raised to avoid a class society related to IT.</p>

corrected abstract:
<p>This paper concerns my research on the culture of the IT-workforce and how it relates to other occupational cultures, as well as how the relation affects the communication between them. I conducted an ethnographic study at an IT-consulting company. My focus was on the interaction between the IT-consultants and their customers. In combination with previous research in the same subject area, the study shows that the IT-workforce has a distinct occupational culture. It is influenced by the reverence of technical knowledge and how the IT-workforce is treated as a group by society in general. This reciprocal prejudice also leads to intercultural dysfunction. Since a lot of companies depend on the IT-workforce there is much to gain if communication is improved. Some solutions are for the IT-workforce to be more pronounced and understanding in the intercultural communication and to find common goals to strive towards. Also the lowest level of IT-knowledge among the general population must be raised to avoid a class society related to IT.</p>
----------------------------------------------------------------------
In diva2:416911 abstract is:
<p>Rolling contact fatigue is a problem encountered with many machine elements.In the current report a numerical study has been performed in order to predictthe crack path and crack propagation cycles of a surface initiated rolling contactfatigue crack. The implementation of the contact problem is based on theasperity point load mechanism for rolling contact fatigue. The practical studiedproblem is gear contact. Different loading types and models are studied andcompared to an experimental spall profile. Good agreement has been observedconsidering short crack lengths with a distributed loading model using normalloads on the asperity and for the cylindrical contact and a tangential load on theasperity. Several different crack propagation criteria have been implemented inorder to verify the validity of the dominant mode I crack propagation assumption.Some general characteristics of rolling contact fatigue cracks have beenhighlighted. A quantitative parameter study of the implemented model hasbeen performed. </p>

corrected abstract:
<p>Rolling contact fatigue is a problem encountered with many machine elements. In the current report a numerical study has been performed in order to predict the crack path and crack propagation cycles of a surface initiated rolling contact fatigue crack. The implementation of the contact problem is based on the asperity point load mechanism for rolling contact fatigue. The practical studied problem is gear contact. Different loading types and models are studied and compared to an experimental spall profile. Good agreement has been observed considering short crack lengths with a distributed loading model using normal loads on the asperity and for the cylindrical contact and a tangential load on the asperity. Several different crack propagation criteria have been implemented in order to verify the validity of the dominant mode I crack propagation assumption. Some general characteristics of rolling contact fatigue cracks have been highlighted. A quantitative parameter study of the implemented model has been performed.</p>
----------------------------------------------------------------------
In diva2:405938 abstract is:
<p>This report describes the process of developing a conceptual design of the Storm Bird, a long distance sailing cruiser. The starting point was a boat designed in the mid nineties by the famous Swedish naval architect Håkan Södergren and the aim with the project is to present an idea as to the renewal of the design in a more modern boat. The new Storm bird was supposed to be a full on blue water cruiser concept, a boat that the presumed owner would not have to change in order to set off on his trip.To get insight in the minds and the needs of long distance sailors an extensive market and customer analysis has been undergone. This together with experience in the design team is a base to the thoughts and the ideas incorporated in this design.The hull design was limited to the existing hull moulds meaning that no changes in the hull shape could be made. An alternative however was the transformation from negative to positive transom which proved a very effective way of making the boat feel bigger.The design and layout have been focused on making an effective, well planned but most of all social yacht. The clear boundary between the inside and outside has been removed thanks to a large opening to the cockpit with big windows and good connection. The cockpit and interior areas have been focused towards each other so as to create one big social area, boundary free.Further on the living quarters, as the rest of the boat, are focused on the main idea of the customer being mainly a cruising couple. Therefore an optimal interior layout with focus on the one master cabin has been developedIn the cockpit, seats are comfortable as well as facing forwards and everyone onboard can follow what is going on through the forward placed navigation central. The wide opening between cockpit and interior makes traditional rope handling impossible. All controls are led aft through a clever arrangement to clutches and winches placed on either side of the cockpit instead of on the deck house. This way all functions are in the right position, close to the helmsman. The ropes are later hidden in boxes to ensure a tangle free cockpit.An intelligent overall solution when it comes to onboard systems has been developed as well. Key words have been weight distribution, serviceability and ease of installation. Stowage space and tank volumes correspond to the yacht’s intended use.The structural design has been carried out focusing on arriving at a realistic weight calculation in order to be able to determine centers of gravity and place equipment and ballast to achieve a working concept. Material and manufacturing techniques have been chosen so as to fit the expertise available at the company.Appendage design has focused on modernizing the underwater body by incorporating a new keel and rudder. The performance of the boat has been increased significantly whilst not making it too extreme for its intended purpose.The finished design concept is believed to be a really attractive choice for a blue water sailor.</p>

corrected abstract:
<p>This report describes the process of developing a conceptual design of the Storm Bird, a long distance sailing cruiser. The starting point was a boat designed in the mid nineties by the famous Swedish naval architect Håkan Södergren and the aim with the project is to present an idea as to the renewal of the design in a more modern boat. The new Storm bird was supposed to be a full on blue water cruiser concept, a boat that the presumed owner would not have to change in order to set off on his trip.</p><p>To get insight in the minds and the needs of long distance sailors an extensive market and customer analysis has been undergone. This together with experience in the design team is a base to the thoughts and the ideas incorporated in this design.</p><p>The hull design was limited to the existing hull moulds meaning that no changes in the hull shape could be made. An alternative however was the transformation from negative to positive transom which proved a very effective way of making the boat feel bigger.</p><p>The design and layout have been focused on making an effective, well planned but most of all social yacht. The clear boundary between the inside and outside has been removed thanks to a large opening to the cockpit with big windows and good connection. The cockpit and interior areas have been focused towards each other so as to create one big social area, boundary free.</p><p>Further on the living quarters, as the rest of the boat, are focused on the main idea of the customer being mainly a cruising couple. Therefore an optimal interior layout with focus on the one master cabin has been developed</p><p>In the cockpit, seats are comfortable as well as facing forwards and everyone onboard can follow what is going on through the forward placed navigation central. The wide opening between cockpit and interior makes traditional rope handling impossible. All controls are led aft through a clever arrangement to clutches and winches placed on either side of the cockpit instead of on the deck house. This way all functions are in the right position, close to the helmsman. The ropes are later hidden in boxes to ensure a tangle free cockpit.</p><p>An intelligent overall solution when it comes to onboard systems has been developed as well. Key words have been weight distribution, serviceability and ease of installation. Stowage space and tank volumes correspond to the yacht’s intended use.</p><p>The structural design has been carried out focusing on arriving at a realistic weight calculation in order to be able to determine centers of gravity and place equipment and ballast to achieve a working concept. Material and manufacturing techniques have been chosen so as to fit the expertise available at the company.</p><p>Appendage design has focused on modernizing the underwater body by incorporating a new keel and rudder. The performance of the boat has been increased significantly whilst not making it too extreme for its intended purpose.</p><p>The finished design concept is believed to be a really attractive choice for a blue water sailor.</p>
----------------------------------------------------------------------
In diva2:1881888 abstract is:
<p>During the development of new small modular gen IV nuclear reactors, promising nuclear fuels such as uraniumnitride (UN) are investigated. Radiation damage and its effects on the fuel material have often been investigatedexperimentally through controlled irradiation methods. The aim of this thesis is to modify and develop existingMolecular Dynamics simulation methods for use in UN irradiation simulations and investigate its viability for suchuse. The present work primarily used the CRA to simulate irradiation damage and conducted Wigner-Seitz analysisto find defects. Pressure data for different probability values converged to approximately 150 kPa at a dose of 1 dpaand simulations indicated volumetric swelling of around 6% to 7%, suggesting that microstructural swelling due todefect accumulation could explain the experimental observations of cracking. Cluster analysis reveals that interstitialclusters increase to a peak, after which their number decreases towards a steady state, while DXA analysis returned ahandful of dislocation lines at 1d pa for both the uranium sublattice as well as for the nitrogen sublattice.</p><p> </p>

corrected abstract:
<p>During the development of new small modular gen IV nuclear reactors, promising nuclear fuels such as uranium nitride (UN) are investigated. Radiation damage and its effects on the fuel material have often been investigated experimentally through controlled irradiation methods. The aim of this thesis is to modify and develop existing Molecular Dynamics simulation methods for use in UN irradiation simulations and investigate its viability for such use. The present work primarily used the CRA to simulate irradiation damage and conducted Wigner-Seitz analysis to find defects. Pressure data for different probability values converged to approximately 150 kPa at a dose of 1 dpa and simulations indicated volumetric swelling of around 6% to 7%, suggesting that microstructural swelling due to defect accumulation could explain the experimental observations of cracking. Cluster analysis reveals that interstitial clusters increase to a peak, after which their number decreases towards a steady state, while DXA analysis returned a handful of dislocation lines at 1d pa for both the uranium sublattice as well as for the nitrogen sublattice.</p>
----------------------------------------------------------------------
In diva2:1881336 abstract is:
<p>Understanding the behavior of viscous incompressible fluids is essential for scientificapplications, yet when modeling them presents significant theoretical and practical chal-lenges. This study aimed to develop a numerical solver especially for the two-dimensionalNavier-Stokes equation, tailored for modeling the dynamics of a viscous incompressiblefluid, to conserve the enstrophy. The goal was to accurately simulate a physical sys-tem, and apply numerical methods such as Runge-Kutta 4, Forward Euler’s method,and pseudo-spectral methods to construct and solve the governing Partial DifferentialEquations (PDEs).</p><p>These methods were evaluated for their ability to conserve the enstrophy. Not onlyenhancing our understanding of the application of the equation in real physical systems,this research also contributes to expanding the understanding of numerical methodologiesfor complicated PDEs in physical simulations.</p><p>Using the aforementioned methods, together with strategically specific initial condi-tions, it is observable that the methods are sufficient for conserving the enstrophy whendealing with only the linear part of Navier-Stokes. To improve the numerical methodsconcerning the non-linear part of the Navier-Stokes, a perturbation method was imple-mented. Outcomes from this method appear promising however, implementation andmore detailed analysis are not included in this report due to time constraints. Thisrecovery strategy represents a foundation for further exploration in further research.</p>

corrected abstract:
<p>Understanding the behavior of viscous incompressible fluids is essential for scientific applications, yet when modeling them presents significant theoretical and practical challenges. This study aimed to develop a numerical solver especially for the two-dimensional Navier-Stokes equation, tailored for modeling the dynamics of a viscous incompressible fluid, to conserve the enstrophy. The goal was to accurately simulate a physical system, and apply numerical methods such as Runge-Kutta 4, Forward Euler’s method, and pseudo-spectral methods to construct and solve the governing Partial Differential Equations (PDEs).</p><p>These methods were evaluated for their ability to conserve the enstrophy. Not only enhancing our understanding of the application of the equation in real physical systems, this research also contributes to expanding the understanding of numerical methodologies for complicated PDEs in physical simulations.</p><p>Using the aforementioned methods, together with strategically specific initial conditions, it is observable that the methods are sufficient for conserving the enstrophy when dealing with only the linear part of Navier-Stokes. To improve the numerical methods concerning the non-linear part of the Navier-Stokes, a perturbation method was implemented. Outcomes from this method appear promising however, implementation and more detailed analysis are not included in this report due to time constraints. This recovery strategy represents a foundation for further exploration in further research.</p>
----------------------------------------------------------------------
In diva2:1875954 abstract is:
<p>This thesis addresses the challenge of denoising microscopy images captured under low-light conditionswith varying intensity levels. The study compares three deep learning models — N2V, CARE, andRCAN — against the collaborative filter BM4D, which serves as a reference point. The models weretrained on two distinct datasets: Endoplasmic Reticulum and Mitochondria datasets, both acquired witha lattice light-sheet microscope.Results show that BM4D maintains stable performance metrics and delivers superior visual quality,when compared to the noisy input. In contrast, the deep learning models exhibit poor performance onnoisy test images when trained on datasets with non-uniform noise levels. Additionally, a sensitivitycomparison of neural parameter between the same models was made. Revealing that supervised modelsare data-specific to some extent, whereas the self-supervised N2V demonstrates consistent neuralparameters, suggesting lower data specificity.</p>

corrected abstract:
<p>This thesis addresses the challenge of denoising microscopy images captured under low-light conditions with varying intensity levels. The study compares three deep learning models — N2V, CARE, and RCAN — against the collaborative filter BM4D, which serves as a reference point. The models were trained on two distinct datasets: Endoplasmic Reticulum and Mitochondria datasets, both acquired with a lattice light-sheet microscope.</p><p>Results show that BM4D maintains stable performance metrics and delivers superior visual quality, when compared to the noisy input. In contrast, the deep learning models exhibit poor performance on noisy test images when trained on dataosets with non-uniform noise levels. Additionally, a sensitivity comparison of neural parameter between the same models was made. Revealing that supervised models are data-specific to some extent, whereas the self-supervised N2V demonstrates consistent neural parameters, suggesting lower data specificity.</p>
----------------------------------------------------------------------
In diva2:1799867 abstract is:
<p>This master’s thesis explores the possibility of calculating and using the pressure andvelocity fields responsible for acoustophoresis. The goal was to use simulated data,similar to two-dimensional particle tracks from a specific microfluidic platform, toestimate the force potential and solve the partial differential equation (PDE) thatgoverns the relationship between the force potential and the acoustic fields. Lastly,the possibility of identifying the mechanical properties of unknown particles, once theacoustic field was known, was investigated. The thesis found that solving the PDE usingthe finite difference method was likely not possible and an alternative method has beensuggested. It also found that the use of particle tracks to measure the compressibilityand density of cells as a biomarker is promising, as most simulated particles wereaccurately measured.</p>

corrected abstract:
<p>This master’s thesis explores the possibility of calculating and using the <em>pressure and velocity fields</em> responsible for <em>acoustophoresis</em>. The goal was to use simulated data, similar to two-dimensional <em>particle tracks</em> from a specific <em>microfluidic</em> platform, to estimate the <em>force potential</em> and solve the <em>partial differential equation</em> (PDE) that governs the relationship between the force potential and the <em>acoustic fields</em>. Lastly, the possibility of identifying the mechanical properties of unknown particles, once the acoustic field was known, was investigated. The thesis found that solving the PDE using the <em>finite difference method</em> was likely not possible and an alternative method has been suggested. It also found that the use of particle tracks to measure the <em>compressibility</em> and <em>density</em> of cells as a <em>biomarker</em> is promising, as most simulated particles were accurately measured.</p>
----------------------------------------------------------------------
In diva2:1791909 abstract is:
<p>This thesis presents an estimator to assist or replace a fighter aircraft’s air datasystem (ADS). The estimator is based on machine learning and LSTM neuralnetworks and uses the statistical correlation between states to estimate the angleof attack, angle of sideslip and Mach number using only the internal sensorsof the aircraft. The model is trained and extensively tested on a fighter jetsimulation model and shows promising results. The methodology and accuracyof the estimator are discussed, together with how a real-world implementationwould work. The estimators presented should act as a proof of concept of thepower of neural networks in state estimation, whilst the report discusses theirstrengths and weaknesses. The estimators can estimate the three targets wellin a vast envelope of altitudes, speeds, winds and manoeuvres. However, thetechnology is quite far from real-world implementation as it lacks transparencybut shows promising potential for future development.</p>

corrected abstract:
<p>This thesis presents an estimator to assist or replace a fighter aircraft’s air data system (ADS). The estimator is based on machine learning and LSTM neural networks and uses the statistical correlation between states to estimate the angle of attack, angle of sideslip and Mach number using only the internal sensors of the aircraft. The model is trained and extensively tested on a fighter jet simulation model and shows promising results. The methodology and accuracy of the estimator are discussed, together with how a real-world implementation would work. The estimators presented should act as a proof of concept of the power of neural networks in state estimation, whilst the report discusses their strengths and weaknesses. The estimators can estimate the three targets well in a vast envelope of altitudes, speeds, winds and manoeuvres. However, the technology is quite far from real-world implementation as it lacks transparency but shows promising potential for future development.</p>
----------------------------------------------------------------------
In diva2:1774381 abstract is:
<p>Calcium ions are one of the most versatile signalling molecules. They are essential tothe proper functioning of various cellular processes in many different types of cells.The calcium signals have been studied in the past using ratiometric dyes like Fura-2.We showed that the genetically encoded calcium indicator GCaMP6m displays highersignal to noise ratio than Fura Red and therefore used it to study the calcium cytosolicconcentration in MDCK II cells. We use fluorescence microscopy to record and Pythonto analyse calcium signals in individual cells. Cellpose was used for automating thesegmentation. The cells were treated with ouabain, a cardiotonic steroid shown toincrease the intercellular communication between cells through gap junctions. Thecells were also transfected with connexins 43. We showed that ouabain does nothave an impact on the number of calcium peaks. We observed higher correlationsin the calcium between adjaçent transfected cells, but the results are not statisticallysignificant. We also observed clearly defined oscillations in one low confluencerecording with a period of around two minutes.</p>

corrected abstract:
<p>Calcium ions are one of the most versatile signalling molecules. They are essential to the proper functioning of various cellular processes in many different types of cells. The calcium signals have been studied in the past using ratiometric dyes like Fura-2. We showed that the genetically encoded calcium indicator GCaMP6m displays higher signal to noise ratio than Fura Red and therefore used it to study the calcium cytosolic concentration in MDCK II cells. We use fluorescence microscopy to record and Python to analyse calcium signals in individual cells. Cellpose was used for automating the segmentation. The cells were treated with ouabain, a cardiotonic steroid shown to increase the intercellular communication between cells through gap junctions. The cells were also transfected with connexins 43. We showed that ouabain does not have an impact on the number of calcium peaks. We observed higher correlations in the calcium between adjaçent transfected cells, but the results are not statistically significant. We also observed clearly defined oscillations in one low confluence recording with a period of around two minutes.</p>
----------------------------------------------------------------------
In diva2:1770544 abstract is:
<p>This thesis presents an analysis of misidentified leptons in the Higgs boson decaychannel H → W W ∗ → lνlν. Misidentified leptons, resulting from jets misidentifiedas leptons, mimic the signal of a Higgs boson decay, resulting in a backgroundcontribution to the signal. The analysis is performed on proton-proton collisions ata center-of-mass energy of 13.6 TeV recorded by the ATLAS experiment at CERN’sLarge Hadron Collider. The estimation of misidentified leptons is done using theso-called Fake Factor method, which is used to assess the contributed backgroundwith misidentified leptons in the signal region. The Fake Factor values increasewith increasing momenta for misidentified electrons while it remains constant formisidentified muons. An analysis of the impact parameters d0 and z0 show thatthe accuracy of the Monte Carlo simulation to correctly predict the contribution ofmisidentified leptons from heavy quarks is high.</p>

corrected abstract:
<p>This thesis presents an analysis of misidentified leptons in the Higgs boson decay channel H → WW<sup>∗</sup> → lνlν. Misidentified leptons, resulting from jets misidentified as leptons, mimic the signal of a Higgs boson decay, resulting in a background contribution to the signal. The analysis is performed on proton-proton collisions at a center-of-mass energy of 13.6 TeV recorded by the ATLAS experiment at CERN’s Large Hadron Collider. The estimation of misidentified leptons is done using the so-called Fake Factor method, which is used to assess the contributed background with misidentified leptons in the signal region. The Fake Factor values increase with increasing momenta for misidentified electrons while it remains constant for misidentified muons. An analysis of the impact parameters <em>d<sub>0</sub></e> and <em>z<sub>0</sub></em> show that the accuracy of the Monte Carlo simulation to correctly predict the contribution of misidentified leptons from heavy quarks is high.</p>
----------------------------------------------------------------------
In diva2:1679050 abstract is:
<p>Clean, drinkable water is nowadays taken for granted in most developed coun-tries. However, over two billion people in the world do not have access to drink-ing water. In an attempt to combat this, capacitive deionization (CDI) hasgained increased attention in recent years. CDI is an emergent method of de-salination through separation of ionic species in aqueous solutions. The perfor-mance of CDI is dependent on materials used and how the device is constructed.This paper investigates key metrics relating the efficiency and applicability oftwo different CDI materials, activated carbon (Zorflex FM10 Chemivron) andCobalt Prussian Blue Analogue (referred to as the active material), in regardsto the electrodes used. These metrics include energy consumption, energy re-covery and Faradaic efficiency. The results were gathered from building a circuitwith the CDI cell as the capacitor and switching the polarity of the cell when adefined threshold of the voltage (1.5 V) was reached. The energy consumptionof the activated carbon (0.450 kWh/m3) was found to be less than that of theactive material (1.45 kWh/m3). The energy recovery was found to be roughlyequal for both materials, 80.6 % for the activated carbon and 79.5 % for theactive material. Finally, the activated carbon had a Faradaic efficiency of 0.75while the active material had 1.8.</p>

corrected abstract:
<p>Clean, drinkable water is nowadays taken for granted in most developed countries. However, over two billion people in the world do not have access to drinking water [1]. In an attempt to combat this, capacitive deionization (CDI) has gained increased attention in recent years. CDI is an emergent method of desalination through separation of ionic species in aqueous solutions. The performance of CDI is dependent on materials used and how the device is constructed. This paper investigates key metrics relating the efficiency and applicability of two different CDI materials, activated carbon (Zorflex FM10 Chemivron) and Cobalt Prussian Blue Analogue (referred to as the active material), in regards to the electrodes used. These metrics include energy consumption, energy recovery and Faradaic efficiency. The results were gathered from building a circuit with the CDI cell as the capacitor and switching the polarity of the cell when a defined threshold of the voltage (1.5 V) was reached. The energy consumption of the activated carbon (0.450 kWh/m<sup>3</sup>) was found to be less than that of the active material (1.45 kWh/m<sup>3</sup>). The energy recovery was found to be roughly equal for both materials, 80.6 % for the activated carbon and 79.5 % for the active material. Finally, the activated carbon had a Faradaic efficiency of 0.75 while the active material had 1.8.</p>
----------------------------------------------------------------------
In diva2:1656073 abstract is:
<p>This study aims to see if it is possible to generate abnormal returns in the Swedishstock market through the use of three different trading strategies based on technicalindicators. As the indicators are based on historical price data only, the study assumesweak market efficiency according to the efficient market hypothesis. The study isconducted using daily prices for OMX Stockholm PI and STOXX 600 Europe from theperiod between 1 January 2010 and 31 December 2019. Trading positions has beentaken in the OMX Stockholm PI index while STOXX 600 Europe has been used torepresent the market portfolio. Abnormal returns has been defined as the Jensen’s αin a Fama French three factor model with Carhart ­extension. This period has beencharacterised by increasing prices (a bull market) which may have had an impact onthe results. Furthermore, a higher frequency of rebalancing for the Fama ­French andCarhart model could also increase the quality of the results. The results indicate thatall three strategies has generated abnormal returns during the period.</p>

corrected abstract:
<p>This study aims to see if it is possible to generate abnormal returns in the Swedish stock market through the use of three different trading strategies based on technical indicators. As the indicators are based on historical price data only, the study assumes weak market efficiency according to the efficient market hypothesis. The study is conducted using daily prices for OMX Stockholm PI and STOXX 600 Europe from the period between 1 January 2010 and 31 December 2019. Trading positions has been taken in the OMX Stockholm PI index while STOXX 600 Europe has been used to represent the market portfolio. Abnormal returns has been defined as the <em>Jensen’s α</em> in a Fama French three factor model with Carhart­extension. This period has been characterised by increasing prices (a <em>bull market</em>) which may have had an impact on the results. Furthermore, a higher frequency of rebalancing for the Fama­French and Carhart model could also increase the quality of the results. The results indicate that all three strategies has generated abnormal returns during the period.</p>
----------------------------------------------------------------------
In diva2:1571710 abstract is:
<p>In this thesis, I demonstrate a single­photon Light Detection And Ranging, (LiDAR)system operating at 1550 nm capable of reconstructing 3D environments live withmm resolution at a rate of 400 points per second using eye­safe laser pulses. Thesystem was built using off­-the-­shelf optical components and analysis was performedusing open-­source software. I utilise a single superconducting nanowire single photondetector (SNSPD) with 19 ps time jitter and 85% detection efficiency to achieve a 4 psdepth resolution in live measurements. I also show that by performing slightly moretime costly post analysis of the data it is possible to increase the details and smoothnessof the images.</p><p>Furthermore, I show that the same LiDAR system and much of the algorithms usedfor 3D LiDAR can be used to perform Optical Time Domain reflectrometry (OTDR)measurements. I demonstrate that the system can identify interfaces between differentrefractive mediums such as fibre to fibre or fibre to air couplings with a depth resolutionof 9 mm along a single line. Using these reflections, I also show that the systemcan identify flaws in optical fibres as well as measure certain characteristics suchas absorption coefficient due to Rayleigh scattering or thermal expansion. Lastly, Idemonstrate that the same OTDR principles used in fibres can be applied to free­s-paceoptical setups and that the system can identify specific optical elements as well asmeasure the quality of the alignment of an optical system.</p>

corrected abstract:
<p>In this thesis, I demonstrate a single­photon <em>Light Detection And Ranging</em>, (LiDAR) system operating at 1550 nm capable of reconstructing 3D environments live with mm resolution at a rate of 400 points per second using eye­safe laser pulses. The system was built using off-the-shelf optical components and analysis was performed using open-source software. I utilise a single <em>superconducting nanowire single photon detector</em> (SNSPD) with 19 ps time jitter and 85% detection efficiency to achieve a 4 ps depth resolution in live measurements. I also show that by performing slightly more time costly post analysis of the data it is possible to increase the details and smoothness of the images.</p><p>Furthermore, I show that the same LiDAR system and much of the algorithms used for 3D LiDAR can be used to perform <em>Optical Time Domain reflectrometry</em> (OTDR) measurements. I demonstrate that the system can identify interfaces between different refractive mediums such as fibre to fibre or fibre to air couplings with a depth resolution of 9 mm along a single line. Using these reflections, I also show that the system can identify flaws in optical fibres as well as measure certain characteristics such as absorption coefficient due to Rayleigh scattering or thermal expansion. Lastly, I demonstrate that the same OTDR principles used in fibres can be applied to free-space optical setups and that the system can identify specific optical elements as well as measure the quality of the alignment of an optical system.</p>
----------------------------------------------------------------------
In diva2:1571214 abstract is:
<p>Drinkable water is a necessary resource for human beings, unfortunately thesupply is not satisfying the increasing demand. Recent requirement for economicallysustainable and energy conservative desalination of water have revived the interestof a method first mentioned in the 1960s, know as “electrochemical demineralization”.Today is the method referred to as ”Capacitive Deionization” or CDI for short,and there are commercial solutions emerging on the market. The adoption isstill in the early stage and faces engineering challenges.The experiment performed in this thesis tries to quantify the performanceof CDI when optimizing the water consumption, but encounter problems withthe experimental setup. The problems affected the results negative such thatthe posed problem statement could not be reliable answered. However, theproblems are related to first time execution of the experiment and providesvaluable insight to successfully execute a identical or similar experiment.</p>

corrected abstract:
<p>Drinkable water is a necessary resource for human beings, unfortunately the supply is not satisfying the increasing demand. Recent requirement for economically sustainable and energy conservative desalination of water have revived the interest of a method first mentioned in the 1960s, know as “electrochemical demineralization”. Today is the method referred to as ”Capacitive Deionization” or CDI for short, and there are commercial solutions emerging on the market. The adoption is still in the early stage and faces engineering challenges. The experiment performed in this thesis tries to quantify the performance of CDI when optimizing the water consumption, but encounter problems with the experimental setup. The problems affected the results negative such that the posed problem statement could not be reliable answered. However, the problems are related to first time execution of the experiment and provides valuable insight to successfully execute a identical or similar experiment.</p>
----------------------------------------------------------------------
In diva2:1229878 abstract is:
<p>Circular dichroism (CD) spectroscopy, exploiting the wavelength-dependentdifferential absorption of left- and right-handed circularly polarized light, isa popular method of protein characterization. Theoretically computed CDspectra from quantum mechanical computer models of peptides can widen theapplicability of the method. In this work, the usefulness of Time-DependentDensity Functional Theory (TD-DFT) with the CAM-B3LYP functional and6-31+G(d) basis set in obtaining CD spectra of model alanine α-helices 3to 15 residues long is investigated.  It is found that including 10 excitedstates per residue in the TD-DFT calculation resolves the characteristic partof the spectra sufficiently.  However, the results suffer from blueshift andimproper weakness of the 222 nm peak of the characteristic α-helix spectracorresponding to the n → π∗ transition, despite extension of the basis set to6-311++G(d,p) and use of the Polarizable Dielectric Continuum Model totreat solvent effects. In this case, these issues are limiting the usefulness ofTD-DFT for prediction of peptide CD spectra. More advanced methods totreat solvent interaction and benchmarking the performance of the functionalwith higher-level ab initio methods are suggestions for future studies.</p><p>A introduction to electronic structure theory and the use of time-dependentperturbation theory to treat spectroscopy is also given in this work.</p>

corrected abstract:
<p>Circular dichroism (CD) spectroscopy, exploiting the wavelength-dependent differential absorption of left- and right-handed circularly polarized light, is a popular method of protein characterization. Theoretically computed CD spectra from quantum mechanical computer models of peptides can widen the applicability of the method. In this work, the usefulness of Time-Dependent Density Functional Theory (TD-DFT) with the CAM-B3LYP functional and 6-31+G(d) basis set in obtaining CD spectra of model alanine α-helices 3 to 15 residues long is investigated.  It is found that including 10 excited states per residue in the TD-DFT calculation resolves the characteristic part of the spectra sufficiently.  However, the results suffer from blueshift and improper weakness of the 222 nm peak of the characteristic α-helix spectra corresponding to the <em>n → π<sup>∗</sup></em> transition, despite extension of the basis set to 6-311++G(d,p) and use of the Polarizable Dielectric Continuum Model to treat solvent effects. In this case, these issues are limiting the usefulness of TD-DFT for prediction of peptide CD spectra. More advanced methods to treat solvent interaction and benchmarking the performance of the functional with higher-level <em>ab initio</e> methods are suggestions for future studies.</p><p>A introduction to electronic structure theory and the use of time-dependent perturbation theory to treat spectroscopy is also given in this work.</p>
----------------------------------------------------------------------
In diva2:1219135 abstract is:
<p>With computers being used for more applications where commands can be spoken it is useful to findalgorithms which can separate voices from each other so that software can turn spoken words intocommands. In this paper our goal is to describe how Independent Component Analysis (ICA) can beused for separation of voices in cases where we have at least the same number of microphones, atdifferent distances from the speakers, as speakers whose voices we wish to separate, the so called``cocktail party problem". This is done by implementing an ICA algorithm on voice recordingscontaining multiple persons and examining the results. The use of both ICA algorithms result in aclear separation of voices, the advantage of fastICA is that the computations take a fraction of thetime needed for the ML-ICA. Both algorithms can also successfully separate voices when recordingsare made by more microphones than speakers. The algorithms were also able to separate some ofthe voices when there were fewer microphones than speakers which was surprising as thealgorithms have no theoretical guarantee for this.</p>

corrected abstract:
<p>With computers being used for more applications where commands can be spoken it is useful to find algorithms which can separate voices from each other so that software can turn spoken words into commands. In this paper our goal is to describe how Independent Component Analysis (ICA) can be used for separation of voices in cases where we have at least the same number of microphones, at different distances from the speakers, as speakers whose voices we wish to separate, the so called ``cocktail party problem". This is done by implementing an ICA algorithm on voice recordings containing multiple persons and examining the results. The use of both ICA algorithms result in a clear separation of voices, the advantage of fastICA is that the computations take a fraction of the time needed for the ML-ICA. Both algorithms can also successfully separate voices when recordings are made by more microphones than speakers. The algorithms were also able to separate some of the voices when there were fewer microphones than speakers which was surprising as the algorithms have no theoretical guarantee for this.</p>
----------------------------------------------------------------------
In diva2:1216824 abstract is:
<p>The core of this project focuses on how to makeaerial vehicles fly autonomously from an initial position to agoal. This is done by making a mathematical model for the UAV,a brief study of the sensors needed to estimate the UAVs state,then designing an LQR controller for the trajectory trackingand finally using an artificial potential field function for thenavigation. The mathematical model is done by studying thekinematics and dynamics for a single UAV, it is then linearisedand the system’s observability and controllability are checked todevelop the LQR. We conduct computer simulations to test thetheoretical findings and evaluate the proposed methods. Finally,we conclude the paper with a discussion and results, and providedirections and ideas to do further research on the topic.</p>

corrected abstract:
<p>The core of this project focuses on how to make aerial vehicles fly autonomously from an initial position to a goal. This is done by making a mathematical model for the UAV, a brief study of the sensors needed to estimate the UAVs state, then designing an LQR controller for the trajectory tracking and finally using an artificial potential field function for the navigation. The mathematical model is done by studying the kinematics and dynamics for a single UAV, it is then linearised and the system’s observability and controllability are checked to develop the LQR. We conduct computer simulations to test the theoretical findings and evaluate the proposed methods. Finally, we conclude the paper with a discussion and results, and provide directions and ideas to do further research on the topic.</p>
----------------------------------------------------------------------
In diva2:1216812 abstract is:
<p>This bachelor's degree project is about designing an airplane for high-altitude rocket launch.The rocket shall carry the satellite PICARD weighing 143 kg and will be fired up to low earthorbit (LEO).The aircraft's payload, the mass of the rocket, is three tons. The rocket is separated above theBaltic Sea, therefore, a range of 600 𝑘𝑚 and 30 minute endurance is required. For the rocketto reach the low earth orbit as quickly as possible, it is released 10 𝑘𝑚 above sea level.The aircraft is designed to mimic and perform as a fighter aircraft with high maneuverability.Therefore, the aircraft is assumed to have a climb rate of 2 km/min as well as take-off andlanding distance 460 m and 700 m respectively. The airplane cruise speed is set to Mach 0.9.Stall speed is assumed to be 200 km/h.This resulted in a total airplane mass of 19 𝑡𝑜𝑛𝑠, 66.2 𝑚2 wing reference area, 14 mwingspan, tail area of 13.8 𝑚2 and 24.1 𝑚2 respectively. The engines of the aircraft resultedin 2x F404-GE-402 turbofan jet engines manufactured by General Electric and giving theaircraft 158 kN propulsion.</p>

corrected abstract:
<p>This bachelor's degree project is about designing an airplane for high-altitude rocket launch. The rocket shall carry the satellite PICARD weighing 143 kg and will be fired up to low earth orbit (LEO).</p><p>The aircraft's payload, the mass of the rocket, is three tons. The rocket is separated above the Baltic Sea, therefore, a range of 600 𝑘𝑚 and 30 minute endurance is required. For the rocket to reach the low earth orbit as quickly as possible, it is released 10 𝑘𝑚 above sea level.</p><p>The aircraft is designed to mimic and perform as a fighter aircraft with high maneuverability. Therefore, the aircraft is assumed to have a climb rate of 2 km/min as well as take-off and landing distance 460 m and 700 m respectively. The airplane cruise speed is set to Mach 0.9. Stall speed is assumed to be 200 km/h.</p><p>This resulted in a total airplane mass of 19 𝑡𝑜𝑛𝑠, 66.2 𝑚<sup>2</sup> wing reference area, 14 m wingspan, tail area of 13.8 𝑚<sup>2</sup> and 24.1 𝑚<sup>2</sup> respectively. The engines of the aircraft resulted in 2x F404-GE-402 turbofan jet engines manufactured by General Electric and giving the aircraft 158 kN propulsion.</p>

Note: A number of the units use characters from the Unicode Mathematical Alphanumeric Symbols block.
----------------------------------------------------------------------
In diva2:1216708 abstract is:
<p>In this project, we aim to find a method for obtainingthe factors in a bull/bear market factor model for asset returnand variance, given an optimal portfolio. The proposed methodwas derived using the Karush-Kuhn-Tucker (KKT) conditionsfor optimal solutions to the convex Markowitz portfolio selectionproblem. For synthetic data where all necessary parameters wereknown exactly, the method could give bounds on the factors. Theexact values of the factors were obtained when short selling wasallowed, and in some instances when short selling was forbidden.The method was evaluated on real-world data with varyingresults, possibly due to estimation errors and invalid assumptionsabout the model of the investor.I. INTRODUC</p>

corrected abstract:
<p>In this project, we aim to find a method for obtaining the factors in a bull/bear market factor model for asset return and variance, given an optimal portfolio. The proposed method was derived using the Karush-Kuhn-Tucker (KKT) conditions for optimal solutions to the convex Markowitz portfolio selection problem. For synthetic data where all necessary parameters were known exactly, the method could give bounds on the factors. The exact values of the factors were obtained when short selling was allowed, and in some instances when short selling was forbidden. The method was evaluated on real-world data with varying results, possibly due to estimation errors and invalid assumptions about the model of the investor.</p>
----------------------------------------------------------------------
In diva2:1188275 abstract is:
<p>Reinforced rubber is thanks to its elastic and dissipative properties found in industrialapplications such as isolators, flexible joints and tires. Its dissipative propertied comes from material related losses which have the effect that energy invested when deforming the material is not retained when returning it back to its initial state. The materiallosses are in turn caused by interactions in the material on a level below the micro scale.These interaction forms a macro stress strain response that is dependent on both strainamplitude, strain rate and temperature.It is thus a challenge to accurately model components made of reinforced rubber andand features of interest related to them, such as the rolling resistance for a tire. It is also difficult to device general design guide lines for such components due to rubbers many and complex dependencies and a simple accurate phenomenological model for modeling these properties are highly sought for in industry today.This thesis presents a method for modeling the strain amplitude and strain rate behavior for cyclically loaded rubber along with a method of choosing its material parameters.The proposed modeling technique results in a model with the same frequencydependency over all strain rates. An approximation which is shown to be valid over a few decades of strain amplitudes and rates and is believed useful for many industrialapplications. The material model presented can in addition be implemented in commercial FEsoftwares by using only pre-defined material models. This was achieved by implementationof the overlay method. The thesis also presents a method for how to implement the modeling technique in simulations with purpose to determine the rolling resistance of a truck tyre.</p>

corrected abstract:
<p>Reinforced rubber is thanks to its elastic and dissipative properties found in industrial applications such as isolators, flexible joints and tires. Its dissipative propertied comes from material related losses which have the effect that energy invested when deforming the material is not retained when returning it back to its initial state. The material losses are in turn caused by interactions in the material on a level below the micro scale. These interaction forms a macro stress strain response that is dependent on both strain amplitude, strain rate and temperature.</p><p>It is thus a challenge to accurately model components made of reinforced rubber and and features of interest related to them, such as the rolling resistance for a tire. It is also difficult to device general design guide lines for such components due to rubbers many and complex dependencies and a simple accurate phenomenological model for modeling these properties are highly sought for in industry today.</p><p>This thesis presents a method for modeling the strain amplitude and strain rate behavior for cyclically loaded rubber along with a method of choosing its material parameters. The proposed modeling technique results in a model with the same frequency dependency over all strain rates. An approximation which is shown to be valid over a few decades of strain amplitudes and rates and is believed useful for many industrial applications.</p><p>The material model presented can in addition be implemented in commercial FE-softwares by using only pre-defined material models. This was achieved by implementation of the overlay method. The thesis also presents a method for how to implement the modeling technique in simulations with purpose to determine the rolling resistance of a truck tyre.</p>
----------------------------------------------------------------------
In diva2:1142926 abstract is:
<p>In this paper deuterium abundances in plasmaenclosing wall materials are analyzed and visualized. The aim isto improve and find new ways of presenting counting statistics.A continuous extension of the sample variance is introducedto incorporate depth resolution when visualizing concentrationas function of depth in a continuous manner. Some ideas ofdesigning measurements of deuterium are discussed, in particularchoosing ion beam energies. By using multiple energies of thehelium-3 beam instead of one, more detector data can be used.With diverse ways of visualizing concentrations derived fromoptimal detector data, one can conveniently quantify deuteriumconcentrations in a surface.</p>

corrected abstract:
<p>In this paper deuterium abundances in plasma enclosing wall materials are analyzed and visualized. The aim is to improve and find new ways of presenting counting statistics. A continuous extension of the sample variance is introduced to incorporate depth resolution when visualizing concentration as function of depth in a continuous manner. Some ideas of designing measurements of deuterium are discussed, in particular choosing ion beam energies. By using multiple energies of the helium-3 beam instead of one, more detector data can be used. With diverse ways of visualizing concentrations derived from optimal detector data, one can conveniently quantify deuterium concentrations in a surface.</p>
----------------------------------------------------------------------
In diva2:1120571 abstract is:
<p>With a recent interest in quantum computers, the properties of quantum mechanicalcounterparts to classical algorithms have been studied in the hope of providing efficientalgorithms for quantum computers. Because of the success of classical random walks inproviding good algorithms on classical computers, attention has been turned to quantumrandom walks, since they may similarly be used to construct efficient probabilisticalgorithms on quantum computers. In this thesis we examine properties of the quantumwalk on the line, in particular the standard deviation and the shape of the probabilitydistribution, and the effect of potentials perturbing the walk. We model these potentialsas rectangular barriers between the walker’s positions and introduce a probability of thewalker failing to perform the step procedure, similar to that of Wong in Ref. [14]. We findthat a potential localized around the starting position leads to an increased standard deviationand makes the walk increasingly ballistic. We also find that uniformly distributedrandom potentials have the general effect of localizing the distribution, similar to that ofAnderson localization.</p>

corrected abstract:
<p>With a recent interest in quantum computers, the properties of quantum mechanical counterparts to classical algorithms have been studied in the hope of providing efficient algorithms for quantum computers. Because of the success of classical random walks in providing good algorithms on classical computers, attention has been turned to quantum random walks, since they may similarly be used to construct efficient probabilistic algorithms on quantum computers. In this thesis we examine properties of the quantum walk on the line, in particular the standard deviation and the shape of the probability distribution, and the effect of potentials perturbing the walk. We model these potentials as rectangular barriers between the walker’s positions and introduce a probability of the walker failing to perform the step procedure, similar to that of Wong in Ref. [14]. We find that a potential localized around the starting position leads to an increased standard deviation and makes the walk increasingly ballistic. We also find that uniformly distributed random potentials have the general effect of localizing the distribution, similar to that of Anderson localization.</p>
----------------------------------------------------------------------
In diva2:1120371 abstract is:
<p>In this thesis we study gravitational waves in the domain of linearised general relativity. Wepresent the fundamental ideas and theory of general relativity, then, using the traditionalmeans of quadrupole approximation, we derive an expression for the power radiated as gravitationalwaves in a binary system. We limit ourselves to binary systems in the Newtonianlimit, and can therefore use Kepler’s laws in our calculations. We then use the derived expressionto explicitly predict the power radiated by the Sun-planet systems in our solar systemspecifically, regarding each system as a binary consisting of the planet and the Sun. Finally,we calculate the resulting orbital decay and find an expression for the time it would take forthe bodies to come into contact if gravitational radiation were the only means by which thesystem lost energy. As expected, the power radiated by gravitational waves is found to bevery small for systems in the Newtonian limit, and the corresponding time until impact isfound to be on the order of many times the age of the Universe.</p>

corrected abstract:
<p>In this thesis we study gravitational waves in the domain of linearised general relativity. We present the fundamental ideas and theory of general relativity, then, using the traditional means of quadrupole approximation, we derive an expression for the power radiated as gravitational waves in a binary system. We limit ourselves to binary systems in the Newtonian limit, and can therefore use Kepler’s laws in our calculations. We then use the derived expression to explicitly predict the power radiated by the Sun-planet systems in our solar system specifically, regarding each system as a binary consisting of the planet and the Sun. Finally, we calculate the resulting orbital decay and find an expression for the time it would take for the bodies to come into contact if gravitational radiation were the only means by which the system lost energy. As expected, the power radiated by gravitational waves is found to be very small for systems in the Newtonian limit, and the corresponding time until impact is found to be on the order of many times the age of the Universe.</p>
----------------------------------------------------------------------
In diva2:1111560 - missing space in title:
"Design and models optimisation of asailing yacht dynamic simulator"
==>
"Design and models optimisation of a sailing yacht dynamic simulator"

abstract is:
<p>This master thesis is concerned with the design and optimisation of a dynamic velocity prediction programmefor high performance sailing yachts. The simulator uses response surfaces methods, maximising the computationaleciency. Insights are given on dynamic theoretical aspects and models are discussed with respect to thestudied ships, 100 feet oshore racing trimarans. The consistency of the currently used models is assessed, andfeasible solutions and methods are proposed and implemented as solutions to the identied defaults.The simulator is subsequently used on a concrete case with the objective of assessing the eects of precise geometricalfeatures of appendages (rudders, foils and boards) on the ship dynamical stability properties. Area, extension,cant angle and tip-shaft angle in particular are studied. Tests cases are developed and multi-parametersstudies carried out. Dynamic results are compared to usual static stability criteria. Simulations show someinconsistencies of dynamic responses with the behaviour expected from the static criteria. Such processes allowbetter understanding of the design and scantling of the studied appendages.</p>

corrected abstract:
<p>This master thesis is concerned with the design and optimisation of a dynamic velocity prediction programme for high performance sailing yachts. The simulator uses response surfaces methods, maximising the computational efficiency. Insights are given on dynamic theoretical aspects and models are discussed with respect to the studied ships, 100 feet offshore racing trimarans. The consistency of the currently used models is assessed, and feasible solutions and methods are proposed and implemented as solutions to the identified defaults.</p><p>The simulator is subsequently used on a concrete case with the objective of assessing the effects of precise geometrical features of appendages (rudders, foils and boards) on the ship dynamical stability properties. Area, extension, cant angle and tip-shaft angle in particular are studied. Tests cases are developed and multi-parameters studies carried out. Dynamic results are compared to usual static stability criteria. Simulations show some inconsistencies of dynamic responses with the behaviour expected from the static criteria. Such processes allow better understanding of the design and scantling of the studied appendages.</p>
----------------------------------------------------------------------
In diva2:1109484 abstract is:
<p>Chute aerators are constructed to protect spillways from cavitation damage.They function by launching the water ow as a jet and supplying airunderneath since having air entrained into the water is an eective way tomitigate cavitation. This report looks at Bergeforsen, a dam in northernSweden, and its spillway as a basis for investigating how altering the aerator'soutlet alters its performance. Five dierent designs are tested using CFD with ANSYS Fluent 17. The designs are evaluated with regard to totalair ow, air ow distribution, duct pressure distribution, cavity length, andair concentration in water jet.It was found that designs with more even duct pressure distribution, thattransported air further toward the spillway center, had somewhat reducedtotal air supply compared to designs that released more of its air in theearly part of the duct. Consequently there appear to be a trade o betweeneectively distributing the air and providing more air ow. The currentdesign used by Bergeforsen strikes this balance quite well, better than theother tested designs.</p>

corrected abstract:
<p>Chute aerators are constructed to protect spillways from cavitation damage. They function by launching the water flow as a jet and supplying air underneath since having air entrained into the water is an effective way to mitigate cavitation. This report looks at Bergeforsen, a dam in northern Sweden, and its spillway as a basis for investigating how altering the aerator's outlet alters its performance. Five different designs are tested using CFD with ANSYS Fluent 17. The designs are evaluated with regard to total air flow, air flow distribution, duct pressure distribution, cavity length, and air concentration in water jet.</p><p>It was found that designs with more even duct pressure distribution, that transported air further toward the spillway center, had somewhat reduced total air supply compared to designs that released more of its air in the early part of the duct. Consequently there appear to be a trade off between effectively distributing the air and providing more air flow. The current design used by Bergeforsen strikes this balance quite well, better than the other tested designs.</p>
----------------------------------------------------------------------
In diva2:1083258 abstract is:
<p>In the thesis, a lifting body has been designed aiming to generate lift force for the pentacopter,called TILT LR (Long Range), at higher velocities during flights to improve the aerodynamicperformances. The configuration, which is used as the skeleton of the long range drone for upto 75 kilometers flights, is based upon a tilting system allowing the rotors to rotate around theirown axis in both pitch and roll angles. This offers the possibility to the TILT LR flying withoutany vertical excess thrust at a proper angle of attack and velocity. This new drone can be directlyapplied to missions require long flight time or cover long distance, such as Search &amp; Rescue(SAR), power lines and off-shore structures inspection, fire monitoring or surveillance.Several main CAD models have been created during the process of design and presented in thereport together with the final design. For each model in the process, CFD simulations have beenapplied to observe the behaviors of the flows around the surfaces of the body during steadyflights, followed by a brief analysis for further modification. A series of simulations withvarying velocities and angle of attack have been performed for the final design, analyzing itsperformances under different air conditions. Flight envelope of the design has been presentedalso, together with some ideas of possible further studies on the pentacopter.</p>

corrected abstract:
<p>In the thesis, a lifting body has been designed aiming to generate lift force for the pentacopter, called TILT LR (Long Range), at higher velocities during flights to improve the aerodynamic performances. The configuration, which is used as the skeleton of the long range drone for up to 75 kilometers flights, is based upon a tilting system allowing the rotors to rotate around their own axis in both pitch and roll angles. This offers the possibility to the TILT LR flying without any vertical excess thrust at a proper angle of attack and velocity. This new drone can be directly applied to missions require long flight time or cover long distance, such as Search &amp; Rescue (SAR), power lines and off-shore structures inspection, fire monitoring or surveillance.</p><p>Several main CAD models have been created during the process of design and presented in the report together with the final design. For each model in the process, CFD simulations have been applied to observe the behaviors of the flows around the surfaces of the body during steady flights, followed by a brief analysis for further modification. A series of simulations with varying velocities and angle of attack have been performed for the final design, analyzing its performances under different air conditions. Flight envelope of the design has been presented also, together with some ideas of possible further studies on the pentacopter.</p>
----------------------------------------------------------------------
In diva2:852969 abstract is:
<p>This bachelor’s thesis aims to investigate the possibility of reconstructing the traditionalJetpack to a more environmental-friendly version, powered by an electric propulsion systemand Li-Ion batteries. Using existing methods of conceptual aircraft design the geometricalcharacteristics of the Jetpack are chosen. With the chosen geometry the four forces acting onan aircraft at any given moment can be obtained as a function of time. These are latersimulated for two different approaches of starting the Jetpack with the conclusion that the bestway of starting this Jetpack, standing on the ground, is with an initial climb angle. TheJetpack then continuously levels out to reach a desired cruise height.</p><p>Different batteries are compared from two different sources and it is shown that, with thebattery having the best energy density (370 Wh/kg), the total flight time is 4.7 minutes whenusing the maximum continuous power output of the engines, at a maximum velocity of 178m/s. Another alternative is to use the optimal velocity for the highest lift-to-drag ratio of 48m/s in order to achieve optimal range, after reaching the maximum velocity. In this case theflight time can be as long as 89 minutes.</p><p>The conclusion of the project is that the conceptual design of the Jetpack is successful andthat further work is to be made in order to design and construct it within a couple of years.</p>

corrected abstract:
<p>This bachelor’s thesis aims to investigate the possibility of reconstructing the traditional Jetpack to a more environmental-friendly version, powered by an electric propulsion system and Li-Ion batteries. Using existing methods of conceptual aircraft design the geometrical characteristics of the Jetpack are chosen. With the chosen geometry the four forces acting on an aircraft at any given moment can be obtained as a function of time. These are later simulated for two different approaches of starting the Jetpack with the conclusion that the best way of starting this Jetpack, standing on the ground, is with an initial climb angle. The Jetpack then continuously levels out to reach a desired cruise height.</p><p>Different batteries are compared from two different sources and it is shown that, with the battery having the best energy density (370 Wh/kg), the total flight time is 4.7 minutes when using the maximum continuous power output of the engines, at a maximum velocity of 178 m/s. Another alternative is to use the optimal velocity for the highest lift-to-drag ratio of 48 m/s in order to achieve optimal range, after reaching the maximum velocity. In this case the flight time can be as long as 89 minutes.</p><p>The conclusion of the project is that the conceptual design of the Jetpack is successful and that further work is to be made in order to design and construct it within a couple of years.</p>
----------------------------------------------------------------------
In diva2:784019 abstract is:
<p>The forces caused by the high frequency vehicle-track interaction have a great impact on thetrack maintenance. They should be represented in vehicle-track models in order to predicttheir impact. As a result, a multi-body system (MBS) should be extended with an advancedtrack model. The MBS represents the vehicle and the wheel-rail contact with a great accuracy.The track will be modeled in two different ways: a moving track model and a continuous trackmodel which is a finite element modeling (FEM). The first one will be a lumped-mass model.The most advanced system will be the second one, which is a MBS-FEM representation andoffers a great precision to represent the high frequency dynamical properties. The system willbe used to simulate pertinent phenomena such as a wheelflats, corrugations and rail joint.Based on the literature and the measurements, the model is validated in a wider frequencyrange than the one currently used (0-20Hz). The results given by both models are close tothe literature and the measurement.</p>

corrected abstract:
<p>The forces caused by the high frequency vehicle-track interaction have a great impact on the track maintenance. They should be represented in vehicle-track models in order to predict their impact. As a result, a multi-body system (MBS) should be extended with an advanced track model. The MBS represents the vehicle and the wheel-rail contact with a great accuracy. The track will be modeled in two different ways: a moving track model and a continuous track model which is a finite element modeling (FEM). The first one will be a lumped-mass model. The most advanced system will be the second one, which is a MBS-FEM representation and offers a great precision to represent the high frequency dynamical properties. The system will be used to simulate pertinent phenomena such as a wheelflats, corrugations and rail joint. Based on the literature and the measurements, the model is validated in a wider frequency range than the one currently used (0-20Hz). The results given by both models are close to the literature and the measurement.</p>
----------------------------------------------------------------------
In diva2:721675 abstract is:
<p>This study investigates the differences in calculationof exposure at default between the current exposure method (CEM) and the newstandardized approach for measuring counterparty credit risk exposures (SA-CCR)for over the counter (OTC) derivatives. The study intends to analyze theconsequence of the usage of different approaches for netting as well as the differencesin EAD between asset classes. After implementing both models and calculating EADon real trades of a Swedish commercial bank it was obvious that SA-CCR has ahigher level of complexity than its predecessor. The results from this studyindicate that SA-CCR gives a lower EAD than CEM because of the higherrecognition of netting but higher EAD when netting is not allowed. Foreignexchange derivatives are affected to a higher extent than interest ratederivatives in this particular study. Foreign exchange derivatives got lowerEAD both when netting was allowed and when netting was not allowed under SA-CCR.A change of method for calculating EAD from CEM to SA-CCR could result in lowerminimum capital requirements</p>

corrected abstract:
<p>This study investigates the differences in calculation of exposure at default between the current exposure method (CEM) and the new standardized approach for measuring counterparty credit risk exposures (SA-CCR) for over the counter (OTC) derivatives. The study intends to analyze the consequence of the usage of different approaches for netting as well as the differences in EAD between asset classes. After implementing both models and calculating EAD on real trades of a Swedish commercial bank it was obvious that SA-CCR has a higher level of complexity than its predecessor. The results from this study indicate that SA-CCR gives a lower EAD than CEM because of the higher recognition of netting but higher EAD when netting is not allowed. Foreign exchange derivatives are affected to a higher extent than interest rate derivatives in this particular study. Foreign exchange derivatives got lower EAD both when netting was allowed and when netting was not allowed under SA-CCR. A change of method for calculating EAD from CEM to SA-CCR could result in lower minimum capital requirements.</p>
----------------------------------------------------------------------
In diva2:704293 abstract is:
<p>The Rational Covariance Extension Problem is a problemin applied mathematics where one tries to find a rational spectral density thatmatches a finite covariance sequence. Applications of this can be used in areaslike speech- and image-processing. This problem has been studied intensivelyover the last decades and recently a related problem, the Circulant RationalCovariance Extension Problem, was solved. This version of the problem dealswith periodic stochastic sequences, and was shown to be a natural way toapproximate the solution to the original problem. Here we look at the specialcase when the process in question is skew-periodic, and show that also in thiscase a unique solution to the problem exists. Moreover we develop numerical solversfor both the periodic and the skew-periodic problem, and use these algorithms toapproximate the spectrum from a speech signal.</p>

corrected abstract:
<p>The Rational Covariance Extension Problem is a problem in applied mathematics where one tries to find a rational spectral density that matches a finite covariance sequence. Applications of this can be used in areas like speech- and image-processing. This problem has been studied intensively over the last decades and recently a related problem, the Circulant Rational Covariance Extension Problem, was solved. This version of the problem deals with periodic stochastic sequences, and was shown to be a natural way to approximate the solution to the original problem. Here we look at the special case when the process in question is skew-periodic, and show that also in this case a unique solution to the problem exists. Moreover we develop numerical solvers for both the periodic and the skew-periodic problem, and use these algorithms to approximate the spectrum from a speech signal.</p>
----------------------------------------------------------------------
In diva2:402293 abstract is:
<p>This thesis is concerned with the classification and effect of cold lap weld defect onthe fatigue strength of a welded structure. Cold lap is a type of weld defect whichoccurs when molten metal does not completely fuse with the cold plate surface. Thisproduces a crack like defect, often very small, which is parallel to the plate. The coldlap weld defect has been classified into three types namely spatter, overlap andspatter-overlap cold lap. Study showed that all three types of cold lap defects have thecorresponded lack of fusion in the interface, which could be considered as initialmacro cracks in different shapes where a possible fatigue crack growth could start.Fatigue life assessment of the above mentioned three types of cold lap defects wascarried out using finite element and crack growth analysis in 2D and 3D. In the 2Danalysis the cold lap defects were modeled as line crack (assuming a/c=0). Based onthe experiments the cold lap defects were visualized as having two probable crackshapes; penny shaped and part through, which required 3D crack growth analysis.Results showed that in 2D analysis the three types of cold lap defects have sameinfluence on fatigue life of the weld. In 3D analysis, the shape of the cold lap defectsdid not show any difference in fatigue life. Overall penny shaped and part throughcracks in 3D analysis predicted 1.75 times longer fatigue life as compared to line crackin 2D analysis.</p>

corrected abstract:
<p>This thesis is concerned with the classification and effect of cold lap weld defect on the fatigue strength of a welded structure. Cold lap is a type of weld defect which occurs when molten metal does not completely fuse with the cold plate surface. This produces a crack like defect, often very small, which is parallel to the plate. The cold lap weld defect has been classified into three types namely spatter, overlap and spatter-overlap cold lap. Study showed that all three types of cold lap defects have the corresponded lack of fusion in the interface, which could be considered as initial macro cracks in different shapes where a possible fatigue crack growth could start.</p><p>Fatigue life assessment of the above mentioned three types of cold lap defects was carried out using finite element and crack growth analysis in 2D and 3D. In the 2D analysis the cold lap defects were modeled as line crack (assuming a/c=0). Based on the experiments the cold lap defects were visualized as having two probable crack shapes; penny shaped and part through, which required 3D crack growth analysis. Results showed that in 2D analysis the three types of cold lap defects have same influence on fatigue life of the weld. In 3D analysis, the shape of the cold lap defects did not show any difference in fatigue life. Overall penny shaped and part through cracks in 3D analysis predicted 1.75 times longer fatigue life as compared to line crack in 2D analysis.</p>
----------------------------------------------------------------------
In diva2:1900955 abstract is:
<p>As the performance of a rocket launcher is closely linked to its flight control system, a significantchallenge in rocket science is the design of attitude control algorithms, to ensure the stabilityof the vehicle, while following a designated trajectory and rejecting external disturbances. Thisreport aims at describing a general method for designing such a controller and finally assessfor its performance. First, a state-of-the-art review of existing attitude control methods isprovided, along with an introduction to linear control theory. Important phenomena influencingthe vehicle, including the rigid-body dynamics, aerodynamics, inertia of the engines, sloshingmodes and bending modes are then introduced. Thereafter, through the example of a given studycase, the parameters describing all of these phenomena are estimated. The linear equationsof motion are then derived and a method for constructing the state-space representation ofthe vehicle and its actuators is presented. Based on this linear model, a step-by-step methodis described to compute a stable PID controller, designed to handle all of the consideredphenomena. Finally, a performance analysis including stability, time response, sensitivity androbustness is conducted to evaluate the behavior of the controller.</p>

corrected abstract:
<p>As the performance of a rocket launcher is closely linked to its flight control system, a significant challenge in rocket science is the design of attitude control algorithms, to ensure the stability of the vehicle, while following a designated trajectory and rejecting external disturbances. This report aims at describing a general method for designing such a controller and finally assess for its performance. First, a state-of-the-art review of existing attitude control methods is provided, along with an introduction to linear control theory. Important phenomena influencing the vehicle, including the rigid-body dynamics, aerodynamics, inertia of the engines, sloshing modes and bending modes are then introduced. Thereafter, through the example of a given study case, the parameters describing all of these phenomena are estimated. The linear equations of motion are then derived and a method for constructing the state-space representation of the vehicle and its actuators is presented. Based on this linear model, a step-by-step method is described to compute a stable PID controller, designed to handle all of the considered phenomena. Finally, a performance analysis including stability, time response, sensitivity and robustness is conducted to evaluate the behavior of the controller.</p>
----------------------------------------------------------------------
In diva2:1817036 abstract is:
<p>This thesis investigates the impact of vibration isolators on circuit boards during harsh vibrationenvironments that occur when they are mounted on the wings of a fighter jet. To examine thisphenomenon, a mathematical model and a simulated model were developed to determine theresonant frequencies of the circuit board under various boundary conditions. Subsequently, theresonant frequencies of the circuit board were validated through experimental tests, allowing forthe establishment of the material properties of the circuit board. In order to prevent structuralfailure, this thesis employs α-gel dampers as the damped attachments for the circuit board.These vibration isolators belong to the category of silicone gel dampers and were evaluatedthrough experimental vibration testing. The two employed vibration isolators are denoted asmodels A1 and A2, exhibiting respective damping ratios of 0.1 and 0.05. By utilizing thesevibration isolators during the experimental vibration tests, the structure demonstrated resilienceagainst natural frequency coupling, thereby preventing failure.</p>

corrected abstract:
<p>This thesis investigates the impact of vibration isolators on circuit boards during harsh vibration environments that occur when they are mounted on the wings of a fighter jet. To examine this phenomenon, a mathematical model and a simulated model were developed to determine the resonant frequencies of the circuit board under various boundary conditions. Subsequently, the resonant frequencies of the circuit board were validated through experimental tests, allowing for the establishment of the material properties of the circuit board. In order to prevent structural failure, this thesis employs α-gel dampers as the damped attachments for the circuit board. These vibration isolators belong to the category of silicone gel dampers and were evaluated through experimental vibration testing. The two employed vibration isolators are denoted as models A1 and A2, exhibiting respective damping ratios of 0.1 and 0.05. By utilizing these vibration isolators during the experimental vibration tests, the structure demonstrated resilience against natural frequency coupling, thereby preventing failure.</p>
----------------------------------------------------------------------
In diva2:1779448 abstract is:
<p>This study investigates the performance of two keypoint detection algorithms, SIFTand LoFTR, for vehicle re-recognition on a 2+1 road in Täby, utilizing three differentmethods: proportion of matches, ”gates” based on the values of the features andSupport Vector Machines (SVM). Data was collected from four strategically placedcameras, with a subset of the data manually annotated and divided into training,validation, and testing sets to minimize overfitting and ensure generalization. TheF1-score was used as the primary metric to evaluate the performance of the variousmethods. Results indicate that LoFTR outperforms SIFT across all methods, with theSVM method demonstrating the best performance and adaptability. The findings havepractical implications in security, traffic management, and intelligent transportationsystems, and suggest directions for future research in real-time implementation andgeneralization across varied camera placements.</p>

corrected abstract:
<p>This study investigates the performance of two keypoint detection algorithms, SIFT and LoFTR, for vehicle re-recognition on a 2+1 road in Täby, utilizing three different methods: proportion of matches, ”gates” based on the values of the features and Support Vector Machines (SVM). Data was collected from four strategically placed cameras, with a subset of the data manually annotated and divided into training, validation, and testing sets to minimize overfitting and ensure generalization. The F1-score was used as the primary metric to evaluate the performance of the various methods. Results indicate that LoFTR outperforms SIFT across all methods, with the SVM method demonstrating the best performance and adaptability. The findings have practical implications in security, traffic management, and intelligent transportation systems, and suggest directions for future research in real-time implementation and generalization across varied camera placements.</p>
----------------------------------------------------------------------
In diva2:1776821 abstract is:
<p>This report considers an application of mixed-integer disjunctive programming (MIDP)where a theoretical robot can jump from one point to another and where the number ofjumps is to be minimized. The robot is only able to jump to the north, south, east andwest. Furthermore, the robot should also be able to navigate and jump around or across anypotential obstacles on the way. The algorithm for solving this problem is set to terminatewhen the robot has reached a set of end coordinates. The goal of this report is to find amethod for solving this problem and to investigate the time complexity of such a method.The problem is converted to big-M representation and solved numerically. Gurobi is theoptimization solver used in this thesis. The model created and implemented with Gurobiyielded optimal solutions to problems of the form above of varying complexity. For most ofcases tested, the time complexity appeared to be linear, but this is likely due to presolvingperformed by Gurobi before running the optimization. Further tests are needed to determinethe time complexity of Gurobi’s optimization algorithm for this specific type of problem.</p>

corrected abstract:
<p>This report considers an application of mixed-integer disjunctive programming (MIDP) where a theoretical robot can jump from one point to another and where the number of jumps is to be minimized. The robot is only able to jump to the north, south, east and west. Furthermore, the robot should also be able to navigate and jump around or across any potential obstacles on the way. The algorithm for solving this problem is set to terminate when the robot has reached a set of end coordinates. The goal of this report is to find a method for solving this problem and to investigate the time complexity of such a method. The problem is converted to big-M representation and solved numerically. Gurobi is the optimization solver used in this thesis. The model created and implemented with Gurobi yielded optimal solutions to problems of the form above of varying complexity. For most of cases tested, the time complexity appeared to be linear, but this is likely due to presolving performed by Gurobi before running the optimization. Further tests are needed to determine the time complexity of Gurobi’s optimization algorithm for this specific type of problem.</p>
----------------------------------------------------------------------
In diva2:1764270 abstract is:
<p>Among the structural materials under consideration for future lead-cooled fastreactors, special attention is paid to ferritic Fe-10Cr-4Al due to its superior corrosionand erosion protective properties, as well as its insensitivity to liquid metalembrittlement in liquid lead. This thesis gives an inside look into the radiation damageproperties of the alloy and the possible embrittlement scenarios. The samples wereirradiated with 5.5 MeV protons and then tested with a slow strain rate testing rig at375oC and 450oC. The results showed that for Fe-10Cr-4Al irradiated to a peak doseof 0.14 dpa, the total elongation to failure was reduced by 3-5%, compared to theunirradiated samples. Moreover, the mechanical properties (yield strength, ultimatetensile strength, and fracture elongation) of the irradiated samples depend stronglyon temperature. The scanning electron microscopy images show no signs of liquidmetal embrittlement. However, the brittle structures at the edges of the samples couldindicate the existence of hydrogen embrittlement.</p>

corrected abstract:
<p>Among the structural materials under consideration for future lead-cooled fast reactors, special attention is paid to ferritic Fe-10Cr-4Al due to its superior corrosion and erosion protective properties, as well as its insensitivity to liquid metal embrittlement in liquid lead. This thesis gives an inside look into the radiation damage properties of the alloy and the possible embrittlement scenarios. The samples were irradiated with 5.5 MeV protons and then tested with a slow strain rate testing rig at 375ºC and 450ºC. The results showed that for Fe-10Cr-4Al irradiated to a peak dose of 0.14 dpa, the total elongation to failure was reduced by 3-5%, compared to the unirradiated samples. Moreover, the mechanical properties (yield strength, ultimate tensile strength, and fracture elongation) of the irradiated samples depend strongly on temperature. The scanning electron microscopy images show no signs of liquid metal embrittlement. However, the brittle structures at the edges of the samples could indicate the existence of hydrogen embrittlement.</p>
----------------------------------------------------------------------
In diva2:1757046 
abstract is:
<p>The relationship between performance for the Swedish industry and changesin prices and volatility of commodities has been examined using multiple linearregression. The study focuses on how commodity price fluctuations correlate withgross profit growth, measuring company performance. Gross profit as a performancemeasure is contrary to most previous studies that use stock performance as thedependent variable. This study has found two commodities whose prices have asignificant relationship with changes in gross profit for the Swedish industry sector,Brent oil, and platinum. The correlation with Brent oil is the most reliable one.Surprisingly, Brent oil has a positive relationship with gross profit, even thougha higher oil price is causing more expensive logistics and manufacturing operations,increasing costs of sold goods. This indicates a possible correlation between oil priceand demand for manufactured products; industrial companies can either increaseprices or produce at a high capacity. Regarding the volatility of commodities, nosignificant correlation with gross profits has been found.</p>


corrected abstract:
<p>The relationship between performance for the Swedish industry and changes in prices and volatility of commodities has been examined using multiple linear regression. The study focuses on how commodity price fluctuations correlate with gross profit growth, measuring company performance. Gross profit as a performance measure is contrary to most previous studies that use stock performance as the dependent variable. This study has found two commodities whose prices have a significant relationship with changes in gross profit for the Swedish industry sector, Brent oil, and platinum. The correlation with Brent oil is the most reliable one.</p><p>Surprisingly, Brent oil has a positive relationship with gross profit, even though a higher oil price is causing more expensive logistics and manufacturing operations, increasing costs of sold goods. This indicates a possible correlation between oil price and demand for manufactured products; industrial companies can either increase prices or produce at a high capacity. Regarding the volatility of commodities, no significant correlation with gross profits has been found.</p>
----------------------------------------------------------------------
In diva2:1718318 
abstract is:
<p>The development of quantum machine learning is bridging the way to fault tolerant quantum computation by providing algorithms running on the current noisy intermediate scale quantum devices.However, it is difficult to find use-cases where quantum computers exceed their classical counterpart.The high energy physics community is experiencing a rapid growth in the amount of data physicists need to collect, store, and analyze within the more complex experiments are being conceived.Our work approaches the study of a particle physics event involving the Higgs boson from a quantum machine learning perspective.We compare quantum support vector machine with the best classical kernel method grounding our study in a new theoretical framework based on metrics observing at three different aspects: the geometry between the classical and quantum learning spaces, the dimensionality of the feature space, and the complexity of the ML models.We exploit these metrics as a compass in the parameter space because of their predictive power. Hence, we can exclude those areas where we do not expect any advantage in using quantum models and guide our study through the best parameter configurations.Indeed, how to select the number of qubits in a quantum circuits and the number of datapoints in a dataset were so far left to trial and error attempts.We observe, in a vast parameter region, that the used classical rbf kernel model overtakes the performances of the devised quantum kernels.We include in this study the projected quantum kernel - a kernel able to reduce the expressivity of the traditional fidelity quantum kernel by projecting its quantum state back to an approximate classical representation through the measurement of local quantum systems.The Higgs dataset has been proved to be low dimensional in the quantum feature space meaning that the quantum encoding selected is not enough expressive for the dataset under study.Nonetheless, the optimization of the parameters on all the kernels proposed, classical and quantum, revealed a quantum advantage for the projected kernel which well classify the Higgs boson events and surpass the classical ML model.</p>

corrected abstract:
<p>The development of quantum machine learning is bridging the way to fault tolerant quantum computation by providing algorithms running on the current noisy intermediate scale quantum devices. However, it is difficult to find use-cases where quantum computers exceed their classical counterpart. The high energy physics community is experiencing a rapid growth in the amount of data physicists need to collect, store, and analyze within the more complex experiments are being conceived. Our work approaches the study of a particle physics event involving the Higgs boson from a quantum machine learning perspective. We compare quantum support vector machine with the best classical kernel method grounding our study in a new theoretical framework based on metrics observing at three different aspects: the geometry between the classical and quantum learning spaces, the dimensionality of the feature space, and the complexity of the ML models. We exploit these metrics as a compass in the parameter space because of their predictive power. Hence, we can exclude those areas where we do not expect any advantage in using quantum models and guide our study through the best parameter configurations. Indeed, how to select the number of qubits in a quantum circuits and the number of datapoints in a dataset were so far left to trial and error attempts. We observe, in a vast parameter region, that the used classical rbf kernel model overtakes the performances of the devised quantum kernels. We include in this study the projected quantum kernel - a kernel able to reduce the expressivity of the traditional fidelity quantum kernel by projecting its quantum state back to an approximate classical representation through the measurement of local quantum systems. The Higgs dataset has been proved to be low dimensional in the quantum feature space meaning that the quantum encoding selected is not enough expressive for the dataset under study. Nonetheless, the optimization of the parameters on all the kernels proposed, classical and quantum, revealed a quantum advantage for the projected kernel which well classify the Higgs boson events and surpass the classical ML model.</p>
----------------------------------------------------------------------
In diva2:1674003 
abstract is: 
<p>The student inclusive Green Raven project of the KTH-Aero faculty requireda small blended wing model of their new flying wing design. The small scalemodel will be used for various flight tests. The goal of this specific project was tocreate an internal structure for the small scale model, including an outer shell.Two-dimensional drawings were created and tested in a simulation software.The model was then drawn in cad. Lastly the wing was strength tested inAnsys mechanical. The beams in the structure are made of Scots pine due toits accessibility and good strength to weight ratio. The outer shell is made outof fiberglass. A quick connection between the wing and the main body wasimplemented for easy transportation. All final testing indicate that the finaldesign had sufficient strength regarding the initial load requirements.</p><p> </p>

corrected abstract:
<p>The student inclusive Green Raven project of the KTH-Aero faculty required a small blended wing model of their new flying wing design. The small scale model will be used for various flight tests. The goal of this specific project was to create an internal structure for the small scale model, including an outer shell. Two-dimensional drawings were created and tested in a simulation software. The model was then drawn in cad. Lastly the wing was strength tested in Ansys mechanical. The beams in the structure are made of Scots pine due to its accessibility and good strength to weight ratio. The outer shell is made out of fiberglass. A quick connection between the wing and the main body was implemented for easy transportation. All final testing indicate that the final design had sufficient strength regarding the initial load requirements.</p>
----------------------------------------------------------------------
In diva2:1595164 
abstract is: 
<p>The electrification of automobiles has emerged as the sustainable powertrain solutionto meet United Nations sustainable development goals of sustainable cities andcommunities, affordable and clean energy, and climate action. The success of theelectrification depends on the efficiency of traction motors. Hence, the automobileindustry is dedicated to improving the performance of electrical traction machinesfor high performance and sustainability. The thesis aims to build various electricalmachine’s concept designs and quantify their behaviour on sustainability andperformance.</p><p>The thesis objective is to design Permanent Magnet Synchronous Motor (PMSM),Synchronous Reluctance Motor (SynRM), and Permanent Magnet SynchronousReluctance Motor (PM­SynRM). The thesis work comprises of accurate performanceestimation and optimisation of these electrical machines through a finite element based method. The in­house scripts are developed to estimate the performance, electrical losses, and efficiency of these electrical machines through flexible open-source tools.</p><p>The performance of PMSM with rare-­earth magnet Neodymium Ferrite Boron(NdFeB) and without rare­-earth magnet (ferrite) is done to evaluate the role of bothmagnets in producing torque density. The SynRM is evaluated and optimized usinggenetic algorithms in the thesis. The electrical machines are designed without the useof rare-­earth magnets to eliminate the degradation of the environment and reduce thecost and weight of the motor.</p>

corrected abstract:
<p>The electrification of automobiles has emerged as the sustainable powertrain solution to meet United Nations sustainable development goals of sustainable cities and communities, affordable and clean energy, and climate action. The success of the electrification depends on the efficiency of traction motors. Hence, the automobile industry is dedicated to improving the performance of electrical traction machines for high performance and sustainability. The thesis aims to build various electrical machine’s concept designs and quantify their behaviour on sustainability and performance.</p><p>The thesis objective is to design Permanent Magnet Synchronous Motor (PMSM), Synchronous Reluctance Motor (SynRM), and Permanent Magnet Synchronous Reluctance Motor (PM-SynRM). The thesis work comprises of accurate performance estimation and optimisation of these electrical machines through a finite element based method. The in­house scripts are developed to estimate the performance, electrical losses, and efficiency of these electrical machines through flexible open-source tools.</p><p>The performance of PMSM with rare-earth magnet Neodymium Ferrite Boron (NdFeB) and without rare-earth magnet (ferrite) is done to evaluate the role of both magnets in producing torque density. The SynRM is evaluated and optimized using genetic algorithms in the thesis. The electrical machines are designed without the use of rare-earth magnets to eliminate the degradation of the environment and reduce the cost and weight of the motor.</p>
----------------------------------------------------------------------
In diva2:1583515 
abstract is: 
<p>This project aimed to improve on an existing MATLAB tool used for real-time monitoring of flutterduring flight testing of the Gripen E fighter jet. Flutter is a dynamic aeroelastic phenomenon thatcauses undamped oscillations in a structure, which may lead to structural failure. Because of thepotentially catastrophic consequences of flutter in an aircraft, it must be proven, through flight fluttertesting, to be free of flutter within its flight envelope. To increase flight safety during flutter flighttesting, accelerometer data from the aircraft is monitored in real-time by flight test engineers on theground. Monitoring is done using different tools including the aforementioned MATLAB tool. Thetool was improved by increasing its robustness, correcting existing flaws, and refining its overallgraphical presentation of data. New analysis tools such as filtering of time signals, an airspeedindicator plot, and a custom data cursor were also implemented.</p>

corrected abstract:
<p>This project aimed to improve on an existing MATLAB tool used for real-time monitoring of flutter during flight testing of the Gripen E fighter jet. Flutter is a dynamic aeroelastic phenomenon that causes undamped oscillations in a structure, which may lead to structural failure. Because of the potentially catastrophic consequences of flutter in an aircraft, it must be proven, through flight flutter testing, to be free of flutter within its flight envelope. To increase flight safety during flutter flight testing, accelerometer data from the aircraft is monitored in real-time by flight test engineers on the ground. Monitoring is done using different tools including the aforementioned MATLAB tool. The tool was improved by increasing its robustness, correcting existing flaws, and refining its overall graphical presentation of data. New analysis tools such as filtering of time signals, an airspeed indicator plot, and a custom data cursor were also implemented.</p>
----------------------------------------------------------------------
In diva2:1572329 
abstract is: 
<p>Sustainable urbanisation is a pressing challenge in some parts of the world and cost-efficient and environmentally friendly building materials could become a solution to achieve sustainability. Bamboo has shown promising properties in tensile- and bending strength to be able to substitute conventional building materials. If reinforced concrete gets implemented inside the anisotropic bamboo it further increases compressive strength, raw durability and makes the material more homogeneous.This thesis report analyses and calculates the stress and deformation on these reinforced bamboo beams when used as a roof structure for a solar cell power charging station in Southeast Asia. The calculations were made in ANSYS Mechanical. Different structural designs were exposed to strong wind loads and the results were compared to optimize the usage of the materials.The results show low values of stress and deformation after implementing reinforced concrete in the whole bamboo which indicates that excessivelyreinforced concrete might have been used. The results also show that using bamboo only in the structure gives considerably higher stress and deformation values even reaching critical levels at the edges of the roof and where the roof connects with the structural pillars.By implementing reinforced concrete at only critical areas the amount of stress in the structure can be decreased to a manageable level. If this is done, no critical levels are reached and the arising stress levels in the bamboo fall below a safety factor of 3. With these results, one can argue to decrease the material usage by only using reinforced concrete at critical areas of the structure. By using the natural strength of bamboo and only complementing with concrete and steel where bamboo is weak, the overall environmental impact is kept low but also the costs for producing and transporting these materials.</p>

corrected abstract:
<p>Sustainable urbanisation is a pressing challenge in some parts of the world and cost-efficient and environmentally friendly building materials could become a solution to achieve sustainability. Bamboo has shown promising properties in tensile- and bending strength to be able to substitute conventional building materials. If reinforced concrete gets implemented inside the anisotropic bamboo it further increases compressive strength, raw durability and makes the material more homogeneous.</p><p>This thesis report analyses and calculates the stress and deformation on these reinforced bamboo beams when used as a roof structure for a solar cell power charging station in Southeast Asia. The calculations were made in ANSYS Mechanical. Different structural designs were exposed to strong wind loads and the results were compared to optimize the usage of the materials.</p><p>The results show low values of stress and deformation after implementing reinforced concrete in the whole bamboo which indicates that excessively reinforced concrete might have been used. The results also show that using bamboo only in the structure gives considerably higher stress and deformation values even reaching critical levels at the edges of the roof and where the roof connects with the structural pillars.</p><p>By implementing reinforced concrete at only critical areas the amount of stress in the structure can be decreased to a manageable level. If this is done, no critical levels are reached and the arising stress levels in the bamboo fall below a safety factor of 3. With these results, one can argue to decrease the material usage by only using reinforced concrete at critical areas of the structure. By using the natural strength of bamboo and only complementing with concrete and steel where bamboo is weak, the overall environmental impact is kept low but also the costs for producing and transporting these materials.</p>
----------------------------------------------------------------------
In diva2:1528145 - missing space in title
#Investigation and Analysis ofAircraft System Integration#
==>
#Investigation and Analysis of Aircraft System Integration#

abstract is: 
<p>Aircraft integration plays a pivotal role in the eective aggregation of varioussub-systems into a group of systems. The fundamental categorization of the aircraftintegration process points to technical and managerial aspects. The technical needsare to be managed eectively for an optimal solution. The objective of the thesiswas to investigate these two aspects as being interdependent. First, simulated theaircraft behavior at various congurations, and mapped the results with ight testdata. Subsequently, the conguration’s behavior was assessed as per certicationrequirements. Second, pre-Validation and Verication (V&amp;V) of the Stall WarningSystem (SWS) was conducted to ensure consistent design and performance as perrequirements. Third, managed one of the FCS system’s environmental tests anddeliverables schedule for its certication maturity by utilizing the Critical PathMethod (CPM).</p>

corrected abstract:
<p>Aircraft integration plays a pivotal role in the effective aggregation of various sub-systems into a group of systems. The fundamental categorization of the aircraft integration process points to technical and managerial aspects. The technical needs are to be managed effectively for an optimal solution. The objective of the thesis was to investigate these two aspects as being interdependent. First, simulated the aircraft behavior at various configurations, and mapped the results with flight test data. Subsequently, the configuration’s behavior was assessed as per certification requirements. Second, pre-Validation and Verification (V&amp;V) of the Stall Warning System (SWS) was conducted to ensure consistent design and performance as per requirements. Third, managed one of the FCS system’s environmental tests and deliverables schedule for its certification maturity by utilizing the Critical Path Method (CPM).</p>
----------------------------------------------------------------------
In diva2:1465541 
abstract is: 
<p>In this project, a chassis concept has been developed for a battery-powered autonomousvehicle. The vehicle is intended to be used at an airport for transporting people betweendifferent terminals. The objective is to develop a chassis which is anchored with modernrequirements and futuristic research based on conventional chassis design methods in orderto find an optimal solution for this specific vehicle. Literature studies have been conductedon future batteries, types of chassis, chassis materials, and optimal cross-sections. Thechassis materials have also been analyzed from an environmental perspective and life cycleanalysis (LCA). Based on this, it was found that the “skateboard” chassis model was optimalfor the intended vehicle while Advanced High Strength Steel (AHSS) proved to be the mostsuitable material for the load-bearing structure. It is essential to keep in mind that thisproject has been carried out on a conceptual level within the framework of a degree project.This master thesis project aims to provide a solid benchmark for further development andresearch within the subject.</p>

corrected abstract:
<p>In this project, a chassis concept has been developed for a battery-powered autonomous vehicle. The vehicle is intended to be used at an airport for transporting people between different terminals. The objective is to develop a chassis which is anchored with modern requirements and futuristic research based on conventional chassis design methods in order to find an optimal solution for this specific vehicle. Literature studies have been conducted on future batteries, types of chassis, chassis materials, and optimal cross-sections. The chassis materials have also been analyzed from an environmental perspective and life cycle analysis (LCA). Based on this, it was found that the “skateboard” chassis model was optimal for the intended vehicle while Advanced High Strength Steel (AHSS) proved to be the most suitable material for the load-bearing structure. It is essential to keep in mind that this project has been carried out on a conceptual level within the framework of a degree project. This master thesis project aims to provide a solid benchmark for further development and research within the subject.</p>
----------------------------------------------------------------------
In diva2:1365507 
abstract is: 
<p>Condition monitoring (CM) is widely used in industry, and there is a growing interest in applying CM on rail vehicle systems. Condition based maintenance has the possibility to increase system safety and availability while at the sametime reduce the total maintenance costs.This thesis investigates the feasibility of using condition monitoring of suspension element components, in this case dampers, in rail vehicles. There are different methods utilized to detect degradations, ranging from mathematicalmodelling of the system to pure "knowledge-based" methods, using only large amount of data to detect patterns on a larger scale. In this thesis the latter approach is explored, where acceleration signals are evaluated on severalplaces on the axleboxes, bogieframes and the carbody of a rail vehicle simulation model. These signals are picked close to the dampers that are monitored in this study, and frequency response functions (FRF) are computed between axleboxes and bogieframes as well as between bogieframes and carbody. The idea is that the FRF will change as the condition of the dampers change, and thus act as indicators of faults. The FRF are then fed to different classificationalgorithms, that are trained and tested to distinguish between the different damper faults.This thesis further investigates which classification algorithm shows promising results for the problem, and which algorithm performs best in terms of classification accuracy as well as two other measures. Another aspect explored is thepossibility to apply dimensionality reduction to the extracted indicators (features). This thesis is also looking into how the three performance measures used are affected by typical varying operational conditions for a rail vehicle,such as varying excitation and carbody mass. The Linear Support Vector Machine classifier using the whole feature space, and the Linear Discriminant Analysis classifier combined with Principal Component Analysis dimensionality reduction on the feature space both show promising results for the taskof correctly classifying upcoming damper degradations.</p>

corrected abstract:
<p>Condition monitoring (CM) is widely used in industry, and there is a growing interest in applying CM on rail vehicle systems. Condition based maintenance has the possibility to increase system safety and availability while at the same time reduce the total maintenance costs.</p><p>This thesis investigates the feasibility of using condition monitoring of suspension element components, in this case dampers, in rail vehicles. There are different methods utilized to detect degradations, ranging from mathematical modelling of the system to pure "knowledge-based" methods, using only large amount of data to detect patterns on a larger scale. In this thesis the latter approach is explored, where acceleration signals are evaluated on several places on the axleboxes, bogieframes and the carbody of a rail vehicle simulation model. These signals are picked close to the dampers that are monitored in this study, and frequency response functions (FRF) are computed between axleboxes and bogieframes as well as between bogieframes and carbody. The idea is that the FRF will change as the condition of the dampers change, and thus act as indicators of faults. The FRF are then fed to different classification algorithms, that are trained and tested to distinguish between the different damper faults.</p><p>This thesis further investigates which classification algorithm shows promising results for the problem, and which algorithm performs best in terms of classification accuracy as well as two other measures. Another aspect explored is the possibility to apply dimensionality reduction to the extracted indicators (features). This thesis is also looking into how the three performance measures used are affected by typical varying operational conditions for a rail vehicle, such as varying excitation and carbody mass. The Linear Support Vector Machine classifier using the whole feature space, and the Linear Discriminant Analysis classifier combined with Principal Component Analysis dimensionality reduction on the feature space both show promising results for the task of correctly classifying upcoming damper degradations.</p>
----------------------------------------------------------------------
In diva2:1341553 
abstract is: 
<p>The purpose of this bachelor thesis in applied mathematics and engineering physicsis to develop a model to describe the price of emissions allowances in EuropeanUnion Emission Trading Scheme, EU ETS, a scheme created to systematically re-duce carbon dioxide emissions with market forces in a free market. With the help ofstatistical regression, the core of the project is to explain the price of these emissionallowances by creating a model based on free available data from both microeconomicand macroeconomic factors in Europe. Stock indices, interest rates, electricity price,and inflation rates are examples of factors that were used. Two models were devel-oped, one with data points from mid-2010 to March 2019 and one with data pointsfrom mid-2010 until March 14, 2018, when a new directive was voted through re-garding the premises of the trading scheme. The models were developed throughstatistical methods such as identification of outliers, detection of multicollinearity,cross validation, and bootstrapping in order to create a model as suitable as pos-sible. Afterward, the model’s credibility and legitimacy were discussed, as well aspossible areas of improvement and further studies to improve the model and theunderstanding of the pricing.</p><p> </p>

corrected abstract:
<p>The purpose of this bachelor thesis in applied mathematics and engineering physics is to develop a model to describe the price of emissions allowances in European Union Emission Trading Scheme, EU ETS, a scheme created to systematically reduce carbon dioxide emissions with market forces in a free market. With the help of statistical regression, the core of the project is to explain the price of these emission allowances by creating a model based on free available data from both microeconomic and macroeconomic factors in Europe. Stock indices, interest rates, electricity price, and inflation rates are examples of factors that were used. Two models were developed, one with data points from mid-2010 to March 2019 and one with data points from mid-2010 until March 14, 2018, when a new directive was voted through regarding the premises of the trading scheme. The models were developed through statistical methods such as identification of outliers, detection of multicollinearity, cross validation, and bootstrapping in order to create a model as suitable as possible. Afterward, the model’s credibility and legitimacy were discussed, as well as possible areas of improvement and further studies to improve the model and the understanding of the pricing.</p>
----------------------------------------------------------------------
In diva2:1244654 
abstract is: 
<p>The aim of this project is to investigate accelerometer readings obtained from a modern smartphone,for use within the discipline of material mechanics. Problems that fall within the domain of classicalmechanics are addressed as well. To obtain data, the smartphone’s integrated accelerometer is used. Allmodern smartphones contain these devices. When conducting experiments, data files are created whichare then transferred to a computer for analysis.A series of experiments were conducted. The equipment for each experiment was chosen specifically forbeing common in households. The acceleration may be used directly for determining force, for obtainingvelocity and displacement upon integration, and for calculating stiffness of systems. Frequency analysisof the acceleration was carried out to find the natural frequencies of structures.The results of these experiments are promising. The accelerometer is well suited for use within thesedisciplines.</p>

corrected abstract:
<p>The aim of this project is to investigate accelerometer readings obtained from a modern smartphone, for use within the discipline of material mechanics. Problems that fall within the domain of classical mechanics are addressed as well. To obtain data, the smartphone’s integrated accelerometer is used. All modern smartphones contain these devices. When conducting experiments, data files are created which are then transferred to a computer for analysis.</p><p>A series of experiments were conducted. The equipment for each experiment was chosen specifically for being common in households. The acceleration may be used directly for determining force, for obtaining velocity and displacement upon integration, and for calculating stiffness of systems. Frequency analysis of the acceleration was carried out to find the natural frequencies of structures.</p><p>The results of these experiments are promising. The accelerometer is well suited for use within these disciplines.</p>
----------------------------------------------------------------------
In diva2:1229773 
abstract is: 
<p>This is a project in automated music analysis. The project was based on theunderlying mathematical relations in western tonal music and the discreteFourier transform. Two distinct but similar algorithms for automated analysisof the polyphonic note content in a given recording were developed.They were also implemented in an unoptimized form, being able to interpretand reproduce the polyphonic note content of simpler recorded material, butfailed at more complex recordings. This is partly because of the unoptimizednature of the implemented versions and partly because of shortcomingsin the algorithms. The algorithms presented are meant to be taken as buildingblocks for more refined versions, not necessarily as algorithms ready forimplementation.</p>

corrected abstract:
<p>This is a project in automated music analysis. The project was based on the underlying mathematical relations in western tonal music and the discrete Fourier transform. Two distinct but similar algorithms for automated analysis of the polyphonic note content in a given recording were developed. They were also implemented in an unoptimized form, being able to interpretand reproduce the polyphonic note content of simpler recorded material, but failed at more complex recordings. This is partly because of the unoptimized nature of the implemented versions and partly because of shortcomings in the algorithms. The algorithms presented are meant to be taken as building blocks for more refined versions, not necessarily as algorithms ready for implementation.</p>
----------------------------------------------------------------------
In diva2:1189537 
abstract is: 
<p>In the frame of increasing flight safety, finite element models are developed to computethe stresses in critical parts. The results obtained often complete the ones derived from fullscale experimental tests and analytical estimations. A finite element model is particularlyuseful to simulate many different flight configurations that can not be tested experimentally.This paper presents the different stages in the development of a finite element model ofa rotor flapping mass. On a helicopter the flapping mass makes the connection betweenblades and rotor hub. This study particularly focuses on the estimation of the fatiguelimit of a composite component. This component, called roving winding, is particularlycritical as it sustains most of the loads applied on the flapping mass. Getting an accuraterepresentation of the stress distribution in the roving was the main objective. The resultsderived from the model presented here were compared to experimental ones to ensure itsaccuracy. The confidence in the model obtained makes possible its use to evaluate theimpact of some material change or geometry modifications. The model also permitted toevaluate the impact of some productions defects on a composite part.</p>

corrected abstract:
<p>In the frame of increasing flight safety, finite element models are developed to compute the stresses in critical parts. The results obtained often complete the ones derived from full scale experimental tests and analytical estimations. A finite element model is particularly useful to simulate many different flight configurations that can not be tested experimentally. This paper presents the different stages in the development of a finite element model of a rotor flapping mass. On a helicopter the flapping mass makes the connection between blades and rotor hub. This study particularly focuses on the estimation of the fatigue limit of a composite component. This component, called roving winding, is particularly critical as it sustains most of the loads applied on the flapping mass. Getting an accurate representation of the stress distribution in the roving was the main objective. The results derived from the model presented here were compared to experimental ones to ensure its accuracy. The confidence in the model obtained makes possible its use to evaluate the impact of some material change or geometry modifications. The model also permitted to evaluate the impact of some productions defects on a composite part.</p>
----------------------------------------------------------------------
In diva2:1142924 
abstract is: 
<p>Time-of-flight elastic recoil detection analysis (ToFERDA)is a method for material analysis which has provenadvantageous when examining wall samples from fusion devicesas well as for tracking tracer isotopes such as deuterium, oxygen-18 and nitrogen-15. When ToF-ERDA data is processed tocalculate the composition of a material, the detection efficiencyis used to compensate for the lower detection rate of lighterelements. The aim of this project is to examine how the efficiencyof the time-of-flight detector in a ToF-ERDA experimental setupdepends on its settings and determine efficiency parameters touse when producing atomic composition depth profiles with ToFERDA.Experiments were performed at the Tandem Laboratoryin Uppsala. The outcome is a suggested set of changes toexperimental settings and a four-parameter fit to measureddetection efficiencies for efficiency compensation.</p>

corrected abstract:
<p>Time-of-flight elastic recoil detection analysis (ToF-ERDA) is a method for material analysis which has proven advantageous when examining wall samples from fusion devices as well as for tracking tracer isotopes such as deuterium, oxygen-18 and nitrogen-15. When ToF-ERDA data is processed to calculate the composition of a material, the detection efficiency is used to compensate for the lower detection rate of lighter elements. The aim of this project is to examine how the efficiency of the time-of-flight detector in a ToF-ERDA experimental setup depends on its settings and determine efficiency parameters to use when producing atomic composition depth profiles with ToFERDA. Experiments were performed at the Tandem Laboratory in Uppsala. The outcome is a suggested set of changes to experimental settings and a four-parameter fit to measured detection efficiencies for efficiency compensation.</p>
----------------------------------------------------------------------
In diva2:1120590 
abstract is: 
<p>The Swedish stock market consists of roughly 750 companies listed on fivedifferent markets. Out of all those companies a significant portion are rarelytraded. Stocks where the trading activity is low not only present a liquidityproblem to shareholders and potential investors but also affects the reputation ofthe traded company. A company whose shares are not actively traded does nothave a market that actively puts a value on the company.This study aims to interpret how daily trade volumes can be explained by bothcategorical and numerical variables associated with the companies listed inSweden.This study, contrary to popular belief, shows that the market of the listed stock isto a large degree irrelevant when explaining daily trade volumes of the stockslisted in Sweden. The study instead reveals the importance of factors such asshareholder structure, free float and number of outstanding shares in a company.</p>

corrected abstract:
<p>The Swedish stock market consists of roughly 750 companies listed on five different markets. Out of all those companies a significant portion are rarely traded. Stocks where the trading activity is low not only present a liquidity problem to shareholders and potential investors but also affects the reputation of the traded company. A company whose shares are not actively traded does not have a market that actively puts a value on the company.</p><p>This study aims to interpret how daily trade volumes can be explained by both categorical and numerical variables associated with the companies listed in Sweden.</p><p>This study, contrary to popular belief, shows that the market of the listed stock is to a large degree irrelevant when explaining daily trade volumes of the stocks listed in Sweden. The study instead reveals the importance of factors such as shareholder structure, free float and number of outstanding shares in a company.</p>
----------------------------------------------------------------------
In diva2:1120565 
abstract is: 
<p>This paper describes methods to compute the decomposition type of representationsof some quivers that appear in topological data analysis. Specically, these are theAn-quivers, and the quivers CL(f) and CL(f; f). A background is given to explain theappearance of these quivers in the data analysis setting. The theoretical backgroundcovers the basic category theoretical concepts used to dene decomposition of repre-sentations, as well as the necessary tools used to nd indecomposable representations.A brief review of how these tools can be applied on An-quivers is given. The mainresults in the paper are the matrices presented at the ends of sections 4 and 5, whichgive linear relations between vectors representing the decomposition types of repre-sentations of these quivers and vectors of numerical invariants of the representations.These invariants are simply the dimensions of some subspaces of the vector spaces inthe representation, and are therefore easily calculated. Finally, more general laddersand grids are considered, and it it proven that the only grids of nite representationtype are the ladders with at most four rungs.</p>

corrected abstract:
<p>This paper describes methods to compute the decomposition type of representations of some quivers that appear in topological data analysis. Specifically, these are the <emZ>A<sub>𝑛</sub></em>-quivers, and the quivers CL(𝑓) and CL(𝑓, 𝑓). A background is given to explain the appearance of these quivers in the data analysis setting. The theoretical background covers the basic category theoretical concepts used to define decomposition of representations, as well as the necessary tools used to find indecomposable representations. A brief review of how these tools can be applied on <em>A<sub>𝑛</sub></em>-quivers is given. The main results in the paper are the matrices presented at the ends of sections 4 and 5, which give linear relations between vectors representing the decomposition types of representations of these quivers and vectors of numerical invariants of the representations. These invariants are simply the dimensions of some subspaces of the vector spaces in the representation, and are therefore easily calculated. Finally, more general ladders and grids are considered, and it it proven that the only grids of finite representation type are the ladders with at most four rungs.</p>
----------------------------------------------------------------------
In diva2:644628 - missing space in title:
"High performance adaptive finite elementmodeling of complex CAD geometry"
==>
"High performance adaptive finite element modeling of complex CAD geometry"


abstract is: 
<p>CAD (Computer Aided Design) and finite elementanalysis are of fundamental importance for numerical simulations. The generalapproach is to design a model using CAD software, create a mesh of a domainthat includes this model and use finite element analysis to perform simulationson that mesh. When using more advanced simulation techniques, like adaptivefinite element methods, it is more and more desired to use CAD information, notonly for the creation of the initial mesh but also during the simulation. Inthis thesis, an approach is presented how to use CAD data during adaptive mesh refinementin a finite element simulation. An error indicator is presented to find theelements in a mesh, which need to be improved for a better geometricapproximation and it is shown how to integrate the different approaches into anexisting high performance finite element solver</p>

corrected abstract:
<p>CAD (Computer Aided Design) and finite element analysis are of fundamental importance for numerical simulations. The general approach is to design a model using CAD software, create a mesh of a domain that includes this model and use finite element analysis to perform simulations on that mesh. When using more advanced simulation techniques, like adaptive finite element methods, it is more and more desired to use CAD information, not only for the creation of the initial mesh but also during the simulation.</p><p>In this thesis, an approach is presented how to use CAD data during adaptive mesh refinement in a finite element simulation. An error indicator is presented to find the elements in a mesh, which need to be improved for a better geometric approximation and it is shown how to integrate the different approaches into an existing high performance finite element solver</p>
----------------------------------------------------------------------
In diva2:642757 
abstract is: 
<p>In this master’s thesis a model for transcription analysis for a one-to-one Relationship ofInquiry were constructed and presented. The model was modified from the model fortranscript analysis in Community of Inquiry. The original three presences from Community ofInquiry, teaching, cognitive and social presence, were chosen to be adapted to Relationship ofInquiry together with a fourth presence, emotional presence. In this study the online coachingproject Math coach were used for the construction and testing of the model.A total of 60 conversations ranging over more than 3000 message units were in this thesisanalysed to test the model. From the data collected it was seen that the coaches and thecoachees had an almost 50-50 share of the message units. Furthermore the presences in thedata collected where distributed so that the most units where coded to cognitive presence, thenin descending order: teaching, emotional, and social presence. In this master’s thesis themodel for transcript analysis is presented and the results are discussed.</p>

corrected abstract:
<p>In this master’s thesis a model for transcription analysis for a one-to-one Relationship of Inquiry were constructed and presented. The model was modified from the model for transcript analysis in Community of Inquiry. The original three presences from Community of Inquiry, teaching, cognitive and social presence, were chosen to be adapted to Relationship of Inquiry together with a fourth presence, emotional presence. In this study the online coaching project Math coach were used for the construction and testing of the model.</p><p>A total of 60 conversations ranging over more than 3000 message units were in this thesis analysed to test the model. From the data collected it was seen that the coaches and the coachees had an almost 50-50 share of the message units. Furthermore the presences in the data collected where distributed so that the most units where coded to cognitive presence, then in descending order: teaching, emotional, and social presence. In this master’s thesis the model for transcript analysis is presented and the results are discussed.</p>
----------------------------------------------------------------------
In diva2:618333  missing spaces in title:
"Wave Energy Propulsion forPure Car and Truck Carriers(PCTCs)"
==>
"Wave Energy Propulsion for Pure Car and Truck Carriers (PCTCs)"

abstract is: 
<p>Wave Energy Propulsion for Pure Car and Truck Carriers (PCTC's)</p><p>The development of ocean wave energy technology has in recent years seena revival due to increased climate concerns and interest in sustainable energy.This thesis investigates whether ocean wave energy could also beused for propulsion of commercial ships, with Pure Car and Truck Carriers(PCTC's) being the model ship type used. Based on current wave energyresearch four technologies are selected as candidates for wave energy propulsion:bow overtopping, thrust generating foils, moving multi-point absorberand turbine-tted anti-roll tanks.Analyses of the selected technologies indicate that the generated propulsivepower does not overcome the added resistance from the system at the shipdesign speed and size used in the study. Conclusions are that further waveenergy propulsion research should focus on systems for ships that are slowerand smaller than current PCTC's.</p>

corrected abstract:
<p><strong>Wave Energy Propulsion for Pure Car and Truck Carriers (PCTC's)<strong></p><p>The development of ocean wave energy technology has in recent years seen a revival due to increased climate concerns and interest in sustainable energy. This thesis investigates whether ocean wave energy could also be used for propulsion of commercial ships, with Pure Car and Truck Carriers (PCTC's) being the model ship type used. Based on current wave energy research four technologies are selected as candidates for wave energy propulsion: bow overtopping, thrust generating foils, moving multi-point absorber and turbine-fitted anti-roll tanks.</p><p>Analyses of the selected technologies indicate that the generated propulsive power does not overcome the added resistance from the system at the ship design speed and size used in the study. Conclusions are that further wave energy propulsion research should focus on systems for ships that are slower and smaller than current PCTC's.</p>
----------------------------------------------------------------------
In diva2:539446 
abstract is: 
<p>This master thesis deals with the understanding of the secondary air system of athree spool turbofan. The main purpose is the creation of secondary air systemand thermal models to evaluate the behavior of this kind of engine architectureand estimate the pros and cons in comparison with a typical two spool turbofan. Afinite element model of the secondary air system of the engine has been designedbased on the experience of typical jet engines manufactured by Snecma. Theinner thermodynamic pattern and mass flow rates of the engine were obtained.Some local improvements were then made by making analogies with the enginesmanufactured by Snecma. After having communicated the results to theperformance unit to get updates thermodynamic cycles, a quite reliable model wasobtained and can be used as a reference for further studies of this kind of engineat Snecma.</p>

corrected abstract:
<p>This master thesis deals with the understanding of the secondary air system of a three spool turbofan. The main purpose is the creation of secondary air system and thermal models to evaluate the behavior of this kind of engine architecture and estimate the pros and cons in comparison with a typical two spool turbofan. A finite element model of the secondary air system of the engine has been designed based on the experience of typical jet engines manufactured by Snecma. The inner thermodynamic pattern and mass flow rates of the engine were obtained. Some local improvements were then made by making analogies with the engines manufactured by Snecma. After having communicated the results to theperformance unit to get updates thermodynamic cycles, a quite reliable model was obtained and can be used as a reference for further studies of this kind of engine at Snecma.</p>
----------------------------------------------------------------------
In diva2:530870 
abstract is: 
<p>This master thesis deals with the understanding of the secondary air system of athree spool turbofan. The main purpose is the creation of secondary air systemand thermal models to evaluate the behavior of this kind of engine architectureand estimate the pros and cons in comparison with a typical two spool turbofan. Afinite element model of the secondary air system of the engine has been designedbased on the experience of typical jet engines manufactured by Snecma. Theinner thermodynamic pattern and mass flow rates of the engine were obtained.Some local improvements were then made by making analogies with the enginesmanufactured by Snecma. After having communicated the results to theperformance unit to get updates thermodynamic cycles, a quite reliable model wasobtained and can be used as a reference for further studies of this kind of engineat Snecma.</p>

corrected abstract:
<p>This master thesis deals with the understanding of the secondary air system of a three spool turbofan. The main purpose is the creation of secondary air system and thermal models to evaluate the behavior of this kind of engine architecture and estimate the pros and cons in comparison with a typical two spool turbofan. A finite element model of the secondary air system of the engine has been designed based on the experience of typical jet engines manufactured by Snecma. The inner thermodynamic pattern and mass flow rates of the engine were obtained. Some local improvements were then made by making analogies with the engines manufactured by Snecma. After having communicated the results to the performance unit to get updates thermodynamic cycles, a quite reliable model was obtained and can be used as a reference for further studies of this kind of engine at Snecma.</p>
----------------------------------------------------------------------
In diva2:328051 
abstract is: 
<p>The phenomenon of drag reduction by polymers in turbulent flow has beenstudied over the last 60 years. New insight have been recently gained by meansof numerical simulation of dilute polymer solution at moderate values of theturbulent Reynolds number and elasticity. In this thesis, we track elastic parti-cles in Lagrangian frame in turbulent channel flow at Reτ = 180, by tracking,where the single particle obeys the FENE (finite extendible nonlinear elastic)formulation for dumbbel model. The feedback from polymers to the flow is notconsidered, while the Lagrangian approach enables us to consider high valuesof polymer elasticity. In addition, the finite time Lyapunov exponent (FTLE)of the flow is computed tracking infinitesimal material elements advected bythe flow. Following the large deviation theory, the Cramer’s function of theprobability density function of the FTLE for large values of time intervals isstudied at different wall-normal positions. The one-way effect of the turbulentflow on polymers is investigated by looking at the elongation and orientation ofthe polymers, with different relaxation times, across the channel. The confor-mation tensor of the polymers deformation which is an important contributionin the momentum balance equation is calculated by averaging in wall-parallelplanes and compared to theories available in the literature.</p>

corrected abstract:
<p>The phenomenon of drag reduction by polymers in turbulent flow has been studied over the last 60 years. New insight have been recently gained by means of numerical simulation of dilute polymer solution at moderate values of the turbulent Reynolds number and elasticity. In this thesis, we track elastic particles in Lagrangian frame in turbulent channel flow at <em>Re<sub>τ</sub></em> = 180, by tracking, where the single particle obeys the FENE (finite extendible nonlinear elastic) formulation for dumbbel model. The feedback from polymers to the flow is not considered, while the Lagrangian approach enables us to consider high values of polymer elasticity. In addition, the finite time Lyapunov exponent (FTLE) of the flow is computed tracking infinitesimal material elements advected by the flow. Following the large deviation theory, the Cramer’s function of the probability density function of the FTLE for large values of time intervals is studied at different wall-normal positions. The one-way effect of the turbulent flow on polymers is investigated by looking at the elongation and orientation of the polymers, with different relaxation times, across the channel. The conformation tensor of the polymers deformation which is an important contribution in the momentum balance equation is calculated by averaging in wall-parallel planes and compared to theories available in the literature.</p>
----------------------------------------------------------------------
In diva2:1900087 
abstract is: 
<p>LiDAR, which stands for Light Detection and Ranging, serves as a remote measurementtechnique for studying atmospheric properties, such as gas and particle concentration, aswell as wind speed [23]. The detectors employed in such measurements play a crucial rolein determining factors such as resolution, maximum range, and acquisition time. SuperconductingNanowire Single Photon Detectors (SNSPDs) exhibit exceptional performance characteristics,including high detection efficiency, minimal timing jitter, low dead time, a high maximum countrate, and a high Signal-to-Noise Ratio (SNR) even in the infrared [7]. Consequently, SNSPDsemerge as promising candidates for enhancing the quality of LiDAR signals within the infraredrange. In this work, I study the absorption and broadening mechanism of CO2 spectral linesaround 2¹m. Furthermore, I measured a backscatter signal from water and candle vapors whilea measurement of backscatter light from CO2 gas is still to come. Lastly, I perform a LiDARmeasurement with a Newtonian telescope co-operating with SNSPDs, to build a 3D image of adepth target.</p>

corrected abstract:
<p>LiDAR, which stands for Light Detection and Ranging, serves as a remote measurement technique for studying atmospheric properties, such as gas and particle concentration, as well as wind speed [23]. The detectors employed in such measurements play a crucial role in determining factors such as resolution, maximum range, and acquisition time. Superconducting Nanowire Single Photon Detectors (SNSPDs) exhibit exceptional performance characteristics, including high detection efficiency, minimal timing jitter, low dead time, a high maximum count rate, and a high Signal-to-Noise Ratio (SNR) even in the infrared [7]. Consequently, SNSPDs emerge as promising candidates for enhancing the quality of LiDAR signals within the infrared range. In this work, I study the absorption and broadening mechanism of CO<sub>2</sub> spectral lines around 2µm. Furthermore, I measured a backscatter signal from water and candle vapors while a measurement of backscatter light from CO<sub>2</sub> gas is still to come. Lastly, I perform a LiDAR measurement with a Newtonian telescope co-operating with SNSPDs, to build a 3D image of a depth target.</p>
----------------------------------------------------------------------
In diva2:1894660 
abstract is: 
<p>This study addresses the valuation challenges of private unlisted loan investmentsby implementing a valuation model to estimate the market value of such financialinstrument. In collaboration with Alecta, Sweden’s largest occupational pensionfund, a Monte Carlo-based valuation model was developed in Python. A sensitivityanalysis was also conducted in order to capture the dynamics between theinstrument’s value and the different input parameters.</p><p>Results from implementing the model demonstrate its ability to providevaluations under varying economic scenarios, highlighting the model’s sensitivityto changes in relevant economic variables.</p><p>The study establishes a foundational basis for additional research that could,for example, improve the model’s capability to capture the characteristics ofsuch financial instruments. This could include exploring additional parametersthat may be incorporated and examining the reasonableness of the underlyingassumptions.</p>

corrected abstract:
<p>This study addresses the valuation challenges of private unlisted loan investments by implementing a valuation model to estimate the market value of such financial instrument. In collaboration with Alecta, Sweden’s largest occupational pension fund, a Monte Carlo-based valuation model was developed in Python. A sensitivity analysis was also conducted in order to capture the dynamics between the instrument’s value and the different input parameters.</p><p>Results from implementing the model demonstrate its ability to provide valuations under varying economic scenarios, highlighting the model’s sensitivity to changes in relevant economic variables.</p><p>The study establishes a foundational basis for additional research that could, for example, improve the model’s capability to capture the characteristics of such financial instruments. This could include exploring additional parameters that may be incorporated and examining the reasonableness of the underlying assumptions.</p>
----------------------------------------------------------------------
In diva2:1880418 - missing space in title:
"How To Break the Second Law of Thermodynamics: Monte Carlo Simulation of Information Machine Realisation andTheory of Information"
==>
"How To Break the Second Law of Thermodynamics: Monte Carlo Simulation of Information Machine Realisation and Theory of Information"

abstract is: 
<p>In 1867, James Clerk Maxwell introduced a thought experiment involving a micro-scopic being (observer) capable of making precise measurements of microscopic quantitiesthrough observation of the micro-dynamics in a thermodynamic system. This observerlater became known as Maxwell’s demon due to its devious impact on thermodynamics,particularly the perceived violation of the second law. Subsequently, Leo Szilard pro-posed a machine, the so-called Szilard Machine, which, by utilising a Maxwell’s demon,successfully extracts work from thermal fluctuations in a closed system, seemingly vio-lating the second law.</p><p>This thesis re-evaluates the second law of thermodynamics in the context of the Szi-lard Machine and Maxwell demons. The study explores the intersection of informationtheory and thermal physics, both theoretically and practically, with the aid of MonteCarlo simulations. The results indicate that machines with information feedback control,such as those utilising a Maxwell demon, challenge classical statements of the second lawof thermodynamics. This is because classical formulations, such as Clausius’ and Kelvin’sstatements, do not account for the entropic content of information. Simulations of thesefeedback processes, in conjunction with the detailed fluctuation theorem, provide a basisfor understanding feedback processes in so-called information machines. Ultimately, thesecond law of thermodynamics is upheld by an alternative statement endorsed by thedetailed fluctuation theorem.</p>

corrected abstract:
<p>In 1867, James Clerk Maxwell introduced a thought experiment involving a microscopic being (observer) capable of making precise measurements of microscopic quantities through observation of the micro-dynamics in a thermodynamic system. This observer later became known as Maxwell’s demon due to its devious impact on thermodynamics, particularly the perceived violation of the second law. Subsequently, Leo Szilard proposed a machine, the so-called Szilard Machine, which, by utilising a Maxwell’s demon, successfully extracts work from thermal fluctuations in a closed system, seemingly violating the second law.</p><p>This thesis re-evaluates the second law of thermodynamics in the context of the Szilard Machine and Maxwell demons. The study explores the intersection of information theory and thermal physics, both theoretically and practically, with the aid of Monte Carlo simulations. The results indicate that machines with information feedback control, such as those utilising a Maxwell demon, challenge classical statements of the second law of thermodynamics. This is because classical formulations, such as Clausius’ and Kelvin's statements, do not account for the entropic content of information. Simulations of these feedback processes, in conjunction with the detailed fluctuation theorem, provide a basis for understanding feedback processes in so-called information machines. Ultimately, the second law of thermodynamics is upheld by an alternative statement endorsed by the detailed fluctuation theorem.</p>
----------------------------------------------------------------------
In diva2:1877676 
abstract is: 
<p>This paper discusses how Q-Learning and Deep Q-Networks (DQN) canbe applied to state-action problems described by a Markov decision process(MDP). These are machine learning methods for finding the optimal choiceof action at each time step, resulting in the optimal policy. The limitationsand advantages for the two methods are discussed, with the main limitationbeing the fact that Q-learning is unable to be used on problems with infinitestate spaces. Q-learning, however, has an advantage in the simplicity of thealgorithm, leading to a better understanding of what the algorithm is actuallydoing. Q-Learning did manage to find the optimal policy for the simpleproblem studied in this paper, but was unable to do so for the advancedproblem. The Deep Q-Network (DQN) approach was able to solve bothproblems, with a drawback in it being harder to understand what the algorithmactually is doing.</p>

corrected abstract:
<p>This paper discusses how Q-Learning and Deep Q-Networks (DQN) can be applied to state-action problems described by a Markov decision process (MDP). These are machine learning methods for finding the optimal choice of action at each time step, resulting in the optimal policy. The limitations and advantages for the two methods are discussed, with the main limitation being the fact that Q-learning is unable to be used on problems with infinite state spaces. Q-learning, however, has an advantage in the simplicity of the algorithm, leading to a better understanding of what the algorithm is actually doing. Q-Learning did manage to find the optimal policy for the simple problem studied in this paper, but was unable to do so for the advanced problem. The Deep Q-Network (DQN) approach was able to solve both problems, with a drawback in it being harder to understand what the algorithm actually is doing.</p>
----------------------------------------------------------------------
In diva2:1872800 
abstract is: 
<p>The inner crust of a neutron star is explored, using nuclear models andimplementing them in an open quantum system formalism. The purposeof the investigation is to extract valuable dynamics of the predictednuclear clusters that may exist in this region. We begin by setting up thenuclear models to extract the total energy per particle per system in orderto see if the results are corroborated with the works of others. Thereafter,the single-particle energies of the nuclear clusters are extracted. Theseenergies are then used in the Lindblad formalism to work out the timedependenceof the density matrix, which will allow us to extract thetime-dependence of the average energy of the system in interaction withan environment.</p>

corrected abstract:
<p>The inner crust of a neutron star is explored, using nuclear models and implementing them in an open quantum system formalism. The purpose of the investigation is to extract valuable dynamics of the predicted nuclear clusters that may exist in this region. We begin by setting up the nuclear models to extract the total energy per particle per system in order to see if the results are corroborated with the works of others. Thereafter, the single-particle energies of the nuclear clusters are extracted. These energies are then used in the Lindblad formalism to work out the time-dependence of the density matrix, which will allow us to extract the time-dependence of the average energy of the system in interaction with an environment.</p>
----------------------------------------------------------------------
In diva2:1827763 
abstract is: 
<p>The importance of sustainability in financial investments has increasedsignificantly in the previous years. With an ever-growing climate crisis andthe implementation of the EU Taxonomy it is very likely that this trendwill continue. It is however inconclusive if there is a positive correlationbetween environmental, social and governance (ESG) sustainability variablesand returns in listed companies. To deepen the knowledge of Erik PenserBank regarding this subject two regression models were constructed to try andcapture this relationship using return on assets (ROA) and 3 year stock returnsas the response variables in the respective models. The results of the studyindicate that such a relationship can not be generally established for Europeanlisted companies</p>

corrected abstract:
<p>The importance of sustainability in financial investments has increased significantly in the previous years. With an ever-growing climate crisis and the implementation of the EU Taxonomy it is very likely that this trend will continue. It is however inconclusive if there is a positive correlation between environmental, social and governance (ESG) sustainability variables and returns in listed companies. To deepen the knowledge of Erik Penser Bank regarding this subject two regression models were constructed to try and capture this relationship using return on assets (ROA) and 3 year stock returns as the response variables in the respective models. The results of the study indicate that such a relationship can not be generally established for European listed companies</p>
----------------------------------------------------------------------
In diva2:1817132 
abstract is: 
<p>This thesis investigates sound transmission loss through flat finite panels using composite materials, which incorporate both carbon and flax fibres. The study wascarried out by performing FEM simulations developed and validated in steps. Thevalidation process was time-consuming but crucial in ensuring the model’s reliability.The analysis demonstrates the relationship between the panels’ mass, stiffness, andsound reduction effectiveness by studying the sound transmission loss for a flat plate.The results show how by adjusting the plate’s stiffness, thickness, and mass one cancontrol its reaction to the incoming sound. This allows resonance frequencies to beshifted away from critical points and avoids coincidence frequencies. By understanding the acoustic behavior of composite panels, this research contributes to improvingsoundproofing properties while preserving their mechanical advantages. In an era ofgrowing demand for better acoustic performance in the automotive industry, exploring innovative methods to optimize the sound transmission loss of composite panelsis essential.</p>

corrected abstract:
<p>This thesis investigates sound transmission loss through flat finite panels using composite materials, which incorporate both carbon and flax fibres. The study was carried out by performing FEM simulations developed and validated in steps. The validation process was time-consuming but crucial in ensuring the model’s reliability. The analysis demonstrates the relationship between the panels’ mass, stiffness, and sound reduction effectiveness by studying the sound transmission loss for a flat plate. The results show how by adjusting the plate’s stiffness, thickness, and mass one can control its reaction to the incoming sound. This allows resonance frequencies to be shifted away from critical points and avoids coincidence frequencies. By understanding the acoustic behavior of composite panels, this research contributes to improving soundproofing properties while preserving their mechanical advantages. In an era of growing demand for better acoustic performance in the automotive industry, exploring innovative methods to optimize the sound transmission loss of composite panels is essential.</p>
----------------------------------------------------------------------
In diva2:1751971 - caannot select the characters of the abstract from the PDF
abstract is: 
<p>Nowadays, nanomedicine is one of the most important application areas fornanoparticles (NPs), where the design and synthesis of new hybrid nanostructures have attracted much interest, when combining properties and functionalities from different constituents. For example, they have been extensivelyused for dual-mode imaging. In this work, a water-based synthesis of hybridNPs with co-precipitation method was studied. To obtain superparamagnetichybrid NPs for X-ray fluorescence computed tomography (XFCT), severalhybridization mechanisms were followed, combining the superparamagneticbehavior of iron oxide NPs with active X-ray fluorescence (XRF) elements –Rh or Ru. The NP surface had to be engineered to improve the NP stabilityand dispersion in water, and to grant high biocompatibility. The resulting hybrid nanostructures exhibit promising characteristics for dual-mode imagingand hyperthermia treatments. We present on the details of the synthetic process, as well as the characterization of the synthesized nanomaterials</p>

corrected abstract:
<p>Nowadays, nanomedicine is one of the most important application areas for nanoparticles (NPs), where the design and synthesis of new hybrid nanostructures have attracted much interest, when combining properties and functionalities from different constituents. For example, they have been extensively used for dual-mode imaging. In this work, a water-based synthesis of hybrid NPs with co-precipitation method was studied. To obtain superparamagnetic hybrid NPs for X-ray fluorescence computed tomography (XFCT), several hybridization mechanisms were followed, combining the superparamagnetic behavior of iron oxide NPs with active X-ray fluorescence (XRF) elements –Rh or Ru. The NP surface had to be engineered to improve the NP stability and dispersion in water, and to grant high biocompatibility. The resulting hybrid nanostructures exhibit promising characteristics for dual-mode imaging and hyperthermia treatments. We present on the details of the synthetic process, as well as the characterization of the synthesized nanomaterials</p>
----------------------------------------------------------------------
In diva2:1739571 
abstract is: 
<p>Irradiation embrittlement is a problematic issue for internal structures, such as acore shroud, in a light water reactor. It is characterized by decreased ductility andtoughness, and increased yield strength and hardening due to increased dislocations.Before this thesis a core shroud from a BWR was cut into pieces based on the estimatedneutron damage on the welds. Two of the pieces cut out was used in this thesis, A2( 0.5dpa) and C2(&lt;0.1 dpa). In this thesis Vickers hardness tests and uniaxial tensile testswere used to give an estimation of the embrittlement of the welds. An estimation ofyield strength of A2 was also calculated based on the resulting hardness. The Vickerstests showed noticeable hardening in the weld, both at 0.1 dpa and 0.5 dpa. The ATTalso showed an increase in yield strength in C2. The calculated yield strength alsoshowed an increase. In conclusion, the core shroud has a decreased toughness, henceincreased risk of unexpected brittle fracture.</p>

corrected abstract:
<p>Irradiation embrittlement is a problematic issue for internal structures, such as a core shroud, in a light water reactors. It is characterized by decreased ductility and toughness, and increased yield strength and hardening due to increased dislocations. Before this thesis a core shroud from a BWR was cut into pieces based on the estimated neutron damage on the welds. Two of the pieces cut out was used in this thesis, A2 (0.5 dpa) and C2 (&lt;0.1 dpa). In this thesis Vickers hardness tests and uniaxial tensile tests were used to give an estimation of the embrittlement of the welds. An estimation of yield strength of A2 was also calculated based on the resulting hardness. The Vickers tests showed noticeable hardening in the weld, both at 0.1 dpa and 0.5 dpa. The ATT also showed an increase in yield strength in C2. The calculated yield strength also showed an increase. In conclusion, the core shroud has a decreased toughness, hence increased risk of unexpected brittle fracture.</p>
----------------------------------------------------------------------
In diva2:1739353 
abstract is: 
<p>To further reduce weight remains a constant goal within aviation. In this project an aircraftconsole is redesigned from aluminium to carbon fibre composite and analysed to see ifit is possible to reduce weight while maintaining the ability to withstand the emergencylanding loads. There are in total 9 different load cases, but the focus is on 9 and 18g forward.The design of the composite console is an iterative process, and the outcome is a sandwichsolution with a basic layup of 3 carbon fibre composite layers, PVC core and another 3carbon composite layers. Each iteration is followed by analysis in Ansys workbench usingFEA, finite element analysis, and if stresses or strains are too high reinforcement layers areadded to areas of concern and a new iteration of layup and analysis is performed.The aluminium console weighs 17 kg and after iterating the sandwich console fivetimes the end result is a console weighing 12.2 kg. Which means a weight reduction ofapproximately 30%.Overall, the new composite sandwich console can withstand higher loads than the oldaluminium one but the fastening to the floor requires further analysis.</p>

corrected abstract:
<p>To further reduce weight remains a constant goal within aviation. In this project an aircraft console is redesigned from aluminium to carbon fibre composite and analysed to see if it is possible to reduce weight while maintaining the ability to withstand the emergency landing loads. There are in total 9 different load cases, but the focus is on 9 and 18g forward. The design of the composite console is an iterative process, and the outcome is a sandwich solution with a basic layup of 3 carbon fibre composite layers, PVC core and another 3carbon composite layers. Each iteration is followed by analysis in Ansys workbench using FEA, finite element analysis, and if stresses or strains are too high reinforcement layers are added to areas of concern and a new iteration of layup and analysis is performed. The aluminium console weighs 17 kg and after iterating the sandwich console five times the end result is a console weighing 12.2 kg. Which means a weight reduction of approximately 30%.Overall, the new composite sandwich console can withstand higher loads than the old aluminium one but the fastening to the floor requires further analysis.</p>
----------------------------------------------------------------------
In diva2:1701469 
abstract is: 
<p>This thesis investigates the effects of power-law correlated disorder on a three-dimensional XY model and the Weinrib-Halperin disorder relevance criterion’s pre-dictive ability. Ising models are used as a map to realise disorder couplings. Simula-tions are conducted using hybrid Monte Carlo method constituting Metropolis’ andWolff’s algorithms. Two cases using two-dimensional and three-dimensional Isinggenerated disorder corresponding to (d + 1)- and d-dimensional models are tested.In addition, a superficial scaling analysis is performed to highlight the change ofuniversality class.It is shown that magnetisation, response functions and Binder ratio along withits temperature derivative display stark differences from the pure XY model case.The results agree with the Weinrib-Halperin criterion in terms of predicting achange of universality class but show a discrepancy in both qualitative and nu-merical results. The main new result is that power-law correlated disorder canintroduce two phase transitions at different critical couplings. This is in disagree-ment with prior established theory and predicts new physics to be investigated insuperconductors and superfluids with correlated disorder.</p>

corrected abstract:
<p>This thesis investigates the effects of power-law correlated disorder on a three-dimensional XY model and the Weinrib-Halperin disorder relevance criterion’s predictive ability. Ising models are used as a map to realise disorder couplings. Simulations are conducted using hybrid Monte Carlo method constituting Metropolis’ and Wolff’s algorithms. Two cases using two-dimensional and three-dimensional Ising generated disorder corresponding to (<em>d</em>+ 1)- and <em>d</em>-dimensional models are tested. In addition, a superficial scaling analysis is performed to highlight the change of universality class.</p><p>It is shown that magnetisation, response functions and Binder ratio along with its temperature derivative display stark differences from the pure XY model case. The results agree with the Weinrib-Halperin criterion in terms of predicting a change of universality class but show a discrepancy in both qualitative and numerical results. The main new result is that power-law correlated disorder can introduce two phase transitions at different critical couplings. This is in disagreement with prior established theory and predicts new physics to be investigated in superconductors and superfluids with correlated disorder.</p>
----------------------------------------------------------------------
In diva2:1698423 
abstract is: 
<p>The prediction of contrails have been studied since the 1930s primarily for militaryapplications. In present times, contrail avoidance strategies are gaining popularity insustainable aviation. In this project, the theory behind their formation, persistenceand the mitigation strategies are explored, studied and implemented.Several methods are evaluated in order to predict the critical temperature. Aircraftparameters are included in the prediction of contrails and implemented. This dynamicprogramming is contrasted against real case operational flight plans.The first implementation considers an operational flight plan for which contrails are pre-dicted along the trajectory. The second involves the prediction of contrails at the end ofthe vertical trajectory optimisation. The optimisation for contrail persistence avoidanceis also accomplished. Results show that contrail persistence can be avoided.</p>

corrected abstract:
<p>The prediction of contrails have been studied since the 1930s primarily for military applications. In present times, contrail avoidance strategies are gaining popularity in sustainable aviation. In this project, the theory behind their formation, persistence and the mitigation strategies are explored, studied and implemented. Several methods are evaluated in order to predict the critical temperature. Aircraft parameters are included in the prediction of contrails and implemented. This dynamic programming is contrasted against real case operational flight plans. The first implementation considers an operational flight plan for which contrails are predicted along the trajectory. The second involves the prediction of contrails at the end of the vertical trajectory optimisation. The optimisation for contrail persistence avoidance is also accomplished. Results show that contrail persistence can be avoided.</p>
----------------------------------------------------------------------
In diva2:1680720 
abstract is: 
<p>The aim of this thesis is derive a set of polynomials defined on simply connected domains, the Faberpolynomials, in which all analytic function on the domain can be uniformly approximated. Importantconcepts and theorems such as isomorphisms, automorphisms and the Riemann mapping theorem areintroduced. Examples and applications are also included. Furthermore, the thesis will aim to introducean important consequence of the Faber polynomials, the method of the Grunsky inequalities.</p><p>The first section introduces important properties of analytic functions and the concept of isomor-phisms, in particular the form of all automorphisms of the unit disc will be derived. The second sectionconsiders the Riemann mapping theorem, a theorem that relates any simply connected region that is notall of ℂ to the unit disc. A proof of the theorem beginning with the Arzelá-Ascoli theorem is provided.An application in constructing harmonic functions on arbitrary simply connected regions will be pre-sented. In the third section, definitions and properties of the Faber polynomials are developed; followedby simple examples. The section concludes with a proof and example of the statement that analyticfunctions can be approximated by Faber polynomials. In the fourth and last section of the thesis, themethod of Grunsky inequalities is presented. Starting off, the Grunsky coefficients are defined using theFaber polynomials. Properties of Grunsky coefficients such as the symmetry property and the Grunskyinequalities are then derived. To conclude it will be shown that the Grunsky inequalities provide aunivalence criterion for analytic functions defined on the unit disc.</p><p> </p>

corrected abstract:
<p>The aim of this thesis is derive a set of polynomials defined on simply connected domains, the Faber polynomials, in which all analytic function on the domain can be uniformly approximated. Important concepts and theorems such as isomorphisms, automorphisms and the Riemann mapping theorem are introduced. Examples and applications are also included. Furthermore, the thesis will aim to introduce an important consequence of the Faber polynomials, the method of the Grunsky inequalities.</p><p>The first section introduces important properties of analytic functions and the concept of isomorphisms, in particular the form of all automorphisms of the unit disc will be derived. The second section considers the Riemann mapping theorem, a theorem that relates any simply connected region that is not all of ℂ to the unit disc. A proof of the theorem beginning with the Arzelá-Ascoli theorem is provided. An application in constructing harmonic functions on arbitrary simply connected regions will be presented. In the third section, definitions and properties of the Faber polynomials are developed; followed by simple examples. The section concludes with a proof and example of the statement that analytic functions can be approximated by Faber polynomials. In the fourth and last section of the thesis, the method of Grunsky inequalities is presented. Starting off, the Grunsky coefficients are defined using the Faber polynomials. Properties of Grunsky coefficients such as the symmetry property and the Grunsky inequalities are then derived. To conclude it will be shown that the Grunsky inequalities provide a univalence criterion for analytic functions defined on the unit disc.</p>
----------------------------------------------------------------------
In diva2:1670542 
abstract is: 
<p>This thesis deals with fraud detection in a real-world environment with datasets coming from Svenska Handelsbanken. The goal was to investigate how well machine learning can classify fraudulent transactions and how new additional features affected classification. The models used were EFSVM, RUTSVM, CS-SVM, ELM, MLP, Decision Tree, Extra Trees, and Random Forests. To determine the best results the Mathew Correlation Coefficient was used as performance metric, which has been shown to have a medium bias for imbalanced datasets. Each model could deal with high imbalanced datasets which is common for fraud detection. Best results were achieved with Random Forest and Extra Trees. The best scores were around 0.4 for the real-world datasets, though the score itself says nothing as it is more a testimony to the dataset’s separability. These scores were obtained when using aggregated features and not the standard raw dataset. The performance measure recall’s scores were around 0.88-0.93 with an increase in precision by 34.4%-67%, resulting in a large decrease of False Positives. Evaluation results showed a great difference compared to test-runs, either substantial increase or decrease. Two theories as to why are discussed, a great distribution change in the evaluation set, and the sample size increase (100%) for evaluation could have lead to the tests not being well representing of the performance. Feature aggregation were a central topic of this thesis, with the main focus on behaviour features which can describe patterns and habits of customers. For these there were five categories: Sender’s fraud history, Sender’s transaction history, Sender’s time transaction history, Sender’shistory to receiver, and receiver’s history. Out of these, the best performance increase was from the first which gave the top score, the other datasets did not show as much potential, with mostn ot increasing the results. Further studies need to be done before discarding these features, to be certain they don’t improve performance. Together with the data aggregation, a tool (t-SNE) to visualize high dimension data was usedto great success. With it an early understanding of what to expect from newly added features would bring to classification. For the best dataset it could be seen that a new sub-cluster of transactions had been created, leading to the belief that classification scores could improve, whichthey did. Feature selection and PCA-reduction techniques were also studied and PCA showedgood results and increased performance. Feature selection had not conclusive improvements. Over- and under-sampling were used and neither improved the scores, though undersampling could maintain the results which is interesting when increasing the dataset.</p>

corrected abstract:
<p>This thesis deals with fraud detection in a real-world environment with datasets coming from Svenska Handelsbanken. The goal was to investigate how well machine learning can classify fraudulent transactions and how new additional features affected classification. The models used were EFSVM, RUTSVM, CS-SVM, ELM, MLP, Decision Tree, Extra Trees, and Random Forests. To determine the best results the Mathew Correlation Coefficient was used as performance metric, which has been shown to have a medium bias for imbalanced datasets. Each model could deal with high imbalanced datasets which is common for fraud detection. Best results were achieved with Random Forest and Extra Trees. The best scores were around 0.4 for the real-world datasets, though the score itself says nothing as it is more a testimony to the dataset’s separability. These scores were obtained when using aggregated features and not the standard raw dataset. The performance measure recall’s scores were around 0.88-0.93 with an increase in precision by 34.4%-67%, resulting in a large decrease of False Positives. Evaluation results showed a great difference compared to test-runs, either substantial increase or decrease. Two theories as to why are discussed, a great distribution change in the evaluation set, and the sample size increase (100%) for evaluation could have lead to the tests not being well representing of the performance.</p><p>Feature aggregation were a central topic of this thesis, with the main focus on behaviour features which can describe patterns and habits of customers. For these there were five categories: Sender’s fraud history, Sender’s transaction history, Sender’s time transaction history, Sender’s history to receiver, and receiver’s history. Out of these, the best performance increase was from the first which gave the top score, the other datasets did not show as much potential, with most not increasing the results. Further studies need to be done before discarding these features, to be certain they don’t improve performance.</p><p>Together with the data aggregation, a tool (t-SNE) to visualize high dimension data was used to great success. With it an early understanding of what to expect from newly added features would bring to classification. For the best dataset it could be seen that a new sub-cluster of transactions had been created, leading to the belief that classification scores could improve, which they did. Feature selection and PCA-reduction techniques were also studied and PCA showed good results and increased performance. Feature selection had not conclusive improvements. Over- and under-sampling were used and neither improved the scores, though undersampling could maintain the results which is interesting when increasing the dataset.</p>
----------------------------------------------------------------------
In diva2:1655933 
abstract is: 
<p>This thesis examines the correlation between some macroeconomic factors andthe monthly difference in gold price with the objective of examining if, and towhat extent they are related. This is done with the ambition to identify the mostinfluential drivers affecting the change in gold price. Multiple linear regression isused to model the relationship between the response variable, monthly change ingold price, and 13 regressor variables considered to be influential macroeconomicfactors on the gold market. The predictor variables include measures likeinflation, money supply and bond rates.</p><p>A final model is obtained, identifying the most significant regressor variablesto be USDX, EU CPI, China CPI, US Money Supply, Gold Futures, US 10yeargovernment bond yields and Gold Ores PPI. The plausibility of the resultsare discussed based on previous studies, macroeconomic theory and possibledelimitations. The thesis also provide suggestions for further studies on topicsnot covered in the study.</p>

corrected abstract:
<p>This thesis examines the correlation between some macroeconomic factors and the monthly difference in gold price with the objective of examining if, and to what extent they are related. This is done with the ambition to identify the most influential drivers affecting the change in gold price. Multiple linear regression is used to model the relationship between the response variable, monthly change in gold price, and 13 regressor variables considered to be influential macroeconomic factors on the gold market. The predictor variables include measures like inflation, money supply and bond rates.</p><p>A final model is obtained, identifying the most significant regressor variables to be <em>USDX</em>, <em>EU CPI</em>, <em>China CPI</em>, <em>US Money Supply</em>, <em>Gold Futures</em>, <em>US 10-year government bond yields</em> and <em>Gold Ores PPI</em>. The plausibility of the results are discussed based on previous studies, macroeconomic theory and possible delimitations. The thesis also provide suggestions for further studies on topics not covered in the study.</p>
----------------------------------------------------------------------
In diva2:1571175 
abstract is: 
<p>Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal form of cancerwith very few available treatment options of which none has great effect.Cancer cells and stromal cells such as stellate cells which exist in abundancein PDAC interact by crosstalk, resulting in a tumorigenic collective response.With the help of a previously developed 3D co-culture spheroid model theeffect of a CRISPR/cas9 knockout of the cellular communication cetworkfactor 1 (CCN1) gene together with gemcitabine (GEM) treatment has beeninvestigated in terms of Panc1 cell viability and gene expression. Spheroidsconsisting of wild-type and knockout cell lines, each identified by westernblots were cultured, imaged and treated. Viability assays and RNA extractionfollowed by PCR showed that the viability of the cancer cells in the spheroidswere higher for the cells with CCN1 knockout. Cancer cells were also coculturedwith stellate cells with the goal of investigating the effect of thecellular crosstalk on chemoresistance.</p>

corrected abstract:
<p>Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal form of cancer with very few available treatment options of which none has great effect. Cancer cells and stromal cells such as stellate cells which exist in abundance in PDAC interact by crosstalk, resulting in a tumorigenic collective response. With the help of a previously developed 3D co-culture spheroid model the effect of a CRISPR/cas9 knockout of the cellular communication cetwork factor 1 (CCN1) gene together with gemcitabine (GEM) treatment has been investigated in terms of Panc1 cell viability and gene expression. Spheroids consisting of wild-type and knockout cell lines, each identified by western blots were cultured, imaged and treated. Viability assays and RNA extraction followed by PCR showed that the viability of the cancer cells in the spheroids were higher for the cells with CCN1 knockout. Cancer cells were also co-cultured with stellate cells with the goal of investigating the effect of the cellular crosstalk on chemoresistance.</p>
----------------------------------------------------------------------
In diva2:1568279 
abstract is: 
<p>A promising new technology in medical imaging is photon-counting detectors (PCD). Itcould allow for images with higher resolution, less noise, improved material decomposi-tion while possibly reducing radiation exposure for patients. Recently, the possibility touse deep-learning denoising in tandem with PCD to increase image quality is starting tobe investigated. In this report we use a variety of standard image quality metrics suchas MSE, SSIM and MTF, on different image phantoms, to evaluate two ways of imple-menting neural networks in the reconstruction process: in the image domain and in thesinogram domain. We show that implementing the network in the image domain seemsto be the most promising choice to increase image quality, observing higher contrast,reduced noise and smaller errors than for the sinogram domain network. We also discusswhy this might be the case. Additionally, we study the effects of optimizing the networksand how well the neural networks generalize to types of phantoms other than the onesthey were trained on.</p>

corrected abstract:
<p>A promising new technology in medical imaging is photon-counting detectors (PCD). It could allow for images with higher resolution, less noise, improved material decomposition while possibly reducing radiation exposure for patients. Recently, the possibility to use deep-learning denoising in tandem with PCD to increase image quality is starting to be investigated. In this report we use a variety of standard image quality metrics such as MSE, SSIM and MTF, on different image phantoms, to evaluate two ways of implementing neural networks in the reconstruction process: in the image domain and in the sinogram domain. We show that implementing the network in the image domain seems to be the most promising choice to increase image quality, observing higher contrast, reduced noise and smaller errors than for the sinogram domain network. We also discuss why this might be the case. Additionally, we study the effects of optimizing the networks and how well the neural networks generalize to types of phantoms other than the ones they were trained on.</p>
----------------------------------------------------------------------
In diva2:1528146 
abstract is: 
<p>The cooling system is a crucial part for helicopter operations. Withoutit, hovering flight could not be operated. The cooling system for the maingearbox of a helicopter is composed of radiators and a fan. A fan is anaerodynamic body and as such it can be improved in terms of aerodynamicefficiency. Therefore di↵erent parameters need to be taken into account whendesigning a new axial fan to have good aerodynamic performance. Simulationshave been carried out to investigate the e↵ects of these parameters andcome up with an optimal design based on the study requirements. The fanhas to enable the cooling system to evacuate an amount of thermal power sothat the helicopter can take o↵ with high outside temperatures. This optimaldesign has shown an increase of the mass flow rate up to a factor of abouttwo for a given pressure loss compared to the original fan.</p>

corrected abstract:
<p>The cooling system is a crucial part for helicopter operations. Without it, hovering flight could not be operated. The cooling system for the main gearbox of a helicopter is composed of radiators and a fan. A fan is an aerodynamic body and as such it can be improved in terms of aerodynamic efficiency. Therefore di↵erent parameters need to be taken into account when designing a new axial fan to have good aerodynamic performance. Simulations have been carried out to investigate the e↵ects of these parameters and come up with an optimal design based on the study requirements. The fan has to enable the cooling system to evacuate an amount of thermal power so that the helicopter can take off with high outside temperatures. This optimal design has shown an increase of the mass flow rate up to a factor of about two for a given pressure loss compared to the original fan.</p>
----------------------------------------------------------------------
In diva2:1436915 
abstract is: 
<p>Technical solutions for diagnosis and treatment of heart diseases is of greatinterest due to it being one of the leading causes of death in the world. Thisstudy focuses on the valve between the left atrium and ventricle, the mitralvalve, and the blood flow in the left ventricle of the human heart. A paramet-ric model for patient-specific simulations of mitral valve dynamics developedby Hoffman J. et al. at KTH and Lucor D. at Universit´e Paris-Saclay is usedand further studied by focusing on the heart disease mitral valve stenosis.Solutions are produced with FEniCS-HPC HeartSolver using computationalfluid dynamics. Our results show that the more severe the disease is, thehigher velocities and vorticity in the ventricle, and the lower the pressure.Further studies of interest include non-Dirichlet boundary conditions by themitral valve, to allow mitral regurgitation that was observed in this study.</p>

corrected abstract:
<p>Technical solutions for diagnosis and treatment of heart diseases is of great interest due to it being one of the leading causes of death in the world. This study focuses on the valve between the left atrium and ventricle, the mitral valve, and the blood flow in the left ventricle of the human heart. A parametric model for patient-specific simulations of mitral valve dynamics developed by Hoffman J. et al. at KTH and Lucor D. at Université Paris-Saclay is used and further studied by focusing on the heart disease mitral valve stenosis. Solutions are produced with FEniCS-HPC HeartSolver using computational fluid dynamics. Our results show that the more severe the disease is, the higher velocities and vorticity in the ventricle, and the lower the pressure. Further studies of interest include non-Dirichlet boundary conditions by the mitral valve, to allow mitral regurgitation that was observed in this study.</p>
----------------------------------------------------------------------
In diva2:1357370 
abstract is: 
<p><strong></strong>The topic of this project is an experimental study of prepreg characteristics, such as tack, consolidation and temperature sensitivity during forming. The aim has first been tounderstand how the material reacts during different manufacturing processes. Secondly, to recommend suitable parameter settings, based on the findings, in order to get a good andreliable manufacturing process.In a literature study it was found that the prepreg tack is difficult to measure. It is debated by the scientific community today how to best describe prepreg tack, and the answer is affectedof what parameters that are sought to be reproduced. Consolidation tests have, in this study, been performed in an Instron machine. The relaxation of two different materials has beenmeasured in room temperature, 40 °C and 60 °C, with a maximum pressure of 2-10 bar. These limits are set to cover the temperature- and pressure scope in a robot forming process.Results show that neither of the materials will experience full consolidation during these tests, and therefore, neither in a robot forming process. It is therefore recommended toconsolidate the material in a separate process, if forming it with a robot. The material 6376/HTS is more temperature sensitive than the other tested material, an aerospacegraded prepreg with T800 fibres.Forming tests was carried out in a vacuum forming box with the goal to find a temperature where no forming defects can be seen by eye. This is found to be true at temperatures above50 °C for the material 6376/HTS when stacked in sequence [45, 0, -45, 90]4s.None of the materials are recommended to be robot formed in room temperature. Results show that one can see correlations between the forming tests and the consolidation tests.The tests are also assessed as a good way to gain basic understanding of the characteristics of a specific material.</p>

corrected abstract:
<p>The topic of this project is an experimental study of prepreg characteristics, such as tack, consolidation and temperature sensitivity during forming. The aim has first been to understand how the material reacts during different manufacturing processes. Secondly, to recommend suitable parameter settings, based on the findings, in order to get a good and reliable manufacturing process.</p><p>In a literature study it was found that the prepreg tack is difficult to measure. It is debated by the scientific community today how to best describe prepreg tack, and the answer is affected of what parameters that are sought to be reproduced. Consolidation tests have, in this study, been performed in an Instron machine. The relaxation of two different materials has been measured in room temperature, 40 °C and 60 °C, with a maximum pressure of 2-10 bar. These limits are set to cover the temperature- and pressure scope in a robot forming process.</p><p>Results show that neither of the materials will experience full consolidation during these tests, and therefore, neither in a robot forming process. It is therefore recommended to consolidate the material in a separate process, if forming it with a robot. The material 6376/HTS is more temperature sensitive than the other tested material, an aerospace graded prepreg with T800 fibres.</p><p>Forming tests was carried out in a vacuum forming box with the goal to find a temperature where no forming defects can be seen by eye. This is found to be true at temperatures above 50 °C for the material 6376/HTS when stacked in sequence [45, 0, -45, 90]<sub<4s</sub>.</p><p>None of the materials are recommended to be robot formed in room temperature. Results show that one can see correlations between the forming tests and the consolidation tests. The tests are also assessed as a good way to gain basic understanding of the characteristics of a specific material.</p>
----------------------------------------------------------------------
In diva2:1333978 
abstract is: 
<p>Brown bears go into hibernation for several months during the winter period,but regain all bodily functions shortly after waking up, such as the strength ofbones. The aim of this thesis has been to characterise the material of activebear’s bones (tibiae) by destructive testing and then fitting a damage modelby the use of finite element simulations. Standard beam theory was used tocompare with the simulated results and supplemental compression testing wasconducted to verify elastic parameters. Examination of results show quite alarge distribution in both material parameters and determined stresses for thetested bones, with elastic moduli varying 3-10 GPa, Poisson’s ratio 0.3-0.45,strain for onset of damage 1-2%, damage rate factor of 25-40 and fracturestresses varying proportionally with stiffness between 50-190 MPa.</p>

corrected abstract:
<p>Brown bears go into hibernation for several months during the winter period, but regain all bodily functions shortly after waking up, such as the strength of bones. The aim of this thesis has been to characterise the material of active bear’s bones (tibiae) by destructive testing and then fitting a damage model by the use of finite element simulations. Standard beam theory was used to compare with the simulated results and supplemental compression testing was conducted to verify elastic parameters. Examination of results show quite a large distribution in both material parameters and determined stresses for the tested bones, with elastic moduli varying 3-10 GPa, Poisson’s ratio 0.3-0.45, strain for onset of damage 1-2%, damage rate factor of 25-40 and fracture stresses varying proportionally with stiffness between 50-190 MPa.</p>
----------------------------------------------------------------------
In diva2:1249325 
abstract is: 
<p>This thesis is about time accuracy on train delays in Sweden. The aim was topredict deviation from the scheduled time at the endstation for a given journey.The main mathematical model than has been used is linear regression with threestates of observation, ”infinitely” long before departure, at departure from firststation and when half of the stations has been passed. Data has been recievedfrom Swedish transport administration and contained around 70 000 data pointsfrom the period January 2016.The parameters that best described the aimed prediction was current delay at theobserved state, type of train and train company. There was a lot of uncertaintieswith the results due to strange contradictions in the data and both obvious andsuspected errors.i</p>

corrected abstract:
<p>This thesis is about time accuracy on train delays in Sweden. The aim was to predict deviation from the scheduled time at the end station for a given journey. The main mathematical model than has been used is linear regression with three states of observation, ”infinitely” long before departure, at departure from first station and when half of the stations has been passed. Data has been received from Swedish transport administration and contained around 70 000 data points from the period January 2016. The parameters that best described the aimed prediction was current delay at the observed state, type of train and train company. There was a lot of uncertaintieswith the results due to strange contradictions in the data and both obvious and suspected errors.</p>
----------------------------------------------------------------------
In diva2:1229785 
abstract is: 
<p>This project aimed to investigate some properties of a quantum annealer by simulatingit on a classical computer. We chose the problem Hamiltonian on the form of theIsing model with interaction energies between all qubits.The results of our investigation showed at what part of the quantum annealing theminimum energy gap typically occurs for randomly generated Hamiltonians. We sawhow the quantum annealing process should proceed in order to achieve a higher probabilityof obtaining the correct answer. Additionally, we noted that the annealing timeincreases with qubit count. Interesting phenomenon as “self-rescue” and wave-like convergencearose during the investigation. We also presented an algorithm for solving thegame Minesweeper in this paper.The results in this project can be used to find potential properties of larger systemsand also as a base for possible future extensions.</p>

corrected abstract:
<p>This project aimed to investigate some properties of a quantum annealer by simulating it on a classical computer. We chose the problem Hamiltonian on the form of the Ising model with interaction energies between all qubits.</p><p>The results of our investigation showed at what part of the quantum annealing the minimum energy gap typically occurs for randomly generated Hamiltonians. We saw how the quantum annealing process should proceed in order to achieve a higher probability of obtaining the correct answer. Additionally, we noted that the annealing time increases with qubit count. Interesting phenomenon as “self-rescue” and wave-like convergence arose during the investigation. We also presented an algorithm for solving the game Minesweeper in this paper.</p><p>The results in this project can be used to find potential properties of larger systems and also as a base for possible future extensions.</p>
----------------------------------------------------------------------
In diva2:1218988 
abstract is: 
<p>In Sweden there is currently a shortage of housing which requires a solution, a part of finding asolution lies in analyzing how people move between different regions. The aim of this report is toidentify factors that explain how many individuals move to and from different municipalities inSweden. By using regression analysis the key factors have been identified, analyzed and comparedwith previous studies.In this project models explaining moving behaviors have been developed with a coefficient ofdetermination reaching up to 68%. From these models a clear connection can be seen between movingand the number of reported crimes, the percentage of people working in the same municipalitythey live in, the percentage of students and the population of the municipality. This report canbe seen as a base analysis, that allows for further research to build upon. The project analyses asociological topic by using statistical methods. If the reader is not familiar with the mathematicalmethods, the conclusions from the report can still be understood and useful.</p>

corrected abstract:
<p>In Sweden there is currently a shortage of housing which requires a solution, a part of finding a solution lies in analyzing how people move between different regions. The aim of this report is to identify factors that explain how many individuals move to and from different municipalities in Sweden. By using regression analysis the key factors have been identified, analyzed and compared with previous studies.</p><p>In this project models explaining moving behaviors have been developed with a coefficient of determination reaching up to 68%. From these models a clear connection can be seen between moving and the number of reported crimes, the percentage of people working in the same municipality they live in, the percentage of students and the population of the municipality. This report can be seen as a base analysis, that allows for further research to build upon. The project analyses a sociological topic by using statistical methods. If the reader is not familiar with the mathematical methods, the conclusions from the report can still be understood and useful.</p>
----------------------------------------------------------------------
In diva2:1188300 
abstract is: 
<p>For a company like Scania CV AB, a vast number of laws and regulations has to be considered when developing a truck. In the constant struggle to keep the generated noise below the allowed levels, the gears are made more slender and flexible. The slenderness in combination with case hardening has brought a new type of gear fracture into the light.The Tooth Interior Fatigue Fracture, TIFF. A 2D-method, and a tool for engineers, was developed in the early 2000’s. However, this tool did not provide sufficient accuracy andcompatibility with the current design process to be adopted by the engineers at Scania.This thesis expands on the current 2D-model and attempts to improve the accuracy by bringing the analysis to 3D. Furthermore, the computational tool is developed in Pythonto allow for a more streamlined interface with the current workflow.The proposed method approximates the tooth as a cantilever-beam, and is only evaluated for this case. However, the stresses are computed with good accuracy. The onlydiscrepancy is one of the stress components, where the error is about 50%. This error isderived from the decision to, in torsion, model the cross-section of the gear tooth as an ellipse. The method has potential to be incorporated into the current design process, but the accuracy of the stresses due to torsion has to be improved, and some of the equations has to be adapted before real gear geometries can be considered.</p>

corrected abstract:
<p>For a company like Scania CV AB, a vast number of laws and regulations has to be considered when developing a truck. In the constant struggle to keep the generated noise below the allowed levels, the gears are made more slender and flexible. The slenderness in combination with case hardening has brought a new type of gear fracture into the light. The Tooth Interior Fatigue Fracture, TIFF. A 2D-method, and a tool for engineers, was developed in the early 2000’s. However, this tool did not provide sufficient accuracy and compatibility with the current design process to be adopted by the engineers at Scania. This thesis expands on the current 2D-model and attempts to improve the accuracy by bringing the analysis to 3D. Furthermore, the computational tool is developed in Python to allow for a more streamlined interface with the current workflow. The proposed method approximates the tooth as a cantilever-beam, and is only evaluated for this case. However, the stresses are computed with good accuracy. The only discrepancy is one of the stress components, where the error is about 50%. This error is derived from the decision to, in torsion, model the cross-section of the gear tooth as an ellipse. The method has potential to be incorporated into the current design process, but the accuracy of the stresses due to torsion has to be improved, and some of the equations has to be adapted before real gear geometries can be considered.</p>
----------------------------------------------------------------------
In diva2:1149193 
abstract is: 
<p>Counterparty credit risk is present in trades offinancial obligations. This master thesis investigates the up and comingtechnology blockchain and how it could be used to mitigate counterparty creditrisk. The study intends to cover essentials of the mathematical model expectedloss, along with an introduction to the blockchain technology. After modellinga simple smart contract and using historical financial data, it was evidentthat there is a possible opportunity to reduce counterparty credit risk withthe use of blockchain. From the market study of this thesis, it is obvious thatthe current financial market needs more education about blockchain technology.</p>

corrected abstract:
<p>Counterparty credit risk is present in trades of financial obligations. This master thesis investigates the up and coming technology blockchain and how it could be used to mitigate counterparty credit risk. The study intends to cover essentials of the mathematical model expected loss, along with an introduction to the blockchain technology. After modelling a simple smart contract and using historical financial data, it was evident that there is a possible opportunity to reduce counterparty credit risk with the use of blockchain. From the market study of this thesis, it is obvious that the current financial market needs more education about blockchain technology.</p>
----------------------------------------------------------------------
In diva2:1142908 
abstract is: 
<p>To enhance the control utilities of the energyprovider, smart meters are being installed nationwide to measurethe households energy consumption in real time. There is agroup of algorithms that is able to read these energy readingsand determines the states of the appliances. This opens up newpossibilities for someone who wants to exploit it and might lead toinfringement of privacy. It might lead to infringement of privacy.This paper will take a look on one approach of these algorithmsand counter measures to ensure privacy. The NIALM we chosewere working poorly in presence of more appliances hence adifferent algorithm should be used if a larger house with moreappliances desired.</p>

corrected abstract:
<p>To enhance the control utilities of the energy provider, smart meters are being installed nationwide to measure the households energy consumption in real time. There is a group of algorithms that is able to read these energy readings and determines the states of the appliances. This opens up new possibilities for someone who wants to exploit it and might lead to infringement of privacy. It might lead to infringement of privacy. This paper will take a look on one approach of these algorithms and counter measures to ensure privacy. The NIALM we chose were working poorly in presence of more appliances hence a different algorithm should be used if a larger house with more appliances desired.</p>
----------------------------------------------------------------------
In diva2:748438 
abstract is: 
<p>The characterization of the vibromixer principles, in particular FUNDAMIX® technology produced bythe Swiss company Dr.Mueller AG, is the focus of this study. Tests varying the vibration’s frequencies andamplitudes, as well as the mixing plate geometry, in terms of number of holes and their diameter, are done.Interesting results regarding these parameters are obtained, proving problem complexity and previousexperience. Higher amplitudes and frequencies result in a better fluid dynamic performance of thevibromixer, i.e. flow rate formed due to pumping capacity of the plate and creating the liquid recirculation.The available total area of the holes should be limited too. Different fluid viscosities (up to 1212mPa/s) aretested and possible carbon fiber improvements in the shaft production briefly discussed. Finally, aComputational Fluid Dynamic approach is done and possible further researches are covered.</p>

corrected abstract:
<p>The characterization of the vibromixer principles, in particular FUNDAMIX® technology produced by the Swiss company Dr.Mueller AG, is the focus of this study. Tests varying the vibration’s frequencies and amplitudes, as well as the mixing plate geometry, in terms of number of holes and their diameter, are done. Interesting results regarding these parameters are obtained, proving problem complexity and previous experience. Higher amplitudes and frequencies result in a better fluid dynamic performance of the vibromixer, i.e. flow rate formed due to pumping capacity of the plate and creating the liquid recirculation. The available total area of the holes should be limited too. Different fluid viscosities (up to 1212mPa/s) are tested and possible carbon fiber improvements in the shaft production briefly discussed. Finally, a Computational Fluid Dynamic approach is done and possible further researches are covered.</p>
----------------------------------------------------------------------
In diva2:705805 - Note: no full text in DiVA
abstract is: 
<p>This research project is part of a wider project called Smart Fault Detection and Diagnosis for HeatPump Systems currently under development by the Royal Institute of Technology (KTH).Generally, maintenance, diagnosis and repair of heat pumps are manual operations. The quality of the service relies almost exclusively on the skills, experience and motivation of the HVAC-Rtechnician. Moreover, professional technicians are only called up after a remarkable failure occursand not to perform routine follow up.The main objective of this master thesis will be to propose a method for fault detection of thebrine to water heat pump systems under operating conditions. It will be done by focusing into ninetests faults related to the ﬁrst boundary level which represents the heat pump unit, the brine andwater loop. A model based approach was developed to generate features and parameters capableof reading the status of the system. The fault detection was done by imposing test faults in themodel and evaluating the trend of the performance parameters. By comparing the predicted fault free values with the actual values (Residuals) from the model, several algorithms were proposed and conducted in order to obtain an online fault detection and diagnosis.It is concluded that the fault trend analysis can, in principle, provide a solution to detect faults inheat pump systems. The algorithms are considered user friendly tools, however more improvements needs to be done to include more faults and increase its resolution.</p>

corrected abstract:
<p>This research project is part of a wider project called Smart Fault Detection and Diagnosis for Heat Pump Systems currently under development by the Royal Institute of Technology (KTH).Generally, maintenance, diagnosis and repair of heat pumps are manual operations. The quality of the service relies almost exclusively on the skills, experience and motivation of the HVAC-R technician. Moreover, professional technicians are only called up after a remarkable failure occurs and not to perform routine follow up. The main objective of this master thesis will be to propose a method for fault detection of the brine to water heat pump systems under operating conditions. It will be done by focusing into nine tests faults related to the ﬁrst boundary level which represents the heat pump unit, the brine and water loop. A model based approach was developed to generate features and parameters capable of reading the status of the system. The fault detection was done by imposing test faults in themodel and evaluating the trend of the performance parameters. By comparing the predicted fault free values with the actual values (Residuals) from the model, several algorithms were proposed and conducted in order to obtain an online fault detection and diagnosis. It is concluded that the fault trend analysis can, in principle, provide a solution to detect faults in heat pump systems. The algorithms are considered user friendly tools, however more improvements needs to be done to include more faults and increase its resolution.</p>
----------------------------------------------------------------------
In diva2:633887 
abstract is: 
<p>Molecular dynamics simulations are a very usefultool to study the behavior and interaction of atoms and molecules in chemicaland bio-molecular systems. With the fast rising complexity of such simulationshybrid systems with both, multi-core processors (CPUs) and multiple graphics processingunits (GPUs), become more and more popular. To obtain an optimal performance thisthesis presents and evaluates two different hybrid algorithms, employing allavailable compute capacity from CPUs and GPUs. The presented algorithms can beapplied for short-range force calculations in arbitrary molecular dynamicssimulations</p>

corrected abstract:
<p>Molecular dynamics simulations are a very useful tool to study the behavior and interaction of atoms and molecules in chemical and bio-molecular systems. With the fast rising complexity of such simulations hybrid systems with both, multi-core processors (CPUs) and multiple graphics processing units (GPUs), become more and more popular. To obtain an optimal performance this thesis presents and evaluates two different hybrid algorithms, employing all available compute capacity from CPUs and GPUs. The presented algorithms can be applied for short-range force calculations in arbitrary molecular dynamics simulations.</p>
----------------------------------------------------------------------
In diva2:624555 
abstract is: 
<p>Inthis degree project, a solution on a coarse grid is recovered by fitting apartial differential equation to a few known data points. The PDE to consideris the heat equation and the Dupire’s equation with their synthetic data,including synthetic data from the Black-Scholes formula. The approach to fit aPDE is by optimal control to derive discrete approximations to regularized Hamiltoncharacteristic equations to which discrete stepping schemes, and parameters forsmoothness, are examined. By non-parametric numerical implementation thedervied method is tested and then a few suggestions on possible improvementsare given</p>

corrected abstract:
<p>In this degree project, a solution on a coarse grid is recovered by fitting a partial differential equation to a few known data points. The PDE to consider is the heat equation and the Dupire’s equation with their synthetic data, including synthetic data from the Black-Scholes formula. The approach to fit a PDE is by optimal control to derive discrete approximations to regularized Hamilton characteristic equations to which discrete stepping schemes, and parameters for smoothness, are examined. By non-parametric numerical implementation the dervied method is tested and then a few suggestions on possible improvements are given.</p>
----------------------------------------------------------------------
In diva2:618555 - Note: no full text in DiVA
abstract is: 
<p>Complex subsea oil and gas components installed on the seabed require protective structures,traditionally made of steel or glass fibre reinforced plastic – GRP. The most critical load case is trawlfishing, where trawl weights of several tons are dragged along the seabed.This master thesis work aims to develop a design methodology for such protective GRP covers and is acontinuation of the master theses work performed by REINERTSEN Jonas Elgered and Marco Nikolic in2011.The trawl load case, which is suitable for a dynamic analysis since much energy is transferred over ashort period of time, is modelled with LS DYNA, using one model built up with shell elements and onewith thick shell elements.The idea is for the methodology to work as a tool for evaluating lamina thicknesses and layups for earlycover-geometry designs.</p>

corrected abstract:
<p>Complex subsea oil and gas components installed on the seabed require protective structures, traditionally made of steel or glass fibre reinforced plastic – GRP. The most critical load case is trawl fishing, where trawl weights of several tons are dragged along the seabed. This master thesis work aims to develop a design methodology for such protective GRP covers and is a continuation of the master theses work performed by REINERTSEN Jonas Elgered and Marco Nikolic in 2011. The trawl load case, which is suitable for a dynamic analysis since much energy is transferred over a short period of time, is modelled with LS DYNA, using one model built up with shell elements and one with thick shell elements. The idea is for the methodology to work as a tool for evaluating lamina thicknesses and layups for early cover-geometry designs.</p>
----------------------------------------------------------------------
In diva2:492846 - Note: no full text in DiVA
abstract is: 
<p>SP has recognized that there today exists a need to develop structures at sea since much can be done to reduce the weight of ships by using lightweight structures hence being able to construct ships with reduced need for fuel consumption or increased load capacity. A great area of interest is then fibre composites as a means to reduce weight.To be able to be involved in the work of developing new lighter constructions, new mathematical models and metrics is needed. One step in acquiring this higher competence is to study how joints between fibre composites and steel can be done in an effective and reliable manner. One application of such joints can be a steel hull with a composite superstructure.Tests have been made on joints between steel and a carbon fibre sandwich panel. The joints are of the type fork joint where a sandwich panel is bonded in to a steel profile with the shape of a two fingered fork using structural adhesive. The tested joints has been manufactured by Kockums AB shipyard in Karlskrona.The tests have been performed under compressive loads in combination with bending.To accommodate the tests a test rig has been built.For measuring the strains and displacements in the joints a non contact optical measurement system called ARAMIS has been used.The results show that the joints were very little affected by the applied design loads implying that they are either over dimensioned or needs to be tested with stronger equipment to test the joints behaviour closer to their yield point. To study how ageing will affect the joints, and how large safety margin is needed for the joints to still be functionalafter many years at sea, would however require further investigations. The use of the ARAMIS system helped in getting a good picture of where in the joints different displacements occurred. A simple 3D FE (Finite Element) model was created and used for comparing against the measurements. The FE model showed results where the strain distribution correspondedto the tests although the strain levels not being an exact match.</p>

corrected abstract:
<p>SP has recognized that there today exists a need to develop structures at sea since much can be done to reduce the weight of ships by using lightweight structures hence being able to construct ships with reduced need for fuel consumption or increased load capacity. A great area of interest is then fibre composites as a means to reduce weight. To be able to be involved in the work of developing new lighter constructions, new mathematical models and metrics is needed. One step in acquiring this higher competence is to study how joints between fibre composites and steel can be done in an effective and reliable manner. One application of such joints can be a steel hull with a composite superstructure. Tests have been made on joints between steel and a carbon fibre sandwich panel. The joints are of the type fork joint where a sandwich panel is bonded in to a steel profile with the shape of a two fingered fork using structural adhesive. The tested joints has been manufactured by Kockums AB shipyard in Karlskrona. The tests have been performed under compressive loads in combination with bending. To accommodate the tests a test rig has been built. For measuring the strains and displacements in the joints a non contact optical measurement system called ARAMIS has been used. The results show that the joints were very little affected by the applied design loads implying that they are either over dimensioned or needs to be tested with stronger equipment to test the joints behaviour closer to their yield point. To study how ageing will affect the joints, and how large safety margin is needed for the joints to still be functional after many years at sea, would however require further investigations. The use of the ARAMIS system helped in getting a good picture of where in the joints different displacements occurred. A simple 3D FE (Finite Element) model was created and used for comparing against the measurements. The FE model showed results where the strain distribution corresponded to the tests although the strain levels not being an exact match.</p>
----------------------------------------------------------------------
In diva2:1900948 
abstract is: 
<p>The study of the flow around a car is of crucial importance in seeking energy efficiency and performance. With Volvo cars rapidly transitioning towards BEV-basedarchitecture (fully electrified fleet by 2030), newer avenues to further improve existing external vehicular aerodynamic knowledge are being explored. This requiresan update of the generic rules of thumb and an overall understanding of the fundamental principles. The goal of this work is to therefore develop an optimization andanalysis workflow that identifies the key contributors to vehicular aerodynamic drag.This will contribute to the creation of a generalized set of geometrical trends, whichis further backed up by physics and CFD results. The study’s findings align closelywith established literature, underlining the importance of adopting tophat-specificoptimization approaches. The guidelines and rules-of-thumb derived from this studymainly pertains to SUVs and select squareback configurations.</p>

corrected abstract:
<p>The study of the flow around a car is of crucial importance in seeking energy efficiency and performance. With Volvo cars rapidly transitioning towards BEV-based architecture (fully electrified fleet by 2030), newer avenues to further improve existing external vehicular aerodynamic knowledge are being explored. This requires an update of the generic rules of thumb and an overall understanding of the fundamental principles. The goal of this work is to therefore develop an optimization and analysis workflow that identifies the key contributors to vehicular aerodynamic drag. This will contribute to the creation of a generalized set of geometrical trends, which is further backed up by physics and CFD results. The study’s findings align closely with established literature, underlining the importance of adopting tophat-specific optimization approaches. The guidelines and rules-of-thumb derived from this study mainly pertains to SUVs and select squareback configurations.</p>
----------------------------------------------------------------------
In diva2:1879339 
abstract is: 
<p>This thesis explores the Turing model for pattern formation and its applicationin controlling reaction-diffusion systems. The goal is to simulate both linear andnonlinear reaction-diffusion models, to understand how patterns emerge and toinvestigate the controllability of these systems with boundary controls. Using thefinite difference method (FDM) and other numerical methods on a discretizedgrid, we generated patterns with nonlinear reaction functions, validating Turing’shypothesis that nonlinear models are more applicable for pattern formation. Thenonlinear models produced stable, organized patterns, whereas linear models re-sulted in divergence, creating unrealistic patterns. In the controllability study, wediscovered that full controllability is achieved when all control inputs are active.Our findings suggest that the placement of minimal control inputs, derived fromspecific patterns, ensures full controllability in small systems, though further re-search is needed to generalize this method to larger grids. This work underscoresthe potential of simplified models like Turing’s to provide insights into the com-plex mechanisms governing natural pattern formation.</p>

corrected abstract:
<p>This thesis explores the Turing model for pattern formation and its application in controlling reaction-diffusion systems. The goal is to simulate both linear and nonlinear reaction-diffusion models, to understand how patterns emerge and to investigate the controllability of these systems with boundary controls. Using the finite difference method (FDM) and other numerical methods on a discretized grid, we generated patterns with nonlinear reaction functions, validating Turing’s hypothesis that nonlinear models are more applicable for pattern formation. The nonlinear models produced stable, organized patterns, whereas linear models resulted in divergence, creating unrealistic patterns. In the controllability study, we discovered that full controllability is achieved when all control inputs are active. Our findings suggest that the placement of minimal control inputs, derived from specific patterns, ensures full controllability in small systems, though further research is needed to generalize this method to larger grids. This work underscores the potential of simplified models like Turing’s to provide insights into the complex mechanisms governing natural pattern formation.</p>
----------------------------------------------------------------------
In diva2:1823810 
abstract is: 
<p>This research paper revolves around the world’s oldest financial asset, gold, and whatdrives its price, which is of importance for all investors looking to be exposed to gold.The aim of this paper is to identify the main drivers behind the gold price, whichis done by performing a multiple linear regression analysis on the gold price and aset of explanatory variables. The results show that the real yield, measured as theTIPS-rate, has the largest impact on the gold price, followed by the inflation rate.The conclusion that is drawn in the paper is that it is reasonable that the real yield isthe main driver of the gold price, because the higher the real yield, the less attractiveit becomes for investors to own gold, as it is not an interest-bearing asset.</p>

corrected abstract:
<p>This research paper revolves around the world’s oldest financial asset, gold, and what drives its price, which is of importance for all investors looking to be exposed to gold. The aim of this paper is to identify the main drivers behind the gold price, which is done by performing a multiple linear regression analysis on the gold price and a set of explanatory variables. The results show that the real yield, measured as the TIPS-rate, has the largest impact on the gold price, followed by the inflation rate. The conclusion that is drawn in the paper is that it is reasonable that the real yield is the main driver of the gold price, because the higher the real yield, the less attractive it becomes for investors to own gold, as it is not an interest-bearing asset.</p>
----------------------------------------------------------------------
In diva2:1823788 
abstract is: 
<p>By analyzing the card properties of Magic the Gathering cards, models have beendeveloped to determine their impact on card prices. Previous studies have notfocused on gameplay properties, which distinguishes this work from previousresearch. To model the effect of gameplay properties, they have been quantifiedand examined using Least Squares Method and Lasso Regression, with the helpof the programming language R. The results indicate that factor directly relateradto collectability and playability have the greatest impact on the price of Magic theGathering cards. These results have been discussed from various perspectives,such as Wizards of the Coast (the publisher of Magic the Gathering), players,collectors, and investors. By focusing on gameplay properties, this study hascontributed to the field in a way that previous research has not, providing a morecomprehensive understanding of the value of Magic the Gathering cards.</p>

corrected abstract:
<p>By analyzing the card properties of Magic the Gathering cards, models have been developed to determine their impact on card prices. Previous studies have not focused on gameplay properties, which distinguishes this work from previous research. To model the effect of gameplay properties, they have been quantified and examined using Least Squares Method and Lasso Regression, with the help of the programming language R. The results indicate that factor directly relaterad to collectability and playability have the greatest impact on the price of Magic the Gathering cards. These results have been discussed from various perspectives, such as Wizards of the Coast (the publisher of Magic the Gathering), players, collectors, and investors. By focusing on gameplay properties, this study has contributed to the field in a way that previous research has not, providing a more comprehensive understanding of the value of Magic the Gathering cards.</p>
----------------------------------------------------------------------
In diva2:1780156 
abstract is: 
<p>This study aims to investigate the similarities in precision and performance between a James WebbSpace Telescope mirror actuator, the part controlling the motion of the primary mirror segments,and two 3D printed replicas. The two replicas were made from the plastics PLA and PETGand based on the designs of Zachary Tong. The plastics were examined concurrently and theresults were compared with simulations in COMSOL. The 3D printed parts were assembled and theresulting replicas were put in motion by an Arduino driven stepper motor. The result showed thatthe replica made from PLA had an average precision of 40 nm and the replica made from PETGhad an average precision of 32 nm. Compared to the James Webb Space Telescope actuator, bothreplicas were approximately one order of magnitude less accurate. The COMSOL simulations gavesimilar results. In conclusion, the study shows that the choice of material matters. The performanceof the James Webb Space Telescope actuator was more accurately emulated by the replica madefrom PETG than by the one made from PLA.</p>

corrected abstract:
<p>This study aims to investigate the similarities in precision and performance between a James Webb Space Telescope mirror actuator, the part controlling the motion of the primary mirror segments, and two 3D printed replicas. The two replicas were made from the plastics PLA and PETG and based on the designs of Zachary Tong<sup>1</sup>. The plastics were examined concurrently and the results were compared with simulations in COMSOL. The 3D printed parts were assembled and the resulting replicas were put in motion by an Arduino driven stepper motor. The result showed that the replica made from PLA had an average precision of 40 nm and the replica made from PETG had an average precision of 32 nm. Compared to the James Webb Space Telescope actuator, both replicas were approximately one order of magnitude less accurate. The COMSOL simulations gave similar results. In conclusion, the study shows that the choice of material matters. The performance of the James Webb Space Telescope actuator was more accurately emulated by the replica made from PETG than by the one made from PLA.</p>

<p><sup>1</sup>Zachary Tong. JWST Mirror Actuator. URL: https://www.thingiverse.com/thing:5232214/apps. (Date accessed: 2023-02-02).</p>

----------------------------------------------------------------------
In diva2:1757003 - Note: no full text in DiVA
abstract is: 
<p>Finding root causes for the differences between countries in life expectancy is relevant forhealth organisations and governments when prioritising public health projects. Previousstudies on the topic have seen a positive correlation between GDP per capita and lifeexpectancy, as well as levels of education and sanitary conditions. Using data from 183countries from the year 2015, this thesis uses multiple regression analysis to study 11different variables correlation to life expectancy at birth. The final result shows that the6 variables GDP per capita, access to drinking water and electricity, low average BMI,and concentration of doctors and pharmacists had significant positive correlations tolife expectancy at birth, while high average BMI had a significant negative correlation.Since the study is only based on one year, a more extensive study is proposed to backup the indications. Such a study could also incorporate health insurance system andlevel of economical inequality as variables.</p>

corrected abstract:
<p>Finding root causes for the differences between countries in life expectancy is relevant for health organisations and governments when prioritising public health projects. Previous studies on the topic have seen a positive correlation between GDP per capita and lifeexpectancy, as well as levels of education and sanitary conditions. Using data from 183countries from the year 2015, this thesis uses multiple regression analysis to study 11different variables correlation to life expectancy at birth. The final result shows that the 6 variables GDP per capita, access to drinking water and electricity, low average BMI, and concentration of doctors and pharmacists had significant positive correlations to life expectancy at birth, while high average BMI had a significant negative correlation. Since the study is only based on one year, a more extensive study is proposed to backup the indications. Such a study could also incorporate health insurance system and level of economical inequality as variables.</p>
----------------------------------------------------------------------
In diva2:1752017 
abstract is: 
<p>Understanding how a system behaves when exposed to different scenarios is key when improvingand developing complex structures. The amount of different approaches is immense and variesfrom case to case. One of the simpler approaches is black-box modelling as it only targetsan input and output to a system, and not necessarily the mathematical interpretations. Fora nonlinear system such as a remote controlled weapon station, this approach is appropriate,as it allows to only focus on a certain scenario and the results obtained for that case. In thisstudy, a remote controlled weapon station is further investigated when exposed to disturbancesfrom a combat vehicle. The data obtained is simulated on a platform and the results are usedin Matlab to analyze and find the best model from these tests. A Hammerstein-Wiener modelwith nonlinear wavelet networks is deemed the best as it gives the most accurate representationof the station’s behavior. The results obtained are considered to be moderately accurate dueto its precision and should only be used as reference point, rather than being interpreted as atrue representation of the system.</p>

corrected abstract:
<p>Understanding how a system behaves when exposed to different scenarios is key when improving and developing complex structures. The amount of different approaches is immense and varies from case to case. One of the simpler approaches is black-box modelling as it only targets an input and output to a system, and not necessarily the mathematical interpretations. For a nonlinear system such as a remote controlled weapon station, this approach is appropriate, as it allows to only focus on a certain scenario and the results obtained for that case. In this study, a remote controlled weapon station is further investigated when exposed to disturbances from a combat vehicle. The data obtained is simulated on a platform and the results are used in Matlab to analyze and find the best model from these tests. A Hammerstein-Wiener model with nonlinear wavelet networks is deemed the best as it gives the most accurate representation of the station’s behavior. The results obtained are considered to be moderately accurate due to its precision and should only be used as reference point, rather than being interpreted as a true representation of the system.</p>
----------------------------------------------------------------------
In diva2:1708047 
abstract is: 
<p>Lead Halide Perovskite is emerging quickly as a promising material for the future solar cellsthanks to their inherent good optoelectrical properties along with their cheap and facile fabri-cation. However, their main drawback before commercialization is their weak stability. In thiswork, a novel carbon-coated perovskite quantum dot has been synthesized, and is to the extentof our knowledge, for the first time. The coated perovskite quantum dots show a remarkable in-creased stability under different conditions while in solution. Their photoluminescence intensityalso increased as time went on, exceeding that of the uncoated perovskite quantum dots aftera few weeks. These coated perovskite quantum dots, while not fully characterized and thusnot fully understood show a promising way on how to combat the low stability in perovskites.Further, Copper/Copper(I)Iodide core/shell nanowires were synthesized as a transparent inte-grated hole transport layer/electrode for solar cells. While limited due to the low controlledfabrication process used, they providing a solid base for further research on the material to beused in solar cells.</p>

corrected abstract:
<p>Lead Halide Perovskite is emerging quickly as a promising material for the future solar cells thanks to their inherent good optoelectrical properties along with their cheap and facile fabrication. However, their main drawback before commercialization is their weak stability. In this work, a novel carbon-coated perovskite quantum dot has been synthesized, and is to the extent of our knowledge, for the first time. The coated perovskite quantum dots show a remarkable increased stability under different conditions while in solution. Their photoluminescence intensity also increased as time went on, exceeding that of the uncoated perovskite quantum dots after a few weeks. These coated perovskite quantum dots, while not fully characterized and thus not fully understood show a promising way on how to combat the low stability in perovskites. Further, Copper/Copper(I)Iodide core/shell nanowires were synthesized as a transparent integrated hole transport layer/electrode for solar cells. While limited due to the low controlled fabrication process used, they providing a solid base for further research on the material to be used in solar cells.</p>
----------------------------------------------------------------------
In diva2:1680714 
abstract is: 
<p>In this paper the Clifford Algebra is introduced and proposed as analternative to Gibbs' vector algebra as a unifying language for geometricoperations on vectors. Firstly, the algebra is constructed using a quotientof the tensor algebra and then its most important properties are proved,including how it enables division between vectors and how it is connected tothe exterior algebra. Further, the Clifford algebra is shown to naturallyembody the complex numbers and quaternions, whereupon its strength indescribing rotations is highlighted. Moreover, the wedge product, is shown asa way to generalize the cross product and reveal the true nature ofpseudovectors as bivectors. Lastly, we show how replacing the cross productwith the wedge product, within the Clifford algebra, naturally leads tosimplifying Maxwell's equations to a single equation.</p>

corrected abstract:
<p>In this paper the Clifford Algebra is introduced and proposed as an alternative to Gibbs' vector algebra as a unifying language for geometric operations on vectors. Firstly, the algebra is constructed using a quotient of the tensor algebra and then its most important properties are proved, including how it enables division between vectors and how it is connected to the exterior algebra. Further, the Clifford algebra is shown to naturally embody the complex numbers and quaternions, where upon its strength in describing rotations is highlighted. Moreover, the wedge product, is shown as a way to generalize the cross product and reveal the true nature of pseudovectors as bivectors. Lastly, we show how replacing the cross product with the wedge product, within the Clifford algebra, naturally leads to simplifying Maxwell's equations to a single equation.</p>
----------------------------------------------------------------------
In diva2:1510573 
abstract is: 
<p>This thesis focuses on the optimization of the electric aircraft propeller in order to increaseflight performance. Electric aircraft have limited energy, particularly the electricmotor torque compared to the fuel engine torque. For that, redesign of the propeller forelectric aircraft is important in order to improve the propeller efficiency. The airplanepropeller theory for Glauert is selected as a design method and incorporated with Brattimprovements of the theory. Glauert theory is a combination of the axial momentum andblade element theory. Pipistrel Alpha Electro airplane specifications have been chosen asa model for the design method. Utilization of variable pitch propeller and the influence ofnumber of blades has been investigated. The obtained design results show that the variablepitch propellers at cruise speed and altitude 3000 m reducing the power consumptionby 0.14 kWh and increase the propeller efficiency by 0.4% compared to the fixed pitchpropeller. Variable pitch propeller improvement was pretty good for electric aircraft. Theoptimum blade number for the design specifications is 3 blades.</p>

corrected abstract:
<p>This thesis focuses on the optimization of the electric aircraft propeller in order to increase flight performance. Electric aircraft have limited energy, particularly the electric motor torque compared to the fuel engine torque. For that, redesign of the propeller for electric aircraft is important in order to improve the propeller efficiency. The airplane propeller theory for Glauert is selected as a design method and incorporated with Bratt improvements of the theory. Glauert theory is a combination of the axial momentum and blade element theory. Pipistrel Alpha Electro airplane specifications have been chosen as a model for the design method. Utilization of variable pitch propeller and the influence of number of blades has been investigated. The obtained design results show that the variable pitch propellers at cruise speed and altitude 3000 m reducing the power consumption by 0.14 kWh and increase the propeller efficiency by 0.4% compared to the fixed pitch propeller. Variable pitch propeller improvement was pretty good for electric aircraft. The optimum blade number for the design specifications is 3 blades.</p>
----------------------------------------------------------------------
In diva2:1464096 
abstract is: 
<p>The lack of strong measures to avoid the possible fatal consequences of global warming is pushing researchers to look for other alternatives such as geoengineering.Within geoengineering, this study focuses on the space based solar radiation management methods. More precisely, the project evaluates the feasibilityof implementing a space sun shade near the first Lagrangian point in the Sun-Earth system within a thirty year period time from now. The study isstructured in three main blocks: spacecraft configuration, trajectory definition and launch. An analysis looking at the minimum cost system was carried out,starting with the definition of the mass and size of spacecraft. Furthermore, an optimization of the trajectory was developed in order to minimize the traveltime to the vicinity of the Lagrangian point. The shades will be formed by swarms of 10 000m2 solar sails that will cover an area of 6:3 x 1012 m2 witha total mass of around 5:7 x 1010 kg. The sails will be injected into a LEO and will start a trajectory to the vicinity of the first Lagrangian point that willtake around 2.3 years. The total cost of the project is approximated to be 10 trillion dollars. The mission appears to be feasible from a technological pointof view, with some development needed in the attitude control subsystem. The main challenge will be the launch of all the spacecraft. A space mission of thisdimensions has never been attempted before so it will require a big advance from the launch vehicle industry.</p>

corrected abstract:
<p>The lack of strong measures to avoid the possible fatal consequences of global warming is pushing researchers to look for other alternatives such as geoengineering. Within geoengineering, this study focuses on the space based solar radiation management methods. More precisely, the project evaluates the feasibility of implementing a space sun shade near the first Lagrangian point in the Sun-Earth system within a thirty year period time from now. The study is structured in three main blocks: spacecraft configuration, trajectory definition and launch. An analysis looking at the minimum cost system was carried out, starting with the definition of the mass and size of spacecraft. Furthermore, an optimization of the trajectory was developed in order to minimize the travel time to the vicinity of the Lagrangian point. The shades will be formed by swarms of 10 000 m<sup>2</sup> solar sails that will cover an area of 6.3 × 10<sup>12</sup> m<sup>2</sup> with a total mass of around 5.7 × 10<sup>10</sup> kg. The sails will be injected into a LEO and will start a trajectory to the vicinity of the first Lagrangian point that will take around 2.3 years. The total cost of the project is approximated to be 10 trillion dollars. The mission appears to be feasible from a technological point of view, with some development needed in the attitude control subsystem. The main challenge will be the launch of all the spacecraft. A space mission of this dimensions has never been attempted before so it will require a big advance from the launch vehicle industry.</p>
----------------------------------------------------------------------
In diva2:1242864 
abstract is: 
<p>In 2015, cardiovascular diseases caused the death of 17.7 million people - 31 % of all deaths globally - making it deadlier than any other cause.Cardiovascular diseases often result from Arteriosclerosis, a disease where plaque builds up and clogs arteries. As this occurs in the carotid arteries and the plaque material is subject to forces caused by blood pressure and flow, the stress that occurs within could lead to rupture of the material. The ruptured particles could then travel to the brain and cause a stroke.To identify and prevent these strokes, computed tomography and ultrasound scans are used today. With the help of such examination, it’s possible to see how much of the artery that is clogged and decide whether or not to operate on the patient. Another way of analyzing the plaques vulnerability of rupture is with the help of biomechanics. Based on the geometry and material composition of the plaque, and using a finite element analysis, the stresses and risk of rupture can be estimated.This thesis invetigates how different plaque geometries, with focus on plaque length and cross- section area coverage, affects the resulting stress in the material. The study was carried out in a total of 12 different plaque models. The models were based on two different lengths, 3 mm and 10 mm and four different cross-sectional coverages.With computed tomography as reference, models of the carotids and plaque were built using a computer-aided design program. The models were imported to the finite element program COMSOL and analyzed using fluid-mechanical and solid-mechanical simulations. The simula- tions were executed on non-linear solid-state simulations and linear-elastic tissue models and the Newtonian fluid assumption.based on the investigated plaque functionalities, this study found that the stress in the plaque tissue shows a peak at 50-60% plaque coverage of the cross-sectional area, such that the risk of rupture seems to be the highest at this area coverage.</p>

corrected abstract:
<p>In 2015, cardiovascular diseases caused the death of 17.7 million people - 31 % of all deaths globally - making it deadlier than any other cause.</p><p>Cardiovascular diseases often result from Arteriosclerosis, a disease where plaque builds up and clogs arteries. As this occurs in the carotid arteries and the plaque material is subject to forces caused by blood pressure and flow, the stress that occurs within could lead to rupture of the material. The ruptured particles could then travel to the brain and cause a stroke.</p><p>To identify and prevent these strokes, computed tomography and ultrasound scans are used today. With the help of such examination, it’s possible to see how much of the artery that is clogged and decide whether or not to operate on the patient. Another way of analyzing the plaques vulnerability of rupture is with the help of biomechanics. Based on the geometry and material composition of the plaque, and using a finite element analysis, the stresses and risk of rupture can be estimated.</p><p>This thesis invetigates how different plaque geometries, with focus on plaque length and cross-section area coverage, affects the resulting stress in the material. The study was carried out in a total of 12 different plaque models. The models were based on two different lengths, 3 mm and 10 mm and four different cross-sectional coverages.</p><p>With computed tomography as reference, models of the carotids and plaque were built using a computer-aided design program. The models were imported to the finite element program COMSOL and analyzed using fluid-mechanical and solid-mechanical simulations. The simulations were executed on non-linear solid-state simulations and linear-elastic tissue models and the Newtonian fluid assumption.</p><p>based on the investigated plaque functionalities, this study found that the stress in the plaque tissue shows a peak at 50-60% plaque coverage of the cross-sectional area, such that the risk of rupture seems to be the highest at this area coverage.</p>


Note: The last paragraph starts with a lower case letter, this is in the original abstract,
----------------------------------------------------------------------
In diva2:1229933 - missing space in title:
"Analysing ConformationalEnsembles using Machine Learning"
==>
"Analysing Conformational Ensembles using Machine Learning"

abstract is: 
<p>G-protein coupled receptors are involved in many diseases and act as target for severalpharmaceutical drugs. To develop methods to understand the dynamics of proteinsinvolved in the communication through the cell membrane is therefore crucial. In thisstudy, data from molecular dynamics trajectories of the !2-adrenergic receptor were studiedwith the aim to train a neural network to identify the receptor due to its bindingmechanism. Represented as conformational ensembles, the protein was examined to identifythe binding state of the receptor based on determinants of its conformation. Thethesis demonstrate that this conceptually simple method can be used to computationallyanalyze the molecular response on drug binding.</p>

corrected abstract:
<p>G-protein coupled receptors are involved in many diseases and act as target for several pharmaceutical drugs. To develop methods to understand the dynamics of proteins involved in the communication through the cell membrane is therefore crucial. In this study, data from molecular dynamics trajectories of the <em>&beta;</em><sub>2</sub>-adrenergic receptor were studied with the aim to train a neural network to identify the receptor due to its binding mechanism. Represented as conformational ensembles, the protein was examined to identify the binding state of the receptor based on determinants of its conformation. The thesis demonstrate that this conceptually simple method can be used to computationally analyze the molecular response on drug binding.</p>
----------------------------------------------------------------------
In diva2:1218565 
abstract is: 
<p>This paper analyzes a data set obtained from a recent study per ormed at The Karolinska Institute. The data set is comprised of 131 children with anxiety disorders, aged 8 - 12, who all underwent a novel treatment against their disorder called internet-delivered cognitive behavior therapy (ICBT). The data set contains standardized clinical severity ratings (CSR) of the patients before and and after the treatment, as well as 233 features for each patient (demographicinformation, symptom reports, information on other diagnoses etc). Before thetreatment, the clinicians also made a "guess", scored on a scale of 1 - 10, answering the question"How successful will ICBT treatment be for this particular patient?". Firstly, this studyfound that the clinicians predicted remission with an accuracy of approximately 50%. Secondly,this study employed machine learning algorithms designed to learn from the data setand make predictions based on the feature information of each particular patient. The topperforming algorithm predicted with an accuracy of 70%. This study therefore suggests thatmachine learning algorithms can predict outcome of ICBT treatmentwith a higher level of accuracythan clinicians. This study then addresses its weaknesses and limitations to this conclusion,most importantly the vagueness of the question and scale that the clinicians basedtheir guesses on.</p>

corrected abstract:
<p>This paper analyzes a data set obtained from a recent study performed at The Karolinska Institute. The data set is comprised of 131 children with anxiety disorders, aged 8 - 12, who all underwent a novel treatment against their disorder called internet-delivered cognitive behavior therapy (ICBT). The data set contains standardized clinical severity ratings (CSR) of the patients before and and after the treatment, as well as 233 features for each patient (demographic information, symptom reports, information on other diagnoses etc). Before the treatment, the clinicians also made a "guess", scored on a scale of 1 - 10, answering the question "How successful will ICBT treatment be for this particular patient?". Firstly, this study found that the clinicians predicted remission with an accuracy of approximately 50%. Secondly, this study employed machine learning algorithms designed to learn from the data set and make predictions based on the feature information of each particular patient. The top performing algorithm predicted with an accuracy of 70%. This study therefore suggests that machine learning algorithms can predict outcome of ICBT treatment with a higher level of accuracy than clinicians. This study then addresses its weaknesses and limitations to this conclusion, most importantly the vagueness of the question and scale that the clinicians based their guesses on.</p>
----------------------------------------------------------------------
In diva2:1215665 
abstract is: 
<p>The aim of this thesis is to investigate thehedging error in Credit Value Adjustment (CVA) produced by using a model forthe simulation of the risk factors different from the one used in the pricingof the derivative contract. The hypothesis is that this inconsistency betweensimulation and pricing models affects the CVA leading to an error in thehedging of credit counterparty risk. When computing the CVA, market factors aresimulated forward in time and the portfolio is priced in each scenario toobtain the Expected Positive Exposure (EPE). To hedge the market risk of CVA weuse a dynamic Delta-hedging strategy. We investigate the hedging error for adefault free portfolio and for its CVA and how it is affected by the mismatchbetween the models.</p>

corrected abstract:
<p>The aim of this thesis is to investigate the hedging error in Credit Value Adjustment (CVA) produced by using a model for the simulation of the risk factors different from the one used in the pricing of the derivative contract. The hypothesis is that this inconsistency between simulation and pricing models affects the CVA leading to an error in the hedging of credit counterparty risk. When computing the CVA, market factors are simulated forward in time and the portfolio is priced in each scenario to obtain the Expected Positive Exposure (EPE). To hedge the market risk of CVA we use a dynamic Delta-hedging strategy. We investigate the hedging error for a default free portfolio and for its CVA and how it is affected by the mismatch between the models.</p>
----------------------------------------------------------------------
In diva2:1142748 
abstract is: 
<p>This paper investigates a smart heat control systeminside a household using a family of dynamic controllers calledPID (Proportional Integrative Derivative), conducted withWireless Sensor Networks (WSN). The laws of thermodynamicsand other physical knowledge was used to model the effect that aheat exchanger has on the temperature of a room. Realisticsimulations have been run to illustrate how a PID control systemoutperforms the traditional on-off approach, in terms of energyefficiency. Simulations shows that the PID controlled system is28% more energy efficient than an on-off. This can make thefuture use of PIDs in home automation more common. Lastly afuture extension to other systems is discussed.</p>

corrected abstract:
<p>This paper investigates a smart heat control system inside a household using a family of dynamic controllers called PID (Proportional Integrative Derivative), conducted with Wireless Sensor Networks (WSN). The laws of thermodynamics and other physical knowledge was used to model the effect that a heat exchanger has on the temperature of a room. Realistic simulations have been run to illustrate how a PID control system outperforms the traditional on-off approach, in terms of energy efficiency. Simulations shows that the PID controlled system is 28% more energy efficient than an on-off. This can make the future use of PIDs in home automation more common. Lastly a future extension to other systems is discussed.</p>
----------------------------------------------------------------------
In diva2:1120495 
abstract is: 
<p>Text data mining is a growing research field where machine learning and NLP areimportant technologies. There are multiple applications concerning categorizinglarge sets of documents. Depending on the size of the documents the methodsdi↵er, when it comes to short text documents the information in individualones are scant. The aim of this paper is to show how well unsupervised textclustering reflects existing class assignments and how sensitive clustering is whencomparing di↵erent text representation and feature selection. The raw datawas collected from several national health surveys. Evaluation was made with aconditional entropy-based method called V-measure which connects the clustersto the categories. We present that some methods perform significantly betteragainst raw data then others.</p>

corrected abstract:
<p>Text data mining is a growing research field where machine learning and NLP are important technologies. There are multiple applications concerning categorizing large sets of documents. Depending on the size of the documents the methods differ, when it comes to short text documents the information in individual ones are scant. The aim of this paper is to show how well unsupervised text clustering reflects existing class assignments and how sensitive clustering is when comparing different text representation and feature selection. The raw data was collected from several national health surveys. Evaluation was made with a conditional entropy-based method called V-measure which connects the clusters to the categories. We present that some methods perform significantly better against raw data then others.</p>
----------------------------------------------------------------------
In diva2:1106219 
abstract is: 
<p>The modeling of non-maturity deposits is a topic that is highlyimportant to many banks because of the large amount of funding that comes fromthese products. It is also a topic that currently is in the focus oflegislators. Although a non-maturity deposit may seem to be a trivial product,it has several characteristics that make it rather complex. One of the twopurposes of this thesis is to compare different models for the deposit rate ofnon-maturity deposits and to investigate the strengths and weaknesses of themodels. The other purpose is to find a new model for the deposit rate ofnon-maturity deposits. Several different models that are suggested in the literatureare described and evaluated based on the four aspects; goodness of fit,stability, negative interest rate environment and simplicity. Three new modelsfor the deposit rate are suggested in this thesis, one of which shows a verygood performance compared to the models that can be found in the literature.</p>

corrected abstract:
<p>The modeling of non-maturity deposits is a topic that is highly important to many banks because of the large amount of funding that comes from these products. It is also a topic that currently is in the focus of legislators. Although a non-maturity deposit may seem to be a trivial product, it has several characteristics that make it rather complex. One of the two purposes of this thesis is to compare different models for the deposit rate of non-maturity deposits and to investigate the strengths and weaknesses of the models. The other purpose is to find a new model for the deposit rate of non-maturity deposits. Several different models that are suggested in the literature are described and evaluated based on the four aspects; goodness of fit, stability, negative interest rate environment and simplicity. Three new models for the deposit rate are suggested in this thesis, one of which shows a very good performance compared to the models that can be found in the literature.</p>
----------------------------------------------------------------------
In diva2:1078080 
abstract is: 
<p>A key aspect in Attitude and Orbital Control Systems (AOCS) is the ability to estimateaccurately a spacecraft’s attitude and angular rate. While a classic gyrometer and star-trackerhybridization features high performances, gyroless solutions using only star-trackers as attitudesensors are often chosen to reduce the mission’s cost, when possible. Such an estimation is improvedusing the knowledge of the torque commanded by the spacecraft’s reaction wheels. The torqueestimate is however imperfect, mostly due to friction occurring on the wheels’ shafts. This paperpresents the research undertaken to improve this knowledge by adding a secondary torqueestimator to an existing gyroless estimator. An additional filter is developed for that matter, tuned,and the global AOCS performance is tested in diverse operating conditions to assess the newmethod’s benefits. While converged state performances are not improved, this solution is shown tomake the estimator more robust to friction torque spikes occurring on some reaction wheels, and toimprove settling performances after spacecraft manoeuvers.</p>

corrected abstract:
<p>A key aspect in Attitude and Orbital Control Systems (AOCS) is the ability to estimate accurately a spacecraft’s attitude and angular rate. While a classic gyrometer and star-tracker hybridization features high performances, gyroless solutions using only star-trackers as attitude sensors are often chosen to reduce the mission’s cost, when possible. Such an estimation is improved using the knowledge of the torque commanded by the spacecraft’s reaction wheels. The torque estimate is however imperfect, mostly due to friction occurring on the wheels’ shafts. This paper presents the research undertaken to improve this knowledge by adding a secondary torque estimator to an existing gyroless estimator. An additional filter is developed for that matter, tuned, and the global AOCS performance is tested in diverse operating conditions to assess the new method’s benefits. While converged state performances are not improved, this solution is shown to make the estimator more robust to friction torque spikes occurring on some reaction wheels, and to improve settling performances after spacecraft manoeuvers.</p>
----------------------------------------------------------------------
