This file contains corrected abstracts from DiVA for SCI added on 2024-12-05
----------------------------------------------------------------------
In diva2:1888871   - correct as is
----------------------------------------------------------------------
In diva2:1881342 
abstract is: 
<p>This thesis investigates the impedance of acoustic liners, to attenuate noise originating from jet engines and enable compliance with international standards and regulations regarding noise from airplane jet engines. Experimental tests of two supplied liners were conducted in an impedance tube; one liner with known and predictable properties, and one liner with unknown properties.</p><p>The tests included tonal excitations in the formats of stepped sine and random noise with frequencies within set boundaries. After post-processing of the captured data, the desired impedance could be analysed in terms of excitated frequencies and sound pressure levels.</p><p>The conclusions from this project are that both of the liners deviated from their expected behavior, which was that liner 1 should have been unaffected by the alternated sound pressure levels, and liner 3 should have shown bigger affection due to the changed sound pressure level. Since the results were different than expected, there might have been minor sources of error during the measurements. It could be investigated if there is leakage from the mounting of the liners, or if the 3D printing resolution is sufficient.</p><p>Because of limitations in time, there is more left in this project to investigate. Therefore, conducting similar studies where more frequencies, sound pressure levels, and multi-tonal measurements can be included, is suggested as future work.</p><p> </p>

corrected abstract:
<p>This thesis investigates the impedance of acoustic liners, to attenuate noise originating from jet engines and enable compliance with international standards and regulations regarding noise from airplane jet engines. Experimental tests of two supplied liners were conducted in an impedance tube; one liner with known and predictable properties, and one liner with unknown properties.</p><p>The tests included tonal excitations in the formats of stepped sine and random noise with frequencies within set boundaries. After post-processing of the captured data, the desired impedance could be analysed in terms of excitated frequencies and sound pressure levels.</p><p>The conclusions from this project are that both of the liners deviated from their expected behavior, which was that liner 1 should have been unaffected by the alternated sound pressure levels, and liner 3 should have shown bigger affection due to the changed sound pressure level. Since the results were different than expected, there might have been minor sources of error during the measurements. It could be investigated if there is leakage from the mounting of the liners, or if the 3D printing resolution is sufficient.</p><p>Because of limitations in time, there is more left in this project to investigate. Therefore, conducting similar studies where more frequencies, sound pressure levels, and multi-tonal measurements can be included, is suggested as future work.</p>

Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1881022 
abstract is: 
<p>This study compares transportation preferences among students and staff from two different universities: KTH in Stockholm and ITBA in Buenos Aires. The aim was to identify the factors influencing transportation decisions and their impact on perceptions regarding its usage. A survey was conducted at both universities to gain insights from participants and analyze the collected information, aiming to propose solutions that enhance the quality and sustainability of transportation. This work presents a novel analysis by comparing two cities with seemingly different characteristics in terms of transportation and urban development, with an emphasis on sustainability. The results obtained are positive as potential improvements in both systems could be observed. Moreover, the results have generated ideas to improve the adoption of green and sustainable transportation, without compromising the quality of existing transportation for members of these educational institutions. The importance of environmental awareness and its positive impact on quality of life and urban mobility is highlighted.</p><p> </p>

corrected abstract:
<p>This study compares transportation preferences among students and staff from two different universities: KTH in Stockholm and ITBA in Buenos Aires. The aim was to identify the factors influencing transportation decisions and their impact on perceptions regarding its usage. A survey was conducted at both universities to gain insights from participants and analyze the collected information, aiming to propose solutions that enhance the quality and sustainability of transportation. This work presents a novel analysis by comparing two cities with seemingly different characteristics in terms of transportation and urban development, with an emphasis on sustainability. The results obtained are positive as potential improvements in both systems could be observed. Moreover, the results have generated ideas to improve the adoption of green and sustainable transportation, without compromising the quality of existing transportation for members of these educational institutions. The importance of environmental awareness and its positive impact on quality of life and urban mobility is highlighted.</p>

Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1880964 
abstract is: 
<p>Urban Air mobility (UAM) promises reduced congestion on roads, reduced travel times and stronger overall efficiency in densely populated areas. However several challenges arise when wanting to implement UAM such as safety and social acceptance. The aim of this paper is to gain valuable insights how to implement safe and socially accepted UAM into society. Current regulations are discussed as well as X, Y and Z volumes in U-space, flight separations with ellipsoidal safety buffers, high speed corridors, landing separation at vertiports and airspace partition with voronoi diagrams are proposed and discussed. Social acceptance is addressed with some of the most prominent concerns e.g. safety, privacy and noise. Examples are set in Stockholm, Sweden, resulting in a maximum airspace occupation of 1 % which means 210 UAS (Unmanned Aircraft Systems) on each flight level. Sensitive areas and people with privacy concerns should have the option to opt-out of having their properties under the flight paths of UAM-vehicles. Concerns with UAM from the public has to be taken into great consideration for a successful implementation of UAM.</p>

corrected abstract:
<p>Urban Air mobility (UAM) promises reduced congestion on roads, reduced travel times and stronger overall efficiency in densely populated areas. However several challenges arise when wanting to implement UAM such as safety and social acceptance. The aim of this paper is to gain valuable insights how to implement safe and socially accepted UAM into society. Current regulations are discussed as well as X, Y and Z volumes in U-space, flight separations with ellipsoidal safety buffers, high speed corridors, landing separation at vertiports and airspace partition with voronoi diagrams are proposed and discussed. Social acceptance is addressed with some of the most prominent concerns e.g. safety, privacy and noise. Examples are set in Stockholm, Sweden resulting in a maximum airspace occupation of 1 % which means 210 UAS (Unmanned Aircraft Systems) on each flight level. Sensitive areas and people with privacy concerns should have the option to opt-out of having their properties under the flight paths of UAM-vehicles. Concerns with UAM from the public has to be taken into great consideration for a successful implementation of UAM.</p>

Note - only change was to remove the comma after "Sweden" as it is not in the original, although grammatically it might be correct. It might also be noted that commas should before and after the "e.g." - but are not in the original.
----------------------------------------------------------------------
In diva2:1880436   - correct as is
----------------------------------------------------------------------
In diva2:1880384   - correct as is
----------------------------------------------------------------------
In diva2:1880323 
abstract is: 
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p><p> </p>

corrected abstract:
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p>


Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:877595 
abstract is: 
<p> </p><p>A recent trend in the world is that more and more countries and therefore their mil-itaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to ad-opt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense com-pany, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an e˙ective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare di˙erent solutions. The delta model includes reference values for relevant costs that can be a˙ected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>
w='di˙erent' val={'c': 'different', 's': ['diva2:877595', 'diva2:1380198', 'diva2:891912', 'diva2:919302', 'diva2:1328904', 'diva2:891537', 'diva2:1440147', 'diva2:1087823'], 'n': 'missing ligature'}
w='ad-opt' val={'c': 'adopt', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='com-pany' val={'c': 'company', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='mil-itaries' val={'c': 'militaries', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='a˙ected' val={'c': 'affected', 's': 'diva2:877595', 'n': 'missing ligature'}
w='e˙ective' val={'c': 'effective', 's': 'diva2:877595', 'n': 'error due to ligature'}

corrected abstract:
<p>A recent trend in the world is that more and more countries and therefore their militaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to adopt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense company, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an effective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare different solutions. The delta model includes reference values for relevant costs that can be affected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>

Note - also remove the empty paragraph at the start.
----------------------------------------------------------------------
In diva2:711193 
abstract is: 
<p>Consensus problem with multi-agent systems has interested researchers in various areas. Its difficulties tend to appear when available information of each agent is limited for achieving consensus. Besides, it is not always the case that agents can catch the whole states of the others; an output is often the only possible measurement for each agent in applications. The idea of graph Laplacian is then of help to address such a troublesome situation. While every single agent obviously makes decision to achieve an individual goal of minimizing its own cost functional, all agents as a team can obtain even more improvement by cooperation in some cases, which leads to cooperative game theoretic approach. The main goal of this master thesis is to accomplish a combination of optimal control theory and cooperative game theory in order to solve the output consensus problem with limited network connectivity. Along with this combination, bargaining problems are considered out of necessity.</p><p> </p>

corrected abstract:
<p>Consensus problem with multi-agent systems has interested researchers in various areas. Its difficulties tend to appear when available information of each agent is limited for achieving consensus. Besides, it is not always the case that agents can catch the whole states of the others; an output is often the only possible measurement for each agent in applications. The idea of graph Laplacian is then of help to address such a troublesome situation.</p><p>While every single agent obviously makes decision to achieve an individual goal of minimizing its own cost functional, all agents as a team can obtain even more improvement by cooperation in some cases, which leads to cooperative game theoretic approach. The main goal of this master thesis is to accomplish a combination of optimal control theory and cooperative game theory in order to solve the output consensus problem with limited network connectivity. Along with this combination, bargaining problems are considered out of necessity.</p>

Note added missing paragraph break and removed empty paragraph
----------------------------------------------------------------------
In diva2:1211524 
abstract is: 
<p>This thesis in applied statistics and industrial economics examines the correlation between a number of market conditions on the Swedish and Global market and the yield difference between the Swedish stock market and the Global stock market. The report is based on data from the index MSCI Sweden Net Return, MSCI World Net Return and the Volatility index S&amp;P 500. The market conditions that have been examined are Bull markets, Bear markets, periods of high volatility. We also examined how the appreciation of the SEK in comparison to the USD and the yield of the Swedish stock market correlated with the yield difference between the Swedish Stock Market and the Global stock market. The correlation was examined using multiple linear regression. The results indicated a positive correlation between the yield difference between the Swedish stock market and the Global stock market and the yield of the Swedish stock market, the appreciation of the SEK compared to the USD and Bull markets. We found a negative correlation with Bear markets and no correlation at all with the volatility.</p><p> </p><p>The results are in line with what could be expected and give a stronger statistical ground for the idea that the Swedish stock market has larger fluctuations than the Global stock market during large-scale market fluctuations.</p>

corrected abstract:
<p>This thesis in applied statistics and industrial economics examines the correlation between a number of market conditions on the Swedish and Global market and the yield difference between the Swedish stock market and the Global stock market. The report is based on data from the index MSCI Sweden Net Return, MSCI World Net Return and the Volatility index S&amp;P 500. The market conditions that have been examined are Bull markets, Bear markets, periods of high volatility. We also examined how the appreciation of the SEK in comparison to the USD and the yield of the Swedish stock market correlated with the yield difference between the Swedish Stock Market and the Global stock market. The correlation was examined using multiple linear regression. The results indicated a positive correlation between the yield difference between the Swedish stock market and the Global stock market and the yield of the Swedish stock market, the appreciation of the SEK compared to the USD and Bull markets. We found a negative correlation with Bear markets and no correlation at all with the volatility.</p><p>The results are in line with what could be expected and give a stronger statistical ground for the idea that the Swedish stock market has larger fluctuations than the Global stock market during large-scale market fluctuations.</p>

Note - only change - removed empty paragraph in the middle
----------------------------------------------------------------------
In diva2:1674019 
abstract is: 
<p>Low Reynolds number airfoil analysis has become increasingly significant as urban air mobility vehicles and unmanned aerial vehicles surge in popularity. The Green Raven project at KTH Aero aims to use reflex airfoils where little data is available beyond classical analysis. Viscous formulations of the panel method and computational fluid dynamics (CFD) have been used to simulate lift, drag and moments for the MH61 and MH104 airfoils at different angles of attack (AOAs). XFOIL and CFD turbulence models such as Spalart-Allmaras (SA), k-w Shear Stress Transport (SST) with and without damping coefficients were used. The strengths and limitations of each model were used to justify results. Due to clear computational advantages, XFOIL produced adequate results and is tailored toward use in initial design stages where repeated measurements are crucial. The SA turbulence stood out as the model produced accurate results in a reasonable time. The abundance of published CFD material comparing different turbulence models increased the credibility of the results. The two airfoils had similar lift and drag characteristics at AOAs of 0-6 deg while the MH104 was superior near stall. However, due to the lack of experimental data of the airfoils no particular model could be commended or verified.</p><p> </p>

corrected abstract:
<p>Low Reynolds number airfoil analysis has become increasingly significant as urban air mobility vehicles and unmanned aerial vehicles surge in popularity. The Green Raven project at KTH Aero aims to use reflex airfoils where little data is available beyond classical analysis. Viscous formulations of the panel method and computational fluid dynamics (CFD) have been used to simulate lift, drag and moments for the MH61 and MH104 airfoils at different angles of attack (AOAs). XFOIL and CFD turbulence models such as Spalart-Allmaras (SA), 𝑘-ω Shear Stress Transport (SST) with and without damping coefficients were used. The strengths and limitations of each model were used to justify results. Due to clear computational advantages, XFOIL produced adequate results and is tailored toward use in initial design stages where repeated measurements are crucial. The SA turbulence stood out as the model produced accurate results in a reasonable time. The abundance of published CFD material comparing different turbulence models increased the credibility of the results. The two airfoils had similar lift and drag characteristics at AOAs of 0-6º while the MH104 was superior near stall. However, due to the lack of experimental data of the airfoils no particular model could be commended or verified.</p>
----------------------------------------------------------------------
In diva2:1778751 
abstract is: 
<p>Formula Student is a global engineering competition where university students collaborate to design, construct, and race formula-style cars. Aerodynamics is one aspect in the vehicle design that can improve on-track performance by increasing cornering and straight-line speed.</p><p>To improve the aerodynamics of KTH Formula Student's DeV18 vehicle, the side structure is being redesigned. The current model, DeV17, features an underperforming tunnel-based side structure. To address this issue, this had the goal to investigate a new multi-element wing design that utilizes ground effect.</p><p>The design study of the DeV18 vehicle is conducted using Siemens NX 2212 for 3D modelling and Simcenter Star-CCM+ 17.06.008-R8 for airflow simulations. To quickly investigate certain design parameters effect on the results, Design Manager Project inside Simcenter Star-CCM+ is used.</p><p>The resulting side structure produces a total of 26 N of downforce and 6 N of drag at 40 kph, more than twice that of DeV17’s side structure while also producing less drag. Although this significant improvement compared to DeV17, it is believed that further increases in performance are necessary to compete with top teams. By using a more sophisticated method to optimize the multi-element wing, such as adjoint optimization, the concept could be improved. However, the overall potential of the concept is still considered too limited to achieve the desired performance goals, which is why it will no longer be investigated further.</p><p> </p>

corrected abstract:
<p>Formula Student is a global engineering competition where university students collaborate to design, construct, and race formula-style cars. Aerodynamics is one aspect in the vehicle design that can improve on-track performance by increasing cornering and straight-line speed.</p><p>To improve the aerodynamics of KTH Formula Student's DeV18 vehicle, the side structure is being redesigned. The current model, DeV17, features an underperforming tunnel-based side structure. To address this issue, this had the goal to investigate a new multi-element wing design that utilizes ground effect.</p><p>The design study of the DeV18 vehicle is conducted using Siemens NX 2212 for 3D modelling and Simcenter Star-CCM+ 17.06.008-R8 for airflow simulations. To quickly investigate certain design parameters effect on the results, Design Manager Project inside Simcenter Star-CCM+ is used.</p><p>The resulting side structure produces a total of 26 N of downforce and 6 N of drag at 40 kph, more than twice that of DeV17’s side structure while also producing less drag. Although this significant improvement compared to DeV17, it is believed that further increases in performance are necessary to compete with top teams. By using a more sophisticated method to optimize the multi-element wing, such as adjoint optimization, the concept could be improved. However, the overall potential of the concept is still considered too limited to achieve the desired performance goals, which is why it will no longer be investigated further.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1776777 
abstract is: 
<p>In this Bachelor thesis, a novel algorithm for sampling from bandlimited circular probability distributions is presented. The algorithm leverages results from Fourier analysis concerning the Fejér kernel to simulate data with some desired probability distribution, realized as a sum of data sampled from a discrete distribution and a small continuous perturbation sampled from the Fejér kernel distribution. Relevant theory is presented before formally proving exact simulation using the algorithm. Experimental results confirm the validity of the theoretical results, and the efficiency of the algorithm is then compared with that of other sampling methods such as rejection sampling with a uniform envelope function.</p><p> </p>

corrected abstract:
<p>In this Bachelor thesis, a novel algorithm for sampling from bandlimited circular probability distributions is presented. The algorithm leverages results from Fourier analysis concerning the Fejér kernel to simulate data with some desired probability distribution, realized as a sum of data sampled from a discrete distribution and a small continuous perturbation sampled from the Fejér kernel distribution. Relevant theory is presented before formally proving exact simulation using the algorithm. Experimental results confirm the validity of the theoretical results, and the efficiency of the algorithm is then compared with that of other sampling methods such as rejection sampling with a uniform envelope function.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:525720 
abstract is: 
<p>Abstract</p><p> </p><p>In this report analysis of foreign exchange rates time series are performed. First, triangular arbitrage is detected and eliminated from data series using linear algebra tools. Then Vector Autoregressive processes are calibrated and used to replicate dynamics of exchange rates as well as to forecast time series. Finally, optimal portfolio of currencies with minimal Expected Shortfall is formed using one time period ahead forecasts</p>

corrected abstract:
<p>In this report analysis of foreign exchange rates time series are performed. First, triangular arbitrage is detected and eliminated from data series using linear algebra tools. Then Vector Autoregressive processes are calibrated and used to replicate dynamics of exchange rates as well as to forecast time series. Finally, optimal portfolio of currencies with minimal Expected Shortfall is formed using one time period ahead forecasts.</p>

Note - removed unnecessary "<p>Abstract</p><p> </p>" and added final missing period.
----------------------------------------------------------------------
In diva2:1566509 
abstract is: 
<p>This report is about a novel approach to attenuation of fan noise in aerial vehicles, by way of implementing a ducted fan in the chassis of a four meter blended wing body plane. Three different one meter PVC pipes were used and their performances as silencers were tested by measuring the sound power level and calculating the insulation loss compared to a fan by itself. The ducts were either empty or lined with acoustic absorbents and micro perforated panels. Experiments were carried out in the reverberation room at KTH Marcus Wallenberg laboratory for sound and vibration research using the guidelines in ISO 3741 (2010). The results showed that the empty duct lead to a 15.3 dB(A) insulation loss with no decrease in thrust from the fan. The absorbent and micro perforated panel, however, lead to a 22.7 dB(A) insulation loss while giving a major decrease in thrust of more than one order of magnitude. The results show the failure of implementation of the latter two silencers due to choking, but also the success of the empty duct. This shows that there is room for improvement and perhaps even a future possibility of a successful implementation in a real vehicle.</p><p> </p>

corrected abstract:
<p>This report is about a novel approach to attenuation of fan noise in aerial vehicles, by way of implementing a ducted fan in the chassis of a four meter blended wing body plane. Three different one meter PVC pipes were used and their performances as silencers were tested by measuring the sound power level and calculating the insulation loss compared to a fan by itself. The ducts were either empty or lined with acoustic absorbents and micro perforated panels. Experiments were carried out in the reverberation room at KTH Marcus Wallenberg laboratory for sound and vibration research using the guidelines in ISO 3741 (2010). The results showed that the empty duct lead to a 15.3 dB(A) insulation loss with no decrease in thrust from the fan. The absorbent and micro perforated panel, however, lead to a 22.7 dB(A) insulation loss while giving a major decrease in thrust of more than one order of magnitude. The results show the failure of implementation of the latter two silencers due to choking, but also the success of the empty duct. This shows that there is room for improvement and perhaps even a future possibility of a successful implementation in a real vehicle.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1673538 
abstract is: 
<p>The purpose of the study is to evaluate the possibility of using gridded ion thrusters as a means of attitude control for a solar sail as a part of the sunshade project, which aims to place 10^8 solar sail sunshade spacecraft, each with an area of about 10 000 m^2, at the Sun-Earth Lagrangian point L1 in order to reduce Earth's global temperature. Two types of solar sail sunshade spacecraft were studied. The first type, referred to as the sunshade demonstrator, had an area of 100 m^2 and a mass of 10 kg, and the second type, referred to as the full-sized sunshade, had an area of 10 000 m^2 and a mass of 90 kg. To determine the significance of using ion thrusters for the attitude control system, the mass of the required fuel, as well as the total mass that had to be added to the spacecraft to implement the attitude control system, was calculated. Two types of journeys were studied for each spacecraft type: starting from Low Earth Orbit (LEO) to L1 and from Geostationary Orbit (GEO) to L1, respectively. The results showed that the duration of the journey of the full-sized spacecraft was about 570 days from LEO to L1 and 370 from GEO to L1, respectively. The required amounts of fuel for the respective journeys were 580 g and 15 g, respectively, and resulted in a total additional mass of 7.8 kg and 7.2 kg, respectively.</p><p> </p>

corrected abstract:
<p>The purpose of the study is to evaluate the possibility of using gridded ion thrusters as a means of attitude control for a solar sail as a part of the sunshade project, which aims to place 10<sup>8</sup> solar sail sunshade spacecraft, each with an area of about 10 000 m<sup>2</sup>, at the Sun-Earth Lagrangian point L<sub>1</sub> in order to reduce Earth's global temperature. Two types of solar sail sunshade spacecraft were studied. The first type, referred to as the sunshade demonstrator, had an area of 100 m<sup>2</sup> and a mass of 10 kg, and the second type, referred to as the full-sized sunshade, had an area of 10 000 m<sup>2</sup> and a mass of 90 kg. To determine the significance of using ion thrusters for the attitude control system, the mass of the required fuel, as well as the total mass that had to be added to the spacecraft to implement the attitude control system, was calculated. Two types of journeys were studied for each spacecraft type: starting from Low Earth Orbit (LEO) to L<sub>1</sub> and from Geostationary Orbit (GEO) to L<sub>1</sub>, respectively. The results showed that the duration of the journey of the full-sized spacecraft was about 570 days from LEO to L<sub>1</sub> and 370 from GEO to L<sub>1</sub>, respectively. The required amounts of fuel for the respective journeys were 580 g and 15 g, respectively, and resulted in a total additional mass of 7.8 kg and 7.2 kg, respectively.</p>
----------------------------------------------------------------------
In diva2:1780538 
abstract is: 
<p>The displacement of the flow by a passing freight train can often result in dangerous conditions for railway equipment and people standing in the vicinity of the train. In this work, Computational Fluid Dynamics (CFD) simulations are performed to study the flow development around a moving freight train comprised of a Class 66 locomotive and four container wagons. The results will give a better insight into the effects that each flow structure can have in the flow within the train's slipstream. Both two- and three-dimensional simulations are carried out around the freight train using three different RANS turbulence models: the Spalart-Allmaras, the SST k-ω and the W&amp;J EARSM. Two cases of 10o and 30o crosswinds are also considered and compared to the no-crosswind case, as side-winds characterize the majority of real-life situations and are known to amplify the slipstream effects. The results are validated against available experimental and numerical data and they are thoroughly presented and discussed. The 30o crosswind case is also computed using a DDES simulation. A meshing strategy which involves the assembly of different mesh blocks with a non-matching interface boundary condition to create the complete domain is used and assessed, as an alternative meshing approach that can simplify and accelerate the set-up of different case-studies. Additionally, the two-dimensional study is used to assess the influence of different parameters on the solution, such as the grid resolution and the moving-ground boundary condition.</p><p> </p>

corrected abstract:
<p>The displacement of the flow by a passing freight train can often result in dangerous conditions for railway equipment and people standing in the vicinity of the train. In this work, Computational Fluid Dynamics (CFD) simulations are performed to study the flow development around a moving freight train comprised of a Class 66 locomotive and four container wagons. The results will give a better insight into the effects that each flow structure can have in the flow within the train's slipstream. Both two- and three-dimensional simulations are carried out around the freight train using three different RANS turbulence models: the Spalart-Allmaras, the SST 𝑘-ω and the W&amp;J EARSM. Two cases of 10º and 30º crosswinds are also considered and compared to the no-crosswind case, as side-winds characterize the majority of real-life situations and are known to amplify the slipstream effects. The results are validated against available experimental and numerical data and they are thoroughly presented and discussed. The 30º crosswind case is also computed using a DDES simulation. A meshing strategy which involves the assembly of different mesh blocks with a non-matching interface boundary condition to create the complete domain is used and assessed, as an alternative meshing approach that can simplify and accelerate the set-up of different case-studies. Additionally, the two-dimensional study is used to assess the influence of different parameters on the solution, such as the grid resolution and the moving-ground boundary condition.</p>
----------------------------------------------------------------------
In diva2:1780172 
abstract is: 
<p>In this report, we analyze general relativistic effects on celestial bodies, including gravitational strength in different metrics, gravitational radiation, and frame-dragging. We present simulation methods for classical and general relativistic motion, through the use of systems of equations that may be numerically integrated. The amount of energy leaving the system as gravitational radiation is approximated using the quadrupole formula, and by using a binary pair of planetary bodies as an approximation for orbital motion. Here we demonstrate that classical approximations may be suitable in low-mass high-distance scenarios. The eccentricity of an orbit also affects the gravitational radiation and would have to be much less than one for reliable results. It is concluded that frame-dragging effects are negligible for slowly rotating objects only, which is a well-known result.</p><p> </p>

corrected abstract:
<p>In this report, we analyze general relativistic effects on celestial bodies, including gravitational strength in different metrics, gravitational radiation, and frame-dragging. We present simulation methods for classical and general relativistic motion, through the use of systems of equations that may be numerically integrated. The amount of energy leaving the system as gravitational radiation is approximated using the quadrupole formula, and by using a binary pair of planetary bodies as an approximation for orbital motion. Here we demonstrate that classical approximations may be suitable in low-mass high-distance scenarios. The eccentricity of an orbit also affects the gravitational radiation and would have to be much less than one for reliable results. It is concluded that frame-dragging effects are negligible for slowly rotating objects only, which is a well-known result.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1218168 - missing space in title:
"Classification of social gestures: Recognizing waving using supervised machinelearning"
==>
"Classification of social gestures: Recognizing waving using supervised machine learning"


abstract is: 
<p>This paper presents an approach to gesture recognition including the use of a tool in order to extract certain key-points of the human body in each frame, and then processing this data and extracting features from this. The gestures recognized were two-handed waving and clapping. The features used were the maximum co-variance from a sine-fit to time-series of arm angles, as well as the max and min of this fitted sinus function. A support vector machine was used for the learning. The result was a promising accuracy of 93% <em>± </em>4% using 5-fold cross-validation. The limitations of the methods used are then discussed, which includes lack of support for more than one gesture in the data as well as some lack of generality in means of the features used. Finally some suggestions are made as to what improvements and further explorations could be made.</p><p> </p>

corrected abstract:
<p>This paper presents an approach to gesture recognition including the use of a tool in order to extract certain key-points of the human body in each frame, and then processing this data and extracting features from this. The gestures recognized were two-handed waving and clapping. The features used were the maximum co-variance from a sine-fit to time-series of arm angles, as well as the max and min of this fitted sinus function. A support vector machine was used for the learning. The result was a promising accuracy of 93%±4% using 5-fold cross-validation. The limitations of the methods used are then discussed, which includes lack of support for more than one gesture in the data as well as some lack of generality in means of the features used. Finally some suggestions are made as to what improvements and further explorations could be made.</p>

Note - removed the empty paragraph and removed the italization of "±" to match the original
----------------------------------------------------------------------
In diva2:802173 
abstract is: 
<p>A bank borrowing some money has to give some securities to the lender, which is called collateral. Different kinds of collateral can be posted, like cash in different currencies or a stock portfolio depending on the terms of the contract, which is called a Credit Support Annex (CSA). Those contracts specify eligible collateral, interest rate, frequency of collateral posting, minimum transfer amounts, etc. This guarantee reduces the counterparty risk associated with this type of transaction.</p><p>If a CSA allows for posting cash in different currencies as collateral, then the party posting collateral can, now and at each future point in time, choose which currency to post. This choice leads to optionality that needs to be accounted for when valuing even the most basic of derivatives such as forwards or swaps.</p><p>In this thesis, we deal with the valuation of embedded optionality in collateral contracts. We consider the case when collateral can be posted in two different currencies, which seems sufficient since collateral contracts are soon going to be simplified.</p><p>This study is based on the conditional independence approach proposed by Piterbarg [8]. This method is compared to both Monte-Carlo simulation and finite- difference method.</p><p>A practical application is finally presented with the example of a contract between Natixis and Barclays.</p><p> </p>

corrected abstract:
<p>A bank borrowing some money has to give some securities to the lender, which is called collateral. Different kinds of collateral can be posted, like cash in different currencies or a stock portfolio depending on the terms of the contract, which is called a Credit Support Annex (CSA). Those contracts specify eligible collateral, interest rate, frequency of collateral posting, minimum transfer amounts, etc. This guarantee reduces the counterparty risk associated with this type of transaction.</p><p>If a CSA allows for posting cash in different currencies as collateral, then the party posting collateral can, now and at each future point in time, choose which currency to post. This choice leads to optionality that needs to be accounted for when valuing even the most basic of derivatives such as forwards or swaps.</p><p>In this thesis, we deal with the valuation of embedded optionality in collateral contracts. We consider the case when collateral can be posted in two different currencies, which seems sufficient since collateral contracts are soon going to be simplified.</p><p>This study is based on the conditional independence approach proposed by Piterbarg [8]. This method is compared to both Monte-Carlo simulation and finite-difference method.</p><p>A practical application is finally presented with the example of a contract between Natixis and Barclays.</p>
----------------------------------------------------------------------
In diva2:1440982 
abstract is: 
<p>This report presents the design of the wing structure for a UAV called Skywalker X8. A model of the UAV was given and analyzed to design a wing box structure that is twice the size of the current model, with "greener" technology and lightweight materials. The loads that act upon the UAV were simulated and thereafter analyzed with the help of the CFD program called Star CCM+. Modifications on the CAD model and the FEM simulations were performed in Siemens NX. Eight different combinations were tested from the following five materials: CFRP (carbon fiber reinforced polymer), LDPE (low density polyethylene), polyethylene, polypropylene, and balsa wood. The results that best fit the requirements given was the combination of polypropylene as the wing skin and balsa as the honeycomb structure. This design weighed 3.576 kg and had the following stresses: 0.671 MPa, 0.340 MPa, 1 MPa, and 4 MPa for the angle of attacks at 1,2,3, and 6 degrees respectively. A modification of the trailing edge, which was the implementation of a Gurney flap, was made to see if it improved the lift-to-drag ratio, but unfortunately it did not so it was not developed further.</p><p> </p>


corrected abstract:
<p>This report presents the design of the wing structure for a UAV called Skywalker X8. A model of the UAV was given and analyzed to design a wing box structure that is twice the size of the current model, with “greener” technology and lightweight materials. The loads that act upon the UAV were simulated and thereafter analyzed with the help of the CFD program called Star CCM+. Modifications on the CAD model and the FEM simulations were performed in Siemens NX. Eight different combinations were tested from the following five materials: CFRP (carbon fiber reinforced polymer), LDPE (low density polyethylene), polyethylene, polypropylene, and balsa wood. The results that best fit the requirements given was the combination of polypropylene as the wing skin and balsa as the honeycomb structure. This design weighed 3.576 kg and had the following stresses: 0.671 MPa, 0.340 MPa, 1 MPa, and 4 MPa for the angle of attacks at 1,2,3, and 6 degrees respectively. A modification of the trailing edge, which was the implementation of a Gurney flap, was made to see if it improved the lift-to-drag ratio, but unfortunately it did not so it was not developed further.</p>

Note error in original
mc='1,2,3' c='1, 2, 3'
Also removed unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:1220102 
abstract is: 
<p>Chatbots, also called conversational agents, with speech interfaces are being used to a greater and greater extent, but there are still many areas that are not completely explored. The idea of this project was born out of the belief that there is a need for an assistant in the kitchen that is able to search for recipes, answer questions regarding them and guide and assist the user throughout the cooking process, all through conversation since the hands are busy. This paper begins with an introduction in the subject of conversational agents and the related technology, then similar, already existing studies and methods are presented with their pros and cons. After follows an in-depth explanation on how the program was constructed into a working kitchen assistant. Lastly, the users’ experiences of the performance and usability of the program was evaluated through tests and discussed. It turns out that conversational agents definitely can be integrated in the kitchen, and according to several sources, in a few years they will be implemented in all possible areas and change the technology of our time.</p><p> </p>

corrected abstract:
<p>Chatbots, also called conversational agents, with speech interfaces are being used to a greater and greater extent, but there are still many areas that are not completely explored. The idea of this project was born out of the belief that there is a need for an assistant in the kitchen that is able to search for recipes, answer questions regarding them and guide and assist the user throughout the cooking process, all through conversation since the hands are busy. This paper begins with an introduction in the subject of conversational agents and the related technology, then similar, already existing studies and methods are presented with their pros and cons. After follows an in-depth explanation on how the program was constructed into a working kitchen assistant. Lastly, the users’ experiences of the performance and usability of the program was evaluated through tests and discussed. It turns out that conversational agents definitely can be integrated in the kitchen, and according to several sources, in a few years they will be implemented in all possible areas and change the technology of our time.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1880482 
abstract is: 
<p>A time-effective coverage path can be decisive in catastrophic and war scenarios for saving countless lives where UAVs are used to scan an area looking for an objective. Given an area shaped as a polygon, a quadratic decomposition method is used to discretize the area into nodes. A model of the optimization problem constraint is created and solved using mixed-integer linear programming, taking into consideration simple dynamics and coverage path planning definitions. Simulations in different scenarios are presented, showing that the presence of no-fly zones can negatively affect the coverage time. The relationship between coverage time and the number of UAVs employed is nonlinear and converges to a constant value. The result has a direct impact on the evaluation of benefits and the cost of adding UAVs to a search mission.</p><p> </p>

corrected abstract:
<p>A time-effective coverage path can be decisive in catastrophic and war scenarios for saving countless lives where UAVs are used to scan an area looking for an objective. Given an area shaped as a convex polygon, a grid-based decomposition method is used to discretize the area into squares, represented by nodes. An optimization problem, considering simple dynamics and coverage path planning definitions, is developed using mixed-integer linear programming framework. Simulations in different scenarios are presented, showing that the presence of no-fly zones can negatively affect the coverage time. The relationship between coverage time and the number of UAVs employed is nonlinear and converges to a constant value. The result has a direct impact on the evaluation of benefits and the cost of adding UAVs to search missions.</p>

Note the change in wording (based on the original) and remove of the unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:414817 
abstract is: 
<p>The purpose with this thesis work is to simulate the deflection due to creep of Kanthal(R) APMT furnace tubes using the finite element method (FEM). Kanthal APMT is a material which shows a larger primary creep compared to other metals. Therefore the creep deformation must be described with a material model which takes both primary and secondary creep into consideration. In this thesis work a material model called time hardening has been used.</p>
<p>*C2 is stress dependent. By modifying C2 so that the results from the simulations better corresponds with test data an equation for how C2 depends on the stress could be obtained.</p>
<p>The value for C2 is then calculated for each tube dimension giving results which are close to the data from sagging tests. The results may be seen as an overestimation of the actual deflection. A sensitivity analysis showed that the model is very sensitive to changes in the material parameters. A few percent change in C2 for example will change the deflection by more than 100 percent. </p>
<p>
<p>* For equation see full text</p>
<p>
<p> </p>
<p> </p>
</p>
</p>

corrected abstract:
<p>The purpose with this thesis work is to simulate the deflection due to creep of Kanthal® APMT furnace tubes using the finite element method (FEM). Kanthal APMT is a material which shows a larger primary creep compared to other metals. Therefore the creep deformation must be described with a material model which takes both primary and secondary creep into consideration. In this thesis work a material model called time hardening has been used.</p>
<p>C2 is stress dependent<sup><a href="#fn1" id="ref1">*</a></sup>. By modifying C2 so that the results from the simulations better corresponds with test data an equation for how C2 depends on the stress could be obtained.</p>
<p>The value for C2 is then calculated for each tube dimension giving results which are close to the data from sagging tests. The results may be seen as an overestimation of the actual deflection. A sensitivity analysis showed that the model is very sensitive to changes in the material parameters. A few percent change in C2 for example will change the deflection by more than 100 percent.</p>
<div id="footnotes">
    <ul style="list-style: '*'; padding-left: 20px;">
        <li id="fn1">For equation see full text <a href="#ref1" aria-label="Back to reference">↩</a></li>
    </ul>
</div>

----------------------------------------------------------------------
In diva2:894096 
abstract is: 
<p>E-sports is growing and the price pools in e-sports tournaments are increasing, Valves video game DotA 2 is one of the bigger e-sports. As professional gamers train to increase their skill, new tools to help the training might become very important. Eye tracking can give an extra training dimension for the gamer. The aim of this master thesis is to develop a Visual Attention Index for DotA 2, that is, a number that reflects the player’s visual attention during a game. Interviews with gamers combined with data collection from gamers with eye trackers and statistical methods were used to find relevant metrics to use in the work. The results show that linear regression did not work very well on the data set, however, since there were a low number of test persons, further data collection and testing needs to be done before any statistically significant conclusions can be drawn. Support Vector Machines (SVM) was also used and turned out to be an effective way of separating better players from less good players. A new SVM method, based on linear programming, was also tested and found to be efficient and easy to apply on the given data set.</p><p> </p>

corrected abstract:
<p>E-sports is growing and the price pools in e-sports tournaments are increasing, Valves video game DotA <span style="font-size: 0.8em;">&#x1D7E4;</span> is one of the bigger e-sports. As professional gamers train to increase their skill, new tools to help the training might become very important. Eye tracking can give an extra training dimension for the gamer. The aim of this master thesis is to develop a Visual Attention Index for DotA <span style="font-size: 0.8em;">&#x1D7E4;</span>, that is, a number that reflects the player’s visual attention during a game. Interviews with gamers combined with data collection from gamers with eye trackers and statistical methods were used to find relevant metrics to use in the work. The results show that linear regression did not work very well on the data set, however, since there were a low number of test persons, further data collection and testing needs to be done before any statistically significant conclusions can be drawn. Support Vector Machines (SVM) was also used and turned out to be an effective way of separating better players from less good players. A new SVM method, based on linear programming, was also tested and found to be efficient and easy to apply on the given data set.</p>

Note removed the unnecessary empty paragraph and change "2" to "&#x1D7E4;" - a Mathematical Sans-Serif Digit Two;  <span style="font-size: 0.8em;"> has been used to reduce the relative font size to match the look of the original
----------------------------------------------------------------------
In diva2:1319924 
abstract is: 
<p>Credit scoring using machine learning has been gaining attention within the research field in recent decades and it is widely used in the financial sector today. Studies covering binary credit scoring of securitized non-performing loans are however very scarce. This paper is using random forest and artificial neural networks to predict debt recovery for such portfolios. As a performance benchmark, logistic regression is used. Due to the nature of high imbalance between the classes, the performance is evaluated mainly on the area under both the receiver operating characteristic curve and the precision-recall curve. This paper shows that random forest, artificial neural networks and logistic regression have similar performance. They all indicate an overall satisfactory ability to predict debt recovery and hold potential to be implemented in day-to-day business related to non-performing loans.</p><p> </p>

corrected abstract:
<p>Credit scoring using machine learning has been gaining attention within the research field in recent decades and it is widely used in the financial sector today. Studies covering binary credit scoring of securitized non-performing loans are however very scarce. This paper is using random forest and artificial neural networks to predict debt recovery for such portfolios. As a performance benchmark, logistic regression is used. Due to the nature of high imbalance between the classes, the performance is evaluated mainly on the area under both the receiver operating characteristic curve and the precision-recall curve. This paper shows that random forest, artificial neural networks and logistic regression have similar performance. They all indicate an overall satisfactory ability to predict debt recovery and hold potential to be implemented in day-to-day business related to non-performing loans.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1568336 
abstract is: 
<p>X-ray computed tomography (CT) has since its introduction in the early 1970s become one of the most important tools used for medical imaging. In CT, a large number of x-ray attenuation measurements are combined and reconstructed to form a three-dimensional image of the targeted area. In the recent years, a new type of detector called photon counting detector (PCD) has attracted considerable interest. This new type of detector acquires spectral information is associated with several benefits and has shown to be very valuable. </p><p>Furthermore, the use of deep learning to reconstruct images produced by CT has attracted significant attention in the last couple of years. However, the best way of incorporating deep learning into the reconstruction chain into the reconstruction chain is still incompletely understood. Additionally, the use of deep learning has mainly been investigated for the case of conventional CT and not for CT performed with PCDs. It these two points that this work aims to address. </p><p>Multiple deep learning architectures were implemented and evaluated on material images acquired by simulating a PCD. The deep-learning part of the reconstruction took the form of image-domain denoising after the material images had been obtained from the material sinograms through filtered back projection. Then, a comparison between the different deep learning architectures was made to find out which architecture is the most suited for denoising images produced by PCDs in the image domain.</p><p> </p>

corrected abstract:
<p>X-ray computed tomography (CT) has since its introduction in the early 1970s become one of the most important tools used for medical imaging. In CT, a large number of x-ray attenuation measurements are combined and reconstructed to form a three-dimensional image of the targeted area. In the recent years, a new type of detector called photon counting detector (PCD) has attracted considerable interest. This new type of detector acquires spectral information is associated with several benefits and has shown to be very valuable.</p><p>Furthermore, the use of deep learning to reconstruct images produced by CT has attracted significant attention in the last couple of years. However, the best way of incorporating deep learning into the reconstruction chain into the reconstruction chain is still incompletely understood. Additionally, the use of deep learning has mainly been investigated for the case of conventional CT and not for CT performed with PCDs. It these two points that this work aims to address.</p><p>Multiple deep learning architectures were implemented and evaluated on material images acquired by simulating a PCD. The deep-learning part of the reconstruction took the form of image-domain denoising after the material images had been obtained from the material sinograms through filtered back projection. Then, a comparison between the different deep learning architectures was made to find out which architecture is the most suited for denoising images produced by PCDs in the image domain.</p>

Note - only change to remove the empty paragraph and removing unnecessary spaces at end of paragraphs.
----------------------------------------------------------------------
In diva2:1334765 
abstract is: 
<p>A global statement about a compact surface with constant Gaussian curvature is derived by elementary differential geometry methods. Surfaces and curves embedded in three-dimensional Euclidian space are introduced, as well as several key properties such as the tangent plane, the first and second fundamental form, and the Weingarten map. Furthermore, intrinsic and extrinsic properties of surfaces are analyzed, and the Gaussian curvature, originally derived as an extrinsic property, is proven to be an intrinsic property in Gauss Theorema Egregium. Lastly, through the aid of umbilical points on a surface, the statement that a compact, connected surface with constant Gaussian curvature is a sphere is proven.</p><p> </p>

corrected abstract:
<p>A global statement about a compact surface with constant Gaussian curvature is derived by elementary differential geometry methods. Surfaces and curves embedded in three-dimensional Euclidian space are introduced, as well as several key properties such as the tangent plane, the first and second fundamental form, and the Weingarten map. Furthermore, intrinsic and extrinsic properties of surfaces are analyzed, and the Gaussian curvature, originally derived as an extrinsic property, is proven to be an intrinsic property in Gauss Theorema Egregium. Lastly, through the aid of umbilical points on a surface, the statement that a compact, connected surface with constant Gaussian curvature is a sphere is proven.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1682470 
abstract is: 
<p>We present and prove some important theorems regarding determinantal point processes. In particular we focus on existance and uniqueness theorems. Furthermore, we present an algorithm for generating determinantal point processes with a finite-dimensional projection kernel. Also, we go through the mathematical preliminaries required to understand the theory.</p><p> </p>

corrected abstract:
<p>We present and prove some important theorems regarding determinantal point processes. In particular we focus on existance and uniqueness theorems. Furthermore, we present an algorithm for generating determinantal point processes with a finite-dimensional projection kernel. Also, we go through the mathematical preliminaries required to understand the theory.</p>

Note spelling error:
----------------------------------------------------------------------
In diva2:1380177 
abstract is: 
<p>The objective of this project is the development of a mission analysis tool for the nanosatellite company GomSpace Sweden. Although there are many existing software, they can be quite complicated and time consuming to use. The goal of this work is to build a simple app to be used at the earliest stages of space missions in order to obtain key figures of merit quickly and easily. By comparing results, assessing the feasibility of customer needs, analysing how various parameters affect each other, it enables immediate deeper understanding of the implications of the main design decisions that are taken at the very beginning of a mission. The tool shall aid the system engineering process of determining orbit manoeuvre capability specifically for CubeSat electric propulsion systems taking into account the most relevant factors for perturbation in Low Earth Orbit (LEO), i.e. atmospheric drag and Earth’s oblateness effects. The manoeuvres investigated are: orbit raising from an insert orbit to an operating orbit, orbit maintenance, deorbiting within the space debris mitigation guidelines and collision avoidance within the 12 to 24 hours that the system has to react. The manoeuvres cost is assessed in terms of Delta v requirements, propellant mass and transfer times. The tool was developed with MATLAB and packaged as a standalone Linux application.</p><p> </p>

corrected abstract:
<p>The objective of this project is the development of a mission analysis tool for the nanosatellite company GomSpace Sweden. Although there are many existing software, they can be quite complicated and time consuming to use. The goal of this work is to build a simple app to be used at the earliest stages of space missions in order to obtain key figures of merit quickly and easily. By comparing results, assessing the feasibility of customer needs, analysing how various parameters affect each other, it enables immediate deeper understanding of the implications of the main design decisions that are taken at the very beginning of a mission. The tool shall aid the system engineering process of determining orbit manoeuvre capability specifically for CubeSat electric propulsion systems taking into account the most relevant factors for perturbation in Low Earth Orbit (LEO), i.e. atmospheric drag and Earth’s oblateness effects. The manoeuvres investigated are: orbit raising from an insert orbit to an operating orbit, orbit maintenance, deorbiting within the space debris mitigation guidelines and collision avoidance within the 12 to 24 hours that the system has to react. The manoeuvres cost is assessed in terms of ∆v requirements, propellant mass and transfer times. The tool was developed with MATLAB and packaged as a standalone Linux application.</p>

Note - only change to remove the empty paragraph and replacing "Delta" with "∆".
----------------------------------------------------------------------
In diva2:1524971 
abstract is: 
<p>In this master thesis an input-model of a Nordic BWR power plant has been developed in APROS. The plant model contains key systems and major thermohydraulic components of the steam cycle, including I&amp;C systems (i.e. power, pressure, level and flow controls). The plant model is primarily designed for balance of plant studies at discrete power levels.</p><p>The input-model of the power plant focuses especially on the steam cycle which is crucial for analysing water and steam behaviour and its influence on the reactor power. At the current stage, the model primarily handles steady-state conditions of full-power operation, which has been the design point. It has also been shown that reduced-power operation can be simulated with a reasonable trendline of pressure and temperature progression over facility components.</p><p> </p>

corrected abstract:
<p>Nuclear power plants have proved to produce reliable and economic electricity but at the same time provoked debates mainly because of the risks involved during operation. To prevent unwanted events, it is important to identify and estimate their occurrence, and introduce measures to counteract them. Modelling and simulations are powerful tools that can be used to gain this type of knowledge and obtain information about the birthplace of incidents. However, the area of use is not limited to the safety perspective, as computer simulations can also introduce true advances in performance and reliability of power plants.</p><p>This master thesis was conducted at Westinghouse Electric Sweden AB, with the purpose to design and implement an input-model of a Nordic Boiling Water Reactor in APROS. The inputmodel of the power plant focused especially on the steam cycle which is crucial for analysing water and steam behaviour in the power plant and its influence on the reactor power. The inputmodel has been limited to representsteady-state conditions at full-power operation, and to some extent reduced-power operation. Thereby, plant model is primarily designed for Balance of Plant studies at discrete power levels.</p><p>The first section of the report contains an introduction to the concepts of nuclear energy and fundamentals of boiling water reactors. It is supposed to provide the reader with a basis for a fair understanding of nuclear power plant operation. Theoretical concepts of thermodynamics and fluid mechanics, which have been crucial for a proper approach in the process of creating the input-model, can be found in the theory section. The report does also contain a brief description of the plant systems upon which the design has been based on.</p><p>The report consists of further sections, where the model components and their implementation are presented followed by a model validation. The model validation is performed by a comparison approach, where simulation data is presented in relationship to reference data. The validation is done for full-power and reduce-power operation, at steady-state conditions, at which the model has shown to have decent compliance with the available reference data.</p><p>At the final stage of the project, the created input-model was used to evaluate an induced perturbation of feedwater temperature. The behaviour of the reactor, dependent of the feedwater temperature, is discussed for two simulation cases; with and without forced power control. The simulation enabled to perform a first step analysis of the effectiveness of the power control system.</p>

Note the abstract in the thesis is completely different from that in DiVA!
----------------------------------------------------------------------
In diva2:699782 
abstract is: 
<p>The dose distribution in the Gamma Knife (developed and produced by Elekta) is optimized over the weights (or Beam-on time) using different models other than the radiosurgical one used in Leksell Gamma Plan . These are based on DVH, EUD, TCP and NTCP. Also adding hypoxic regions are tested in the Gamma Knife to see whether or not the dose can be guided to these areas. This is done in two ways. For the DVH and EUD model the hypoxic area is regarded as a organ by itself and higher constraints is defined on it. In the TCP case blood vessels are outlined and the α and β parameters are perturbed to describe a hypoxic area. The models are tested in two cases. The first one is one tumour close to the brainstem and the second case is two tumours located far away from each other. Finally the results are compared to the dose distribution computed by the Gamma Knife.</p><p> </p>

corrected abstract:
<p>The dose distribution in the Gamma Knife is optimized over the weights (or Beam-on time) using different models other than the radiosurgical one used in Leksell Gamma Plan®. These are based on DVH, EUD, TCP and NTCP. Also adding hypoxic regions are tested in the Gamma Knife to see whether or not the dose can be guided to these areas. This is done in two ways. For the DVH and EUD model the hypoxic area is regarded as a organ by itself and higher constraints is defined on it. In the TCP case blood vessels are outlined and the α and β parameters are perturbed to describe a hypoxic area. The models are tested in two cases. The first one is one tumour close to the brainstem and the second case is two tumours located far away from each other. Finally the results are compared to the dose distribution computed by the Gamma Knife.</p>

Nore removed text that was not in the original, added the registered trademark symbol, and eliminated the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1519571 
abstract is: 
<p>For the purpose of americium recycling, the effect of americium content on the nuclear fuel behaviour needs to be investigated. Atomic scale simulations and classical molecular dynamic simulations provide a tool of choice for the study of thermophysical properties of the nuclear fuel.</p><p>In this work, we fitted a new interatomic empirical potential for (U,Am)O2 based on the CRG formalism. Our work enabled us to propose at the same time a new potential for the study of the Am-O system. The proposed potentials show good agreement with lattice parameters and enthalpy increments. We finally computed the heat capacity of (U,Am)O2 from 350 K to 3200 K for 0, 10, 20, 30, 40 and 50% americium contents using the potential obtained. The heat capacities calculated reveal a Bredig transition, as seen in UO2 and (U,Pu)O2. This transition shifts toward lower temperatures and its peak decreases in intensity when the Am content increases.</p><p> </p>

corrected abstract:
<p>For the purpose of americium recycling, the effect of americium content on the nuclear fuel behaviour needs to be investigated. Atomic scale simulations and classical molecular dynamic simulations provide a tool of choice for the study of thermophysical properties of the nuclear fuel.</p><p>In this work, we fitted a new interatomic empirical potential for (U,Am)O<sub>2</sub> based on the CRG formalism. Our work enabled us to propose at the same time a new potential for the study of the Am-O system. The proposed potentials show good agreement with lattice parameters and enthalpy increments. We finally computed the heat capacity of (U,Am)O<sub>2</sub> from 350 K to 3200 K for 0, 10, 20, 30, 40 and 50% americium contents using the potential obtained. The heat capacities calculated reveal a Bredig transition, as seen in UO<sub>2</sub> and (U,Pu)O<sub>2</sub>. This transition shifts toward lower temperatures and its peak decreases in intensity when the Am content increases.</p>

Note - only change to remove the empty paragraph and adding the subscripts
----------------------------------------------------------------------
In diva2:1779375 
abstract is: 
<p>The purpose of this project is to simulate the detection of γ-ray spectra emitted by radon isotopes and their daughters. This is done as a contribution to the development of radiation detectors to be used in a research project investigating the possibility of using increased amounts of the radioactive gas radon as an earthquake precursor. Before the onset of an earthquake, microcracks are formed in the surrounding stone structures due to stress, releasing greater than usual amounts of radon gas contained within the rock pores. A way of predicting an upcoming earthquake would then be to place radiation detectors in areas with high seismicity in order to measure possible changes. This could be done in soil, groundwater (via springs, wells, and boreholes), or air. In this project, we aim to understand how measurements in groundwater would differ from ones in air, and how to best make use of the spectra as seen in water. This was done by simulating a scenario in which a scintillator detector, made of cesium iodide, is placed in each media and then assessing the resulting γ-ray spectra.</p><p> </p>

corrected abstract:
<p>The purpose of this project is to simulate the detection of γ-ray spectra emitted by radon isotopes and their daughters. This is done as a contribution to the development of radiation detectors to be used in a research project investigating the possibility of using increased amounts of the radioactive gas radon as an earthquake precursor. Before the onset of an earthquake, microcracks are formed in the surrounding stone structures due to stress, releasing greater than usual amounts of radon gas contained within the rock pores. A way of predicting an upcoming earthquake would then be to place radiation detectors in areas with high seismicity in order to measure possible changes. This could be done in soil, groundwater (via springs, wells, and boreholes), or air. In this project, we aim to understand how measurements in groundwater would differ from ones in air, and how to best make use of the spectra as seen in water. This was done by simulating a scenario in which a scintillator detector, made of cesium iodide, is placed in each media and then assessing the resulting γ-ray spectra.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:373694
Note: no full text in DiVA

abstract is: 
<p>The department of Medical Technology, where I have done Master thesis project, develops and researches new method and technique within areas where ultrasound can be used to obtain the image of anatomical structure, functional capabilities and to suggest required treatment.</p>
<p>Nowadays cardio-vascular diseases, such as infarct, atherosclerosis and ischemic syndrome, are one of the most widespread diseases in the world that’s why timely detection, identification and treatment are so important.</p>
<p>The Master of Science qualification report consists 3 major parts: Medico-biological part, Design and Research parts.</p>
<p><strong>In Medico-biological part </strong>has been analyzed anatomical and physiological structure of the heart, current status of echocardiography with comparing with other techniques, summary of ultrasound methods with list of parameters that can be achieved is presented.</p>
<p><strong>In Design part </strong>has been developed new graphical modality based on Delta-V pump model using vector based statistical analysis for identification patients with ischemia. Software algorithm for automatically determine characteristic points for state diagram written in MatLab has been developed and implemented.</p>
<p><strong>In Research part </strong>in the first task using commercially available software based on Principal Component Analysis collected data from the hospital patients has been studied, results proved hypothesis concerning time variables importance; in the second task graphical module has been examined using collected data from the hospital patients both normal and with different cardio-vascular disease, and the results show good detection power of the algorithm.</p>
<p>At the end of the project presentation has been done and report has been published.</p>
<p>This project has been done in collaboration with the biggest medical institute in Sweden – Karolinska Institute - and results will be used in medical practice in Karolinska University Hospital in Huddinge and for future scientific needs.</p>
<p> </p>

corrected abstract:
<p>The department of Medical Technology, where I have done Master thesis project, develops and researches new method and technique within areas where ultrasound can be used to obtain the image of anatomical structure, functional capabilities and to suggest required treatment.</p>
<p>Nowadays cardio-vascular diseases, such as infarct, atherosclerosis and ischemic syndrome, are one of the most widespread diseases in the world that’s why timely detection, identification and treatment are so important.</p>
<p>The Master of Science qualification report consists 3 major parts: Medico-biological part, Design and Research parts.</p>
<p><strong>In Medico-biological part </strong>has been analyzed anatomical and physiological structure of the heart, current status of echocardiography with comparing with other techniques, summary of ultrasound methods with list of parameters that can be achieved is presented.</p>
<p><strong>In Design part </strong>has been developed new graphical modality based on Delta-V pump model using vector based statistical analysis for identification patients with ischemia. Software algorithm for automatically determine characteristic points for state diagram written in MatLab has been developed and implemented.</p>
<p><strong>In Research part </strong>in the first task using commercially available software based on Principal Component Analysis collected data from the hospital patients has been studied, results proved hypothesis concerning time variables importance; in the second task graphical module has been examined using collected data from the hospital patients both normal and with different cardio-vascular disease, and the results show good detection power of the algorithm.</p>
<p>At the end of the project presentation has been done and report has been published.</p>
<p>This project has been done in collaboration with the biggest medical institute in Sweden – Karolinska Institute - and results will be used in medical practice in Karolinska University Hospital in Huddinge and for future scientific needs.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1673952 
abstract is: 
<p>The conventional tube-and-wing aircraft has been around since the 1950s, with little to no innovative progress being made towards redesigning the conventional aircraft. The blended wing body (BWB) shape fuses the wing of the aircraft with the fuselage increasing structural strength while also increasing potential surface area to create lift, making it more efficient than conventional wing shapes. Today aviation has a 2 % CO2 contribution to global emissions. Aircraft manufacturers are predicting a steady rise for the aviation industry. The contribution of green-house gases is set to increase exponentially. Hydrogen fuel cells could deem a good fit between traditional combustion engine aircraft and electrical aircraft having a high efficiency but also being fuel-based. This report investigates the possibility of a prototype model of the Project ''Green Raven'' from KTH of creating a hybrid fuel cell BWB UAV with a 4 m wingspan. The analytical data is from literature and available benchmark data. First, an electrically driven subscale prototype is made and tested, and then the full-scale model is made. The prototype is pro-posed to be driven by a single two-bladed propeller with 10 x 4.7-inch dimensions running at 10000-13000 rpm with a takeoff weight of 4 kg, where 0.75 kg of the weight was from 5 Li-Po batteries. Performance parameters were calculated by given data with a given cruise speed of 30 m/s and a cruise endurance of 1 hour. The prototype will fly for close to maximum load at climb with an angle of 6°. With the Li-Po batteries with a total of 11 Ah, the aircraft has more than 10 % to spare for safety reasons.</p><p> </p>

corrected abstract:
<p>The conventional tube-and-wing aircraft has been around since the 1950s, with little to no innovative progress being made towards redesigning the conventional aircraft. The blended wing body (BWB) shape fuses the wing of the aircraft with the fuselage increasing structural strength while also increasing potential surface area to create lift, making it more efficient than conventional wing shapes. Today aviation has a 2 % CO<sub>2</sub> contribution to global emissions. Aircraft manufacturers are predicting a steady rise for the aviation industry. The contribution of greenhouse gases is set to increase exponentially. Hydrogen fuel cells could deem a good fit between traditional combustion engine aircraft and electrical aircraft having a high efficiency but also being fuel-based. This report investigates the possibility of a prototype model of the Project ”Green Raven” from KTH of creating a hybrid fuel cell BWB UAV with a 4 m wingspan. The analytical data is from literature and available benchmark data. First, an electrically driven subscale prototype is made and tested, and then the full-scale model is made. The prototype is proposed to be driven by a single two-bladed propeller with 10 x 4.7-inch dimensions running at 10000-13000 rpm with a takeoff weight of 4 kg, where 0.75 kg of the weight was from 5 Li-Po batteries. Performance parameters were calculated by given data with a given cruise speed of 30 m/s and a cruise endurance of 1 hour. The prototype will fly for close to maximum load at climb with an angle of 6°. With the Li-Po batteries with a total of 11 Ah, the aircraft has more than 10 % to spare for safety reasons.</p>

Note - only changes to remove the empty paragraph, added subscript, and removed the unnecessary hpyehn in "greenhouse".
----------------------------------------------------------------------
In diva2:1342226 
abstract is: 
<p>Functional testing is a vital process when building a satellite. However, often using flight-ready hardware for testing is not feasible. The work in this project has been to construct a flight representative model of the antenna deployment system for the KTH student-built MIST satellite. Specifically, the focus has been on creating a physical simulator for the antenna system. The purpose of the simulator created is to achieve the correct behavior, but without the need to use the real flight hardware. The challenges mainly concern establishing communication between the on-board computer of the satellite and the microcontroller on the created antenna deployment system, via the I$^2$C bus, and ensuring that physical responses occur in a useful manner. Further, the simulator needed to implement software with the same functionality as the real system. The microcontroller used in this project was an Arduino Due that represented the antenna deployment system's microcontroller. All the functions, e.g. temperature sensor and LEDs, were put together on a custom-made add-on circuit for the Arduino. Moreover, a 3D-printed model has been made for the deployment mechanism of the antenna elements. A simulation of the antenna system has been produced, determining whether a custom-built simulator can be used for functional testing of the antenna deployment system. The simulator can later be used for functional testing of the MIST satellite and also be the base for testing the deployment of the solar panels.</p><p> </p>

corrected abstract:
<p>Functional testing is a vital process when building a satellite. However, often using flight-ready hardware for testing is not feasible. The work in this project has been to construct a flight representative model of the antenna deployment system for the KTH student-built MIST satellite. Specifically, the focus has been on creating a physical simulator for the antenna system. The purpose of the simulator created is to achieve the correct behavior, but without the need to use the real flight hardware.</p><p>The challenges mainly concern establishing communication between the on-board computer of the satellite and the microcontroller on the created antenna deployment system, via the I<sup>2</sub>C bus, and ensuring that physical responses occur in a useful manner. Further, the simulator needed to implement software with the same functionality as the real system. The microcontroller used in this project was an Arduino Due that represented the antenna deployment system's microcontroller.</p><p>All the functions, e.g. temperature sensor and LEDs, were put together on a custom-made add-on circuit for the Arduino. Moreover, a 3D-printed model has been made for the deployment mechanism of the antenna elements. A simulation of the antenna system has been produced, determining whether a custom-built simulator can be used for functional testing of the antenna deployment system. The simulator can later be used for functional testing of the MIST satellite and also be the base for testing the deployment of the solar panels.</p>
----------------------------------------------------------------------
In diva2:1334807 
abstract is: 
<p>In this report, we study an abstract representation of reflection groups called Coxeter groups. Firstly, we introduce some important aspects of group theory. Next, we describe a concept called the word problem. Then, a way of defining groups given a set of generators and relations is presented. This theory is used to define the Coxeter groups, followed by a complete classification of the finite Coxeter groups as presented by H.S.M. Coxeter in 1935. Finally, we present a solution to the word problem for Coxeter groups and discuss some applications.</p><p> </p>

corrected abstract:
<p>In this report, we study an abstract representation of reflection groups called Coxeter groups. Firstly, we introduce some important aspects of group theory. Next, we describe a concept called the word problem. Then, a way of defining groups given a set of generators and relations is presented. This theory is used to define the Coxeter groups, followed by a complete classification of the finite Coxeter groups as presented by H.S.M. Coxeter in 1935. Finally, we present a solution to the word problem for Coxeter groups and discuss some applications.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:839498 
abstract is: 
<p>Our immune system uses antibodies to neutralize pathogens such as bacteria and viruses. Antibodies bind to parts of foreign proteins with high efficiency and specificity. We call such parts epitopes. The identification of epitopes, namely epitope mapping, may contribute to various immunological applications such as vaccine design, antibody production and immunological diagnosis.</p><p>Therefore, a fast and reliable method that can predict epitopes from the whole proteome is highly desirable.</p><p> </p><p>In this work we have developed a computational method that predicts epitopes based on sequence information. We focus on using local alignment to extract features from peptides and classifying them using Support Vector Machine. We also propose two approaches to optimize the features. Results show that our method can reliably predict epitopes and significantly outperforms some most commonly used tools.</p><p> </p>

corrected abstract:
<p>Our immune system uses antibodies to neutralize pathogens such as bacteria and viruses. Antibodies bind to parts of foreign proteins with high efficiency and specificity. We call such parts epitopes. The identification of epitopes, namely epitope mapping, may contribute to various immunological applications such as vaccine design, antibody production and immunological diagnosis. Therefore, a fast and reliable method that can predict epitopes from the whole proteome is highly desirable.</p><p>In this work we have developed a computational method that predicts epitopes based on sequence information. We focus on using local alignment to extract features from peptides and classifying them using Support Vector Machine. We also propose two approaches to optimize the features. The results show that our method can reliably predict epitopes and significantly outperforms some most commonly used tools.</p>
----------------------------------------------------------------------
In diva2:1879496 
abstract is: 
<p>Life insurance companies rely on mortality rate models to set appropriate premiums for their services. Over the past century, average life expectancy has increased and continues to do so, necessitating more accurate models. Two commonly used models are the Gompertz-Makeham law of mortality and the Lee-Carter model. The Gompertz-Makeham model depends solely on an age variable, while the Lee-Carter model incorporates a time-varying aspect which accounts for the increase in life expectancy over time. This paper constructs both models using training data acquired from Skandia Mutual Life Insurance Company and compares them to validation data from the same set. The study suggests that the Lee-Carter model may be able to offer some improvements compared to the Gompertz-Makeham law of mortality in terms of predicting future mortality rates. However, due to a lack of qualitative data, creating a competitive Lee-Carter model through Singular Value Decomposition, SVD, proved to be problematic. Switching from the current Gompertz-Makeham model to the Lee-Carter model should, therefore, be explored further when more high quality data becomes available.</p><p> </p>

corrected abstract:
<p>Life insurance companies rely on mortality rate models to set appropriate premiums for their services. Over the past century, average life expectancy has increased and continues to do so, necessitating more accurate models. Two commonly used models are the Gompertz-Makeham law of mortality and the Lee-Carter model. The Gompertz-Makeham model depends solely on an age variable, while the Lee-Carter model incorporates a time-varying aspect which accounts for the increase in life expectancy over time. This paper constructs both models using training data acquired from Skandia Mutual Life Insurance Company and compares them to validation data from the same set. The study suggests that the Lee-Carter model may be able to offer some improvements compared to the Gompertz-Makeham law of mortality in terms of predicting future mortality rates. However, due to a lack of qualitative data, creating a competitive Lee-Carter model through Singular Value Decomposition, SVD, proved to be problematic. Switching from the current Gompertz-Makeham model to the Lee-Carter model should, therefore, be explored further when more high quality data becomes available.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1800189 
abstract is: 
<p>In this master thesis, the accuracy of the crack depth meter RMG 4015 was evaluated for different types of cracks with various damage mechanisms. In total, 61 crack depth measurements were conducted with the crack depth meter on 56 cracks which were located in the 23 different test pieces supplied by Kiwa. The measured crack depths were then compared to the true crack depths, which were determined by cutting the test pieces and measuring directly on the cross-sections of the cracks using a light optical microscope. The results of the comparison showed that the RMG 4015, which uses potential drop techniques, was very accurate at measuring both strain induced and alkaline stress corrosion cracks. However, the results also showed that the crack depth meter underestimates chloride induced stress corrosion cracks, corrosion fatigue cracks and stress corrosion cracks/hydrogen embrittlement cracks at varying degrees. Therefore, the main recommendation for Kiwa is to switch the RMG 4015 to a crack depth meter that uses ultrasonic techniques instead.</p><p>The master thesis also explored the possibilities to improve an FE model produced by Kiwa in a previous project which involved an analysis of a cracked component. The present crack depth measure program included a test piece from this component. The stress distribution in the original model did not represent the cracks found in the real structure and it was suspected to be the result of some boundary conditions not corresponding to those acting in the actual pipe system. Some adjustments to the boundary conditions and contact regions were made and a new improved model with a better representing stress distribution was found.</p><p> </p>

corrected abstract:
<p>In this master thesis, the accuracy of the crack depth meter RMG 4015 was evaluated for different types of cracks with various damage mechanisms. In total, 61 crack depth measurements were conducted with the crack depth meter on 56 cracks which were located in the 23 different test pieces supplied by Kiwa. The measured crack depths were then compared to the true crack depths, which were determined by cutting the test pieces and measuring directly on the cross-sections of the cracks using a light optical microscope. The results of the comparison showed that the RMG 4015, which uses potential drop techniques, was very accurate at measuring both strain induced and alkaline stress corrosion cracks. However, the results also showed that the crack depth meter underestimates chloride induced stress corrosion cracks, corrosion fatigue cracks and stress corrosion cracks/hydrogen embrittlement cracks at varying degrees. Therefore, the main recommendation for Kiwa is to switch the RMG 4015 to a crack depth meter that uses ultrasonic techniques instead.</p><p>The master thesis also explored the possibilities to improve an FE model produced by Kiwa in a previous project which involved an analysis of a cracked component. The present crack depth measure program included a test piece from this component. The stress distribution in the original model did not represent the cracks found in the real structure and it was suspected to be the result of some boundary conditions not corresponding to those acting in the actual pipe system. Some adjustments to the boundary conditions and contact regions were made and a new improved model with a better representing stress distribution was found.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1111160 
abstract is: 
<p>Gaussian process methods are flexible non-parametric Bayesian methods used for regression and classification. They allow for explicit handling of uncertainty and are able to learn complex structures in the data. Their main limitation is their scaling characteristics: for n training points the complexity is <em>O</em>(n³) for training and <em>O</em>(n²) for prediction per test data point.</p><p>This makes full Gaussian process methods prohibitive to use on training sets larger than a few thousand data points. There has been recent research on approximation methods to make Gaussian processes scalable without severely affecting the performance. Some of these new approximation techniques are still not fully investigated and in a practical situation it is hard to know which method to choose. This thesis examines and evaluates scalable GP methods, especially focusing on the framework Massively Scalable Gaussian Processes introduced by Wilson et al. in 2016, which reduces the training complexity to nearly <em>O</em>(<em>n</em>) and the prediction complexity to <em>O</em>(1). The framework involves inducing point methods, local covariance function interpolation, exploitations of structured matrices and projections to low-dimensional spaces. The properties of the different approximations are studied and the possibilities of making improvements are discussed.</p><p> </p>

corrected abstract:
<p>Gaussian process methods are flexible non-parametric Bayesian methods used for regression and classification. They allow for explicit handling of uncertainty and are able to learn complex structures in the data. Their main limitation is their scaling characteristics: for 𝑛 training points the complexity is &Oscr;(𝑛<sup>3</sup>) for training and &Oscr;(𝑛<sup>2</sup>) for prediction per test data point. This makes full Gaussian process methods prohibitive to use on training sets larger than a few thousand data points.</p><p>There has been recent research on approximation methods to make Gaussian processes scalable without severely affecting the performance. Some of these new approximation techniques are still not fully investigated and in a practical situation it is hard to know which method to choose. This thesis examines and evaluates scalable GP methods, especially focusing on the framework Massively Scalable Gaussian Processes introduced by Wilson et al. in 2016, which reduces the training complexity to nearly &Oscr;(𝑛) and the prediction complexity to &Oscr;(1). The framework involves inducing point methods, local covariance function interpolation, exploitations of structured matrices and projections to low-dimensional spaces. The properties of the different approximations are studied and the possibilities of making improvements are discussed.</p>
----------------------------------------------------------------------
In diva2:1440824
Note: no full text in DiVA

abstract is: 
<p>This thesis focuses on the design and qualities of a gamma type Stirling engine from a thermodynamic point of view. The purpose is to calculate the efficiency of the gamma type Stirling engine, as well as the work output. An ideal thermodynamic Stirling cycle consists of an isothermal expansion, isochoric heat removal, isothermal compression, and lastly isochoric heat addition. An advantage with the Stirling engine is that it is able to work using any form of working gas and is described to work in a closed regenerative thermodynamic state. The gathered data from measuring the pressure and volume when energy is applied is used to calculate different values such as efficiency and net work from the engine. The data was collected by using different tools. A heat sensor was taped on the bottom plate to measure the temperature at a specific time. Secondly, a pressure sensor was connected to one of the six tubes on the top plate. Where the pressure tube was connected, varied in order to analyze if there was a difference in pressure at different distances from the center of the top plate. A photosensor was used to indicate when a full revolution had occurred so that the right data to represent a full cycle could be collected. The result was PV and Ts-diagrams for each pipe at different temperatures. The results indicated that there is a pressure difference of 800 Pa. By integrating these diagrams, the net work could be calculated. The highest measured net work was $0.32mJ$ through pipe 2 when the bottom plate has a temperature of 80°C. In conclusion, changing the placement of the pipe showed no remarkable differences, however, the theoretical efficiency increased with the temperature. The engine has more parts that can be analyzed such as the materialistic parameters and the relation in volume and temperature difference but are not taken into account in this thesis.</p><p> </p>

corrected abstract:
<p>This thesis focuses on the design and qualities of a gamma type Stirling engine from a thermodynamic point of view. The purpose is to calculate the efficiency of the gamma type Stirling engine, as well as the work output. An ideal thermodynamic Stirling cycle consists of an isothermal expansion, isochoric heat removal, isothermal compression, and lastly isochoric heat addition. An advantage with the Stirling engine is that it is able to work using any form of working gas and is described to work in a closed regenerative thermodynamic state. The gathered data from measuring the pressure and volume when energy is applied is used to calculate different values such as efficiency and net work from the engine. The data was collected by using different tools. A heat sensor was taped on the bottom plate to measure the temperature at a specific time. Secondly, a pressure sensor was connected to one of the six tubes on the top plate. Where the pressure tube was connected, varied in order to analyze if there was a difference in pressure at different distances from the center of the top plate. A photosensor was used to indicate when a full revolution had occurred so that the right data to represent a full cycle could be collected. The result was PV and Ts-diagrams for each pipe at different temperatures. The results indicated that there is a pressure difference of 800 Pa. By integrating these diagrams, the net work could be calculated. The highest measured net work was <em>0.32mJ</em> through pipe 2 when the bottom plate has a temperature of 80°C. In conclusion, changing the placement of the pipe showed no remarkable differences, however, the theoretical efficiency increased with the temperature. The engine has more parts that can be analyzed such as the materialistic parameters and the relation in volume and temperature difference but are not taken into account in this thesis.</p>

Note - only change to remove the empty paragraph and to set "0.32mJ" in italics (as it was an inline equation)
----------------------------------------------------------------------
In diva2:1342442 
abstract is: 
<p>In this report we demonstrate the usefulness of hidden Markov model estimation as a method to construct models of mouse behavior. We used a neural network to retrieve positional data of different body parts from overhead video recordings of lone mice in an enclosure. We then extracted features such as velocity and elongation from the positional data and used an implementation of the Baum-Welch algorithm to fit hidden Markov models to the feature data. We could identify recurring behaviors such as "running next to wall" and "investigating wall" among the estimated states in several different mice, which was consistent with what we could see in the actual videos. We thereby demonstrate that hidden Markov model estimation by the Baum-Welch algorithm can be utilized to automatically find models of mouse behavior.</p><p> </p>

corrected abstract:
<p>In this report we demonstrate the usefulness of hidden Markov model estimation as a method to construct models of mouse behavior. We used a neural network to retrieve positional data of different body parts from overhead video recordings of lone mice in an enclosure. We then extracted features such as velocity and elongation from the positional data and used an implementation of the Baum-Welch algorithm to fit hidden Markov models to the feature data. We could identify recurring behaviors such as ”running next to wall” and ”investigating wall” among the estimated states in several different mice, which was consistent with what we could see in the actual videos. We thereby demonstrate that hidden Markov model estimation by the Baum-Welch algorithm can be utilized to automatically find models of mouse behavior.</p>

Note - only change to remove the empty paragraph and fixed double quotes to match the original
----------------------------------------------------------------------
In diva2:1781495 
abstract is: 
<p>Capacitive Deionization (CDI) is an energy-efficient desalination technology that utilizes an electric field to extract ions from water. Flow-through CDI systems show potential for superior desalination performance compared to traditional flow-by CDI; however, they face the challenge of increased occurrence of Faradaic reactions, leading to undesired by-products and reduced energy efficiency. In this study, we constructed a flow-through CDI cell and investigated the desalination performance of the two possible cell configurations: upstream anode mode and downstream anode mode. A series of experiments were conducted, measuring conductivity and pH of the effluent solution during charging and discharging phases. The results were analyzed in terms of salt adsorption capacity and charge efficiency. We used pH fluctuations in the effluent solution as indicators of Faradaic reactions. It was found that upstream anode mode yielded superior desalination, with a salt adsorption capacity of 6.79 mg/g and charge efficiency of 64.3%, compared to downstream anode mode, which displayed a salt adsorption capacity of 5.19 mg/g and charge efficiency of 50.8%. However, upstream anode mode also produced more pronounced pH oscillations, suggesting a higher occurrence of Faradaic reactions. Reconciling these conflicting results and shedding light on the complex processes within the CDI cell calls for further investigation.</p><p> </p>

corrected abstract:
<p>Capacitive Deionization (CDI) is an energy-efficient desalination technology that utilizes an electric field to extract ions from water. Flow-through CDI systems show potential for superior desalination performance compared to traditional flow-by CDI; however, they face the challenge of increased occurrence of Faradaic reactions, leading to undesired by-products and reduced energy efficiency. In this study, we constructed a flow-through CDI cell and investigated the desalination performance of the two possible cell configurations: upstream anode mode and downstream anode mode. A series of experiments were conducted, measuring conductivity and pH of the effluent solution during charging and discharging phases. The results were analyzed in terms of salt adsorption capacity and charge efficiency. We used pH fluctuations in the effluent solution as indicators of Faradaic reactions. It was found that upstream anode mode yielded superior desalination, with a salt adsorption capacity of 6.79 mg g<sup>-1</sup> and charge efficiency of 64.3%, compared to downstream anode mode, which displayed a salt adsorption capacity of 5.19 mg g<sup>-1</sup> and charge efficiency of 50.8%. However, upstream anode mode also produced more pronounced pH oscillations, suggesting a higher occurrence of Faradaic reactions. Reconciling these conflicting results and shedding light on the complex processes within the CDI cell calls for further investigation.</p>

Note - only change to remove the empty paragraph and change the "/g" into "g<sup>-1</sup>" as in the original
----------------------------------------------------------------------
In diva2:1354140 
Note: no full text in DiVA

abstract is: 
<p>In this project, the fatigue behaviour of aluminium (AL 5083 H111) gusset and flange joined with a fillet weld, is investigated through experiments and numerical methods. The work aims at improved knowledge on fatigue in an aluminium welded joint subjected to constant amplitude varying load.</p><p> </p><p>The results of the experiments are investigated with the Basquin equation. The mean curve is estimated by the maximum likelihood estimation (MLE) by using both the failed specimen data and the run-out data. From the mean curve, a component specific design curve is estimated. Comparison of the component specific design curve with the recommendation’s design curve highlighted the inherent conservatism of the recommendation’s design curve.</p><p> </p><p>Nominal, hot-spot and equivalent notch methods were evaluated, and a comparative study was performed. The numerical investigation showed that the predicted fatigue life increased with model complexity. Comparison to the experimentally derived component specific design curve highlighted non-conservatism of the numerically predicted fatigue life for large stress ranges. The degree of conservatism of the numerical methods is however strongly affected by the slope of the considered design curve.</p>

corrected abstract:
<p>In this project, the fatigue behaviour of aluminium (AL 5083 H111) gusset and flange joined with a fillet weld, is investigated through experiments and numerical methods. The work aims at improved knowledge on fatigue in an aluminium welded joint subjected to constant amplitude varying load.</p><p>The results of the experiments are investigated with the Basquin equation. The mean curve is estimated by the maximum likelihood estimation (MLE) by using both the failed specimen data and the run-out data. From the mean curve, a component specific design curve is estimated. Comparison of the component specific design curve with the recommendation’s design curve highlighted the inherent conservatism of the recommendation’s design curve.</p><p>Nominal, hot-spot and equivalent notch methods were evaluated, and a comparative study was performed. The numerical investigation showed that the predicted fatigue life increased with model complexity. Comparison to the experimentally derived component specific design curve highlighted non-conservatism of the numerically predicted fatigue life for large stress ranges. The degree of conservatism of the numerical methods is however strongly affected by the slope of the considered design curve.</p>


Note - only change to remove the empty paragraphs
----------------------------------------------------------------------
In diva2:1436832 
abstract is: 
<p>In this report, we first briefly summarize Hermitian quantum mechanics before moving on to the non-Hermitian case. We then review PT-symmetric quantum mechanics with a focus on finite-dimensional systems, and include a novel generalization of a perturbative calculation of the C-operator. After briefly covering the basics of neutrino oscillations, we perturbatively examine a PT-symmetric addition to the neutrino oscillation Hamiltonian. We examine the effects of the addition with two different definitions of transition probabilities. However, probability is not conserved to first order with either definition. Further, we note that the effect of the chosen perturbation is to shift the transition probabilities by some phase, and to change the amplitudes of the transition probabilities.</p><p> </p>

corrected abstract:
<p>In this report, we first briefly summarize Hermitian quantum mechanics before moving on to the non-Hermitian case. We then review &Pscr;&Tscr;-symmetric quantum mechanics with a focus on finite-dimensional systems, and include a novel generalization of a perturbative calculation of the &Cscr;-operator. After briefly covering the basics of neutrino oscillations, we perturbatively examine a &Pscr;&Tscr;-symmetric addition to the neutrino oscillation Hamiltonian. We examine the effects of the addition with two different definitions of transition probabilities. However, probability is not conserved to first order with either definition. Further, we note that the effect of the chosen perturbation is to shift the transition probabilities by some phase, and to change the amplitudes of the transition probabilities.</p>

Note - removed the empty paragraph and corrected the script characters
----------------------------------------------------------------------
In diva2:1878884 
abstract is: 
<p>Reinforcement learning (RL) algorithms aim to identify optimal action sequences for an agent in a given environment, traditionally maximizing the expected rewards received from the environment by taking each action and transitioning between states. This thesis explores approaching RL distributionally, replacing the expected reward function by the full distribution over the possible rewards received, known as the value distribution. We focus on the quantile regression distributional RL (QR-DQN) algorithm introduced by Dabney et al. (2017), which models the value distribution by representing its quantiles. With such information of the value distribution, we modify the QR-DQN algorithm to enhance the agent's risk sensitivity. Our risk-averse algorithm is evaluated against the original QR-DQN in the Atari 2600 and in the Gymnasium environment, specifically in the games Breakout, Pong, Lunar Lander and Cartpole. Results indicate that the risk-averse variant performs comparably in terms of rewards while exhibiting increased robustness and risk aversion. Potential refinements of the risk-averse algorithm are presented.</p><p> </p>

corrected abstract:
<p>Reinforcement learning (RL) algorithms aim to identify optimal action sequences for an agent in a given environment, traditionally maximizing the expected rewards received from the environment by taking each action and transitioning between states. This thesis explores approaching RL distributionally, replacing the expected reward function by the full distribution over the possible rewards received, known as the value distribution. We focus on the quantile regression distributional RL (QR-DQN) algorithm introduced by Dabney et al. (2017), which models the value distribution by representing its quantiles. With such information of the value distribution, we modify the QR-DQN algorithm to enhance the agent's risk sensitivity. Our risk-averse algorithm is evaluated against the original QR-DQN in the Atari 2600 and in the Gymnasium environment, specifically in the games Breakout, Pong, Lunar Lander and Cartpole. Results indicate that the risk-averse variant performs comparably in terms of rewards while exhibiting increased robustness and risk aversion. Potential refinements of the risk-averse algorithm are presented.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:735921 
abstract is: 
<p>This paper presents PDEs that describes sedimentation by a system of diffusion and transportation equations. These PDEs are implemented with a semi-implicit scheme and solved on a Graphics Processing Unit (GPU). The equations are solved with the iterative solvers (conjugate gradient and biconjugate gradient stabilized method) provided by the software ViennaCL. The timings from these operations are compared with a CPU implementation.</p><p>Before using the iterative solvers, a sparse matrix and a right hand side vector is set. The sparse matrix and the right hand side vector are efficiently updated on the GPU. The implicit terms of the PDEs are stored in the sparse matrix and the explicit terms in the right hand side vector. The sparse matrix is stored in the compressed sparse row (CSR) format. Algorithms to update the sparse matrix for the PDEs, which have Neumann or a mix of Neumann and Dirichlet boundary conditions, are presented. As the values in the sparse matrix depend on values from the previous results, the sparse matrix has to be updated frequently. Considerable time is saved by updating the sparse matrix on the GPU instead of on the CPU (slow data transfers between CPU and GPU are reduced).</p><p>The speedup for the GPU implementation was found to be 8-10 and 12-18 for the GPUs GTX 590 and K20m respectively, depending on grid size. The high speedup is due to the CPU model of the CPUs used for timings being an older model. If a newer CPU model were used, the speedup would be lower. Due to limited access to newer hardware, a more accurate value for speedup comparison has not been acquired. Indications still prove that the GPU implementation is faster than the sequential CPU implementation.</p><p> </p>

corrected abstract:
<p>This paper presents PDEs that describes sedimentation by a system of diffusion and transportation equations. These PDEs are implemented with a semi-implicit scheme and solved on a Graphics Processing Unit (GPU). The equations are solved with the iterative solvers (conjugate gradient and biconjugate gradient stabilized method) provided by the software ViennaCL. The timings from these operations are compared with a CPU implementation.</p><p>Before using the iterative solvers, a sparse matrix and a right hand side vector is set. The sparse matrix and the right hand side vector are efficiently updated on the GPU. The implicit terms of the PDEs are stored in the sparse matrix and the explicit terms in the right hand side vector. The sparse matrix is stored in the compressed sparse row (CSR) format. Algorithms to update the sparse matrix for the PDEs, which have Neumann or a mix of Neumann and Dirichlet boundary conditions, are presented. As the values in the sparse matrix depend on values from the previous results, the sparse matrix has to be updated frequently. Considerable time is saved by updating the sparse matrix on the GPU instead of on the CPU (slow data transfers between CPU and GPU are reduced).</p><p>The speedup for the GPU implementation was found to be 8-10 and 12-18 for the GPUs GTX 590 and K20m respectively, depending on grid size. The high speedup is due to the CPU model of the CPUs used for timings being an older model. If a newer CPU model were used, the speedup would be lower. Due to limited access to newer hardware, a more accurate value for speedup comparison has not been acquired. Indications still prove that the GPU implementation is faster than the sequential CPU implementation.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1350076 
abstract is: 
<p>In this report we investigate the exotic hadrons known as pentaquarks. A brief overview of relevant concepts and theory is initially presented in order to aid the reader. Thereafter, the history of this field with regards to theory and experiments is discussed. In particular, a group theoretic classification of these states is studied. A simple mass formula for pentaquark states is examined and predictions are subsequently made about the composition and mass of possible pentaquark states. Furthermore, this mass formula is modified to examine and predict additional pentaquark states. A number of numerical fits concerning the masses of pentaquarks are performed and studied. Future research is explored with regards to the information presented in this thesis.</p><p> </p>

corrected abstract:
<p>In this report we investigate the exotic hadrons known as pentaquarks. A brief overview of relevant concepts and theory is initially presented in order to aid the reader. Thereafter, the history of this field with regards to theory and experiments is discussed. In particular, a group theoretic classification of these states is studied. A simple mass formula for pentaquark states is examined and predictions are subsequently made about the composition and mass of possible pentaquark states. Furthermore, this mass formula is modified to examine and predict additional pentaquark states. A number of numerical fits concerning the masses of pentaquarks are performd and studied. Future research is explored with regards to the information presented in this thesis.</p>

Note - only change to remove the empty paragraph and replacing "performed" with "performd" - error in the original
----------------------------------------------------------------------
In diva2:1873671 
abstract is: 
<p>In this report, we present a novel Bayesian inference framework to reconstruct the three-dimensional initial conditions of cosmic structure formation from data. To achieve this goal, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. According to the cosmological paradigm, all observable structures were formed from tiny primordial quantum fluctuations generated during the early stages of the Universe. As time passed, these seed fluctuations grew via gravitational aggregation to form the presently observed cosmic web traced by galaxies. For this reason, the specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we develop a novel machine learning approach that leverages the hierarchical nature of structure formation. We demonstrate our method in a mock analysis and find that we can recover the initial conditions with high accuracy, showing the potential of our model.</p><p> </p>

corrected abstract:
<p>In this report, we present a novel Bayesian inference framework to reconstruct the three-dimensional initial conditions of cosmic structure formation from data. To achieve this goal, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. According to the cosmological paradigm, all observable structures were formed from tiny primordial quantum fluctuations generated during the early stages of the Universe. As time passed, these seed fluctuations grew via gravitational aggregation to form the presently observed cosmic web traced by galaxies. For this reason, the specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we develop a novel machine learning approach that leverages the hierarchical nature of structure formation. We demonstrate our method in a mock analysis and find that we can recover the initial conditions with high accuracy, showing the potential of our model.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1342319 
abstract is: 
<p>To balance and control different aircraft, it is often necessary to use some type of control system, the less stable the craft is without any control system, the more advanced the system is required to be. This project is an experiment which purpose is to attempt to control a very unstable craft using steerable rudders. A design of the craft is modeled in CAD after a rough estimation to determine the required capacity of the components. Then a simulation of the craft is modeled in Matlab’s Simulink environment, which is used to test the control system’s capabilities and determine its optimal settings. Finally a physical model is built to see if the control system is sufficiently designed to stabilize the vessel under real conditions, which when flown did not successfully balance due to insufficient roll capability. Different solutions to this problem and other potential improvements is then discussed.</p><p> </p>

corrected abstract:
<p>To balance and control different aircraft it is often necessary to use some type of control system, the less stable the craft is without any control system, the more advanced the system is required to be. This project is an experiment which goal is to attempt to control a very unstable craft using steerable rudders. A design of the craft is modeled in CAD after a rough estimation to determine the required capacity of the components. Then a simulation of the craft is modeled in Matlab’s Simulink environment which is used to test the control system’s capabilities and determine its optimal settings, the simulation was considered successful because it showed that the craft could balance. Finally a physical model is built to test if the control system is sufficient to stabilize the vessel under real world conditions. When the real aircraft was flown it did not successfully balance due to insufficient roll capability caused by the motor torque being larger than expected. Different solutions to this problem and other potential improvements is then discussed.</p>

Note many changes in the wording betweeen the DiVA and original text, also removed the unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:1450318 
abstract is: 
<p>Sheet-swept connection solutions are a method for joining structural pipes in aircraft constructions. It is a simple approach that avoids the extensive demands placed on, among other things, welded connections. Previously, calculation data were not available, which this study aims to meet in the form of specifications and comparisons. Graham Lee's drawings and design specifications for the replica variant of the Nieuport 12 aircraft have been followed in the analysis of the connection solution. The method is based on three parts, calculations, experimental testing and FEM analysis. These form a specification of the joint's four most central load cases and their strength: tension (1130 N), plane deflection (4.5 Nm), in plane deflection (17.5 Nm) and torsion (4.5 Nm). The results are compared with calculations of loads in the pendulum rudder to determine how well the connection solution is suitable for this purpose. Identified load cases in the rudder are: plane deflection (3.2 Nm) and torsion (≤3.2 Nm). The results indicate that this type of connection is weak in the proposed purpose. The analysis highlights clear weaknesses in the connection solution and recommendations for improvements are given, these are primarily aimed at the thickness of the gusset and the introduction of an additional rivet. Furthermore, the sheet-swept joint solution is compared with welded joints which prove to be more suitable for this application.</p><p> </p>

corrected abstract:
<p>Sheet-swept connection solutions are a method for joining structural pipes in aircraft constructions. It is a simple approach that avoids the extensive demands placed on, among other things, welded connections. Previously, calculation data were not available, which this study aims to meet in the form of specifications and comparisons. Graham Lee's drawings and design specifications for the replica variant of the Nieuport 12 aircraft have been followed in the analysis of the connection solution. The method is based on three parts, calculations, experimental testing and FEM analysis. These form a specification of the joint's four most central load cases and their strength: tension (1130 N), plane deflection (4.5 Nm), in plane deflection (17.5 Nm) and torsion (4.5 Nm). The results are compared with calculations of loads in the pendulum rudder to determine how well the connection solution is suitable for this purpose. Identified load cases in the rudder are: plane deflection (3.2 Nm) and torsion (≤3.2 Nm). The results indicate that this type of connection is weak in the proposed purpose. The analysis highlights clear weaknesses in the connection solution and recommendations for improvements are given, these are primarily aimed at the thickness of the gusset and the introduction of an additional rivet. Furthermore, the sheet-swept joint solution is compared with welded joints which prove to be more suitable for this application.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1441692 
abstract is: 
<p>This report is part of a bachelor’s degree project in solid mechanics at KTH, Stockholm. It is performed by two students on behalf of the social enterprise Better Shelter, which provides shelters for people displaced by war and natural disasters. The scope of the project is to expand Better Shelters product specifications by providing improvements of the shelter earth anchors. This would allow use of the shelter in areas affected by higher wind speeds and thereby help more people in need of temporary housing and shelters. The earth anchors prevent the shelter from uplifting and tilting by taking uplift forces when horizontal wind loads acts on the structure. Two wind models with wind speeds up to 36 m/s were created to find the reaction forces on the anchors resisting the wind load. The wind models were compared with each other to validate the results and find the largest reaction forces on the anchors. Simulations of the anchors were made to analyse occurring stresses due to wind loads. Redesigns of the current earth anchor were made to find improvements of the anchor shape and reduce the stresses on the anchor. Experiments were then performed to analyse the redesigned anchor shapes in practise. The redesign, calculations and computational analyses of the anchors were done using the programs SolidEdge, ANSYS and Matlab. Results showed that redesigning the anchor contact area with the anchor wire reduced the stresses on the anchors drastically. Increasing the wing size of the anchors proved to be successful for preventing anchors from being pulled out of the soil. This allows better use of the current anchor material volume. Experiments also proved that burying the anchor deeper into the soil is an effective way of increasing the resistance from being pulled out of the ground. By reducing the stresses on the anchor, more materials are available for use. This could be explored further and is a suggested as a continuation of this project. The current anchor material is aluminium, and most aluminium alloys can be used with the redesigned ball joint connection to the anchor wire even when wind forces are large.</p><p> </p>

corrected abstract:
<p>This report is part of a bachelor’s degree project in solid mechanics at KTH, Stockholm. It is performed by two students on behalf of the social enterprise Better Shelter, which provides shelters for people displaced by war and natural disasters. The scope of the project is to expand Better Shelters product specifications by providing improvements of the shelter earth anchors. This would allow use of the shelter in areas affected by higher wind speeds and thereby help more people in need of temporary housing and shelters.</p><p>The earth anchors prevent the shelter from uplifting and tilting by taking uplift forces when horizontal wind loads acts on the structure. Two wind models with wind speeds up to 36 𝑚/𝑠 were created to find the reaction forces on the anchors resisting the wind load. The wind models were compared with each other to validate the results and find the largest reaction forces on the anchors. Simulations of the anchors were made to analyse occurring stresses due to wind loads.</p><p>Redesigns of the current earth anchor were made to find improvements of the anchor shape and reduce the stresses on the anchor. Experiments were then performed to analyse the redesigned anchor shapes in practise. The redesign, calculations and computational analyses of the anchors were done using the programs SolidEdge, ANSYS and Matlab.</p><p>Results showed that redesigning the anchor contact area with the anchor wire reduced the stresses on the anchors drastically. Increasing the wing size of the anchors proved to be successful for preventing anchors from being pulled out of the soil. This allows better use of the current anchor material volume. Experiments also proved that burying the anchor deeper into the soil is an effective way of increasing the resistance from being pulled out of the ground.</p><p>By reducing the stresses on the anchor, more materials are available for use. This could be explored further and is a suggested as a continuation of this project. The current anchor material is aluminium, and most aluminium alloys can be used with the redesigned ball joint connection to the anchor wire even when wind forces are large.</p>
----------------------------------------------------------------------
In diva2:1876745 
abstract is: 
<p>The Thermo-Calc software is a key tool in the research process for many material engineers. However, integrating multiple modules in Thermo-Calc requires the user to write code in a Python-based language, which can be challenging for novice programmers. This project aims to enable the generation of such code from user prompts by using existing generative AI models. In particular, we use a retrieval-augmented generation architecture applied to LLaMA and Mistral models. We use Code LLaMA-Instruct models with 7, 13, and 34 billion parameters, and a Mistral-Instruct model with 7 billion parameters. These models are all based on LLaMA 2. We also use a LLaMA 3-Instruct model with 8 billion parameters. All these models are instruction-tuned, which suggests that they have the capability to interpret natural language and identify appropriate options for a command-line program such as Python. In our testing, the LLaMA 3-Instruct model performed best, achieving 53% on the industry benchmark HumanEval and 49% on our internal adequacy assessment at pass@1, which is the expected probability of getting a correct solution when generating a response. This indicates that the model generates approximately every other answer correct. Due to GPU memory limitations, we had to apply quantisation to process the 13 and 34 billion parameter models. Our results revealed a mismatch between model size and optimal levels of quantisation, indicating that reduced precision adversely affects the performance of these models. Our findings suggest that a properly customised large language model can greatly reduce the coding effort of novice programmers, thereby improving productivity in material research.</p><p> </p>

corrected abstract:
<p>The Thermo-Calc software is a key tool in the research process for many material engineers. However, integrating multiple modules in Thermo-Calc requires the user to write code in a Python-based language, which can be challenging for novice programmers. This project aims to enable the generation of such code from user prompts by using existing generative AI models. In particular, we use a retrieval-augmented generation architecture applied to LLaMA and Mistral models. We use Code LLaMA-Instruct models with 7, 13, and 34 billion parameters, and a Mistral-Instruct model with 7 billion parameters. These models are all based on LLaMA 2. We also use a LLaMA 3-Instruct model with 8 billion parameters. All these models are instruction-tuned, which suggests that they have the capability to interpret natural language and identify appropriate options for a command-line program such as Python. In our testing, the LLaMA 3-Instruct model performed best, achieving 53% on the industry benchmark HumanEval and 49% on our internal adequacy assessment at pass@1, which is the expected probability of getting a correct solution when generating a response. This indicates that the model generates approximately every other answer correct. Due to GPU memory limitations, we had to apply quantisation to process the 13 and 34 billion parameter models. Our results revealed a mismatch between model size and optimal levels of quantisation, indicating that reduced precision adversely affects the performance of these models. Our findings suggest that a properly customised large language model can greatly reduce the coding effort of novice programmers, thereby improving productivity in material research.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1285800 
abstract is: 
<p> As the challenge grows in the vehicle industry, tightening the margins on financial and environmental costs of the vehicle development, computer aided engineering becomes more and more attractive. Extensive work is being invested in creating detailed models that can replicate vehicle behaviour accurately and efficiently. The work in this thesis starts with studying objective and subjective evaluations of vehicles as well as their counterparts in vehicle models and a simulator environment. Then, it continues to locate the weaknesses in the models, and investigate the possible improvements. The first part of the thesis focused on performing a literature study concerning the objective metrics and their use in the vehicle industry, as well as the use of simulators. This served as a foundation for the use of objective metrics in the validation of the CarRealTime models. The tools used in the thesis were also introduced. The work continued with the study of previously collected data concerning vehicle evaluation through subjective assessment and objective metrics, with different anti-roll bar configurations, to build trust in the ability of the drivers in evaluating these criteria. Similar data from the CarRealTime models and the simulator were also studied. The aim was to evaluate the simulator driving experience accuracy through the subjective assessment. The weaknesses of the model were identified, and an improved steering model was introduced, replacing the old lookup tables with a Pfeffer model from CarRealTime combined with the steering assist unit in Simulink. An extensive parameter study was performed to understand the effect of selected parameters on the driving experience. Using the same model, the simulator delays were studied in terms of replicating yaw and lateral movements, and how this can affect the driver’s perception of the driving experience. Finally, the results from the parameter study were used to assign the weight parameters in the optimization objective function where the goal was to study the possibility of improving the accuracy of the driving experience as well as counteracting the effects of simulator delays. The Matlab Optimization Toolkit was used in the process. As a conclusion, it was shown that the subjective assessment together with the objective metrics played a crucial role in identifying model and simulator weaknesses. The parameter study showed promising opportunities in solving the aforementioned issues, with the optimization tool and boundaries needing more elaborate work to reach conclusive results.</p><p> </p>

corrected abstract:
<p> As the challenge grows in the vehicle industry, tightening the margins on financial and environmental costs of the vehicle development, computer aided engineering becomes more and more attractive. Extensive work is being invested in creating detailed models that can replicate vehicle behaviour accurately and efficiently. The work in this thesis starts with studying objective and subjective evaluations of vehicles as well as their counterparts in vehicle models and a simulator environment. Then, it continues to locate the weaknesses in the models, and investigate the possible improvements. The first part of the thesis focused on performing a literature study concerning the objective metrics and their use in the vehicle industry, as well as the use of simulators. This served as a foundation for the use of objective metrics in the validation of the CarRealTime models. The tools used in the thesis were also introduced. The work continued with the study of previously collected data concerning vehicle evaluation through subjective assessment and objective metrics, with different anti-roll bar configurations, to build trust in the ability of the drivers in evaluating these criteria. Similar data from the CarRealTime models and the simulator were also studied. The aim was to evaluate the simulator driving experience accuracy through the subjective assessment. The weaknesses of the model were identified, and an improved steering model was introduced, replacing the old lookup tables with a Pfeffer model from CarRealTime combined with the steering assist unit in Simulink. An extensive parameter study was performed to understand the effect of selected parameters on the driving experience. Using the same model, the simulator delays were studied in terms of replicating yaw and lateral movements, and how this can affect the driver’s perception of the driving experience. Finally, the results from the parameter study were used to assign the weight parameters in the optimization objective function where the goal was to study the possibility of improving the accuracy of the driving experience as well as counteracting the effects of simulator delays. The Matlab Optimization Toolkit was used in the process. As a conclusion, it was shown that the subjective assessment together with the objective metrics played a crucial role in identifying model and simulator weaknesses. The parameter study showed promising opportunities in solving the aforementioned issues, with the optimization tool and boundaries needing more elaborate work to reach conclusive results.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:741548 
abstract is: 
<p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p><p> </p>
skipping mc='isa'

partal corrected: diva2:741548: <p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p><p> </p>

corrected abstract:
<p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1781509 
Note: no full text in DiVA

abstract is: 
<p>The objective of this report is to explore and assess the potential of two modified designs derived from a perfectly circular nested tubes NANF configuration. By altering the curvature profile and adopting a rounded triangular shape, various parameters are systematically varied to evaluate their impact on the confinement loss properties within the operating range around 850 nm. By leveraging geometric optics, an attempt is made to increase the angle of incidence of light on the tubes in order to maximize reflection. Through extensive simulations, the behavior of the modified designs is analyzed and compared against the literature-based NANF-B. The simulation results indicate that the simulated NANF-B exhibits a skewed wavelength range towards lower wavelengths compared to the literature version probably because of slight differences in how the model was designed. However, the proposed alterations demonstrate significantly reduced bandwidth and only show agreement with NANF-B within the lower wavelength range. Furthermore, the simulations reveal that the points where the tubes connect to the outer cladding play a critical role in the overall loss characteristics at longer wavelengths. This finding suggests that designs incorporating inward-curving tubes towards the outer cladding offer improved performance throughout the anti-resonant window where propagation is feasible.</p><p> </p>

corrected abstract:
<p>The objective of this report is to explore and assess the potential of two modified designs derived from a perfectly circular nested tubes NANF configuration. By altering the curvature profile and adopting a rounded triangular shape, various parameters are systematically varied to evaluate their impact on the confinement loss properties within the operating range around 850 nm. By leveraging geometric optics, an attempt is made to increase the angle of incidence of light on the tubes in order to maximize reflection. Through extensive simulations, the behavior of the modified designs is analyzed and compared against the literature-based NANF-B. The simulation results indicate that the simulated NANF-B exhibits a skewed wavelength range towards lower wavelengths compared to the literature version probably because of slight differences in how the model was designed. However, the proposed alterations demonstrate significantly reduced bandwidth and only show agreement with NANF-B within the lower wavelength range. Furthermore, the simulations reveal that the points where the tubes connect to the outer cladding play a critical role in the overall loss characteristics at longer wavelengths. This finding suggests that designs incorporating inward-curving tubes towards the outer cladding offer improved performance throughout the anti-resonant window where propagation is feasible.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1865115 
abstract is: 
<p>This thesis deals with Large Eddy Simulations (LES) of the Atmospheric Boundary Layer (ABL), focusing on studying the resolution dependence of turbulent passive scalar transport within the layer. The ABL is the lowest part of the atmosphere, where humans live and conduct most of their daily activities. Here, a scalar was injected at four different heights in a mixed shear- and convective-driven ABL, which was simulated using the Spectral Element Method (SEM) code Nek5000. The statistics of the four scalars were analysed and their resolution dependence was studied and compared to that of non-scalar quantities. No significant resolution dependence was found with regards to non-scalar quantities, while scalar quantities show a rather strong dependence on resolution especially in the first quarter of the simulation. Negative concentration values are found within the layer and some approaches to solve the problem are proposed. Statistics alone provide an accurate description of the general ABL behaviour, but are found to be insufficient to capture the dynamics of the scalar injection, which ought to be analysed with more advanced methods (e.g. modal decomposition). The structures arising within the layer are also analysed, and further work regarding the study of scalar fronts is suggested.</p><p> </p>

corrected abstract:
<p>This thesis deals with Large Eddy Simulations (LES) of the Atmospheric Boundary Layer (ABL), focusing on studying the resolution dependence of turbulent passive scalar transport within the layer. The ABL is the lowest part of the atmosphere, where humans live and conduct most of their daily activities. Here, a scalar was injected at four different heights in a mixed shear- and convective-driven ABL, which was simulated using the Spectral Element Method (SEM) code Nek5000. The statistics of the four scalars were analysed and their resolution dependence was studied and compared to that of non-scalar quantities. No significant resolution dependence was found with regards to non-scalar quantities, while scalar quantities show a rather strong dependence on resolution especially in the first quarter of the simulation. Negative concentration values are found within the layer and some approaches to solve the problem are proposed. Statistics alone provide an accurate description of the general ABL behaviour, but are found to be insufficient to capture the dynamics of the scalar injection, which ought to be analysed with more advanced methods (e.g. modal decomposition). The structures arising within the layer are also analysed, and further work regarding the study of scalar fronts is suggested.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1197616 
abstract is: 
<p>The use of turbocharged Diesel engines is nowadays a widespread practice in the automotive sector: heavy-duty vehicles like trucks or buses, in particular, are often equipped with turbocharged engines. An accurate study of the flow field developing inside both the main components of a turbocharger, i.e. compressor and turbine, is therefore necessary: the synergistic use of CFD simulations and experimental tests allows to fulfill this requirement.</p><p>The aim of this thesis is to investigate the performance and the flow field that develops inside a centrifugal compressor for automotive turbochargers. The study is carried out by means of numerical simulations, both steady-state and transient, based on RANS models (Reynolds Averaged Navier-Stokes equations). The code utilized for the numerical simulations is Ansys CFX.</p><p> </p><p>The first part of the work is an engineering attempt to develop a CFD method for predicting the performance of a centrifugal compressor which is based solely on steady-state RANS models. The results obtained are then compared with experimental observations. The study continues with an analysis of the sensitivity of the developed CFD method to different parameters: influence of both position and model used for the rotor-stator interfaces and the axial tip-clearance on the global performances is studied and quantified.</p><p> </p><p>In the second part, a design optimization study based on the Design of Experiments (DoE) approach is performed. In detail, transient RANS simulations are used to identify which geometry of the recirculation cavity hollowed inside the compressor shroud (ported shroud design) allows to mitigate the backflow that appears at low mass-flow rates. Backflow can be observed when the operational point of the compressor is suddenly moved from design to surge conditions. On actual heavy-duty vehicles, these conditions may arise when a rapid gear shift is performed.</p>

corrected abstract:
<p>The use of turbocharged Diesel engines is nowadays a widespread practice in the automotive sector: heavy-duty vehicles like trucks or buses, in particular, are often equipped with turbocharged engines. An accurate study of the flow field developing inside both the main components of a turbocharger, i.e. compressor and turbine, is therefore necessary: the synergistic use of CFD simulations and experimental tests allows to fulfill this requirement.</p><p>The aim of this thesis is to investigate the performance and the flow field that develops inside a centrifugal compressor for automotive turbochargers. The study is carried out by means of numerical simulations, both steady-state and transient, based on RANS models (Reynolds Averaged Navier-Stokes equations). The code utilized for the numerical simulations is Ansys CFX.</p><p>The first part of the work is an engineering attempt to develop a CFD method for predicting the performance of a centrifugal compressor which is based solely on steady-state RANS models. The results obtained are then compared with experimental observations. The study continues with an analysis of the sensitivity of the developed CFD method to different parameters: influence of both position and model used for the rotor-stator interfaces and the axial tip-clearance on the global performances is studied and quantified.</p><p>In the second part, a design optimization study based on the Design of Experiments (DoE) approach is performed. In detail, transient RANS simulations are used to identify which geometry of the recirculation cavity hollowed inside the compressor shroud (ported shroud design) allows to mitigate the backflow that appears at low mass-flow rates. Backflow can be observed when the operational point of the compressor is suddenly moved from design to surge conditions. On actual heavy-duty vehicles, these conditions may arise when a rapid gear shift is performed.</p>


Note - only change to remove the empty paragraphs
----------------------------------------------------------------------
In diva2:1341272 
abstract is: 
<p>In the autumn 2018 political elections were held in Sweden and consequently it is interesting to investigate what can affect how people vote. The purpose with this report is investigating if there are correspondences between the characteristics of a municipality and how the people in that municipality voted in the general election. Clustering on data sets with municipality characteristics and municipality general election statistics from 2018 is the basis of this study. K-means clustering and hierarchical clustering are the clustering methods that are used. In the report results of the clustering and the construction of a method for comparing clusterings are presented. The results show that there are some correspondences but that clustering is not the optimal method for analysing this data set.</p><p> </p>

corrected abstract:
<p>In the autumn 2018 political elections were held in Sweden and consequently it is interesting to investigate what can affect how people vote. The purpose with this report is investigating if there are correspondences between the characteristics of a municipality and how the people in that municipality voted in the general election. Clustering on data sets with municipality characteristics and municipality general election statistics from 2018 is the basis of this study. 𝐾-means clustering and hierarchical clustering are the clustering methods that are used. In the report results of the clustering and the construction of a method for comparing clusterings are presented. The results show that there are some correspondences but that clustering is not the optimal method for analysing this data set.</p>

Note - only change to remove the empty paragraph and replace "K" by "𝐾".
----------------------------------------------------------------------
In diva2:1342347 
abstract is: 
<p>The current trend in the automotive industry towards more fuel efficient vehicles requires all components to be as light as possible while still meeting other demands such as stiffness and feasible cost. The purpose of this study was to investigate the possibility to replace a partial chassis structure in a Scania low-entry city bus. The partial chassis to be replaced consists of a steel structure and an inner flooring, with the purpose to support loads that the bus is subject to. This was to be done with a composite sandwich structure, with the primary goal to reduce weight by at least 40% and number of components by 50%. The replacement structure needed to meet the stiffness and strength requirements that the current structure fulfils. This was achieved by designing two concepts, concept 1 and concept 2, through an iterative FE-analysis in ANSYS. Two prototypes where built and tested for real world load applications. The result from this study showed that it was possible to meet both the weight and component reduction goal. Concept 1 and concept 2 achieved a weight reduction of 62% and 68% respectively and the number of components was reduced significantly. Further work would be to investigate the interface between the new structure and the rest of the bus, modal- and fatigue analyses, production implementation and economical aspects to name a few.</p><p> </p>

corrected abstract:
<p>The current trend in the automotive industry towards more fuel efficient vehicles requires all components to be as light as possible while still meeting other demands such as stiffness and feasible cost.</p><p>The purpose of this study was to investigate the possibility to replace a partial chassis structure in a Scania low-entry city bus. The partial chassis to be replaced consists of a steel structure and an inner flooring, with the purpose to support loads that the bus is subject to. This was to be done with a composite sandwich structure, with the primary goal to reduce weight by at least 40% and number of components by 50%. The replacement structure needed to meet the stiffness and strength requirements that the current structure fulfils. This was achieved by designing two concepts, <em>concept 1</em> and <em>concept 2</em>, through an iterative FE-analysis in ANSYS. Two prototypes where built and tested for real world load applications.</p><p>The result from this study showed that it was possible to meet both the weight and component reduction goal. Concept 1 and concept 2 achieved a weight reduction of 62% and 68% respectively and the number of components was reduced significantly. Further work would be to investigate the interface between the new structure and the rest of the bus, modal- and fatigue analyses, production implementation and economical aspects to name a few.</p>

Note - only changes to remove the empty paragraph,  add paragraph breaks, and add italics
----------------------------------------------------------------------
In diva2:1335215 
abstract is: 
<p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for functioning quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si3N4 has been used as a strip waveguide, placed on top of a silicon oxide SiO2 wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p><p> </p>
mc='functionin' c='function in'

partal corrected: diva2:1335215: <p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for function ing quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si3N4 has been used as a strip waveguide, placed on top of a silicon oxide SiO2 wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p><p> </p>

corrected abstract:
<p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for functioning quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si<sub>3</sub>N<sub>4</sub> has been used as a strip waveguide, placed on top of a silicon oxide SiO<sub>2</sub> wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p>

Note - only change to remove the empty paragraph and added subscripts
----------------------------------------------------------------------
In diva2:1334842 
abstract is: 
<p>Mathematics contains many hard problems. In this paper we discuss how some of these hard problems can be solved with techniques from other math fields than the problems own discipline. First we solve some combinatorial problems using the knowledge that a maximum amount of vectors in a linearly independent set over a subset of a vector space F^n over a field F is n. Then we discuss and explain Z.Dvir's famous proof regarding Kakeya sets over finite fields. He is able to establish a lower bound of the size of Kakeya sets using polynomials.</p><p> </p>

corrected abstract:
<p>Mathematics contains many hard problems. In this paper we discuss how some of these hard problems can be solved with techniques from other math fields than the problems own discipline. First we solve some combinatorial problems using the knowledge that a maximum amount of vectors in a linearly independent set over a subset of a vector space 𝔽<sup>𝑛</sup> over a field 𝔽 is 𝑛. Then we discuss and explain Z.Dvir's famous proof regarding Kakeya sets over finite fields. He is able to establish a lower bound of the size of Kakeya sets using polynomials.</p>

Note spelling error in original:
mc='Z.Dvir' c='Z. Dvir'
----------------------------------------------------------------------
In diva2:704889 
abstract is: 
<p>In this thesis a corporate bond valuation model based on Dick-Nielsen, Feldhütter, and Lando (2011) and Chen, Lesmond, and Wei (2007) is examined. The aim is for the model to price corporate bond spreads and in particular capture the price effects of liquidity as well as credit risk. The valuation model is based on linear regression and is conducted on the Swedish market with data provided by Handelsbanken. Two measures of liquidity are analyzed: the bid-ask spread and the zero-trading days. The investigation shows that the bid-ask spread outperforms the zero-trading days in both significance and robustness. The valuation model with the bid-ask spread explains 59% of the cross-sectional variation and has a standard error of 56 bps in its pricing predictions of corporate spreads. A reduced version of the valuation model is also developed to address simplicity and target a larger group of users. The reduced model is shown to maintain a large proportion of the explanation power while including fewer and simpler variables.</p><p> </p>

corrected abstract:
<p>In this thesis a corporate bond valuation model based on Dick-Nielsen, Feldhütter, and Lando (2011) and Chen, Lesmond, and Wei (2007) is examined. The aim is for the model to price corporate bond spreads and in particular capture the price effects of liquidity as well as credit risk. The valuation model is based on linear regression and is conducted on the Swedish market with data provided by <em lang="sv">Handelsbanken</em>. Two measures of liquidity are analyzed: the bid-ask spread and the zero-trading days. The investigation shows that the bid-ask spread outperforms the zero-trading days in both significance and robustness. The valuation model with the bid-ask spread explains 59% of the cross-sectional variation and has a standard error of 56 bps in its pricing predictions of corporate spreads. A reduced version of the valuation model is also developed to address simplicity and target a larger group of users. The reduced model is shown to maintain a large proportion of the explanation power while including fewer and simpler variables.</p>

Note - only change to remove the empty paragraph and added italics
----------------------------------------------------------------------
In diva2:1442642 
abstract is: 
<p>In recent decades, sound reproduction research has shown great progress. Audio reproduction changed from a simple two-channel stereo system to a surround sound system and three dimensional sound system. The use of height and angle related speakers was introduced with the aim of improving sound reproduction. With the listener’s experience in focus, this developed reproductive systems were examined. This bachelor’s thesis in sound and vibration compiles three articles on sound quality with regard to different sound reproduction media, with the aim of getting a better understanding of the various psychoacoustical and technical parameters that influence the sound experience. Various subjective evaluation methods are presented, including the PREQUEL method (PerceptualReproduction Quality Evaluation for Loudspeakers), MUSHRA method (MUlti Stimulus test withHidden Reference and Anchor) and OLE method (Overall Listening Experience). The results show how a surround sound system and a three-dimensional sound system provide a more appreciated sound experience and how this experience can be enhanced with a height-related sound system .</p><p> </p>
mc='withHidden' c='with Hidden'
mc='PerceptualReproduction' c='Perceptual Reproduction'

corrected abstract:
<p>In recent decades, sound reproduction research has shown great progress. Audio reproduction changed from a simple two-channel stereo system to a surround sound system and three dimensional sound system. The use of height and angle related speakers was introduced with the aim of improving sound reproduction. With the listener’s experience in focus, this developed reproductive systems were examined.</p><p>This bachelor’s thesis in sound and vibration compiles three articles on sound quality with regard to different sound reproduction media, with the aim of getting a better understanding of the various psychoacoustical and technical parameters that influence the sound experience. Various subjective evaluation methods are presented, including the PREQUEL method (perceptual reproduction quality evaluation for loudspeakers), MUSHRA method (MUlti Stimulus test with Hidden Reference and Anchor) and OLE method (overall listening experience)). The results show how a surround sound system and a three-dimensional sound system provide a more appreciated sound experience and how this experience can be enhanced with a height-related sound system .</p>

Note that there is a space before the final period, several of the spelledout versions of acronyms are in lower case. Also removed the empty paragraph.
----------------------------------------------------------------------
In diva2:415054 
abstract is: 
<p> </p>
<p>Abstract</p>
<p>Thermally-controlled exchange coupling between two strong ferromagnetic (FM) layers separated by a weak FM spacer has promising application in current-driven spintronic devices. In this thesis magnetic property of Ni xFe1-x thin films in the Invar composition range are studied. The focus is on investigating the magnetic transition from ferromagnetic to paramagnetic state of the material and finding the composition with lowest Curie point.</p>
<p>The fabrication process of NixFe1-x  thin films having variable Ni concentration using multi magnetron sputtering is described in the first part of the thesis. In the second part of the thesis the magnetic characterization technique and measurement results are discussed. The Invar effect is observed at approximately 30% Ni content, where the films exhibit a pronounced minimum in the Curie temperature versus Ni concentration</p>

corrected abstract:
<p>Thermally-controlled exchange coupling between two strong ferromagnetic (FM) layers separated by a weak FM spacer has promising application in current-driven spintronic devices. In this thesis magnetic property of Ni<sub>x</sub>Fe<sub>1-x</sub> thin films in the Invar composition range are studied. The focus is on investigating the magnetic transition from ferromagnetic to paramagnetic state of the material and finding the composition with lowest Curie point. The fabrication process of Ni<sub>x</sub>Fe<sub>1-x</sub> thin films having variable Ni concentration using multi magnetron sputtering is described in the first part of the thesis. In the second part of the thesis the magnetic characterization technique and measurement results are discussed. The Invar effect is observed at approximately 30% Ni content, where the films exhibit a pronounced minimum in the Curie temperature versus Ni concentration.</p>
----------------------------------------------------------------------
In diva2:1335210 
abstract is: 
<p>G protein-coupled receptors are one of the biggest targets for pharmaceutical drugs today. The aim with this project was to use different machine learning algorithms to classify the protein into different functional states and compare the results obtained by different algorithms. Both the supervised and unsupervised methods implemented in this project identified similar regions of the protein as important for classification of their functional state. More specifically, the supervised methods Random Forest and Multilayer Perceptron were able to make predictions of the functional state of a protein with great accuracy. The methods investigated will be useful for designing and analyzing the molecular dynamics simulations of GPCRs. Ultimately this will further our understanding of the drug binding mechanics, a critical step for the rational development of new drugs to treat various diseases.</p><p> </p>

corrected abstract:
<p>G protein-coupled receptors are one of the biggest targets for pharmaceutical drugs today. The aim with this project was to use different machine learning algorithms to classify the protein into different functional states and compare the results obtained by different algorithms. Both the supervised and unsupervised methods implemented in this project identified similar regions of the protein as important for classification of their functional state. More specifically, the supervised methods Random Forest and Multilayer Perceptron were able to make predictions of the functional state of a protein with great accuracy. The methods investigated will be useful for designing and analyzing the molecular dynamics simulations of GPCRs. Ultimately this will further our understanding of the drug binding mechanics, a critical step for the rational development of new drugs to treat various diseases.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1879658 
abstract is: 
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>

corrected abstract:
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>
In diva2:1879658 
abstract is: 
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>

corrected abstract:
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1363231 
abstract is: 
<p>In the truck industry, it is crustal to test components against fatigue to make sure that the trucks stand up to the high demands on durability. Today’s testing methods have some disadvantages; it is quite a time-consuming process, but more important, similar tested components cannot easily be compared due to the load spread the components are subjected to. It is therefore desirable to test the components in a standardized way. One way to do this is to use a synthetic signal which is a large number of unique truck measurements combined. The synthetic signal only contains information of the frame’s vibration and not any components. The purpose of this project was to create a model that uses the synthetic signal to describe the motion of components.</p><p> </p><p>Two approaches were used, the first was to base the model on previous measurements, the second one was to base the model on analytical equations. These models were experimentally tested in a 4 channel shake rig, and a silencer was the component chosen to be tested. For the model based on measurements, the load was shown to have a large spread which was hard to control due to the spread in the measurements. The second model was easier to control where the damping factor can be chosen and varied. A promising model was the analytical model using 10% damping applied to the synthetic signal, it covers most measurements without overestimate the load of the component. However, the model was only developed for the silencer acceleration in the z-direction, and it is recommended to develop it for the x-direction as well. The method used in this project could also be used to develop models for other components.</p>

corrected abstract:
<p>In the truck industry, it is crustal to test components against fatigue to make sure that the trucks stand up to the high demands on durability. Today’s testing methods have some disadvantages; it is quite a time-consuming process, but more important, similar tested components cannot easily be compared due to the load spread the components are subjected to. It is therefore desirable to test the components in a standardized way. One way to do this is to use a synthetic signal which is a large number of unique truck measurements combined. The synthetic signal only contains information of the frame’s vibration and not any components. The purpose of this project was to create a model that uses the synthetic signal to describe the motion of components.</p><p>Two approaches were used, the first was to base the model on previous measurements, the second one was to base the model on analytical equations. These models were experimentally tested in a 4 channel shake rig, and a silencer was the component chosen to be tested. For the model based on measurements, the load was shown to have a large spread which was hard to control due to the spread in the measurements. The second model was easier to control where the damping factor can be chosen and varied. A promising model was the analytical model using 10% damping applied to the synthetic signal, it covers most measurements without overestimate the load of the component. However, the model was only developed for the silencer acceleration in the z-direction, and it is recommended to develop it for the x-direction as well. The method used in this project could also be used to develop models for other components.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1137873 
abstract is: 
<p>This report was carried out at Gleechi, a Swedish start-up company working with implementing hand use in Virtual Reality. The thesis presents hand models used to generate natural looking grasping motions. One model were made for each of the thirty-three different grasp types in Feix’s The GRASP Taxonomy.</p><p>Each model is based on functional principal components analysis which was performed on data containing recorded joint angles of grasping motions from real subjects. Prior to functional principal components analysis, dynamic time warping was performed on the recorded joint angles in order to put them on the same length and make them suitable for statistical analysis. The last step of the analysis was to project the data onto the functional principal components and train Gaussian mixture models on the weights obtained. New grasping motions could be generated by sampling weights from the Gaussian mixture models and attaching them to the functional principal components.</p><p>The generated grasps were in general satisfying, but all of the thirty-three grasps were not distinguishable from each other. This was most likely caused by the fact that each degree of freedom was modelled in isolation from each other, so that no correlation between them was included in the model.</p><p> </p>

corrected abstract:
<p>This report was carried out at Gleechi, a Swedish start-up company working with implementing hand use in Virtual Reality. The thesis presents hand models used to generate natural looking grasping motions. One model were made for each of the thirty-three different grasp types in Feix’s <em>The GRASP Taxonomy</em>.</p><p>Each model is based on functional principal components analysis which was performed on data containing recorded joint angles of grasping motions from real subjects. Prior to functional principal components analysis, dynamic time warping was performed on the recorded joint angles in order to put them on the same length and make them suitable for statistical analysis. The last step of the analysis was to project the data onto the functional principal components and train Gaussian mixture models on the weights obtained. New grasping motions could be generated by sampling weights from the Gaussian mixture models and attaching them to the functional principal components.</p><p>The generated grasps were in general satisfying, but all of the thirty-three grasps were not distinguishable from each other. This was most likely caused by the fact that each degree of freedom was modelled in isolation from each other, so that no correlation between them was included in the model.</p>

Note - only change to remove the empty paragraph and adding italics
----------------------------------------------------------------------
In diva2:738788 - - unnessary period at end of title:
"Modellering av turboladdarens surge-beteende för fordonsindustrin."
==>
"Modellering av turboladdarens surge-beteende för fordonsindustrin"

abstract is: 
<p>The turbocharger was originally designed and used to boost engine power of vehicles. Nowadays, when the demand for low carbon vehicles is increasing rapidly, a new application for the turbocharger has been found by using it to downsizing the engine. Using experimental as well as theoretical simulation models we can estimate the outcome and behavior of the compressor in an extended work range. An aspect that has a substantial effect on the turbocharger and engine is the surge line. Surge is a problematic stage where the compressor creates unwanted behavior which could damage the turbocharger as well as the engine. The surge line is a line where the transition to surge occurs. By changing the surge line, through simulations and calculations surge can be avoided, you can optimize and improve the turbocharger. This report mainly discusses and investigates the possibility to use fast one-dimensional simulation software instead of full scaled laboratories in the automotive industry, and estimate the work range of the given compressor.</p><p> </p>

corrected abstract:
<p>The turbocharger was originally designed and used to boost engine power of vehicles. Nowadays, when the demand for low carbon vehicles is increasing rapidly, a new application for the turbocharger has been found by using it to downsizing the engine. Using experimental as well as theoretical simulation models we can estimate the outcome and behavior of the compressor in an extended work range. An aspect that has a substantial effect on the turbocharger and engine is the surge line. Surge is a problematic stage where the compressor creates unwanted behavior which could damage the turbocharger as well as the engine. The surge line is a line where the transition to surge occurs. By changing the surge line, through simulations and calculations surge can be avoided, you can optimize and improve the turbocharger. This report mainly discusses and investigates the possibility to use fast one-dimensional simulation software instead of full scaled laboratories in the automotive industry, and estimate the work range of the given compressor.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:716518 
abstract is: 
<p>Cancer is a common cause of death worldwide and radiotherapy is one of the treatments used. Since treatment planning is a time consuming matter for the radiation therapist, a way to decrease the time spent finding the plan would be an improvement. This can be achieved by precalculating a number of optimal plans and then choosing among these in real-time.</p><p>In this thesis a dual algorithm for approximation of the Pareto optimal plans suggested by Bokrantz and Forsgren, was adapted to the parameters of the Leksell Gamma Knife®. A Graphical User Interface was also created, based on the navigation tool described by Monz et al to enable choosing among the pre-calculated dose plans.</p><p>The computational time of the algorithm was investigated and the dimensionality of the solutions and Pareto optimal points were looked at to see if it might be possible to reduce the number of dimensions to speed up computations.</p><p>Although no certain conclusions can be drawn about dimensionality reduction, I found no reason to rule that possibility out. It was also confirmed that there is reason to keep the number of objectives low to get a better approximation.</p><p> </p>

corrected abstract:
<p>Cancer is a common cause of death worldwide and radiotherapy is one of the treatments used. Since treatment planning is a time consuming matter for the radiation therapist, a way to decrease the time spent finding the plan would be an improvement. This can be achieved by precalculating a number of optimal plans and then choosing among these in real-time.</p><p>In this thesis a dual algorithm for approximation of the Pareto optimal plans suggested by Bokrantz and Forsgren, was adapted to the parameters of the Leksell Gamma Knife®. A Graphical User Interface was also created, based on the navigation tool described by Monz et al to enable choosing among the pre-calculated dose plans.</p><p>The computational time of the algorithm was investigated and the dimensionality of the solutions and Pareto optimal points were looked at to see if it might be possible to reduce the number of dimensions to speed up computations.</p><p>Although no certain conclusions can be drawn about dimensionality reduction, I found no reason to rule that possibility out. It was also confirmed that there is reason to keep the number of objectives low to get a better approximation.</p>

Note - only change to remove the empty paragraph
Note spelling error:
"et al" should be "et al."
----------------------------------------------------------------------
In diva2:1848437 
abstract is: 
<p>Turbulent mixing of single or multi-phase flows is common in diverse research fields, and direct numerical simulation is useful for understanding such phenomenon. To study the scaler transport in turbulence, the computational grids must resolve the Batchelor scale, which is smaller than the Kolmogorov scale by a factor of √Sc. This would commonly lead to the over-resolving of the Navier-Stokes equation, making DNS even more expensive. To overcome this issue, this thesis presents a method to reduce the computational cost in scalar turbulent flows, by using a coarse grid for the velocity and a fine grid for the scalar. A divergence-free Hermite interpolation is implemented for the velocity field to ensure the continuity equation is fulfilled on the fine grid. The interpolation scheme is validated by cases of Arnold–Beltrami–Childress flow and Taylor–Green vortex. For the active scaler, the integration schemes are included for fine-to-coarse integration. The multi-resolution method is parallelised by MPI and integrated into the open-source multiphase flow solver FluTAS. For the diffuse interface modelling in FluTAS, the indicator function is updated on the fine grid, and the surface tension force is then calculated and extrapolated back to the coarse grid for momentum equation update. The method is evaluated against the single-resolution method at different refinement factors.</p><p> </p>

corrected abstract:
<p>Turbulent mixing in multi-phase flows is prevalent across various research domains, and direct numerical simulation (DNS) is often used for understanding such phenomena. However, DNS of turbulent scalar transport entails resolving the Batchelor scale, which is typically smaller than the Kolmogorov scale by a factor of 𝑆𝑐<sup>−1/2</sup>. This can lead to increased number of grid points and the over-resolve of NaiverStokes equations, making the DNS even more expensive. To overcome this issue, this thesis presents a method to reduce the computational cost, by using a coarse grid for solving the velocity and pressure, and a fine grid for solving the scalar. A divergencefree interpolation scheme is implemented for velocity to ensure the continuity equation is fulfilled on the fine grid. The interpolation scheme is validated by Arnold–Beltrami– Childress flow and Taylor–Green vortex. For active scalars, a scheme of weighted averaging facilitates the fine-to-coarse integration. The multi-resolution method is parallelised by MPI and integrated into the open-source code FluTAS for multiphase flow simulation. For the diffuse-interface modelling in FluTAS, the phase-field variable and surface tension force are computed on the fine grid and integrated to the coarse grid to couple with the momentum equation. The presented method is evaluated against the single-resolution method using the rising bubble benchmark.</p>

Note - major differences between DiVA and original abstract and removed empty paragraph
----------------------------------------------------------------------
In diva2:1776757 
abstract is: 
<p>This paper aims at presenting the necessary tools to prove that a scheme of finite type over Z exhibits the same singularities as those which occur on a Grassmann variety. First, basic theory regarding the combinatorial objects matroids is presented. Some important examples for the remainder of the paper are given, which also serve to aid the reader in intuition and understanding of matroids. Basic algebraic geometry is presented, and the building blocks affine varieties, projective varieties and general varieties are introduced. These object are generalised in the following subsection as affine schemes and schemes, which are the central object of study in modern algebraic geometry. Important results from the theory of algebraic groups are shown in order to better understand the formulation and proof of the Gelfand–MacPherson theorem, which in turn is utilised, together with Mnëv’s universality theorem, to prove the main result of the paper.</p><p> </p>

corrected abstract:
<p>This paper aims at presenting the necessary tools to prove that a scheme of finite type over ℤ exhibits the same singularities as those which occur on a Grassmann variety. First, basic theory regarding the combinatorial objects matroids is presented. Some important examples for the remainder of the paper are given, which also serve to aid the reader in intuition and understanding of matroids. Basic algebraic geometry is presented, and the building blocks affine varieties, projective varieties and general varieties are introduced. These object are generalised in the following subsection as affine schemes and schemes, which are the central object of study in modern algebraic geometry. Important results from the theory of algebraic groups are shown in order to better understand the formulation and proof of the Gelfand–MacPherson theorem, which in turn is utilised, together with Mnëv’s universality theorem, to prove the main result of the paper.</p>

Note - only change to remove the empty paragraph and replacement of "Z" by "ℤ"
----------------------------------------------------------------------
In diva2:1436960 
abstract is: 
<p>The Aviation industry is important to the European economy and development, therefore a study of the sensitivity of the European flight network is interesting. If clusters exist within the network, that could indicate possible vulnerabilities or bottlenecks, since that would represent a group of airports poorly connected to other parts of the network. In this paper a cluster analysis using spectral clustering is performed with flight data from 34 different European countries. The report also looks at how to implement the spectral clustering algorithm for large data sets. After performing the spectral clustering it appears as if the European flight network is not clustered, and thus does not appear to be sensitive.</p><p> </p>

corrected abstract:
<p>The Aviation industry is important to the European economy and development, therefore a study of the sensitivity of the European flight network is interesting. If clusters exist within the network, that could indicate possible vulnerabilities or bottlenecks, since that would represent a group of airports poorly connected to other parts of the network. In this paper a cluster analysis using spectral clustering is performed with flight data from 34 different European countries. The report also looks at how to implement the spectral clustering algorithm for large data sets. After performing the spectral clustering it appears as if the European flight network is not clustered, and thus does not appear to be sensitive.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:892288 
abstract is: 
<p>Particle suspensions occur in many situations in nature and industry. In this master’s thesis, the motion of a single rigid spheroidal particle immersed in Stokes flow is studied numerically using a boundary integral method and a new specialized quadrature method known as quadrature by expansion (QBX). This method allows the spheroid to be massless or inertial, and placed in any kind of underlying Stokesian flow.</p><p> </p><p>A parameter study of the QBX method is presented, together with validation cases for spheroids in linear shear flow and quadratic flow. The QBX method is able to compute the force and torque on the spheroid as well as the resulting rigid body motion with small errors in a short time, typically less than one second per time step on a regular desktop computer. Novel results are presented for the motion of an inertial spheroid in quadratic flow, where in contrast to linear shear flow the shear rate is not constant. It is found that particle inertia induces a translational drift towards regions in the fluid with higher shear rate.</p>

corrected abstract:
<p>Particle suspensions occur in many situations in nature and industry. In this master’s thesis, the motion of a single rigid spheroidal particle immersed in Stokes flow is studied numerically using a boundary integral method and a new specialized quadrature method known as quadrature by expansion (QBX). This method allows the spheroid to be massless or inertial, and placed in any kind of underlying Stokesian flow.</p><p>A parameter study of the QBX method is presented, together with validation cases for spheroids in linear shear flow and quadratic flow. The QBX method is able to compute the force and torque on the spheroid as well as the resulting rigid body motion with small errors in a short time, typically less than one second per time step on a regular desktop computer. Novel results are presented for the motion of an inertial spheroid in quadratic flow, where in contrast to linear shear flow the shear rate is not constant. It is found that particle inertia induces a translational drift towards regions in the fluid with higher shear rate.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1341297 
Note: no full text in DiVA

abstract is: 
<p>Stochastic gradient descent is an algorithm used in optimization. Learning rate plays a central role in the stochastic gradient descent algorithm. If the selected learning rate lies within the appropriate interval, it affects the algorithm's convergence rate towards local minimum as well as its accuracy. In this work the stochastic gradient descent algorithm was used to treat two simple optimization problems. Firstly, a series of numerical experiments for a plethora of learning rate were performed where the behavior of the algorithm was studied. Secondly, the training of a neural network using the stochastic gradient descent was experimentally studied. The effect of learning rate values was tested as well as the neural network’s performance by varying parameters such as the number of nodes, the activation function and combinations of the above.</p><p> </p>

corrected abstract:
<p>Stochastic gradient descent is an algorithm used in optimization. Learning rate plays a central role in the stochastic gradient descent algorithm. If the selected learning rate lies within the appropriate interval, it affects the algorithm's convergence rate towards local minimum as well as its accuracy. In this work the stochastic gradient descent algorithm was used to treat two simple optimization problems. Firstly, a series of numerical experiments for a plethora of learning rate were performed where the behavior of the algorithm was studied. Secondly, the training of a neural network using the stochastic gradient descent was experimentally studied. The effect of learning rate values was tested as well as the neural network’s performance by varying parameters such as the number of nodes, the activation function and combinations of the above.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1877780 
abstract is: 
<p>This bachelor thesis aims to give an introduction to various Hopf algebras that arise in combinatorics, with a view towards symmetric functions. We begin by covering the algebraic background needed to define Hopf algebras, including a discussion of the algebra-coalgebra duality. Takeuchi's formula for the antipode is stated and proved. It is then generalised to incidence Hopf algebras. This is followed by a discussion of the Hopf algebra of symmetric functions. It is shown that the Hopf algebra of symmetric functions is self-dual. We also show that the graded dual of the Hopf algebra of quasisymmetric functions is the Hopf algebra of non-commutative symmetric functions. Relations to the Hopf algebra of symmetric functions in non-commuting variables are emphasised. Finally, we state and prove the Aguiar-Bergeron-Sottile universality theorem.</p><p> </p>

corrected abstract:
<p>This bachelor thesis aims to give an introduction to various Hopf algebras that arise in combinatorics, with a view towards symmetric functions. We begin by covering the algebraic background needed to define Hopf algebras, including a discussion of the algebra-coalgebra duality. Takeuchi's formula for the antipode is stated and proved. It is then generalised to incidence Hopf algebras. This is followed by a discussion of the Hopf algebra of symmetric functions. It is shown that the Hopf algebra of symmetric functions is self-dual. We also show that the graded dual of the Hopf algebra of quasisymmetric functions is the Hopf algebra of non-commutative symmetric functions. Relations to the Hopf algebra of symmetric functions in non-commuting variables are emphasised. Finally, we state and prove the Aguiar-Bergeron-Sottile universality theorem.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:740516 - unnessary period at end of title:
"On the 1932 Discovery of the Positron."
==>
"On the 1932 Discovery of the Positron."

abstract is: 
<p>An experiment on Cosmic rays performed by Carl D Anderson led to the accidental discovery of the positron in 1932. The discovery was a turning point in particle physics which led to numerous other theories and has been discussed by scientists all over the world. Anderson had photographed a 63 MeV, upward moving electron. The possible origin of such a positron has never before been discussed and is what this report will aim to explain. The report will include some evidence that the particle is in fact a positron as well as a discussion of the four main theories whose possibility and probability will be discussed; pion decay, muon decay, magnetic field bending and pair production. The report will also cover a historical background for Anderson’s experiment, as well as a theoretical background needed for the theories of the origin. The probability of discovering a positron with any of the theorized origins is extremely low and for some theories, even impossible.</p><p> </p>

corrected abstract:
<p>An experiment on Cosmic rays performed by Carl D Anderson led to the accidental discovery of the positron in 1932. The discovery was a turning point in particle physics which led to numerous other theories and has been discussed by scientists all over the world. Anderson had photographed a 63 MeV, upward moving electron. The possible origin of such a positron has never before been discussed and is what this report will aim to explain. The report will include some evidence that the particle is in fact a positron as well as a discussion of the four main theories whose possibility and probability will be discussed; pion decay, muon decay, magnetic field bending and pair production. The report will also cover a historical background for Anderson’s experiment, as well as a theoretical background needed for the theories of the origin. The probability of discovering a positron with any of the theorized origins is extremely low and for some theories, even impossible.</p>

Note - only change to remove the empty paragraph
Note spelling error: there should be a period after "D." as it is an initial - this is correct in the Swedish abstract
----------------------------------------------------------------------
In diva2:1779459 
abstract is: 
<p>The focus of this thesis is to find, visualize and analyze the optimal flow of autonomous electric vehicles with charge constraints in urban traffic with respect to energy consumption. The traffic has been formulated as a static multi-commodity network flow problem, for which two different models have been implemented to handle the charge constraints. The first model uses a recursive algorithm to find the optimal solution fulfilling the charge constraints, while the second model discretizes the commodities’ battery to predetermined battery levels. An implementation of both methods is provided through simulations on scenarios of three different sizes. The results show that both methods are capable of representing the traffic flow with charge constraints, with limitations given by the size of the problem. In particular, the recursive model has the advantage of considering the charge as a continuous quantity. On the other hand the discretization of battery levels allows to handle charge constraint setups with higher complexity, that is when longer detours are needed to fulfill the charge constraints.</p><p> </p>

corrected abstract:
<p>The focus of this thesis is to find, visualize and analyze the optimal flow of autonomous electric vehicles with charge constraints in urban traffic with respect to energy consumption. The traffic has been formulated as a static multi-commodity network flow problem, for which two different models have been implemented to handle the charge constraints. The first model uses a recursive algorithm to find the optimal solution fulfilling the charge constraints, while the second model discretizes the commodities’ battery to predetermined battery levels. An implementation of both methods is provided through simulations on scenarios of three different sizes. The results show that both methods are capable of representing the traffic flow with charge constraints, with limitations given by the size of the problem. In particular, the recursive model has the advantage of considering the charge as a continuous quantity. On the other hand the discretization of battery levels allows to handle charge constraint setups with higher complexity, that is when longer detours are needed to fulfill the charge constraints.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1352113 
abstract is: 
<p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p> </p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome beforehand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>
mc='forehand' c='fore hand'

partal corrected: diva2:1352113: <p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p> </p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome before hand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>

corrected abstract:
<p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome beforehand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
