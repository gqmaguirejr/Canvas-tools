This file contains corrected abstracts from DiVA for SCI added on 2024-12-05
----------------------------------------------------------------------
In diva2:1888871   - correct as is
----------------------------------------------------------------------
In diva2:1881342 
abstract is: 
<p>This thesis investigates the impedance of acoustic liners, to attenuate noise originating from jet engines and enable compliance with international standards and regulations regarding noise from airplane jet engines. Experimental tests of two supplied liners were conducted in an impedance tube; one liner with known and predictable properties, and one liner with unknown properties.</p><p>The tests included tonal excitations in the formats of stepped sine and random noise with frequencies within set boundaries. After post-processing of the captured data, the desired impedance could be analysed in terms of excitated frequencies and sound pressure levels.</p><p>The conclusions from this project are that both of the liners deviated from their expected behavior, which was that liner 1 should have been unaffected by the alternated sound pressure levels, and liner 3 should have shown bigger affection due to the changed sound pressure level. Since the results were different than expected, there might have been minor sources of error during the measurements. It could be investigated if there is leakage from the mounting of the liners, or if the 3D printing resolution is sufficient.</p><p>Because of limitations in time, there is more left in this project to investigate. Therefore, conducting similar studies where more frequencies, sound pressure levels, and multi-tonal measurements can be included, is suggested as future work.</p><p> </p>

corrected abstract:
<p>This thesis investigates the impedance of acoustic liners, to attenuate noise originating from jet engines and enable compliance with international standards and regulations regarding noise from airplane jet engines. Experimental tests of two supplied liners were conducted in an impedance tube; one liner with known and predictable properties, and one liner with unknown properties.</p><p>The tests included tonal excitations in the formats of stepped sine and random noise with frequencies within set boundaries. After post-processing of the captured data, the desired impedance could be analysed in terms of excitated frequencies and sound pressure levels.</p><p>The conclusions from this project are that both of the liners deviated from their expected behavior, which was that liner 1 should have been unaffected by the alternated sound pressure levels, and liner 3 should have shown bigger affection due to the changed sound pressure level. Since the results were different than expected, there might have been minor sources of error during the measurements. It could be investigated if there is leakage from the mounting of the liners, or if the 3D printing resolution is sufficient.</p><p>Because of limitations in time, there is more left in this project to investigate. Therefore, conducting similar studies where more frequencies, sound pressure levels, and multi-tonal measurements can be included, is suggested as future work.</p>

Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1881022 
abstract is: 
<p>This study compares transportation preferences among students and staff from two different universities: KTH in Stockholm and ITBA in Buenos Aires. The aim was to identify the factors influencing transportation decisions and their impact on perceptions regarding its usage. A survey was conducted at both universities to gain insights from participants and analyze the collected information, aiming to propose solutions that enhance the quality and sustainability of transportation. This work presents a novel analysis by comparing two cities with seemingly different characteristics in terms of transportation and urban development, with an emphasis on sustainability. The results obtained are positive as potential improvements in both systems could be observed. Moreover, the results have generated ideas to improve the adoption of green and sustainable transportation, without compromising the quality of existing transportation for members of these educational institutions. The importance of environmental awareness and its positive impact on quality of life and urban mobility is highlighted.</p><p> </p>

corrected abstract:
<p>This study compares transportation preferences among students and staff from two different universities: KTH in Stockholm and ITBA in Buenos Aires. The aim was to identify the factors influencing transportation decisions and their impact on perceptions regarding its usage. A survey was conducted at both universities to gain insights from participants and analyze the collected information, aiming to propose solutions that enhance the quality and sustainability of transportation. This work presents a novel analysis by comparing two cities with seemingly different characteristics in terms of transportation and urban development, with an emphasis on sustainability. The results obtained are positive as potential improvements in both systems could be observed. Moreover, the results have generated ideas to improve the adoption of green and sustainable transportation, without compromising the quality of existing transportation for members of these educational institutions. The importance of environmental awareness and its positive impact on quality of life and urban mobility is highlighted.</p>

Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1880964 
abstract is: 
<p>Urban Air mobility (UAM) promises reduced congestion on roads, reduced travel times and stronger overall efficiency in densely populated areas. However several challenges arise when wanting to implement UAM such as safety and social acceptance. The aim of this paper is to gain valuable insights how to implement safe and socially accepted UAM into society. Current regulations are discussed as well as X, Y and Z volumes in U-space, flight separations with ellipsoidal safety buffers, high speed corridors, landing separation at vertiports and airspace partition with voronoi diagrams are proposed and discussed. Social acceptance is addressed with some of the most prominent concerns e.g. safety, privacy and noise. Examples are set in Stockholm, Sweden, resulting in a maximum airspace occupation of 1 % which means 210 UAS (Unmanned Aircraft Systems) on each flight level. Sensitive areas and people with privacy concerns should have the option to opt-out of having their properties under the flight paths of UAM-vehicles. Concerns with UAM from the public has to be taken into great consideration for a successful implementation of UAM.</p>

corrected abstract:
<p>Urban Air mobility (UAM) promises reduced congestion on roads, reduced travel times and stronger overall efficiency in densely populated areas. However several challenges arise when wanting to implement UAM such as safety and social acceptance. The aim of this paper is to gain valuable insights how to implement safe and socially accepted UAM into society. Current regulations are discussed as well as X, Y and Z volumes in U-space, flight separations with ellipsoidal safety buffers, high speed corridors, landing separation at vertiports and airspace partition with voronoi diagrams are proposed and discussed. Social acceptance is addressed with some of the most prominent concerns e.g. safety, privacy and noise. Examples are set in Stockholm, Sweden resulting in a maximum airspace occupation of 1 % which means 210 UAS (Unmanned Aircraft Systems) on each flight level. Sensitive areas and people with privacy concerns should have the option to opt-out of having their properties under the flight paths of UAM-vehicles. Concerns with UAM from the public has to be taken into great consideration for a successful implementation of UAM.</p>

Note - only change was to remove the comma after "Sweden" as it is not in the original, although grammatically it might be correct. It might also be noted that commas should before and after the "e.g." - but are not in the original.
----------------------------------------------------------------------
In diva2:1880436   - correct as is
----------------------------------------------------------------------
In diva2:1880384   - correct as is
----------------------------------------------------------------------
In diva2:1880323 
abstract is: 
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p><p> </p>

corrected abstract:
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p>


Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:877595 
abstract is: 
<p> </p><p>A recent trend in the world is that more and more countries and therefore their mil-itaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to ad-opt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense com-pany, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an e˙ective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare di˙erent solutions. The delta model includes reference values for relevant costs that can be a˙ected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>
w='di˙erent' val={'c': 'different', 's': ['diva2:877595', 'diva2:1380198', 'diva2:891912', 'diva2:919302', 'diva2:1328904', 'diva2:891537', 'diva2:1440147', 'diva2:1087823'], 'n': 'missing ligature'}
w='ad-opt' val={'c': 'adopt', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='com-pany' val={'c': 'company', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='mil-itaries' val={'c': 'militaries', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='a˙ected' val={'c': 'affected', 's': 'diva2:877595', 'n': 'missing ligature'}
w='e˙ective' val={'c': 'effective', 's': 'diva2:877595', 'n': 'error due to ligature'}

corrected abstract:
<p>A recent trend in the world is that more and more countries and therefore their militaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to adopt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense company, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an effective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare different solutions. The delta model includes reference values for relevant costs that can be affected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>

Note - also remove the empty paragraph at the start.
----------------------------------------------------------------------
In diva2:711193 
abstract is: 
<p>Consensus problem with multi-agent systems has interested researchers in various areas. Its difficulties tend to appear when available information of each agent is limited for achieving consensus. Besides, it is not always the case that agents can catch the whole states of the others; an output is often the only possible measurement for each agent in applications. The idea of graph Laplacian is then of help to address such a troublesome situation. While every single agent obviously makes decision to achieve an individual goal of minimizing its own cost functional, all agents as a team can obtain even more improvement by cooperation in some cases, which leads to cooperative game theoretic approach. The main goal of this master thesis is to accomplish a combination of optimal control theory and cooperative game theory in order to solve the output consensus problem with limited network connectivity. Along with this combination, bargaining problems are considered out of necessity.</p><p> </p>

corrected abstract:
<p>Consensus problem with multi-agent systems has interested researchers in various areas. Its difficulties tend to appear when available information of each agent is limited for achieving consensus. Besides, it is not always the case that agents can catch the whole states of the others; an output is often the only possible measurement for each agent in applications. The idea of graph Laplacian is then of help to address such a troublesome situation.</p><p>While every single agent obviously makes decision to achieve an individual goal of minimizing its own cost functional, all agents as a team can obtain even more improvement by cooperation in some cases, which leads to cooperative game theoretic approach. The main goal of this master thesis is to accomplish a combination of optimal control theory and cooperative game theory in order to solve the output consensus problem with limited network connectivity. Along with this combination, bargaining problems are considered out of necessity.</p>

Note added missing paragraph break and removed empty paragraph
----------------------------------------------------------------------
In diva2:1211524 
abstract is: 
<p>This thesis in applied statistics and industrial economics examines the correlation between a number of market conditions on the Swedish and Global market and the yield difference between the Swedish stock market and the Global stock market. The report is based on data from the index MSCI Sweden Net Return, MSCI World Net Return and the Volatility index S&amp;P 500. The market conditions that have been examined are Bull markets, Bear markets, periods of high volatility. We also examined how the appreciation of the SEK in comparison to the USD and the yield of the Swedish stock market correlated with the yield difference between the Swedish Stock Market and the Global stock market. The correlation was examined using multiple linear regression. The results indicated a positive correlation between the yield difference between the Swedish stock market and the Global stock market and the yield of the Swedish stock market, the appreciation of the SEK compared to the USD and Bull markets. We found a negative correlation with Bear markets and no correlation at all with the volatility.</p><p> </p><p>The results are in line with what could be expected and give a stronger statistical ground for the idea that the Swedish stock market has larger fluctuations than the Global stock market during large-scale market fluctuations.</p>

corrected abstract:
<p>This thesis in applied statistics and industrial economics examines the correlation between a number of market conditions on the Swedish and Global market and the yield difference between the Swedish stock market and the Global stock market. The report is based on data from the index MSCI Sweden Net Return, MSCI World Net Return and the Volatility index S&amp;P 500. The market conditions that have been examined are Bull markets, Bear markets, periods of high volatility. We also examined how the appreciation of the SEK in comparison to the USD and the yield of the Swedish stock market correlated with the yield difference between the Swedish Stock Market and the Global stock market. The correlation was examined using multiple linear regression. The results indicated a positive correlation between the yield difference between the Swedish stock market and the Global stock market and the yield of the Swedish stock market, the appreciation of the SEK compared to the USD and Bull markets. We found a negative correlation with Bear markets and no correlation at all with the volatility.</p><p>The results are in line with what could be expected and give a stronger statistical ground for the idea that the Swedish stock market has larger fluctuations than the Global stock market during large-scale market fluctuations.</p>

Note - only change - removed empty paragraph in the middle
----------------------------------------------------------------------
In diva2:1674019 
abstract is: 
<p>Low Reynolds number airfoil analysis has become increasingly significant as urban air mobility vehicles and unmanned aerial vehicles surge in popularity. The Green Raven project at KTH Aero aims to use reflex airfoils where little data is available beyond classical analysis. Viscous formulations of the panel method and computational fluid dynamics (CFD) have been used to simulate lift, drag and moments for the MH61 and MH104 airfoils at different angles of attack (AOAs). XFOIL and CFD turbulence models such as Spalart-Allmaras (SA), k-w Shear Stress Transport (SST) with and without damping coefficients were used. The strengths and limitations of each model were used to justify results. Due to clear computational advantages, XFOIL produced adequate results and is tailored toward use in initial design stages where repeated measurements are crucial. The SA turbulence stood out as the model produced accurate results in a reasonable time. The abundance of published CFD material comparing different turbulence models increased the credibility of the results. The two airfoils had similar lift and drag characteristics at AOAs of 0-6 deg while the MH104 was superior near stall. However, due to the lack of experimental data of the airfoils no particular model could be commended or verified.</p><p> </p>

corrected abstract:
<p>Low Reynolds number airfoil analysis has become increasingly significant as urban air mobility vehicles and unmanned aerial vehicles surge in popularity. The Green Raven project at KTH Aero aims to use reflex airfoils where little data is available beyond classical analysis. Viscous formulations of the panel method and computational fluid dynamics (CFD) have been used to simulate lift, drag and moments for the MH61 and MH104 airfoils at different angles of attack (AOAs). XFOIL and CFD turbulence models such as Spalart-Allmaras (SA), 𝑘-ω Shear Stress Transport (SST) with and without damping coefficients were used. The strengths and limitations of each model were used to justify results. Due to clear computational advantages, XFOIL produced adequate results and is tailored toward use in initial design stages where repeated measurements are crucial. The SA turbulence stood out as the model produced accurate results in a reasonable time. The abundance of published CFD material comparing different turbulence models increased the credibility of the results. The two airfoils had similar lift and drag characteristics at AOAs of 0-6º while the MH104 was superior near stall. However, due to the lack of experimental data of the airfoils no particular model could be commended or verified.</p>
----------------------------------------------------------------------
In diva2:1778751 
abstract is: 
<p>Formula Student is a global engineering competition where university students collaborate to design, construct, and race formula-style cars. Aerodynamics is one aspect in the vehicle design that can improve on-track performance by increasing cornering and straight-line speed.</p><p>To improve the aerodynamics of KTH Formula Student's DeV18 vehicle, the side structure is being redesigned. The current model, DeV17, features an underperforming tunnel-based side structure. To address this issue, this had the goal to investigate a new multi-element wing design that utilizes ground effect.</p><p>The design study of the DeV18 vehicle is conducted using Siemens NX 2212 for 3D modelling and Simcenter Star-CCM+ 17.06.008-R8 for airflow simulations. To quickly investigate certain design parameters effect on the results, Design Manager Project inside Simcenter Star-CCM+ is used.</p><p>The resulting side structure produces a total of 26 N of downforce and 6 N of drag at 40 kph, more than twice that of DeV17’s side structure while also producing less drag. Although this significant improvement compared to DeV17, it is believed that further increases in performance are necessary to compete with top teams. By using a more sophisticated method to optimize the multi-element wing, such as adjoint optimization, the concept could be improved. However, the overall potential of the concept is still considered too limited to achieve the desired performance goals, which is why it will no longer be investigated further.</p><p> </p>

corrected abstract:
<p>Formula Student is a global engineering competition where university students collaborate to design, construct, and race formula-style cars. Aerodynamics is one aspect in the vehicle design that can improve on-track performance by increasing cornering and straight-line speed.</p><p>To improve the aerodynamics of KTH Formula Student's DeV18 vehicle, the side structure is being redesigned. The current model, DeV17, features an underperforming tunnel-based side structure. To address this issue, this had the goal to investigate a new multi-element wing design that utilizes ground effect.</p><p>The design study of the DeV18 vehicle is conducted using Siemens NX 2212 for 3D modelling and Simcenter Star-CCM+ 17.06.008-R8 for airflow simulations. To quickly investigate certain design parameters effect on the results, Design Manager Project inside Simcenter Star-CCM+ is used.</p><p>The resulting side structure produces a total of 26 N of downforce and 6 N of drag at 40 kph, more than twice that of DeV17’s side structure while also producing less drag. Although this significant improvement compared to DeV17, it is believed that further increases in performance are necessary to compete with top teams. By using a more sophisticated method to optimize the multi-element wing, such as adjoint optimization, the concept could be improved. However, the overall potential of the concept is still considered too limited to achieve the desired performance goals, which is why it will no longer be investigated further.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1776777 
abstract is: 
<p>In this Bachelor thesis, a novel algorithm for sampling from bandlimited circular probability distributions is presented. The algorithm leverages results from Fourier analysis concerning the Fejér kernel to simulate data with some desired probability distribution, realized as a sum of data sampled from a discrete distribution and a small continuous perturbation sampled from the Fejér kernel distribution. Relevant theory is presented before formally proving exact simulation using the algorithm. Experimental results confirm the validity of the theoretical results, and the efficiency of the algorithm is then compared with that of other sampling methods such as rejection sampling with a uniform envelope function.</p><p> </p>

corrected abstract:
<p>In this Bachelor thesis, a novel algorithm for sampling from bandlimited circular probability distributions is presented. The algorithm leverages results from Fourier analysis concerning the Fejér kernel to simulate data with some desired probability distribution, realized as a sum of data sampled from a discrete distribution and a small continuous perturbation sampled from the Fejér kernel distribution. Relevant theory is presented before formally proving exact simulation using the algorithm. Experimental results confirm the validity of the theoretical results, and the efficiency of the algorithm is then compared with that of other sampling methods such as rejection sampling with a uniform envelope function.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:525720 
abstract is: 
<p>Abstract</p><p> </p><p>In this report analysis of foreign exchange rates time series are performed. First, triangular arbitrage is detected and eliminated from data series using linear algebra tools. Then Vector Autoregressive processes are calibrated and used to replicate dynamics of exchange rates as well as to forecast time series. Finally, optimal portfolio of currencies with minimal Expected Shortfall is formed using one time period ahead forecasts</p>

corrected abstract:
<p>In this report analysis of foreign exchange rates time series are performed. First, triangular arbitrage is detected and eliminated from data series using linear algebra tools. Then Vector Autoregressive processes are calibrated and used to replicate dynamics of exchange rates as well as to forecast time series. Finally, optimal portfolio of currencies with minimal Expected Shortfall is formed using one time period ahead forecasts.</p>

Note - removed unnecessary "<p>Abstract</p><p> </p>" and added final missing period.
----------------------------------------------------------------------
In diva2:1566509 
abstract is: 
<p>This report is about a novel approach to attenuation of fan noise in aerial vehicles, by way of implementing a ducted fan in the chassis of a four meter blended wing body plane. Three different one meter PVC pipes were used and their performances as silencers were tested by measuring the sound power level and calculating the insulation loss compared to a fan by itself. The ducts were either empty or lined with acoustic absorbents and micro perforated panels. Experiments were carried out in the reverberation room at KTH Marcus Wallenberg laboratory for sound and vibration research using the guidelines in ISO 3741 (2010). The results showed that the empty duct lead to a 15.3 dB(A) insulation loss with no decrease in thrust from the fan. The absorbent and micro perforated panel, however, lead to a 22.7 dB(A) insulation loss while giving a major decrease in thrust of more than one order of magnitude. The results show the failure of implementation of the latter two silencers due to choking, but also the success of the empty duct. This shows that there is room for improvement and perhaps even a future possibility of a successful implementation in a real vehicle.</p><p> </p>

corrected abstract:
<p>This report is about a novel approach to attenuation of fan noise in aerial vehicles, by way of implementing a ducted fan in the chassis of a four meter blended wing body plane. Three different one meter PVC pipes were used and their performances as silencers were tested by measuring the sound power level and calculating the insulation loss compared to a fan by itself. The ducts were either empty or lined with acoustic absorbents and micro perforated panels. Experiments were carried out in the reverberation room at KTH Marcus Wallenberg laboratory for sound and vibration research using the guidelines in ISO 3741 (2010). The results showed that the empty duct lead to a 15.3 dB(A) insulation loss with no decrease in thrust from the fan. The absorbent and micro perforated panel, however, lead to a 22.7 dB(A) insulation loss while giving a major decrease in thrust of more than one order of magnitude. The results show the failure of implementation of the latter two silencers due to choking, but also the success of the empty duct. This shows that there is room for improvement and perhaps even a future possibility of a successful implementation in a real vehicle.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1673538 
abstract is: 
<p>The purpose of the study is to evaluate the possibility of using gridded ion thrusters as a means of attitude control for a solar sail as a part of the sunshade project, which aims to place 10^8 solar sail sunshade spacecraft, each with an area of about 10 000 m^2, at the Sun-Earth Lagrangian point L1 in order to reduce Earth's global temperature. Two types of solar sail sunshade spacecraft were studied. The first type, referred to as the sunshade demonstrator, had an area of 100 m^2 and a mass of 10 kg, and the second type, referred to as the full-sized sunshade, had an area of 10 000 m^2 and a mass of 90 kg. To determine the significance of using ion thrusters for the attitude control system, the mass of the required fuel, as well as the total mass that had to be added to the spacecraft to implement the attitude control system, was calculated. Two types of journeys were studied for each spacecraft type: starting from Low Earth Orbit (LEO) to L1 and from Geostationary Orbit (GEO) to L1, respectively. The results showed that the duration of the journey of the full-sized spacecraft was about 570 days from LEO to L1 and 370 from GEO to L1, respectively. The required amounts of fuel for the respective journeys were 580 g and 15 g, respectively, and resulted in a total additional mass of 7.8 kg and 7.2 kg, respectively.</p><p> </p>

corrected abstract:
<p>The purpose of the study is to evaluate the possibility of using gridded ion thrusters as a means of attitude control for a solar sail as a part of the sunshade project, which aims to place 10<sup>8</sup> solar sail sunshade spacecraft, each with an area of about 10 000 m<sup>2</sup>, at the Sun-Earth Lagrangian point L<sub>1</sub> in order to reduce Earth's global temperature. Two types of solar sail sunshade spacecraft were studied. The first type, referred to as the sunshade demonstrator, had an area of 100 m<sup>2</sup> and a mass of 10 kg, and the second type, referred to as the full-sized sunshade, had an area of 10 000 m<sup>2</sup> and a mass of 90 kg. To determine the significance of using ion thrusters for the attitude control system, the mass of the required fuel, as well as the total mass that had to be added to the spacecraft to implement the attitude control system, was calculated. Two types of journeys were studied for each spacecraft type: starting from Low Earth Orbit (LEO) to L<sub>1</sub> and from Geostationary Orbit (GEO) to L<sub>1</sub>, respectively. The results showed that the duration of the journey of the full-sized spacecraft was about 570 days from LEO to L<sub>1</sub> and 370 from GEO to L<sub>1</sub>, respectively. The required amounts of fuel for the respective journeys were 580 g and 15 g, respectively, and resulted in a total additional mass of 7.8 kg and 7.2 kg, respectively.</p>
----------------------------------------------------------------------
In diva2:1780538 
abstract is: 
<p>The displacement of the flow by a passing freight train can often result in dangerous conditions for railway equipment and people standing in the vicinity of the train. In this work, Computational Fluid Dynamics (CFD) simulations are performed to study the flow development around a moving freight train comprised of a Class 66 locomotive and four container wagons. The results will give a better insight into the effects that each flow structure can have in the flow within the train's slipstream. Both two- and three-dimensional simulations are carried out around the freight train using three different RANS turbulence models: the Spalart-Allmaras, the SST k-ω and the W&amp;J EARSM. Two cases of 10o and 30o crosswinds are also considered and compared to the no-crosswind case, as side-winds characterize the majority of real-life situations and are known to amplify the slipstream effects. The results are validated against available experimental and numerical data and they are thoroughly presented and discussed. The 30o crosswind case is also computed using a DDES simulation. A meshing strategy which involves the assembly of different mesh blocks with a non-matching interface boundary condition to create the complete domain is used and assessed, as an alternative meshing approach that can simplify and accelerate the set-up of different case-studies. Additionally, the two-dimensional study is used to assess the influence of different parameters on the solution, such as the grid resolution and the moving-ground boundary condition.</p><p> </p>

corrected abstract:
<p>The displacement of the flow by a passing freight train can often result in dangerous conditions for railway equipment and people standing in the vicinity of the train. In this work, Computational Fluid Dynamics (CFD) simulations are performed to study the flow development around a moving freight train comprised of a Class 66 locomotive and four container wagons. The results will give a better insight into the effects that each flow structure can have in the flow within the train's slipstream. Both two- and three-dimensional simulations are carried out around the freight train using three different RANS turbulence models: the Spalart-Allmaras, the SST 𝑘-ω and the W&amp;J EARSM. Two cases of 10º and 30º crosswinds are also considered and compared to the no-crosswind case, as side-winds characterize the majority of real-life situations and are known to amplify the slipstream effects. The results are validated against available experimental and numerical data and they are thoroughly presented and discussed. The 30º crosswind case is also computed using a DDES simulation. A meshing strategy which involves the assembly of different mesh blocks with a non-matching interface boundary condition to create the complete domain is used and assessed, as an alternative meshing approach that can simplify and accelerate the set-up of different case-studies. Additionally, the two-dimensional study is used to assess the influence of different parameters on the solution, such as the grid resolution and the moving-ground boundary condition.</p>
----------------------------------------------------------------------
In diva2:1780172 
abstract is: 
<p>In this report, we analyze general relativistic effects on celestial bodies, including gravitational strength in different metrics, gravitational radiation, and frame-dragging. We present simulation methods for classical and general relativistic motion, through the use of systems of equations that may be numerically integrated. The amount of energy leaving the system as gravitational radiation is approximated using the quadrupole formula, and by using a binary pair of planetary bodies as an approximation for orbital motion. Here we demonstrate that classical approximations may be suitable in low-mass high-distance scenarios. The eccentricity of an orbit also affects the gravitational radiation and would have to be much less than one for reliable results. It is concluded that frame-dragging effects are negligible for slowly rotating objects only, which is a well-known result.</p><p> </p>

corrected abstract:
<p>In this report, we analyze general relativistic effects on celestial bodies, including gravitational strength in different metrics, gravitational radiation, and frame-dragging. We present simulation methods for classical and general relativistic motion, through the use of systems of equations that may be numerically integrated. The amount of energy leaving the system as gravitational radiation is approximated using the quadrupole formula, and by using a binary pair of planetary bodies as an approximation for orbital motion. Here we demonstrate that classical approximations may be suitable in low-mass high-distance scenarios. The eccentricity of an orbit also affects the gravitational radiation and would have to be much less than one for reliable results. It is concluded that frame-dragging effects are negligible for slowly rotating objects only, which is a well-known result.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1218168 - missing space in title:
"Classification of social gestures: Recognizing waving using supervised machinelearning"
==>
"Classification of social gestures: Recognizing waving using supervised machine learning"


abstract is: 
<p>This paper presents an approach to gesture recognition including the use of a tool in order to extract certain key-points of the human body in each frame, and then processing this data and extracting features from this. The gestures recognized were two-handed waving and clapping. The features used were the maximum co-variance from a sine-fit to time-series of arm angles, as well as the max and min of this fitted sinus function. A support vector machine was used for the learning. The result was a promising accuracy of 93% <em>± </em>4% using 5-fold cross-validation. The limitations of the methods used are then discussed, which includes lack of support for more than one gesture in the data as well as some lack of generality in means of the features used. Finally some suggestions are made as to what improvements and further explorations could be made.</p><p> </p>

corrected abstract:
<p>This paper presents an approach to gesture recognition including the use of a tool in order to extract certain key-points of the human body in each frame, and then processing this data and extracting features from this. The gestures recognized were two-handed waving and clapping. The features used were the maximum co-variance from a sine-fit to time-series of arm angles, as well as the max and min of this fitted sinus function. A support vector machine was used for the learning. The result was a promising accuracy of 93%±4% using 5-fold cross-validation. The limitations of the methods used are then discussed, which includes lack of support for more than one gesture in the data as well as some lack of generality in means of the features used. Finally some suggestions are made as to what improvements and further explorations could be made.</p>

Note - removed the empty paragraph and removed the italization of "±" to match the original
----------------------------------------------------------------------
In diva2:802173 
abstract is: 
<p>A bank borrowing some money has to give some securities to the lender, which is called collateral. Different kinds of collateral can be posted, like cash in different currencies or a stock portfolio depending on the terms of the contract, which is called a Credit Support Annex (CSA). Those contracts specify eligible collateral, interest rate, frequency of collateral posting, minimum transfer amounts, etc. This guarantee reduces the counterparty risk associated with this type of transaction.</p><p>If a CSA allows for posting cash in different currencies as collateral, then the party posting collateral can, now and at each future point in time, choose which currency to post. This choice leads to optionality that needs to be accounted for when valuing even the most basic of derivatives such as forwards or swaps.</p><p>In this thesis, we deal with the valuation of embedded optionality in collateral contracts. We consider the case when collateral can be posted in two different currencies, which seems sufficient since collateral contracts are soon going to be simplified.</p><p>This study is based on the conditional independence approach proposed by Piterbarg [8]. This method is compared to both Monte-Carlo simulation and finite- difference method.</p><p>A practical application is finally presented with the example of a contract between Natixis and Barclays.</p><p> </p>

corrected abstract:
<p>A bank borrowing some money has to give some securities to the lender, which is called collateral. Different kinds of collateral can be posted, like cash in different currencies or a stock portfolio depending on the terms of the contract, which is called a Credit Support Annex (CSA). Those contracts specify eligible collateral, interest rate, frequency of collateral posting, minimum transfer amounts, etc. This guarantee reduces the counterparty risk associated with this type of transaction.</p><p>If a CSA allows for posting cash in different currencies as collateral, then the party posting collateral can, now and at each future point in time, choose which currency to post. This choice leads to optionality that needs to be accounted for when valuing even the most basic of derivatives such as forwards or swaps.</p><p>In this thesis, we deal with the valuation of embedded optionality in collateral contracts. We consider the case when collateral can be posted in two different currencies, which seems sufficient since collateral contracts are soon going to be simplified.</p><p>This study is based on the conditional independence approach proposed by Piterbarg [8]. This method is compared to both Monte-Carlo simulation and finite-difference method.</p><p>A practical application is finally presented with the example of a contract between Natixis and Barclays.</p>
----------------------------------------------------------------------
In diva2:1440982 
abstract is: 
<p>This report presents the design of the wing structure for a UAV called Skywalker X8. A model of the UAV was given and analyzed to design a wing box structure that is twice the size of the current model, with "greener" technology and lightweight materials. The loads that act upon the UAV were simulated and thereafter analyzed with the help of the CFD program called Star CCM+. Modifications on the CAD model and the FEM simulations were performed in Siemens NX. Eight different combinations were tested from the following five materials: CFRP (carbon fiber reinforced polymer), LDPE (low density polyethylene), polyethylene, polypropylene, and balsa wood. The results that best fit the requirements given was the combination of polypropylene as the wing skin and balsa as the honeycomb structure. This design weighed 3.576 kg and had the following stresses: 0.671 MPa, 0.340 MPa, 1 MPa, and 4 MPa for the angle of attacks at 1,2,3, and 6 degrees respectively. A modification of the trailing edge, which was the implementation of a Gurney flap, was made to see if it improved the lift-to-drag ratio, but unfortunately it did not so it was not developed further.</p><p> </p>


corrected abstract:
<p>This report presents the design of the wing structure for a UAV called Skywalker X8. A model of the UAV was given and analyzed to design a wing box structure that is twice the size of the current model, with “greener” technology and lightweight materials. The loads that act upon the UAV were simulated and thereafter analyzed with the help of the CFD program called Star CCM+. Modifications on the CAD model and the FEM simulations were performed in Siemens NX. Eight different combinations were tested from the following five materials: CFRP (carbon fiber reinforced polymer), LDPE (low density polyethylene), polyethylene, polypropylene, and balsa wood. The results that best fit the requirements given was the combination of polypropylene as the wing skin and balsa as the honeycomb structure. This design weighed 3.576 kg and had the following stresses: 0.671 MPa, 0.340 MPa, 1 MPa, and 4 MPa for the angle of attacks at 1,2,3, and 6 degrees respectively. A modification of the trailing edge, which was the implementation of a Gurney flap, was made to see if it improved the lift-to-drag ratio, but unfortunately it did not so it was not developed further.</p>

Note error in original
mc='1,2,3' c='1, 2, 3'
Also removed unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:1220102 
abstract is: 
<p>Chatbots, also called conversational agents, with speech interfaces are being used to a greater and greater extent, but there are still many areas that are not completely explored. The idea of this project was born out of the belief that there is a need for an assistant in the kitchen that is able to search for recipes, answer questions regarding them and guide and assist the user throughout the cooking process, all through conversation since the hands are busy. This paper begins with an introduction in the subject of conversational agents and the related technology, then similar, already existing studies and methods are presented with their pros and cons. After follows an in-depth explanation on how the program was constructed into a working kitchen assistant. Lastly, the users’ experiences of the performance and usability of the program was evaluated through tests and discussed. It turns out that conversational agents definitely can be integrated in the kitchen, and according to several sources, in a few years they will be implemented in all possible areas and change the technology of our time.</p><p> </p>

corrected abstract:
<p>Chatbots, also called conversational agents, with speech interfaces are being used to a greater and greater extent, but there are still many areas that are not completely explored. The idea of this project was born out of the belief that there is a need for an assistant in the kitchen that is able to search for recipes, answer questions regarding them and guide and assist the user throughout the cooking process, all through conversation since the hands are busy. This paper begins with an introduction in the subject of conversational agents and the related technology, then similar, already existing studies and methods are presented with their pros and cons. After follows an in-depth explanation on how the program was constructed into a working kitchen assistant. Lastly, the users’ experiences of the performance and usability of the program was evaluated through tests and discussed. It turns out that conversational agents definitely can be integrated in the kitchen, and according to several sources, in a few years they will be implemented in all possible areas and change the technology of our time.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1880482 
abstract is: 
<p>A time-effective coverage path can be decisive in catastrophic and war scenarios for saving countless lives where UAVs are used to scan an area looking for an objective. Given an area shaped as a polygon, a quadratic decomposition method is used to discretize the area into nodes. A model of the optimization problem constraint is created and solved using mixed-integer linear programming, taking into consideration simple dynamics and coverage path planning definitions. Simulations in different scenarios are presented, showing that the presence of no-fly zones can negatively affect the coverage time. The relationship between coverage time and the number of UAVs employed is nonlinear and converges to a constant value. The result has a direct impact on the evaluation of benefits and the cost of adding UAVs to a search mission.</p><p> </p>

corrected abstract:
<p>A time-effective coverage path can be decisive in catastrophic and war scenarios for saving countless lives where UAVs are used to scan an area looking for an objective. Given an area shaped as a convex polygon, a grid-based decomposition method is used to discretize the area into squares, represented by nodes. An optimization problem, considering simple dynamics and coverage path planning definitions, is developed using mixed-integer linear programming framework. Simulations in different scenarios are presented, showing that the presence of no-fly zones can negatively affect the coverage time. The relationship between coverage time and the number of UAVs employed is nonlinear and converges to a constant value. The result has a direct impact on the evaluation of benefits and the cost of adding UAVs to search missions.</p>

Note the change in wording (based on the original) and remove of the unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:414817 
abstract is: 
<p>The purpose with this thesis work is to simulate the deflection due to creep of Kanthal(R) APMT furnace tubes using the finite element method (FEM). Kanthal APMT is a material which shows a larger primary creep compared to other metals. Therefore the creep deformation must be described with a material model which takes both primary and secondary creep into consideration. In this thesis work a material model called time hardening has been used.</p>
<p>*C2 is stress dependent. By modifying C2 so that the results from the simulations better corresponds with test data an equation for how C2 depends on the stress could be obtained.</p>
<p>The value for C2 is then calculated for each tube dimension giving results which are close to the data from sagging tests. The results may be seen as an overestimation of the actual deflection. A sensitivity analysis showed that the model is very sensitive to changes in the material parameters. A few percent change in C2 for example will change the deflection by more than 100 percent. </p>
<p>
<p>* For equation see full text</p>
<p>
<p> </p>
<p> </p>
</p>
</p>

corrected abstract:
<p>The purpose with this thesis work is to simulate the deflection due to creep of Kanthal® APMT furnace tubes using the finite element method (FEM). Kanthal APMT is a material which shows a larger primary creep compared to other metals. Therefore the creep deformation must be described with a material model which takes both primary and secondary creep into consideration. In this thesis work a material model called time hardening has been used.</p>
<p>C2 is stress dependent<sup><a href="#fn1" id="ref1">*</a></sup>. By modifying C2 so that the results from the simulations better corresponds with test data an equation for how C2 depends on the stress could be obtained.</p>
<p>The value for C2 is then calculated for each tube dimension giving results which are close to the data from sagging tests. The results may be seen as an overestimation of the actual deflection. A sensitivity analysis showed that the model is very sensitive to changes in the material parameters. A few percent change in C2 for example will change the deflection by more than 100 percent.</p>
<div id="footnotes">
    <ul style="list-style: '*'; padding-left: 20px;">
        <li id="fn1">For equation see full text <a href="#ref1" aria-label="Back to reference">↩</a></li>
    </ul>
</div>

----------------------------------------------------------------------
In diva2:894096 
abstract is: 
<p>E-sports is growing and the price pools in e-sports tournaments are increasing, Valves video game DotA 2 is one of the bigger e-sports. As professional gamers train to increase their skill, new tools to help the training might become very important. Eye tracking can give an extra training dimension for the gamer. The aim of this master thesis is to develop a Visual Attention Index for DotA 2, that is, a number that reflects the player’s visual attention during a game. Interviews with gamers combined with data collection from gamers with eye trackers and statistical methods were used to find relevant metrics to use in the work. The results show that linear regression did not work very well on the data set, however, since there were a low number of test persons, further data collection and testing needs to be done before any statistically significant conclusions can be drawn. Support Vector Machines (SVM) was also used and turned out to be an effective way of separating better players from less good players. A new SVM method, based on linear programming, was also tested and found to be efficient and easy to apply on the given data set.</p><p> </p>

corrected abstract:
<p>E-sports is growing and the price pools in e-sports tournaments are increasing, Valves video game DotA <span style="font-size: 0.8em;">&#x1D7E4;</span> is one of the bigger e-sports. As professional gamers train to increase their skill, new tools to help the training might become very important. Eye tracking can give an extra training dimension for the gamer. The aim of this master thesis is to develop a Visual Attention Index for DotA <span style="font-size: 0.8em;">&#x1D7E4;</span>, that is, a number that reflects the player’s visual attention during a game. Interviews with gamers combined with data collection from gamers with eye trackers and statistical methods were used to find relevant metrics to use in the work. The results show that linear regression did not work very well on the data set, however, since there were a low number of test persons, further data collection and testing needs to be done before any statistically significant conclusions can be drawn. Support Vector Machines (SVM) was also used and turned out to be an effective way of separating better players from less good players. A new SVM method, based on linear programming, was also tested and found to be efficient and easy to apply on the given data set.</p>

Note removed the unnecessary empty paragraph and change "2" to "&#x1D7E4;" - a Mathematical Sans-Serif Digit Two;  <span style="font-size: 0.8em;"> has been used to reduce the relative font size to match the look of the original
----------------------------------------------------------------------
In diva2:1319924 
abstract is: 
<p>Credit scoring using machine learning has been gaining attention within the research field in recent decades and it is widely used in the financial sector today. Studies covering binary credit scoring of securitized non-performing loans are however very scarce. This paper is using random forest and artificial neural networks to predict debt recovery for such portfolios. As a performance benchmark, logistic regression is used. Due to the nature of high imbalance between the classes, the performance is evaluated mainly on the area under both the receiver operating characteristic curve and the precision-recall curve. This paper shows that random forest, artificial neural networks and logistic regression have similar performance. They all indicate an overall satisfactory ability to predict debt recovery and hold potential to be implemented in day-to-day business related to non-performing loans.</p><p> </p>

corrected abstract:
<p>Credit scoring using machine learning has been gaining attention within the research field in recent decades and it is widely used in the financial sector today. Studies covering binary credit scoring of securitized non-performing loans are however very scarce. This paper is using random forest and artificial neural networks to predict debt recovery for such portfolios. As a performance benchmark, logistic regression is used. Due to the nature of high imbalance between the classes, the performance is evaluated mainly on the area under both the receiver operating characteristic curve and the precision-recall curve. This paper shows that random forest, artificial neural networks and logistic regression have similar performance. They all indicate an overall satisfactory ability to predict debt recovery and hold potential to be implemented in day-to-day business related to non-performing loans.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1568336 
abstract is: 
<p>X-ray computed tomography (CT) has since its introduction in the early 1970s become one of the most important tools used for medical imaging. In CT, a large number of x-ray attenuation measurements are combined and reconstructed to form a three-dimensional image of the targeted area. In the recent years, a new type of detector called photon counting detector (PCD) has attracted considerable interest. This new type of detector acquires spectral information is associated with several benefits and has shown to be very valuable. </p><p>Furthermore, the use of deep learning to reconstruct images produced by CT has attracted significant attention in the last couple of years. However, the best way of incorporating deep learning into the reconstruction chain into the reconstruction chain is still incompletely understood. Additionally, the use of deep learning has mainly been investigated for the case of conventional CT and not for CT performed with PCDs. It these two points that this work aims to address. </p><p>Multiple deep learning architectures were implemented and evaluated on material images acquired by simulating a PCD. The deep-learning part of the reconstruction took the form of image-domain denoising after the material images had been obtained from the material sinograms through filtered back projection. Then, a comparison between the different deep learning architectures was made to find out which architecture is the most suited for denoising images produced by PCDs in the image domain.</p><p> </p>

corrected abstract:
<p>X-ray computed tomography (CT) has since its introduction in the early 1970s become one of the most important tools used for medical imaging. In CT, a large number of x-ray attenuation measurements are combined and reconstructed to form a three-dimensional image of the targeted area. In the recent years, a new type of detector called photon counting detector (PCD) has attracted considerable interest. This new type of detector acquires spectral information is associated with several benefits and has shown to be very valuable.</p><p>Furthermore, the use of deep learning to reconstruct images produced by CT has attracted significant attention in the last couple of years. However, the best way of incorporating deep learning into the reconstruction chain into the reconstruction chain is still incompletely understood. Additionally, the use of deep learning has mainly been investigated for the case of conventional CT and not for CT performed with PCDs. It these two points that this work aims to address.</p><p>Multiple deep learning architectures were implemented and evaluated on material images acquired by simulating a PCD. The deep-learning part of the reconstruction took the form of image-domain denoising after the material images had been obtained from the material sinograms through filtered back projection. Then, a comparison between the different deep learning architectures was made to find out which architecture is the most suited for denoising images produced by PCDs in the image domain.</p>

Note - only change to remove the empty paragraph and removing unnecessary spaces at end of paragraphs.
----------------------------------------------------------------------
In diva2:1334765 
abstract is: 
<p>A global statement about a compact surface with constant Gaussian curvature is derived by elementary differential geometry methods. Surfaces and curves embedded in three-dimensional Euclidian space are introduced, as well as several key properties such as the tangent plane, the first and second fundamental form, and the Weingarten map. Furthermore, intrinsic and extrinsic properties of surfaces are analyzed, and the Gaussian curvature, originally derived as an extrinsic property, is proven to be an intrinsic property in Gauss Theorema Egregium. Lastly, through the aid of umbilical points on a surface, the statement that a compact, connected surface with constant Gaussian curvature is a sphere is proven.</p><p> </p>

corrected abstract:
<p>A global statement about a compact surface with constant Gaussian curvature is derived by elementary differential geometry methods. Surfaces and curves embedded in three-dimensional Euclidian space are introduced, as well as several key properties such as the tangent plane, the first and second fundamental form, and the Weingarten map. Furthermore, intrinsic and extrinsic properties of surfaces are analyzed, and the Gaussian curvature, originally derived as an extrinsic property, is proven to be an intrinsic property in Gauss Theorema Egregium. Lastly, through the aid of umbilical points on a surface, the statement that a compact, connected surface with constant Gaussian curvature is a sphere is proven.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1682470 
abstract is: 
<p>We present and prove some important theorems regarding determinantal point processes. In particular we focus on existance and uniqueness theorems. Furthermore, we present an algorithm for generating determinantal point processes with a finite-dimensional projection kernel. Also, we go through the mathematical preliminaries required to understand the theory.</p><p> </p>

corrected abstract:
<p>We present and prove some important theorems regarding determinantal point processes. In particular we focus on existance and uniqueness theorems. Furthermore, we present an algorithm for generating determinantal point processes with a finite-dimensional projection kernel. Also, we go through the mathematical preliminaries required to understand the theory.</p>

Note spelling error:
----------------------------------------------------------------------
In diva2:1380177 
abstract is: 
<p>The objective of this project is the development of a mission analysis tool for the nanosatellite company GomSpace Sweden. Although there are many existing software, they can be quite complicated and time consuming to use. The goal of this work is to build a simple app to be used at the earliest stages of space missions in order to obtain key figures of merit quickly and easily. By comparing results, assessing the feasibility of customer needs, analysing how various parameters affect each other, it enables immediate deeper understanding of the implications of the main design decisions that are taken at the very beginning of a mission. The tool shall aid the system engineering process of determining orbit manoeuvre capability specifically for CubeSat electric propulsion systems taking into account the most relevant factors for perturbation in Low Earth Orbit (LEO), i.e. atmospheric drag and Earth’s oblateness effects. The manoeuvres investigated are: orbit raising from an insert orbit to an operating orbit, orbit maintenance, deorbiting within the space debris mitigation guidelines and collision avoidance within the 12 to 24 hours that the system has to react. The manoeuvres cost is assessed in terms of Delta v requirements, propellant mass and transfer times. The tool was developed with MATLAB and packaged as a standalone Linux application.</p><p> </p>

corrected abstract:
<p>The objective of this project is the development of a mission analysis tool for the nanosatellite company GomSpace Sweden. Although there are many existing software, they can be quite complicated and time consuming to use. The goal of this work is to build a simple app to be used at the earliest stages of space missions in order to obtain key figures of merit quickly and easily. By comparing results, assessing the feasibility of customer needs, analysing how various parameters affect each other, it enables immediate deeper understanding of the implications of the main design decisions that are taken at the very beginning of a mission. The tool shall aid the system engineering process of determining orbit manoeuvre capability specifically for CubeSat electric propulsion systems taking into account the most relevant factors for perturbation in Low Earth Orbit (LEO), i.e. atmospheric drag and Earth’s oblateness effects. The manoeuvres investigated are: orbit raising from an insert orbit to an operating orbit, orbit maintenance, deorbiting within the space debris mitigation guidelines and collision avoidance within the 12 to 24 hours that the system has to react. The manoeuvres cost is assessed in terms of ∆v requirements, propellant mass and transfer times. The tool was developed with MATLAB and packaged as a standalone Linux application.</p>

Note - only change to remove the empty paragraph and replacing "Delta" with "∆".
----------------------------------------------------------------------
In diva2:1524971 
abstract is: 
<p>In this master thesis an input-model of a Nordic BWR power plant has been developed in APROS. The plant model contains key systems and major thermohydraulic components of the steam cycle, including I&amp;C systems (i.e. power, pressure, level and flow controls). The plant model is primarily designed for balance of plant studies at discrete power levels.</p><p>The input-model of the power plant focuses especially on the steam cycle which is crucial for analysing water and steam behaviour and its influence on the reactor power. At the current stage, the model primarily handles steady-state conditions of full-power operation, which has been the design point. It has also been shown that reduced-power operation can be simulated with a reasonable trendline of pressure and temperature progression over facility components.</p><p> </p>

corrected abstract:
<p>Nuclear power plants have proved to produce reliable and economic electricity but at the same time provoked debates mainly because of the risks involved during operation. To prevent unwanted events, it is important to identify and estimate their occurrence, and introduce measures to counteract them. Modelling and simulations are powerful tools that can be used to gain this type of knowledge and obtain information about the birthplace of incidents. However, the area of use is not limited to the safety perspective, as computer simulations can also introduce true advances in performance and reliability of power plants.</p><p>This master thesis was conducted at Westinghouse Electric Sweden AB, with the purpose to design and implement an input-model of a Nordic Boiling Water Reactor in APROS. The inputmodel of the power plant focused especially on the steam cycle which is crucial for analysing water and steam behaviour in the power plant and its influence on the reactor power. The inputmodel has been limited to representsteady-state conditions at full-power operation, and to some extent reduced-power operation. Thereby, plant model is primarily designed for Balance of Plant studies at discrete power levels.</p><p>The first section of the report contains an introduction to the concepts of nuclear energy and fundamentals of boiling water reactors. It is supposed to provide the reader with a basis for a fair understanding of nuclear power plant operation. Theoretical concepts of thermodynamics and fluid mechanics, which have been crucial for a proper approach in the process of creating the input-model, can be found in the theory section. The report does also contain a brief description of the plant systems upon which the design has been based on.</p><p>The report consists of further sections, where the model components and their implementation are presented followed by a model validation. The model validation is performed by a comparison approach, where simulation data is presented in relationship to reference data. The validation is done for full-power and reduce-power operation, at steady-state conditions, at which the model has shown to have decent compliance with the available reference data.</p><p>At the final stage of the project, the created input-model was used to evaluate an induced perturbation of feedwater temperature. The behaviour of the reactor, dependent of the feedwater temperature, is discussed for two simulation cases; with and without forced power control. The simulation enabled to perform a first step analysis of the effectiveness of the power control system.</p>

Note the abstract in the thesis is completely different from that in DiVA!
----------------------------------------------------------------------
In diva2:699782 
abstract is: 
<p>The dose distribution in the Gamma Knife (developed and produced by Elekta) is optimized over the weights (or Beam-on time) using different models other than the radiosurgical one used in Leksell Gamma Plan . These are based on DVH, EUD, TCP and NTCP. Also adding hypoxic regions are tested in the Gamma Knife to see whether or not the dose can be guided to these areas. This is done in two ways. For the DVH and EUD model the hypoxic area is regarded as a organ by itself and higher constraints is defined on it. In the TCP case blood vessels are outlined and the α and β parameters are perturbed to describe a hypoxic area. The models are tested in two cases. The first one is one tumour close to the brainstem and the second case is two tumours located far away from each other. Finally the results are compared to the dose distribution computed by the Gamma Knife.</p><p> </p>

corrected abstract:
<p>The dose distribution in the Gamma Knife is optimized over the weights (or Beam-on time) using different models other than the radiosurgical one used in Leksell Gamma Plan®. These are based on DVH, EUD, TCP and NTCP. Also adding hypoxic regions are tested in the Gamma Knife to see whether or not the dose can be guided to these areas. This is done in two ways. For the DVH and EUD model the hypoxic area is regarded as a organ by itself and higher constraints is defined on it. In the TCP case blood vessels are outlined and the α and β parameters are perturbed to describe a hypoxic area. The models are tested in two cases. The first one is one tumour close to the brainstem and the second case is two tumours located far away from each other. Finally the results are compared to the dose distribution computed by the Gamma Knife.</p>

Nore removed text that was not in the original, added the registered trademark symbol, and eliminated the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1519571 
abstract is: 
<p>For the purpose of americium recycling, the effect of americium content on the nuclear fuel behaviour needs to be investigated. Atomic scale simulations and classical molecular dynamic simulations provide a tool of choice for the study of thermophysical properties of the nuclear fuel.</p><p>In this work, we fitted a new interatomic empirical potential for (U,Am)O2 based on the CRG formalism. Our work enabled us to propose at the same time a new potential for the study of the Am-O system. The proposed potentials show good agreement with lattice parameters and enthalpy increments. We finally computed the heat capacity of (U,Am)O2 from 350 K to 3200 K for 0, 10, 20, 30, 40 and 50% americium contents using the potential obtained. The heat capacities calculated reveal a Bredig transition, as seen in UO2 and (U,Pu)O2. This transition shifts toward lower temperatures and its peak decreases in intensity when the Am content increases.</p><p> </p>

corrected abstract:
<p>For the purpose of americium recycling, the effect of americium content on the nuclear fuel behaviour needs to be investigated. Atomic scale simulations and classical molecular dynamic simulations provide a tool of choice for the study of thermophysical properties of the nuclear fuel.</p><p>In this work, we fitted a new interatomic empirical potential for (U,Am)O<sub>2</sub> based on the CRG formalism. Our work enabled us to propose at the same time a new potential for the study of the Am-O system. The proposed potentials show good agreement with lattice parameters and enthalpy increments. We finally computed the heat capacity of (U,Am)O<sub>2</sub> from 350 K to 3200 K for 0, 10, 20, 30, 40 and 50% americium contents using the potential obtained. The heat capacities calculated reveal a Bredig transition, as seen in UO<sub>2</sub> and (U,Pu)O<sub>2</sub>. This transition shifts toward lower temperatures and its peak decreases in intensity when the Am content increases.</p>

Note - only change to remove the empty paragraph and adding the subscripts
----------------------------------------------------------------------
In diva2:1779375 
abstract is: 
<p>The purpose of this project is to simulate the detection of γ-ray spectra emitted by radon isotopes and their daughters. This is done as a contribution to the development of radiation detectors to be used in a research project investigating the possibility of using increased amounts of the radioactive gas radon as an earthquake precursor. Before the onset of an earthquake, microcracks are formed in the surrounding stone structures due to stress, releasing greater than usual amounts of radon gas contained within the rock pores. A way of predicting an upcoming earthquake would then be to place radiation detectors in areas with high seismicity in order to measure possible changes. This could be done in soil, groundwater (via springs, wells, and boreholes), or air. In this project, we aim to understand how measurements in groundwater would differ from ones in air, and how to best make use of the spectra as seen in water. This was done by simulating a scenario in which a scintillator detector, made of cesium iodide, is placed in each media and then assessing the resulting γ-ray spectra.</p><p> </p>

corrected abstract:
<p>The purpose of this project is to simulate the detection of γ-ray spectra emitted by radon isotopes and their daughters. This is done as a contribution to the development of radiation detectors to be used in a research project investigating the possibility of using increased amounts of the radioactive gas radon as an earthquake precursor. Before the onset of an earthquake, microcracks are formed in the surrounding stone structures due to stress, releasing greater than usual amounts of radon gas contained within the rock pores. A way of predicting an upcoming earthquake would then be to place radiation detectors in areas with high seismicity in order to measure possible changes. This could be done in soil, groundwater (via springs, wells, and boreholes), or air. In this project, we aim to understand how measurements in groundwater would differ from ones in air, and how to best make use of the spectra as seen in water. This was done by simulating a scenario in which a scintillator detector, made of cesium iodide, is placed in each media and then assessing the resulting γ-ray spectra.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:373694
Note: no full text in DiVA

abstract is: 
<p>The department of Medical Technology, where I have done Master thesis project, develops and researches new method and technique within areas where ultrasound can be used to obtain the image of anatomical structure, functional capabilities and to suggest required treatment.</p>
<p>Nowadays cardio-vascular diseases, such as infarct, atherosclerosis and ischemic syndrome, are one of the most widespread diseases in the world that’s why timely detection, identification and treatment are so important.</p>
<p>The Master of Science qualification report consists 3 major parts: Medico-biological part, Design and Research parts.</p>
<p><strong>In Medico-biological part </strong>has been analyzed anatomical and physiological structure of the heart, current status of echocardiography with comparing with other techniques, summary of ultrasound methods with list of parameters that can be achieved is presented.</p>
<p><strong>In Design part </strong>has been developed new graphical modality based on Delta-V pump model using vector based statistical analysis for identification patients with ischemia. Software algorithm for automatically determine characteristic points for state diagram written in MatLab has been developed and implemented.</p>
<p><strong>In Research part </strong>in the first task using commercially available software based on Principal Component Analysis collected data from the hospital patients has been studied, results proved hypothesis concerning time variables importance; in the second task graphical module has been examined using collected data from the hospital patients both normal and with different cardio-vascular disease, and the results show good detection power of the algorithm.</p>
<p>At the end of the project presentation has been done and report has been published.</p>
<p>This project has been done in collaboration with the biggest medical institute in Sweden – Karolinska Institute - and results will be used in medical practice in Karolinska University Hospital in Huddinge and for future scientific needs.</p>
<p> </p>

corrected abstract:
<p>The department of Medical Technology, where I have done Master thesis project, develops and researches new method and technique within areas where ultrasound can be used to obtain the image of anatomical structure, functional capabilities and to suggest required treatment.</p>
<p>Nowadays cardio-vascular diseases, such as infarct, atherosclerosis and ischemic syndrome, are one of the most widespread diseases in the world that’s why timely detection, identification and treatment are so important.</p>
<p>The Master of Science qualification report consists 3 major parts: Medico-biological part, Design and Research parts.</p>
<p><strong>In Medico-biological part </strong>has been analyzed anatomical and physiological structure of the heart, current status of echocardiography with comparing with other techniques, summary of ultrasound methods with list of parameters that can be achieved is presented.</p>
<p><strong>In Design part </strong>has been developed new graphical modality based on Delta-V pump model using vector based statistical analysis for identification patients with ischemia. Software algorithm for automatically determine characteristic points for state diagram written in MatLab has been developed and implemented.</p>
<p><strong>In Research part </strong>in the first task using commercially available software based on Principal Component Analysis collected data from the hospital patients has been studied, results proved hypothesis concerning time variables importance; in the second task graphical module has been examined using collected data from the hospital patients both normal and with different cardio-vascular disease, and the results show good detection power of the algorithm.</p>
<p>At the end of the project presentation has been done and report has been published.</p>
<p>This project has been done in collaboration with the biggest medical institute in Sweden – Karolinska Institute - and results will be used in medical practice in Karolinska University Hospital in Huddinge and for future scientific needs.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1673952 
abstract is: 
<p>The conventional tube-and-wing aircraft has been around since the 1950s, with little to no innovative progress being made towards redesigning the conventional aircraft. The blended wing body (BWB) shape fuses the wing of the aircraft with the fuselage increasing structural strength while also increasing potential surface area to create lift, making it more efficient than conventional wing shapes. Today aviation has a 2 % CO2 contribution to global emissions. Aircraft manufacturers are predicting a steady rise for the aviation industry. The contribution of green-house gases is set to increase exponentially. Hydrogen fuel cells could deem a good fit between traditional combustion engine aircraft and electrical aircraft having a high efficiency but also being fuel-based. This report investigates the possibility of a prototype model of the Project ''Green Raven'' from KTH of creating a hybrid fuel cell BWB UAV with a 4 m wingspan. The analytical data is from literature and available benchmark data. First, an electrically driven subscale prototype is made and tested, and then the full-scale model is made. The prototype is pro-posed to be driven by a single two-bladed propeller with 10 x 4.7-inch dimensions running at 10000-13000 rpm with a takeoff weight of 4 kg, where 0.75 kg of the weight was from 5 Li-Po batteries. Performance parameters were calculated by given data with a given cruise speed of 30 m/s and a cruise endurance of 1 hour. The prototype will fly for close to maximum load at climb with an angle of 6°. With the Li-Po batteries with a total of 11 Ah, the aircraft has more than 10 % to spare for safety reasons.</p><p> </p>

corrected abstract:
<p>The conventional tube-and-wing aircraft has been around since the 1950s, with little to no innovative progress being made towards redesigning the conventional aircraft. The blended wing body (BWB) shape fuses the wing of the aircraft with the fuselage increasing structural strength while also increasing potential surface area to create lift, making it more efficient than conventional wing shapes. Today aviation has a 2 % CO<sub>2</sub> contribution to global emissions. Aircraft manufacturers are predicting a steady rise for the aviation industry. The contribution of greenhouse gases is set to increase exponentially. Hydrogen fuel cells could deem a good fit between traditional combustion engine aircraft and electrical aircraft having a high efficiency but also being fuel-based. This report investigates the possibility of a prototype model of the Project ”Green Raven” from KTH of creating a hybrid fuel cell BWB UAV with a 4 m wingspan. The analytical data is from literature and available benchmark data. First, an electrically driven subscale prototype is made and tested, and then the full-scale model is made. The prototype is proposed to be driven by a single two-bladed propeller with 10 x 4.7-inch dimensions running at 10000-13000 rpm with a takeoff weight of 4 kg, where 0.75 kg of the weight was from 5 Li-Po batteries. Performance parameters were calculated by given data with a given cruise speed of 30 m/s and a cruise endurance of 1 hour. The prototype will fly for close to maximum load at climb with an angle of 6°. With the Li-Po batteries with a total of 11 Ah, the aircraft has more than 10 % to spare for safety reasons.</p>

Note - only changes to remove the empty paragraph, added subscript, and removed the unnecessary hpyehn in "greenhouse".
----------------------------------------------------------------------
In diva2:1342226 
abstract is: 
<p>Functional testing is a vital process when building a satellite. However, often using flight-ready hardware for testing is not feasible. The work in this project has been to construct a flight representative model of the antenna deployment system for the KTH student-built MIST satellite. Specifically, the focus has been on creating a physical simulator for the antenna system. The purpose of the simulator created is to achieve the correct behavior, but without the need to use the real flight hardware. The challenges mainly concern establishing communication between the on-board computer of the satellite and the microcontroller on the created antenna deployment system, via the I$^2$C bus, and ensuring that physical responses occur in a useful manner. Further, the simulator needed to implement software with the same functionality as the real system. The microcontroller used in this project was an Arduino Due that represented the antenna deployment system's microcontroller. All the functions, e.g. temperature sensor and LEDs, were put together on a custom-made add-on circuit for the Arduino. Moreover, a 3D-printed model has been made for the deployment mechanism of the antenna elements. A simulation of the antenna system has been produced, determining whether a custom-built simulator can be used for functional testing of the antenna deployment system. The simulator can later be used for functional testing of the MIST satellite and also be the base for testing the deployment of the solar panels.</p><p> </p>

corrected abstract:
<p>Functional testing is a vital process when building a satellite. However, often using flight-ready hardware for testing is not feasible. The work in this project has been to construct a flight representative model of the antenna deployment system for the KTH student-built MIST satellite. Specifically, the focus has been on creating a physical simulator for the antenna system. The purpose of the simulator created is to achieve the correct behavior, but without the need to use the real flight hardware.</p><p>The challenges mainly concern establishing communication between the on-board computer of the satellite and the microcontroller on the created antenna deployment system, via the I<sup>2</sub>C bus, and ensuring that physical responses occur in a useful manner. Further, the simulator needed to implement software with the same functionality as the real system. The microcontroller used in this project was an Arduino Due that represented the antenna deployment system's microcontroller.</p><p>All the functions, e.g. temperature sensor and LEDs, were put together on a custom-made add-on circuit for the Arduino. Moreover, a 3D-printed model has been made for the deployment mechanism of the antenna elements. A simulation of the antenna system has been produced, determining whether a custom-built simulator can be used for functional testing of the antenna deployment system. The simulator can later be used for functional testing of the MIST satellite and also be the base for testing the deployment of the solar panels.</p>
----------------------------------------------------------------------
In diva2:1334807 
abstract is: 
<p>In this report, we study an abstract representation of reflection groups called Coxeter groups. Firstly, we introduce some important aspects of group theory. Next, we describe a concept called the word problem. Then, a way of defining groups given a set of generators and relations is presented. This theory is used to define the Coxeter groups, followed by a complete classification of the finite Coxeter groups as presented by H.S.M. Coxeter in 1935. Finally, we present a solution to the word problem for Coxeter groups and discuss some applications.</p><p> </p>

corrected abstract:
<p>In this report, we study an abstract representation of reflection groups called Coxeter groups. Firstly, we introduce some important aspects of group theory. Next, we describe a concept called the word problem. Then, a way of defining groups given a set of generators and relations is presented. This theory is used to define the Coxeter groups, followed by a complete classification of the finite Coxeter groups as presented by H.S.M. Coxeter in 1935. Finally, we present a solution to the word problem for Coxeter groups and discuss some applications.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:839498 
abstract is: 
<p>Our immune system uses antibodies to neutralize pathogens such as bacteria and viruses. Antibodies bind to parts of foreign proteins with high efficiency and specificity. We call such parts epitopes. The identification of epitopes, namely epitope mapping, may contribute to various immunological applications such as vaccine design, antibody production and immunological diagnosis.</p><p>Therefore, a fast and reliable method that can predict epitopes from the whole proteome is highly desirable.</p><p> </p><p>In this work we have developed a computational method that predicts epitopes based on sequence information. We focus on using local alignment to extract features from peptides and classifying them using Support Vector Machine. We also propose two approaches to optimize the features. Results show that our method can reliably predict epitopes and significantly outperforms some most commonly used tools.</p><p> </p>

corrected abstract:
<p>Our immune system uses antibodies to neutralize pathogens such as bacteria and viruses. Antibodies bind to parts of foreign proteins with high efficiency and specificity. We call such parts epitopes. The identification of epitopes, namely epitope mapping, may contribute to various immunological applications such as vaccine design, antibody production and immunological diagnosis. Therefore, a fast and reliable method that can predict epitopes from the whole proteome is highly desirable.</p><p>In this work we have developed a computational method that predicts epitopes based on sequence information. We focus on using local alignment to extract features from peptides and classifying them using Support Vector Machine. We also propose two approaches to optimize the features. The results show that our method can reliably predict epitopes and significantly outperforms some most commonly used tools.</p>
----------------------------------------------------------------------
In diva2:1879496 
abstract is: 
<p>Life insurance companies rely on mortality rate models to set appropriate premiums for their services. Over the past century, average life expectancy has increased and continues to do so, necessitating more accurate models. Two commonly used models are the Gompertz-Makeham law of mortality and the Lee-Carter model. The Gompertz-Makeham model depends solely on an age variable, while the Lee-Carter model incorporates a time-varying aspect which accounts for the increase in life expectancy over time. This paper constructs both models using training data acquired from Skandia Mutual Life Insurance Company and compares them to validation data from the same set. The study suggests that the Lee-Carter model may be able to offer some improvements compared to the Gompertz-Makeham law of mortality in terms of predicting future mortality rates. However, due to a lack of qualitative data, creating a competitive Lee-Carter model through Singular Value Decomposition, SVD, proved to be problematic. Switching from the current Gompertz-Makeham model to the Lee-Carter model should, therefore, be explored further when more high quality data becomes available.</p><p> </p>

corrected abstract:
<p>Life insurance companies rely on mortality rate models to set appropriate premiums for their services. Over the past century, average life expectancy has increased and continues to do so, necessitating more accurate models. Two commonly used models are the Gompertz-Makeham law of mortality and the Lee-Carter model. The Gompertz-Makeham model depends solely on an age variable, while the Lee-Carter model incorporates a time-varying aspect which accounts for the increase in life expectancy over time. This paper constructs both models using training data acquired from Skandia Mutual Life Insurance Company and compares them to validation data from the same set. The study suggests that the Lee-Carter model may be able to offer some improvements compared to the Gompertz-Makeham law of mortality in terms of predicting future mortality rates. However, due to a lack of qualitative data, creating a competitive Lee-Carter model through Singular Value Decomposition, SVD, proved to be problematic. Switching from the current Gompertz-Makeham model to the Lee-Carter model should, therefore, be explored further when more high quality data becomes available.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1800189 
abstract is: 
<p>In this master thesis, the accuracy of the crack depth meter RMG 4015 was evaluated for different types of cracks with various damage mechanisms. In total, 61 crack depth measurements were conducted with the crack depth meter on 56 cracks which were located in the 23 different test pieces supplied by Kiwa. The measured crack depths were then compared to the true crack depths, which were determined by cutting the test pieces and measuring directly on the cross-sections of the cracks using a light optical microscope. The results of the comparison showed that the RMG 4015, which uses potential drop techniques, was very accurate at measuring both strain induced and alkaline stress corrosion cracks. However, the results also showed that the crack depth meter underestimates chloride induced stress corrosion cracks, corrosion fatigue cracks and stress corrosion cracks/hydrogen embrittlement cracks at varying degrees. Therefore, the main recommendation for Kiwa is to switch the RMG 4015 to a crack depth meter that uses ultrasonic techniques instead.</p><p>The master thesis also explored the possibilities to improve an FE model produced by Kiwa in a previous project which involved an analysis of a cracked component. The present crack depth measure program included a test piece from this component. The stress distribution in the original model did not represent the cracks found in the real structure and it was suspected to be the result of some boundary conditions not corresponding to those acting in the actual pipe system. Some adjustments to the boundary conditions and contact regions were made and a new improved model with a better representing stress distribution was found.</p><p> </p>

corrected abstract:
<p>In this master thesis, the accuracy of the crack depth meter RMG 4015 was evaluated for different types of cracks with various damage mechanisms. In total, 61 crack depth measurements were conducted with the crack depth meter on 56 cracks which were located in the 23 different test pieces supplied by Kiwa. The measured crack depths were then compared to the true crack depths, which were determined by cutting the test pieces and measuring directly on the cross-sections of the cracks using a light optical microscope. The results of the comparison showed that the RMG 4015, which uses potential drop techniques, was very accurate at measuring both strain induced and alkaline stress corrosion cracks. However, the results also showed that the crack depth meter underestimates chloride induced stress corrosion cracks, corrosion fatigue cracks and stress corrosion cracks/hydrogen embrittlement cracks at varying degrees. Therefore, the main recommendation for Kiwa is to switch the RMG 4015 to a crack depth meter that uses ultrasonic techniques instead.</p><p>The master thesis also explored the possibilities to improve an FE model produced by Kiwa in a previous project which involved an analysis of a cracked component. The present crack depth measure program included a test piece from this component. The stress distribution in the original model did not represent the cracks found in the real structure and it was suspected to be the result of some boundary conditions not corresponding to those acting in the actual pipe system. Some adjustments to the boundary conditions and contact regions were made and a new improved model with a better representing stress distribution was found.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1111160 
abstract is: 
<p>Gaussian process methods are flexible non-parametric Bayesian methods used for regression and classification. They allow for explicit handling of uncertainty and are able to learn complex structures in the data. Their main limitation is their scaling characteristics: for n training points the complexity is <em>O</em>(n³) for training and <em>O</em>(n²) for prediction per test data point.</p><p>This makes full Gaussian process methods prohibitive to use on training sets larger than a few thousand data points. There has been recent research on approximation methods to make Gaussian processes scalable without severely affecting the performance. Some of these new approximation techniques are still not fully investigated and in a practical situation it is hard to know which method to choose. This thesis examines and evaluates scalable GP methods, especially focusing on the framework Massively Scalable Gaussian Processes introduced by Wilson et al. in 2016, which reduces the training complexity to nearly <em>O</em>(<em>n</em>) and the prediction complexity to <em>O</em>(1). The framework involves inducing point methods, local covariance function interpolation, exploitations of structured matrices and projections to low-dimensional spaces. The properties of the different approximations are studied and the possibilities of making improvements are discussed.</p><p> </p>

corrected abstract:
<p>Gaussian process methods are flexible non-parametric Bayesian methods used for regression and classification. They allow for explicit handling of uncertainty and are able to learn complex structures in the data. Their main limitation is their scaling characteristics: for 𝑛 training points the complexity is &Oscr;(𝑛<sup>3</sup>) for training and &Oscr;(𝑛<sup>2</sup>) for prediction per test data point. This makes full Gaussian process methods prohibitive to use on training sets larger than a few thousand data points.</p><p>There has been recent research on approximation methods to make Gaussian processes scalable without severely affecting the performance. Some of these new approximation techniques are still not fully investigated and in a practical situation it is hard to know which method to choose. This thesis examines and evaluates scalable GP methods, especially focusing on the framework Massively Scalable Gaussian Processes introduced by Wilson et al. in 2016, which reduces the training complexity to nearly &Oscr;(𝑛) and the prediction complexity to &Oscr;(1). The framework involves inducing point methods, local covariance function interpolation, exploitations of structured matrices and projections to low-dimensional spaces. The properties of the different approximations are studied and the possibilities of making improvements are discussed.</p>
----------------------------------------------------------------------
In diva2:1440824
Note: no full text in DiVA

abstract is: 
<p>This thesis focuses on the design and qualities of a gamma type Stirling engine from a thermodynamic point of view. The purpose is to calculate the efficiency of the gamma type Stirling engine, as well as the work output. An ideal thermodynamic Stirling cycle consists of an isothermal expansion, isochoric heat removal, isothermal compression, and lastly isochoric heat addition. An advantage with the Stirling engine is that it is able to work using any form of working gas and is described to work in a closed regenerative thermodynamic state. The gathered data from measuring the pressure and volume when energy is applied is used to calculate different values such as efficiency and net work from the engine. The data was collected by using different tools. A heat sensor was taped on the bottom plate to measure the temperature at a specific time. Secondly, a pressure sensor was connected to one of the six tubes on the top plate. Where the pressure tube was connected, varied in order to analyze if there was a difference in pressure at different distances from the center of the top plate. A photosensor was used to indicate when a full revolution had occurred so that the right data to represent a full cycle could be collected. The result was PV and Ts-diagrams for each pipe at different temperatures. The results indicated that there is a pressure difference of 800 Pa. By integrating these diagrams, the net work could be calculated. The highest measured net work was $0.32mJ$ through pipe 2 when the bottom plate has a temperature of 80°C. In conclusion, changing the placement of the pipe showed no remarkable differences, however, the theoretical efficiency increased with the temperature. The engine has more parts that can be analyzed such as the materialistic parameters and the relation in volume and temperature difference but are not taken into account in this thesis.</p><p> </p>

corrected abstract:
<p>This thesis focuses on the design and qualities of a gamma type Stirling engine from a thermodynamic point of view. The purpose is to calculate the efficiency of the gamma type Stirling engine, as well as the work output. An ideal thermodynamic Stirling cycle consists of an isothermal expansion, isochoric heat removal, isothermal compression, and lastly isochoric heat addition. An advantage with the Stirling engine is that it is able to work using any form of working gas and is described to work in a closed regenerative thermodynamic state. The gathered data from measuring the pressure and volume when energy is applied is used to calculate different values such as efficiency and net work from the engine. The data was collected by using different tools. A heat sensor was taped on the bottom plate to measure the temperature at a specific time. Secondly, a pressure sensor was connected to one of the six tubes on the top plate. Where the pressure tube was connected, varied in order to analyze if there was a difference in pressure at different distances from the center of the top plate. A photosensor was used to indicate when a full revolution had occurred so that the right data to represent a full cycle could be collected. The result was PV and Ts-diagrams for each pipe at different temperatures. The results indicated that there is a pressure difference of 800 Pa. By integrating these diagrams, the net work could be calculated. The highest measured net work was <em>0.32mJ</em> through pipe 2 when the bottom plate has a temperature of 80°C. In conclusion, changing the placement of the pipe showed no remarkable differences, however, the theoretical efficiency increased with the temperature. The engine has more parts that can be analyzed such as the materialistic parameters and the relation in volume and temperature difference but are not taken into account in this thesis.</p>

Note - only change to remove the empty paragraph and to set "0.32mJ" in italics (as it was an inline equation)
----------------------------------------------------------------------
In diva2:1342442 
abstract is: 
<p>In this report we demonstrate the usefulness of hidden Markov model estimation as a method to construct models of mouse behavior. We used a neural network to retrieve positional data of different body parts from overhead video recordings of lone mice in an enclosure. We then extracted features such as velocity and elongation from the positional data and used an implementation of the Baum-Welch algorithm to fit hidden Markov models to the feature data. We could identify recurring behaviors such as "running next to wall" and "investigating wall" among the estimated states in several different mice, which was consistent with what we could see in the actual videos. We thereby demonstrate that hidden Markov model estimation by the Baum-Welch algorithm can be utilized to automatically find models of mouse behavior.</p><p> </p>

corrected abstract:
<p>In this report we demonstrate the usefulness of hidden Markov model estimation as a method to construct models of mouse behavior. We used a neural network to retrieve positional data of different body parts from overhead video recordings of lone mice in an enclosure. We then extracted features such as velocity and elongation from the positional data and used an implementation of the Baum-Welch algorithm to fit hidden Markov models to the feature data. We could identify recurring behaviors such as ”running next to wall” and ”investigating wall” among the estimated states in several different mice, which was consistent with what we could see in the actual videos. We thereby demonstrate that hidden Markov model estimation by the Baum-Welch algorithm can be utilized to automatically find models of mouse behavior.</p>

Note - only change to remove the empty paragraph and fixed double quotes to match the original
----------------------------------------------------------------------
In diva2:1781495 
abstract is: 
<p>Capacitive Deionization (CDI) is an energy-efficient desalination technology that utilizes an electric field to extract ions from water. Flow-through CDI systems show potential for superior desalination performance compared to traditional flow-by CDI; however, they face the challenge of increased occurrence of Faradaic reactions, leading to undesired by-products and reduced energy efficiency. In this study, we constructed a flow-through CDI cell and investigated the desalination performance of the two possible cell configurations: upstream anode mode and downstream anode mode. A series of experiments were conducted, measuring conductivity and pH of the effluent solution during charging and discharging phases. The results were analyzed in terms of salt adsorption capacity and charge efficiency. We used pH fluctuations in the effluent solution as indicators of Faradaic reactions. It was found that upstream anode mode yielded superior desalination, with a salt adsorption capacity of 6.79 mg/g and charge efficiency of 64.3%, compared to downstream anode mode, which displayed a salt adsorption capacity of 5.19 mg/g and charge efficiency of 50.8%. However, upstream anode mode also produced more pronounced pH oscillations, suggesting a higher occurrence of Faradaic reactions. Reconciling these conflicting results and shedding light on the complex processes within the CDI cell calls for further investigation.</p><p> </p>

corrected abstract:
<p>Capacitive Deionization (CDI) is an energy-efficient desalination technology that utilizes an electric field to extract ions from water. Flow-through CDI systems show potential for superior desalination performance compared to traditional flow-by CDI; however, they face the challenge of increased occurrence of Faradaic reactions, leading to undesired by-products and reduced energy efficiency. In this study, we constructed a flow-through CDI cell and investigated the desalination performance of the two possible cell configurations: upstream anode mode and downstream anode mode. A series of experiments were conducted, measuring conductivity and pH of the effluent solution during charging and discharging phases. The results were analyzed in terms of salt adsorption capacity and charge efficiency. We used pH fluctuations in the effluent solution as indicators of Faradaic reactions. It was found that upstream anode mode yielded superior desalination, with a salt adsorption capacity of 6.79 mg g<sup>-1</sup> and charge efficiency of 64.3%, compared to downstream anode mode, which displayed a salt adsorption capacity of 5.19 mg g<sup>-1</sup> and charge efficiency of 50.8%. However, upstream anode mode also produced more pronounced pH oscillations, suggesting a higher occurrence of Faradaic reactions. Reconciling these conflicting results and shedding light on the complex processes within the CDI cell calls for further investigation.</p>

Note - only change to remove the empty paragraph and change the "/g" into "g<sup>-1</sup>" as in the original
----------------------------------------------------------------------
In diva2:1354140 
Note: no full text in DiVA

abstract is: 
<p>In this project, the fatigue behaviour of aluminium (AL 5083 H111) gusset and flange joined with a fillet weld, is investigated through experiments and numerical methods. The work aims at improved knowledge on fatigue in an aluminium welded joint subjected to constant amplitude varying load.</p><p> </p><p>The results of the experiments are investigated with the Basquin equation. The mean curve is estimated by the maximum likelihood estimation (MLE) by using both the failed specimen data and the run-out data. From the mean curve, a component specific design curve is estimated. Comparison of the component specific design curve with the recommendation’s design curve highlighted the inherent conservatism of the recommendation’s design curve.</p><p> </p><p>Nominal, hot-spot and equivalent notch methods were evaluated, and a comparative study was performed. The numerical investigation showed that the predicted fatigue life increased with model complexity. Comparison to the experimentally derived component specific design curve highlighted non-conservatism of the numerically predicted fatigue life for large stress ranges. The degree of conservatism of the numerical methods is however strongly affected by the slope of the considered design curve.</p>

corrected abstract:
<p>In this project, the fatigue behaviour of aluminium (AL 5083 H111) gusset and flange joined with a fillet weld, is investigated through experiments and numerical methods. The work aims at improved knowledge on fatigue in an aluminium welded joint subjected to constant amplitude varying load.</p><p>The results of the experiments are investigated with the Basquin equation. The mean curve is estimated by the maximum likelihood estimation (MLE) by using both the failed specimen data and the run-out data. From the mean curve, a component specific design curve is estimated. Comparison of the component specific design curve with the recommendation’s design curve highlighted the inherent conservatism of the recommendation’s design curve.</p><p>Nominal, hot-spot and equivalent notch methods were evaluated, and a comparative study was performed. The numerical investigation showed that the predicted fatigue life increased with model complexity. Comparison to the experimentally derived component specific design curve highlighted non-conservatism of the numerically predicted fatigue life for large stress ranges. The degree of conservatism of the numerical methods is however strongly affected by the slope of the considered design curve.</p>


Note - only change to remove the empty paragraphs
----------------------------------------------------------------------
In diva2:1436832 
abstract is: 
<p>In this report, we first briefly summarize Hermitian quantum mechanics before moving on to the non-Hermitian case. We then review PT-symmetric quantum mechanics with a focus on finite-dimensional systems, and include a novel generalization of a perturbative calculation of the C-operator. After briefly covering the basics of neutrino oscillations, we perturbatively examine a PT-symmetric addition to the neutrino oscillation Hamiltonian. We examine the effects of the addition with two different definitions of transition probabilities. However, probability is not conserved to first order with either definition. Further, we note that the effect of the chosen perturbation is to shift the transition probabilities by some phase, and to change the amplitudes of the transition probabilities.</p><p> </p>

corrected abstract:
<p>In this report, we first briefly summarize Hermitian quantum mechanics before moving on to the non-Hermitian case. We then review &Pscr;&Tscr;-symmetric quantum mechanics with a focus on finite-dimensional systems, and include a novel generalization of a perturbative calculation of the &Cscr;-operator. After briefly covering the basics of neutrino oscillations, we perturbatively examine a &Pscr;&Tscr;-symmetric addition to the neutrino oscillation Hamiltonian. We examine the effects of the addition with two different definitions of transition probabilities. However, probability is not conserved to first order with either definition. Further, we note that the effect of the chosen perturbation is to shift the transition probabilities by some phase, and to change the amplitudes of the transition probabilities.</p>

Note - removed the empty paragraph and corrected the script characters
----------------------------------------------------------------------
In diva2:1878884 
abstract is: 
<p>Reinforcement learning (RL) algorithms aim to identify optimal action sequences for an agent in a given environment, traditionally maximizing the expected rewards received from the environment by taking each action and transitioning between states. This thesis explores approaching RL distributionally, replacing the expected reward function by the full distribution over the possible rewards received, known as the value distribution. We focus on the quantile regression distributional RL (QR-DQN) algorithm introduced by Dabney et al. (2017), which models the value distribution by representing its quantiles. With such information of the value distribution, we modify the QR-DQN algorithm to enhance the agent's risk sensitivity. Our risk-averse algorithm is evaluated against the original QR-DQN in the Atari 2600 and in the Gymnasium environment, specifically in the games Breakout, Pong, Lunar Lander and Cartpole. Results indicate that the risk-averse variant performs comparably in terms of rewards while exhibiting increased robustness and risk aversion. Potential refinements of the risk-averse algorithm are presented.</p><p> </p>

corrected abstract:
<p>Reinforcement learning (RL) algorithms aim to identify optimal action sequences for an agent in a given environment, traditionally maximizing the expected rewards received from the environment by taking each action and transitioning between states. This thesis explores approaching RL distributionally, replacing the expected reward function by the full distribution over the possible rewards received, known as the value distribution. We focus on the quantile regression distributional RL (QR-DQN) algorithm introduced by Dabney et al. (2017), which models the value distribution by representing its quantiles. With such information of the value distribution, we modify the QR-DQN algorithm to enhance the agent's risk sensitivity. Our risk-averse algorithm is evaluated against the original QR-DQN in the Atari 2600 and in the Gymnasium environment, specifically in the games Breakout, Pong, Lunar Lander and Cartpole. Results indicate that the risk-averse variant performs comparably in terms of rewards while exhibiting increased robustness and risk aversion. Potential refinements of the risk-averse algorithm are presented.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:735921 
abstract is: 
<p>This paper presents PDEs that describes sedimentation by a system of diffusion and transportation equations. These PDEs are implemented with a semi-implicit scheme and solved on a Graphics Processing Unit (GPU). The equations are solved with the iterative solvers (conjugate gradient and biconjugate gradient stabilized method) provided by the software ViennaCL. The timings from these operations are compared with a CPU implementation.</p><p>Before using the iterative solvers, a sparse matrix and a right hand side vector is set. The sparse matrix and the right hand side vector are efficiently updated on the GPU. The implicit terms of the PDEs are stored in the sparse matrix and the explicit terms in the right hand side vector. The sparse matrix is stored in the compressed sparse row (CSR) format. Algorithms to update the sparse matrix for the PDEs, which have Neumann or a mix of Neumann and Dirichlet boundary conditions, are presented. As the values in the sparse matrix depend on values from the previous results, the sparse matrix has to be updated frequently. Considerable time is saved by updating the sparse matrix on the GPU instead of on the CPU (slow data transfers between CPU and GPU are reduced).</p><p>The speedup for the GPU implementation was found to be 8-10 and 12-18 for the GPUs GTX 590 and K20m respectively, depending on grid size. The high speedup is due to the CPU model of the CPUs used for timings being an older model. If a newer CPU model were used, the speedup would be lower. Due to limited access to newer hardware, a more accurate value for speedup comparison has not been acquired. Indications still prove that the GPU implementation is faster than the sequential CPU implementation.</p><p> </p>

corrected abstract:
<p>This paper presents PDEs that describes sedimentation by a system of diffusion and transportation equations. These PDEs are implemented with a semi-implicit scheme and solved on a Graphics Processing Unit (GPU). The equations are solved with the iterative solvers (conjugate gradient and biconjugate gradient stabilized method) provided by the software ViennaCL. The timings from these operations are compared with a CPU implementation.</p><p>Before using the iterative solvers, a sparse matrix and a right hand side vector is set. The sparse matrix and the right hand side vector are efficiently updated on the GPU. The implicit terms of the PDEs are stored in the sparse matrix and the explicit terms in the right hand side vector. The sparse matrix is stored in the compressed sparse row (CSR) format. Algorithms to update the sparse matrix for the PDEs, which have Neumann or a mix of Neumann and Dirichlet boundary conditions, are presented. As the values in the sparse matrix depend on values from the previous results, the sparse matrix has to be updated frequently. Considerable time is saved by updating the sparse matrix on the GPU instead of on the CPU (slow data transfers between CPU and GPU are reduced).</p><p>The speedup for the GPU implementation was found to be 8-10 and 12-18 for the GPUs GTX 590 and K20m respectively, depending on grid size. The high speedup is due to the CPU model of the CPUs used for timings being an older model. If a newer CPU model were used, the speedup would be lower. Due to limited access to newer hardware, a more accurate value for speedup comparison has not been acquired. Indications still prove that the GPU implementation is faster than the sequential CPU implementation.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1350076 
abstract is: 
<p>In this report we investigate the exotic hadrons known as pentaquarks. A brief overview of relevant concepts and theory is initially presented in order to aid the reader. Thereafter, the history of this field with regards to theory and experiments is discussed. In particular, a group theoretic classification of these states is studied. A simple mass formula for pentaquark states is examined and predictions are subsequently made about the composition and mass of possible pentaquark states. Furthermore, this mass formula is modified to examine and predict additional pentaquark states. A number of numerical fits concerning the masses of pentaquarks are performed and studied. Future research is explored with regards to the information presented in this thesis.</p><p> </p>

corrected abstract:
<p>In this report we investigate the exotic hadrons known as pentaquarks. A brief overview of relevant concepts and theory is initially presented in order to aid the reader. Thereafter, the history of this field with regards to theory and experiments is discussed. In particular, a group theoretic classification of these states is studied. A simple mass formula for pentaquark states is examined and predictions are subsequently made about the composition and mass of possible pentaquark states. Furthermore, this mass formula is modified to examine and predict additional pentaquark states. A number of numerical fits concerning the masses of pentaquarks are performd and studied. Future research is explored with regards to the information presented in this thesis.</p>

Note - only change to remove the empty paragraph and replacing "performed" with "performd" - error in the original
----------------------------------------------------------------------
In diva2:1873671 
abstract is: 
<p>In this report, we present a novel Bayesian inference framework to reconstruct the three-dimensional initial conditions of cosmic structure formation from data. To achieve this goal, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. According to the cosmological paradigm, all observable structures were formed from tiny primordial quantum fluctuations generated during the early stages of the Universe. As time passed, these seed fluctuations grew via gravitational aggregation to form the presently observed cosmic web traced by galaxies. For this reason, the specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we develop a novel machine learning approach that leverages the hierarchical nature of structure formation. We demonstrate our method in a mock analysis and find that we can recover the initial conditions with high accuracy, showing the potential of our model.</p><p> </p>

corrected abstract:
<p>In this report, we present a novel Bayesian inference framework to reconstruct the three-dimensional initial conditions of cosmic structure formation from data. To achieve this goal, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. According to the cosmological paradigm, all observable structures were formed from tiny primordial quantum fluctuations generated during the early stages of the Universe. As time passed, these seed fluctuations grew via gravitational aggregation to form the presently observed cosmic web traced by galaxies. For this reason, the specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we develop a novel machine learning approach that leverages the hierarchical nature of structure formation. We demonstrate our method in a mock analysis and find that we can recover the initial conditions with high accuracy, showing the potential of our model.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1342319 
abstract is: 
<p>To balance and control different aircraft, it is often necessary to use some type of control system, the less stable the craft is without any control system, the more advanced the system is required to be. This project is an experiment which purpose is to attempt to control a very unstable craft using steerable rudders. A design of the craft is modeled in CAD after a rough estimation to determine the required capacity of the components. Then a simulation of the craft is modeled in Matlab’s Simulink environment, which is used to test the control system’s capabilities and determine its optimal settings. Finally a physical model is built to see if the control system is sufficiently designed to stabilize the vessel under real conditions, which when flown did not successfully balance due to insufficient roll capability. Different solutions to this problem and other potential improvements is then discussed.</p><p> </p>

corrected abstract:
<p>To balance and control different aircraft it is often necessary to use some type of control system, the less stable the craft is without any control system, the more advanced the system is required to be. This project is an experiment which goal is to attempt to control a very unstable craft using steerable rudders. A design of the craft is modeled in CAD after a rough estimation to determine the required capacity of the components. Then a simulation of the craft is modeled in Matlab’s Simulink environment which is used to test the control system’s capabilities and determine its optimal settings, the simulation was considered successful because it showed that the craft could balance. Finally a physical model is built to test if the control system is sufficient to stabilize the vessel under real world conditions. When the real aircraft was flown it did not successfully balance due to insufficient roll capability caused by the motor torque being larger than expected. Different solutions to this problem and other potential improvements is then discussed.</p>

Note many changes in the wording betweeen the DiVA and original text, also removed the unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:1450318 
abstract is: 
<p>Sheet-swept connection solutions are a method for joining structural pipes in aircraft constructions. It is a simple approach that avoids the extensive demands placed on, among other things, welded connections. Previously, calculation data were not available, which this study aims to meet in the form of specifications and comparisons. Graham Lee's drawings and design specifications for the replica variant of the Nieuport 12 aircraft have been followed in the analysis of the connection solution. The method is based on three parts, calculations, experimental testing and FEM analysis. These form a specification of the joint's four most central load cases and their strength: tension (1130 N), plane deflection (4.5 Nm), in plane deflection (17.5 Nm) and torsion (4.5 Nm). The results are compared with calculations of loads in the pendulum rudder to determine how well the connection solution is suitable for this purpose. Identified load cases in the rudder are: plane deflection (3.2 Nm) and torsion (≤3.2 Nm). The results indicate that this type of connection is weak in the proposed purpose. The analysis highlights clear weaknesses in the connection solution and recommendations for improvements are given, these are primarily aimed at the thickness of the gusset and the introduction of an additional rivet. Furthermore, the sheet-swept joint solution is compared with welded joints which prove to be more suitable for this application.</p><p> </p>

corrected abstract:
<p>Sheet-swept connection solutions are a method for joining structural pipes in aircraft constructions. It is a simple approach that avoids the extensive demands placed on, among other things, welded connections. Previously, calculation data were not available, which this study aims to meet in the form of specifications and comparisons. Graham Lee's drawings and design specifications for the replica variant of the Nieuport 12 aircraft have been followed in the analysis of the connection solution. The method is based on three parts, calculations, experimental testing and FEM analysis. These form a specification of the joint's four most central load cases and their strength: tension (1130 N), plane deflection (4.5 Nm), in plane deflection (17.5 Nm) and torsion (4.5 Nm). The results are compared with calculations of loads in the pendulum rudder to determine how well the connection solution is suitable for this purpose. Identified load cases in the rudder are: plane deflection (3.2 Nm) and torsion (≤3.2 Nm). The results indicate that this type of connection is weak in the proposed purpose. The analysis highlights clear weaknesses in the connection solution and recommendations for improvements are given, these are primarily aimed at the thickness of the gusset and the introduction of an additional rivet. Furthermore, the sheet-swept joint solution is compared with welded joints which prove to be more suitable for this application.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1441692 
abstract is: 
<p>This report is part of a bachelor’s degree project in solid mechanics at KTH, Stockholm. It is performed by two students on behalf of the social enterprise Better Shelter, which provides shelters for people displaced by war and natural disasters. The scope of the project is to expand Better Shelters product specifications by providing improvements of the shelter earth anchors. This would allow use of the shelter in areas affected by higher wind speeds and thereby help more people in need of temporary housing and shelters. The earth anchors prevent the shelter from uplifting and tilting by taking uplift forces when horizontal wind loads acts on the structure. Two wind models with wind speeds up to 36 m/s were created to find the reaction forces on the anchors resisting the wind load. The wind models were compared with each other to validate the results and find the largest reaction forces on the anchors. Simulations of the anchors were made to analyse occurring stresses due to wind loads. Redesigns of the current earth anchor were made to find improvements of the anchor shape and reduce the stresses on the anchor. Experiments were then performed to analyse the redesigned anchor shapes in practise. The redesign, calculations and computational analyses of the anchors were done using the programs SolidEdge, ANSYS and Matlab. Results showed that redesigning the anchor contact area with the anchor wire reduced the stresses on the anchors drastically. Increasing the wing size of the anchors proved to be successful for preventing anchors from being pulled out of the soil. This allows better use of the current anchor material volume. Experiments also proved that burying the anchor deeper into the soil is an effective way of increasing the resistance from being pulled out of the ground. By reducing the stresses on the anchor, more materials are available for use. This could be explored further and is a suggested as a continuation of this project. The current anchor material is aluminium, and most aluminium alloys can be used with the redesigned ball joint connection to the anchor wire even when wind forces are large.</p><p> </p>

corrected abstract:
<p>This report is part of a bachelor’s degree project in solid mechanics at KTH, Stockholm. It is performed by two students on behalf of the social enterprise Better Shelter, which provides shelters for people displaced by war and natural disasters. The scope of the project is to expand Better Shelters product specifications by providing improvements of the shelter earth anchors. This would allow use of the shelter in areas affected by higher wind speeds and thereby help more people in need of temporary housing and shelters.</p><p>The earth anchors prevent the shelter from uplifting and tilting by taking uplift forces when horizontal wind loads acts on the structure. Two wind models with wind speeds up to 36 𝑚/𝑠 were created to find the reaction forces on the anchors resisting the wind load. The wind models were compared with each other to validate the results and find the largest reaction forces on the anchors. Simulations of the anchors were made to analyse occurring stresses due to wind loads.</p><p>Redesigns of the current earth anchor were made to find improvements of the anchor shape and reduce the stresses on the anchor. Experiments were then performed to analyse the redesigned anchor shapes in practise. The redesign, calculations and computational analyses of the anchors were done using the programs SolidEdge, ANSYS and Matlab.</p><p>Results showed that redesigning the anchor contact area with the anchor wire reduced the stresses on the anchors drastically. Increasing the wing size of the anchors proved to be successful for preventing anchors from being pulled out of the soil. This allows better use of the current anchor material volume. Experiments also proved that burying the anchor deeper into the soil is an effective way of increasing the resistance from being pulled out of the ground.</p><p>By reducing the stresses on the anchor, more materials are available for use. This could be explored further and is a suggested as a continuation of this project. The current anchor material is aluminium, and most aluminium alloys can be used with the redesigned ball joint connection to the anchor wire even when wind forces are large.</p>
----------------------------------------------------------------------
In diva2:1876745 
abstract is: 
<p>The Thermo-Calc software is a key tool in the research process for many material engineers. However, integrating multiple modules in Thermo-Calc requires the user to write code in a Python-based language, which can be challenging for novice programmers. This project aims to enable the generation of such code from user prompts by using existing generative AI models. In particular, we use a retrieval-augmented generation architecture applied to LLaMA and Mistral models. We use Code LLaMA-Instruct models with 7, 13, and 34 billion parameters, and a Mistral-Instruct model with 7 billion parameters. These models are all based on LLaMA 2. We also use a LLaMA 3-Instruct model with 8 billion parameters. All these models are instruction-tuned, which suggests that they have the capability to interpret natural language and identify appropriate options for a command-line program such as Python. In our testing, the LLaMA 3-Instruct model performed best, achieving 53% on the industry benchmark HumanEval and 49% on our internal adequacy assessment at pass@1, which is the expected probability of getting a correct solution when generating a response. This indicates that the model generates approximately every other answer correct. Due to GPU memory limitations, we had to apply quantisation to process the 13 and 34 billion parameter models. Our results revealed a mismatch between model size and optimal levels of quantisation, indicating that reduced precision adversely affects the performance of these models. Our findings suggest that a properly customised large language model can greatly reduce the coding effort of novice programmers, thereby improving productivity in material research.</p><p> </p>

corrected abstract:
<p>The Thermo-Calc software is a key tool in the research process for many material engineers. However, integrating multiple modules in Thermo-Calc requires the user to write code in a Python-based language, which can be challenging for novice programmers. This project aims to enable the generation of such code from user prompts by using existing generative AI models. In particular, we use a retrieval-augmented generation architecture applied to LLaMA and Mistral models. We use Code LLaMA-Instruct models with 7, 13, and 34 billion parameters, and a Mistral-Instruct model with 7 billion parameters. These models are all based on LLaMA 2. We also use a LLaMA 3-Instruct model with 8 billion parameters. All these models are instruction-tuned, which suggests that they have the capability to interpret natural language and identify appropriate options for a command-line program such as Python. In our testing, the LLaMA 3-Instruct model performed best, achieving 53% on the industry benchmark HumanEval and 49% on our internal adequacy assessment at pass@1, which is the expected probability of getting a correct solution when generating a response. This indicates that the model generates approximately every other answer correct. Due to GPU memory limitations, we had to apply quantisation to process the 13 and 34 billion parameter models. Our results revealed a mismatch between model size and optimal levels of quantisation, indicating that reduced precision adversely affects the performance of these models. Our findings suggest that a properly customised large language model can greatly reduce the coding effort of novice programmers, thereby improving productivity in material research.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1285800 
abstract is: 
<p> As the challenge grows in the vehicle industry, tightening the margins on financial and environmental costs of the vehicle development, computer aided engineering becomes more and more attractive. Extensive work is being invested in creating detailed models that can replicate vehicle behaviour accurately and efficiently. The work in this thesis starts with studying objective and subjective evaluations of vehicles as well as their counterparts in vehicle models and a simulator environment. Then, it continues to locate the weaknesses in the models, and investigate the possible improvements. The first part of the thesis focused on performing a literature study concerning the objective metrics and their use in the vehicle industry, as well as the use of simulators. This served as a foundation for the use of objective metrics in the validation of the CarRealTime models. The tools used in the thesis were also introduced. The work continued with the study of previously collected data concerning vehicle evaluation through subjective assessment and objective metrics, with different anti-roll bar configurations, to build trust in the ability of the drivers in evaluating these criteria. Similar data from the CarRealTime models and the simulator were also studied. The aim was to evaluate the simulator driving experience accuracy through the subjective assessment. The weaknesses of the model were identified, and an improved steering model was introduced, replacing the old lookup tables with a Pfeffer model from CarRealTime combined with the steering assist unit in Simulink. An extensive parameter study was performed to understand the effect of selected parameters on the driving experience. Using the same model, the simulator delays were studied in terms of replicating yaw and lateral movements, and how this can affect the driver’s perception of the driving experience. Finally, the results from the parameter study were used to assign the weight parameters in the optimization objective function where the goal was to study the possibility of improving the accuracy of the driving experience as well as counteracting the effects of simulator delays. The Matlab Optimization Toolkit was used in the process. As a conclusion, it was shown that the subjective assessment together with the objective metrics played a crucial role in identifying model and simulator weaknesses. The parameter study showed promising opportunities in solving the aforementioned issues, with the optimization tool and boundaries needing more elaborate work to reach conclusive results.</p><p> </p>

corrected abstract:
<p> As the challenge grows in the vehicle industry, tightening the margins on financial and environmental costs of the vehicle development, computer aided engineering becomes more and more attractive. Extensive work is being invested in creating detailed models that can replicate vehicle behaviour accurately and efficiently. The work in this thesis starts with studying objective and subjective evaluations of vehicles as well as their counterparts in vehicle models and a simulator environment. Then, it continues to locate the weaknesses in the models, and investigate the possible improvements. The first part of the thesis focused on performing a literature study concerning the objective metrics and their use in the vehicle industry, as well as the use of simulators. This served as a foundation for the use of objective metrics in the validation of the CarRealTime models. The tools used in the thesis were also introduced. The work continued with the study of previously collected data concerning vehicle evaluation through subjective assessment and objective metrics, with different anti-roll bar configurations, to build trust in the ability of the drivers in evaluating these criteria. Similar data from the CarRealTime models and the simulator were also studied. The aim was to evaluate the simulator driving experience accuracy through the subjective assessment. The weaknesses of the model were identified, and an improved steering model was introduced, replacing the old lookup tables with a Pfeffer model from CarRealTime combined with the steering assist unit in Simulink. An extensive parameter study was performed to understand the effect of selected parameters on the driving experience. Using the same model, the simulator delays were studied in terms of replicating yaw and lateral movements, and how this can affect the driver’s perception of the driving experience. Finally, the results from the parameter study were used to assign the weight parameters in the optimization objective function where the goal was to study the possibility of improving the accuracy of the driving experience as well as counteracting the effects of simulator delays. The Matlab Optimization Toolkit was used in the process. As a conclusion, it was shown that the subjective assessment together with the objective metrics played a crucial role in identifying model and simulator weaknesses. The parameter study showed promising opportunities in solving the aforementioned issues, with the optimization tool and boundaries needing more elaborate work to reach conclusive results.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:741548 
abstract is: 
<p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p><p> </p>
skipping mc='isa'

partal corrected: diva2:741548: <p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p><p> </p>

corrected abstract:
<p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1781509 
Note: no full text in DiVA

abstract is: 
<p>The objective of this report is to explore and assess the potential of two modified designs derived from a perfectly circular nested tubes NANF configuration. By altering the curvature profile and adopting a rounded triangular shape, various parameters are systematically varied to evaluate their impact on the confinement loss properties within the operating range around 850 nm. By leveraging geometric optics, an attempt is made to increase the angle of incidence of light on the tubes in order to maximize reflection. Through extensive simulations, the behavior of the modified designs is analyzed and compared against the literature-based NANF-B. The simulation results indicate that the simulated NANF-B exhibits a skewed wavelength range towards lower wavelengths compared to the literature version probably because of slight differences in how the model was designed. However, the proposed alterations demonstrate significantly reduced bandwidth and only show agreement with NANF-B within the lower wavelength range. Furthermore, the simulations reveal that the points where the tubes connect to the outer cladding play a critical role in the overall loss characteristics at longer wavelengths. This finding suggests that designs incorporating inward-curving tubes towards the outer cladding offer improved performance throughout the anti-resonant window where propagation is feasible.</p><p> </p>

corrected abstract:
<p>The objective of this report is to explore and assess the potential of two modified designs derived from a perfectly circular nested tubes NANF configuration. By altering the curvature profile and adopting a rounded triangular shape, various parameters are systematically varied to evaluate their impact on the confinement loss properties within the operating range around 850 nm. By leveraging geometric optics, an attempt is made to increase the angle of incidence of light on the tubes in order to maximize reflection. Through extensive simulations, the behavior of the modified designs is analyzed and compared against the literature-based NANF-B. The simulation results indicate that the simulated NANF-B exhibits a skewed wavelength range towards lower wavelengths compared to the literature version probably because of slight differences in how the model was designed. However, the proposed alterations demonstrate significantly reduced bandwidth and only show agreement with NANF-B within the lower wavelength range. Furthermore, the simulations reveal that the points where the tubes connect to the outer cladding play a critical role in the overall loss characteristics at longer wavelengths. This finding suggests that designs incorporating inward-curving tubes towards the outer cladding offer improved performance throughout the anti-resonant window where propagation is feasible.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1865115 
abstract is: 
<p>This thesis deals with Large Eddy Simulations (LES) of the Atmospheric Boundary Layer (ABL), focusing on studying the resolution dependence of turbulent passive scalar transport within the layer. The ABL is the lowest part of the atmosphere, where humans live and conduct most of their daily activities. Here, a scalar was injected at four different heights in a mixed shear- and convective-driven ABL, which was simulated using the Spectral Element Method (SEM) code Nek5000. The statistics of the four scalars were analysed and their resolution dependence was studied and compared to that of non-scalar quantities. No significant resolution dependence was found with regards to non-scalar quantities, while scalar quantities show a rather strong dependence on resolution especially in the first quarter of the simulation. Negative concentration values are found within the layer and some approaches to solve the problem are proposed. Statistics alone provide an accurate description of the general ABL behaviour, but are found to be insufficient to capture the dynamics of the scalar injection, which ought to be analysed with more advanced methods (e.g. modal decomposition). The structures arising within the layer are also analysed, and further work regarding the study of scalar fronts is suggested.</p><p> </p>

corrected abstract:
<p>This thesis deals with Large Eddy Simulations (LES) of the Atmospheric Boundary Layer (ABL), focusing on studying the resolution dependence of turbulent passive scalar transport within the layer. The ABL is the lowest part of the atmosphere, where humans live and conduct most of their daily activities. Here, a scalar was injected at four different heights in a mixed shear- and convective-driven ABL, which was simulated using the Spectral Element Method (SEM) code Nek5000. The statistics of the four scalars were analysed and their resolution dependence was studied and compared to that of non-scalar quantities. No significant resolution dependence was found with regards to non-scalar quantities, while scalar quantities show a rather strong dependence on resolution especially in the first quarter of the simulation. Negative concentration values are found within the layer and some approaches to solve the problem are proposed. Statistics alone provide an accurate description of the general ABL behaviour, but are found to be insufficient to capture the dynamics of the scalar injection, which ought to be analysed with more advanced methods (e.g. modal decomposition). The structures arising within the layer are also analysed, and further work regarding the study of scalar fronts is suggested.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1197616 
abstract is: 
<p>The use of turbocharged Diesel engines is nowadays a widespread practice in the automotive sector: heavy-duty vehicles like trucks or buses, in particular, are often equipped with turbocharged engines. An accurate study of the flow field developing inside both the main components of a turbocharger, i.e. compressor and turbine, is therefore necessary: the synergistic use of CFD simulations and experimental tests allows to fulfill this requirement.</p><p>The aim of this thesis is to investigate the performance and the flow field that develops inside a centrifugal compressor for automotive turbochargers. The study is carried out by means of numerical simulations, both steady-state and transient, based on RANS models (Reynolds Averaged Navier-Stokes equations). The code utilized for the numerical simulations is Ansys CFX.</p><p> </p><p>The first part of the work is an engineering attempt to develop a CFD method for predicting the performance of a centrifugal compressor which is based solely on steady-state RANS models. The results obtained are then compared with experimental observations. The study continues with an analysis of the sensitivity of the developed CFD method to different parameters: influence of both position and model used for the rotor-stator interfaces and the axial tip-clearance on the global performances is studied and quantified.</p><p> </p><p>In the second part, a design optimization study based on the Design of Experiments (DoE) approach is performed. In detail, transient RANS simulations are used to identify which geometry of the recirculation cavity hollowed inside the compressor shroud (ported shroud design) allows to mitigate the backflow that appears at low mass-flow rates. Backflow can be observed when the operational point of the compressor is suddenly moved from design to surge conditions. On actual heavy-duty vehicles, these conditions may arise when a rapid gear shift is performed.</p>

corrected abstract:
<p>The use of turbocharged Diesel engines is nowadays a widespread practice in the automotive sector: heavy-duty vehicles like trucks or buses, in particular, are often equipped with turbocharged engines. An accurate study of the flow field developing inside both the main components of a turbocharger, i.e. compressor and turbine, is therefore necessary: the synergistic use of CFD simulations and experimental tests allows to fulfill this requirement.</p><p>The aim of this thesis is to investigate the performance and the flow field that develops inside a centrifugal compressor for automotive turbochargers. The study is carried out by means of numerical simulations, both steady-state and transient, based on RANS models (Reynolds Averaged Navier-Stokes equations). The code utilized for the numerical simulations is Ansys CFX.</p><p>The first part of the work is an engineering attempt to develop a CFD method for predicting the performance of a centrifugal compressor which is based solely on steady-state RANS models. The results obtained are then compared with experimental observations. The study continues with an analysis of the sensitivity of the developed CFD method to different parameters: influence of both position and model used for the rotor-stator interfaces and the axial tip-clearance on the global performances is studied and quantified.</p><p>In the second part, a design optimization study based on the Design of Experiments (DoE) approach is performed. In detail, transient RANS simulations are used to identify which geometry of the recirculation cavity hollowed inside the compressor shroud (ported shroud design) allows to mitigate the backflow that appears at low mass-flow rates. Backflow can be observed when the operational point of the compressor is suddenly moved from design to surge conditions. On actual heavy-duty vehicles, these conditions may arise when a rapid gear shift is performed.</p>


Note - only change to remove the empty paragraphs
----------------------------------------------------------------------
In diva2:1341272 
abstract is: 
<p>In the autumn 2018 political elections were held in Sweden and consequently it is interesting to investigate what can affect how people vote. The purpose with this report is investigating if there are correspondences between the characteristics of a municipality and how the people in that municipality voted in the general election. Clustering on data sets with municipality characteristics and municipality general election statistics from 2018 is the basis of this study. K-means clustering and hierarchical clustering are the clustering methods that are used. In the report results of the clustering and the construction of a method for comparing clusterings are presented. The results show that there are some correspondences but that clustering is not the optimal method for analysing this data set.</p><p> </p>

corrected abstract:
<p>In the autumn 2018 political elections were held in Sweden and consequently it is interesting to investigate what can affect how people vote. The purpose with this report is investigating if there are correspondences between the characteristics of a municipality and how the people in that municipality voted in the general election. Clustering on data sets with municipality characteristics and municipality general election statistics from 2018 is the basis of this study. 𝐾-means clustering and hierarchical clustering are the clustering methods that are used. In the report results of the clustering and the construction of a method for comparing clusterings are presented. The results show that there are some correspondences but that clustering is not the optimal method for analysing this data set.</p>

Note - only change to remove the empty paragraph and replace "K" by "𝐾".
----------------------------------------------------------------------
In diva2:1342347 
abstract is: 
<p>The current trend in the automotive industry towards more fuel efficient vehicles requires all components to be as light as possible while still meeting other demands such as stiffness and feasible cost. The purpose of this study was to investigate the possibility to replace a partial chassis structure in a Scania low-entry city bus. The partial chassis to be replaced consists of a steel structure and an inner flooring, with the purpose to support loads that the bus is subject to. This was to be done with a composite sandwich structure, with the primary goal to reduce weight by at least 40% and number of components by 50%. The replacement structure needed to meet the stiffness and strength requirements that the current structure fulfils. This was achieved by designing two concepts, concept 1 and concept 2, through an iterative FE-analysis in ANSYS. Two prototypes where built and tested for real world load applications. The result from this study showed that it was possible to meet both the weight and component reduction goal. Concept 1 and concept 2 achieved a weight reduction of 62% and 68% respectively and the number of components was reduced significantly. Further work would be to investigate the interface between the new structure and the rest of the bus, modal- and fatigue analyses, production implementation and economical aspects to name a few.</p><p> </p>

corrected abstract:
<p>The current trend in the automotive industry towards more fuel efficient vehicles requires all components to be as light as possible while still meeting other demands such as stiffness and feasible cost.</p><p>The purpose of this study was to investigate the possibility to replace a partial chassis structure in a Scania low-entry city bus. The partial chassis to be replaced consists of a steel structure and an inner flooring, with the purpose to support loads that the bus is subject to. This was to be done with a composite sandwich structure, with the primary goal to reduce weight by at least 40% and number of components by 50%. The replacement structure needed to meet the stiffness and strength requirements that the current structure fulfils. This was achieved by designing two concepts, <em>concept 1</em> and <em>concept 2</em>, through an iterative FE-analysis in ANSYS. Two prototypes where built and tested for real world load applications.</p><p>The result from this study showed that it was possible to meet both the weight and component reduction goal. Concept 1 and concept 2 achieved a weight reduction of 62% and 68% respectively and the number of components was reduced significantly. Further work would be to investigate the interface between the new structure and the rest of the bus, modal- and fatigue analyses, production implementation and economical aspects to name a few.</p>

Note - only changes to remove the empty paragraph,  add paragraph breaks, and add italics
----------------------------------------------------------------------
In diva2:1335215 
abstract is: 
<p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for functioning quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si3N4 has been used as a strip waveguide, placed on top of a silicon oxide SiO2 wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p><p> </p>
mc='functionin' c='function in'

partal corrected: diva2:1335215: <p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for function ing quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si3N4 has been used as a strip waveguide, placed on top of a silicon oxide SiO2 wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p><p> </p>

corrected abstract:
<p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for functioning quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si<sub>3</sub>N<sub>4</sub> has been used as a strip waveguide, placed on top of a silicon oxide SiO<sub>2</sub> wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p>

Note - only change to remove the empty paragraph and added subscripts
----------------------------------------------------------------------
In diva2:1334842 
abstract is: 
<p>Mathematics contains many hard problems. In this paper we discuss how some of these hard problems can be solved with techniques from other math fields than the problems own discipline. First we solve some combinatorial problems using the knowledge that a maximum amount of vectors in a linearly independent set over a subset of a vector space F^n over a field F is n. Then we discuss and explain Z.Dvir's famous proof regarding Kakeya sets over finite fields. He is able to establish a lower bound of the size of Kakeya sets using polynomials.</p><p> </p>

corrected abstract:
<p>Mathematics contains many hard problems. In this paper we discuss how some of these hard problems can be solved with techniques from other math fields than the problems own discipline. First we solve some combinatorial problems using the knowledge that a maximum amount of vectors in a linearly independent set over a subset of a vector space 𝔽<sup>𝑛</sup> over a field 𝔽 is 𝑛. Then we discuss and explain Z.Dvir's famous proof regarding Kakeya sets over finite fields. He is able to establish a lower bound of the size of Kakeya sets using polynomials.</p>

Note spelling error in original:
mc='Z.Dvir' c='Z. Dvir'
----------------------------------------------------------------------
In diva2:704889 
abstract is: 
<p>In this thesis a corporate bond valuation model based on Dick-Nielsen, Feldhütter, and Lando (2011) and Chen, Lesmond, and Wei (2007) is examined. The aim is for the model to price corporate bond spreads and in particular capture the price effects of liquidity as well as credit risk. The valuation model is based on linear regression and is conducted on the Swedish market with data provided by Handelsbanken. Two measures of liquidity are analyzed: the bid-ask spread and the zero-trading days. The investigation shows that the bid-ask spread outperforms the zero-trading days in both significance and robustness. The valuation model with the bid-ask spread explains 59% of the cross-sectional variation and has a standard error of 56 bps in its pricing predictions of corporate spreads. A reduced version of the valuation model is also developed to address simplicity and target a larger group of users. The reduced model is shown to maintain a large proportion of the explanation power while including fewer and simpler variables.</p><p> </p>

corrected abstract:
<p>In this thesis a corporate bond valuation model based on Dick-Nielsen, Feldhütter, and Lando (2011) and Chen, Lesmond, and Wei (2007) is examined. The aim is for the model to price corporate bond spreads and in particular capture the price effects of liquidity as well as credit risk. The valuation model is based on linear regression and is conducted on the Swedish market with data provided by <em lang="sv">Handelsbanken</em>. Two measures of liquidity are analyzed: the bid-ask spread and the zero-trading days. The investigation shows that the bid-ask spread outperforms the zero-trading days in both significance and robustness. The valuation model with the bid-ask spread explains 59% of the cross-sectional variation and has a standard error of 56 bps in its pricing predictions of corporate spreads. A reduced version of the valuation model is also developed to address simplicity and target a larger group of users. The reduced model is shown to maintain a large proportion of the explanation power while including fewer and simpler variables.</p>

Note - only change to remove the empty paragraph and added italics
----------------------------------------------------------------------
In diva2:1442642 
abstract is: 
<p>In recent decades, sound reproduction research has shown great progress. Audio reproduction changed from a simple two-channel stereo system to a surround sound system and three dimensional sound system. The use of height and angle related speakers was introduced with the aim of improving sound reproduction. With the listener’s experience in focus, this developed reproductive systems were examined. This bachelor’s thesis in sound and vibration compiles three articles on sound quality with regard to different sound reproduction media, with the aim of getting a better understanding of the various psychoacoustical and technical parameters that influence the sound experience. Various subjective evaluation methods are presented, including the PREQUEL method (PerceptualReproduction Quality Evaluation for Loudspeakers), MUSHRA method (MUlti Stimulus test withHidden Reference and Anchor) and OLE method (Overall Listening Experience). The results show how a surround sound system and a three-dimensional sound system provide a more appreciated sound experience and how this experience can be enhanced with a height-related sound system .</p><p> </p>
mc='withHidden' c='with Hidden'
mc='PerceptualReproduction' c='Perceptual Reproduction'

corrected abstract:
<p>In recent decades, sound reproduction research has shown great progress. Audio reproduction changed from a simple two-channel stereo system to a surround sound system and three dimensional sound system. The use of height and angle related speakers was introduced with the aim of improving sound reproduction. With the listener’s experience in focus, this developed reproductive systems were examined.</p><p>This bachelor’s thesis in sound and vibration compiles three articles on sound quality with regard to different sound reproduction media, with the aim of getting a better understanding of the various psychoacoustical and technical parameters that influence the sound experience. Various subjective evaluation methods are presented, including the PREQUEL method (perceptual reproduction quality evaluation for loudspeakers), MUSHRA method (MUlti Stimulus test with Hidden Reference and Anchor) and OLE method (overall listening experience)). The results show how a surround sound system and a three-dimensional sound system provide a more appreciated sound experience and how this experience can be enhanced with a height-related sound system .</p>

Note that there is a space before the final period, several of the spelledout versions of acronyms are in lower case. Also removed the empty paragraph.
----------------------------------------------------------------------
In diva2:415054 
abstract is: 
<p> </p>
<p>Abstract</p>
<p>Thermally-controlled exchange coupling between two strong ferromagnetic (FM) layers separated by a weak FM spacer has promising application in current-driven spintronic devices. In this thesis magnetic property of Ni xFe1-x thin films in the Invar composition range are studied. The focus is on investigating the magnetic transition from ferromagnetic to paramagnetic state of the material and finding the composition with lowest Curie point.</p>
<p>The fabrication process of NixFe1-x  thin films having variable Ni concentration using multi magnetron sputtering is described in the first part of the thesis. In the second part of the thesis the magnetic characterization technique and measurement results are discussed. The Invar effect is observed at approximately 30% Ni content, where the films exhibit a pronounced minimum in the Curie temperature versus Ni concentration</p>

corrected abstract:
<p>Thermally-controlled exchange coupling between two strong ferromagnetic (FM) layers separated by a weak FM spacer has promising application in current-driven spintronic devices. In this thesis magnetic property of Ni<sub>x</sub>Fe<sub>1-x</sub> thin films in the Invar composition range are studied. The focus is on investigating the magnetic transition from ferromagnetic to paramagnetic state of the material and finding the composition with lowest Curie point. The fabrication process of Ni<sub>x</sub>Fe<sub>1-x</sub> thin films having variable Ni concentration using multi magnetron sputtering is described in the first part of the thesis. In the second part of the thesis the magnetic characterization technique and measurement results are discussed. The Invar effect is observed at approximately 30% Ni content, where the films exhibit a pronounced minimum in the Curie temperature versus Ni concentration.</p>
----------------------------------------------------------------------
In diva2:1335210 
abstract is: 
<p>G protein-coupled receptors are one of the biggest targets for pharmaceutical drugs today. The aim with this project was to use different machine learning algorithms to classify the protein into different functional states and compare the results obtained by different algorithms. Both the supervised and unsupervised methods implemented in this project identified similar regions of the protein as important for classification of their functional state. More specifically, the supervised methods Random Forest and Multilayer Perceptron were able to make predictions of the functional state of a protein with great accuracy. The methods investigated will be useful for designing and analyzing the molecular dynamics simulations of GPCRs. Ultimately this will further our understanding of the drug binding mechanics, a critical step for the rational development of new drugs to treat various diseases.</p><p> </p>

corrected abstract:
<p>G protein-coupled receptors are one of the biggest targets for pharmaceutical drugs today. The aim with this project was to use different machine learning algorithms to classify the protein into different functional states and compare the results obtained by different algorithms. Both the supervised and unsupervised methods implemented in this project identified similar regions of the protein as important for classification of their functional state. More specifically, the supervised methods Random Forest and Multilayer Perceptron were able to make predictions of the functional state of a protein with great accuracy. The methods investigated will be useful for designing and analyzing the molecular dynamics simulations of GPCRs. Ultimately this will further our understanding of the drug binding mechanics, a critical step for the rational development of new drugs to treat various diseases.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1879658 
abstract is: 
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>

corrected abstract:
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>
In diva2:1879658 
abstract is: 
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>

corrected abstract:
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1363231 
abstract is: 
<p>In the truck industry, it is crustal to test components against fatigue to make sure that the trucks stand up to the high demands on durability. Today’s testing methods have some disadvantages; it is quite a time-consuming process, but more important, similar tested components cannot easily be compared due to the load spread the components are subjected to. It is therefore desirable to test the components in a standardized way. One way to do this is to use a synthetic signal which is a large number of unique truck measurements combined. The synthetic signal only contains information of the frame’s vibration and not any components. The purpose of this project was to create a model that uses the synthetic signal to describe the motion of components.</p><p> </p><p>Two approaches were used, the first was to base the model on previous measurements, the second one was to base the model on analytical equations. These models were experimentally tested in a 4 channel shake rig, and a silencer was the component chosen to be tested. For the model based on measurements, the load was shown to have a large spread which was hard to control due to the spread in the measurements. The second model was easier to control where the damping factor can be chosen and varied. A promising model was the analytical model using 10% damping applied to the synthetic signal, it covers most measurements without overestimate the load of the component. However, the model was only developed for the silencer acceleration in the z-direction, and it is recommended to develop it for the x-direction as well. The method used in this project could also be used to develop models for other components.</p>

corrected abstract:
<p>In the truck industry, it is crustal to test components against fatigue to make sure that the trucks stand up to the high demands on durability. Today’s testing methods have some disadvantages; it is quite a time-consuming process, but more important, similar tested components cannot easily be compared due to the load spread the components are subjected to. It is therefore desirable to test the components in a standardized way. One way to do this is to use a synthetic signal which is a large number of unique truck measurements combined. The synthetic signal only contains information of the frame’s vibration and not any components. The purpose of this project was to create a model that uses the synthetic signal to describe the motion of components.</p><p>Two approaches were used, the first was to base the model on previous measurements, the second one was to base the model on analytical equations. These models were experimentally tested in a 4 channel shake rig, and a silencer was the component chosen to be tested. For the model based on measurements, the load was shown to have a large spread which was hard to control due to the spread in the measurements. The second model was easier to control where the damping factor can be chosen and varied. A promising model was the analytical model using 10% damping applied to the synthetic signal, it covers most measurements without overestimate the load of the component. However, the model was only developed for the silencer acceleration in the z-direction, and it is recommended to develop it for the x-direction as well. The method used in this project could also be used to develop models for other components.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1137873 
abstract is: 
<p>This report was carried out at Gleechi, a Swedish start-up company working with implementing hand use in Virtual Reality. The thesis presents hand models used to generate natural looking grasping motions. One model were made for each of the thirty-three different grasp types in Feix’s The GRASP Taxonomy.</p><p>Each model is based on functional principal components analysis which was performed on data containing recorded joint angles of grasping motions from real subjects. Prior to functional principal components analysis, dynamic time warping was performed on the recorded joint angles in order to put them on the same length and make them suitable for statistical analysis. The last step of the analysis was to project the data onto the functional principal components and train Gaussian mixture models on the weights obtained. New grasping motions could be generated by sampling weights from the Gaussian mixture models and attaching them to the functional principal components.</p><p>The generated grasps were in general satisfying, but all of the thirty-three grasps were not distinguishable from each other. This was most likely caused by the fact that each degree of freedom was modelled in isolation from each other, so that no correlation between them was included in the model.</p><p> </p>

corrected abstract:
<p>This report was carried out at Gleechi, a Swedish start-up company working with implementing hand use in Virtual Reality. The thesis presents hand models used to generate natural looking grasping motions. One model were made for each of the thirty-three different grasp types in Feix’s <em>The GRASP Taxonomy</em>.</p><p>Each model is based on functional principal components analysis which was performed on data containing recorded joint angles of grasping motions from real subjects. Prior to functional principal components analysis, dynamic time warping was performed on the recorded joint angles in order to put them on the same length and make them suitable for statistical analysis. The last step of the analysis was to project the data onto the functional principal components and train Gaussian mixture models on the weights obtained. New grasping motions could be generated by sampling weights from the Gaussian mixture models and attaching them to the functional principal components.</p><p>The generated grasps were in general satisfying, but all of the thirty-three grasps were not distinguishable from each other. This was most likely caused by the fact that each degree of freedom was modelled in isolation from each other, so that no correlation between them was included in the model.</p>

Note - only change to remove the empty paragraph and adding italics
----------------------------------------------------------------------
In diva2:738788 - - unnessary period at end of title:
"Modellering av turboladdarens surge-beteende för fordonsindustrin."
==>
"Modellering av turboladdarens surge-beteende för fordonsindustrin"

abstract is: 
<p>The turbocharger was originally designed and used to boost engine power of vehicles. Nowadays, when the demand for low carbon vehicles is increasing rapidly, a new application for the turbocharger has been found by using it to downsizing the engine. Using experimental as well as theoretical simulation models we can estimate the outcome and behavior of the compressor in an extended work range. An aspect that has a substantial effect on the turbocharger and engine is the surge line. Surge is a problematic stage where the compressor creates unwanted behavior which could damage the turbocharger as well as the engine. The surge line is a line where the transition to surge occurs. By changing the surge line, through simulations and calculations surge can be avoided, you can optimize and improve the turbocharger. This report mainly discusses and investigates the possibility to use fast one-dimensional simulation software instead of full scaled laboratories in the automotive industry, and estimate the work range of the given compressor.</p><p> </p>

corrected abstract:
<p>The turbocharger was originally designed and used to boost engine power of vehicles. Nowadays, when the demand for low carbon vehicles is increasing rapidly, a new application for the turbocharger has been found by using it to downsizing the engine. Using experimental as well as theoretical simulation models we can estimate the outcome and behavior of the compressor in an extended work range. An aspect that has a substantial effect on the turbocharger and engine is the surge line. Surge is a problematic stage where the compressor creates unwanted behavior which could damage the turbocharger as well as the engine. The surge line is a line where the transition to surge occurs. By changing the surge line, through simulations and calculations surge can be avoided, you can optimize and improve the turbocharger. This report mainly discusses and investigates the possibility to use fast one-dimensional simulation software instead of full scaled laboratories in the automotive industry, and estimate the work range of the given compressor.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:716518 
abstract is: 
<p>Cancer is a common cause of death worldwide and radiotherapy is one of the treatments used. Since treatment planning is a time consuming matter for the radiation therapist, a way to decrease the time spent finding the plan would be an improvement. This can be achieved by precalculating a number of optimal plans and then choosing among these in real-time.</p><p>In this thesis a dual algorithm for approximation of the Pareto optimal plans suggested by Bokrantz and Forsgren, was adapted to the parameters of the Leksell Gamma Knife®. A Graphical User Interface was also created, based on the navigation tool described by Monz et al to enable choosing among the pre-calculated dose plans.</p><p>The computational time of the algorithm was investigated and the dimensionality of the solutions and Pareto optimal points were looked at to see if it might be possible to reduce the number of dimensions to speed up computations.</p><p>Although no certain conclusions can be drawn about dimensionality reduction, I found no reason to rule that possibility out. It was also confirmed that there is reason to keep the number of objectives low to get a better approximation.</p><p> </p>

corrected abstract:
<p>Cancer is a common cause of death worldwide and radiotherapy is one of the treatments used. Since treatment planning is a time consuming matter for the radiation therapist, a way to decrease the time spent finding the plan would be an improvement. This can be achieved by precalculating a number of optimal plans and then choosing among these in real-time.</p><p>In this thesis a dual algorithm for approximation of the Pareto optimal plans suggested by Bokrantz and Forsgren, was adapted to the parameters of the Leksell Gamma Knife®. A Graphical User Interface was also created, based on the navigation tool described by Monz et al to enable choosing among the pre-calculated dose plans.</p><p>The computational time of the algorithm was investigated and the dimensionality of the solutions and Pareto optimal points were looked at to see if it might be possible to reduce the number of dimensions to speed up computations.</p><p>Although no certain conclusions can be drawn about dimensionality reduction, I found no reason to rule that possibility out. It was also confirmed that there is reason to keep the number of objectives low to get a better approximation.</p>

Note - only change to remove the empty paragraph
Note spelling error:
"et al" should be "et al."
----------------------------------------------------------------------
In diva2:1848437 
abstract is: 
<p>Turbulent mixing of single or multi-phase flows is common in diverse research fields, and direct numerical simulation is useful for understanding such phenomenon. To study the scaler transport in turbulence, the computational grids must resolve the Batchelor scale, which is smaller than the Kolmogorov scale by a factor of √Sc. This would commonly lead to the over-resolving of the Navier-Stokes equation, making DNS even more expensive. To overcome this issue, this thesis presents a method to reduce the computational cost in scalar turbulent flows, by using a coarse grid for the velocity and a fine grid for the scalar. A divergence-free Hermite interpolation is implemented for the velocity field to ensure the continuity equation is fulfilled on the fine grid. The interpolation scheme is validated by cases of Arnold–Beltrami–Childress flow and Taylor–Green vortex. For the active scaler, the integration schemes are included for fine-to-coarse integration. The multi-resolution method is parallelised by MPI and integrated into the open-source multiphase flow solver FluTAS. For the diffuse interface modelling in FluTAS, the indicator function is updated on the fine grid, and the surface tension force is then calculated and extrapolated back to the coarse grid for momentum equation update. The method is evaluated against the single-resolution method at different refinement factors.</p><p> </p>

corrected abstract:
<p>Turbulent mixing in multi-phase flows is prevalent across various research domains, and direct numerical simulation (DNS) is often used for understanding such phenomena. However, DNS of turbulent scalar transport entails resolving the Batchelor scale, which is typically smaller than the Kolmogorov scale by a factor of 𝑆𝑐<sup>−1/2</sup>. This can lead to increased number of grid points and the over-resolve of NaiverStokes equations, making the DNS even more expensive. To overcome this issue, this thesis presents a method to reduce the computational cost, by using a coarse grid for solving the velocity and pressure, and a fine grid for solving the scalar. A divergencefree interpolation scheme is implemented for velocity to ensure the continuity equation is fulfilled on the fine grid. The interpolation scheme is validated by Arnold–Beltrami– Childress flow and Taylor–Green vortex. For active scalars, a scheme of weighted averaging facilitates the fine-to-coarse integration. The multi-resolution method is parallelised by MPI and integrated into the open-source code FluTAS for multiphase flow simulation. For the diffuse-interface modelling in FluTAS, the phase-field variable and surface tension force are computed on the fine grid and integrated to the coarse grid to couple with the momentum equation. The presented method is evaluated against the single-resolution method using the rising bubble benchmark.</p>

Note - major differences between DiVA and original abstract and removed empty paragraph
----------------------------------------------------------------------
In diva2:1776757 
abstract is: 
<p>This paper aims at presenting the necessary tools to prove that a scheme of finite type over Z exhibits the same singularities as those which occur on a Grassmann variety. First, basic theory regarding the combinatorial objects matroids is presented. Some important examples for the remainder of the paper are given, which also serve to aid the reader in intuition and understanding of matroids. Basic algebraic geometry is presented, and the building blocks affine varieties, projective varieties and general varieties are introduced. These object are generalised in the following subsection as affine schemes and schemes, which are the central object of study in modern algebraic geometry. Important results from the theory of algebraic groups are shown in order to better understand the formulation and proof of the Gelfand–MacPherson theorem, which in turn is utilised, together with Mnëv’s universality theorem, to prove the main result of the paper.</p><p> </p>

corrected abstract:
<p>This paper aims at presenting the necessary tools to prove that a scheme of finite type over ℤ exhibits the same singularities as those which occur on a Grassmann variety. First, basic theory regarding the combinatorial objects matroids is presented. Some important examples for the remainder of the paper are given, which also serve to aid the reader in intuition and understanding of matroids. Basic algebraic geometry is presented, and the building blocks affine varieties, projective varieties and general varieties are introduced. These object are generalised in the following subsection as affine schemes and schemes, which are the central object of study in modern algebraic geometry. Important results from the theory of algebraic groups are shown in order to better understand the formulation and proof of the Gelfand–MacPherson theorem, which in turn is utilised, together with Mnëv’s universality theorem, to prove the main result of the paper.</p>

Note - only change to remove the empty paragraph and replacement of "Z" by "ℤ"
----------------------------------------------------------------------
In diva2:1436960 
abstract is: 
<p>The Aviation industry is important to the European economy and development, therefore a study of the sensitivity of the European flight network is interesting. If clusters exist within the network, that could indicate possible vulnerabilities or bottlenecks, since that would represent a group of airports poorly connected to other parts of the network. In this paper a cluster analysis using spectral clustering is performed with flight data from 34 different European countries. The report also looks at how to implement the spectral clustering algorithm for large data sets. After performing the spectral clustering it appears as if the European flight network is not clustered, and thus does not appear to be sensitive.</p><p> </p>

corrected abstract:
<p>The Aviation industry is important to the European economy and development, therefore a study of the sensitivity of the European flight network is interesting. If clusters exist within the network, that could indicate possible vulnerabilities or bottlenecks, since that would represent a group of airports poorly connected to other parts of the network. In this paper a cluster analysis using spectral clustering is performed with flight data from 34 different European countries. The report also looks at how to implement the spectral clustering algorithm for large data sets. After performing the spectral clustering it appears as if the European flight network is not clustered, and thus does not appear to be sensitive.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:892288 
abstract is: 
<p>Particle suspensions occur in many situations in nature and industry. In this master’s thesis, the motion of a single rigid spheroidal particle immersed in Stokes flow is studied numerically using a boundary integral method and a new specialized quadrature method known as quadrature by expansion (QBX). This method allows the spheroid to be massless or inertial, and placed in any kind of underlying Stokesian flow.</p><p> </p><p>A parameter study of the QBX method is presented, together with validation cases for spheroids in linear shear flow and quadratic flow. The QBX method is able to compute the force and torque on the spheroid as well as the resulting rigid body motion with small errors in a short time, typically less than one second per time step on a regular desktop computer. Novel results are presented for the motion of an inertial spheroid in quadratic flow, where in contrast to linear shear flow the shear rate is not constant. It is found that particle inertia induces a translational drift towards regions in the fluid with higher shear rate.</p>

corrected abstract:
<p>Particle suspensions occur in many situations in nature and industry. In this master’s thesis, the motion of a single rigid spheroidal particle immersed in Stokes flow is studied numerically using a boundary integral method and a new specialized quadrature method known as quadrature by expansion (QBX). This method allows the spheroid to be massless or inertial, and placed in any kind of underlying Stokesian flow.</p><p>A parameter study of the QBX method is presented, together with validation cases for spheroids in linear shear flow and quadratic flow. The QBX method is able to compute the force and torque on the spheroid as well as the resulting rigid body motion with small errors in a short time, typically less than one second per time step on a regular desktop computer. Novel results are presented for the motion of an inertial spheroid in quadratic flow, where in contrast to linear shear flow the shear rate is not constant. It is found that particle inertia induces a translational drift towards regions in the fluid with higher shear rate.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1341297 
Note: no full text in DiVA

abstract is: 
<p>Stochastic gradient descent is an algorithm used in optimization. Learning rate plays a central role in the stochastic gradient descent algorithm. If the selected learning rate lies within the appropriate interval, it affects the algorithm's convergence rate towards local minimum as well as its accuracy. In this work the stochastic gradient descent algorithm was used to treat two simple optimization problems. Firstly, a series of numerical experiments for a plethora of learning rate were performed where the behavior of the algorithm was studied. Secondly, the training of a neural network using the stochastic gradient descent was experimentally studied. The effect of learning rate values was tested as well as the neural network’s performance by varying parameters such as the number of nodes, the activation function and combinations of the above.</p><p> </p>

corrected abstract:
<p>Stochastic gradient descent is an algorithm used in optimization. Learning rate plays a central role in the stochastic gradient descent algorithm. If the selected learning rate lies within the appropriate interval, it affects the algorithm's convergence rate towards local minimum as well as its accuracy. In this work the stochastic gradient descent algorithm was used to treat two simple optimization problems. Firstly, a series of numerical experiments for a plethora of learning rate were performed where the behavior of the algorithm was studied. Secondly, the training of a neural network using the stochastic gradient descent was experimentally studied. The effect of learning rate values was tested as well as the neural network’s performance by varying parameters such as the number of nodes, the activation function and combinations of the above.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1877780 
abstract is: 
<p>This bachelor thesis aims to give an introduction to various Hopf algebras that arise in combinatorics, with a view towards symmetric functions. We begin by covering the algebraic background needed to define Hopf algebras, including a discussion of the algebra-coalgebra duality. Takeuchi's formula for the antipode is stated and proved. It is then generalised to incidence Hopf algebras. This is followed by a discussion of the Hopf algebra of symmetric functions. It is shown that the Hopf algebra of symmetric functions is self-dual. We also show that the graded dual of the Hopf algebra of quasisymmetric functions is the Hopf algebra of non-commutative symmetric functions. Relations to the Hopf algebra of symmetric functions in non-commuting variables are emphasised. Finally, we state and prove the Aguiar-Bergeron-Sottile universality theorem.</p><p> </p>

corrected abstract:
<p>This bachelor thesis aims to give an introduction to various Hopf algebras that arise in combinatorics, with a view towards symmetric functions. We begin by covering the algebraic background needed to define Hopf algebras, including a discussion of the algebra-coalgebra duality. Takeuchi's formula for the antipode is stated and proved. It is then generalised to incidence Hopf algebras. This is followed by a discussion of the Hopf algebra of symmetric functions. It is shown that the Hopf algebra of symmetric functions is self-dual. We also show that the graded dual of the Hopf algebra of quasisymmetric functions is the Hopf algebra of non-commutative symmetric functions. Relations to the Hopf algebra of symmetric functions in non-commuting variables are emphasised. Finally, we state and prove the Aguiar-Bergeron-Sottile universality theorem.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:740516 - unnessary period at end of title:
"On the 1932 Discovery of the Positron."
==>
"On the 1932 Discovery of the Positron."

abstract is: 
<p>An experiment on Cosmic rays performed by Carl D Anderson led to the accidental discovery of the positron in 1932. The discovery was a turning point in particle physics which led to numerous other theories and has been discussed by scientists all over the world. Anderson had photographed a 63 MeV, upward moving electron. The possible origin of such a positron has never before been discussed and is what this report will aim to explain. The report will include some evidence that the particle is in fact a positron as well as a discussion of the four main theories whose possibility and probability will be discussed; pion decay, muon decay, magnetic field bending and pair production. The report will also cover a historical background for Anderson’s experiment, as well as a theoretical background needed for the theories of the origin. The probability of discovering a positron with any of the theorized origins is extremely low and for some theories, even impossible.</p><p> </p>

corrected abstract:
<p>An experiment on Cosmic rays performed by Carl D Anderson led to the accidental discovery of the positron in 1932. The discovery was a turning point in particle physics which led to numerous other theories and has been discussed by scientists all over the world. Anderson had photographed a 63 MeV, upward moving electron. The possible origin of such a positron has never before been discussed and is what this report will aim to explain. The report will include some evidence that the particle is in fact a positron as well as a discussion of the four main theories whose possibility and probability will be discussed; pion decay, muon decay, magnetic field bending and pair production. The report will also cover a historical background for Anderson’s experiment, as well as a theoretical background needed for the theories of the origin. The probability of discovering a positron with any of the theorized origins is extremely low and for some theories, even impossible.</p>

Note - only change to remove the empty paragraph
Note spelling error: there should be a period after "D." as it is an initial - this is correct in the Swedish abstract
----------------------------------------------------------------------
In diva2:1779459 
abstract is: 
<p>The focus of this thesis is to find, visualize and analyze the optimal flow of autonomous electric vehicles with charge constraints in urban traffic with respect to energy consumption. The traffic has been formulated as a static multi-commodity network flow problem, for which two different models have been implemented to handle the charge constraints. The first model uses a recursive algorithm to find the optimal solution fulfilling the charge constraints, while the second model discretizes the commodities’ battery to predetermined battery levels. An implementation of both methods is provided through simulations on scenarios of three different sizes. The results show that both methods are capable of representing the traffic flow with charge constraints, with limitations given by the size of the problem. In particular, the recursive model has the advantage of considering the charge as a continuous quantity. On the other hand the discretization of battery levels allows to handle charge constraint setups with higher complexity, that is when longer detours are needed to fulfill the charge constraints.</p><p> </p>

corrected abstract:
<p>The focus of this thesis is to find, visualize and analyze the optimal flow of autonomous electric vehicles with charge constraints in urban traffic with respect to energy consumption. The traffic has been formulated as a static multi-commodity network flow problem, for which two different models have been implemented to handle the charge constraints. The first model uses a recursive algorithm to find the optimal solution fulfilling the charge constraints, while the second model discretizes the commodities’ battery to predetermined battery levels. An implementation of both methods is provided through simulations on scenarios of three different sizes. The results show that both methods are capable of representing the traffic flow with charge constraints, with limitations given by the size of the problem. In particular, the recursive model has the advantage of considering the charge as a continuous quantity. On the other hand the discretization of battery levels allows to handle charge constraint setups with higher complexity, that is when longer detours are needed to fulfill the charge constraints.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1352113 
abstract is: 
<p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p> </p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome beforehand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>
mc='forehand' c='fore hand'

partal corrected: diva2:1352113: <p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p> </p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome before hand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>

corrected abstract:
<p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome beforehand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:904606 
abstract is: 
<p>Today’s society keeps trying to improve efficiency in production and general reliability in systems. This is true for all types of companies that are driven to use optimization techniques in their work. One major field impacted by these studies is transport. Railway companies set the example and make optimization a priority in order to handle train flows the best way possible. That is why the French National Railway Company imagined an informatic tool to decide how trains are to be parked optimally. The thesis exposed in this report has been proposed for treating the modeling and programming parts of the project.</p><p>Modeling the station has been done in association with railway experts who bring their knowledge about the station infrastructure. They are also a reference for determining the constraints a train must obey when arriving at the station, parking and leaving. Only useful aspects of the station must be taken into account. That is why a compromise has been chosen in the level of detail modeled.</p><p>Then, a mathematical formulation of the problem has been proposed. The decision variables are for each train its arrival path, parking track and departure path. There are lots of possibilities and the choice can be difficult. The constraints are quite numerous as well. Thus, it has been decided to divide the constraints in two parts: hard and soft constraints. The general idea is to find a solution that obeys all hard constraints and minimizes the number of soft constraints not respected. </p><p>The project consisted also in choosing a language to code the model and a solver. The mathematical model has been written with the modeling language OPL that calls the solver Cplex. This software is one of the best to solve large scale problems that may arise for complex stations. Since the problem was a Mixed Integer Programming; the solver uses the Branch &amp; Cut algorithm that is a combination between different techniques. It consists in applying the Branch &amp; Bound algorithm while cutting the feasibility region. Heuristics techniques are also used by the algorithm in order to provide quickly a feasible solution. After running the program, we get a solution which is optimal according to the approach discussed earlier on. More precisely, it would determine for each train an arrival path, a parking track and a departure path.</p><p> </p><p>The program developed during this thesis is proposed to the stations. It would help them in deciding how trains are parked on one day. This decision is made in a conception phase happening several months before the actual day treated. Several major stations have tested the optimization tool. Thanks to a regularity coefficient, they are able to judge the quality of the solution. Since this score has been increased, the solution is judged as rather good. Furthermore, they will use it for the year 2016. A significant advantage is also the time for making the decision. When the modeling phase is done, a few minutes are enough to produce a solution for a given train set, whereas it can take several weeks of work by hand.</p>

corrected abstract:
<p>Today’s society keeps trying to improve efficiency in production and general reliability in systems. This is true for all types of companies that are driven to use optimization techniques in their work. One major field impacted by these studies is transport. Railway companies set the example and make optimization a priority in order to handle train flows the best way possible. That is why the French National Railway Company imagined an informatic tool to decide how trains are to be parked optimally. The thesis exposed in this report has been proposed for treating the modeling and programming parts of the project.</p><p>Modeling the station has been done in association with railway experts who bring their knowledge about the station infrastructure. They are also a reference for determining the constraints a train must obey when arriving at the station, parking and leaving. Only useful aspects of the station must be taken into account. That is why a compromise has been chosen in the level of detail modeled.</p><p>Then, a mathematical formulation of the problem has been proposed. The decision variables are for each train its arrival path, parking track and departure path. There are lots of possibilities and the choice can be difficult. The constraints are quite numerous as well. Thus, it has been decided to divide the constraints in two parts: hard and soft constraints. The general idea is to find a solution that obeys all hard constraints and minimizes the number of soft constraints not respected.</p><p>The project consisted also in choosing a language to code the model and a solver. The mathematical model has been written with the modeling language OPL that calls the solver Cplex. This software is one of the best to solve large scale problems that may arise for complex stations. Since the problem was a Mixed Integer Programming; the solver uses the Branch &amp; Cut algorithm that is a combination between different techniques. It consists in applying the Branch &amp; Bound algorithm while cutting the feasibility region. Heuristics techniques are also used by the algorithm in order to provide quickly a feasible solution. After running the program, we get a solution which is optimal according to the approach discussed earlier on. More precisely, it would determine for each train an arrival path, a parking track and a departure path.</p><p>The program developed during this thesis is proposed to the stations. It would help them in deciding how trains are parked on one day. This decision is made in a conception phase happening several months before the actual day treated. Several major stations have tested the optimization tool. Thanks to a regularity coefficient, they are able to judge the quality of the solution. Since this score has been increased, the solution is judged as rather good. Furthermore, they will use it for the year 2016. A significant advantage is also the time for making the decision. When the modeling phase is done, a few minutes are enough to produce a solution for a given train set, whereas it can take several weeks of work by hand.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1360595
Note: no full text in DiVA

abstract is: 
<p>Investigations on the Circular Restricted 3- Body Problem (CR3BP) and the motion about the Lagrangian points are not recent. Several past (upcoming) missions have used (plan to use) its dynamics. The existence of specific periodic orbits and their associated invariant manifolds is one property of the CR3BP that raises the interest. These periodic orbits are interesting for their great observation properties, eclipse avoidance, communication’s continuity with the Earth, etc. However, to reach them, optimized transfer trajectories have to be found. A numerical tool is developed to construct firstly these orbits, before using them as input parameters for the invariant manifolds. Indeed, when a spacecraft is inserted into one of these manifolds, it shall naturally reach the orbit without any additional cost. This numerical computation provides manifolds’s insertion points, which are used in return by a Nonlinear Programming (NLP) tool to eventually find the optimized trajectory. Families of periodic orbits and manifolds, together with optimized transfer trajectories, have been successfully computed, with a focus on the Halo Orbits of the Earth-Moon system. Some members of this family, the Near-Rectilinear Halo Orbits (NRHOs), are of a great interest both for their geometry characteristics (close approach of the secondary body) and stability properties. However, in the Earth-Moon system, the associated manifolds do not have points relatively close from the Earth. The thesis work hence does not ensure that using manifolds as transfer arcs is beneficial, compared to a direct transfer. Besides, the Time-Of-Flight (TOF) is significantly larger. Transfer strategies making use of the CR3BP dynamics still are interesting, radically different from the usual trajectories and offering a larger number of opportunities. They may be less expansive, and could particularly be used for uncrewed space missions.</p><p> </p>

corrected abstract:
<p>Investigations on the Circular Restricted 3- Body Problem (CR3BP) and the motion about the Lagrangian points are not recent. Several past (upcoming) missions have used (plan to use) its dynamics. The existence of specific periodic orbits and their associated invariant manifolds is one property of the CR3BP that raises the interest. These periodic orbits are interesting for their great observation properties, eclipse avoidance, communication’s continuity with the Earth, etc. However, to reach them, optimized transfer trajectories have to be found. A numerical tool is developed to construct firstly these orbits, before using them as input parameters for the invariant manifolds. Indeed, when a spacecraft is inserted into one of these manifolds, it shall naturally reach the orbit without any additional cost. This numerical computation provides manifolds’s insertion points, which are used in return by a Nonlinear Programming (NLP) tool to eventually find the optimized trajectory. Families of periodic orbits and manifolds, together with optimized transfer trajectories, have been successfully computed, with a focus on the Halo Orbits of the Earth-Moon system. Some members of this family, the Near-Rectilinear Halo Orbits (NRHOs), are of a great interest both for their geometry characteristics (close approach of the secondary body) and stability properties. However, in the Earth-Moon system, the associated manifolds do not have points relatively close from the Earth. The thesis work hence does not ensure that using manifolds as transfer arcs is beneficial, compared to a direct transfer. Besides, the Time-Of-Flight (TOF) is significantly larger. Transfer strategies making use of the CR3BP dynamics still are interesting, radically different from the usual trajectories and offering a larger number of opportunities. They may be less expansive, and could particularly be used for uncrewed space missions.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1198117 
abstract is: 
<p>Climate change is an evermore urging existential treat to the human enterprise. Mean temperature and greenhouse gas emissions have in-creased exponentially since the industrial revolution. But solutions are also mushrooming with exponential pace. Renewable energy technologies, such as wind and solar power, are deployed like never before and their costs have decreased signiﬁcantly. In order to allow for further transformation of the energy system these technologies must be reﬁned and optimised. In wind energy one important ﬁeld with high potential of reﬁnement is aerodynamics. The aerodynamics of wind turbines constitutes one challenging research frontier in aerodynamics today.</p><p> </p><p>In this study, a novel approach for calculating wind turbine ﬂow is developed. The approach is based on the partially parabolic Navier-Stokes equations, which can be solved computationally with higher eﬃciency as compared to the fully elliptic version. The modelling of wind turbine thrust is done using actuator-disk theory and the torque is modelled by application of the Joukowsky rotor. A validation of the developed model and force implementation is conducted using four diﬀerent validation cases.</p><p> </p><p>In order to provide value for industrial wind energy projects, the model must be extended to account for turbulence (and terrain in case of onshore projects). Possible candidates for turbulence modelling are parabolic k-ε and explicit Reynolds stress turbulence models. The terrain could possibly be incorporated consistently with the used projection method by altering the ﬁnite diﬀerence grid layout.</p>

corrected abstract:
<p>Climate change is an evermore urging existential treat to the human enterprise. Mean temperature and greenhouse gas emissions have increased exponentially since the industrial revolution. But solutions are also mushrooming with exponential pace. Renewable energy technologies, such as wind and solar power, are deployed like never before and their costs have decreased significantly. In order to allow for further transformation of the energy system these technologies must be refined and optimised. In wind energy one important field with high potential of refinement is aerodynamics. The aerodynamics of wind turbines constitutes one challenging research frontier in aerodynamics today.</p><p>In this study, a novel approach for calculating wind turbine flow is developed. The approach is based on the partially parabolic Navier-Stokes equations, which can be solved computationally with higher efficiency as compared to the fully elliptic version. The modelling of wind turbine thrust is done using actuator-disk theory and the torque is modelled by application of the Joukowsky rotor. A validation of the developed model and force implementation is conducted using four different validation cases.</p><p>In order to provide value for industrial wind energy projects, the model must be extended to account for turbulence (and terrain in case of onshore projects). Possible candidates for turbulence modelling are parabolic 𝑘-<em>ε</em> and explicit Reynolds stress turbulence models. The terrain could possibly be incorporated consistently with the used projection method by altering the finite difference grid layout.</p>

Note - only changes to remove the empty paragraphs, removed an unnecessary hyphen, and replaced "k" with "𝑘" and added italics for the omega.
----------------------------------------------------------------------
In diva2:560786 
abstract is: 
<p>Several models for predicting future customer profitability early into customer life-cycles in the property and casualty business are constructed and studied. The objective is to model risk at a customer level with input data available early into a private consumer’s lifespan. Two retained models, one using Generalized Linear Model another using a multilayer perceptron, a special form of Artificial Neural Network are evaluated using actual data. Numerical results show that differentiation on estimated future risk is most effective for customers with highest claim frequencies.</p><p> </p>

corrected abstract:
<p>Several models for predicting future customer profitability early into customer life-cycles in the property and casualty business are constructed and studied. The objective is to model risk at a customer level with input data available early into a private consumer’s lifespan. Two retained models, one using Generalized Linear Model another using a multilayer perceptron, a special form of Artificial Neural Network are evaluated using actual data. Numerical results show that differentiation on estimated future risk is most effective for customers with highest claim frequencies.</p>
----------------------------------------------------------------------
In diva2:1878726 
abstract is: 
<p>This project applied statistical inference methods to historical data of mixed martial arts (MMA) matches from the Ultimate Fighting Championship (UFC). The goal of the project was to create a model to predict the outcome of Ultimate Fighting Championship matches with the best possible accuracy. The main methods used in the project were logistic regression and Bayesian regression. The data used for said model was taken from matches between early April 2000 and mid April 2024. The predictions made by these models were compared with the predictions of various betting sites as well as with the true outcomes of the matches. The logistic regression model and the Bayesian model predicted the true outcome of the matches 60% and 70% of the time respectively, with both having comparable predictions to those of the betting sites.</p><p> </p>

corrected abstract:
<p>This project applied statistical inference methods to historical data of mixed martial arts (MMA) matches from the Ultimate Fighting Championship (UFC). The goal of the project was to create a model to predict the outcome of Ultimate Fighting Championship matches with the best possible accuracy. The main methods used in the project were logistic regression and Bayesian regression. The data used for said model was taken from matches between early April 2000 and mid April 2024. The predictions made by these models were compared with the predictions of various betting sites as well as with the true outcomes of the matches. The logistic regression model and the Bayesian model predicted the true outcome of the matches 60% and 70% of the time respectively, with both having comparable predictions to those of the betting sites.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:690036 - unnessary period at end of title:
"Prediktion av villapris och dess faktorers inverkan."
==>
"Prediktion av villapris och dess faktorers inverkan"

abstract is: 
<p> </p><p><em>A villas price </em>depends on several important factors. By statistical data, a mathematical <em>multiple regression model </em>was modeled. The model has important explanatory variables such as living space, renovation year and standard points has been taken into consideration, in order to assess their impact on the <em>final price </em>for private homes.</p><p>By using a statistical program,</p><p><em>Minitab 16</em>, the final model was selected with <em>eight explanatory variables</em>. The regression for this model explains up to <em>67.3 % </em>of the variation on the final price.</p><p>The results showed percentage wise that the standard points had the greatest impact on the price, there after renovation year and then living space.</p>

corrected abstract:
<p>A villas <em>price</em> depends on several important factors. By statistical data, a mathematical <em>multiple regression model</em> was modeled. The model has important explanatory variables such as living space, renovation year and standard points has been taken into consideration, in order to assess their impact on the <em>final price</em> for private homes.</p><p>By using a statistical program, <em>Minitab 16</em>, the final model was selected with <em>eight explanatory variables</em>. The regression for this model explains up to <em>67.3 %</em> of the variation on the final price.</p><p>The results showed percentage wise that the standard points had the greatest impact on the price, there after renovation year and then living space.</p>

Note - removed the empty paragraph and adjusted the italics
----------------------------------------------------------------------
In diva2:1341561 
abstract is: 
<p>This report describes the design and production of a 3-axis Helmholtz coil assembly and its control unit. The purpose of the system is to simulate the magnetic environment that the CubeSat MIST will need to measure in order to determine and control its attitude. To achieve this, the system consists of three Helmholtz coils with diameters of roughly 1 metre, supplied by a circuit that filters, transforms, and amplifies signals of 0-5 V (e.g. Arduino signals) to -50 to 50 V. The size of the coils allow for a near-homogeneous magnetic field large enough to cover the whole satellite. By adjusting the input, two necessary tests can be done on the satellite's attitude determination and control system. The first consists of verifying the magnetometer's correct measurement of the direction of the ambient magnetic field, and the other of testing the detumbling capability of the system when the satellite is in a rotating field. The equipment produced has been tested to verify its operation meets set requirements for testing.</p><p> </p>

corrected abstract:
<p>This report describes the design and production of a 3-axis Helmholtz coil assembly and its control unit. The purpose of the system is to simulate the magnetic environment that the CubeSat MIST will need to measure in order to determine and control its attitude. To achieve this, the system consists of three Helmholtz coils with diameters of roughly 1 metre, supplied by a circuit that filters, transforms, and amplifies signals of 0-5 V (e.g. Arduino signals) to -50 to 50 V. The size of the coils allow for a near-homogeneous magnetic field large enough to cover the whole satellite. By adjusting the input, two necessary tests can be done on the satellite's attitude determination and control system. The first consists of verifying the magnetometer's correct measurement of the direction of the ambient magnetic field, and the other of testing the detumbling capability of the system when the satellite is in a rotating field. The equipment produced has been tested to verify its operation meets set requirements for testing.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1798454 
abstract is: 
<p>GaN-HEMTs (Gallium Nitride-based High Electron Mobility Transistors) have, thanks to the large band gap of GaN, electrical properties that are suitable for applications of high electrical voltages, high currents, and fast switching. The large band gap also gives GaN-HEMTs a high resistance to radiation. In this degree project, the effects of 2 MeV proton irradiation of GaN-HEMTs constructed on both silicon carbide and silicon substrates are investigated. 20 transistors per substrate were irradiated in the particle accelerator 5 MV NEC Pelletron in the Ångström laboratory at Uppsala University. These transistors were exposed to radiation doses in the range of 10^11 to 10^15 protons/cm^2. The analysis shows that both transistors on silicon, as well as silicon carbide, are unaffected by proton irradiation up to a dose of 10^14 protons/cm^2. GaN-on-Si transistors show less influence of radiation than GaN-on-SiC transistors. The capacitances between gate and drain as well as drain and source for both GaN-on-SiC and GaN-on-Si HEMTs show hysteresis as a function of forward and backward gate voltage sweeps for the radiation dose of 10^15 protons/cm^2.</p><p> </p>

corrected abstract:
<p>GaN-HEMTs (Gallium Nitride-based High Electron Mobility Transistors) have, thanks to the large band gap of GaN, electrical properties that are suitable for applications of high electrical voltages, high currents, and fast switching. The large band gap also gives GaN-HEMTs a high resistance to radiation. In this degree project, the effects of 2 MeV proton irradiation of GaN-HEMTs constructed on both silicon carbide and silicon substrates are investigated. 20 transistors per substrate were irradiated in the particle accelerator 5 MV NEC Pelletron in the Ångström laboratory at Uppsala University. These transistors were exposed to radiation doses in the range of 10<sup>11</sup> to 10<sup>15</sup> protons/cm<sup>2</sup>. The analysis shows that both transistors on silicon, as well as silicon carbide are unaffected by proton irradiation up to a dose of 10<sup>14</sup> protons/cm<sup>2</sup>. GaN-on-Si transistors show less influence of radiation than GaN-on-SiC transistors. The capacitances between gate and drain as well as drain and source for both GaN-on-SiC and GaN-on-Si HEMTs show hysteresis as a function of forward and backward gate voltage sweeps for the radiation dose of 10<sup>15</sup> protons/cm<sup>2</sup>.</p>

Note - removed the empty paragraph and fixed the superscripts
----------------------------------------------------------------------
In diva2:858615 
abstract is: 
<p>This thesis describes a Coq formalization of realizability interpretations of arithmetic. The realizability interpretations are based on partial combinatory algebras—to each partial combinatory algebra there is an associated realizability interpretation. I construct two partial combinatory algebras. One of these gives a realizability interpretation equivalent to Kleene’s original one, without involving the usual recursion-theoretic machinery.</p><p> </p>

corrected abstract:
<p>This thesis describes a Coq formalization of realizability interpretations of arithmetic. The realizability interpretations are based on partial combinatory algebras&mdash;to each partial combinatory algebra there is an associated realizability interpretation. I construct two partial combinatory algebras. One of these gives a realizability interpretation equivalent to Kleene’s original one, without involving the usual recursion-theoretic machinery.</p>

Note - removed the empty paragraph and change on ". " to an &mdash;"
----------------------------------------------------------------------
In diva2:1570223 
abstract is: 
<p>Spotify, which is one of the worlds biggest music services, posted a data set and an open-ended challenge for music recommendation research. This study's goal is to recommend songs to playlists with the given data set from Spotify using Spectral clustering. While the given data set had 1 000 000 playlists, Spectral clustering was performed on a subset with 16 000 playlists due to the lack of computational resources. With four different weighting methods describing the connection between playlists, the study shows results of reasonable clusters where similar category of playlists were clustered together although most of the results also had a very large clusters where a lot of different sorts of playlists were clustered together. The conclusion of the results were that the data was overly connected as an effect of our weighting methods. While the results show the possibility of recommending songs to a limited number of playlists, hierarchical clustering would possibly be helpful to be able to recommend song to a larger amount of playlists, but that is left to future research to conclude.</p><p> </p>

corrected abstract:
<p>Spotify, which is one of the worlds biggest music services, posted a data set and an open-ended challenge for music recommendation research. This study's goal is to recommend songs to playlists with the given data set from Spotify using Spectral clustering. While the given data set had 1 000 000 playlists, Spectral clustering was performed on a subset with 16 000 playlists due to the lack of computational resources. With four different weighting methods describing the connection between playlists, the study shows results of reasonable clusters where similar category of playlists were clustered together although most of the results also had a very large clusters where a lot of different sorts of playlists were clustered together. The conclusion of the results were that the data was overly connected as an effect of our weighting methods. While the results show the possibility of recommending songs to a limited number of playlists, hierarchical clustering would possibly be helpful to be able to recommend song to a larger amount of playlists, but that is left to future research to conclude.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:555815 
abstract is: 
<p>Abstract</p><p> </p><p>As accurately as possible, creditors wish to determine if a potential debtor will repay the borrowed sum. To achieve this mathematical models known as credit scorecards quantifying the risk of default are used. In this study it is investigated whether the scorecard can be improved by using reject inference and thereby include the characteristics of the rejected population when refining the scorecard. The reject inference method used is parcelling. Logistic regression is used to estimate probability of default based on applicant characteristics. Two models, one with and one without reject inference, are compared using Gini coefficient and estimated profitability. The results yield that, when comparing the two models, the model with reject inference both has a slightly higher Gini coefficient as well a showing an increase in profitability. Thus, this study suggests that reject inference does improve the predictive power of the scorecard, but in order to verify the results additional testing on a larger calibration set is needed</p>

corrected abstract:
<p>As accurately as possible, creditors wish to determine if a potential debtor will repay the borrowed sum. To achieve this mathematical models known as credit scorecards quantifying the risk of default are used. In this study it is investigated whether the scorecard can be improved by using reject inference and thereby include the characteristics of the rejected population when refining the scorecard. The reject inference method used is parcelling. Logistic regression is used to estimate probability of default based on applicant characteristics. Two models, one with and one without reject inference, are compared using Gini coefficient and estimated profitability. The results yield that, when comparing the two models, the model with reject inference both has a slightly higher Gini coefficient as well as showing an increase in profitability. Thus, this study suggests that reject inference does improve the predictive power of the scorecard, but in order to verify the results additional testing on a larger calibration set is needed.</p>

Note - removed "<p>Abstract</p><p> </p>" and corrected "a showing" to "as showing"
----------------------------------------------------------------------
In diva2:1334832 
abstract is: 
<p>This paper will present a concrete study of representations of the special orthogonal group, SO(3), a group of great importance in physics. Specifically, we will study a natural representation of SO(3) in the space of polynomials in three variables with complex coefficients, C[x, y, z]. We will find that this special case provides all irreducible representations of SO(3), and also present some corollaries about spherical harmonics. Some preparatory theory regarding abstract algebra, linear algebra, topology and measure theory will also be presented.</p><p> </p>

corrected abstract:
<p>This paper will present a concrete study of representations of the special orthogonal group, <em>SO(3)</em>, a group of great importance in physics. Specifically, we will study a natural representation of <em>SO(3)</em> in the space of polynomials in three variables with complex coefficients, ℂ[𝑥, 𝑦, 𝑧]. We will find that this special case provides all irreducible representations of <em>SO(3)</em>, and also present some corollaries about spherical harmonics. Some preparatory theory regarding abstract algebra, linear algebra, topology and measure theory will also be presented.</p>

Note changes in mathematical elements and removed the empty paragraph
----------------------------------------------------------------------
In diva2:1462397 
abstract is: 
<p>The space industry and the technological developments regarding space exploration hasn’t been this popular since the first moon landing. The privatization of space exploration and the vertical landing rockets made rocket science mainstream again. While being able to reuse rockets is efficient both in terms of profitability and popularity, these developments are still in their early stages. Vertical landing has challenges that, if neglected, can cause disastrous consequences. The existing studies on the matter usually don’t account for aerodynamics forces and corresponding controls, which results in higher fuel consumption thus lessening the economical benefits of vertical landing. Similar problems have been tackled in studies not regarding booster landings but regarding planetary landings. And while multiple solutions have been proposed for these problems regarding planetary landings, the fact that the reinforcement learning concepts work well and provide robustness made them a valid candidate for applying to booster landings. In this study, we focus on developing a vertical booster descent guidance and control law that’s robust by applying reinforcement learning concept. Since reinforcement learning method that is chosen requires solving Optimal Control Problems (OCP), we also designed and developed an OCP solver software. The robustness of resulting hybrid guidance and control policy will be examined against various different uncertainties including but not limited to wind, delay and aerodynamic uncertainty.</p><p> </p>

corrected abstract:
<p>The space industry and the technological developments regarding space exploration hasn’t been this popular since the first moon landing. The privatization of space exploration and the vertical landing rockets made rocket science mainstream again. While being able to re-use rockets is efficient both in terms of profitability and popularity, these developments are still in their early stages. Vertical landing has challenges that, if neglected, can cause disastrous consequences. The existing studies on the matter usually don’t account for aerodynamics forces and corresponding controls, which results in higher fuel consumption thus lessening the economical benefits of vertical landing.</p><p>Similar problems have been tackled in studies not regarding booster landings but regarding planetary landings. And while multiple solutions have been proposed for these problems regarding planetary landings, the fact that the reinforcement learning concepts work well and provide robustness made them a valid candidate for applying to booster landings. In this study, we focus on developing a vertical booster descent guidance and control law that’s robust by applying reinforcement learning concept. Since reinforcement learning method that is chosen requires solving Optimal Control Problems (OCP), we also designed and developed an OCP solver software. The robustness of resulting hybrid guidance and control policy will be examined against various different uncertainties including but not limited to wind, delay and aerodynamic uncertainty.</p>

Note - removed the empty paragraph, inserted missing paragraph break, and inserted missing hyphen
----------------------------------------------------------------------
In diva2:1776588 
abstract is: 
<p>The objective of robust portfolio optimization is to find a way to allocate capital to some financial assets such that portfolio return is maximized in the worst-case scenario, which is desirable for investors with a low tolerance for risk. This study aims to apply the robust approach to asset allocation based on 30 of the biggest stocks on the Stockholm Stock Exchange. Three models with different constraints on portfolio return and variance are obtained and solved using the Gurobi Optimizer. The result of any one of the models could be proposed as a low-risk portfolio. The choice between the models is a trade-off between higher expected return and lower variance, and it depends on the individual preferences of the investor.</p><p> </p>

corrected abstract:
<p>The objective of robust portfolio optimization is to find a way to allocate capital to some financial assets such that portfolio return is maximized in the worst-case scenario, which is desirable for investors with a low tolerance for risk. This study aims to apply the robust approach to asset allocation based on 30 of the biggest stocks on the Stockholm Stock Exchange. Three models with different constraints on portfolio return and variance are obtained and solved using the Gurobi Optimizer. The result of any one of the models could be proposed as a low-risk portfolio. The choice between the models is a trade-off between higher expected return and lower variance, and it depends on the individual preferences of the investor.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:902529 
abstract is: 
<p>The topic of this thesis is the implementation of rapid branching to find an integer solution for the train timetabling problem. The techniques that rapid branching are based on are presented. The important aspect of rapid branching are discussed and then the algorithm is applied to some artificial problems. It is shown that rapid branching can be both faster and slower than a standard integer solver depending on the problem instance. For the most realistic set of the examined instances, rapid branching turned out to be faster than the standard integer solver and produce satisficingly high quality solutions.</p><p> </p>

corrected abstract:
<p>The topic of this thesis is the implementation of rapid branching to find an integer solution for the train timetabling problem. The techniques that rapid branching are based on are presented. The important aspects of rapid branching are discussed and then the algorithm is applied to some artificial problems. It is shown that rapid branching can be both faster and slower than a standard integer solver depending on the problem instance. For the most realistic set of the examined instances, rapid branching turned out to be faster than the standard integer solver and produce satisficingly high quality solutions.</p>

Note - removed the empty paragraph and added the "s" to "aspects"
----------------------------------------------------------------------
In diva2:1219083 
abstract is: 
<p>The purpose of this project was to implement an inverse diffusion algorithm to locate the sources of an emitted substance. This algorithm has yielded successful results when applied to biological cell detection, and it has been suggested that the run time could be greatly reduced if adaptions for a computation graph framework are made. This would automate calculations of gradients and allow for faster execution on a graphics processing unit.</p><p>The algorithm implementation was realized in TensorFlow, which is primarily a machine learning oriented programming library. Computer-generated biological test images were then used to evaluate the performance using regular image analysis software and accuracy metrics.</p><p>Comparisons reveal that the TensorFlow implementation of the algorithm can match the accuracy metrics of traditional implementations of the same algorithm. Viewed in a broader scope this serves as an example to highlight the possibility of using computation graph frameworks to solve large scale optimization problems, and more specifically inverse problems.</p><p> </p>

corrected abstract:
<p>The purpose of this project was to implement an inverse diffusion algorithm to locate the sources of an emitted substance. This algorithm has yielded successful results when applied to biological cell detection, and it has been suggested that the run time could be greatly reduced if adaptions for a computation graph framework are made. This would automate calculations of gradients and allow for faster execution on a graphics processing unit.</p><p>The algorithm implementation was realized in TensorFlow, which is primarily a machine learning oriented programming library. Computer-generated biological test images were then used to evaluate the performance using regular image analysis software and accuracy metrics.</p><p>Comparisons reveal that the TensorFlow implementation of the algorithm can match the accuracy metrics of traditional implementations of the same algorithm. Viewed in a broader scope this serves as an example to highlight the possibility of using computation graph frameworks to solve large scale optimization problems, and more specifically inverse problems.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1570353 
abstract is: 
<p>Climate is a tremendously complex topic, affecting many aspects of human activity and constantly changing. Defining some structures and rules for how it works is thereof of the utmost importance even though it might only cover a small part of the complexity. Cluster analysis is a tool developed in data analysis that is able to categorize data into groups of similar type. In this paper data from the Swedish Meteorological and Hydrological Institute (SMHI) is clustered to find a partitioning. The cluster analysis used is called Spectral clustering which is a family of methods making use of the spectral properties of graphs. Concrete results over different groupings of climate over Sweden were found.</p><p> </p>

corrected abstract:
<p>Climate is a tremendously complex topic, affecting many aspects of human activity and constantly changing. Defining some structures and rules for how it works is thereof of the utmost importance even though it might only cover a small part of the complexity. Cluster analysis is a tool developed in data analysis that is able to categorize data into groups of similar type. In this paper data from the Swedish Meteorological and Hydrological Institute (SMHI) is clustered to find a partitioning. The cluster analysis used is called Spectral clustering which is a family of methods making use of the spectral properties of graphs. Concrete results over different groupings of climate over Sweden were found.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1742865 
abstract is: 
<p>As cyber-physical systems become increasingly complex, the management and verification of requirements during design is essential. A new language called CRML (Common Requirement Modelling Language) has been created during the European EMBrACE project to formalize realistic dynamical requirements, but a method for representing these requirements and a framework for using them as a design aid must be defined to ease appropriation by engineers. This report therefore presents a draft of a new graphical method for representing system requirements and evaluating different architectures, extending classical Systems Engineering (SE) approaches to the complexity of dynamic physical systems. Once the method completed, the verification and validation process can then be carried out through the simulation of solution models with CRML models. Solution models state how the system will behave while the CRML models state whether this behaviour is compliant with the requirements. The new graphical approach has been applied to two energy systems found in the nuclear industry. This report presents the first promising results as well as some perspectives to consolidate it.</p><p> </p>

corrected abstract:
<p>As cyber-physical systems become increasingly complex, the management and verification of requirements during design is essential. A new language called CRML (Common Requirement Modelling Language) has been created during the European EMBrACE project to formalize realistic dynamical requirements, but a method for representing these requirements and a framework for using them as a design aid must be defined to ease appropriation by engineers. This report therefore presents a draft of a new graphical method for representing system requirements and evaluating different architectures, extending classical Systems Engineering (SE) approaches to the complexity of dynamic physical systems. Once the method completed, the verification and validation process can then be carried out through the simulation of solution models with CRML models. Solution models state how the system will behave while the CRML models state whether this behaviour is compliant with the requirements. The new graphical approach has been applied to two energy systems found in the nuclear industry. This report presents the first promising results as well as some perspectives to consolidate it.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:558590 
abstract is: 
<p>It is of great importance to find an analytical copula that will represent the empirical lower tail dependence. In this study, the pairwise empirical copula are estimated using data of the S&amp;P 500 stocks during the period 2007-2010.Different optimization methods and measures of dependence have been used to fit Gaussian, t and Clayton copula to the empirical copulas, in order to represent the empirical lower tail dependence. These different measures of dependence and optimization methods with their restrictions, point at different analytical copulas being optimal. In this study the t copula with 5 degrees of freedom is giving the most fulfilling result, when it comes to representing lower tail dependence. The t copula with 5 degrees of freedom gives the best representation of empirical lower tail dependence, whether one uses the 'Empirical maximum likelihood estimator', or 'Equal Ƭ' as an approach.</p><p> </p>

corrected abstract:
<p>It is of great importance to find an analytical copula that will represent the empirical lower tail dependence. In this study, the pairwise empirical copula are estimated using data of the S&amp;P 500 stocks during the period 2007-2010. Different optimization methods and measures of dependence have been used to fit Gaussian, 𝑡 and Clayton copula to the empirical copulas, in order to represent the empirical lower tail dependence. These different measures of dependence and optimization methods with their restrictions, point at different analytical copulas being optimal. In this study the 𝑡 copula with 5 degrees of freedom is giving the most fulfilling result, when it comes to representing lower tail dependence. The 𝑡 copula with 5 degrees of freedom gives the best representation of empirical lower tail dependence, whether one uses the ’Empirical maximum likelihood estimator’, or ’Equal &#x1D70F;’ as an approach.</p>

Note - removed the empty paragraph, fixed quote marks to match original, and fixed the math symbols
----------------------------------------------------------------------
In diva2:813868 
abstract is: 
<p>Feasibly computable analytic solutions for systems of many particles in fluid dynamics and electrostatics are few and far-between. Simulations and numerical approximations are essential to studying these systems. This is commonly done without directly calculating the interacting field between particles. In this report a method utilizing the spectral accuracy of the Fourier transform is studied to calculate particle velocities via the surrounding fluid velocity field. The method is applied to a periodic cube of a suspension of small, spherical particles sedimenting in a fluid affected by gravity, in an attempt to mimic the behaviour of a similar infinite system. Results for a few particles qualitatively relate the shape of the solution to the choice of interpolation between particles to grid and quantitatively maps some convergence properties of a certain class of interpolating functions, cardinal B-splines. The properties of the method on the periodic system are also examined and compared to a similar study of the infinite system for many, ~1000, particles.</p><p> </p>

corrected abstract:
<p>Feasibly computable analytic solutions for systems of many particles in fluid dynamics and electrostatics are few and far-between. Simulations and numerical approximations are essential to studying these systems. This is commonly done without directly calculating the interacting field between particles. In this report a method utilizing the spectral accuracy of the Fourier transform is studied to calculate particle velocities via the surrounding fluid velocity field. The method is applied to a periodic cube of a suspension of small, spherical particles sedimenting in a fluid affected by gravity, in an attempt to mimic the behaviour of a similar infinite system. Results for a few particles qualitatively relate the shape of the solution to the choice of interpolation between particles to grid and quantitatively maps some convergence properties of a certain class of interpolating functions, cardinal B-splines. The properties of the method on the periodic system are also examined and compared to a similar study of the infinite system for many, ∼ 1000, particles.</p>
----------------------------------------------------------------------
In diva2:1572684 
abstract is: 
<p>Aortic diseases are a common and fatal health issue, regarding various conditions targeting the aorta. The arterial wall consists of two proteins; collagen and elastin (fibers), and they will contribute to the elasticity and strength of the wall. In recent times there has been a growing interest in studying the microstructure of the aortic tissues because it is believed that changes in the amount and/or the construction of the fibers will result in mechanical as well as functional changes that are associated with these heart conditions. Therefore, a better understanding of collagen and its load-carrying properties in vascular tissue is needed for others to be able to develop new designs of cardiovascular medical devices that may help the medical field to find other therapies and treatments for patients with aortic diseases. </p><p>This bachelor's degree thesis is based on a literature study, experimental testing, finite element method (FEM) analysis, and a microscopy study. To get a broader understanding of the arterial wall, the collagen, and its mechanical properties a literature study was conducted. The experimental testing was made with tensile testing equipment called CellScales BioTester 5000 uniaxial bio-testings and the test specimen used was porcine aorta from a local abattoir. The data obtained from the testing was put in MATLAB to produce graphs visualizing different data, such as; stresses, strain, forces and stiffness. Then a FEM-simulation was made by Christopher Miller and the data and images obtained from the analysis were used to compare with the results from MATLAB. Furthermore, the ruptured test specimen after the tensile testing was sent to Karolinska Institutionen (KI) for a microscope study.</p><p>The results from MATLAB were used to receive information regarding the material properties, to calculate the stiffness as well as the strain at the rupture. There were two samples made, sample 7 and sample 10, and the data from the MATLAB graphs were used to determine where the rupture occurred. For sample 7 this occurs when the force is 7.29 [N], elongation is 22.93 x 10^(-3) [m] and stress is 1210 [kPa], for sample 10 the rupture of sample 10 occurred when the force is 6.65 [N], elongation is 17.4 x 10^(-3) [m] and stress is 606 [kPa]. The FEM-simulation showed where the maximum deformation takes place, which was in the middle of our tissues. From the microscope images from KI, the accuracy of the FEM-model could be seen.</p><p> </p>

corrected abstract:
<p>A common and fatal health issue in the world is aortic diseases, which are various conditions targeting the aorta. To get a better understanding of these types of diseases there has been a growing interest in studying the microstructure and mechanical behavior of the aortic tissue, because it is believed that changes in the amount and/or the construction of the fibers (collagen and elastin) will result in mechanical as well as functional changes that can be associated with these types of different vascular conditions.  Therefore, a better understanding of collagen and its load-carrying properties in vascular tissues is needed for others to be able to develop new designs of cardiovascular medical devices that will proceed to help the medical field to find other therapies and treatments for patients with aortic diseases.</p><p>This bachelor’s degree thesis explores the role of collagen in the vessel wall by a literature study, experimental testing, Finite Element Method (FEM) as well as a microscopy study. A literature study was conducted to get a broader understanding of the arterial wall, collagen, and its mechanical properties.  The experimental testing was performed with tensile testing equipment called CellScale BioTester 5000 and the test specimen used was a porcine aorta bought from a local abattoir. The data obtained from the tensile testing was put into MATLAB for post-processing to receive graphs that visualized different data, such as; stresses, strain, forces, and stiffness. Then a FEM-simulation was executed and the data and images obtained from the analysis were used to compare with the results from the post-processing. Furthermore, the ruptured test specimen from the tensile testing was sent to Karolinska Institutionen (KI) for a microscopy study.</p><p>The results from the post-processing were used to receive information regarding the material properties, to calculate the stiffness as well as the strain at the rupture. The data from the post-processing graphs for Sample 10 was used to determine where the rupture occurred which was when the force was 6.65 [N], the elongation 17.4 [mm] and stress 606 [kPa]. The stiffness in the tissue sample showed a decrease with the ramps, cycles, and stretch. This could be explained through that the tissue’s resistance towards elastic deformation decreases.</p><p>The FEM-simulation (like the tensile test and the microscopic images) show that the biggest and considerable deformation takes place at the smallest diameter. The FEM-graphs like the physical tests exhibit a relaxation even without an increased displacement. The graphs also exhibit that the FEManalysis relates to the physical tests and with the aid of the FEM-analysis it is possible to see where the prominent stresses are located in the tissue and where the accuracy of the FEM-model is confirmed by the microscopic pictures. From the microscope images from KI, the rupture of collagen could be visualized.</p><p>The post-processing results of the stiffness and stress-strain curve, the FEManalysis and the microscope observations were compared and show a clear qualitative agreement, which indicates that the method is suitable for future development.</p>

Note major differences in wording between DiVA and original - the above is based on the original
also removed the empty paragraph
----------------------------------------------------------------------
In diva2:1165816 
abstract is: 
<p>In 1983 Grothendieck wrote a letter to Faltings, [Gro83], outlining what is today known as the anabelian conjectures. These conjectures concern the possibility to reconstruct curves and schemes from their étale fundamental group. Although Faltings never replied to the letter, his student Mochizuki began working on it. A major achievement by Mochizuki and Tamagawa was to prove several important versions of these conjectures.</p><p>In this thesis we will first introduce Grothendieck’s Galois theory with the aim to define the étale fundamental group and formulate Mochizuki’s result. After recalling some necessary homotopy theory, we will introduce the étale homotopy type, which is an extension of the étale fundamental group developed by Artin, Mazur and Friedlander. This is done in order to describe some recentwork by Schmidt and Stix that improves on the results of Mochizuki and Tamagawa by extending them from étale fundamental groups to étale homotopy types of certain (possibly higher-dimensional) schemes.</p><p> </p>

corrected abstract:
<p>In 1983 Grothendieck wrote a letter to Faltings, [Gro83], outlining what is today known as the anabelian conjectures. These conjectures concern the possibility to reconstruct curves and schemes from their étale fundamental group. Although Faltings never replied to the letter, his student Mochizuki began working on it. A major achievement by Mochizuki and Tamagawa was to prove several important versions of these conjectures.</p><p>In this thesis we will first introduce Grothendieck’s Galois theory with the aim to define the étale fundamental group and formulate Mochizuki’s result. After recalling some necessary homotopy theory, we will introduce the étale homotopy type, which is an extension of the étale fundamental group developed by Artin, Mazur and Friedlander. This is done in order to describe some recent work by Schmidt and Stix that improves on the results of Mochizuki and Tamagawa by extending them from étale fundamental groups to étale homotopy types of certain (possibly higher-dimensional) schemes.</p>
----------------------------------------------------------------------
In diva2:1880323 
abstract is: 
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p><p> </p>

corrected abstract:
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1571248 
abstract is: 
<p>This thesis aims to investigate whether a fiber Bragg grating array (FBGA) can be designed to reduce the bandwidth of light from a 1.5 µm fiber laser while the distances between the fiber Bragg gratings are in the 10-20 cm range. The results from simulations show that for FBGA with multiple cavities, small changes in cavity length impact transmission drastically. For FBGA consisting of only two fiber Bragg gratings, the quality of the laser cavity's spectrum is improved, as the peak width is reduced and stability is increased. The conclusions of the experiments and the simulations are that FBGA with a single cavity gives best performance. For future studies, if higher computational power and production capabilities are available, an in depth analysis of a multi cavity FBGA could be conducted.</p><p> </p>

corrected abstract:
<p>This thesis aims to investigate whether a fiber Bragg grating array (FBGA) can be designed to reduce the bandwidth of light from a 1.5 µm fiber laser while the distances between the fiber Bragg gratings are in the 10-20 cm range. The results from simulations show that for FBGA with multiple cavities, small changes in cavity length impact transmission drastically. For FBGA consisting of only two fiber Bragg gratings, the quality of the laser cavity's spectrum is improved, as the peak width is reduced and stability is increased. The conclusions of the experiments and the simulations are that FBGA with a single cavity gives best performance. For future studies, if higher computational power and production capabilities are available, an in depth analysis of a multi cavity FBGA could be conducted.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1779454 
abstract is: 
<p>The dopamine receptor is one of the main therapeutic targets for neurological disorders, such as Parkinson’s disease and schizophrenia. Although dopamine and adrenaline are structurally similar and both bind to the D2 dopamine receptor, they activate the receptor differently. This study aims to investigate the mechanism by which dopamine and adrenaline trigger conformational changes resulting in the receptor activation. To achieve this, two data sets derived from molecular dynamic (MD) simulations were analyzed, in which one integrates D2 ensembles with and without ligand bound and the other includes D2 ensembles in the presence of the dopamine and adrenaline bound. Both supervised and unsupervised machine learning methods were applied to interpret the high dimensional data provided by the MD simulations. The results obtained from these methods provided the important residues in the D2 dopamine receptor, suggesting they constitute different conformations and interactions with surrounding residues under various conditions. The NPxxY motif in the bottom of transmembrane helix 7 was identified as the most important to distinguish different states induced by distinct ligands binding, according to the supervised methods. Meanwhile, the unsupervised methods showed a general trend of high importance around certain sections, such as the intracellular portion of the transmembrane helix number 6. Taken together, our findings lay the groundwork for the understanding of molecular activation mechanism of D2 dopamine receptor modulated by different ligands.</p><p> </p>

corrected abstract:
<p>The dopamine receptor is one of the main therapeutic targets for neurological disorders, such as Parkinson’s disease and schizophrenia. Although dopamine and adrenaline are structurally similar and both bind to the D<sub>2</sub> dopamine receptor, they activate the receptor differently. This study aims to investigate the mechanism by which dopamine and adrenaline trigger conformational changes resulting in the receptor activation. To achieve this, two data sets derived from molecular dynamic (MD) simulations were analyzed, in which one integrates D<sub>2</sub> ensembles with and without ligand bound and the other includes D<sub>2</sub> ensembles in the presence of the dopamine and adrenaline bound. Both supervised and unsupervised machine learning methods were applied to interpret the high dimensional data provided by the MD simulations. The results obtained from these methods provided the important residues in the D<sub>2</sub> dopamine receptor, suggesting they constitute different conformations and interactions with surrounding residues under various conditions. The NPxxY motif in the bottom of transmembrane helix 7 was identified as the most important to distinguish different states induced by distinct ligands binding, according to the supervised methods. Meanwhile, the unsupervised methods showed a general trend of high importance around certain sections, such as the intracellular portion of the transmembrane helix number 6. Taken together, our findings lay the groundwork for the understanding of molecular activation mechanism of D<sub>2</sub> dopamine receptor modulated by different ligands.</p>

Note - removed the empty paragraph and fixed subscripts
----------------------------------------------------------------------
In diva2:1880199 
abstract is: 
<p>This work explores the unsolvability of the general quintic equation through the lens of Galois theory. We begin by providing a historical perspective on the problem. This starts with the solution of the general cubic equation derived by Italian mathematicians. We then move on to Lagrange's insights on the importance of studying the permutations of roots. Finally, we discuss the critical contributions of Évariste Galois, who connected the solvability of polynomials to the properties of permutation groups. Central to our thesis is the introduction and motivation of key concepts such as fields, solvable groups, Galois groups, Galois extensions, and radical extensions.</p><p>We rigorously develop the theory that connects the solvability of a polynomial to the solvability of its Galois group. After developing this theoretical framework, we go on to show that there exist quintic polynomials with Galois groups that are isomorphic to the symmetric group S5. Given that S5 is not a solvable group, we establish that the general quintic polynomial is not solvable by radicals. Our work aims to provide a comprehensive and intuitive understanding of the deep connections between polynomial equations and abstract algebra.</p><p> </p>

corrected abstract:
<p>This work explores the unsolvability of the general quintic equation through the lens of Galois theory. We begin by providing a historical perspective on the problem. This starts with the solution of the general cubic equation derived by Italian mathematicians. We then move on to Lagrange's insights on the importance of studying the permutations of roots. Finally, we discuss the critical contributions of Évariste Galois, who connected the solvability of polynomials to the properties of permutation groups. Central to our thesis is the introduction and motivation of key concepts such as fields, solvable groups, Galois groups, Galois extensions, and radical extensions.</p><p>We rigorously develop the theory that connects the solvability of a polynomial to the solvability of its Galois group. After developing this theoretical framework, we go on to show that there exist quintic polynomials with Galois groups that are isomorphic to the symmetric group 𝑆<sub>5</sub>. Given that 𝑆<sub>5</sub> is not a solvable group, we establish that the general quintic polynomial is not solvable by radicals. Our work aims to provide a comprehensive and intuitive understanding of the deep connections between polynomial equations and abstract algebra.</p>

Note - removed the empty paragraph, fixed the subscripts, and replaced "S" with "𝑆"
----------------------------------------------------------------------
In diva2:1592993 
abstract is: 
<p>Digitization of the energy industry, introduction of smart grids and increasing regulation of electricity consumption metering have resulted in vast amounts of electricity data. This data presents a unique opportunity to understand the electricity usage and to make it more efficient, reducing electricity consumption and carbon emissions. An important initial step in analyzing the data is to identify anomalies.</p><p>In this thesis the problem of anomaly detection in electricity consumption series is addressed using four machine learning methods: density based spatial clustering for applications with noise (DBSCAN), local outlier factor (LOF), isolation forest (iForest) and one-class support vector machine (OC-SVM). In order to evaluate the methods synthetic anomalies were introduced to the electricity consumption series and the methods were then evaluated for the two anomaly types point anomaly and collective anomaly. In addition to electricity consumption data, features describing the prior consumption, outdoor temperature and date-time properties were included in the models. Results indicate that the addition of the temperature feature and the lag features generally impaired anomaly detection performance, while the inclusion of date-time features improved it. Of the four methods, OC-SVM was found to perform the best at detecting point anomalies, while LOF performed the best at detecting collective anomalies. In an attempt to improve the models' detection power the electricity consumption series were de-trended and de-seasonalized and the same experiments were carried out. The models did not perform better on the decomposed series than on the non-decomposed.</p><p> </p>

corrected abstract:
<p>Digitization of the energy industry, introduction of smart grids and increasing regulation of electricity consumption metering have resulted in vast amounts of electricity data. This data presents a unique opportunity to understand the electricity usage and to make it more efficient, reducing electricity consumption and carbon emissions. An important initial step in analyzing the data is to identify anomalies.</p><p>In this thesis the problem of anomaly detection in electricity consumption series is addressed using four machine learning methods: density based spatial clustering for applications with noise (DBSCAN), local outlier factor (LOF), isolation forest (iForest) and one-class support vector machine (OC-SVM). In order to evaluate the methods synthetic anomalies were introduced to the electricity consumption series and the methods were then evaluated for the two anomaly types <em>point anomaly</em> and <em>collective anomaly</em>. In addition to electricity consumption data, features describing the prior consumption, outdoor temperature and date-time properties were included in the models. Results indicate that the addition of the temperature feature and the lag features generally impaired anomaly detection performance, while the inclusion of date-time features improved it. Of the four methods, OC-SVM was found to perform the best at detecting point anomalies, while LOF performed the best at detecting collective anomalies. In an attempt to improve the models' detection power the electricity consumption series were de-trended and de-seasonalized and the same experiments were carried out. The models did not perform better on the decomposed series than on the non-decomposed.</p>

Note - removed the empty paragraph and added italics as per the original
---------------------------------------------------------------------
In diva2:692474 
abstract is: 
<p> </p><p></p><p>With the increases in fuel costs due to the depletion of the world oil reserves and the increase of greenhouse gasses as a consequence to using oil as a fuel many companies are looking to new and innovative ways to power their aircraft. One of these new ways to power an aircraft is using fuel cells powered using hydrogen and oxygen, thus producing nothing but water vapour and small amounts of nitrogen dioxide as well as trace amounts of other emissions. Both Boeing (1) and Politecnico di Torino (2) have shown that it is possible to build an all-electric aircraft powered by fuel cells. Both flights used small, two-seater aircraft and a constant between them was the loss of the co-pilot seat due to weight and lack of space. As this paper will deal with a commercial aircraft a primary concern is the cargo and passenger capacity and whatever impact switching propulsion system has on these. The aircraft used to test the feasibility of these fuel-cells is the SAAB 340 passenger aircraft/airliner chosen for its twin-engine turboprop configuration and generally conventional design. Its engines and fuel tanks were removed and electrical motors, fuel cells and hydrogen tanks were installed, all the while taking care not to move its centre of gravity too much. Based upon the calculations performed the new aircraft appears to be airworthy though it has a very low rate of climb and because of this an extremely short range. The vehicles passenger and cargo carrying capacity has been severely diminished due to the weight and size of the new components. Other parameters have also decreased, such as speeds and power outputs from the motors. Despite performance reductions the aircraft seem to be able to fulfil the demands placed upon it though carrying capacity appears to be severely diminished.</p>

corrected abstract:
<p>With the increases in fuel costs due to the depletion of the world oil reserves and the increase of greenhouse gasses as a consequence to using oil as a fuel many companies are looking to new and innovative ways to power their aircraft. One of these new ways to power an aircraft is using fuel cells powered using hydrogen and oxygen, thus producing nothing but water vapour and small amounts of nitrogen dioxide as well as trace amounts of other emissions. Both Boeing (1) and Politecnico di Torino (2) have shown that it is possible to build an all-electric aircraft powered by fuel cells. Both flights used small, two-seater aircraft and a constant between them was the loss of the co-pilot seat due to weight and lack of space. As this paper will deal with a commercial aircraft a primary concern is the cargo and passenger capacity and whatever impact switching propulsion system has on these. The aircraft used to test the feasibility of these fuel-cells is the SAAB 340 passenger aircraft/airliner chosen for its twin-engine turboprop configuration and generally conventional design. Its engines and fuel tanks were removed and electrical motors, fuel cells and hydrogen tanks were installed, all the while taking care not to move its centre of gravity too much. Based upon the calculations performed the new aircraft appears to be airworthy though it has a very low rate of climb and because of this an extremely short range. The vehicles passenger and cargo carrying capacity has been severely diminished due to the weight and size of the new components. Other parameters have also decreased, such as speeds and power outputs from the motors. Despite performance reductions the aircraft seem to be able to fulfil the demands placed upon it though carrying capacity appears to be severely diminished.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1431653 
abstract is: 
<p>This thesis tackles the problem of predicting the collision risk for vehicles driving in complex traffic scenes for a few seconds into the future. The method is based on previous research using dynamic Bayesian networks to represent the state of the system.</p><p>Common risk prediction methods are often categorized into three different groups depending on their abstraction level. The most complex of these are interaction-aware models which take driver interactions into account. These models often suffer from high computational complexity which is a key limitation in practical use. The model studied in this work takes interactions between drivers into account by considering driver intentions and the traffic rules in the scene.</p><p>The state of the traffic scene used in the model contains the physical state of vehicles, the intentions of drivers and the expected behaviour of drivers according to the traffic rules. To allow for real-time risk assessment, an approximate inference of the state given the noisy sensor measurements is done using sequential importance resampling. Two different measures of risk are studied. The first is based on driver intentions not matching the expected maneuver, which in turn could lead to a dangerous situation. The second measure is based on a trajectory prediction step and uses the two measures time to collision (TTC) and time to critical collision probability (TTCCP).</p><p>The implemented model can be applied in complex traffic scenarios with numerous participants. In this work, we focus on intersection and roundabout scenarios. The model is tested on simulated and real data from these scenarios. %Simulations of these scenarios is used to test the model. In these qualitative tests, the model was able to correctly identify collisions a few seconds before they occur and is also able to avoid false positives by detecting the vehicles that will give way.</p><p> </p>

corrected abstract:
<p>This thesis tackles the problem of predicting the collision risk for vehicles driving in complex traffic scenes for a few seconds into the future. The method is based on previous research using dynamic Bayesian networks to represent the state of the system.</p><p>Common risk prediction methods are often categorized into three different groups depending on their abstraction level. The most complex of these are interaction-aware models which take driver interactions into account. These models often suffer from high computational complexity which is a key limitation in practical use. The model studied in this work takes interactions between drivers into account by considering driver intentions and the traffic rules in the scene.</p><p>The state of the traffic scene used in the model contains the physical state of vehicles, the intentions of drivers and the expected behaviour of drivers according to the traffic rules. To allow for real-time risk assessment, an approximate inference of the state given the noisy sensor measurements is done using sequential importance resampling. Two different measures of risk are studied. The first is based on driver intentions not matching the expected maneuver, which in turn could lead to a dangerous situation. The second measure is based on a trajectory prediction step and uses the two measures <em>time to collision</em> (TTC) and <em>time to critical collision probability</em> (TTCCP).</p><p>The implemented model can be applied in complex traffic scenarios with numerous participants. In this work, we focus on intersection and roundabout scenarios. The model is tested on simulated and real data from these scenarios. In these qualitative tests, the model was able to correctly identify collisions a few seconds before they occur and is also able to avoid false positives by detecting the vehicles that will give way.</p>

Note - removed the empty paragraph, added itliacs as per the original, and removed "%Simulations of these scenarios is used to test the model." - a sentence that was commented out in the LaTeX.
----------------------------------------------------------------------
In diva2:1218276 
abstract is: 
<p> </p><p>In this thesis, image classification of difficult data through the use of machine learning algorithms is evaluated using a Kernel Machine. When trying to classify objects in im- ages in real world situations the object in question might be behind some form of transparent obstruction or barrier. By implementing a simple machine learning algorithm this thesis aims to provide an approximate lower bound for the performance of machine learning algorithms in such circumstances.</p><p>Results show that machine learning is a viable option even though performance decrease with barrier complexity and thickness. In the best case performance dropped less than one percentage point when using a simple barrier compared to using no barrier and allowing the algorithm to train on images with objects behind said barrier. Performance is much worse when not allowing the algorithm to train on images with barriers. Furthermore, performance seems to be largely independent on image size despite the loss of information associated with introducing barriers.</p>

corrected abstract:
<p>In this thesis, image classification of difficult data through the use of machine learning algorithms is evaluated using a Kernel Machine. When trying to classify objects in images in real world situations the object in question might be behind some form of transparent obstruction or barrier. By implementing a simple machine learning algorithm this thesis aims to provide an approximate lower bound for the performance of machine learning algorithms in such circumstances.</p><p>Results show that machine learning is a viable option even though performance decrease with barrier complexity and thickness. In the best case performance dropped less than one percentage point when using a simple barrier compared to using no barrier and allowing the algorithm to train on images with objects behind said barrier. Performance is much worse when not allowing the algorithm to train on images with barriers. Furthermore, performance seems to be largely independent on image size despite the loss of information associated with introducing barriers.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1450330 
abstract is: 
<p>The purpose of this report was to investigate whether the weight of Marsblade's new steel runner could be reduced without making the blade too weak and still retain its properties. Marsblade introduces a unique concept with a moving steel runner in the holder which creates many challenges for the geometry as it has to cope with stresses and at the same time fulfill it's unique function. However, the unique geometry contributes to increased weight and a weight reduction are therefore desirable, in order to make the blade more competitive with other brands on the market. Marsblade's steel runner has thus been compared to a conventional steel runner which is considered to meet the requirements. When analyzing the steel runners, FEA analyses have been performed to investigate their strength properties. A carefully made trade off between strength and reduced weight resulted in the new weight optimized steel runner weighs 13 grams less than the original one, this weight loss means that the weight has decreased by 8.8%. The new geometry is optimized in such a way that the occurrence of stress concentrations at dimensional transitions has decreased.</p><p> </p>

corrected abstract:
<p>The purpose of this report was to investigate whether the weight of Marsblade's new steel runner could be reduced without making the blade too weak and still retain its properties. Marsblade introduces a unique concept with a moving steel runner in the holder which creates many challenges for the geometry as it has to cope with stresses and at the same time fulfill it's unique function. However, the unique geometry contributes to increased weight and a weight reduction are therefore desirable, in order to make the blade more competitive with other brands on the market. Marsblade's steel runner has thus been compared to a conventional steel runner which is considered to meet the requirements. When analyzing the steel runners, FEA analyses have been performed to investigate their strength properties. A carefully made trade off between strength and reduced weight resulted in the new weight optimized steel runner weighs 13 grams less than the original one, this weight loss means that the weight has decreased by 8.8%. The new geometry is optimized in such a way that the occurrence of stress concentrations at dimensional transitions has decreased.</p>
----------------------------------------------------------------------
In diva2:1832663 
abstract is: 
<p>This study aims to investigate the relationship between multiple air pollution and different vehicle variables, such as vehicle year, fuel type and vehicle type, on Hornsgatan in Stockholm. The study intends to answer which factors have the greatest impact on air quality. The implementation is based on the two machine learning algorithms Random Forest and Support Vector Regression, which are compared based on R^2 and RMSE. The models created with Random Forest outperform Support Vector Regression for the various air pollutants. The best performing model was the carbon monoxide model which had an R^2-value of 99.7%. The model that gave predictions with the lowest R^2-value, 68.4%, was the model for nitrogen dioxide. Overall, the results were good in relation to previous studies. With regards to these models, the impact of variables and different measures that can be introduced in the City of Stockholm and on Hornsgatan to improve air quality are discussed.</p>

corrected abstract:
<p>This study aims to investigate the relationship between multiple air pollution and different vehicle variables, such as vehicle year, fuel type and vehicle type, on Hornsgatan in Stockholm. The study intends to answer which factors have the greatest impact on air quality. The implementation is based on the two machine learning algorithms Random Forest and Support Vector Regression, which are compared based on R<sup>2</sup> and RMSE. The models created with Random Forest outperform Support Vector Regression for the various air pollutants. The best performing model was the carbon monoxide model which had an R<sup>2</sup>-value of 99.7%. The model that gave predictions with the lowest R<sup>2</sup>-value, 68.4%, was the model for nitrogen dioxide. Overall, the results were good in relation to previous studies. With regards to these models, the impact of variables and different measures that can be introduced in the City of Stockholm and on Hornsgatan to improve air quality are discussed.</p>

Note - removed the empty paragraph and fixed superscripts
----------------------------------------------------------------------
In diva2:1788396 
abstract is: 
<p>It is important to understand the aerodynamic properties of tensioned cables (e.g. used in suspension bridges and yacht riggings), both for drag reduction and vibrational suppression purposes. In this study, the cross-sectional shape and surface structure of solid cables were investigated in order to improve the performance of sailing racing yachts. The apparent wind angle range 15-60° was identified as the most important for drag reduction. Thereafter, the aerodynamic properties of different shapes and surfaces were investigated in the Reynolds number range 5 x 10^3 ≤ Re ≤ 4 x 10^4, by performing computational fluid dynamics simulations and wind tunnel tests (the aerodynamic forces were measured using load cells). No significant effect of changing the surface roughness could be found for the investigated Reynolds number range. The results were compared to literature values for validation.</p><p>Elliptical shapes with a fineness ratio between 1:1-3:1, together with three complex shapes, were tested. It could be shown that the largest performance gain was obtained for cables with more sail-like aerodynamic properties (for apparent wind angles below 90° a large lift/drag ratio is sought). This study was performed in collaboration with Carbo-Link AG, as an outlook, the manufacturability of carbon fiber reinforced polymer cables in the most aerodynamically efficient shape was explored.</p>

corrected abstract:
<p>It is important to understand the aerodynamic properties of tensioned cables (e.g. used in suspension bridges and yacht riggings), both for drag reduction and vibrational suppression purposes. In this study, the cross-sectional shape and surface structure of solid cables were investigated in order to improve the performance of sailing racing yachts. The apparent wind angle range 15-60° was identified as the most important for drag reduction. Thereafter, the aerodynamic properties of different shapes and surfaces were investigated in the Reynolds number range 5 x 10<sup>3</sup> ≤ 𝑅𝑒 ≤ 4 x 10<<sup>4</sup>, by performing computational fluid dynamics simulations and wind tunnel tests (the aerodynamic forces were measured using load cells). No significant effect of changing the surface roughness could be found for the investigated Reynolds number range. The results were compared to literature values for validation.</p><p>Elliptical shapes with a fineness ratio between 1:1-3:1, together with three complex shapes, were tested. It could be shown that the largest performance gain was obtained for cables with more sail-like aerodynamic properties (for apparent wind angles below 90° a large lift/drag ratio is sought). This study was performed in collaboration with Carbo-Link AG, as an outlook, the manufacturability of carbon fiber reinforced polymer cables in the most aerodynamically efficient shape was explored.</p>
----------------------------------------------------------------------
In diva2:1282825 
abstract is: 
<p>In this thesis, we study the non-symmetric Macdonald polynomials E_λ (x;q,t) at t=0 from a combinatorial point of view, using the combinatorial formula found by J. Haglund, M. Haiman, and N. Loehr. Our primary focus is when λ is a partition. We summarize the known theory about this specialization and prove some new results related to this combinatorial formula. We also define the cyclic sieving phenomenon (CSP). For rectangular λ, we present an instance of cyclic sieving with E_λ (1,q,q^2,...,q^(k-1);1,0) as CSP-polynomial. We also conjecture another instance of CSP with E_λ (1,1,...,1;q,0) as CSP-polynomial. This conjecture generalizes a previously known CSP-triple. Furthermore, we prove this conjecture in the case when is λ an m×2 diagram.</p>

corrected abstract:
<p>In this thesis, we study the non-symmetric Macdonald polynomials E<sub>λ</sub>(𝑥; 𝑞, 𝑡) at 𝑡=0 from a combinatorial point of view, using the combinatorial formula found by J. Haglund, M. Haiman, and N. Loehr. Our primary focus is when λ is a partition. We summarize the known theory about this specialization and prove some new results related to this combinatorial formula. We also define the cyclic sieving phenomenon (CSP). For rectangular λ, we present an instance of cyclic sieving with E<sub>λ</sub>(1,𝑞,𝑞<sup>2</sup>,...,𝑞<sup>𝑘-1</sup>; 1,0) as CSP-polynomial. We also conjecture another instance of CSP with E<sub>λ</sub> (1,1,...,1; 𝑞, 0) as CSP-polynomial. This conjecture generalizes a previously known CSP-triple. Furthermore, we prove this conjecture in the case when is λ an 𝑚 × 2 diagram.</p>
----------------------------------------------------------------------
In diva2:1738124 
abstract is: 
<p>The treatment of spatial infinity is one of the remaining major open problems in the theory of isolated self-gravitating systems. Especially when one wants to model scattering of gravitational radiation in spacetime. In this thesis the conformal theory is used to study simple electromagnetic fields, close to spatial infinity. In particular, the trajectory of the moving Coulomb field is studied in compactified Minkowski space. In the formalism, introduced by Penrose, Minkowski metric is rescaled g = Ω^2η to Einstein’s universe, R × S^3. A dual particle, passing through spatial infinity in Einstein’s Universe, emerges from the conformally extended Coulomb field. The particle pair moves antipodally with respect to the retarded and advanced directions. Furthermore, a more recent treatment of spatial infinity, proposed by Friedrich, is studied in conjunction with the electromagnetic field. In this treatment, spatial infinity is blown-up to a cylinder that is a total characteristic of the spacetime. The Newman-Penrose formalism is central to the theory and is used here to rewrite Maxwell’s equations. The blow-up is linked to the sigma-process, a process used to treat singularities in the theory of differential equations. Boosted space-like curves are linked to points on the cylinder via a bijective function. The Newman-Penrose scalars are studied on the cylinder. Finally, a global treatment of spacetime, using global coordinates for adS2 ×S^2, is proposed for further study of spatial infinity in e.g. numerical codes and Newman-Penrose formalism.</p>

corrected abstract:
<p>The treatment of spatial infinity is one of the remaining major open problems in the theory of isolated self-gravitating systems. Especially when one wants to model scattering of gravitational radiation in spacetime. In this thesis the conformal theory is used to study simple electromagnetic fields, close to spatial infinity. In particular, the trajectory of the moving Coulomb field is studied in compactified Minkowski space. In the formalism, introduced by Penrose, Minkowski metric is rescaled 𝑔 = Ω<sup>2</sup>η to Einstein’s Universe, ℝ × 𝑆<sup>3</sup>. A dual particle, passing through spatial infinity in Einstein’s Universe, emerges from the conformally extended Coulomb field. The particle pair moves antipodally with respect to the retarded and advanced directions. Furthermore, a more recent treatment of spatial infinity, proposed by Friedrich, is studied in conjunction with the electromagnetic field. In this treatment, spatial infinity is blown-up to a cylinder that is a total characteristic of the spacetime. The Newman-Penrose formalism is central to the theory and is used here to rewrite Maxwell’s equations. The blow-up is linked to the sigma-process, a process used to treat singularities in the theory of differential equations. Boosted space-like curves are linked to points on the cylinder via a bijective function. The Newman-Penrose scalars are studied on the cylinder. Finally, a global treatment of spacetime, using global coordinates for adS<sub>2</sub> ×𝑆<sup>2</sup>, is proposed for further study of spatial infinity in e.g. numerical codes and Newman-Penrose formalism.</p>
----------------------------------------------------------------------
In diva2:1341293 
abstract is: 
<p>This report of a conceptual design of an electric driven aircraft was driven with the goal of making the flying sector more environmentally viable. The designated mission was chosen freely for the airplane. The result became a short distance plane with a range of 500 km, seating for eight passengers, primarily aimed towards companies. It was decided that the plane would have its cruising altitude at 4500 m, with a cruising speed of 280 km/h and have a short takeoff and landing distance. The airplane would be able to climb with a vertical speed of 6.67 m/s and have a stall speed of 150 km/h. From the specifications and the assumptions regarding different variables, the fuel weight and total weight was decided to be 2654 kg and 8294 kg respectively. The range of a corresponding aircraft driven with fossil fuels were calculated to be 2070 km. A constraint diagram was then constructed based on five chosen requirements. From this diagram the least power to weight and the highest possible wingloading was determined. A point slightly higher than the least required power to weight was chosen, leading to the engines needing to produce 1900 hp at takeoff. The wing area could be calculated from the decided wingloading and it ended up at 44 m^2 with a wingspan of 23.8 m because of a previously chosen aspect ratio. The length of the fuselage was calculated to be 16.2 m and its effective diameter 2.03 m. Finally an initial layout could be developed where a relationship between the wing and horizontal stabilizer was calculated and the center of gravity for the airplane was placed. The final airplane has a longer wingspan, is heavier and has a shorter range than what similar aircraft that are not driven with electricity have. It can be seen that the cruise speed can be increased above the 280 km/h in the constraint diagram, but the specified requirements were met, which was the main priority.</p>

corrected abstract:
<p>This report of a conceptual design of an electric driven aircraft was driven with the goal of making the flying sector more environmentally viable. The designated mission was chosen freely for the airplane. The result became a short distance plane with a range of 500 km, seating for eight passengers, primarily aimed towards companies. It was decided that the plane would have its cruising altitude at 4500 m, with a cruising speed of 280 km/h and have a short takeoff and landing distance. The airplane would be able to climb with a vertical speed of 6.67 m/s and have a stall speed of 150 km/h.</p><p>From the specifications and the assumptions regarding different variables, the fuel weight and total weight was decided to be 2654 kg and 8294 kg respectively. The range of a corresponding aircraft driven with fossil fuels were calculated to be 2070 km. A constraint diagram was then constructed based on five chosen requirements. From this diagram the least power to weight and the highest possible wingloading was determined. A point slightly higher than the least required power to weight was chosen, leading to the engines needing to produce 1900 hp at takeoff.</p><p>The wing area could be calculated from the decided wingloading and it ended up at 44 m<sup>2</sup> with a wingspan of 23.8 m because of a previously chosen aspect ratio. The length of the fuselage was calculated to be 16.2 m and its effective diameter 2.03 m. Finally an initial layout could be developed where a relationship between the wing and horizontal stabilizer was calculated and the center of gravity for the airplane was placed.</p><p>The final airplane has a longer wingspan, is heavier and has a shorter range than what similar aircraft that are not driven with electricity have. It can be seen that the cruise speed can be increased above the 280 km/h in the constraint diagram, but the specified requirements were met, which was the main priority.</p>
----------------------------------------------------------------------
In diva2:1578887 
abstract is: 
<p>Dryout and Departure from Nucleate boiling (DNB) are utmost thermal-hydraulic concerns for the safety of LWRs. The behavior of two-phase flows at these conditions is still not fully understood. There is at least a need for a good local velocity and void fraction database at these conditions. This database can be exploited by CFD codes, thereby leading to understanding and predicting DNB and boiling crisis. Since these conditions occur in LWR at pressures greater than 70 bar and temperatures above 285 $^oC$, most instrumentations fail at these conditions. So there is a need for developing or optimizing new instruments for this specific objective. This study will look into the application of Hot Wire Anemometry (HWA) for this application. Previous experiments at near saturation conditions were studied, the hurdles of application of HWA in the HWAT loop at KTH were also investigated. Finally, the deposition of thin film on the HWA sensors for protection was studied.</p>

corrected abstract:
<p>Dryout and Departure from Nucleate boiling (DNB) are utmost thermal-hydraulic concerns for the safety of LWRs. The behavior of two-phase flows at these conditions is still not fully understood. There is at least a need for a good local velocity and void fraction database at these conditions. This database can be exploited by CFD codes, thereby leading to understanding and predicting DNB and boiling crisis. Since these conditions occur in LWR at pressures greater than 70 bar and temperatures above 285ºC, most instrumentations fail at these conditions. So there is a need for developing or optimizing new instruments for this specific objective. This study will look into the application of Hot Wire Anemometry (HWA) for this application. Previous experiments at near saturation conditions were studied, the hurdles of application of HWA in the HWAT loop at KTH were also investigated. Finally, the deposition of thin film on the HWA sensors for protection was studied.</p>
----------------------------------------------------------------------
In diva2:1780561 
abstract is: 
<p>In this report, a Fourier spectral approximation of the solution to the linear convection--diffusion equation for initial conditions of different smoothness, and for Burger's equation for the initial condition f(x) = sin(x), was constructed, and implemented. Three different filters (Cesàro, Lanczos, and 4--th order exponential cutoff) were either applied to the initial condition or to the numerical approximation after the last integration step has been performed. The local error was then calculated in order to compare the performance of the three filters. Filtering was found to improve the local accuracy of the numerical approximation for the linear convection--diffusion equation for a diffusivity constant of 0 and for initial conditions of low smoothness, i.e. discontinuous functions or functions in C^0 or C^1. For initial conditions of infinite smoothness and a larger diffusivity constant, filtering did not improve the local accuracy but rather made it worse. No significant difference was found between filtering the initial condition and filtering after the final integration step. The 4--th order exponential cutoff performed best overall for most initial conditions. For Burger's equation, filtering only improved the local accuracy when applied after the final integration step and only if a discontinuity had started to form. For a discontinuity to form, the diffusivity constant furthermore needed to be sufficiently small. In conclusion, filtering is applicable when solving the linear convection--diffusion equation for a low diffusivity constant and initial conditions of low smoothness. For Burger's equation, filtering is applicable after a discontinuity starts to form. These results were in line with the theory presented in the report.</p>

corrected abstract:
<p>In this report, a Fourier spectral approximation of the solution to the linear convection-diffusion equation for initial conditions of different smoothness, and for Burger's equation for the initial condition 𝑓(𝑥) = sin(𝑥), was constructed, and implemented. Three different filters (Cesàro, Lanczos, and 4-th order exponential cutoff) were either applied to the initial condition or to the numerical approximation after the last integration step has been performed. The local error was then calculated in order to compare the performance of the three filters. Filtering was found to improve the local accuracy of the numerical approximation for the linear convection-diffusion equation for a diffusivity constant of 0 and for initial conditions of low smoothness, i.e. discontinuous functions or functions ∈ 𝐶<sup>0</sup> or 𝐶<sup>1</sup>. For initial conditions of infinite smoothness and a larger diffusivity constant, filtering did not improve the local accuracy but rather made it worse. No significant difference was found between filtering the initial condition and filtering after the final integration step. The 4-th order exponential cutoff performed best overall for most initial conditions. For Burger's equation, filtering only improved the local accuracy when applied after the final integration step and only if a discontinuity had started to form. For a discontinuity to form, the diffusivity constant furthermore needed to be sufficiently small. In conclusion, filtering is applicable when solving the linear convection-diffusion equation for a low diffusivity constant and initial conditions of low smoothness. For Burger's equation, filtering is applicable after a discontinuity starts to form. These results were in line with the theory presented in the report.</p>
----------------------------------------------------------------------
In diva2:1579559 
abstract is: 
<p>While there are existing methods of gamma ray-track reconstruction in specialized detectors such as AGATA, including backtracking and clustering, it is naturally of interest to diversify the portfolio of available tools to provide us viable alternatives. In this study some possibilities found in the field of machine learning were investigated, more specifically within the field of graph neural networks.</p><p>In this project there was attempt to reconstruct gamma tracks in a germanium solid using data simulated in Geant4. The data consists of photon energies below the pair production limit and so we are limited to the processes of photoelectric absorption and Compton scattering. The author turned to the field of graph networks to utilize its edge and node structure for data of such variable input size as found in this task. A graph neural network (GNN) was implemented and trained on a variety of gamma multiplicities and energies and was subsequently tested in terms of various accuracy parameters and generated energy spectra.</p><p>In the end the best result involved an edge classifier trained on a large dataset containing a 10^6 tracks bundled together into separate events to be resolved. The network was capable of recalling up to 95 percent of the connective edges for the selected threshold in the infinite resolution case with a peak-to-total ratio of 68 percent for a set of packed data with a model trained on simulated data including realistic uncertainties in both position and energy.</p>

corrected abstract:
<p>While there are existing methods of gamma ray-track reconstruction in specialized detectors such as AGATA, including backtracking and clustering, it is naturally of interest to diversify the portfolio of available tools to provide us viable alternatives. In this study some possibilities found in the field of machine learning were investigated, more specifically within the field of graph neural networks.</p><p>In this project there was attempt to reconstruct gamma tracks in a germanium solid using data simulated in Geant4. The data consists of photon energies below the pair production limit and so we are limited to the processes of photoelectric absorption and Compton scattering. The author turned to the field of graph networks to utilize its edge and node structure for data of such variable input size as found in this task. A graph neural network (GNN) was implemented and trained on a variety of gamma multiplicities and energies and was subsequently tested in terms of various accuracy parameters and generated energy spectra.</p><p>In the end the best result involved an edge classifier trained on a large dataset containing a 10<sup>6</sup> tracks bundled together into separate events to be resolved. The network was capable of recalling up to 95 percent of the connective edges for the selected threshold in the infinite resolution case with a peak-to-total ratio of 68 percent for a set of packed data with a model trained on simulated data including realistic uncertainties in both position and energy.</p>
----------------------------------------------------------------------
In diva2:1568137 
abstract is: 
<p>Supernovae (SNe) are explosions following the death of massive stars. Core-collapse supernovae (CCSNe) occur when the heavy iron core of these stars collapse in on themselves. The resulting remnant of the core of a CCSN is a compact object: either a black hole or a neutron star. During the collapse and following explosion, massive amounts of energy and material are expelled. The compact objects emit high-energy radiation. With X-ray astronomy, we can observe it and study the processes behind these events. In this thesis, we determine a limit on the X-ray luminosity of SN 2002ap, and constrain the parameters for the magnetic field of the central object, potentially a neutron star. We model the absorption of the radiation by the material in the surrounding area, the so-called SN ejecta, as well as the absorption by the interstellar medium (ISM). We construct the model using the spectral fitting program XSPEC. Assumptions about the abundance of X-ray absorbing elements in the ejecta and ISM are based on earlier models and the explosion energy is taken from previous estimations. The mass of the ejecta is assumed to be 2.5-5 solar masses and the distance 9.34 Mpc. We compare the absorption model to the data taken by the Chandra telescope in 2018. From this comparison, we determine the maximum luminosity to be L &lt; 2*10^40 erg/s and constrain the magnetic field to a minimum of B &gt; 3*10^13 G.</p>

corrected abstract:
<p>Supernovae (SNe) are explosions following the death of massive stars. Core-collapse supernovae (CCSNe) occur when the heavy iron core of these stars collapse in on themselves. The resulting remnant of the core of a CCSN is a compact object: either a black hole or a neutron star. During the collapse and following explosion, massive amounts of energy and material are expelled. The compact objects emit high-energy radiation. With X-ray astronomy, we can observe it and study the processes behind these events. In this thesis, we determine a limit on the X-ray luminosity of SN 2002ap, and constrain the parameters for the magnetic field of the central object, potentially a neutron star. We model the absorption of the radiation by the material in the surrounding area, the so-called SN ejecta, as well as the absorption by the interstellar medium (ISM). We construct the model using the spectral fitting program XSPEC. Assumptions about the abundance of X-ray absorbing elements in the ejecta and ISM are based on earlier models and the explosion energy is taken from previous estimations. The mass of the ejecta is assumed to be 2.5-5 M<sub>☉</sub> and the distance 9.34 Mpc. We compare the absorption model to the data taken by the Chandra telescope in 2018. From this comparison, we determine the maximum luminosity to be 𝐿 ≲ 2 × 10<sup>40</sup> erg s<sup>-1</sup> and constrain the magnetic field to a minimum of 𝐵 ≳ 3 × 10<sup>13</sup> G.</p>
----------------------------------------------------------------------
In diva2:1334784 
abstract is: 
<p>Given an integer n, this text explores different ways of finding factors of n, with focus on Shanks’ Class Group Method as described by Henri Cohen in A Course in Algebraic Number Theory, although brief summaries of Pollard’s p - 1 and rho algorithms for factoring an integer are given as well. The class group is introduced as a set of equivalence classes of fractional ideals in imaginary quadratic fields before an isomorphism with a set of equivalence classes of binary quadratic forms of a given discriminant, D, is given. This isomorphism gives rise to an abstract group operation under which elements of order 2, so called ambiguous forms, constitute a factorisation of n. The Class Group Method describes how to find ambiguous elements and factorise n in O(|D|^(1/4)) time, where the main problem lies in finding the exponent of the class group, which is done using Shanks’ Baby-Steps-Giant-Steps method for finite groups. Implementations in Python 3 are found in the appendix.</p>

corrected abstract:
<p>Given an integer 𝑛, this text explores different ways of finding factors of 𝑛, with focus on Shanks’ Class Group Method as described by Henri Cohen in <em>A Course in Algebraic Number Theory</em>, although brief summaries of Pollard’s 𝑝 - 1 and ρ algorithms for factoring an integer are given as well. The class group is introduced as a set of equivalence classes of fractional ideals in imaginary quadratic fields before an isomorphism with a set of equivalence classes of binary quadratic forms of a given discriminant, 𝐷, is given. This isomorphism gives rise to an abstract group operation under which elements of order 2, so called ambiguous forms, constitute a factorisation of 𝑛. The Class Group Method describes how to find ambiguous elements and factorise 𝑛 in &Oscr;(|𝐷|<sup>1/4</sup>) time, where the main problem lies in finding the exponent of the class group, which is done using Shanks’ Baby-Steps-Giant-Steps method for finite groups. Implementations in Python 3 are found in the appendix.</p>
----------------------------------------------------------------------
In diva2:1341556 
abstract is: 
<p>The purpose of this bachelor thesis is to design an electric powered commercial short-range aircraft that is set to take off in 2030 with reasonable technical advancement assumptions made. The aircraft is designed with the ATR 42-500 as inspiration and has therefore similar requirements. The aircraft has a payload of 5070 kg and cruises at 7600 m above sea level. It has a max speed of Mach 0.5 and a stall speed of 41 m/s. Climb rate is 560 m/min, takeoff distance is 1165 m and landing distance is 960 m. The conceptually designed aircraft has a range of 400 km that is approximately the distance London-Amsterdam and is able to carry up to 48 passengers in a two by two seat configuration. Batteries are expected to improve with 30 % during the next ten years resulting in a maximum takeoff weight of 19900 kg, where 3220 kg is battery weight. Fuel powered it has a maximum takeoff weight of 19200 kg and a fuel weight of 2900 kg. The power needed for propulsion was found to be 4.18 MW which would be equally divided over the engines that drive the two propellers. These are positioned one on each wing. The 26 m long aircraft is equipped with an unswept high mounted wing with a wingspan of 29 m and a wing reference area of 75 m^2. The horizontal stabilizer is 12 m^2 and the vertical stabilizer is 11 m^2.</p>

corrected abstract:
<p>The purpose of this bachelor thesis is to design an electric powered commercial short range aircraft that is set to take-off in 2030 with reasonable technical advancement assumptions made.</p><p>The aircraft is designed with the ATR 42-500 as inspiration and has therefore similar requirements. The aircraft has a payload of 5070 kg and cruises at 7600 m above sea level. It has a max speed of Mach 0.5 and a stall speed of 41 m/s. Climb rate is 560 m/min, take-off distance is 1165 m and landing distance is 960 m.</p><p>The conceptually designed aircraft has a range of 400 km that is approximately the distance London-Amsterdam and is able to carry up to 48 passengers in a two by two seat configuration. Batteries are expected to improve with 30 % during the next ten years resulting in a maximum take-off weight of 19900 kg, where 3220 kg is battery weight. Fuel powered it has a maximum take-off weight of 19200 kg and a fuel weight of 2900 kg.</p><p>The power needed for propulsion was found to be 4.18 MW which would be equally divided over the engines that drive the two propellers. These are positioned one on each wing.</p><p>The 26 m long aircraft is equipped with an unswept high mounted wing with a wingspan of 29 m and a wing reference area of 75 m<sup>2</sup>. The horizontal stabilizer is 13 m<sup>2</sup> and the vertical stabilizer is 11 m<sup>2</sup>.</p>
----------------------------------------------------------------------
In diva2:1827769 
abstract is: 
<p>This study investigated the relationship between Sweden’s CO2e (Carbon Dioxide Equivalent) emissions and key macroeconomic factors, for the period 2008Q1- 2022Q3. The aim was to enhance the understanding of the link between macroeconomic factors and greenhouse gas emissions in a post-industrial economy, using multiple regression analysis. The study identified several significant macroeconomic factors affecting CO2e emissions and examined the extent to which these variables explain the fluctuations in Sweden’s emissions. Additionally, the study assessed the validity of the Environmental Kuznets Curve and Porter Hypothesis within Sweden’s environmental context. In the study, two multiple regression models were developed. Model 1 had an R^2 of 0.90, using the macroeconomic variables Industry Fuel Consumption, Population, Net Export, and Oil Prices. However, since the first model displayed moderate autocorrelation, a second model was also built by introducing a lagged dependent variable which yielded an R^2 of 0.92.</p>

corrected abstract:
<p>This study investigated the relationship between Sweden’s CO<sub>2</sub>e (Carbon Dioxide Equivalent) emissions and key macroeconomic factors, for the period 2008Q1- 2022Q3. The aim was to enhance the understanding of the link between macroeconomic factors and greenhouse gas emissions in a post-industrial economy, using multiple regression analysis. The study identified several significant macroeconomic factors affecting CO<sub>2</sub>e emissions and examined the extent to which these variables explain the fluctuations in Sweden’s emissions. Additionally, the study assessed the validity of the Environmental Kuznets Curve and Porter Hypothesis within Sweden’s environmental context. In the study, two multiple regression models were developed. Model 1 had an 𝑅<sup>2</sup> of 0.90, using the macroeconomic variables Industry Fuel Consumption, Population, Net Export, and Oil Prices. However, since the first model displayed moderate autocorrelation, a second model was also built by introducing a lagged dependent variable which yielded an 𝑅<sup>2</sup> of 0.92.</p>
----------------------------------------------------------------------
In diva2:1827787 
abstract is: 
<p>This study aims to identify whether a relationship between ESG performance and financial performance exists for Nordic publicly-listed companies, by conducting a multiple linear regression analysis. Also, it will be observed which (if any) ESG variables are of relevance.</p><p>The regression analysis conducted in this study arrives at the conclusion that there is a relationship between ESG performance and financial performance. However, the models have low explanatory power, with Adjusted R^2 values of 0.36 for the accounting-based financial measure Return on Assets (ROA), and 0.30 for the market-based financial measure Tobin´s Q.In both the ROA and Tobin's Q model, social variables are the most significant. Supplier evaluation disclosure is the only variable that is highly significant and positively correlated to both ROA and Tobin's Q. Consistent with previous literature, our results show that female board participation is positively correlated with ROA. The results also show that ROA correlates negatively with compensation of board members and senior executives being linked to environmental and social factors. In conclusion, some variables were identified that are significant for financial performance. However, the overall explanatory power of the model is low. It is suggested that future studies adopt a materiality approach.</p>

corrected abstract:
<p>This study aims to identify whether a relationship between ESG performance and financial performance exists for Nordic publicly-listed companies, by conducting a multiple linear regression analysis. Also, it will be observed which (if any) ESG variables are of relevance.</p><p>The regression analysis conducted in this study arrives at the conclusion that there is a relationship between ESG performance and financial performance. However, the models have low explanatory power, with Adjusted 𝑅<sup>2</sup> values of 0.36 for the accounting-based financial measure Return on Assets (ROA), and 0.30 for the market-based financial measure Tobin´s Q. In both the ROA and Tobin's Q model, social variables are the most significant. Supplier evaluation disclosure is the only variable that is highly significant and positively correlated to both ROA and Tobin's Q. Consistent with previous literature, our results show that female board participation is positively correlated with ROA. The results also show that ROA correlates negatively with compensation of board members and senior executives being linked to environmental and social factors. In conclusion, some variables were identified that are significant for financial performance. However, the overall explanatory power of the model is low. It is suggested that future studies adopt a materiality approach.</p>
----------------------------------------------------------------------
In diva2:690751 
abstract is: 
<p>A one phase Hele-Shaw flow, described by a domain D(t) (t represents time) in the plane is the flow of a liquid injected at a constant rate in the separation between two narrowly separated parallel planes. This thesis deals with the formulation and proof of existence for a multiple phase Hele-Shaw flow in arbitrary dimension R^n exhibiting separation of the phases. A smooth version of the problem, depending on a small parameter epsilon, has been considered. Solutions to this smooth problem approximate the multiple-phase Hele-Shaw flow. We show that the smooth problem has a solution using a variational technique with functions u=u(t;eps) in the Sobolev space H_0^1 describing the Hele-Shaw flow with D(t)=support(u(t;eps)). As we let the parameter epsilon tend to zero we get that the solutions u(t;eps) converges weakly to a family of functions u(t) in the same Sobolev space which describe the desired Hele-Shaw flow. Furthermore the phases represented by the components of u(t) are separated in the sense that the overlap of any two distinct phases has vanishing n-dimensional Lebesgue measure. </p><p>We also touch upon a formulation of the multiple phase Hele-Shaw flow which would, beyond separation of the phases, provide freezing of the intersecting boundary of two phases. This formulation of the problem tries to incorporate memory in to the system via means of an integration over previous states. </p>

corrected abstract:
<p>A one phase Hele-Shaw flow, described by a domain</p> <p>𝐷<sub>𝑡</sub> = {𝑥 : 𝑢<sub>𝑡</sub>(𝑥) > 0} ∪ 𝐷<sub>0</sub></p> for some functions u<sub>𝑡</sub> : ℝ<sup>n</sup> ⊃ Ω → ℝ and 𝑡 ≥ 0, is the flow of a liquid injected at a constant rate in the separation between two narrowly separated parallel planes.  This thesis deals with the formulation and proof of existence for a multiple phase Hele-Shaw flow exhibiting separation of the phases. A smooth version of the problem, depending on the parameter ε > 0, has been considered giving rise to the equations in 𝐻<sup>−1</sup>(Ω) for the phases 𝑢<sup>𝑖</sup></p>
<p>(0.1)&nbsp;&nbsp;&nbsp;&nbsp; −∆𝑢<sup>𝑖</sup> + (1 − χ<sub>𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span></sub>)β<sub>ε</sub>(𝑢<sup>𝑖</sup>) = 𝑡µ<sup>𝑖</sup> − (1/ε)∑<sub>𝑗&ne;𝑖</sub> B<sub>ε</sub> (𝑢<sup>𝑗</sup>)β<sub>ε</sub>(𝑢<sup>𝑖</sup>) for 𝑖 = 1, . . . , m</p>
<p>where 𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span> ⊂ Ω ⊂ ℝ<sup>n</sup>, µ<sup> </sup> ∈ H<sup>−1</sup>(Ω), β<sub>ε</sub> is a mollification of the Heaviside step function, 𝐵(𝑠) = &int;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑠</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span> β<sub>ε</sub>(𝑠&prime;)𝑑𝑠&prime;, 𝑢 = (𝑢<sup>𝑖</sup>)<sub>𝑖 = 1, . . . , m</sub> a vector with components 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω) and 𝐻<sup>−1</sup>(Ω) is the dual of the Sobolev space 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω). We show that the smooth problem has a solution [0, ∞) &in; 𝑡 ↦ 𝑢<sub>𝑡</sub>;ε ∈ 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω; ℝ<sup>m</sup>) depending on ε using a variational technique. Upon letting ε → 0<sup>+</sup>, for fixed 𝑡, the solution 𝑢<sub>𝑡;ε</sub> converges weakly to some 𝑢<sub>𝑡</sub> ∈ 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω; ℝ<sup>m</sup>) solving</p>
<p>(0.2)&nbsp;&nbsp;&nbsp;&nbsp; −∆𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> + (1 − χ<sub>𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span></sub>)χ<sub>{𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.6rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> &gt; 0}</sub> = 𝑡µ<sup>𝑖</sup> - κ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span>  in H<sup>−1</sup>(Ω),</p><p>for some non-negative elements κ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> ∈ H<sup>−1</sup>(Ω) having support on ∂𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span>. Furthermore the phases represented by the components of 𝑢<sub>𝑡</sub> are separated in the sense that the overlap of any two distinct phases has vanishing n-dimensional Lebesgue measure i.e.</p>
<p>(0.3) &nbsp;&nbsp;&nbsp;&nbsp; ∣supp 𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> &cap; supp 𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑗</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span>∣ = 0 for 𝑖 &ne; 𝑗.</p>
<p>We also touch upon a formulation of the multiple phase Hele-Shaw flow which would, beyond separation of the phases, provide freezing of the intersecting boundary &Gamma;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖𝑗</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> = ∂𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> &cap; ∂𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑗</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> of two phases. This formulation of the problem tries to incorporate memory in to the system via means of an integration over previous states.</p>

----------------------------------------------------------------------
In diva2:1780566 
abstract is: 
<p>This study evaluates the feasibility of reusing lightweight solar sails in order to transport 1.69 * 10^6 sunshades, made out of occulting membranes with free-standing SiO2 nanotube films, to the adjusted sun-Earth Lagrange point, L1'. The purpose of the study was therefore to evaluate if this method is sufficient enough to lower Earth's average surface temperature by 1 degree C within a reasonable time frame, due to the rapid climate change, and compare the total launch mass to previously proposed methods. Two mission times of 10 years and 15 years were used, and three different starting altitudes, the GEO, MEO and LEO orbits, were investigated. The results showed that the method in this study was feasible for all combinations of starting altitudes and mission times. The solution where the mission time was set to 15 years and where the starting altitude was set to the GEO orbit, resulted in a launch mass of 11\% of the mass of the previously proposed solution. Furthermore, the investigation showed that high altitude starting orbits and long mission times resulted in a lower launch mass. However, in order to fulfill the goal of reducing the average temperature by 1 degree C in a reasonable time frame, the mission time cannot be too long. Finally, the results and calculations in this study are partially based on assumptions and simplifications, and therefore the results should be considered as approximations and not exact analytical solutions.</p>

corrected abstract:
<p>This study evaluates the feasibility of reusing lightweight solar sails in order to transport 1.69 · 10<sup>6</sup> sunshades, made out of occulting membranes with free-standing <em>SiO<sub>2</sub></em> nanotube films, to the adjusted sun-Earth Lagrange point, 𝐿1&prime;. The purpose of the study was therefore to evaluate if this method is sufficient enough to lower Earth's average surface temperature by 1ºC within a reasonable time frame, due to the rapid climate change, and compare the total launch mass to previously proposed methods. Two mission times of 10 years and 15 years were used, and three different starting altitudes, the GEO, MEO and LEO orbits, were investigated. The results showed that the method in this study was feasible for all combinations of starting altitudes and mission times. The solution where the mission time was set to 15 years and where the starting altitude was set to the GEO orbit, resulted in a launch mass of 11% of the mass of the previously proposed solution. Furthermore, the investigation showed that high altitude starting orbits and long mission times resulted in a lower launch mass. However, in order to fulfill the goal of reducing the average temperature by 1ºC in a reasonable time frame, the mission time cannot be too long. Finally, the results and calculations in this study are partially based on assumptions and simplifications, and therefore the results should be considered as approximations and not exact analytical solutions.</p>
----------------------------------------------------------------------
In diva2:1851005 
abstract is: 
<p>Renormalization is a powerful tool showing up in different contexts of mathematics and physics. In the context of circle diffeomorphisms, the renormalization operator acts like a microscope and allows to study the dynamics of a circle diffeomorphism on a small scale. The convergence of renormalization leads to a proof of the so-called rigidity theorem, which classifies the dynamics of circle diffeomorphisms geometrically: the conjugacy between $C^3$ circle diffeomorphism with Diophantine rotation number and the corresponding rotation is $C^1$.</p><p>In this thesis, we define the renormalization of circle diffeomorphisms and study its dynamics. In particular, we prove that the renormalization of orientation preserving $C^3$ circle diffeomorphisms with irrational rotation number of bounded type converges to rotations at exponential speed. We also introduce the necessary relevant concepts such as rotation number, distortion and non-linearity and discuss some of their properties.</p><p>This thesis is a summary and supplement to the book One-Dimensional Dynamics: from Poincaré to Renormalization.</p>

corrected abstract:
<p>Renormalization is a powerful tool showing up in different contexts of mathematics and physics. In the context of circle diffeomorphisms, the renormalization operator acts like a microscope and allows to study the dynamics of a circle diffeomorphism on a small scale. The convergence of renormalization leads to a proof of the so-called rigidity theorem, which classifies the dynamics of circle diffeomorphisms geometrically: the conjugacy between 𝐶<sup>3</sup> circle diffeomorphism with Diophantine rotation number and the corresponding rotation is 𝐶<sup>1</sup>.</p><p>In this thesis, we define the renormalization of circle diffeomorphisms and study its dynamics. In particular, we prove that the renormalization of orientation preserving 𝐶<sup>3</sup> circle diffeomorphisms with irrational rotation number of bounded type converges to rotations at exponential speed. We also introduce the necessary relevant concepts such as rotation number, distortion and non-linearity and discuss some of their properties.</p><p>This thesis is a summary and supplement to the book <em>One-Dimensional Dynamics: From Poincaré to Renormalization</em>.</p>
----------------------------------------------------------------------
In diva2:558519 
abstract is: 
<p>How should n points be distributed in a given region F in R^d such that they are separated as much as possible?</p><p>This general problem is studied in this paper, for some combinations of F, d, n, and the ways one can state the problem mathematically. Some numerical optimization methods are suggested and tested, both on the point separation problem and the closely related circle packing problem. The results are compared with some known analytical results. The main conclusion is that the suggested numerical methods are useful general tools to obtain optimal solutions to the considered problems.</p>

corrected abstract:
<p>How should 𝑛 points be distributed in a given region <img style="display: inline;" src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?\mathcal{F}" /> in ℝ<sup>𝑑</sup> such that they are separated as much as possible? This general problem is studied in this paper, for some combinations of <img style="display: inline;" src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?\mathcal{F}" />, 𝑑, 𝑛, and the ways one can state this problem mathematically. Some numerical optimization methods are suggested and tested, both on the point separation problem and the closely related circle packing problem. The results are compared with some known analytical results. The main conclusion is that the suggested numerical methods are useful general tools to obtain optimal solutions to the considered problems.</p>
----------------------------------------------------------------------
In diva2:1806023 
abstract is: 
<p>A Linear Program is a problem where the goal is to maximize a linear function subject to a set of linear inequalities. Geometrically, this can be rephrased as finding the highest point on a polyhedron. The Simplex method is a commonly used algorithm to solve Linear Programs. It traverses the vertices of the polyhedron, and in each step, it selects one adjacent better vertex and moves there. There can be multiple vertices to choose from, and therefore the Simplex method has different variants deciding how the next vertex is selected. One of the most natural variants is Random Edge, which in each step of the Simplex method uniformly at random selects one of the better adjacent vertices.</p><p>It is interesting and non-trivial to study the complexity of variants of the Simplex method in the number of variables, d, and inequalities, N. In 2011, Friedmann, Hansen, and Zwick found a class of Linear Programs for which the Random Edge algorithm is subexponential with complexity 2^Ω(N^(1/4)), where d=Θ(N). Previously all known lower bounds were polynomial. We give an improved lower bound of 2^Ω(N^(1/2)), for Random Edge on Linear Programs where d=Θ(N).</p><p>Another well studied variant of the Simplex method is Random Facet. It is upper bounded by 2^O(N^(1/2)) when d=Θ(N). Thus we prove that Random Edge is not faster than Random Facet on Linear Programs where d=Θ(N).</p><p>Our construction is very similar to the previous construction of Friedmann, Hansen and Zwick. We construct a Markov Decision Process which behaves like a binary counter with linearly many levels and linearly many nodes on each level. The new idea is a new type of delay gadget which can switch quickly from 0 to 1 in some circumstances, leading to fewer nodes needed on each level of the construction. The key idea is that it is worth taking a large risk of getting a small negative reward if the potential positive reward is large enough in comparison.</p>

corrected abstract:
<p>A Linear Program is a problem where the goal is to maximize a linear function subject to a set of linear inequalities. Geometrically, this can be rephrased as finding the highest point on a polyhedron. The Simplex method is a commonly used algorithm to solve Linear Programs. It traverses the vertices of the polyhedron, and in each step, it selects one adjacent better vertex and moves there. There can be multiple vertices to choose from, and therefore the Simplex method has different variants deciding how the next vertex is selected. One of the most natural variants is Random Edge, which in each step of the Simplex method uniformly at random selects one of the better adjacent vertices.</p><p>It is interesting and non-trivial to study the complexity of variants of the Simplex method in the number of variables, 𝑑, and inequalities, 𝑁. In 2011, Friedmann, Hansen, and Zwick found a class of Linear Programs for which the Random Edge algorithm is subexponential with complexity 2<sup>Ω(𝑁<sup>1/4</sup>)</sup>, where 𝑑=Θ(𝑁). Previously all known lower bounds were polynomial. We give an improved lower bound of 2<sup>Ω(√<span style="text-decoration: overline;">𝑁</span>)</sup>, for Random Edge on Linear Programs where 𝑑=Θ(𝑁).</p><p>Another well studied variant of the Simplex method is Random Facet. It is upper bounded by 2<sup>O(√<span style="text-decoration: overline;">𝑁</span>)</sup> when 𝑑=Θ(𝑁). Thus we prove that Random Edge is not faster than Random Facet on Linear Programs where 𝑑=Θ(𝑁).</p><p>Our construction is very similar to the previous construction of Friedmann, Hansen and Zwick. We construct a Markov Decision Process which behaves like a binary counter with linearly many levels and linearly many nodes on each level. The new idea is a new type of delay gadget which can switch quickly from 0 to 1 in some circumstances, leading to fewer nodes needed on each level of the construction. The key idea is that it is worth taking a large risk of getting a small negative reward if the potential positive reward is large enough in comparison.</p>
----------------------------------------------------------------------
In diva2:1801622 
abstract is: 
<p>It is established that paper properties depend on the loading rate. The rule of thumb is that the in-plane strength and stiffness increases about 10\%, when the strain rate increases by a factor of 10. Converting of paperboard into packages requires creasing of the paperboard followed by folding to make 3 dimensional packages. Crease response is controlled by in-plane properties, which contribute to the loading and the spring back of the crease which gives the paperboard its final geometry. </p><p>This work aims to characterize the rate and time dependent properties of paper, done by tensile testing at high strain rates of up-to 100 000 mm/min using an electro-mechanical testing machine. Also investigated in this work are the rate dependence and characterization of the plies for a deeper understanding of the contributing factors to this rate dependence. At the end of this work the aim is to retrieve the rate dependent behavior of the materials and compare them with the existing rule of thumb.</p><p>In this work it was concluded that the rule of thumb is accurate for the ultimate strength of the material in the strain rate range of 10^4 to 10^1 strains/second. It was also observed that stiffness of the material increases, but at a rate lower than the stated rule of thumb.</p>

corrected abstract:
<p>It is established that paper properties depend on the loading rate. The rule of thumb is that the in-plane strength and stiffness increases about 10%, when the strain rate increases by a factor of 10. Converting of paperboard into packages requires creasing of the paperboard followed by folding to make 3 dimensional packages. Crease response is controlled by in-plane properties, which contribute to the loading and the spring back of the crease which gives the paperboard its final geometry.</p><p>This work aims to characterise the rate and time dependent properties of paper, done by tensile testing at high strain rates of up-to 100 000 mm/min using an electro-mechanical testing machine. Also investigated in this work are the rate dependence and characterization of the plies for a deeper understanding of the contributing factors to this rate dependence. At the end of this work the aim is to retrieve the rate dependent behaviour of the materials and compare them with the existing rule of thumb.</p><p>In this work it was concluded that the rule of thumb is accurate for the ultimate strength of the material in the strain rate range of 10<sup>4</sup> to 10<sup>1</sup> strains/second. It was also observed that stiffness of the material increases but at a rate lower than the stated rule of thumb.</p>
----------------------------------------------------------------------
In diva2:1626655 
abstract is: 
<p>The failure of the Standard Model of particle physics to predict neutrino masses invites us to amend it. We have no reason to believe that the interactions currently described by the gauge group are the whole picture, nor should we expect that the number of fermions hitherto observed is correct. In this thesis, we explore two amendments to the Standard Model, and examine whether either of these is consistent with measured data. Firstly, we consider the sterile neutrino, which has no interactions described by the Standard Model. We examine the effect of this new fermion on the neutrino oscillation probabilities and present what could be a detectable signal in the TeV energy range. Secondly, we consider interactions beyond the Standard Model, possibly stemming from a higher-order theory. We show how the parameters of these Non-Standard Interactions (NSI) can modify the oscillation probabilities and within which energy range we expect to discern this signal. We use data from and simulate event counts in two Cherenkov detectors: IceCube and DeepCore. Moreover, we generate data and simulate a proposed upgrade of the DeepCore detector: PINGU. Using IceCube track events, we obtain best-fit values ∆m^2_41 = 0.01eV^2 and θ_24 = 0.67 for our sterile neutrino hypothesis at a p-value of 20%, which is not statistically significant. Hence, we found no evidence of a sterile neutrino in IceCube data. Moreover, we were unable to distinguish a signal from θ_34 in our IceCube simulation. We obtain stringent bounds on the NSI parameters and compare those to previous results in literature. We show that PINGU is expected to narrow the bound further on ε_μτ , especially by considering a joint analysis with IceCube and DeepCore. Finally, we see that an anti-correlation between ε_eμ and ε_eτ at probability level was propagated down to event level, which we expect to be observable by PINGU.</p>

corrected abstract:
<p>The failure of the Standard Model of particle physics to predict neutrino masses invites us to amend it. We have no reason to believe that the interactions currently described by the gauge group are the whole picture, nor should we expect that the number of fermions hitherto observed is correct. In this thesis, we explore two amendments to the Standard Model, and examine whether either of these are consistent with measured data.</p><p>Firstly, we consider the sterile neutrino, which has no interactions described by the Standard Model. We examine the effect of this new fermion on the neutrino oscillation probabilities, and present what could be a detectable signal in the TeV energy range. Secondly, we consider interactions beyond the Standard Model, possibly stemming from a higher-order theory. We show how the parameters of these Non-Standard Interactions (NSI) can modify the oscillation probabilities and within which energy range we expect to discern this signal. We use data from and simulate event counts in two Cherenkov detectors: IceCube and DeepCore. Moreover, we generate data and simulate a proposed upgrade of the DeepCore detector: PINGU.</p><p>Using IceCube track events, we obtain best-fit values ∆m<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>2</sup><sub>41</sub></span></span> = 0.01eV<sup>2</sup> and θ<sub>24</sub> = 0.67 for our sterile neutrino hypothesis at a p-value of 20%, which is not statistically significant. Hence, we found no evidence of a sterile neutrino in IceCube data. Moreover, we were unable to distinguish a signal from θ<sub>34</sub> in our IceCube simulation. We obtain stringent bounds on the NSI parameters and compare those to previous results in literature. We show that PINGU is expected to narrow the bound further on ε<sub>μτ</sub>, especially by considering a joint analysis with IceCube and DeepCore. Finally, we see that an anti-correlation between ε<sub>eμ</sub> and ε<sub>eτ</sub> at probability level was propagated down to event level, which we expect to be observable by PINGU.</p>
----------------------------------------------------------------------
In diva2:1652347 
abstract is: 
<p>CUBES is an X-ray detector that will be placed aboard the KTH 3U CubeSat mission, MIST. Its purpose is to detect high energy X-rays as well as to test various components in a space environment. Two CUBES will be placed on the satellite. Each CUBES consists of a printed circuit board (PCB) with three multi pixel photon counters (MPPCs). On top of these, three Germanium Aluminium Gadolinium Garnet (GAGG) scintillators are glued. These GAGG scintillators are of the dimension 1X1X1 cm^3 and are covered with PTFE tape and an opaque potting compound to prevent photons from leaving the scintillator. The MPPCs consists of a large amount of semi conductors operated in Geiger mode. The data is processed by an application specific integrated circuit (ASIC). In order to prepare the CUBES instrument for satellite flight, energy and thermal characterisation have been performed. The energy range was determined to be 40-1200 keV. The detector system shows linear behaviour and operates stably in a temperature range of -20 °C to +30 °C. The preparation of the boards and test results are presented in this thesis.</p>

corrected abstract:
<p>CUBES is an X-ray detector that will be placed aboard the KTH 3U CubeSat mission, MIST. Its purpose is to detect high energy X-rays as well as to test various components in a space environment. Two CUBES will be placed on the satellite. Each CUBES consists of a printed circuit board (PCB) with three multi pixel photon counters (MPPCs). On top of these, three Gd<sub>3</sub>Al<sub>2</sub>Ga<sub>3</sub>O<sub>12</sub> (GAGG) scintillators are glued. These GAGG scintillators are of the dimension 1 × 1 × 1 cm<sup>3</sup> and are covered with PTFE tape and an opaque potting compound to prevent photons from leaving the detector. The MPPCs consists of a large amount of semi conductors operated in Geiger mode. The data is processed by an application specific integrated circuit (ASIC). In order to prepare the CUBES instrument for satellite flight, energy and thermal characterisation have been performed. The energy range was determined to be 40-1200 keV. The detector system shows linear behaviour and operates stably in a temperature range of -20 °C to +30 °C. The preparation of the boards and test results are presented in this thesis.</p>
----------------------------------------------------------------------
In diva2:1445991 
abstract is: 
<p>During 2018, the Public Transport Administration (Trafikförvaltningen) in the Stockholm region spent approximately 2.2 billion SEK on new infrastructure investments related to the public transport system, many of which were based on their public transport models. The previously used method for validating these models has lacked scientific rigour, efficiency and a systematic approach, which has led to uncertainty in decision making. Furthermore, few scientific studies have been conducted to develop validation methodologies for large-scale models, such as public transport models. For these reasons, a scientific validation methodology for public transport models has been developed in this thesis. This validation methodology has been applied on the 2014 route assignment model used by Trafikförvaltningen, for the transport modes bus, commuter train and local tram.</p><p>In the developed validation methodology, the selected validation metrics called MAPE, %RMSE and R^2 are used to compare link loads from a route assignment model with observed link loads from an Automatic Passenger Counting (APC) system. To obtain an overview of the performance of the route assignment model, eight different scenarios are set, based on whether the validation metrics meet acceptable thresholds or not.</p><p>In the application of the developed validation methodology, the average link loads for the morning rush have been validated. To adjust the developed validation methodology to system-specific factors and to set acceptable metric thresholds, discussions with model practitioners have taken place. The validation has been performed on both lines and links, and for bus entire line number series have been validated as well. The validation results show that commuter train meets the set threshold values in a higher proportion than bus and local tram do. However, Trafikförvaltningen is recommended to further calibrate the route assignment model in order to achieve a better model performance.</p><p>The developed validation methodology can be used for validation of public transport models, and can in combination with model calibration be used in an iterative process to fine-tune model parameters for optimising validation results. Finally, a number of recommendations are proposed for Trafikförvaltningen to increase the efficiency and quality of the validation process, such as synchronising model data with the observed data.</p>

corrected abstract:
<p>During 2018, the Public Transport Administration (Trafikförvaltningen) in the Stockholm region spent approximately 2.2 billion SEK on new infrastructure investments related to the public transport system, many of which were based on their public transport models. The previously used method for validating these models has lacked scientific rigour, efficiency and a systematic approach, which has led to uncertainty in decision making. Furthermore, few scientific studies have been conducted to develop validation methodologies for large-scale models, such as public transport models. For these reasons, a scientific validation methodology for public transport models has been developed in this thesis. This validation methodology has been applied on the 2014 route assignment model used by Trafikförvaltningen, for the transport modes bus, commuter train and local tram.</p><p>In the developed validation methodology, the selected validation metrics called MAPE, %RMSE and R<sup>2</sup> are used to compare link loads from a route assignment model with observed link loads from an Automatic Passenger Counting (APC) system. To obtain an overview of the performance of the route assignment model, eight different scenarios are set, based on whether the validation metrics meet acceptable thresholds or not.</p><p>In the application of the developed validation methodology, the average link loads for the morning rush have been validated. To adjust the developed validation methodology to system-specific factors and to set acceptable metric thresholds, discussions with model practitioners have taken place. The validation has been performed on both lines and links, and for bus entire line number series have been validated as well. The validation results show that commuter train meets the set threshold values in a higher proportion than bus and local tram do. However, Trafikförvaltningen is recommended to further calibrate the route assignment model in order to achieve a better model performance.</p><p>The developed validation methodology can be used for validation of public transport models, and can in combination with model calibration be used in an iterative process to fine-tune model parameters for optimising validation results. Finally, a number of recommendations are proposed for Trafikförvaltningen to increase the efficiency and quality of the validation process, such as synchronising model data with the observed data.</p>
----------------------------------------------------------------------
In diva2:1360711 
abstract is: 
<p>The Giraffe 1X is a mobile short range 3D radar from Saab used for example to detect threats and create protection in a ground based air defence system. It can also be used on naval platforms for air and surface surveillance. During the development of the radar, the system needs to be tested for both sea and mobile land applications. The most convenient place for testing is on the roof of Saab’s facility in Gothenburg. There elevators can raise the radar to the roof giving an excellent view of for example Landvetter airport and the sea. To aid future verification experiments of the radar system, this project was started in order to develop and construct a motion platform used to simulate sea- and vehicle motions. During a six month period at Saab, the work of the project was started with a thorough research of motions platforms to conduct preliminary concept studies. Furthermore the concepts were drawn as 3D-CAD models in Creo Parametric in order to visualise the different solutions and present them for suppliers. The report also covers the assembly of the produced parts, together with the development of a user interface to control the motion platform.</p><p>Lastly, the result of product development is a two-degree of freedom (DOF) motion platform influenced by the gyroscopic gimbal concept. The G1X radar is mounted on a gimbal platform which is made out of two aluminium frames, whereas the outer frame rotates around an horizontal axis while the inner frame rotates around a transversely mounted horizontal axis mounted on the outer frame. Each aluminium frame is attached to a link arm which is mounted on a motor that is used to tilt the frame. The platform can be tilted _ 22 o in pitch and _ 22 o in roll. The gimbal is supported by a steel structure to allow ground clearance and to raise the radar to a comfortable working height.</p>

corrected abstract:
<p>The Giraffe 1X is a mobile short range 3D radar from Saab used for example to detect threats and create protection in a ground based air defence system. It can also be used on naval platforms for air and surface surveillance. During the development of the radar, the system needs to be tested for both sea and mobile land applications. The most convenient place for testing is on the roof of Saab’s facility in Gothenburg. There elevators can raise the radar to the roof giving an excellent view of for example Landvetter airport and the sea. To aid future verification experiments of the radar system, this project was started in order to develop and construct a motion platform used to simulate sea- and vehicle motions. During a six month period at Saab, the work of the project was started with a thorough research of motions platforms to conduct preliminary concept studies. Furthermore the concepts were drawn as 3D-CAD models in Creo Parametric in order to visualise the different solutions and present them for suppliers. The report also covers the assembly of the produced parts, together with the development of a user interface to control the motion platform.</p><p>Lastly, the result of product development is a two-degree of freedom (DOF) motion platform influenced by the gyroscopic gimbal concept. The G1X radar is mounted on a gimbal platform which is made out of two aluminium frames, whereas the outer frame rotates around an horizontal axis while the inner frame rotates around a transversely mounted horizontal axis mounted on the outer frame. Each aluminium frame is attached to a link arm which is mounted on a motor that is used to tilt the frame. The platform can be tilted ± 22 º in pitch and ± 22 º in roll. The gimbal is supported by a steel structure to allow ground clearance and to raise the radar to a comfortable working height.</p>
----------------------------------------------------------------------
In diva2:1698174 
abstract is: 
<p>This study has investigated the conceptual feasibility of a rocket propelled kinetic energy penetrator (KEP), designed for the handheld recoilless rifle Carl-Gustaf® 84 mm calibre system, from an exterior ballistics perspective. The methodology is based upon evaluating the aerodynamic properties of different conceptual design proposals through CFD-simulations and then performing trajectory analysis to assess their exterior ballistic performance. In particular, the main focus has been to optimize the stability, velocity and spin rate of the KEP. The results of the study indicates that the final chosen KEP design retains, from an aerodynamic perspective, longitudinal stability for Mach numbers up to 4.5, regardless if the rocket motor is ignited or not. Furthermore, if using NK1384 propellant, the final chosen design in the study is, according to the calculations, able to achieve a maximum velocity of 0.7⋅v_ref and retain a minimum velocity of 0.628⋅v_ref in the horizontal range of [0.318⋅x_ref,0.648⋅x_ref] measured from the shooter. In addition, the angular spin velocity achieves a maximum value of 15.5 Hz, satisfying the performance limitation of the rocket motor which only functions properly for frequencies up to 30 Hz, while simultaneously providing a sufficiently considered spin rate in order to/ average possible thrust and mass deviations of the KEP. The results also show that if using ammonium dinitramide (ADN) propellant, the KEP is able to achieve a maximum velocity of 0.786⋅v_ref, retain a minimum velocity of 0.628⋅v_ref in the horizontal range of [0.28⋅x_ref,0.98⋅x_ref] and achieve a maximum spin rate of 17.5 Hz.</p>

corrected abstract:
<p>This study has investigated the conceptual feasibility of a rocket propelled kinetic energy penetrator (KEP), designed for the handheld recoilless rifle Carl-Gustaf® 84 𝑚𝑚 calibre system, from an exterior ballistics perspective. The methodology is based upon evaluating the aerodynamic properties of different conceptual design proposals through CFD-simulations and then performing trajectory analysis to assess their exterior ballistic performance. In particular, the main focus has been to optimize the stability, velocity and spin rate of the KEP. The results of the study indicates that the final chosen KEP design retains, from an aerodynamic perspective, longitudinal stability for Mach numbers up to 4.5, regardless if the rocket motor is ignited or not. Furthermore, if using NK1384 propellant, the final chosen design in the study is, according to the calculations, able to achieve a maximum velocity of 0.7 ⋅ 𝑣<sub>ref</sub> and retain a minimum velocity of 0.628⋅𝑣<sub>ref</sub> in the horizontal range of <span style="font-size: 1.25rem;">[</span>0.318⋅𝑥<sub>ref</sub>,0.648⋅𝑥<sub>ref</sub><span style="font-size: 1.25rem;">]</span> measured from the shooter. In addition, the angular spin velocity achieves a maximum value of 15.5 𝐻𝑧, satisfying the performance limitation of the rocket motor which only functions properly for frequencies up to 30 𝐻𝑧, while simultaneously providing a sufficiently considered spin rate in order to average possible thrust and mass deviations of the KEP. The results also show that if using ammonium dinitramide (ADN) propellant, the KEP is able to achieve a maximum velocity of 0.786⋅𝑣<sub>ref</sub>, retain a minimum velocity of 0.628⋅𝑣<sub>ref</sub> in the horizontal range of <span style="font-size: 1.25rem;">[</span>0.28⋅𝑥<sub>ref</sub>,0.98⋅𝑥<sub>ref</sub><span style="font-size: 1.25rem;">]</span> and achieve a maximum spin rate of 17.5 𝐻𝑧.</p>
----------------------------------------------------------------------
In diva2:612264 
abstract is: 
<p>Switched systems form a special class of hybrid dynamical systems, i.e. systems with both continuous and discrete dynamics. A switched system contains a family of continuous subsystems and a discrete variable that governs the switching between them. The problem of finding necessary and sufficient conditions for asymptotic stability under arbitrary switching has recently been solved for the two-dimensional focused switched system <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cdot%7Bx%7D=uAx+(1-u)Bx" /> . We consider a generalization <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cdot%7Bx%7D=uA(x-x_c)+(1-u)Bx" />  where the equilibrium points of the two subsystems have been separated, a <em>defocused</em> system. Using geometrical arguments we show that whenever the focused system is asymptotically stable, a corresponding defocused system will contain a ‘smallest’ invariant set <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" />. In the case when both the subsystems have non-real eigenvalues we are able to completely characterize <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" /> and prove that all trajectories converge to <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" />. We investigate topological properties of <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" /> and classify the possible irregularities of <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cpartial%5COmega" />. We also build time-optimal syntheses inside <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" /> which exhibit the behavior of minimal-time trajectories. Finally we briefly look at additional phenomena that occur when real eigenvalues are present in the subsystems.</p>

corrected abstract:
<p>Switched systems form a special class of hybrid dynamical systems, i.e. systems with both continuous and discrete dynamics. A switched system contains a family of continuous subsystems and a discrete variable that governs the switching between them. The problem of finding necessary and sufficient conditions for asymptotic stability under arbitrary switching has recently been solved for the two-dimensional <em>focused</em> switched system <em>ẋ</em> = 𝑢𝐴𝑥 + (1 - 𝑢)𝐵𝑥. We consider a generalization <em>ẋ</em> = 𝑢𝐴(𝑥 - 𝑥<sub>𝑐</sub>) + (1 - 𝑢)𝐵𝑥 where the equilibrium points of the two subsystems have been separated, a <em>defocused</em> system. Using geometrical arguments we show that whenever the focused system is asymptotically stable, a corresponding defocused system will contain a ‘smallest’ invariant set Ω. In the case when both the subsystems have non-real eigenvalues we are able to completely characterize Ω and prove that all trajectories converge to Ω. We investigate topological properties of Ω and classify the possible irregularities of ∂Ω. We also build time-optimal syntheses inside Ω which exhibit the behavior of minimal-time trajectories. Finally we briefly look at additional phenomena that occur when real eigenvalues are present in the subsystems.</p>

Note the equations were simple enough that they could simply be done in unicode and HTML
----------------------------------------------------------------------
In diva2:1441946 
abstract is: 
<p>The purpose of this project is to investigate the angled nutrunner, which is a hand held torque tool often used in the industry to tighten bolted joints. The goal is to estimate parameter values for an existing model that can describe the reaction force and the angular displacement of the tool as a function of the torque transferred to the joint. The model is based on a damped mass-spring system with one degree of freedom. The short torque pulse in the tool will induce an oscillating motion of the system. A large amount of data is used that has been collected at an assembly factory. Different tightening conditions tested include tightening strategy (Quick Step and Turbo Tight), joint stiffness (hard and soft joint), tightening pace (5 and 8 tightenings per minute) and target torque. Joint torque, angular acceleration for the tool and, for the Quick Step tightenings, the reaction force is measured. The grey-box model is used in order to estimate the parameters of the model through curve fitting of the data describing the angular displacement. The reaction force on the user is also examined with different methods. The resulting mean values of the model parameters are the mass m = 2.31 kg, the dampening constant m = 103.45 kg/s and the spring stiffness k = 2314.84 N/m. The mean value of the natural frequency of the hand-arm system is f_n = 5.46 Hz. A statistical significance is found for all conditions except the tightening pace.</p>

corrected abstract:
<p>The purpose of this project is to investigate the angled nutrunner, which is a hand held torque tool often used in the industry to tighten bolted joints. The goal is to estimate parameter values for an existing model that can describe the reaction force and the angular displacement of the tool as a function of the torque transferred to the joint.</p><p>The model is based on a damped mass-spring system with one degree of freedom. The short torque pulse in the tool will induce an oscillating motion of the system.</p><p>A large amount of data is used that has been collected at an assembly factory. Different tightening conditions tested include tightening strategy (Quick Step and Turbo Tight), joint stiffness (hard and soft joint), tightening pace (5 and 8 tightenings per minute) and target torque. Joint torque, angular acceleration for the tool and, for the Quick Step tightenings, the reaction force is measured. The <em>grey-box model</em> is used in order to estimate the parameters of the model through curve fitting of the data describing the angular displacement. The reaction force on the user is also examined with different methods.</p><p>The resulting mean values of the model parameters are the mass 𝑚 = 2.31 kg, the dampening constant 𝑐 = 103.45 kg/s and the spring stiffness 𝑘 = 2314.84 N/m. The mean value of the natural frequency of the hand-arm system is 𝑓<sub>𝑛</sub> = 5.46 Hz. A statistical significance is found for all conditions except the tightening pace.</p>
----------------------------------------------------------------------
In diva2:1231299 
Note: no full text in DiVA

abstract is: 
<p>An important step in manufacturing humanoid robots is being able to imitate human movement. In the case of this project, optimal movement patterns retrieved from solving an optimal control problem serve as a substitute for human movement. A supervised learning algorithm learns to model the control signal of a mobile manipulator striking a projectile to hit a specific target. Data fed to the algorithm contains trajectories generated by solving an optimal control problem. The supervised learning algorithm applied to the problem was written in Python using the TensorFlow software library. By dividing the data in two sets, one for training and one for testing, progress is measured and overfitting estimated by calculating the relative percentage error between values predicted by the model and the corresponding values in the two data sets. A mean training accuracy of 90.7 percent and a mean validation accuracy of -226 percent. The source code can be found on https://github.com/JeremiGrosz/Optimal_control_supervised_learning</p>

corrected abstract:
<p>An important step in manufacturing humanoid robots is being able to imitate human movement. In the case of this project, optimal movement patterns retrieved from solving an optimal control problem serve as a substitute for human movement. A supervised learning algorithm learns to model the control signal of a mobile manipulator striking a projectile to hit a specific target. Data fed to the algorithm contains trajectories generated by solving an optimal control problem. The supervised learning algorithm applied to the problem was written in Python using the TensorFlow software library. By dividing the data in two sets, one for training and one for testing, progress is measured and overfitting estimated by calculating the relative percentage error between values predicted by the model and the corresponding values in the two data sets. A mean training accuracy of 90.7 percent and a mean validation accuracy of -226 percent. The source code can be found on <a href="https://github.com/JeremiGrosz/Optimal_control_supervised_learning">https://github.com/JeremiGrosz/Optimal_control_supervised_learning</a></p>
----------------------------------------------------------------------
In diva2:643302 
abstract is: 
<p>There are various technologies used for reducing fuel consumption of automobiles. Hybrid electric vehicles is one approach that has been used, which can reduce fuel consumption by 10-30% compared to conventional vehicles.</p><p>In this master thesis the minimization of fuel consumption of a power-split type HEV along a given route is considered, where the vehicle speed has been assumed to be known <em>a priori</em>. This minimization was made by first deriving a model of the HEV powertrain, followed by creating a Dynamical programming based program for finding the optimal distribution of torques.</p><p>The performance was evaluated through the commercial software GT-Suite. The resulting control from the Dynamic program could follow the reference speed in many situations. However the battery state-of-charge calculated in the Dynamic program did not update properly, resulting in a depleted battery in some cases.</p><p>The model derived could follow the dynamics of the vehicle, but there are some parts which could be improved. One of them is the dynamical model of the rotational speed for the engine <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Comega_%7Be%7D" />.</p><p> The Dynamic program works for finding the controller, and can be modified to work with improved state-equations.</p>

corrected abstract:
<p>There are various technologies used for reducing fuel consumption of automobiles. Hybrid electric vehicles is one approach that has been used, which can reduce fuel consumption by 10-30% compared to conventional vehicles.</p><p>In this master thesis the minimization of fuel consumption of a power-split type HEV along a given route is considered, where the vehicle speed has been assumed to be known <em>a priori</em>. This minimization was made by first deriving a model of the HEV powertrain, followed by creating a Dynamical programming based program for finding the optimal distribution of torques.</p><p>The performance was evaluated through the commercial software GT-Suite. The resulting control from the Dynamic program could follow the reference speed in many situations. However the battery state-of-charge calculated in the Dynamic program did not update properly, resulting in a depleted battery in some cases.</p><p>The model derived could follow the dynamics of the vehicle, but there are some parts which could be improved. One of them is the dynamical model of the rotational speed for the engine <em>ω</em><sub>𝑒</sub>.</p><p> The Dynamic program works for finding the controller, and can be modified to work with improved state-equations.</p>
----------------------------------------------------------------------
In diva2:1658938 
abstract is: 
<p>In this thesis, logistic regression, random forest and statistical analysis are used to both predict and explain insurance purchases. The models are tested together with the oversampling method known as SMOTE. The result was that the random forest model together with SMOTE best predicted insurance purchases with an $F_{1}$-score of 93.2\% and ROC-AUC of 96\%. Another important discovery comes from the explanatory part where it turns out that the price of the order and the length between the order and departure date greatly influences insurance purchases. Increased values for these features increase the insurance purchase rate. For prices of the order belonging to the 90th percentile, the insurance purchase rate is approximately 2 times higher than the average and for prices of the order belonging to the 99th percentile, the insurance purchase rate is approximately 3 times higher than the average. The purchase rate is approximately 2.5 times higher for lengths longer than 8 months between order and departure compared to the length being less than one month. These are useful insights from a business perspective that can improve the travel agency’s pricing. Other more general findings include that SMOTE worked well to handle the class imbalance in this data set and that the F1-score seems to be superior to ROC-AUC as evaluation metric when it comes to the sort of problems with unbalanced data where the positive class is a minority and most important.</p>

corrected abstract:
<p>In this thesis, logistic regression, random forest and statistical analysis are used to both predict and explain insurance purchases. The models are tested together with the oversampling method known as SMOTE. The result was that the random forest model together with SMOTE best predicted insurance purchases with an 𝐹<sub>1</sub>-score of 93.2% and ROC-AUC of 96%. Another important discovery comes from the explanatory part where it turns out that the price of the order and the length between the order and departure date greatly influences insurance purchases. Increased values for these features increase the insurance purchase rate. For prices of the order belonging to the 90th percentile, the insurance purchase rate is approximately 2 times higher than the average and for prices of the order belonging to the 99th percentile, the insurance purchase rate is approximately 3 times higher than the average. The purchase rate is approximately 2.5 times higher for lengths longer than 8 months between order and departure compared to the length being less than one month. These are useful insights from a business perspective that can improve the travel agency’s pricing. Other more general findings include that SMOTE worked well to handle the class imbalance in this data set and that the F1-score seems to be superior to ROC-AUC as evaluation metric when it comes to the sort of problems with unbalanced data where the positive class is a minority and most important.</p>
----------------------------------------------------------------------
In diva2:1655639 
abstract is: 
<p>This project aims to analyze the risk measures Value-at-Risk and Conditional-Value-at-Risk for three stock portfolios with the purpose of evaluating each method's accuracy in modelling Black Swan events. This is achieved by utilizing a parametric approach in the form of a modified (C)VaR with a Cornish-Fisher expansion, a historic approach with a time series spanning ten years and a Markov Monte Carlo simulation modeled with a Brownian motion. From this, it is revealed that the parametric approach at the 99\%-level generates the most favorable results for a 30-day-(C)VaR estimation for each portfolio, followed by the historic approach and, lastly, the Markov Monte Carlo simulation. As such, it is concluded that the parametric approach may serve as a method of evaluating a portfolio's exposure to Black Swan events.</p>

corrected abstract:
<p>This project aims to analyze the risk measures Value-at-Risk and Conditional-Value-at-Risk for three stock portfolios with the purpose of evaluating each method's accuracy in modelling Black Swan events. This is achieved by utilizing a parametric approach in the form of a modified (C)VaR with a Cornish-Fisher expansion, a historic approach with a time series spanning ten years and a Markov Monte Carlo simulation modeled with a Brownian motion. From this, it is revealed that the parametric approach at the 99%-level generates the most favorable results for a 30-day-(C)VaR estimation for each portfolio, followed by the historic approach and, lastly, the Markov Monte Carlo simulation. As such, it is concluded that the parametric approach may serve as a method of evaluating a portfolio's exposure to Black Swan events.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1737350 
abstract is: 
<p>The aim of this master thesis is to develop and optimize the main wing of a T-shaped hydrofoil for a small passenger vessel called FoilCart. The project is in collaboration with KTH Maritime Robotic Lab.</p><p>The existing prototype of the hydrofoil is built by a company called ZPARQ\cite{Zparq}, but the efficiency of the main wing is not optimized. Thus, based on the genetic algorithm (GA), a 2D optimization method is applied to two 2D profile candidates to increase the efficiency (Cl/Cd). After the optimization, a parametric 3D modelling process is developed in Rhino 7 Grasshopper to create the main wing from 2D profiles and configure other components in the hydrofoil. Simulations in computational fluid dynamics (CFD) are carried out in STAR CCM+ to evaluate the optimization results in 3D.</p><p>The conclusion is that the optimization is proved to be effective, with 10.7\% and 32.59\% improvement for 3D models with optimized NACA 63-412 and optimized Eppler E836 respectively, we can say that the aims of this project are completed. Nevertheless, further model refinements need to be done and the simulation results should be verified with model tests.</p>
mc='ZPARQ\\cite{Zparq' c='ZPARQ \\cite{Zparq'

partal corrected: diva2:1737350: <p>The aim of this master thesis is to develop and optimize the main wing of a T-shaped hydrofoil for a small passenger vessel called FoilCart. The project is in collaboration with KTH Maritime Robotic Lab.</p><p>The existing prototype of the hydrofoil is built by a company called ZPARQ \cite{Zparq}, but the efficiency of the main wing is not optimized. Thus, based on the genetic algorithm (GA), a 2D optimization method is applied to two 2D profile candidates to increase the efficiency (Cl/Cd). After the optimization, a parametric 3D modelling process is developed in Rhino 7 Grasshopper to create the main wing from 2D profiles and configure other components in the hydrofoil. Simulations in computational fluid dynamics (CFD) are carried out in STAR CCM+ to evaluate the optimization results in 3D.</p><p>The conclusion is that the optimization is proved to be effective, with 10.7\% and 32.59\% improvement for 3D models with optimized NACA 63-412 and optimized Eppler E836 respectively, we can say that the aims of this project are completed. Nevertheless, further model refinements need to be done and the simulation results should be verified with model tests.</p>

corrected abstract:
<p>The aim of this master thesis is to develop and optimize the main wing of a T-shaped hydrofoil for a small passenger vessel called FoilCart. The project is in collaboration with KTH Maritime Robotic Lab.</p><p>The existing prototype of the hydrofoil is built by a company called ZPARQ[22], but the efficiency of the main wing is not optimized. Thus, based on the genetic algorithm (GA), a 2D optimization method is applied to two 2D profile candidates to increase the efficiency (Cl/Cd). After the optimization, a parametric 3D modelling process is developed in Rhino 7 Grasshopper to create the main wing from 2D profiles and configure other components in the hydrofoil. Simulations in computational fluid dynamics (CFD) are carried out in STAR CCM+ to evaluate the optimization results in 3D.</p><p>The conclusion is that the optimization is proved to be effective, with 10.7% and 32.59% improvement for 3D models with optimized NACA 63-412 and optimized Eppler E836 respectively, we can say that the aims of this project are completed. Nevertheless, further model refinements need to be done and the simulation results should be verified with model tests.</p>

Note removed the "\" before "%" and inserted the citation
----------------------------------------------------------------------
In diva2:1333887 
abstract is: 
<p>Why is it that the number of travellers in Stockholm's public transportation differs from day to day? Is the difference arbitrary or do factors such as population, temperature, weather conditions, months, or even weekdays have a significant role in this variation?</p><p>This thesis aims to explore these external variables and their effect on public transportation, as well as how this type of data driven information can result in well supported decisions. The method applied to the study was multiple linear regression and the data used was retrieved from Trafikförvaltningen, SMHI, and SCB. The study concluded that the variations in the number of travellers in Stockholm's public transportation is up to 84\% explained by population, as well as month and weekday.</p>

corrected abstract:
<p>Why is it that the number of travellers in Stockholm's public transportation differs from day to day? Is the difference arbitrary or do factors such as population, temperature, weather conditions, months, or even weekdays have a significant role in this variation?</p><p>This thesis aims to explore these external variables and their effect on public transportation, as well as how this type of data driven information can result in well supported decisions. The method applied to the study was multiple linear regression and the data used was retrieved from Trafikförvaltningen, SMHI, and SCB. The study concluded that the variations in the number of travellers in Stockholm's public transportation is up to 84% explained by population, as well as month and weekday.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1880247 
abstract is: 
<p>This thesis investigates the performance of deep learning models, specifically Resnet50 and TransUnet, in semantic image segmentation on microwell images containing tumor and natural killer (NK) cells. The main goal is to examine the effect of only using bright-field data (1-channel) as input instead of both fluorescent and brightfield data (4-channel); this is interesting since fluorescent imaging can cause damage to the cells being analyzed. The network performance is measured by Intersection over Union (IoU), the networks were trained and using manually annotated data from Onfelt Lab. TransUnet consistently outperformed the Resnet50 for both the 4-channel and 1-channel data. Moreover, the 4-channel input generally resulted in a better IoU compared to using only the bright-field channel. Furthermore, a significant decline in performance is observed when the networks are tested on the control data. For the control data, the overall IoU for the best performing 4-channel model dropped from 86.2\% to 73.9\%. The best performing 1-channel model dropped from 83.8\% to 70.8\% overall IoU.</p>

corrected abstract:
<p>This thesis investigates the performance of deep learning models, specifically Resnet50 and TransUnet, in semantic image segmentation on microwell images containing tumor and natural killer (NK) cells. The main goal is to examine the effect of only using bright-field data (1-channel) as input instead of both fluorescent and bright-field data (4-channel); this is interesting since fluorescent imaging can cause damage to the cells being analyzed. The network performance is measured by Intersection over Union (IoU), the networks were trained and using manually annotated data from Onfelt Lab. TransUnet consistently outperformed the Resnet50 for both the 4-channel and 1-channel data. Moreover, the 4-channel input generally resulted in a better IoU compared to using only the bright-field channel. Furthermore, a significant decline in performance is observed when the networks are tested on the control data. For the control data, the overall IoU for the best performing 4-channel model dropped from 86.2% to 73.9%. The best performing 1-channel model dropped from 83.8% to 70.8% overall IoU.</p>

Note removed the "\" before "%" and added hyphen to "bright-field"
----------------------------------------------------------------------
In diva2:1811793 
abstract is: 
<p>In this paper, a study is conducted to investigate the use of Curriculum Learning as an approach to address accuracy issues in a neural network caused by training on a Long-Tailed dataset. The thesis problem is presented by a Swedish e-commerce company. Currently, they are using a neural network that has been modified by them using a CORAL framework. This adaptation means that instead of having a classic binary regression model, it is an ordinal regression model. The data used for training the model has a Long-Tail distribution, which leads to inaccuracies when predicting a price distribution for items that are part of the tail-end of the data. The current method applied to remedy this problem is Re-balancing in the form of down-sampling and up-sampling. A linear training scheme is introduced, increasing in increments of $10\%$ while applying Curriculum Learning. As a method for sorting the data in an appropriate way, inspiration is drawn from Knowledge Distillation, specifically the Teacher-Student model approach. The teacher models are trained as specialists on three different subsets, and furthermore, those models are used as a basis for sorting the data before training the student model. During the training of the student model, the Curriculum Learning approach is used. The results show that for Imbalance Ratio, Kullback-Liebler divergence, Class Balance, and the Gini Coefficient, the data is clearly less Long-Tailed after dividing the data into subsets. With the correct settings before training, there is also an improvement in the training speed of the student model compared to the base model. The accuracy for both the student model and the base model is comparable. There is a slight advantage for the base model when predicting items in the head part of the data, while the student model shows improvements for items that are between the head and the tail.</p>

corrected abstract:
<p>In this paper, a study is conducted to investigate the use of Curriculum Learning as an approach to address accuracy issues in a neural network caused by training on a Long-Tailed dataset. The thesis problem is presented by a Swedish e-commerce company. Currently, they are using a neural network that has been modified by them using a CORAL framework. This adaptation means that instead of having a classic binary regression model, it is an ordinal regression model. The data used for training the model has a Long-Tail distribution, which leads to inaccuracies when predicting a price distribution for items that are part of the tail-end of the data. The current method applied to remedy this problem is Re-balancing in the form of down-sampling and up-sampling. A linear training scheme is introduced, increasing in increments of 10% while applying Curriculum Learning. As a method for sorting the data in an appropriate way, inspiration is drawn from Knowledge Distillation, specifically the Teacher-Student model approach. The teacher models are trained as specialists on three different subsets, and furthermore, those models are used as a basis for sorting the data before training the student model. During the training of the student model, the Curriculum Learning approach is used. The results show that for Imbalance Ratio, Kullback-Liebler divergence, Class Balance, and the Gini Coefficient, the data is clearly less Long-Tailed after dividing the data into subsets. With the correct settings before training, there is also an improvement in the training speed of the student model compared to the base model. The accuracy for both the student model and the base model is comparable. There is a slight advantage for the base model when predicting items in the head part of the data, while the student model shows improvements for items that are between the head and the tail.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1341581 
abstract is: 
<p>The purpose of this study is to create targeted adversarial examples for an audio classifier without access to the neural networks internal structure. Previous work in this domain has shown white box attacks that generate adversarial examples with very high measures of similarity and more noisy adversarial examples generated by black box attacks. By using an algorithm that iteratively applies noise to the audio file and selects the best candidates based on the output layer of the neural network we have managed to create new audio that is 98\% similar to the original but manages to fool the speech to text audio classifier. By evaluating the generated candidates based on different measures of similarity between the proposed candidate and the original audio file we managed to create high quality black box audio adversarial examples using genetic algorithms.</p>

corrected abstract:
<p>The purpose of this study is to create targeted adversarial examples for an audio classifier without access to the neural networks internal structure. Previous work in this domain has shown white box attacks that generate adversarial examples with very high measures of similarity and more noisy adversarial examples generated by black box attacks. By using an algorithm that iteratively applies noise to the audio file and selects the best candidates based on the output layer of the neural network we have managed to create new audio that is 98% similar to the original but manages to fool the speech to text audio classifier. By evaluating the generated candidates based on different measures of similarity between the proposed candidate and the original audio file we managed to create high quality black box audio adversarial examples using genetic algorithms.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1776799 
abstract is: 
<p>We show that with the implementation presented in this paper, the Random Forest Classification model was able to predict whether or not a stock was going to increase in value during the coming day with an accuracy higher than 50\% for all stocks included in this study. Furthermore, we show that the active trading strategy presented in this paper generated higher returns and higher risk-adjusted returns than the passive investment in the stocks underlying the strategy. Therefore, we conclude \textit{(i)} that a Random Forest Classification model can be used to provide valuable insight on publicly traded stocks, and \textit{(ii)} that it is probably possible to create a profitable trading strategy based on a Random Forest Classifier, but that this requires a more sophisticated implementation than the one presented in this paper.</p>

corrected abstract:
<p>This paper investigates the performance of the Random Forest Classification model for stock market trading. The performance is evaluated first by evaluating its ability to make predictions about the direction of future stock price changes, and secondly by evaluating simulated returns based on a classification-based trading strategy presented in this paper. The performance is evaluated using the five largest companies included in the OMXS30 index, during the period ranging from 2013-02-12 to 2022-12-30.</p>
<p>We show that with the implementation presented in this paper, the Random Forest Classification model was able to predict whether or not a stock was going to increase in value during the coming day with an accuracy higher than 50% for all stocks included in this study. Furthermore, we show that the active trading strategy presented in this paper generated higher returns and higher risk-adjusted returns than the passive investment in the stocks underlying the strategy. Therefore, we conclude <em>(i)</em> that a Random Forest Classification model can be used to provide valuable insight on publicly traded stocks, and <em>(ii)</em> that it is probably possible to create a profitable trading strategy based on a Random Forest Classifier, but that this requires a more sophisticated implementation than the one presented in this paper.</p>

Note removed the "\" before "%" and fixed the italics
----------------------------------------------------------------------
In diva2:1341400 
abstract is: 
<p>Early leakage detection is one way to make water distribution networks more efficient and sustainable. The goal of this project is to investigate the possibility to detect leakages in water distribution networks with the help of artificial neural networks. The project is based on real data collected from Stockholm water distribution network and is focusing on how to present the prediction from neural networks in an intellectual manner, by implementing and analyzing the need of a time filter. The study shows that it might be possible to detect leakages in a water distribution network with a binary accuracy of 87\%. An improvement to 98\% was achieved by implementing a time filter.</p>

corrected abstract:
<p>Early leakage detection is one way to make water distribution networks more efficient and sustainable. The goal of this project is to investigate the possibility to detect leakages in water distribution networks with the help of artificial neural networks. The project is based on real data collected from Stockholm water distribution network and is focusing on how to present the prediction from neural networks in an intellectual manner, by implementing and analyzing the need of a time filter. The study shows that it might be possible to detect leakages in a water distribution network with a binary accuracy of 87%. An improvement to 98% was achieved by implementing a time filter.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1698380 
abstract is: 
<p>A key component of traffic models for simulating bicycle traffic focuses on capturing the interactions between cyclists and the cycling infrastructure. One of the most relevant features of the infrastructure that has a significant impact in bicycle traffic is the gradient of a bicycle path. Bicycle traffic simulations are a rather uninvestigated topic since historically, most focus on simulations has been on cars. However, bicycle simulations are an important tool to further investigate and understand cyclist’s behaviour. Therefore, the main objective of this thesis is to investigate and simulate free-riding behavior of cyclists in connection to the gradient, particularly on downhills. To do so, trajectory data of cyclists traveling on a downhill with a maximum gradient of 5.5\% are analysed to identify the impact of gradient on the speed and acceleration. The data received needed processing in order to be useful. This included filtering of the trajectories and excluding the data from cyclists which could not to be regarded as free-riding. As a result, a linear correlation is found between pedaling power and the gradient that can be used in microscopic bicycle traffic simulation. Based on this knowledge regarding this linearity, the approach used for modeling the gradient’s effect on the pedaling power is linear regression. The model can be developed in various ways, so instead of only choosing one model, several were developed and compared against each other. These models are then used for the simulation. The results indicate that the simulation captures well the impact of downhill gradients in a population of cyclists as it reproduces similar speed profiles to the ones observed. Therefore, it can be concluded that a power-based model is suitable for simulating free-riding behaviour of cyclists traveling in downhills.</p>

corrected abstract:
<p>A key component of traffic models for simulating bicycle traffic focuses on capturing the interactions between cyclists and the cycling infrastructure. One of the most relevant features of the infrastructure that has a significant impact in bicycle traffic is the gradient of a bicycle path. Bicycle traffic simulations are a rather uninvestigated topic since historically, most focus on simulations has been on cars. However, bicycle simulations are an important tool to further investigate and understand cyclist’s behaviour. Therefore, the main objective of this thesis is to investigate and simulate free-riding behavior of cyclists in connection to the gradient, particularly on downhills. To do so, trajectory data of cyclists traveling on a downhill with a maximum gradient of 5.5% are analysed to identify the impact of gradient on the speed and acceleration. The data received needed processing in order to be useful. This included filtering of the trajectories and excluding the data from cyclists which could not to be regarded as free-riding. As a result, a linear correlation is found between pedaling power and the gradient that can be used in microscopic bicycle traffic simulation. Based on this knowledge regarding this linearity, the approach used for modeling the gradient’s effect on the pedaling power is linear regression. The model can be developed in various ways, so instead of only choosing one model, several were developed and compared against each other. These models are then used for the simulation. The results indicate that the simulation captures well the impact of downhill gradients in a population of cyclists as it reproduces similar speed profiles to the ones observed. Therefore, it can be concluded that a power-based model is suitable for simulating free-riding behaviour of cyclists traveling in downhills.</p>
----------------------------------------------------------------------
In diva2:1656056 
abstract is: 
<p>During the last few years, several stablecoins have emerged, in the form of algorithmic asset-collateralized cryptocurrencies designed to maximize price stability by expanding and contracting the circulating supply of coins. But over time as the relative prices of the underlying collateral assets change, there is a need to rebalance the underlying collateral portfolio of a stablecoin in order to maintain price stability and match the value-weighted target portfolio distribution. A critical part of the rebalancing process is therefore to find a pricing method that approximates the true prices of each collateral asset in order to accurately perform rebalancing.</p><p>For decentralized exchanges, which adopt a stablecoin as a basis for transactions, a pricing method requires a high degree of accuracy and responsiveness as well as security mechanisms in order to prevent attacks on the system from malicious actors. As the available pricing methods in the form of trusted feeds and external oracle networks have several issues related to governance, costs, complexity, and security we instead propose an internal oracle, that can estimate the true prices by only using information already available on the blockchain.</p><p>We consider a current proposal for an internal on-chain oracle and evaluate it with regards to performance and security by modeling and simulating an exchange. We identify several critical problems that need to be resolved and prove mathematically that an attacker can manipulate the pricing method without taking any trading loss. </p><p>Based on this analysis we propose new requirements for an internal oracle and formulate three new alternative pricing methods. In particular, we utilize statistical learning and regression analysis on simulated data in order to train and optimize the pricing models that minimizes the expected error. </p><p>Our results indicate that all three pricing methods significantly outperform the initial proposal with regards to security and accuracy with up to 56\%. Furthermore, an overall high system performance motivates our conclusion that an on-chain internal oracle has the potential to replace an external oracle. We conclude by noting that factors such as ownership and trading volume can be used as parameters in the pricing method, in order to reduce the risk of attacks even further.</p>

corrected abstract:
<p>During the last few years, several stablecoins have emerged, in the form of algorithmic asset-collateralized cryptocurrencies designed to maximize price stability by expanding and contracting the circulating supply of coins. But over time as the relative prices of the underlying collateral assets change, there is a need to rebalance the underlying collateral portfolio of a stablecoin in order to maintain price stability and match the value-weighted target portfolio distribution. A critical part of the rebalancing process is therefore to find a pricing method that approximates the true prices of each collateral asset in order to accurately perform rebalancing.</p><p>For decentralized exchanges, which adopt a stablecoin as a basis for transactions, a pricing method requires a high degree of accuracy and responsiveness as well as security mechanisms in order to prevent attacks on the system from malicious actors. As the available pricing methods in the form of trusted feeds and external oracle networks have several issues related to governance, costs, complexity, and security we instead propose an internal oracle, that can estimate the true prices by only using information already available on the blockchain.</p><p>We consider a current proposal for an internal on-chain oracle and evaluate it with regards to performance and security by modeling and simulating an exchange. We identify several critical problems that need to be resolved and prove mathematically that an attacker can manipulate the pricing method without taking any trading loss.</p><p>Based on this analysis we propose new requirements for an internal oracle and formulate three new alternative pricing methods. In particular, we utilize statistical learning and regression analysis on simulated data in order to train and optimize the pricing models that minimizes the expected error.</p><p>Our results indicate that all three pricing methods significantly outperform the initial proposal with regards to security and accuracy with up to 56%. Furthermore, an overall high system performance motivates our conclusion that an on-chain internal oracle has the potential to replace an external oracle. We conclude by noting that factors such as ownership and trading volume can be used as parameters in the pricing method, in order to reduce the risk of attacks even further.</p>

Note removed the "\" before "%" and removed spaced before end of paragraph
----------------------------------------------------------------------
In diva2:1795589 
abstract is: 
<p>The calibration of model parameters is a crucial step in the process of valuation of complex derivatives. It consists of choosing the model parameters that correspond to the implied market data especially the call and put prices.</p><p>We discuss in this thesis the calibration strategy for the Heston model, one of the most used stochastic volatility models for pricing complex derivatives. The main problem with this model is that the asset price does not have a known probability distribution function. Thus we use either Fourier expansions through its characteristic function or Monte Carlo simulations to have access to it. We hence discuss the approximation induced by these methods and elaborate a calibration strategy with a focus on the choice of the objective function and the choice of inputs for the calibration.</p><p>We assess that the put option prices are a better input than the call prices for the optimization function. Then through a set of experiments on simulated put prices, we find that the sum of squared error performs better choice of the objective function for the differential evolution optimization. We also establish that the put option prices where the Black Scholes delta is equal to 10\%, 25\%, 50\% 75\% and 90\% gives enough in formations on the implied volatility surface for the calibration of the Heston model. We then implement this calibration strategy on real market data of Eurostoxx50 Index and observe the same distribution of errors as in the set of experiments.</p>

corrected abstract:
<p>The calibration of model parameters is a crucial step in the process of valuation of complex derivatives. It consists of choosing the model parameters that correspond to the implied market data especially the call and put prices.</p><p>We discuss in this thesis the calibration strategy for the Heston model, one of the most used stochastic volatility models for pricing complex derivatives. The main problem with this model is that the asset price does not have a known probability distribution function. Thus we use either Fourier expansions through its characteristic function or Monte Carlo simulations to have access to it. We hence discuss the approximation induced by these methods and elaborate a calibration strategy with a focus on the choice of the objective function and the choice of inputs for the calibration.</p><p>We assess that put option prices are a better input than call prices for the Differential Evolution Optimization algorithm. Then through a set of experiments on simulated put prices, we find that the sum of squared errors performs better as objective function for the differential evolution optimization than the other tested objective functions. We also establish that put option prices where the Black Scholes delta is equal to 10%, 25%, 50% 75% and 90% give enough information about the implied volatility surface for the calibration of the Heston model. We then implement this calibration strategy on real market data of Eurostoxx50 Index and observe the same distribution of errors as in the set of experiments.</p>

Note removed the "\" before "%" - also fixed some wording differences between DiVA and original
----------------------------------------------------------------------
In diva2:1567949 - missing space in title:
"Review of ElectricRoad Systems: Both a conventional and innovative technology"
==>
"Review of Electric Road Systems: Both a conventional and innovative technology"

abstract is: 
<p>A more sustainable society can be accomplished by electrifying the road transportation sector, which the year 2016 emitted 11,9 \% of the global greenhouse emissions. To accelerate this transformation, dynamic charging via electric road systems (ERS) can be implemented, allowing electric vehicles (EV) to become cheaper and achieve lower energy consumption thanks to smaller batteries. </p><p>The purpose of this thesis is to expand the knowledge of different charging technologies and present, analyze and compare various ERSs. Based on a comprehensive literature study conductive, inductive, and capacitive supply systems are found to be the main charging technologies. After explaining the physics, the dynamic charging systems using these technologies from the side, above, underneath the vehicle, and via the wheels are presented together with my concepts. </p><p>Analyzing all researched ERSs, the overhead catenary conductive system is most mature but only useful for trucks and buses. Its high efficiency is also found in conductive rail systems which can be used by all EVs but have an impact on the road. A resonant inductive coupling under the vehicle is a very practical system that has a lower impact but also a little lower efficiency, power transfer capability, and is more expensive. Placing the coupling in the wheels results in a more complex system with worse power transfer capability due to the steel belt inside the tyre, but has great development potential. The capacitive coupling in the wheels has also been reviewed and judged to have a low power transfer due to safety reasons of radiofrequency electric fields transmission.</p>

corrected abstract:
<p>A more sustainable society can be accomplished by electrifying the road transportation sector, which the year 2016 emitted 11,9 % of the global greenhouse emissions. To accelerate this transformation, dynamic charging via electric road systems (ERS) can be implemented, allowing electric vehicles (EV) to become cheaper and achieve lower energy consumption thanks to smaller batteries.</p><p>The purpose of this thesis is to expand the knowledge of different charging technologies and present, analyze and compare various ERSs. Based on a comprehensive literature study conductive, inductive, and capacitive supply systems are found to be the main charging technologies. After explaining the physics, the dynamic charging systems using these technologies from the side, above, underneath the vehicle, and via the wheels are presented together with my concepts.</p><p>Analyzing all researched ERSs, the overhead catenary conductive system is most mature but only useful for trucks and buses. Its high efficiency is also found in conductive rail systems which can be used by all EVs but have an impact on the road. A resonant inductive coupling under the vehicle is a very practical system that has a lower impact but also a little lower efficiency, power transfer capability, and is more expensive. Placing the coupling in the wheels results in a more complex system with worse power transfer capability due to the steel belt inside the tyre, but has great development potential. The capacitive coupling in the wheels has also been reviewed and judged to have a low power transfer due to safety reasons of radiofrequency electric fields transmission.</p>

Note removed the "\" before "%" and removed spaces at end of paragraphs
----------------------------------------------------------------------
 diva2:1335380 
abstract is: 
<p>Some tasks, like recognizing digits and spoken words, are simple for humans to complete yet hard to solve for computer programs. For instance the human intuition behind recognizing the number eight, ''\textit{8}'', is to identify two loops on top of each other and it turns out this is not easy to represent as an algorithm. With machine learning one can tackle the problem in a new, easier, way where the computer program learns to recognize patterns and make conclusions from them. In this bachelor thesis a digit recognizing program is implemented and the parameters of the stochastic gradient descent optimizing algorithm are analyzed based on how their effect on the computation speed and accuracy. These parameters being the learning rate $\Delta t$ and batch size $N$. The implemented digit recognizing program yielded an accuracy of around $95$ \% when tested and the time per iteration stayed constant during the training session and increased linearly with batch size. Low learning rates yielded a slower rate of convergence while larger ones yielded faster but more unstable convergence. Larger batch sizes also improved the convergence but at the cost of more computational power.</p>
mc='\\textit{8' c='\\textit{ 8'

partal corrected: diva2:1335380: <p>Some tasks, like recognizing digits and spoken words, are simple for humans to complete yet hard to solve for computer programs. For instance the human intuition behind recognizing the number eight, ''\textit{ 8}'', is to identify two loops on top of each other and it turns out this is not easy to represent as an algorithm. With machine learning one can tackle the problem in a new, easier, way where the computer program learns to recognize patterns and make conclusions from them. In this bachelor thesis a digit recognizing program is implemented and the parameters of the stochastic gradient descent optimizing algorithm are analyzed based on how their effect on the computation speed and accuracy. These parameters being the learning rate $\Delta t$ and batch size $N$. The implemented digit recognizing program yielded an accuracy of around $95$ \% when tested and the time per iteration stayed constant during the training session and increased linearly with batch size. Low learning rates yielded a slower rate of convergence while larger ones yielded faster but more unstable convergence. Larger batch sizes also improved the convergence but at the cost of more computational power.</p>

corrected abstract:
<p>Some tasks, like recognizing digits and spoken words, are simple for humans to complete yet hard to solve for computer programs. For instance the human intuition behind recognizing the number eight, ”<em>8</em>”, is to identify two loops on top of each other and it turns out this is not easy to represent as an algorithm. With machine learning one can tackle the problem in a new, easier, way where the computer program learns to recognize patterns and make conclusions from them. In this bachelor thesis a digit recognizing program is implemented and the parameters of the stochastic gradient descent optimizing algorithm are analyzed based on how their effect on the computation speed and accuracy. These parameters being the learning rate &Delta; 𝑡 and batch size 𝑁. The implemented digit recognizing program yielded an accuracy of around 95 % when tested and the time per iteration stayed constant during the training session and increased linearly with batch size. Low learning rates yielded a slower rate of convergence while larger ones yielded faster but more unstable convergence. Larger batch sizes also improved the convergence but at the cost of more computational power.</p>

Note removed the "\" before "%" and fixed some of the math symbols
----------------------------------------------------------------------
In diva2:1817059 
abstract is: 
<p>Nowadays, the forestry industry still uses heavy machinery damaging both the forest and the soil. The start-up AirForestry is currently developing a sustainable way to thin and harvest trees. With their 6.2m wide electric drone carrying a harvesting tool, they can reach, thin, cut and carry trees without the need for access roads. Naturally, the drone needs to be as lightweight as possible to increase its endurance and operation range. Therefore, the first version of the drone was manufactured before the thesis with a carbon fibre laminate. </p><p>The purpose of this thesis is to study and optimize the composite structure of the drone. The first step is to characterize the existing design through experiments and simulations using the software ANSYS. Static bending loads, free vibrations, and forced vibrations are investigated against a set of predefined design requirements. This study shows that the contact surfaces between each arm and with the motor holders have high-stress concentrations compared to the rest of the arm. This means that most of the arm can be made thinner to lessen the weight with some extra reinforcement on those problematic areas. </p><p>The second step is to optimize the laminate to decrease the weight of the structure. A preliminary optimization was made and manufactured at the beginning of the thesis with strict limitations on the choice of the lamina or available thicknesses. Similar bending and vibration experiments and simulations are conducted on the new design to compare it with the older model. While the mass was expected to decrease by about 30 $\%$, the final measured weight of the arms indicates a drop of only 15 $\%$ of the mass. The model is then optimized further with more freedom in the design variables. Several variables are successively optimized: the material choice for the laminae, the thickness then of the laminae, and the angles of the plies. The mass of the structure with the final laminate has an expected decrease in mass of 45$\%$, saving more than 12kg in total</p>

corrected abstract:
<p>Nowadays, the forestry industry still uses heavy machinery damaging both the forest and the soil. The start-up AirForestry is currently developing a sustainable way to thin and harvest trees. With their 6.2m wide electric drone carrying a harvesting tool, they can reach, thin, cut and carry trees without the need for access roads. Naturally, the drone needs to be as lightweight as possible to increase its endurance and operation range. Therefore, the first version of the drone was manufactured before the thesis with a carbon fibre laminate.</p><p>The purpose of this thesis is to study and optimize the composite structure of the drone. The first step is to characterize the existing design through experiments and simulations using the software ANSYS. Static bending loads, free vibrations, and forced vibrations are investigated against a set of predefined design requirements. This study shows that the contact surfaces between each arm and with the motor holders have high-stress concentrations compared to the rest of the arm. This means that most of the arm can be made thinner to lessen the weight with some extra reinforcement on those problematic areas.</p><p>The second step is to optimize the laminate to decrease the weight of the structure. A preliminary optimization was made and manufactured at the beginning of the thesis with strict limitations on the choice of the lamina or available thicknesses. Similar bending and vibration experiments and simulations are conducted on the new design to compare it with the older model. While the mass was expected to decrease by about 30 %, the final measured weight of the arms indicates a drop of only 15 % of the mass. The model is then optimized further with more freedom in the design variables. Several variables are successively optimized: the material choice for the laminae, the thickness then of the laminae, and the angles of the plies. The mass of the structure with the final laminate has an expected decrease in mass of 45%, saving more than 12kg in total</p>

Note removed the "\" before "%" - not the last paragraph does not have terminal punctuation in the original
----------------------------------------------------------------------
In diva2:1375766  - document was scanned
abstract is: 
<p>In this thesis I use seven data-sets of weighted, directed graphs and present them as weighted, directed k-simplicial complexes. Then, I analyse the topological properties of each data-set in question using Flagser by \cite{flagser} and the cluster at the TU Darmstadt. I then proceed to run two versions of an alternative to the PageRank algorithm. One version contracting and deleting every node visited, and the other deleting only those visited nodes with less than 3 neighbours. I record snapshots of how the data-sets throughout the run of the algorithm, and compute the same topological properties computed before the run. I compare the changes in their homology to understand how the algorithm alters the topology of the graph. I also run several runs of the algorithms to get an idea of how the average graph looks like after the algorithm has been run. I record their new topological properties to find a correlation between the performance of the algorithm and the change in the topology of the graphs.</p>

corrected abstract:
<p>We provide a first test and overview of an alternative to Google's PageRank algorithm using persistent homology and topological data analysis. First, we present a data-set consisting of weighted, directed graphs. Then, we explain how we can transform said graphs to weighted, directed simplicial complexes. After that, we compute the persistent homology of the resulting complexes using Flagser by Daniel Luetgehetmann. Once we have the starting homological data, we introduce the two versions of the algorithm. One version contracts and deletes every node visited. The other deletes only visited nodes with less than 3 neighbours. We run the algorithm and re-compute persistent homology. Using the results, we analyse changes in the homology to determine how the algorithm changes the topological structure of our data-sets. Subsequently, we try to answer the question on whether there is a correlation between the changes in the persistent homology and the effectiveness and speed of the algorithm.</p>

Note - the DIVA abstract was very different from the original
----------------------------------------------------------------------
In diva2:1880210 
abstract is: 
<p>This thesis investigates the application of the Black-Scholes model for pricing long-maturity options, primarily utilizing historical data on S\&amp;P500 options. It compares prices computed with the Black-Scholes formula to actual market prices and critically examines the validity of the Black-Scholes model assumptions over long time frames. The assumptions mainly focused on are the constant volatility assumption, the assumption of normally distributed returns, the constant interest rate assumption and the no transaction cost assumption. The results show that the differences between computed prices and actual prices decrease as options get closer to maturity. They also show that several of the Black-Scholes model assumptions are not entirely realistic over long time frames. The conclusion of the thesis is that there are several limitations to the Black-Scholes model when it comes to pricing long-maturity options.</p>

corrected abstract:
<p>This thesis investigates the application of the Black-Scholes model for pricing long-maturity options, primarily utilizing historical data on S&amp;P500 options. It compares prices computed with the Black-Scholes formula to actual market prices and critically examines the validity of the Black-Scholes model assumptions over long time frames. The assumptions mainly focused on are the constant volatility assumption, the assumption of normally distributed returns, the constant interest rate assumption and the no transaction cost assumption. The results show that the differences between computed prices and actual prices decrease as options get closer to maturity. They also show that several of the Black-Scholes model assumptions are not entirely realistic over long time frames. The conclusion of the thesis is that there are several limitations to the Black-Scholes model when it comes to pricing long-maturity options.</p>

Note removed the "\" before "&"
----------------------------------------------------------------------
In diva2:1880272 
abstract is: 
<p>In recent decades, advancements in laser technology has made the creation of femtosecond lasers possible. This is a special type of laser where the laser beam consists of repeated high energy light bursts just a few hundred femtoseconds long as opposed to of the continuous laser beams found in every common laser pointer. The short pulse duration paired with the high energy in each burst results in a significant peak power, making the laser capable of processing materials in a way that a regular laser cannot. However, the large size and weight of the machines capable of producing femtosecond laser beams often require them to remain stationary. To utilize the laser beam for processing, precise redirection is necessary. In this report, we describe our process of converting a regular CNC machine into a laser processing station and present our findings from writing on glass, metal foil and KTP crystals. The machine is capable of following CAD instructions with micrometer precision to alter, inscribe and cut a range of materials. Processing was conducted with green ($\lambda$=514 nm) as well as infrared laser ($\lambda$=1028 nm), yielding better results for the latter. The finished laser setup could be used to repeatedly and reliably process all materials, with promising results on KTP when combined with chemical etching.</p>

corrected abstract:
<p>In recent decades, advancements in laser technology has made the creation of femtosecond lasers possible. This is a special type of laser where the laser beam consists of repeated high energy light bursts just a few hundred femtoseconds long as opposed to of the continuous laser beams found in every common laser pointer. The short pulse duration paired with the high energy in each burst results in a significant peak power, making the laser capable of processing materials in a way that a regular laser cannot. However, the large size and weight of the machines capable of producing femtosecond laser beams often require them to remain stationary. To utilize the laser beam for processing, precise redirection is necessary. In this report, we describe our process of converting a regular CNC machine into a laser processing station and present our findings from writing on glass, metal foil and KTP crystals. The machine is capable of following CAD instructions with micrometer precision to alter, inscribe and cut a range of materials. Processing was conducted with green (λ=514 nm) as well as infrared laser (λ=1028 nm), yielding better results for the latter. The finished laser setup could be used to repeatedly and reliably process all materials, with promising results on KTP when combined with chemical etching.</p>

Note replaced the LaTeX equation by the symbol for &lambda;
----------------------------------------------------------------------
In diva2:1444046 
abstract is: 
<p>Developing countermeasures dispenser systems requires many and careful tests. When it comes to testing products with pyrotechnics, testing can often be very complicated and expensive. This might lead to no testing at all due to time or resource shortages. Products to be used in the military requires further testing and even more thorough reviews to meet the strict demands placed on the products. In order to enable more tests of pyrotechnic flares in the countermeasures industry, this degree project aims to increase the ability to perform tests without the need for pyrotechnic means. This was done by designing, constructing and optimizing a recoil emulator, an apparatus that imitates the force-time curve obtained by pyrotechnic flares without the need of pyrotechnic means.</p><p>The construction of the recoil emulator was conducted at a department that develops countermeasure systems at Saab Surveillance in Järfälla. The apparatus aims to be used in the future for testing and verification of product series of countermeasures dispenser systems. The design of the apparatus was based on a test result provided by a flare manufacturer of an arbitrarily chosen flare, typical in the countermeasures industry. Based on the provided test result, three measures were chosen that together describe the fundamental and essential characteristic parts of the recoil motion behavior of pyrotechnic flares. These three measures are in the thesis called \textit{recoil measure} and defined as the Peak Recoil, the Impulse, and the Peak-Width.</p><p>To be able to verify the recoil emulator, the three recoil measures were implemented in an error model, which was based on the squares of error. In order to make the emulator imitate the desired recoil motion behavior as pleasant as possible, the error model was implemented in an optimization model. By minimizing the error of data points from each of the recoil measures obtained from the real test provided by the manufacturer with results obtained from the recoil emulator, the emulator was verified and optimized accordingly.</p><p>Results showed that the selected design of the recoil emulator resulted in a force-time curve that principally mimics the curve given by the real tests. The conclusion from the project was, therefore, that it is possible perform tests on countermeasures systems without pyrotechnics when considering the impact of recoil. Further development of this thesis could be to improve the construction of the recoil emulator and perform more research on flares and damping materials. Other future work could be to implement the emulator in existing test and validation processes at companies within the countermeasure industry.</p>

corrected abstract:
<p>Developing countermeasures dispenser systems requires many and careful tests. When it comes to testing products with pyrotechnics, testing can often be very complicated and expensive. This might lead to no testing at all due to time or resource shortages. Products to be used in the military requires further testing and even more thorough reviews to meet the strict demands placed on the products. In order to enable more tests of pyrotechnic flares in the countermeasures industry, this degree project aims to increase the ability to perform tests without the need for pyrotechnic means. This was done by designing, constructing and optimizing a recoil emulator, an apparatus that imitates the force-time curve obtained by pyrotechnic flares without the need of pyrotechnic means.</p><p>The construction of the recoil emulator was conducted at a department that develops countermeasure systems at Saab Surveillance in Järfälla. The apparatus aims to be used in the future for testing and verification of product series of countermeasures dispenser systems. The design of the apparatus was based on a test result provided by a flare manufacturer of an arbitrarily chosen flare, typical in the countermeasures industry. Based on the provided test result, three measures were chosen that together describe the fundamental and essential characteristic parts of the recoil motion behavior of pyrotechnic flares. These three measures are in the thesis called <em>recoil measure</em> and defined as the Peak Recoil, the Impulse, and the Peak-Width./p><p>To be able to verify the recoil emulator, the three recoil measures were implemented in an error model, which was based on the squares of error. In order to make the emulator imitate the desired recoil motion behavior as pleasant as possible, the error model was implemented in an optimization model. By minimizing the error of data points from each of the recoil measures obtained from the real test provided by the manufacturer with results obtained from the recoil emulator, the emulator was verified and optimized accordingly.</p><p>Results showed that the selected design of the recoil emulator resulted in a force-time curve that principally mimics the curve given by the real tests. The conclusion from the project was, therefore, that it is possible perform tests on countermeasures systems without pyrotechnics when considering the impact of recoil. Further development of this thesis could be to improve the construction of the recoil emulator and perform more research on flares and damping materials. Other future work could be to implement the emulator in existing test and validation processes at companies within the countermeasure industry.</p>

Note added the italics from the LaTeX command
----------------------------------------------------------------------
In diva2:1670637 
abstract is: 
<p>Electric buses often have batteries installed on the roof structure to have a better space utilization. This increases the height of the centre of gravity of the vehicle, affecting its roll stability. The existing vehicle setup uses an anti-roll bar to provide the roll stiffness. However, increasing the roll stiffness of the anti-roll bar for providing the required roll stability of an electric bus is limited due to the increase in weight of the anti-roll bar, its material properties and the design constraints. An alternate for the air suspension system is identified through a literature study. The identified system, an interconnected hydro-pneumatic suspension, is modelled analytically and compared to the air spring system. Multi-body simulations are performed to understand the roll performance.\par The thesis work also estimates the system's energy efficiency, and the feasibility of packaging the system within the existing vehicle architecture is studied in CAD software. The roll gradient of the vehicle is shown to improve compared to the existing air-spring system. The study also find that implementation of hydraulic-interconnected system can result in reduction of the unsprung mass of the vehicle.</p>
mc='performance.\\par' c='performance. \\par'

partal corrected: diva2:1670637: <p>Electric buses often have batteries installed on the roof structure to have a better space utilization. This increases the height of the centre of gravity of the vehicle, affecting its roll stability. The existing vehicle setup uses an anti-roll bar to provide the roll stiffness. However, increasing the roll stiffness of the anti-roll bar for providing the required roll stability of an electric bus is limited due to the increase in weight of the anti-roll bar, its material properties and the design constraints. An alternate for the air suspension system is identified through a literature study. The identified system, an interconnected hydro-pneumatic suspension, is modelled analytically and compared to the air spring system. Multi-body simulations are performed to understand the roll performance. \par The thesis work also estimates the system's energy efficiency, and the feasibility of packaging the system within the existing vehicle architecture is studied in CAD software. The roll gradient of the vehicle is shown to improve compared to the existing air-spring system. The study also find that implementation of hydraulic-interconnected system can result in reduction of the unsprung mass of the vehicle.</p>

corrected abstract:
<p>Electric buses often have batteries installed on the roof structure to have a better space utilization. This increases the height of the centre of gravity of the vehicle, affecting its roll stability. The existing vehicle setup uses an anti-roll bar to provide the roll stiffness. However, increasing the roll stiffness of the anti-roll bar for providing the required roll stability of an electric bus is limited due to the increase in weight of the anti-roll bar, its material properties and the design constraints. An alternate for the air suspension system is identified through a literature study. The identified system, an interconnected hydro-pneumatic suspension, is modelled analytically and compared to the air spring system. Multi-body simulations are performed to understand the roll performance.</p><p>The thesis work also estimates the system's energy efficiency, and the feasibility of packaging the system within the existing vehicle architecture is studied in CAD software. The roll gradient of the vehicle is shown to improve compared to the existing air-spring system. The study also find that implementation of hydraulic-interconnected system can result in reduction of the unsprung mass of the vehicle.</p>

Note replaced the \par with a </p><p>
----------------------------------------------------------------------
In diva2:1707010 
abstract is: 
<p>Density functional theory (DFT) calculations of polyethylene (PE) HVDC cable insulation have been performed for systems containing four different chemical impurities: acetophenone, cumene, $\alpha$-methyl styrene and $\alpha$-cumyl alcohol. Systems were generated by molecular dynamics (MD) equilibration at four different temperatures relevant for cable insulation applications: 277 K, 293 K, 343 K and 363 K. With the goal of gaining better measure of variations in hole and electron traps energies, four initial configurations were also stochastically generated at each temperature, which yielded four different final configurations after equilibration. The counterpoise correction scheme was implemented for DFT calculations, by distributing ghost atoms thought any empty pockets of space in between the PE chains. The PBE functional was selected for DFT simulations. The resulting band gaps were in agreement with those of earlier GGA-based studies, and thus lower by 3 eV than empirical band gaps. For all impurities, the first HOMO state and the first two LUMO states were generally located on the impurity molecule, forming one hole trap and two electron traps, but certain configurations generated increased electron trap numbers, or eliminated hole traps. No dependence could be derived between temperature and trap depth for either electron or hole traps. Mean electron trap energies were largely in agreement with results from earlier studies, they were deepest for acetophenone, and they varied by as much as 0.6 eV between different configurations. Hole traps are universally shallow and vary by up to 0.7 eV between configurations, and are similar in depth for all impurities. Results suggest that electron trap depths correlate with the presence of molecular features such as oxygen atoms and conjugated double bonds. The dependence of trap depth on the spatial configuration of the impurity molecule suggests that results could be improved by more precise quantum mechanical treatment of the dynamics of the impurity.</p>

corrected abstract:
<p>Density functional theory (DFT) calculations of polyethylene (PE) HVDC cable insulation have been performed for systems containing four different chemical impurities: acetophenone, cumene, α-methyl styrene and α-cumyl alcohol. Systems were generated by molecular dynamics (MD) equilibration at four different temperatures relevant for cable insulation applications: 277 K, 293 K, 343 K and 363 K. With the goal of gaining better measure of variations in hole and electron traps energies, four initial configurations were also stochastically generated at each temperature, which yielded four different final configurations after equilibration. The counterpoise correction scheme was implemented for DFT calculations, by distributing ghost atoms thought any empty pockets of space in between the PE chains. The PBE functional was selected for DFT simulations. The resulting band gaps were in agreement with those of earlier GGA-based studies, and thus lower by 3 eV than empirical band gaps. For all impurities, the first HOMO state and the first two LUMO states were generally located on the impurity molecule, forming one hole trap and two electron traps, but certain configurations generated increased electron trap numbers, or eliminated hole traps. No dependence could be derived between temperature and trap depth for either electron or hole traps. Mean electron trap energies were largely in agreement with results from earlier studies, they were deepest for acetophenone, and they varied by as much as 0.6 eV between different configurations. Hole traps are universally shallow and vary by up to 0.7 eV between configurations, and are similar in depth for all impurities. Results suggest that electron trap depths correlate with the presence of molecular features such as oxygen atoms and conjugated double bonds. The dependence of trap depth on the spacial configuration of the impurity molecule suggests that results could be improved by more precise quantum mechanical treatment of the dynamics of the impurity.</p>

Note replace the latex for \alpha with the character "α"
Note spelling error in original:
"spacial" should be "spatial"
----------------------------------------------------------------------
In diva2:1839766 
abstract is: 
<p>This thesis explores the spherically symmetric gravitational collapse of a massless scalar field in a minimally modified gravity theory denoted VCDM (V replaces $\Lambda$ in the $\Lambda$CDM abbreviation), a class of theories propagating the same degrees of freedom as general relativity at the expense of broken 4D diffeomorphism invariance. Numerical evolution of the equations of motion reveals that for small initial scalar profile amplitudes, no black hole forms from the collapse. However, for larger amplitudes, collapse leads to an apparent horizon's formation in finite time. Outside the horizon, the solution resembles the Schwarzschild geometry, while inside, the lapse function continues to decrease toward zero, implying the formation of a singularity/foliation breakdown. This suggests a need for a UV completion for the theory inside the horizon. Despite this, VCDM can describe the entire time evolution of the universe outside the black hole horizon without requiring knowledge of such a UV completion.</p>

corrected abstract:
<p>This thesis explores the spherically symmetric gravitational collapse of a massless scalar field in a minimally modified gravity theory denoted VCDM (V replaces Λ in the ΛCDM abbreviation), a class of theories propagating the same degrees of freedom as general relativity at the expense of broken 4D diffeomorphism invariance. Numerical evolution of the equations of motion reveals that for small initial scalar profile amplitudes, no black hole forms from the collapse. However, for larger amplitudes, collapse leads to an apparent horizon's formation in finite time. Outside the horizon, the solution resembles the Schwarzschild geometry, while inside, the lapse function continues to decrease toward zero, implying the formation of a singularity/foliation breakdown. This suggests a need for a UV completion for the theory inside the horizon. Despite this, VCDM can describe the entire time evolution of the universe outside the black hole horizon without requiring knowledge of such a UV completion.</p>

Note replace \Lambda with "Λ"
----------------------------------------------------------------------
In diva2:1817008 
abstract is: 
<p>This study investigates methods for assessing threats in space. Space services are crucial to both civilian and military capabilities, and a loss of such systems could have severe consequences. Space systems are exposed to various types of threats. To ensure the benefits of space-based applications, protect space assets, improve security, and maintain the space environment, it is crucial to assess threats in space. This thesis focuses on co-orbital antagonistic threats arising from satellites that are capable of performing precision manoeuvres. These satellites could either perform physical attacks or perform operations such as inspection, eavesdropping, or disruption on other satellites. Lambert's problem can be utilised for calculating orbital transfers. By solving the problem iteratively over a range of values of when the transfer is executed and the transfer time, it is possible to detect when a transfer is feasible. This can be used to assess when a satellite can pose a threat to a target. The calculations of orbital transfers are improved by the implementation of a genetic algorithm. The algorithm can solve for both direct transfers to the target and transfers using multiple impulses. Furthermore, a genetic algorithm, called NSGA-II, which can handle multiple objective functions is also analysed. The implemented methods show the potential of being employed to assess threats, especially for direct transfers where a single impulse is executed to transfer to a target. In this case, it is possible to identify threats based on the satellite's $\Delta v$ budget. However, when additional impulses are introduced it becomes more complicated. It is more difficult to estimate when an attack is more likely to commence. The implemented methods show potential, but further research is required in order to develop a robust method to assess co-orbital threats. </p><p>The conducted analysis has highlighted a few aspects that are crucial for assessing co-orbital threats. Information about the $\Delta v$ budget of the satellite that potentially could pose a threat must be available. Furthermore, space surveillance and tracking capabilities are essential to detect orbital changes, which can be vital to perform counter-operations in the event of an attack</p>

corrected abstract:
<p>This study investigates methods for assessing threats in space. Space services are crucial to both civilian and military capabilities, and a loss of such systems could have severe consequences. Space systems are exposed to various types of threats. To ensure the benefits of space-based applications, protect space assets, improve security, and maintain the space environment, it is crucial to assess threats in space. This thesis focuses on co-orbital antagonistic threats arising from satellites that are capable of performing precision manoeuvres. These satellites could either perform physical attacks or perform operations such as inspection, eavesdropping, or disruption on other satellites. Lambert's problem can be utilised for calculating orbital transfers. By solving the problem iteratively over a range of values of when the transfer is executed and the transfer time, it is possible to detect when a transfer is feasible. This can be used to assess when a satellite can pose a threat to a target. The calculations of orbital transfers are improved by the implementation of a genetic algorithm. The algorithm can solve for both direct transfers to the target and transfers using multiple impulses. Furthermore, a genetic algorithm, called NSGA-II, which can handle multiple objective functions is also analysed. The implemented methods show the potential of being employed to assess threats, especially for direct transfers where a single impulse is executed to transfer to a target. In this case, it is possible to identify threats based on the satellite's ∆𝑣 budget. However, when additional impulses are introduced it becomes more complicated. It is more difficult to estimate when an attack is more likely to commence. The implemented methods show potential, but further research is required in order to develop a robust method to assess co-orbital threats.</p><p>The conducted analysis has highlighted a few aspects that are crucial for assessing co-orbital threats. Information about the ∆𝑣 budget of the satellite that potentially could pose a threat must be available. Furthermore, space surveillance and tracking capabilities are essential to detect orbital changes, which can be vital to perform counter-operations in the event of an attack.</p>

Note replaced the latex commands with "∆𝑣"
----------------------------------------------------------------------
In diva2:1781237 
abstract is: 
<p>In this report, we aim to assess the sensitivity and 5$\sigma$ discovery potential of IceCube, the largest neutrino observatory on Earth, and compare it with prior findings. Our thesis will focus on a point source analysis, exploring the energy and declination dependencies, with particular emphasis on high-energy neutrinos. The primary objective is to establish the feasibility of detecting 5$\sigma$ evidence supporting the hypothesis that blazars serve as sources of neutrinos in the Southern sky, as suggested in a recent publication. Our findings indicate a substantial improvement in both discovery potential and sensitivity for the Southern sky in recent years. Furthermore, we highlight the increasing significance of investigating the origins of high-energy neutrinos in the Southern sky.</p>

corrected abstract:
<p>In this report, we aim to assess the sensitivity and 5σ discovery potential of IceCube, the largest neutrino observatory on Earth, and compare it with prior findings. Our thesis will focus on a point source analysis, exploring the energy and declination dependencies, with particular emphasis on high-energy neutrinos. The primary objective is to establish the feasibility of detecting 5σ evidence supporting the hypothesis that blazars serve as sources of neutrinos in the Southern sky, as suggested in a recent publication. Our findings indicate a substantial improvement in both discovery potential and sensitivity for the Southern sky in recent years. Furthermore, we highlight the increasing significance of investigating the origins of high-energy neutrinos in the Southern sky.</p>

Note replaced latex command with "σ"
----------------------------------------------------------------------
In diva2:1777317 
abstract is: 
<p>In chaos theory there are many different problems still unsolved. One of which is the optimization of infinite time average functionals on manifolds. To try one of the different tools to solve this problem we want to find stable manifolds in chaotic dynamical systems.In this thesis we find different manifolds for the Lorenz system when using a time dependent $\mu$ parameter and perform a sensitivity analysis on some of them. The existence of these manifolds are motivated numerically with the help of the shadowing lemma and extensive comparison of different numerical solvers.</p>

corrected abstract:
<p>In chaos theory there are many different problems still unsolved. One of which is the optimization of infinite time average functionals on manifolds. To try one of the different tools to solve this problem we want to find stable manifolds in chaotic dynamical systems. In this thesis we find different manifolds for the Lorenz system when using a time dependent µ parameter and perform a sensitivity analysis on some of them. The existence of these manifolds are motivated numerically with the help of the shadowing lemma and extensive comparison of different numerical solvers.</p>

Note replaced latex command with "µ"
----------------------------------------------------------------------
In diva2:664507 
abstract is: 
<p>  </p><p>This master thesis was performed at the section of Flight Mechanics and Performance at SAAB Aeronautics in Linköping as a part of my Master of Science in Engineering Physics at KTH, Stockholm. The aim of the thesis is to enable desktop simulations of missions from take-off to landing of JAS 39 Gripen.</p><p>The mission is set up by a series of task to be performed. Each tasks then link to a pilot model that controls the aircraft to perform the given task. The main part of the work has been to create these pilot models as an extension of the work by Ajdén and Backlund presented in [1].</p><p>The tasks that are simulated are, take-off, climb, turn, cruise, combat simulation, descent and landing at a given point. In order to perform these tasks both open and closed loop controls are used. To perform the landing first a path planing based on Dubins minimum path is calculated and then the nonlinear guidance logic presented by Park, Desyst and How in \cite{Park4} is implemented and used for trajectory tracking.</p><p>The results from a simulation of a test mission are presented and shows that mission simulations are possible and that the pilot models perform the intended tasks.</p><p>        </p>

corrected abstract:
<p>This master thesis was performed at the section of Flight Mechanics and Performance at SAAB Aeronautics in Linköping as a part of my Master of Science in Engineering Physics at KTH, Stockholm. The aim of the thesis is to enable desktop simulations of missions from take-off to landing of JAS 39 Gripen.</p><p>The mission is set up by a series of task to be performed. Each tasks then link to a pilot model that controls the aircraft to perform the given task. The main part of the work has been to create these pilot models as an extension of the work by Ajdén and Backlund presented in [1].</p><p>The tasks that are simulated are, take-off, climb, turn, cruise, combat simulation, descent and landing at a given point. In order to perform these tasks both open and closed loop controls are used. To perform the landing first a path planing based on Dubins minimum path is calculated and then the nonlinear guidance logic presented by Park, Desyst and How in [5] is implemented and used for trajectory tracking.</p><p>The results from a simulation of a test mission are presented and shows that mission simulations are possible and that the pilot models perform the intended tasks.</p>

Note put in the citation
----------------------------------------------------------------------
In diva2:1781262 
abstract is: 
<p>This thesis is the result of a literature study regarding the relationship between the Riemann zeta function and prime numbers. We introduce the $\zeta$ function, discussing its properties. Then, starting from Riemann's original series, we derive the Euler product formula and functional equation for $\zeta$. We then discuss finite-order integral functions and Hadamard products, as well as the Riemann $\xi$ function in order to determine some properties of the nontrivial zeros of $\zeta$. Then, we derive an integral function of the second Chebyshev function $\psi$, and use residue calculus in order to represent it as a sum over the zeros of $\zeta$. Lastly, we rewrite $\psi$ in order to get an estimate of $\pi$, proving the prime number theorem with a certain error term.</p>

corrected abstract:
<p>This thesis is the result of a literature study regarding the relationship between the Riemann zeta function and prime numbers. We introduce the ζ function, discussing its properties. Then, starting from Riemann's original series, we derive the Euler product formula and functional equation for ζ. We then discuss finite-order integral functions and Hadamard products, as well as the Riemann ξ function in order to determine some properties of the nontrivial zeros of ζ. Then, we derive an integral function of the second Chebyshev function ψ, and use residue calculus to represent it as a sum over the zeros of ζ. Lastly, we rewrite ψ to get an estimate of π, proving the prime number theorem with a certain error term.</p>

Note replaced the LaTeX command wtih the name symbol
----------------------------------------------------------------------
In diva2:1880221   - correct as is
----------------------------------------------------------------------
In diva2:1879615   - correct as is
----------------------------------------------------------------------
In diva2:1879416 
abstract is: 
<p>This thesis explores the process of porting a passive radar system from one SDR to another. Passive radar makes use of existing electromagnetic signals from sources such as TV and mobile phone towers to detect various objects. By leveraging consumer-grade SDRs, which have become increasingly accessible and powerful in recent years, new types of passive radar systems can be created. </p><p>The project involves adapting a passive radar system originally implemented on a KrakenSDR, to an AntSDR, which supports significantly higher bandwidth. This transition necessitates modifications to both the hardware and software setup to accommodate differences in firmware and sampling capabilities between the two SDRs. Through theoretical analysis and practical implementation, I detail required considerations for the passive radar systems' sampling, signal processing, and display.</p><p>The performance of the two systems in real-world scenarios is compared, focusing on their ability to detect and track aircraft in the vicinity of the Saab Järfälla site using a digital TV tower as the transmitter.Results demonstrate that the AntSDR system offers improved precision and detection capabilities due to its higher sampling rate, though challenges such as processing time and signal noise persist. This thesis underscores the feasibility and benefits of using modern SDRs for passive radar applications, while highlighting challenges and potential areas for further development.</p>
mc='transmitter.Results' c='transmitter. Results'

partal corrected: diva2:1879416: <p>This thesis explores the process of porting a passive radar system from one SDR to another. Passive radar makes use of existing electromagnetic signals from sources such as TV and mobile phone towers to detect various objects. By leveraging consumer-grade SDRs, which have become increasingly accessible and powerful in recent years, new types of passive radar systems can be created. </p><p>The project involves adapting a passive radar system originally implemented on a KrakenSDR, to an AntSDR, which supports significantly higher bandwidth. This transition necessitates modifications to both the hardware and software setup to accommodate differences in firmware and sampling capabilities between the two SDRs. Through theoretical analysis and practical implementation, I detail required considerations for the passive radar systems' sampling, signal processing, and display.</p><p>The performance of the two systems in real-world scenarios is compared, focusing on their ability to detect and track aircraft in the vicinity of the Saab Järfälla site using a digital TV tower as the transmitter. Results demonstrate that the AntSDR system offers improved precision and detection capabilities due to its higher sampling rate, though challenges such as processing time and signal noise persist. This thesis underscores the feasibility and benefits of using modern SDRs for passive radar applications, while highlighting challenges and potential areas for further development.</p>

corrected abstract:
<p>This thesis explores the process of porting a passive radar system from one SDR to another. Passive radar makes use of existing electromagnetic signals from sources such as TV and mobile phone towers to detect various objects. By leveraging consumer-grade SDRs, which have become increasingly accessible and powerful in recent years, new types of passive radar systems can be created.</p><p>The project involves adapting a passive radar system originally implemented on a KrakenSDR, to an AntSDR, which supports significantly higher bandwidth. This transition necessitates modifications to both the hardware and software setup to accommodate differences in firmware and sampling capabilities between the two SDRs. Through theoretical analysis and practical implementation, I detail required considerations for the passive radar systems' sampling, signal processing, and display.</p><p>The performance of the two systems in real-world scenarios is compared, focusing on their ability to detect and track aircraft in the vicinity of the Saab Järfälla site using a digital TV tower as the transmitter. Results demonstrate that the AntSDR system offers improved precision and detection capabilities due to its higher sampling rate, though challenges such as processing time and signal noise persist. This thesis underscores the feasibility and benefits of using modern SDRs for passive radar applications, while highlighting challenges and potential areas for further development.</p>

Note - only change deleted an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1878874 
abstract is: 
<p>Since their introduction in 1995, Support Vector Machines (SVM) have come to be a widely employed machine learning model for binary classification, owing to their explainable architecture, efficient forward inference, and good ability to generalize. A common desire, not only for SVMs but for machine learning classifiers in general, is to have the model do feature selection, using only a limited subset of the available attributes in its predictions. Various alterations to the SVM problem formulation exist that address this, and in this report we compare a range of such SVM models. We compare how the accuracy and feature selection compare between the models for different datasets, both real and synthetic, and we also investigate the impact of dataset size on the aforementioned quantities. </p><p>Our conclusions are that models trained to classify samples based on a smaller subset of features, tend to perform at a comparable level to dense models, with particular advantage when the dataset is small. Furthermore, as the training dataset grows in size, the number of selected features also increases, giving a more complex classifier when prompted with a larger data supply.</p>

corrected abstract:
<p>Since their introduction in 1995, Support Vector Machines (SVM) have come to be a widely employed machine learning model for binary classification, owing to their explainable architecture, efficient forward inference, and good ability to generalize. A common desire, not only for SVMs but for machine learning classifiers in general, is to have the model do feature selection, using only a limited subset of the available attributes in its predictions. Various alterations to the SVM problem formulation exist that address this, and in this report we compare a range of such SVM models. We compare how the accuracy and feature selection compare between the models for different datasets, both real and synthetic, and we also investigate the impact of dataset size on the aforementioned quantities.</p><p>Our conclusions are that models trained to classify samples based on a smaller subset of features, tend to perform at a comparable level to dense models, with particular advantage when the dataset is small. Furthermore, as the training dataset grows in size, the number of selected features also increases, giving a more complex classifier when prompted with a larger data supply.</p>

Note only change was to remove a space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1878717   - correct as is
......................................................................
In diva2:1876801   - correct as is
Note: no full text in DiVA
----------------------------------------------------------------------
In diva2:1876073   - correct as is
----------------------------------------------------------------------
In diva2:1875967   - correct as is
----------------------------------------------------------------------
In diva2:1873902 - the body of the thesis is a bitmap at under 72 dpi - so it cannot be OCR'd
abstract   - correct as is
----------------------------------------------------------------------
In diva2:1872789   - correct as is
----------------------------------------------------------------------
In diva2:1356906 
abstract is: 
<p>The vehicle sector is facing the biggest revolution of the last 70 years, whose main target is the depletion of emission and a steady decrease of fuel consumption. The focus in this report is the Green House Gas Emissions model (GEM) developed by the United States Environmental Protection Agency (EPA). The model is one of the newest and most complete simulation tool aimed to provide estimation of fuel consumption and CO2 emissions for the US market depending on the input inserted and the type of vehicle chosen. The usage of these tools is expected to increase in the next decades as it presents a more economical solution compared to chassis dynamometer and real driving test. Furthermore, all the environmental agencies that are developing these programs play also a key role in establishing new rules and regulations.</p><p>Scania, as one of the leading manufacturer companies of heavy duty vehicle worldwide, needs to learn about the new certification processes in US to be able to expand in this specific market. The aim of this work is to learn more about the GEM and to validate its results.</p><p>A theoretical review is performed to understand how GEM is working and how each parameter could possibly affect the outcomes. Several parameter variations on a long haulage HDV are performed with the GEM and output results presented. To validate the GEM the European simulation tool VECTO and an internal Scania worksheet for fuel consumption and emissions analysis are used as they were previously validated by means of the chassis dynamometer test. </p><p>GEM shows very similar results in this comparison with VECTO and the internal Scania worksheet.</p><p>Assessing the reliability of this programme on a larger scale could represent a key aspect for Original Equipment Manufacturers (OEMs) as they could use this simulation tool to easily and rapidly have some fuel economy results solely based on the specifications of the vehicle in analysis.</p>

corrected abstract:
<p>The vehicle sector is facing the biggest revolution of the last 70 years, whose main target is the depletion of emission and a steady decrease of fuel consumption. The focus in this report is the Green House Gas Emissions model (GEM) developed by the United States Environmental Protection Agency (EPA). The model is one of the newest and most complete simulation tool aimed to provide estimation of fuel consumption and CO<sub>2</sub> emissions for the US market depending on the input inserted and the type of vehicle chosen. The usage of these tools is expected to increase in the next decades as it presents a more economical solution compared to chassis dynamometer and real driving test. Furthermore, all the environmental agencies that are developing these programs play also a key role in establishing new rules and regulations.</p><p>Scania, as one of the leading manufacturer companies of heavy duty vehicle worldwide, needs to learn about the new certification processes in US to be able to expand in this specific market. The aim of this work is to learn more about the GEM and to validate its results.</p><p>A theoretical review is performed to understand how GEM is working and how each parameter could possibly affect the outcomes. Several parameter variations on a long haulage HDV are performed with the GEM and output results presented. To validate the GEM the European simulation tool VECTO and an internal Scania worksheet for fuel consumption and emissions analysis are used as they were previously validated by means of the chassis dynamometer test.</p><p>GEM shows very similar results in this comparison with VECTO and the internal Scania worksheet.</p><p>Assessing the reliability of this programme on a larger scale could represent a key aspect for Original Equipment Manufacturers (OEMs) as they could use this simulation tool to easily and rapidly have some fuel economy results solely based on the specifications of the vehicle in analysis.</p>

Note - only changes add subscript and removed space at end of a paragraph
Note that the original sets CO<sub>2</sub> as CO<span style="font-size: 0.8em;">2</span>
----------------------------------------------------------------------
In diva2:1698396 
abstract is: 
<p>Currently, the road cargo system with low or zero CO2 emission is under rapid development. Heavy-duty trucks with electrified driveline systems will be the workhorse of future freight. But developing such a brand new and very complex system and adapting it to various application scenarios, such as long-haul freight, city distribution or construction loading, is still a big problem, because there is no previous experience to refer to. There is no standard development procedure or constraint framework for uncertainty either. Simulation on a massive scale with thousands of truck agents will be of great use for developing such a road-cargo system.</p><p>System engineering will be the guiding methodology for this thesis project about developing a high-performance and multi-adaptive electrified driveline system. Referring to the classical V-shape development methodology, the complex concept will be divided into different levels of subsystems, from the large application scenarios to traffic simulation, driveline system simulation, electric motor and controller blocks development, and the system integration, performance verification and output of the results.</p><p>The massive scale of traffic simulation will be implemented in AnyLogic, which does not contain any accurate agent model with vehicle dynamic motion during simulation. Thus, a precise vehicle agent model needs to be developed and embedded into AnyLogic’s simulation scenario, so as to make the simulation very close to reality, and to be able to evaluate vehicle concepts as well. The driveline system will be developed in Matlab/Simulink while the information communication between them will be realised in the form of computational calculation functions through the C language program.</p><p>The development of the driveline model is also progressive. First, an equation-based full glider model was constructed. It simulates the scenario of a heavy-loaded truck driving on a steep slope (30% grade), decelerating from the initial 70 km/h to 0 km/h and then remaining stationary. The second model added the functionality of velocity input and output, enabling information exchange with AnyLogic. It will judge the real-time speed and the desired speed to decide whether to accelerate or decelerate and it uses the “Bang-Bang” control method of the electric motor. But this control mode results in a massive and frequent change in the electric motor output power, leading to extremely high energy consumption and in real life significantly shortened motor lifetime. So a powerful PI controller was introduced to the third Simulink model. The PI controller is embedded in the electric motor and it will replace the “Bang-Bang” control method. The “PID” control method provides a more stable power output so that the truck’s real-time speed can approach the target speed more smoothly. This control system can adapt to a variety of speed inputs and it can decide whether to output full power or partial power, depending on the speed difference.</p><p>The third version of the Simulink model with PI controller has been verified as an acceptable model through various inputs of different speeds, and it will be converted into a C language program to be embedded in AnyLogic for massive traffic simulation.</p>

corrected abstract:
<p>Currently, the road cargo system with low or zero CO<sub>2</sub> emission is under rapid development. Heavy-duty trucks with electrified driveline systems will be the workhorse of future freight. But developing such a brand new and very complex system and adapting it to various application scenarios, such as long-haul freight, city distribution or construction loading, is still a big problem, because there is no previous experience to refer to. There is no standard development procedure or constraint framework for uncertainty either. Simulation on a massive scale with thousands of truck agents will be of great use for developing such a road-cargo system.</p><p>System engineering will be the guiding methodology for this thesis project about developing a high-performance and multi-adaptive electrified driveline system. Referring to the classical V-shape development methodology, the complex concept will be divided into different levels of subsystems, from the large application scenarios to traffic simulation, driveline system simulation, electric motor and controller blocks development, and the system integration, performance verification and output of the results.</p><p>The massive scale of traffic simulation will be implemented in AnyLogic, which does not contain any accurate agent model with vehicle dynamic motion during simulation. Thus, a precise vehicle agent model needs to be developed and embedded into AnyLogic’s simulation scenario, so as to make the simulation very close to reality, and to be able to evaluate vehicle concepts as well. The driveline system will be developed in Matlab/Simulink while the information communication between them will be realised in the form of computational calculation functions through the C language program.</p><p>The development of the driveline model is also progressive. First, an equation-based full glider model was constructed. It simulates the scenario of a heavy-loaded truck driving on a steep slope (30% grade), decelerating from the initial 70 km/h to 0 km/h and then remaining stationary. The second model added the functionality of velocity input and output, enabling information exchange with AnyLogic. It will judge the real-time speed and the desired speed to decide whether to accelerate or decelerate and it uses the “Bang-Bang” control method of the electric motor. But this control mode results in a massive and frequent change in the electric motor output power, leading to extremely high energy consumption and in real life significantly shortened motor lifetime. So a powerful Proportional–Integral (PI) controller was introduced to the third Simulink model. The PI controller is embedded in the electric motor and it will replace the “Bang-Bang” control method. The “Proportional–Integral–Derivative (PID)” control method provides a more stable power output so that the truck’s real-time speed can approach the target speed more smoothly. This control system can adapt to a variety of speed inputs and it can decide whether to output full power or partial power, depending on the speed difference.</p><p>The third version of the Simulink model with PI controller has been verified as an acceptable model through various inputs of different speeds, and it will be converted into a C language program to be embedded in AnyLogic for massive traffic simulation.</p>

Note the original spelled out the acronyms and has a subscript
----------------------------------------------------------------------
In diva2:1737481 - missing subscript in title:
"Estimating CO2 emissions with satellite and traffic data: a Swedish practical case study"
==>
"Estimating CO<sub>2</sub> emissions with satellite and traffic data: a Swedish practical case study"

abstract is: 
<p>The large carbon footprint of industries is one of the main issues raised when talking about climate change. Active carbon monitoring methods need to be implemented to give transparency to the industry market and to spread awareness and information. This thesis investigates multiple CO2 emissions monitoring via satellite monitoring for four different industries in the EU. The pulp and paper industry was monitored in Sweden through smoke detection coming from the chimneys of factories. The CO2 aggregated emissions of 14 Swedish factories were calculated with a mean error of 12%. The metal ore roasting, and sintering industry were successfully monitored as well through smoke detection. In Sweden with an error 4.6%, and in the EU with an error 9.5 %. The production of lime and the calcination of dolomite were unsuccessfully monitored due to no suitable method found. Finally, coke industry emissions were monitored through burned gas monitoring. The CO2 emissions were correlated to the real emissions with a mean correlation coefficient of 0.64. This study took part in a public information campaign lead by a Swedish start-up, and some results were displayed in Stockholm, Sweden.</p>

corrected abstract:
<p>The large carbon footprint of industries is one of the main issues raised when talking about climate change. Active carbon monitoring methods need to be implemented to give transparency to the industry market and to spread awareness and information. This thesis investigates multiple CO<sub>2</sub> emissions monitoring via satellite monitoring for four different industries in the EU. The pulp and paper industry was monitored in Sweden through smoke detection coming from the chimneys’ factories. The CO<sub>2</sub> aggregated emissions of 14 Swedish factories were calculated with a mean error of 12%. The metal ore roasting and sintering industry were successfully monitored as well through smoke detection. In Sweden with an error 4.6%, and in the EU with an error 9.5%. The production of lime and the calcination of dolomite were unsuccessfully monitored due to no suitable method found. Finally, coke industry emissions were monitored through burned gas monitoring. The CO<sub>2</sub> emissions were correlated to the real emissions with a mean correlation coefficient of 0.64. This study took part in a public information campaign lead by a Swedish start-up, and some results were displayed in Stockholm, Sweden.</p>

Note there were small wording differences and the subscripts were missing in the DiVA entry
----------------------------------------------------------------------
In diva2:1757025 
abstract is: 
<p>One of the biggest topics of discussion in the last decade has been the climate change and the related global warming. This has rightfully contributed to people being more aware than ever about how to live a more sustainable life and to some extension know how they should act in their everyday life in order to reduce their carbon footprint. These are maybe the most important actions one could take in order to secure a healthy planet for generations to come. What is less known is how to invest in a sustainable way and how to understand if companies are operating sustainably. This thesis aims to solve this problem, enabling people to make more sustainable investment decisions through quickly analyzing the investment target’s financial performance. The result of the thesis is a model that can partly help investors to understand if a company is operating in a sustainable manner. The findings are that ROA, short for return on assets, is the most influential financial performance metric to look at when analyzing a company’s sustainable performance. Unfortunately, the model produced and suggested in this thesis can only explain 15.4% of the CO2 emissions produced by a company. That being said, there is still much to learn about how companies </p>

corrected abstract:
<p>One of the biggest topics of discussion in the last decade has been the climate change and the related global warming. This has rightfully contributed to people being more aware than ever about how to live a more sustainable life and to some extension know how they should act in their everyday life in order to reduce their carbon footprint. These are maybe the most important actions one could take in order to secure a healthy planet for generations to come. What is less known is how to invest in a sustainable way and how to understand if companies are operating sustainably. This thesis aims to solve this problem, enabling people to make more sustainable investment decisions through quickly analyzing the investment target’s financial performance. The result of the thesis is a model that can partly help investors to understand if a company is operating in a sustainable manner. The findings are that ROA, short for return on assets, is the most influential financial performance metric to look at when analyzing a company’s sustainable performance. Unfortunately, the model produced and suggested in this thesis can only explain 15.4% of the CO<sub>2</sub> emissions produced by a company. That being said, there is still much to learn about how companies </p>

Note added the subscript
----------------------------------------------------------------------
In diva2:1707861 
abstract is: 
<p>The impact of air travel on the climate, along with its increasing share in CO2 emissions have raised the demand for sustainable air travel solutions. The current aircraft technologies have seen significant improvement throughout the years. Although, the rate at which new aircraft technologies are developed can not keep up with the increased demand for air travel. Hence, a different approach to reduce the aviation’s impact on climate can be achieved by optimizing the vertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming.</p><p>Upon departure, an optimization of the vertical flight path is initiated and an optimal flight plan is suggested to the flight crew.</p><p> The fuel saving produced by the optimal flight plan is a potential saving that can only be fully achieved if the flight crew chose to fly according to the optimized flight path. However, restrictions from the Air Traffic Control, as well as the flight crew’s willingness to follow the optimized flight path can affect the achieved saving. Hence, a tool is developed in order to compute trip fuel consumption from post-flight data obtained from the Automatic Dependent Surveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start and end positions of cruise segments is successfully implemented. Two methods of calculating the fuel are implemented and compared. The first method is based on simulating the actual flight, which uses the same performance model as for the simulation of the operational flight plan trip and optimized trip. The second method is based on utilizing the ADS-B data to obtain the aircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft, hence the trip is not simulated. The results reveals that the simulation method produces flight trajectories that are comparable to the operational and optimized flight plans since they use the same model structure. However, using ADS-B data to obtain fuel consumption represents the actual flight trajectory more accurately.</p><p> Furthermore, an optimization algorithm based on the onboard Flight Management Computer is implemented. According to the results, the FMC optimization offers a sufficient optimization of the cruise phase, when compared to the OFP trip, however performs worse than the dynamic programming, which provides a global optimal solution.</p>

corrected abstract:
<p>The impact of air travel on the climate, along with its increasing share in CO<sub>2</sub> emissions have raised the demand for sustainable air travel solutions. The current aircraft technologies have seen significant improvement throughout the years. Although, the rate at which new aircraft technologies are developed can not keep up with the increased demand for air travel. Hence, a different approach to reduce the aviation’s impact on climate can be achieved by optimizing the vertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming. Upon departure, an optimization of the vertical flight path is initiated and an optimal flight plan is suggested to the flight crew.</p><p>The fuel saving produced by the optimal flight plan is a potential saving that can only be fully achieved if the flight crew chose to fly according to the optimized flight path. However, restrictions from the Air Traffic Control, as well as the flight crew’s willingness to follow the optimized flight path can affect the achieved saving. Hence, a tool is developed in order to compute trip fuel consumption from post-flight data obtained from the Automatic Dependent Surveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start and end positions of cruise segments is successfully implemented. Two methods of calculating the fuel are implemented and compared. The first method is based on simulating the actual flight, which uses the same performance model as for the simulation of the operational flight plan trip and optimized trip. The second method is based on utilizing the ADS-B data to obtain the aircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft, hence the trip is not simulated. The results reveals that the simulation method produces flight trajectories that are comparable to the operational and optimized flight plans since they use the same model structure. However, using ADS-B data to obtain fuel consumption represents the actual flight trajectory more accurately.</p><p>Furthermore, an optimization algorithm based on the on-board Flight Management Computer is implemented. According to the results, the FMC optimization offers a sufficient optimization of the cruise phase, when compared to the OFP trip, however performs worse than the dynamic programming, which provides a global optimal solution.</p>

Note removed misplaced paragraph break, added missing hyphen, and added subscripts
----------------------------------------------------------------------
In diva2:561616 
abstract is: 
<p>Growing pressure on the packaging design to enhance the environmental and logistics performance of a packaging system stresses the packaging designers to search new design strategies that not only fulfill logistics requirements in the supply chain, but also reduce the CO</p><p>2emissions during the packaging life cycle.</p><p>This thesis focuses on the packaging design process and suggests some improvements by considering its logistics performance and CO</p><p>2emissions. A Green packaging development model was proposed for corrugated box design to explore the inter-dependencies that exist among compressive strength, waste and CO2emissions.</p><p>The verification of the proposed model unveils the significance of a holistic view of the packaging system in the packaging design process and reveals the importance of packaging design decisions on the logistics performance and CO</p><p>2 emissions. The thesis finally concluded that the packaging logistics performance should be considered in a packaging design process to explore the Green packaging design solution.</p>

corrected abstract:
<p>Growing pressure on the packaging design to enhance the environmental and logistics performance of a packaging system stresses the packaging designers to search new design strategies that not only fulfill logistics requirements in the supply chain, but also reduce the CO<sub>2</sub> emissions during the packaging life cycle.</p><p>This thesis focuses on the packaging design process and suggests some improvements by considering its logistics performance and CO<sub>2</sub> emissions. A Green packaging development model was proposed for corrugated box design to explore the interdependencies that exist among compressive strength, waste and CO<sub>2</sub> emissions.</p><p>The verification of the proposed model unveils the significance of a holistic view of the packaging system in the packaging design process and reveals the importance of packaging design decisions on the logistics performance and CO<sub>2</sub> emissions. The thesis finally concluded that the packaging logistics performance should be considered in a packaging design process to explore the Green packaging design solution.</p>

Note corrected "CO</p><p>2" to CO<sub>2</p>" and added missing instances, and removed an unnecessary hyphen.
----------------------------------------------------------------------
In diva2:1698107 
abstract is: 
<p>The automotive industry and the whole transport sector are currently facing the need to act against climate change. In fact, over the globe the passenger road vehicles emitted 3.6 Gt of CO2 in 2018 and the road freight vehicles emitted 2.4 Gt of CO2 [1]. These road and freight emissions represents 11% and 7% of the total CO2 emissions that year respectively [2]. One solution that has been chosen to limit and reduce the greenhouse gas (GHG) emissions from road transportation is to shift from internal combustion engine (ICE)-based vehicles to electric vehicles which will emit no GHG during operation. There are mainly two types of electric vehicles suitable for this purpose. The first one is the battery electric vehicles (BEVs) which is already commercially and industrially mature and already on the road (11.3 million in 2020 [3]). It uses large Li-ion batteries to store the energy on-board to then power the electric motors. The second one is the fuel cell electric vehicles (FCEVs) which is still being researched and whose number on the road is quite limited (34.8 thousand in 2020 [4]). Yet, this technology is suitable for many applications and especially the light commercial vehicles (LCVs). The stakes of this technology have been studied regarding the current market of LCVs in France and by comparing it to BEVs in terms of cost and mass. To better frame the development of new hydrogen LCVs, a tool has also been developed to calculate the range of such vehicles throughout the life of the project and the evolution of its specifications. The analysis of the market and the comparison between FCEVs and BEVs is not exhaustive and only some specific points have been dealt with, enough to give an overview of the main stakes of hydrogen LCVs. The tool developed is limited to simple input data as it aims to be used with little information at an early stage of the project.</p>

corrected abstract:
<p>The automotive industry and the whole transport sector are currently facing the need to act against climate change. In fact, over the globe the passenger road vehicles emitted 3.6 Gt of CO<sub>2</sub> in 2018 and the road freight vehicles emitted 2.4 Gt of CO<sub>2</sub> [1]. These road and freight emissions represents 11% and 7% of the total CO<sub>2</sub> emissions that year respectively [2]. One solution that has been chosen to limit and reduce the greenhouse gas (GHG) emissions from road transportation is to shift from internal combustion engine (ICE)-based vehicles to electric vehicles which will emit no GHG during operation. There are mainly two types of electric vehicles suitable for this purpose. The first one is the battery electric vehicles (BEVs) which is already commercially and industrially mature and already on the road (11.3 million in 2020 [3]). It uses large Li-ion batteries to store the energy on-board to then power the electric motors. The second one is the fuel cell electric vehicles (FCEVs) which is still being researched and whose number on the road is quite limited (34.8 thousand in 2020 [4]). Yet, this technology is suitable for many applications and especially the light commercial vehicles (LCVs). The stakes of this technology have been studied regarding the current market of LCVs in France and by comparing it to BEVs in terms of cost and mass. To better frame the development of new hydrogen LCVs, a tool has also been developed to calculate the range of such vehicles throughout the life of the project and the evolution of its specifications. The analysis of the market and the comparison between FCEVs and BEVs is not exhaustive and only some specific points have been dealt with, enough to give an overview of the main stakes of hydrogen LCVs. The tool developed is limited to simple input data as it aims to be used with little information at an early stage of the project.</p>

Note that the original sets CO<sub>2</sub> as CO<span style="font-size: 0.8em;">2</span>
----------------------------------------------------------------------
In diva2:754073 
abstract is: 
<p>In today’s fast growing and closely connected society, a reliable and energy efficient transportation system is more than ever desirable. Nowadays the significant part of the transportation sector’s energy demand is supplied by fossil fuels.</p><p>Improving energy efficiency in combustion engines will result in reduction of fuel consumption and CO2 emissions. A modern internal combustion engine has an efficiency of 30-45 %, where the most energy loss occurs as result of heat losses in the exhaust and cooling systems. By recovering and converting the thermal energy of a combustion engine to mechanical/electric power the efficiency of the combustion engine can be increased.</p><p>In this work the performance of a truck engine has been investigated with the aim to increase its efficiency by decreasing the heat losses with a Waste Heat Recovery (WHR) system. Intercooler, coolant system, EGR and exhaust systems have been studied for their heat loss potentials.</p><p>A model of an ideal Rankine process has been implemented in MATLAB with water as the working fluid. Experimental data sets from a Scania DC1306 and a Volvo D11 engines have served as inputs to the developed MATLAB model. The study shows that an efficiency gain of approximately 2 % can be achieved with a WHR-system where EGR cooler serves as a heat source and 2.5 % with the exhaust as heat source. The combination of both systems can provide an efficiency gain between 4-5 %.</p>

corrected abstract:
<p>In today’s fast growing and closely connected society, a reliable and energy efficient transportation system is more than ever desirable. Nowadays the significant part of the transportation sector’s energy demand is supplied by fossil fuels.</p><p>Improving energy efficiency in combustion engines will result in reduction of fuel consumption and CO<sub>2</sub> emissions. A modern internal combustion engine has an efficiency of 30-45 %, where the most energy loss occurs as result of heat losses in the exhaust and cooling systems. By recovering and converting the thermal energy of a combustion engine to mechanical/electric power the efficiency of the combustion engine can be increased.</p><p>In this work the performance of a truck engine has been investigated with the aim to increase its efficiency by decreasing the heat losses with a Waste Heat Recovery (WHR) system. Intercooler, coolant system, EGR and exhaust systems have been studied for their heat loss potentials.</p><p>A model of an ideal Rankine process has been implemented in MATLAB with water as the working fluid. Experimental data sets from a Scania DC1306 and a Volvo D11 engines have served as inputs to the developed MATLAB model. The study shows that an efficiency gain of approximately 2 % can be achieved with a WHR-system where EGR cooler serves as a heat source and 2.5 % whit the exhaust as heat source. The combination of both systems can provide an efficiency gain between 4-5 %.</p>

Note added subscript
Note spelling error:
"whit" should be "with" - error in original
----------------------------------------------------------------------
In diva2:1847362 
Note: no full text in DiVA

abstract is: 
<p>In the wake of an escalating environmental crisis, underscored by global warming and its profound impacts on our planet, the quest for innovative solutions has never been more critical. This urgency propels the exploration of new materials and sustainable innovations to revolutionize energy and environmental remediation technologies. This study delves into the performance of Zinc Oxide (ZnO) and Titanium Dioxide (TiO2) nanorods, alongside a Co-Fe Prussian Blue Analogue (PBA) nanocomposite, within photofenton-like reactors aimed at wastewater treatment and hydrogen gas production from cellulose. Through a detailed literature review and subsequent experiments, it becomes evident that ZnO nanorods, with their approximate length of 800 nm, and TiO2 nanorods, measuring around 1 µm, exhibit photocatalytic degradation capabilities in a near-neutral environment (pH = 6). Specifically, 1 mg/ml of TiO2-PBA was capable of decomposing 92% of 10 ppm of Rhodamine B in 40 minutes with the aid of just 0.5 mM of persulfate (Peroxydisulfate, PDS), while the photoreforming of rapeseed cellulose with 4 mM of persulfate results in the production of 90 mmol/gh of hydrogen. Furthermore, the study illuminates the distinctive Fenton-like behavior and proton conductivity of Co-Fe PBA nanocubes without the aid of noble metals like Platinum (Pt), underlining their critical role in advanced oxidation processes. This study reveals how semiconductor materials interact with reactive oxidative species to efficiently degrade pollutants and produce hydrogen, highlighting significant advancements in water purification and sustainable energy solutions. By advancing our understanding of these photocatalytic systems, this research contributes significantly to the global efforts aimed at environmental preservation and energy sustainability, marking a pivotal step towards mitigating the adverse effects of climate change through innovative technological advancements.</p>

corrected abstract:
<p>In the wake of an escalating environmental crisis, underscored by global warming and its profound impacts on our planet, the quest for innovative solutions has never been more critical. This urgency propels the exploration of new materials and sustainable innovations to revolutionize energy and environmental remediation technologies. This study delves into the performance of Zinc Oxide (ZnO) and Titanium Dioxide (TiO<sub>2</sub>) nanorods, alongside a Co-Fe Prussian Blue Analogue (PBA) nanocomposite, within photofenton-like reactors aimed at wastewater treatment and hydrogen gas production from cellulose. Through a detailed literature review and subsequent experiments, it becomes evident that ZnO nanorods, with their approximate length of 800 nm, and TiO<sub>2</sub> nanorods, measuring around 1 µm, exhibit photocatalytic degradation capabilities in a near-neutral environment (pH = 6). Specifically, 1 mg/ml of TiO<sub>2</sub>-PBA was capable of decomposing 92% of 10 ppm of Rhodamine B in 40 minutes with the aid of just 0.5 mM of persulfate (Peroxydisulfate, PDS), while the photoreforming of rapeseed cellulose with 4 mM of persulfate results in the production of 90 mmol/gh of hydrogen. Furthermore, the study illuminates the distinctive Fenton-like behavior and proton conductivity of Co-Fe PBA nanocubes without the aid of noble metals like Platinum (Pt), underlining their critical role in advanced oxidation processes. This study reveals how semiconductor materials interact with reactive oxidative species to efficiently degrade pollutants and produce hydrogen, highlighting significant advancements in water purification and sustainable energy solutions. By advancing our understanding of these photocatalytic systems, this research contributes significantly to the global efforts aimed at environmental preservation and energy sustainability, marking a pivotal step towards mitigating the adverse effects of climate change through innovative technological advancements.</p>

Note - added subscripts
----------------------------------------------------------------------
In diva2:1670559   - correct as is
----------------------------------------------------------------------
In diva2:802086 
abstract is: 
<p>Road transport emission levels are at an all-time low and post Euro VI regulations are now up for discussion. A literature study of unregulated diesel emissions in Europe; CO2, N2O, NO2, CH4 and aldehydes has been made to determine the effects and importance of the emissions in today´s heavy-duty vehicles. This work aims to give better knowledge of the emissions with fundamental information about each emission’s formation, environmental effects, health effects, measuring methods, and reduction methods. Also examined is the possibility of limiting these emissions and what policies can be enforced in any future legislative directives.</p><p>The greenhouse gas emissions, CO2, N2O and CH4, from road transport are getting a lot of attention since they are hugely responsible for an increase of the global temperature. CO2 will clearly be the focus of future regulations. It is the most abundant emission and is the main cause of global warming. Reduction is best achieved through more fuel efficient vehicles but regulations and political means will also be needed to lower CO2 levels in the atmosphere. The European Commission have therefore agreed in 2014 to come up with a plan to cut their CO2 emissions from road transport. The most important ozone depleting substance today and the most potent of the greenhouse gases is N2O which has a GWP of 298. It is mainly produced by aftertreatment systems and its formation is highly dependent on temperature. CH4 is a regulated emission for CNG but not for diesel where the levels are much lower. It has a GWP of 34 and is plays a big role in global warming. Although it is an important emission to examine, the levels of CH4 from diesel vehicles today are negligible. In modern diesel vehicles NO2 emissions come from platinum catalysed DOCs and DPFs. NO2 is used for DPF regeneration and causes respiratory problems as well as contributing heavily to ozone formation and smog pollution. By adopting a better urea dosing strategy and choosing DPF coating material with less platinum NO2 can be reduced. Aldehydes are found in low concentrations in diesel but more in alternative fuels such as ethanol, and are important to study because of their carcinogenic properties and large contribution smog pollution. Studies have shown that the most abundant forms of aldehydes are formaldehyde and acetaldehyde for almost all fuel types, and that they can be reduced with efficient catalysts and high quality fuel. Different measurement techniques are used to analyse each of these mentioned emissions but their low levels require more accurate instruments with greater level of detail for measuring the substances.</p>

corrected abstract:
<p>Road transport emission levels are at an all-time low and post Euro VI regulations are now up for discussion. A literature study of unregulated diesel emissions in Europe; CO<sub>2</sub>, N<sub>2</sub>O, NO<sub>2</sub>, CH<sub>4</sub> and aldehydes has been made to determine the effects and importance of the emissions in today´s heavy-duty vehicles. This work aims to give better knowledge of the emissions with fundamental information about each emission’s formation, environmental effects, health effects, measuring methods, and reduction methods. Also examined is the possibility of limiting these emissions and what policies can be enforced in any future legislative directives.</p><p>The greenhouse gas emissions, CO<sub>2</sub>, N<sub>2</sub>O and CH<sub>4</sub>, from road transport are getting a lot of attention since they are hugely responsible for an increase of the global temperature. CO<sub>2</sub> will clearly be the focus of future regulations. It is the most abundant emission and is the main cause of global warming. Reduction is best achieved through more fuel efficient vehicles but regulations and political means will also be needed to lower CO<sub>2</sub> levels in the atmosphere. The European Commission have therefore agreed in 2014 to come up with a plan to cut their CO<sub>2</sub> emissions from road transport. The most important ozone depleting substance today and the most potent of the greenhouse gases is N<sub>2</sub>O which has a GWP of 298. It is mainly produced by aftertreatment systems and its formation is highly dependent on temperature. CH<sub>4</sub> is a regulated emission for CNG but not for diesel where the levels are much lower. It has a GWP of 34 and is plays a big role in global warming. Although it is an important emission to examine, the levels of CH<sub>4</sub> from diesel vehicles today are negligible. In modern diesel vehicles NO<sub>2</sub> emissions come from platinum catalysed DOCs and DPFs. NO<sub>2</sub> is used for DPF regeneration and causes respiratory problems as well as contributing heavily to ozone formation and smog pollution. By adopting a better urea dosing strategy and choosing DPF coating material with less platinum NO<sub>2</sub> can be reduced. Aldehydes are found in low concentrations in diesel but more in alternative fuels such as ethanol, and are important to study because of their carcinogenic properties and large contribution smog pollution. Studies have shown that the most abundant forms of aldehydes are formaldehyde and acetaldehyde for almost all fuel types, and that they can be reduced with efficient catalysts and high quality fuel. Different measurement techniques are used to analyse each of these mentioned emissions but their low levels require more accurate instruments with greater level of detail for measuring the substances.</p>

Note added missing subscripts
----------------------------------------------------------------------
In diva2:1342310 
abstract is: 
<p>In general, mankind has a need for transports. A significant part of these transports in Sweden is executed by car. For travels and transports that are too long or too time consuming for public transport, walking and/or bicycling, the need for the car within the Swedish society will persist. Travels by any car today is associated with significant carbon dioxide equivalent emissions - therefore it is desirable to use cars with minimal emissions, with regard to the vehicles entire life cycle. This report develops a method to carrying out a life cycle analysis (LCA) for a car with regards to a few data points about the vehicle. To achieve this, a literature review is carried out in the area to obtain data on emissions in the five life stages of material production, manufacturing, use, service and recycling for a general car, the variables being certain data points. These data points are then paired with the cars data to approximate their emissions. Two passenger car categories are chosen: sedan and SUV, within each five engine types are analysed: gasoline, diesel, HEV, PHEV and BEV. Also, a petrol and a diesel version of a used car that is considered to be common in the used market is analysed. The intention is to get an indication of which type of fuel has the lowest emissions in a life cycle, and to examine whether a new or used car has the lowest emissions. According to the results in this report, at an average mileage and lifetime and for the chosen vehicles, BEV is proven to be the vehicle with the lowest CO2e emissions over its entire life cycle. This is the true both for the sedan- and the SUV-category, assuming a maximum of one battery change. Then, in ascending order regarding the emission of CO2e follows PHEV, HEV, used diesel, new diesel, used gasoline and new gasoline. At an annual mileage above the average, and with the chosen vehicles within this report, the used diesel is proven to be the vehicle with the lowest emissions. For an annual mileage higher than the average, the vehicle within the selected vehicles with the lowest emission of CO2e is instead the BEV. This applies to both categories. One of the strengths of this reports method is its simplicity; With a few data points that can easily be adapted to a specific vehicle and driving habit, an approximation of the vehicle's net emission can be produced. Its main drawback is that many simplifications is made throughout the calculations, so the approximation becomes rough. However, this can be counteracted by replacing the general data used in the function with specific data for a particular vehicle, where such data is known. One problem is that the data used is sometimes old enough to be considered not completely accurate and usually not explicitly applicable to Sweden. More research in the field, and more locally specific research, is therefore desirable. A label similar to the energy label which ranks a vehicle's climate impact in the various life-parts separately would facilitate consumers to make a more climate-smart choice based on their own driving habits and would provide a more fair verdict regarding the emissions than just, as it is today, the energy use during the vehicles use phase.</p>

corrected abstract:
<p>In general, mankind has a need to transport themselves. In Sweden a significant part of this need is met via car. For travels and transports that are too long or too time consuming for public transport, walking or bicycling, the need for the car within the Swedish society will persist. Travels by any car today is associated with significant climate effect – therefore it is desirable to use cars with minimal emissions, with regard to the vehicles’ entire life cycle.</p><p>This report develops a method to carrying out an approximation of a life cycle analysis (LCA) for a car with regards to a few data points about the vehicle. To achieve this, a literature review is carried out in the area to obtain data on emissions in the five life stages of material production, manufacturing, use, service, and recycling for a general car, the variables being certain data points. These data points are then paired with the cars data to approximate their emissions. Two passenger car categories are chosen: sedan and SUV, within each five drive-train types are analysed: gasoline, diesel, HEV, PHEV and BEV.  Included are also a petrol and a diesel version of a used car that is considered to be common in the used market. The intention is to get an indication of which type of fuel has the lowest emissions in a life cycle, and to investigate how used cars compare to newly manufactured in terms of CO2e emissions.</p><p>According to the results in this report, at an average annual driving distance of 12,110 km and a lifespan of 17 years, BEV closely followed by PHEV are the types with the lowest CO2e emissions over their entire life cycles. This is the true both for the sedan and the SUV category, assuming a maximum of one battery change. Two other cases are tested, at an annual driving distance lower than the average, typically the need for drivers in metropolitan areas, PHEV is proven to be the vehicle with the lowest CO2e emissions while BEV has the highest CO2e emissions. At an annual driving distance higher than the average, typical for the average diesel car driver, BEV is proven to be the vehicle with the lowest CO2e emissions, which applies to both categories.</p><p>One of the strengths of this report’s method is its simplicity; with a few data points that can easily be adapted to a specific vehicle and driving habit, an approximation of the vehicle's net emission can be produced. Its main drawback is that many simplifications are made throughout the calculations, so the approximation becomes rough. However, this can be counteracted by replacing the general data used in the function with specific data for a particular vehicle, where such data is known.</p><p>One problem is that the data used is sometimes old enough to be considered not completely accurate and usually not explicitly applicable to Sweden. More research in the field, and more locally specific research, is therefore desirable. A label similar to the energy label which ranks a vehicle's climate impact in the various life-parts separately would facilitate consumers to make a more climate-smart choice based on their own driving habits and would provide a more fair verdict regarding the emissions than just, as it is today, the energy use during the vehicles use phase.</p>

Note changes in wording between DiVA and original
Note that the original does not set the 2 as a subscript in "CO2e"
----------------------------------------------------------------------
In diva2:892095   - correct as is
----------------------------------------------------------------------
In diva2:1721327 
abstract is: 
<p>Developments in computer vision has sought to design deep neural networks which trained on a large set of images are able to generate high quality artificial images which share semantic qualities with the original image set. A pivotal shift was made with the introduction of the generative adversarial network (GAN) by Goodfellow et al.. Building on the work by Goodfellow more advanced models using the same idea have shown great improvements in terms of both image quality and data diversity. GAN models generate images by feeding samples from a vector space into a generative neural network. The structure of these so called latent vector samples show to correspond to semantic similarities of their corresponding generated images. In this thesis the DCGAN model is trained on a novel data set consisting of image sequences of the growth process of basil plants from germination to harvest. We evaluate the trained model by comparing the DCGAN performance on benchmark data sets such as MNIST and CIFAR10 and conclude that the model trained on the basil plant data set achieved similar results compared to the MNIST data set and better results in comparison to the CIFAR10 data set. To argue for the potential of using more advanced GAN models we compare the results from the DCGAN model with the contemporary StyleGAN2 model. We also investigate the latent vector space produced by the DCGAN model and confirm that in accordance with previous research, namely that the DCGAN model is able to generate a latent space with data specific semantic structures. For the DCGAN model trained on the data set of basil plants, the latent space is able to distinguish between images of early stage basil plants from late stage plants in the growth phase. Furthermore, utilizing the sequential semantics of the basil plant data set, an attempt at generating an artificial growth sequence is made using linear interpolation. Finally we present an unsuccessful attempt at visualising the latent space produced by the DCGAN model using a rudimentary approach at inverting the generator network function.</p>

corrected abstract:
<p>Developments in computer vision has sought to design deep neural networks which trained on a large set of images are able to generate high quality artificial images which share semantic qualities with the original image set. A pivotal shift was made with the introduction of the generative adversarial network (GAN) by Goodfellow et al. [1]. Building on the work by Goodfellow more advanced models using the same idea have shown great improvements in terms of both image quality and data diversity. GAN models generate images by feeding samples from a vector space into a generative neural network. The structure of these so called latent vector samples show to correspond to semantic similarities of their corresponding generated images [2]. In this thesis the DCGAN model [2] is trained on a novel data set consisting of image sequences of the growth process of basil plants from germination to harvest. We evaluate the trained model by comparing the DCGAN performance on benchmark data sets such as MNIST and CIFAR10 and conclude that the model trained on the basil plant data set achieved similar results compared to the MNIST data set and better results in comparison to the CIFAR10 data set. To argue for the potential of using more advanced GAN models we compare the results from the DCGAN model with the contemporary StyleGAN2 model. We also investigate the latent vector space produced by the DCGAN model and confirm that in accordance with previous research, namely that the DCGAN model is able to generate a latent space with data specific semantic structures. For the DCGAN model trained on the data set of basil plants, the latent space is able to distinguish between images of early stage basil plants from late stage plants in the growth phase. Furthermore, utilizing the sequential semantics of the basil plant data set, an attempt at generating an artificial growth sequence is made using linear interpolation. Finally we present an unsuccessful attempt at visualising the latent space produced by the DCGAN model using a rudimentary approach at inverting the generator network function.</p>

Note added missing citations.
----------------------------------------------------------------------
In diva2:919841 
abstract is: 
<p>By continued advances in mechanical strength of press hardened boron alloyed steel (22MnB5) weight savings can be achieved by the use of less material. However, with reduced material thickness stiffness problems such as buckling arises and the high strength of the material can’t fully be taken advantage of. A solution to the stiffness problem could be to create a sandwich structure using 22MnB5 steel.</p><p>To create a sandwich structure the faces have to be joined to the core. Work has been done concerning various methods of joining both coated and uncoated 22Mnb5 after being press hardened. A more direct approach would be to implement a joining process in the hot stamping line prior to forming and quenching. A joining method suitable for this task is Controlled Atmosphere Brazing (CAB), which today is used for both aluminum and steel brazing and could be implemented in a hot stamping furnace. Using a tube furnace with continuous gas flow of N2 overlap braze joints were produced on both AlSi-coated 22MnB5 (USIBOR® 1500P AS150) and plain 22MnB5. Material combination evaluated included two different brazing foil and two different flux paste in combination with various heat cycles. Both one-step braze cycles with brazing directly at austenite temperature and two-step braze cycles with brazing at a lower temperature followed by heat treatment at austenite temperature were developed. Evaluation of braze joint strength was done using tensile testing and the same was done to evaluate the coating strength of USIBOR® 1500P AS150 after heat treatment using adhesive to create an overlap joint. An adhesion pull-off test was used to determine USIBOR® 1500P  AS150 coating strength after various heat cycles. Scanning Electron Microscope (SEM) with Energy Dispersive Spectrometry (EDS) was used to investigate amount of Fe-diffusion from substrate into both coating and joint due to heat cycles and to determine phases connected to fracture location of joints. With NiCr-based brazing foil joints between plain 22MnB5 were produced with a braze time of 10 minutes at 950° C that had an avg. shear strength of 20 MPa.</p><p>Brazing above the liquidus temperature of the filler material for 5 minutes with a higher furnace temperature showed no decrease in shear strength of joints, but reduced time in furnace by 16 minutes giving a total furnace time of 9 minutes. Best results for joints between USIBOR® 1500P AS150 substrates were achieved using a two-step brazing cycle. Brazing was done at 593° C for 35 minutes with AlSi12 brazing foil and flux paste recommended for aluminum brazing. Afterwards specimens where heated for 4 minutes above austenite temperature and had an avg. a shear strength of 5,4 MPa. EDS-analysis showed that fracture in braze joints and in coating of USIBOR® 1500P AS150 was connected to the intermetallic phase Al5Fe2 as well as Fe-diffusion from substrate was higher than in as received conventionally press hardened USIBOR® 1500P AS150. Adhesion pull-off tests indicated that the heat cycles used in this study significantly reduced coating strength compared to as received conventionally press hardened USIBOR® 1500P AS150.</p>

corrected abstract:
<p>By continued advances in mechanical strength of press hardened boron alloyed steel (22MnB5) weight savings can be achieved by the use of less material. However, with reduced material thickness stiffness problems such as buckling arises and the high strength of the material can’t fully be taken advantage of. A solution to the stiffness problem could be to create a sandwich structure using 22MnB5 steel.</p><p>To create a sandwich structure the faces have to be joined to the core. Work has been done concerning various methods of joining both coated and uncoated 22Mnb5 after being press hardened. A more direct approach would be to implement a joining process in the hot stamping line prior to forming and quenching.</p><p>A joining method suitable for this task is Controlled Atmosphere Brazing (CAB), which today is used for both aluminum and steel brazing and could be implemented in a hot stamping furnace. Using a tube furnace with continuous gas flow of N<sub>2</sub> overlap braze joints were produced on both AlSi-coated 22MnB5 (USIBOR® 1500P AS150) and plain 22MnB5. Material combination evaluated included two different brazing foil and two different flux paste in combination with various heat cycles. Both one-step braze cycles with brazing directly at austenite temperature and two-step braze cycles with brazing at a lower temperature followed by heat treatment at austenite temperature were developed. Evaluation of braze joint strength was done using tensile testing and the same was done to evaluate the coating strength of USIBOR® 1500P AS150 after heat treatment using adhesive to create an overlap joint. An adhesion pull-off test was used to determine USIBOR® 1500P AS150 coating strength after various heat cycles. Scanning Electron Microscope (SEM) with Energy Dispersive Spectrometry (EDS) was used to investigate amount of Fe-diffusion from substrate into both coating and joint due to heat cycles and to determine phases connected to fracture location of joints.</p><p>With NiCr-based brazing foil joints between plain 22MnB5 were produced with a braze time of 10 minutes at 950° C that had an avg. shear strength of 20 MPa. Brazing above the liquidus temperature of the filler material for 5 minutes with a higher furnace temperature showed no decrease in shear strength of joints, but reduced time in furnace by 16 minutes giving a total furnace time of 9 minutes. Best results for joints between USIBOR® 1500P AS150 substrates were achieved using a two-step brazing cycle. Brazing was done at 593° C for 35 minutes with AlSi12 brazing foil and flux paste recommended for aluminum brazing. Afterwards specimens where heated for 4 minutes above austenite temperature and had an avg. a shear strength of 5,4 MPa. EDS-analysis showed that fracture in braze joints and in coating of USIBOR® 1500P AS150 was connected to the intermetallic phase Al5Fe2 as well as Fe-diffusion from substrate was higher than in as received conventionally press hardened USIBOR® 1500P AS150. Adhesion pull-off tests indicated that the heat cycles used in this study significantly reduced coating strength compared to as received conventionally press hardened USIBOR® 1500P AS150.</p>

Note fixed paragraph breaks and added subscript
----------------------------------------------------------------------
In diva2:1728803 
abstract is: 
<p>The fuel cladding is an essential component in the defence-in-depth strategy for nuclear safety. Its integrity and durability are therefore critical for maintaining acceptable safety conditions. However, the integrity of the cladding can be compromised during normal operation due to corrosion and hydriding. To ensure a sufficient level of safety, design and safety criteria have been established to limit oxidation and hydriding. EDF has various multiphysics software tools at its disposal to ensure that these criteria are met. One such tool, CYRANO3, uses oxide thickness measurements from the beginning of the French nuclear industry to model corrosion and hydriding. This study aims to improve CYRANO3 by expanding its validation database and improving its models.</p><p>The first part of the study focuses on improving the CYRANO3 database by providing a more comprehensive understanding of normal corrosion in a pressurized water reactor, allowing the models to be recalibrated to better represent actual corrosion behaviour. </p><p>In the second part, a deeper analysis is conducted to improve the models and increase knowledge of the parameters that influence corrosion. This analysis highlights the significance of temperature and power as input parameters, which will affect the accuracy of CYRANO3 results. Additionally, this study has identified areas for further improvement, including modifications to the implemented corrosion models and a better understanding of the assumptions made about input data.</p>

corrected abstract:
<p>The fuel cladding is an essential component in the defence-in-depth strategy for nuclear safety. Its integrity and durability are therefore critical for maintaining acceptable safety conditions. However, the integrity of the cladding can be compromised during normal operation due to corrosion and hydriding. To ensure a sufficient level of safety, design and safety criteria have been established to limit oxidation and hydriding. EDF has various multiphysics software tools at its disposal to ensure that these criteria are met. One such tool, CYRANO3, uses oxide thickness measurements from the beginning of the French nuclear industry to model corrosion and hydriding. This study aims to improve CYRANO3 by expanding its validation database and improving its models.</p><p>The first part of the study focuses on improving the CYRANO3 database by providing a more comprehensive understanding of normal corrosion in a pressurized water reactor, allowing the models to be recalibrated to better represent actual corrosion behaviour.</p><p>In the second part, a deeper analysis is conducted to improve the models and increase knowledge of the parameters that influence corrosion. This analysis highlights the significance of temperature and power as input parameters, which will affect the accuracy of CYRANO3 results. Additionally, this study has identified areas for further improvement, including modifications to the implemented corrosion models and a better understanding of the assumptions made about input data.</p>

Note - only change was to remove an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:693569   - correct as is
----------------------------------------------------------------------
In diva2:1163182   - correct as is
----------------------------------------------------------------------
In diva2:1872323 
abstract is: 
<p>This report introduces a signal discrimination framework for particle physics processes, including a novel ensemble learning method using multiple machine learning models. The framework is tested with a signal region of the Higgs boson decay channel <em>H → WW∗ → lνlν</em> with two or more jets. The final state consists of leptons with the same flavor but opposite electrical charges, and the Higgs bosons are produced by Vector-boson fusion (VBF). The background region consists of the three largest processes with the same final state without originating from the signal process. Multiple models are trained and evaluated on Monte Carlo samples corresponding to a subset of the full Run 2 dataset of proton-proton collisions recorded by the ATLAS experiment at CERN’s Large Hadron Collider (LHC). The analysis in this report shows that ensemble methods improve background rejection leading to increased discrimination between the signal and background region compared with individual machine learning models.</p>

corrected abstract:
<p>This report introduces a signal discrimination framework for particle physics processes, including a novel ensemble learning method using multiple machine learning models. The framework is tested with a signal region of the Higgs boson decay channel 𝐻 → 𝑊𝑊<sup>∗</sup> → ℓνℓν with two or more jets. The final state consists of leptons with the same flavor but opposite electrical charges, and the Higgs bosons are produced by Vector-boson fusion (VBF). The background region consists of the three largest processes with the same final state without originating from the signal process. Multiple models are trained and evaluated on Monte Carlo samples corresponding to a subset of the full Run 2 dataset of proton-proton collisions recorded by the ATLAS experiment at CERN’s Large Hadron Collider (LHC). The analysis in this report shows that ensemble methods improve background rejection leading to increased discrimination between the signal and background region compared with individual machine learning models.</p>

Note - rather than use italics, I put the formula in with unicode
----------------------------------------------------------------------
In diva2:1871652 
abstract is: 
<p>This thesis project has been conducted during a five-month research exchange visit to the Space Structure Dynamics and Control research group at University College Dublin. This report presents the design, development, and validation of a nanosatellite attitude control testbed. The testbed was designed to replicate the microgravity conditions of space by utilising an air bearing, enabling single-axis rotational motion for a 1U CubeSat-sized nanosatellite. The novel aspect of this research is the inclusion of two-degree-of-freedom, lumped-mass flexible appendages on either side of the nanosatellite, emulating a lightweight, flexible space structure. These flexible appendages were designed based on the stiffness characteristics of a deployable CubeSat solar array system found in existing literature, with exaggerated motion to amplify the measurable effects of various control approaches. The central focus of this project was the development of an avionics stack closely resembling CubeSat attitude control boards. The stack uses an STM32 microcontroller as the primary attitude control computer, and a suite of off the shelf breakout boards for sensors and wireless telemetry systems. Power, serial and I2C buses connect the attitude control board and the onboard computer board. A reaction wheel actuator controls the Euler heading attitude. The testbed was designed as an experimental platform for validating control algorithms developed through a model-based approach. Integration with the Simulink Embedded Coder toolbox allows for the compilation of Simulink models into C code, facilitating direct execution on the testbed. The testbed’s physical construction involves 3D printed ABS components, with the inclusion of load cells to measure disturbance torques from the excited flexible appendages. Results from validation experiments show that a simple PID controller causes significant excitation in the flexible appendages during a slew manoeuvre. However, the introduction of an input shaped attitude profile tailored to the natural frequency of the appendages successfully reduced the measured appendage excitation by 50%. Conversely, the force impedance wave based control approach did not show a reduction in appendage excitation, but shows promise for further developments in future work. In conclusion, the testbed has successfully achieved its predefined project objectives, albeit requiring further refinement, particularly in the telemetry down-link system. It is recommended that future work focuses on enhancement of the telemetry system, and validation of a model based approach to controller design.</p>

corrected abstract:
<p>This thesis project has been conducted during a five-month research exchange visit to the Space Structure Dynamics and Control research group at University College Dublin. This report presents the design, development, and validation of a nanosatellite attitude control testbed. The testbed was designed to replicate the microgravity conditions of space by utilising an air bearing, enabling single-axis rotational motion for a 1U CubeSat-sized nanosatellite. The novel aspect of this research is the inclusion of two-degree-of-freedom, lumped-mass flexible appendages on either side of the nanosatellite, emulating a lightweight, flexible space structure. These flexible appendages were designed based on the stiffness characteristics of a deployable CubeSat solar array system found in existing literature, with exaggerated motion to amplify the measurable effects of various control approaches. The central focus of this project was the development of an avionics stack closely resembling CubeSat attitude control boards. The stack uses an STM32 microcontroller as the primary attitude control computer, and a suite of off the shelf breakout boards for sensors and wireless telemetry systems. Power, serial and I<sup>2</sup>C buses connect the attitude control board and the onboard computer board. A reaction wheel actuator controls the Euler heading attitude. The testbed was designed as an experimental platform for validating control algorithms developed through a model-based approach. Integration with the Simulink Embedded Coder toolbox allows for the compilation of Simulink models into C code, facilitating direct execution on the testbed. The testbed’s physical construction involves 3D printed ABS components, with the inclusion of load cells to measure disturbance torques from the excited flexible appendages. Results from validation experiments show that a simple PID controller causes significant excitation in the flexible appendages during a slew manoeuvre. However, the introduction of an input shaped attitude profile tailored to the natural frequency of the appendages successfully reduced the measured appendage excitation by 50%. Conversely, the force impedance wave based control approach did not show a reduction in appendage excitation, but shows promise for further developments in future work. In conclusion, the testbed has successfully achieved its predefined project objectives, albeit requiring further refinement, particularly in the telemetry down-link system. It is recommended that future work focuses on enhancement of the telemetry system, and validation of a model based approach to controller design.</p>

Note - only change adding the missing superscript
----------------------------------------------------------------------
In diva2:1861338 
abstract is: 
<p>This thesis evaluates the efficacy of Physics-Informed Neural Networks (PINNs) in simulating fluid dynamics challenges, focusing on the Burgers' equation and the lid-driven cavity problem, to develop a robust PINN framework for nuclear engineering applications such as the Sustainable Nuclear Energy Research In Sweden (SUNRISE) project. The research compares various PINN models to traditional Computational Fluid Dynamics (CFD) simulations to enhance predictive accuracy and computational efficiency for reactor design.</p><p>The study analyses and optimises diverse PINN configurations, employing automatic and numerical differentiation techniques and their integrative approaches, while investigating the incorporation of advanced artificial viscosity methods to augment model robustness and address limitations of standalone PINN methods.</p><p>Results show that enhanced PINN strategies achieve superior accuracy in solving the Burgers' equation and the lid-driven cavity problem at increased Reynolds numbers. For the Burgers' equation, one method with artificial viscosity achieved a Mean Squared Error (MSE) of 1.19⨉10⁻³. For the lid-driven cavity problem at Re 1000, another method without artificial viscosity yielded MSEs of 2.27⨉10⁻⁴, 9.54⨉10⁻⁵, and 1.81⨉10⁻⁵ for u, v, and p, respectively. These advancements highlight the potential of PINNs in nuclear engineering applications, particularly in tackling flow-accelerated corrosion and erosion in lead-cooled fast reactors within the SUNRISE project.</p>

corrected abstract:
<p>This thesis evaluates the efficacy of Physics-Informed Neural Networks (PINNs) in simulating fluid dynamics challenges, focusing on the Burgers' equation and the lid-driven cavity problem, to develop a robust PINN framework for nuclear engineering applications such as the Sustainable Nuclear Energy Research In Sweden (SUNRISE) project. The research compares various PINN models to traditional Computational Fluid Dynamics (CFD) simulations to enhance predictive accuracy and computational efficiency for reactor design.</p><p>The study analyses and optimises diverse PINN configurations, employing automatic and numerical differentiation techniques and their integrative approaches while investigating the incorporation of advanced artificial viscosity methods to augment model robustness and address limitations of standalone PINN methods.</p><p>Results show that enhanced PINN strategies achieve superior accuracy in solving the Burgers' equation and the lid-driven cavity problem at increased Reynolds numbers. For the Burgers' equation, one method with artificial viscosity achieved a Mean Squared Error (MSE) of 1.19 × 10<sup>−3</sup>. For the lid-driven cavity problem at Re 1000, another method without artificial viscosity yielded MSEs of 2.27 × 10<sup>-4</sup>, 9.54 × 10<sup>-5</sup>, and 1.81 × 10<sup>-5</sup> for 𝑢, 𝑣, and 𝑝, respectively. These advancements highlight the potential of PINNs in nuclear engineering applications, particularly in tackling flow-accelerated corrosion and erosion in lead-cooled fast reactors within the SUNRISE project.</p>

Note set the superscript with <sup>xxxx</sup> rather than superscript characters. Changed the large "⨉" to "×" to match the original
----------------------------------------------------------------------
In diva2:1752114 
abstract is: 
<p>The proximity of Supernova 1987A provides a great opportunity to study the aftermath of the impact of the ejecta with the dense clumps (observed as ”hot spots”) located in the equatorial circumstellar ring. This thesis examines the properties of the clumps by using Hubble Space Telescope optical imaging taken with the F625W and F675W broad filters between 1994 and 2022. The centroids and widths of the spots are measured by a 2-dimensional fitting of their intensities. These are subsequently used to determine the evolution of the spots.</p><p>The analysis shows that the spots are radially expanding initially with velocities from 140 km s⁻¹ up to ∼ 1700 km s⁻¹. This wide range of velocities could imply a wide range of densities for the clumps. There is evidence that the spots are slowing down after ∼ 7000 days, which might be a consequence of the blast wave leaving the ring. The acceleration of the spots remains undetectable, indicating that it might be happening in faster times than the observational time scale of one year. Data further suggest that the brightest spots are spatially resolved with projected diameters of (3 − 5) × 10¹⁶ cm. It is hinted that the spots are increasing in size after the impact, suggesting that new gas is being swept up, though the dissolution of the spots might also be responsible for this effect. A rough mass estimate for the resolved spots of order 10⁻² M⊙ is inferred from the measured widths and an upper limit of 1.6 M⊙ on the mass of the shocked gas in the clumps in the whole ring. These estimates though depend on the geometry and composition idealizations used.</p>

corrected abstract:
<p>The proximity of Supernova 1987A provides a great opportunity to study the aftermath of the impact of the ejecta with the dense clumps (observed as ”hot spots”) located in the equatorial circumstellar ring. This thesis examines the properties of the clumps by using Hubble Space Telescope optical imaging taken with the F625W and F675W broad filters between 1994 and 2022. The centroids and widths of the spots are measured by a 2-dimensional fitting of their intensities. These are subsequently used to determine the evolution of the spots.</p><p>The analysis shows that the spots are radially expanding initially with velocities from 140 km s<sup>-1</sup> up to ∼ 1700 km s<sup>-1</sup>. This wide range of velocities could imply a wide range of densities for the clumps. There is evidence that the spots are slowing down after ∼ 7000 days, which might be a consequence of the blast wave leaving the ring. The acceleration of the spots remains undetectable, indicating that it might be happening in faster times than the observational time scale of one year. Data further suggest that the brightest spots are spatially resolved with projected diameters of (3 − 5) × 10<sup>16</sup> cm. It is hinted that the spots are increasing in size after the impact, suggesting that new gas is being swept up, though the dissolution of the spots might also be responsible for this effect. A rough mass estimate for the resolved spots of order 10<sup>−2</sup> M<sub>⊙</sub> is inferred from the measured widths and an upper limit of 1.6 M<sub>⊙</sub> on the mass of the shocked gas in the clumps in the whole ring. These estimates though depend on the geometry and composition idealizations used.</p>

Note set the superscript with <sup>xxxx</sup> rather than superscript characters. 
Also added the subscripts.
----------------------------------------------------------------------
In diva2:1861024   - correct as is
Note: no full text in DiVA
----------------------------------------------------------------------
In diva2:1852457   - correct as is
Note: no full text in DiVA
----------------------------------------------------------------------
In diva2:1640963 
abstract is: 
<p>A superconducting nanowire single-photon detector (SNSPD) is an emerging, and today commercially available technology, for photon-counting and quantum cryptography. Yet, the photon detection event is not fully understood and current modeling efforts require substantial computational resources which motivates studies of simpler models. </p><p>This thesis introduces a model for vortex dynamics in thin-layered superconductors, such as SNSPDs, using a simplified approach, which leads to a 2D Coulomb gas model where the vortices are modeled as electrostatic charges. The model is carefully constructed from the method of images to describe a wire with open boundary conditions and an applied supercurrent. Subsequently, equilibrium and non-equilibrium properties are sampled with the Metropolis-Hastings algorithm and further analyzed and discussed.</p><p>The suggested model is shown to be effective and successfully reproduces expected SNSPD behavior; most importantly critical behavior and voltage pulses which are directly measured during detection events. In conclusion, a 2D Coulomb gas model can be a preferred alternative for modeling vortex dynamics in SNSPDs at a small computational cost, motivating further development and studies.</p>

corrected abstract:
<p>A superconducting nanowire single-photon detector (SNSPD) is an emerging, and today commercially available technology, for photon-counting and quantum cryptography. Yet, the photon detection event is not fully understood and current modeling efforts require substantial computational resources which motivates studies of simpler models.</p><p>This thesis introduces a model for vortex dynamics in thin-layered superconductors, such as SNSPDs, using a simplified approach, which leads to a 2D Coulomb gas model where the vortices are modeled as electrostatic charges. The model is carefully constructed from the method of images to describe a wire with open boundary conditions and an applied supercurrent. Subsequently, equilibrium and non-equilibrium properties are sampled with the Metropolis-Hastings algorithm and further analyzed and discussed.</p><p>The suggested model is shown to be effective and successfully reproduces expected SNSPD behavior; most importantly critical behavior and voltage pulses which are directly measured during detection events. In conclusion, a 2D Coulomb gas model can be a preferred alternative for modeling vortex dynamics in SNSPDs at a small computational cost, motivating further development and studies.</p>

Note - only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:812003 
abstract is: 
<p>The purpose of this thesis is to develop an algorithm which solves the Vehicle Relocation Problem in the One-Way Car-Sharing (VRLPOWCS) as fast as possible. The problem describes the task of relocating the cars to areas with the largest demand. The chauffeurs who relocate the cars are transported by shuttle buses. Each car is assigned an individual relocation utility. The objective is to find shuttle tours that maximise in a given time the relocation utility while balancing the distribution of the cars. The VRLPOWCS is formulated as a mixed integer linear program. Since this problem is NP<em>-complete </em>we choose the branch-and-cut method to solve it. Using additional cutting planes – which exploit the structure of the VRLPOWCS – we enhance this method. Tests on real data show that this extended algorithm can solve the VRLPOWCS faster.</p>

corrected abstract:
<p>The purpose of this thesis is to develop an algorithm which solves the Vehicle Relocation Problem in the One-Way Car-Sharing (VRLPOWCS) as fast as possible. The problem describes the task of relocating the cars to areas with the largest demand. The chauffeurs who relocate the cars are transported by shuttle buses. Each car is assigned an individual relocation utility. The objective is to find shuttle tours that maximise in a given time the relocation utility while balancing the distribution of the cars. The VRLPOWCS is formulated as a mixed integer linear program. Since this problem is ℕℙ-<em>complete</em> we choose the branch-and-cut method to solve it. Using additional cutting planes – which exploit the structure of the VRLPOWCS – we enhance this method. Tests on real data show that this extended algorithm can solve the VRLPOWCS faster.</p>

Note only changes: replace "NP" with "ℕℙ" and mobe the start and end of the italics.
----------------------------------------------------------------------
In diva2:562079 - unnessary period at end of title:
"A Brief Study of the Multifractal Model of Asset Returns."
==>
"A Brief Study of the Multifractal Model of Asset Returns"

abstract   - correct as is 
----------------------------------------------------------------------
In diva2:1282822 
abstract is: 
<p>This thesis addresses the problem with minimizing the cost of labor shifts for the workforce at an airport. The cost of idle time is specifically a difficulty for the employer. With idle time means time that it is not a break at the same time as there is no work task to be performed. This originates from big variations in the traffic flow which lead to the workload to be characterized by peaks and valleys. This situation has increased the demand among airport service companies for efficient staff schedules. Even small reductions of the idle time mean considerable savings for the employer. This thesis uses authentic data from an international airport in Europe. The method used to solve the task is an algorithm based on column generation. The mathematical model used has a high flexibility and handles breaks, multi-activity, such as boarding, and non-splittable tasks, in other words tasks that has to be performed by one employee in one shift. The subproblem is a binary integer program that generates feasible shifts following certain rules and is solved using a commercial solver. The results have shown possible improvements. In the best test scenario, the idle time is reduced to 4.7 percent of the total worktime. There is room for improvement of the model and the results. One possible improvement is to reduce the running time of the program which also could lead to improved results.</p>

corrected abstract:
<p>This thesis addresses the problem with minimizing the cost of labor shifts for the workforce at an airport. The cost of idle time is specifically a difficulty for the employer. With idle time means time that it is not a break at the same time as there is no work task to be performed. This originates from big variations in the traffic flow which lead to the workload to be characterized by peaks and valleys.</p><p>This situation has increased the demand among airport service companies for efficient staff schedules. Even small reductions of the idle time mean considerable savings for the employer.</p><p>This thesis uses authentic data from an international airport in Europe.</p><p>The method used to solve the task is an algorithm based on column generation. The mathematical model used has a high flexibility and handles breaks, multi-activity, such as boarding, and non-splittable tasks, in other words tasks that has to be performed by one employee in one shift. The subproblem is a binary integer program that generates feasible shifts following certain rules and is solved using a commercial solver.</p><p>The results have shown possible improvements. In the best test scenario, the idle time is reduced to 4.7 percent of the total worktime. There is room for improvement of the model and the results. One possible improvement is to reduce the running time of the program which also could lead to improved results.</p>

Note only change was to add the missing paragraph breaks
----------------------------------------------------------------------
In diva2:1319871   - correct as is
----------------------------------------------------------------------
In diva2:1438308 
abstract is: 
<p>Which numerical methods are ideal for training a neural network? In this report four different optimization methods are analysed and compared to each other. First, the most basic method Stochastic Gradient Descent that steps in the negative gradients direction. We continue with a slightly more advanced algorithm called ADAM, often used in practice to train neural networks. Finally, we study two second order methods, the Conjugate Gradient Method which follows conjugate directions, and L-BFGS, a Quasi-Newton method which approximates the inverse of the Hessian matrix. The methods are tasked to solve a classification problem with hyperspheres acting as decision boundaries and multiple different network configurations are used. Our results indicate why first order methods are so commonly used today and that second order methods can be difficult to use effectively when the number of parameters are large.</p>

corrected abstract:
<p>Which numerical methods are ideal for training a neural network? In this report four different optimization methods are analysed and compared to each other. First, the most basic method <em>Stochastic Gradient Descent</em> that steps in the negative gradients direction. We continue with a slightly more advanced algorithm called <em>ADAM</em>, often used in practice to train neural networks. Finally, we study two second order methods, the <em>Conjugate Gradient Method</em> which follows conjugate directions, and <em>L-BFGS</em>, a Quasi-Newton method which approximates the inverse of the Hessian matrix. The methods are tasked to solve a classification problem with hyperspheres acting as decision boundaries and multiple different network configurations are used. Our results indicate why first order methods are so commonly used today and that second order methods can be difficult to use effectively when the number of parameters are large.</p>

Note - added the italics
----------------------------------------------------------------------
In diva2:747997   - correct as is
----------------------------------------------------------------------
In diva2:1849045   - correct as is
----------------------------------------------------------------------
In diva2:866670   - correct as is
----------------------------------------------------------------------
In diva2:953108 
abstract is: 
<p>This thesis deals with cut finite element methods (CutFEM) for solving partial differential equations (PDEs) on evolving interfaces. Such PDEs arise for example in the study of insoluble surfactants in multiphase flow. In CutFEM, the interface is embedded in a larger mesh which need not respect the geometry of the interface. For example, the mesh of a two dimensional space containing a curve, may be used in order to solve a PDE on the curve. Consequently, in time-dependent problems, a fixed background mesh, in which the time-dependent domain is embedded, may be used. </p><p>The cut finite element method requires a representation of the interface. Previous work on CutFEM has mostly been done using linear segments to represent the interfaces. Due to the linear interface representation the proposed methods have been of, at most, second order. Higher order methods require better than linear interface representation. In this thesis, a second order CutFEM is implemented using an explicit spline representation of the interface and the convection-diffusion equation for surfactant transport along a deforming interface is solved on a curve subject to a given velocity field. </p><p>The markers, used to explicitly represent the interface, may due to the velocity field spread out alternately cluster. This may cause the interface representation to worsen. A method for keeping the interface markers evenly spread, proposed by Hou et al., is numerically investigated in the case of convection-diffusion. The method, as implemented, is shown to not be useful.</p>

corrected abstract:
<p>This thesis deals with cut finite element methods (<em>CutFEM</em>) for solving partial differential equations (<em>PDEs</em>) on evolving interfaces. Such PDEs arise for example in the study of insoluble surfactants in multiphase flow. In CutFEM, the interface is embedded in a larger mesh which need not respect the geometry of the interface. For example, the mesh of a two dimensional space containing a curve, may be used in order to solve a PDE on the curve. Consequently, in time-dependent problems, a fixed background mesh, in which the time-dependent domain is embedded, may be used.</p><p>The cut finite element method requires a representation of the interface. Previous work on CutFEM has mostly been done using linear segments to represent the interfaces. Due to the linear interface representation the proposed methods have been of, at most, second order,  see for example [6]. Higher order methods require better than linear interface representation. In this thesis, a second order CutFEM is implemented using an explicit spline representation of the interface and the convection-diffusion equation for surfactant transport along a deforming interface is solved on a curve subject to a given velocity field.</p><p>The markers, used to explicitly represent the interface, may due to the velocity field spread out alternately cluster. This may cause the interface representation to worsen. A method for keeping the interface markers evenly spread, proposed by Hou et al. in [8], is numerically investigated in the case of convection-diffusion. The method, as implemented, is shown to not be useful.</p>

Note added missing text and citations, also eliminated the unnecessary space at the end of paragraphs; Also added italics
----------------------------------------------------------------------
In diva2:849704 - the abstract page seems to have been scanned, but it was possible to OCR it
abstract is: 
<p>As an innovation project at Ecole Central de Paris, we were to develop a SaaS platform (Software as a Service) giving the user the possibility to perform forecasts based on data coming from various posts of the income statement and/or sales data. The platform was supposed to be able to perform short-term, mid-term as well as long-term forecasts based on these data. The platform has been developed through the web application framework Django, which is based on the programming language Python. The forecasting algorithm used is based on the preceding project on the subject, however it has been modified and improved.</p>

corrected abstract:
<p>As an innovation project at Ecole Centrale Paris, we were to develop a SaaS platform (Software as a Service) giving the user the possibility to perform forecasts based on data coming from various posts of the income statement and/or sales data. The platform was supposed to be able to perform short-term, mid-term as well as long-term forecasts based on these data. The platform has been developed through the web application framework Django, which is based on the programming language Python. The forecasting algorithm used is based on the preceding project on the subject, however it has been modified and improved.</p>

Note only a minor change to correct "Central de" to "Centrale"
----------------------------------------------------------------------
In diva2:1823772 
abstract is: 
<p>The aim of this study was to explore how match-related statistics contribute to winning association football matches. This is relevant for stakeholders in the football industry to facilitate the understanding of what factors contribute to winning matches and can thus be of use when formulating match tactics. A model was constructed through the use of binary logistic regression, where winning/not winning was used as the response variable and standardized match-related statistics were used as predictor variables. Using the acquired coefficients, it was concluded that, among other variables, the home advantage and the ability of a team to finish on target has a strong correlation with winninggames. Further, the study explores the impact of a team’s ability to win football games on the financial landscape of the modern football world. The results show that some of the examined statistics are well correlated to winning a match, but that the tactical useability of these insights is low.</p>

mc='winninggames' c='winning games'

corrected abstract:
<p>The aim of this study was to explore how match-related statistics contribute to winning association football matches. This is relevant for stakeholders in the football industry to facilitate the understanding of what factors contribute to winning matches and can thus be of use when formulating match tactics. A model was constructed through the use of binary logistic regression, where winning/not winning was used as the response variable, and standardized match-related statistics were used as predictor variables. Using the acquired coefficients, it was concluded that, among other variables, the home advantage and the ability of a team to finish on target has a strong correlation with winning games. Further, the study explores the impact of a team’s ability to win football games on the financial landscape of the modern football world. The results show that some of the examined statistics are well correlated to winning a match, but that the tactical useability of these insights is low.</p>
----------------------------------------------------------------------
In diva2:932583   - correct as is
----------------------------------------------------------------------
In diva2:902335   - correct as is
----------------------------------------------------------------------
In diva2:1737164   - correct as is
----------------------------------------------------------------------
***** 'diva2:1811727' *** need to comeback and look at this as the DivA abstract is completely different from the original in the full text of the thesis
----------------------------------------------------------------------
In diva2:892101 
abstract is: 
<p>The Swedish company Strömsholmen AB, which develops and manufactures gas springs for the tool and die industry, have entered the market of heavy duty off-road vehicles where they advertise their hydropneumatic suspension system. One of their customers is the Finnish defense company Patria, which produces the eight-wheeled military vehicle Patria AMV. In order to produce a more optimized suspension system for Patria, Strömsholmen is in need of learning more about how the vehicle dynamic properties such as handling are influenced by the suspension settings. To achieve this knowledge Strömsholmen started a collaboration with the consultant company FS Dynamics and initiated this master thesis work. The aim with this master thesis is to produce a simulation model of Patria AMV and investigate the influence of the suspension system settings on vehicle dynamic properties such as handling. The thesis work has resulted in a vehicle model in Adams/Car that is verified against experimental data from two slalom maneuvers. The model shows a good correlation with the validation data, taking into account the many assumptions and estimations that had to be made during the work due to lack of vehicle parameter data. A suspension parameter study was performed investigating vehicle maneuvers such as steady-state cornering and single-lane change. The results are summarized in a lookup table which can be used during future vehicle tests. Recommendations for future work is to verify some of the estimated vehicle parameters in the vehicle model as well as correlate test drivers subjective feel of the vehicle response to the calculated objective handling measures in this work.</p>

corrected abstract:
<p>The Swedish company Strömsholmen AB, which develops and manufactures gas springs for the tool and die industry, have entered the market of heavy duty off-road vehicles where they advertise their hydropneumatic suspension system. One of their customers is the Finnish defense company Patria, which produces the eight-wheeled military vehicle Patria AMV.</p><p>In order to produce a more optimized suspension system for Patria, Strömsholmen is in need of learning more about how the vehicle dynamic properties such as handling are influenced by the suspension settings. To achieve this knowledge Strömsholmen started a collaboration with the consultant company FS Dynamics and initiated this master thesis work.</p><p>The aim with this master thesis is to produce a simulation model of Patria AMV and investigate the influence of the suspension system settings on vehicle dynamic properties such as handling.</p><p>The thesis work has resulted in a vehicle model in Adams/Car that is verified against experimental data from two slalom maneuvers. The model shows a good correlation with the validation data, taking into account the many assumptions and estimations that had to be made during the work due to lack of vehicle parameter data.</p><p>A suspension parameter study was performed investigating vehicle maneuvers such as steady-state cornering and single-lane change. The results are summarized in a lookup table which can be used during future vehicle tests.</p><p>Recommendations for future work is to verify some of the estimated vehicle parameters in the vehicle model as well as correlate test drivers subjective feel of the vehicle response to the calculated objective handling measures in this work.</p>

Note - only added the missing paragraph breaks
----------------------------------------------------------------------
In diva2:1678831   - correct as is
----------------------------------------------------------------------
In diva2:1800177 
abstract is: 
<p>This paper uses a back-propagating neural network (BPN) to predict the price movements of major crypto currencies, leveraging technical factors as well as measurements of collective sentiment derived from the micro-blogging network Twitter. Our dataset consists of daily, hourly and minutely price levels for Bitcoin, Ether and Litecoin along with 8 popular technical indicators, as well as all tweets with the currencies' cash tags during respective time periods. Insprired by previous research which suggest that artificial neural networks are superior forecasting models in this setting, we were able to create a system generating automated investment decisions on a daily, hourly and minutely time basis. The study concluded that price trends are indeed predictable, with a correct prediction rate above 50% for all models, and corrensponding profitable trading strategies for all currencies on an hourly basis when neglecting trading fees, buy-sell spreads and order delays. The overall highest predictability is obtained on the hourly trading interval for Bitcoin, yielding an accuracy of 55.74% and a cumulative return of 175.1% between October 16, 2021 and December 31, 2021.</p>


corrected abstract:
<p>This paper uses a back-propagating neural network (BPN) to predict the price movements of major crypto currencies, leveraging technical factors as well as measurements of collective sentiment derived from the micro-blogging network Twitter. Our dataset consists of daily, hourly and minutely price levels for Bitcoin, Ether and Litecoin along with 8 popular technical indicators, as well as all tweets with the currencies' cash tags during respective time periods. Inspired by previous research which suggests that artificial neural networks are superior forecasting models in this setting, we were able to create a system generating automated investment decisions on a daily, hourly and minutely time basis. The study concluded that price trends are indeed predictable, with a correct prediction rate above 50% for all models, and corresponding profitable trading strategies for all currencies on an hourly basis when neglecting trading fees, buy-sell spreads and order delays. The overall highest predictability is obtained on the hourly trading interval for Bitcoin, yielding an accuracy of 55.74% and a cumulative return of 175.1% between October 16, 2021 and December 31, 2021.</p>
----------------------------------------------------------------------
In diva2:439972   - correct as is
----------------------------------------------------------------------
In diva2:1888155   - correct as is
----------------------------------------------------------------------
In diva2:1191148 
abstract is: 
<p>The vehicle routing problem is an old and well-studied problem that arise in last mile logistics. The rapid increase of e-commerce, in particular with an increasing the demand for time scheduled home deliveries on the customer’s terms, is making the problem ever more relevant. By minimizing the cost and environmental impact, we have the setting for mathematical problem called the vehicle routing problem with time windows.</p><p>Since the problem is NP-Hard, heuristic methods are often used. In practice, they work very well and typically offer a good tradeoff between speed and quality. However, since the heuristics are often tailormade to fit the needs of the underlying problem, no known algorithm dominates the other on all problems.</p><p>One way to overcome the need for specialization is to produce heuristics that are adaptive. In this thesis, an offline learning method is proposed to generate an adaptive heuristic using local search heuristics and reinforcement learning.</p><p>The reinforcement learning agents explored in this thesis are situated in both discrete and continuous state representations. Common to all state spaces are that they are inspired by human-crafted reference models where the last action and the result of that action define the state. Four different reinforcement learning models are evaluated in the various environments.</p><p>By evaluating the models on a subset of the Solomon benchmark instances, we find that all models but one improve upon a random baseline. The average learner from each of the successful models was slightly worse than the human crafted baseline. However, the best among the generated models was an actor-critic based model which outperformed the best human baseline model.</p><p>Due to the scalar objective function, the results are not directly comparable to the Solomon benchmark results with hierarchal objectives. None the less, the results are encouraging as a proof of principle with results in line with the human crafted baseline. The results indicate two clear paths for further work. First, applying the formulation to more complex environments with more actions and more powerful state spaces. Secondly, investigate models based on stochastic policies and recurrent neural networks to cope with the inherently partially observed environment.</p>

corrected abstract:
<p>The vehicle routing problem is an old and well-studied problem that arise in last mile logistics. The rapid increase of e-commerce, in particular with an increasing the demand for time scheduled home deliveries on the customer’s terms, is making the problem ever more relevant. By minimizing the cost and environmental impact, we have the setting for mathematical problem called the vehicle routing problem with time windows.</p><p>Since the problem is NP-Hard, heuristic methods are often used. In practice, they work very well and typically offer a good tradeoff between speed and quality. However, since the heuristics are often tailormade to fit the needs of the underlying problem, no known algorithm dominates the other on all problems.</p><p>One way to overcome the need for specialization is to produce heuristics that are adaptive. In this thesis, an offline learning method is proposed to generate an adaptive heuristic using local search heuristics and reinforcement learning.</p><p>The reinforcement learning agents explored in this thesis are situated in both discrete and continuous state representations. Common to all state spaces are that they are inspired by human-crafted reference models where the last action and the result of that action define the state. Four different reinforcement learning models are evaluated in the various environments.</p><p>By evaluating the models on a subset of the Solomon benchmark instances, we find that all models but one improve upon a random baseline. The average learner from each of the successful models was slightly worse than the human crafted baseline. However, the best among the generated models was an actor-critic based model which outperformed the best human baseline model.</p><p>Due to the scalar objective function, the results are not directly comparable to the Solomon benchmark results with hierarchal objectives. None the less, the results are encouraging as a proof of principle with results in line with the human crafted baseline.</p><p>The results indicate two clear paths for further work. First, applying the formulation to more complex environments with more actions and more powerful state spaces. Secondly, investigate models based on stochastic policies and recurrent neural networks to cope with the inherently partially observed environment.</p>

Note added missing paragraph break for the last paragraph
----------------------------------------------------------------------
In diva2:706524 
abstract is: 
<p>A method for efficient analysis of variations in utter speed predictions caused by parameter variations in the mass, stiffness and aerodynamic models of a wing structure is presented. The analysis method considers a linear uncertainty formulation and performs a perturbation analysis based on computation of eigenvalue differentials of a nominal system matrix. The method is applied in a test case study in which utter speed variations caused by variations in mass and aerodynamic properties of a delta wing model is analyzed. The report is concluded with a discussion of the validity of the results and of how the applicability of the method is affected by the assumptions on which it builds.</p>

corrected abstract:
<p>A method for efficient analysis of variations in flutter speed predictions caused by parameter variations in the mass, stiffness and aerodynamic models of a wing structure is presented. The analysis method considers a linear uncertainty formulation and performs a perturbation analysis based on computation of eigenvalue differentials of a nominal system matrix. The method is applied in a test case study in which flutter speed variations caused by variations in mass and aerodynamic properties of a delta wing model is analyzed. The report is concluded with a discussion of the validity of the results and of how the applicability of the method is affected by the assumptions on which it builds.</p>

Note added the missing "fl" due to a ligrature (in flutter)
----------------------------------------------------------------------
In diva2:874841 
abstract is: 
<p>A modal Perfectly Matched Layer (PML) is constructed for the two-dimensional, non-linear, Shallow Water Equations (SWEs) in conservative variables. The result is an analytical continuation of the original equations where absorption is applied to the outgoing wave modes which are damped exponentially fast in the direction of propagation. Numerical tests are performed using a variation of the Diagonally Implicit Runge-Kutta (DIRK) integration scheme in conjunction with Lax-Wendroff’s method for smooth so- lutions and some variation of Roe’s method for discontinuous solutions.</p><p>Different absorption functions are used, i.e. the absorption function of the original PML constructed by Berenger for electromagnetic waves, and some variations of the hyperbola functions. The results clearly show that the PML is better than the characteristic boundary condition, but also that improvements through some sort of optimization should lead to better parameter choices, potentially decreasing the reflections further.</p>

corrected abstract:
<p>A modal Perfectly Matched Layer (PML) is constructed for the two-dimensional, non-linear, Shallow Water Equations (SWEs) in conservative variables. The result is an analytical continuation of the original equations where absorption is applied to the outgoing wave modes which are damped exponentially fast in the direction of propagation. Numerical tests are performed using a variation of the Diagonally Implicit Runge-Kutta (DIRK) integration scheme in conjunction with Lax-Wendroff’s method for smooth solutions and some variation of Roe’s method for discontinuous solutions. Different absorption functions are used, i.e. the absorption function of the original PML constructed by Berenger for electromagnetic waves, and some variations of the hyperbola functions. The results clearly show that the PML is better than the characteristic boundary condition, but also that improvements through some sort of optimization should lead to better parameter choices, potentially decreasing the reflections further.</p>

Note removed unnecessary hyphen and an unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1655643   - correct as is
----------------------------------------------------------------------
In diva2:1221288   - correct as is
----------------------------------------------------------------------
In diva2:1795170 
abstract is: 
<p>With an increasingly globalised market and growing asset universe, estimating the market covariance matrix becomes even more challenging. In recent years, there has been an extensive development of methods aimed at mitigating these issues. This thesis takes its starting point in the recently developed Hierarchical Principal Component Analysis, in which a priori known information is taken into account when modelling the market correlation matrix. However, while showing promising results, the current framework only allows for fairly simple hierarchies with a depth of one. In this thesis, we introduce a generalisation of the framework that allows for an arbitrary hierarchical depth. We also evaluate the method in a risk-based portfolio allocation setting with Futures contracts. </p><p>Furthermore, we introduce a shrinkage method called Hierarchical Shrinkage, which uses the hierarchical structure to further regularise the matrix. The proposed models are evaluated with respect to how well-conditioned they are, how well they predict eigenportfolio risk and portfolio performance when they are used to form the Minimum Variance Portfolio. We show that the proposed models result in sparse and easy-to-interpret eigenvector structures, improved risk prediction, lower condition numbers and longer holding periods while achieving Sharpe ratios that are at par with our benchmarks.</p>

corrected abstract:
<p>With an increasingly globalised market and growing asset universe, estimating the market covariance matrix becomes even more challenging. In recent years, there has been an extensive development of methods aimed at mitigating these issues. This thesis takes its starting point in the recently developed Hierarchical Principal Component Analysis, in which a priori known information is taken into account when modelling the market correlation matrix. However, while showing promising results, the current framework only allows for fairly simple hierarchies with a depth of one. In this thesis, we introduce a generalisation of the framework that allows for an arbitrary hierarchical depth. We also evaluate the method in a risk-based portfolio allocation setting with Futures contracts.</p><p>Furthermore, we introduce a shrinkage method called Hierarchical Shrinkage, which uses the hierarchical structure to further regularise the matrix. The proposed models are evaluated with respect to how well-conditioned they are, how well they predict eigenportfolio risk and portfolio performance when they are used to form the Minimum Variance Portfolio. We show that the proposed models result in sparse and easy-to-interpret eigenvector structures, improved risk prediction, lower condition numbers and longer holding periods while achieving Sharpe ratios that are at par with our benchmarks.</p>

Note only change was to remove the unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1334739 
abstract is: 
<p>Small and medium-sized enterprises (SMEs) have long been considered the backbone in any country’s economy for their contribution to growth and prosperity. It is therefore of great importance that the government and legislators adopt policies that optimise the success of SMEs. Recent concerns of an impending recession has made this topic even more relevant since small companies will have greater difficulty withstanding such an event. This thesis will focus on the effects of macroeconomic factors on SMEs in Sweden, with the usage of multiple linear regression. Data was collected for a 10 year period, from 2009 to 2019 at a monthly interval. The end result was a five variable model with an coefficient of determination of 98%.</p>

corrected abstract:
<p>Small and medium-sized enterprises (SMEs) have long been considered the backbone in any country’s economy for their contribution to growth and prosperity. It is therefore of great importance that the government and legislators adopt policies that optimise the success of SMEs. Recent concerns of an impending recession has made this topic even more relevant since small companies will have greater difficulty withstanding such an event.</p><p>This thesis will focus on the effects of macroeconomic factors on SMEs in Sweden, with the usage of multiple linear regression. Data was collected for a 10 year period, from 2009 to 2019, at a monthly interval. The end result was a five variable model with a coefficient of determination of 98%.</p>

Note added missing paragraph break and missing comma.
----------------------------------------------------------------------
In diva2:1222265 
abstract is: 
<p>This thesis explored the possibilities of using a Hidden Markov Model approach for multi-target localisation in an urban environment, with observations generated from Wi-Fi sensors. The area is modelled as a network of nodes and arcs, where the arcs represent sidewalks in the area and constitutes the hidden states in the model. The output of the model is the expected amount of people at each road segment throughout the day. In addition to this, two methods for analyzing the impact of events in the area are proposed. The first method is based on a time series analysis, and the second one is based on the updated transition matrix using the Baum-Welch algorithm. Both methods reveal which road segments are most heavily affected by a surge of traffic in the area, as well as potential bottleneck areas where congestion is likely to have occurred.</p>

corrected abstract:
<p>This thesis explored the possibilities of using a Hidden Markov Model approach for multi-target localisation in an urban environment, with observations generated from Wi-Fi sensors. The area is modelled as a network of nodes and arcs, where the arcs represent sidewalks in the area and constitutes the hidden states in the model. The output of the model is the expected amount of people at each road segment throughout the day. In addition to this, two methods for analysing the impact of events in the area are proposed. The first method is based on a time series analysis, and the second one is based on the updated transition matrix using the Baum-Welch algorithm. Both methods reveal which road segments are most heavily affected by a surge of traffic in the area, as well as potential bottleneck areas where congestion is likely to have occurred.</p>

Note - only change "analyzing" to "analysing" - to match the original
----------------------------------------------------------------------
In diva2:1833747   - correct as is
----------------------------------------------------------------------
In diva2:1800205 
abstract is: 
<p>This thesis investigates how neural networks can be used to produce investors' views for the Black-Litterman market model. The study uses two data sets, one with global stock market indexes and one with stock market data from the S&amp;P 500. The task of the neural networks is to produce forecasts for the returns for the next quarter and the following year. The neural network will have to predict whether the market will move up or down and determine if the market movement is less than or equal to one standard deviation, creating four different scenarios. The forecasts are used as input to the Black-Litterman model to generate new portfolios, which are backtested from 2017 until 2022. The index data set was compared to a benchmark portfolio and a portfolio with naive risk diversification, while the S&amp;P 500 data set was compared to market capitalization-weighted and naive portfolios. This resulted in eight different backtests where the neural networks obtained AUC values in the range of 0.56-0.73 and prediction accuracies in the range of 20.9% - 42.1%. The network used for yearly predictions on the index data set was the only network to outperform the benchmark portfolio. It obtained a Sharpe ratio of 1.782, a Sortino ratio of 2.165, and a maximum drawdown of -30.9% compared to the benchmark portfolio, where the corresponding metrics were 1.544, 1.879, and -32.8%.</p>

corrected abstract:
<p>This thesis investigates how neural networks can be used to produce investors' views for the Black-Litterman market model. The study uses two data sets, one with global stock market indices and one with stock market data from the S&amp;P 500. The task of the neural networks is to produce forecasts for the returns for the next quarter and the following year. The neural network will have to predict whether the market will move up or down and determine if the market movement is less than or equal to one standard deviation, creating four different scenarios. The forecasts are used as input to the Black-Litterman model to generate new portfolios, which are backtested from 2017 until 2022. The index data set was compared to a benchmark portfolio and a portfolio with naive risk diversification, while the S&amp;P 500 data set was compared to market capitalization-weighted and naive portfolios. This resulted in eight different backtests where the neural networks obtained AUC values in the range of 0.56 - 0.73 and prediction accuracies in the range of 20.9% - 42.1%. The network used for yearly predictions on the index data set was the only network to outperform the benchmark portfolio. It obtained a Sharpe ratio of 1.782, a Sortino ratio of 2.165, and a maximum drawdown of -30.9% compared to the benchmark portfolio, where the corresponding metrics were 1.544, 1.879, and -32.8%.</p>

Note change is the dashes (hypens) used.
----------------------------------------------------------------------
In diva2:1215621 
abstract is: 
<p>This study investigates a neural networks approach to portfolio choice. Linear regression models are extensively used for prediction. With the return as the output variable, one can come to understand its relation to the explanatory variables the linear regression is built upon. However, if the relationship between the output and input variables is non-linear, the linear regression model may not be a suitable choice. An Artificial Neural Network (ANN) is a non-linear statistical model that has been shown to be a “good” approximator of non-linear functions. In this study, two different ANN models are considered, Feed-forward Neural Networks (FNN) and Recurrent Neural Networks (RNN). Networks from these models are trained to predict monthly returns on asset data consisting of macroeconomic data and market data. The predicted returns are then used in a long-short portfolio strategy. The performance of these networks and their corresponding portfolios are then compared to a benchmark linear regression model. Metrics such as average hit-rate, mean squared prediction error, portfolio value and riskadjusted returns are used to evaluate the model performances. The linear regression and the feed-forward model yielded good average hit-rates and mean squared-errors, but poor portfolio performances. The recurrent neural network models yielded worse average hit-rates and mean squared prediction errors, but had outstanding portfolio performances</p>

corrected abstract:
<p>This study investigates a neural networks approach to portfolio choice. Linear regression models are extensively used for prediction. With the return as the output variable, one can come to understand its relation to the explanatory variables the linear regression is built upon. However, if the relationship between the output and input variables is non-linear, the linear regression model may not be a suitable choice. An Artificial Neural Network (ANN) is a non-linear statistical model that has been shown to be a “good” approximator of non-linear functions. In this study, two different ANN models are considered, Feed-forward Neural Networks (FNN) and Recurrent Neural Networks (RNN). Networks from these models are trained to predict monthly returns on asset data consisting of macroeconomic data and market data. The predicted returns are then used in a long-short portfolio strategy. The performance of these networks and their corresponding portfolios are then compared to a benchmark linear regression model. Metrics such as average hit-rate, mean squared prediction error, portfolio value and risk-adjusted returns are used to evaluate the model performances. The linear regression and the feed-forward model yielded good average hit-rates and mean squared-errors, but poor portfolio performances. The recurrent neural network models yielded worse average hit-rates and mean squared prediction errors, but had outstanding portfolio performances.</p>

Note added hyphen to "risk-adjusted" and added terminal period to last paragraph
----------------------------------------------------------------------
In diva2:1222440 
abstract is: 
<p>Maribot Vane is an autonomous sailboat project at KTH Royal Institute of Technology. The use of autonomous boats is being recognised all over the world as a cost-efficient alternative to traditional manned ships for oceanographic research. Vane consists of an International 2.4 mR hull propelled by a self-adjusting wing that is controlled by a flap. A self-steering mechanism is currently under development. Field testing of the boat in the summer of 2017 showed that the boat was leaking where the mast enters the deck as well as through a hatch covering the former cockpit.</p><p>This report deals with developing a new sealing solution to prevent water from entering the boat. It should be a durable and waterproof solution. Minimizing friction is of great importance to reduce interference with the self-adjusting wing. The problem is divided into two sub-problems: creating a sealing where the mast enters the boat and designing a new hatch. A housing made of 3D-printed plastic will be placed around the mast. By establishing models depicting “worst-case” scenarios calculations are done to determine how long the housing can stay submerged as well as how much impact it has to endure when being hit by a wave. Experiments are then performed on prototypes of the housing to determine how accurate the theoretical models are.</p><p>A housing that theoretically can stay submerged for approximately three seconds is developed. Analysis suggests that it is durable enough to withstand the impact from being hit by a wave. A hatch consisting of two parts is also developed. One placed in the front where the mast goes through and one in the back that should be easy to open, providing access to the inner parts of the boat even when in water.</p>

corrected abstract:
<p><em>Maribot Vane</em> is an autonomous sailboat project at KTH Royal Institute of Technology. The use of autonomous boats is being recognised all over the world as a cost-efficient alternative to traditional manned ships for oceanographic research. <em>Vane</em> consists of an International 2.4 mR hull propelled by a self-adjusting wing that is controlled by a flap. A self-steering mechanism is currently under development. Field testing of the boat in the summer of 2017 showed that the boat was leaking where the mast enters the deck as well as through a hatch covering the former cockpit.</p><p>This report deals with developing a new sealing solution to prevent water from entering the boat. It should be a durable and waterproof solution. Minimizing friction is of great importance to reduce interference with the self-adjusting wing. The problem is divided into two sub-problems: creating a sealing where the mast enters the boat and designing a new hatch. A housing made of 3D-printed plastic will be placed around the mast. By establishing models depicting “worst-case” scenarios calculations are done to determine how long the housing can stay submerged as well as how much impact it has to endure when being hit by a wave. Experiments are then performed on prototypes of the housing to determine how accurate the theoretical models are.</p><p>A housing that theoretically can stay submerged for approximately three seconds is developed. Analysis suggests that it is durable enough to withstand the impact from being hit by a wave. A hatch consisting of two parts is also developed. One placed in the front where the mast goes through and one in the back that should be easy to open, providing access to the inner parts of the boat even when in water.</p>

Note - only change was to add the italics for the name of te sailboat project
----------------------------------------------------------------------
In diva2:1438316   - correct as is
----------------------------------------------------------------------
In diva2:1644826   - correct as is
----------------------------------------------------------------------
In diva2:753569 
abstract is: 
<p>When a mammalian cell is exposed to a carcinogen, the carcinogen gives rise to new compounds. In this paper a numerical approach is taken to find the amount of these compounds overtime. The concentration of the compounds are assumed to satisfy the reaction and/or the diffusion equation, techniques used for solving these equations include the one-dimensional finite element method (FEM) and the lumped parameter approach. The mathematically derived results are compared to in-vitro measurements. The mathematical model set up in this paper will prove insufficient.</p>

corrected abstract:
<p>When a mammalian cell is exposed to a carcinogen, the carcinogen gives rise to new compounds. In this paper a numerical approach is taken to find the amount of these compounds over time. The concentration of the compounds are assumed to satisfy the reaction and/or the diffusion equation, techniques used for solving these equations include the one-dimensional finite element method (FEM) and the lumped parameter approach. The mathematically derived results are compared to in-vitro measurements. The mathematical model set up in this paper will prove insufficient.</p>

Note - only change to split "overtime" in to "over time"
----------------------------------------------------------------------
In diva2:559080 Note spelling error:
----------------------------------------------------------------------
In diva2:855388   - correct as is
----------------------------------------------------------------------
In diva2:1827745 
abstract is: 
<p>Churn refers to the discontinuation of a contract; consequently, customer churn occurs when existing customers stop being customers. Predicting customer churn is a challenging task in customer retention, but with the advancements made in the field of artificial intelligence and machine learning, the feasibility to predict customer churn has increased. Prior studies have demonstrated that machine learning can be utilized to forecast customer churn. The aim of this thesis was to develop and implement a machine learning model to predict customer churn and identify the customer features that have a significant impact on churn. This Study has been conducted in cooperation with the Swedish insurance company Bliwa, who expressed interest in gaining an increased understanding of why customers choose to leave. </p><p>Three models, Logistic Regression, Random Forest, and Gradient Boosting, were used and evaluated. Bayesian optimization was used to optimize the models. After obtaining an indication of their predictive performance during evaluation using Cross-Validation, it was concluded that LightGBM provided the best result in terms of PR-AUC, making it the most effective approach for the problem at hand.</p><p>Subsequently, a SHAP-analysis was carried out to gain insights into which customer features that have an impact on whether or not a customer churn. The outcome of the SHAP-analysis revealed specific customer features that had a significant influence on churn. This knowledge can be utilized to proactively implement measures aimed at reducing the probability of churn.</p>

corrected abstract:
<p>Churn refers to the discontinuation of a contract; consequently, customer churn occurs when existing customers stop being customers. Predicting customer churn is a challenging task in customer retention, but with the advancements made in the field of artificial intelligence and machine learning, the feasibility to predict customer churn has increased. Prior studies have demonstrated that machine learning can be utilized to forecast customer churn. The aim of this thesis was to develop and implement a machine learning model to predict customer churn and identify the customer features that have a significant impact on churn. This Study has been conducted in cooperation with the Swedish insurance company Bliwa, who expressed interest in gaining an increased understanding of why customers choose to leave.</p><p>Three models, Logistic Regression, Random Forest, and Gradient Boosting, were used and evaluated. Bayesian optimization was used to optimize the models. After obtaining an indication of their predictive performance during evaluation using Cross-Validation, it was concluded that LightGBM provided the best result in terms of PR-AUC, making it the most effective approach for the problem at hand.</p><p>Subsequently, a SHAP-analysis was carried out to gain insights into which customer features that have an impact on whether or not a customer churn. The outcome of the SHAP-analysis revealed specific customer features that had a significant influence on churn. This knowledge can be utilized to proactively implement measures aimed at reducing the probability of churn.</p>

Note - only change to delete unnecessary space at end of a paragraph
----------------------------------------------------------------------
In diva2:1143829 - missing space in title:
"A predictor corrector method for a Finite ElementMethod for the variable density Navier-Stokes equations"
==>
"A predictor corrector method for a Finite Element Method for the variable density Navier-Stokes equations"

abstract   - correct as is 
----------------------------------------------------------------------
In diva2:1894668   - correct as is
----------------------------------------------------------------------
In diva2:1589052   - correct as is
----------------------------------------------------------------------
In diva2:1655792   - correct as is
----------------------------------------------------------------------
In diva2:1795466 - missing space in title:
"A Review of Anomaly Detection Techniques forHeterogeneous Datasets"
==>
"A Review of Anomaly Detection Techniques for Heterogeneous Datasets"

abstract is: 
<p>Anomaly detection is a field of study that is closely associated with machine learning and it is the process of finding irregularities in datasets. Developing and maintaining multiple machine learning models for anomaly detection takes time and can be an expensive task. One proposed solution is to combine all datasets and create a single model. This creates a heterogeneous dataset with a wide variation in its distribution, making it difficult to find anomalies in the dataset. The objective of this thesis is then to identify a framework that is suitable for anomaly detection in heterogeneous datasets.</p><p>A selection of five methods were implemented in this project - 2 supervised learning approaches and 3 unsupervised learning approaches. These models are trained on 3 synthetic datasets that have been designed to be heterogeneous with an imbalance between the classes as anomalies are rare events. The performance of the models are evaluated with the AUC and the F1-score, aswell as observing the Precision-Recall Curve.</p><p>The results makes it evident that anomaly detection in heterogeneous datasets is a challenging task. The best performing approach was with a random forest model where the class imbalance problem had been solved by generating synthetic samples of the anomaly class by implementing a generative adversarial network.</p>

corrected abstract:
<p>Anomaly detection is a field of study that is closely associated with machine learning and it is the process of finding irregularities in datasets. Developing and maintaining multiple machine learning models for anomaly detection takes time and can be an expensive task. One proposed solution is to combine all datasets and create a single model. This creates a heterogeneous dataset with a wide variation in its distribution, making it difficult to find anomalies in the dataset. The objective of this thesis is then to identify a framework that is suitable for anomaly detection in heterogeneous datasets.</p><p>A selection of five methods were implemented in this project - 2 supervised learning approaches and 3 unsupervised learning approaches. These models are trained on 3 synthetic datasets that have been designed to be heterogeneous with an imbalance between the classes as anomalies are rare events. The performance of the models are evaluated with the AUC and the 𝐹<sub>1</sub>-score, aswell as observing the <em>Precision-Recall Curve</em>.</p><p>The results makes it evident that anomaly detection in heterogeneous datasets is a challenging task. The best performing approach was with a random forest model where the class imbalance problem had been solved by generating synthetic samples of the anomaly class by implementing a generative adversarial network.</p>

Note added "𝐹" to "F" in F1 and added the subscript and added italics
----------------------------------------------------------------------
In diva2:711139   - correct as is
----------------------------------------------------------------------
In diva2:633872 - errors in title:
"A Scenario Based Allocation Model Using Entropy Pooling for Computing the cenarioProbabilities"
==>
"A Scenario Based Allocation Model Using Entropy Pooling for Computing the Scenario Probabilities"

abstract is: 
<p>We introduce a scenario based allocation model (SBAM) that uses entropy pooling for computing scenario probabilities. Compared to most other models that allow the investor to blend historical data with subjective views about the future, the SBAM does not require the investor to quantify a level of confidence in the subjective views.</p><p> A quantitative test is performed on a simulated systematic fund offered by the fund company Informed Portfolio Management in Stockholm, Sweden. The simulated fund under study consists of four individual systematic trading strategies and the test is simulated on a monthly basis during the years 1986-2010.</p><p> We study how the selection of views might affect the SBAM portfolios, creating three systematic views and combining them in different variations creating seven SBAM portfolios. We also compare how the size of sample data affects the results. </p><p> Furthermore, the SBAM is compared to more common allocation methods, namely an equally weighted portfolio and a portfolio optimization based only on historical data.</p><p> We find that the SBAM portfolios produced higher annual returns and information ratio than the equally weighted portfolio or the portfolio optimized only on historical data.</p>

corrected abstract:
<p>We introduce a scenario based allocation model (SBAM) that uses entropy pooling for computing scenario probabilities. Compared to most other models that allow the investor to blend historical data with subjective views about the future, the SBAM does not require the investor to quantify a level of confidence in the subjective views.</p><p>A quantitative test is performed on a simulated systematic fund offered by the fund company Informed Portfolio Management in Stockholm, Sweden. The simulated fund under study consists of four individual systematic trading strategies and the test is simulated on a monthly basis during the years 1986-2010.</p><p>We study how the selection of views might affect the SBAM portfolios, creating three systematic views and combining them in different variations creating seven SBAM portfolios. We also compare how the size of sample data affects the results.</p><p>Furthermore, the SBAM is compared to more common allocation methods, namely an equally weighted portfolio and a portfolio optimization based only on historical data.</p><p>We find that the SBAM portfolios produced higher annual returns and information ratio than the equally weighted portfolio or the portfolio optimized only on historical data.</p>

Note - removed unnecessary spaces at paragraph breaks
----------------------------------------------------------------------
In diva2:1212535 
abstract is: 
<p>Bond liquidity risk is complex and something that every bond-investor needs to take into account. In this paper we investigate how well a selfnormalizing neural network (SNN) can be used to classify bonds with respect to their liquidity, and compare the results with that of a simpler logistic regression. This is done by analyzing the two algorithms' predictive capabilities on the Swedish bond market. Performing this analysis we find that the performance of the SNN and the logistic regression are broadly on the same level. However, the substantive overfitting to the training data in the case of the SNN suggests that a better performing model could be created by applying regularization techniques. As such, the conclusion is formed as such that there is need of more research in order to determine whether neural networks are the premier method to modelling liquidity.</p>

corrected abstract:
<p>Bond liquidity risk is complex and something that every bond-investor needs to take into account. In this paper we investigate how well a self-normalizing neural network (SNN) can be used to classify bonds with respect to their liquidity, and compare the results with that of a simpler logistic regression. This is done by analyzing the two algorithms' predictive capabilities on the Swedish bond market. Performing this analysis we find that the performance of the SNN and the logistic regression are broadly on the same level. However, the substantive overfitting to the training data in the case of the SNN suggests that a better performing model could be created by applying regularization techniques. As such, the conclusion is formed as such that there is need of more research in order to determine whther neural networks are the premier method to modelling liquidity.</p>

Note error in original
"whther" should be "whether"
----------------------------------------------------------------------
In diva2:1359762 
abstract is: 
<p>When designing a building, sound is one of the problems to take into account. Vibrating machines, such as ventilation fans, water pumps and compressors, generate structure-borne sound. The structure-borne sound travels up the structure of the building and generates sound in adjacent rooms. To be able to predict the sound radiated in the adjacent rooms when designing a building, a semi-analytical model has been developed. Using the incident vibrations from the floor plate where the vibrating machine is standing, the transmission loss in the junction between the floor plates and the wall plate is calculated. This can bed one in every junction in the building, creating a system of multiple junctions. The sound radiation to the adjacent rooms is later approximated using the velocity of the plates.The model is verified with measurements in two case studies. This shows that the model has good potential in predicting the normal acceleration amplitudes in the relevant plates. The two case studies have different geometric properties and different sources. The comparison between the model and the measurement gives similar results. The model analyses the output of the bending waves since this is the wave type that radiates sound, but longitudinal waves are present in the model. With only two case studies it is too early to say that the model works for all systems, but it could be used as a fist approach. The model, right now, is restricted to isotropic, homogeneous material without losses. A parametric study shows that the transmission loss is dependent on the ratio between the thicknesses of the floor plate and the wall plate. The ratio should be as large as possible to get a high transmission loss, but depends on how the junction is structured.</p>

corrected abstract:
<p>When designing a building, sound is one of the problems to take into account. Vibrating machines, such as ventilation fans, water pumps and compressors, generate structure-borne sound. The structure-borne sound travels up the structure of the building and generates sound in adjacent rooms. To be able to predict the sound radiated in the adjacent rooms when designing a building, a semi-analytical model has been developed. Using the incident vibrations from the floor plate where the vibrating machine is standing, the transmission loss in the junction between the floor plates and the wall plate is calculated. This can be done in every junction in the building, creating a system of multiple junctions. The sound radiation to the adjacent rooms is later approximated using the velocity of the plates.</p><p>The model is verified with measurements in two case studies. This shows that the model has good potential in predicting the normal acceleration amplitudes in the relevant plates. The two case studies have different geometric properties and different sources. The comparison between the model and the measurement gives similar results. The model analyses the output of the bending waves since this is the wave type that radiates sound, but longitudinal waves are present in the model. With only two case studies it is too early to say that the model works for all systems, but it could be used as a fist approach. The model, right now, is restricted to isotropic, homogeneous material without losses.</p><p>A parametric study shows that the transmission loss is dependent on the ratio between the thicknesses of the floor plate and the wall plate. The ratio should be as large as possible to get a high transmission loss, but depends on how the junction is structured.</p>

Note - fixed "bed one" to "be done" and added paragraph breaks
----------------------------------------------------------------------
In diva2:912816 
abstract is: 
<p>For an apparatus as big as the pension system, the financial stability is essential. An important feature in the existing pension system is the balance mechanism, which secures the stability of the system. The balance ratio is obtained by dividing the assets by the liabilities. When this ratio drops below 1.0000, it triggers the so-called automatic balancing.</p><p>While the existing pension system has achieved its goal of being financially stable, it has become clear that the indexation of the pensions during balancing periods has properties that are not optimal. On a short-term perspective the income pension system is exposed to the risk of reacting with a lag, or reacting unnecessarily strong. This gave rise to a new legislative proposal, issued by the government. The goal of the proposal is to obtain a smoother and more up-to-date development of the income pension, i.e. a shorter lag period, without jeopardizing the financial stability. In addition to this it is also desirable to simplify and improve the existing calculation methods. In order to compare the existing calculation methods in the pension system with the new legislative proposal, a simplified model of the existing pension system and the modified version of it, are created.</p><p>The results of this study shows that the new legislative proposal decreases the volatility in the pensions and it avoids the deepest valleys in the balance ratio. The development of the pension disbursements in the new system has a higher correlation with the development of the average pension-qualifying income than in the current system. Moreover, the results show that the new system has a shorter lag period which makes the income pension system more up- to-date with the current economic and demographic situation.</p><p>The financial stability is still contained, and the new system also handles variations in the inflation better than the current system</p>

corrected abstract:
<p>For an apparatus as big as the pension system, the financial stability is essential. An important feature in the existing pension system is the balance mechanism, which secures the stability of the system. The balance ratio is obtained by dividing the assets by the liabilities. When this ratio drops below 1.0000, it triggers the so-called automatic balancing.</p><p>While the existing pension system has achieved its goal of being financially stable, it has become clear that the indexation of the pensions during balancing periods has properties that are not optimal. On a short-term perspective the income pension system is exposed to the risk of reacting with a lag, or reacting unnecessarily strong. This gave rise to a new legislative proposal, issued by the government. The goal of the proposal is to obtain a smoother and more up-to-date development of the income pension, i.e. a shorter lag period, without jeopardizing the financial stability. In addition to this it is also desirable to simplify and improve the existing calculation methods. In order to compare the existing calculation methods in the pension system with the new legislative proposal, a simplified model of the existing pension system and the modified version of it, are created.</p><p>The results of this study shows that the new legislative proposal decreases the volatility in the pensions and it avoids the deepest valleys in the balance ratio. The development of the pension disbursements in the new system has a higher correlation with the development of the average pension-qualifying income than in the current system. Moreover, the results show that the new system has a shorter lag period which makes the income pension system more up- to-date with the current economic and demographic situation. The financial stability is still contained, and the new system also handles variations in the inflation better than the current system</p>

Note - removed unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1450189   - correct as is
----------------------------------------------------------------------
In diva2:1652587   - correct as is
----------------------------------------------------------------------
In diva2:1833673   - correct as is
----------------------------------------------------------------------
In diva2:753596   - correct as is
----------------------------------------------------------------------
In diva2:1114454   - correct as is
----------------------------------------------------------------------
In diva2:1816770   - correct as is
----------------------------------------------------------------------
In diva2:1546201   - correct as is
----------------------------------------------------------------------
In diva2:1287729   - correct as is
----------------------------------------------------------------------
In diva2:1211493   - correct as is
----------------------------------------------------------------------
In diva2:1216728 
abstract is: 
<p>Reinforcement learning has recently gained popularity due to its many successfulapplications in various fields. In this project reinforcement learning is imple- mented in a simple warehouse situation where robots have to learn to interact with each other while performing specific tasks. The aim is to study whether reinforcement learning can be used to train multiple agents. Two different meth- ods have been used to achieve this aim, Q-learning and deep Q-learning. Due to practical constraints, this paper cannot provide a comprehensive review of real life robot interactions. Both methods are tested on single-agent and multi-agent models in Python computer simulations.</p><p>The results show that the deep Q-learning model performed better in the multi- agent simulations than the Q-learning model and it was proven that agents can learn to perform their tasks to some degree. Although, the outcome of this project cannot yet be considered sufficient for moving the simulation into real- life, it was concluded that reinforcement learning and deep learning methods can be seen as suitable for modelling warehouse robots and their interactions.</p>
mc='successfulapplications' c='successful applications'

corrected abstract:
<p>Reinforcement learning has recently gained popularity due to its many successful applications in various fields. In this project reinforcement learning is implemented in a simple warehouse situation where robots have to learn to interact with each other while performing specific tasks. The aim is to study whether reinforcement learning can be used to train multiple agents. Two different methods have been used to achieve this aim, Q-learning and deep Q-learning. Due to practical constraints, this paper cannot provide a comprehensive review of real life robot interactions. Both methods are tested on single-agent and multi-agent models in Python computer simulations.</p><p>The results show that the deep Q-learning model performed better in the multiagent simulations than the Q-learning model and it was proven that agents can learn to perform their tasks to some degree. Although, the outcome of this project cannot yet be considered sufficient for moving the simulation into reallife, it was concluded that reinforcement learning and deep learning methods can be seen as suitable for modelling warehouse robots and their interactions.</p>
----------------------------------------------------------------------
In diva2:1761947 
abstract is: 
<p>This thesis provides an end-to-end picture of the modelling of interest rates and Foreign Exchange (FX) rates. We start by defining the FX rates and the interest rates. After having a good understanding of the basics, we take a deep dive into the approaches commonly used to model interest rates and FX rates respectively. In particular, we present an interest rate model and a FX rate model that I have developed for man- aging Swedbank’s Counterparty Credit Risk (CCR). In addition to the mathematical derivations, we describe the theories underlying the models, discuss the model com- parisons, and explain the model choices made in practical applications. Finally, we provide a prototype of model implementation to illustrate how theory can be put into practice.</p><p>I had some doubts about the interest rate model and the FX rate model that I have developed for managing Swedbank’s CCR. These doubts have been cleared up through this thesis work. Both the doubts and the clarifications are described in this thesis.</p>

corrected abstract:
<p>This thesis provides an end-to-end picture of the modelling of interest rates and Foreign Exchange (FX) rates. We start by defining the FX rates and the interest rates. After having a good understanding of the basics, we take a deep dive into the approaches commonly used to model interest rates and FX rates respectively. In particular, we present an interest rate model and a FX rate model that I have developed for managing Swedbank’s Counterparty Credit Risk (CCR). In addition to the mathematical derivations, we describe the theories underlying the models, discuss the model comparisons, and explain the model choices made in practical applications. Finally, we provide a prototype of model implementation to illustrate how theory can be put into practice.</p><p>I had some doubts about the interest rate model and the FX rate model that I have developed for managing Swedbank’s CCR. These doubts have been cleared up through this thesis work. Both the doubts and the clarifications are described in this thesis.</p>
----------------------------------------------------------------------
In diva2:1360578   - correct as is
----------------------------------------------------------------------
In diva2:942546 
abstract is: 
<p>This bachelor thesis in applied mathematics and industrial engineering aims to determine if and how weather affects the consumption of goods at small grocery stores. To study this, we conducted a regression analysis based on sales data from an ICA Nära. We have collected one year’s weather- and sales data and used mathematical statistics to determine how weather affects the sales for different product groups. Our belief is that weather does affect the consumption.</p><p>Several large actors in the industry have some sort of consumption of goods forecast. None of these takes weather into account when creating their sale forecast. Hopefully, this thesis will provide information aiding companies in deciding whether or not to use weather forecasts as a prediction parameter. </p><p>The results indicate a large effect on sales for some groups of products. The regression reveals how much the sale of a group increase along with an increase of one unit of the different measured weather factors. There is, most likely, not a perfect linear relation between our response variable and the explanatory variables. Therefore, one must interpret the results carefully. In addition, we discuss how a possible implementation affects the supply chain of a large grocery store company and the importance of flexibility in one’s supply chain.</p>

corrected abstract:
<p>This bachelor thesis in applied mathematics and industrial engineering aims to determine if and how weather affects the consumption of goods at small grocery stores. To study this, we conducted a regression analysis based on sales data from an ICA Nära. We have collected one year’s weather- and sales data and used mathematical statistics to determine how weather affects the sales for different product groups. Our belief is that weather does affect the consumption.</p><p>Several large actors in the industry have some sort of consumption of goods forecast. None of these takes weather into account when creating their sale forecast. Hopefully, this thesis will provide information aiding companies in deciding whether or not to use weather forecasts as a prediction parameter.</p><p>The results indicate a large effect on sales for some groups of products. The regression reveals how much the sale of a group increase along with an increase of one unit of the different measured weather factors. There is, most likely, not a perfect linear relation between our response variable and the explanatory variables. Therefore, one must interpret the results carefully. In addition, we discuss how a possible implementation affects the supply chain of a large grocery store company and the importance of flexibility in one’s supply chain.</p>

Note - only change was to remove an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1117098   - correct as is
----------------------------------------------------------------------
In diva2:1319633   - correct as is
----------------------------------------------------------------------
In diva2:433851 
abstract is: 
<p>The molten core-concrete interaction (MCCI) is treated as one of the important phenomena that may lead to the late containment failure by basemat penetration in a hypothetical severe accident of light water reactors (LWRs). The earlier research has showed that heat transfer limitation exists for the coolability of ex-vessel corium by atop water flooding due to crust formation on the melt/water interface that will isolate melt from water. However, several cooling mechanisms were identified in a series of intense investigations. A code (CORQUENCH) was developed and updated to incorporate the newly identified cooling mechanisms for the better predictions of cavity erosion and corium cooling behaviors. A description about such cooling mechanisms (i.e., bulking cooling, water ingression, eruption and crust breach) and the concrete ablation models implemented in the code is presented in this thesis.</p>
<p>The technical work in the thesis includes two parts: first, the verification and validation of the code were performed against the CCI tests from the OECD/MCCI projects; and then a reactor-scale simulation was carried out for MCCI and ex-vessel corium coolability of a reference PWR with LCS concrete. The calculations of CCI tests have a plausible agreement with the experimental data.</p>
<p>The calculation predicts an optimistic result for the reactor case, and a fast quenching achieved at about 145 minutes. In addition, a sensitivity study was also conducted on several important parameters, i.e., concrete type, corium composition, water flooding time, atmosphere pressure, concrete ablation temperature, initial temperature, decay power, cavity geometry, concrete decomposition model and melt upper heat transfer model. An attempt to explain the physics of the different predicted phenomena is presented as well.</p>
<p>Finally, comparative calculations were performed by the other codes (ASTEC and FinCCI) for the same reactor-scale configuration. Discrepancies are found in the results. Some suggestions are proposed to improve the CORQUENCH code.</p>
<p></p>

corrected abstract:
<p>The molten core-concrete interaction (MCCI) is treated as one of the important phenomena that may lead to the late containment failure by basemat penetration in a hypothetical severe accident of light water reactors (LWRs). The earlier research has showed that heat transfer limitation exists for the coolability of ex-vessel corium by atop water flooding due to crust formation on the melt/water interface that will isolate melt from water. However, several cooling mechanisms were identified in a series of intense investigations. A code (CORQUENCH) was developed and updated to incorporate the newly identified cooling mechanisms for the better predictions of cavity erosion and corium cooling behaviors. A description about such cooling mechanisms (i.e., bulking cooling, water ingression, eruption and crust breach) and the concrete ablation models implemented in the code is presented in this thesis.</p>
<p>The technical work in the thesis includes two parts: first, the verification and validation of the code were performed against the CCI tests from the OECD/MCCI projects; and then a reactor-scale simulation was carried out for MCCI and ex-vessel corium coolability of a reference PWR with LCS concrete. The calculations of CCI tests have a plausible agreement with the experimental data. The calculation predicts an optimistic result for the reactor case, and a fast quenching achieved at about 145 minutes. In addition, a sensitivity study was also conducted on several important parameters, i.e., concrete type, corium composition, water flooding time, atmosphere pressure, concrete ablation temperature, initial temperature, decay power, cavity geometry, concrete decomposition model and melt upper heat transfer model. An attempt to explain the physics of the different predicted phenomena is presented as well.</p>
<p>Finally, comparative calculations were performed by the other codes (ASTEC and FinCCI) for the same reactor-scale configuration. Discrepancies are found in the results. Some suggestions are proposed to improve the CORQUENCH code.</p>

Note - only change was to remove an unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1332815 
abstract is: 
<p>During the past few decades, social responsible investing (SRI) has rapidly grown to become a renowned investment strategy. Because of the contradictory findings on how successful this strategy is in terms of financial return, the aim of this thesis is to compare the performance of sustainable and conventional funds in four different geographical areas during the last three years. With the use of regression analysis, the correlation between the Portfolio Sustainability Score of a fund, which is a Morningstar-provided rating that represents how well a fund incorporates ESG, and its risk-adjusted return is determined.</p><p>The final results of this analysis varies among the four geographical regions. The correlation between the two variables is positive in USA and Asia ex-Japan, whereas a negative relationship is found in Europe and the Nordic region. However, the obtained findings are not of statistical significance, implying that there is no difference between the risk-adjusted returns of sustainable versus conventional funds.</p>

corrected abstract:
<p>During the past few decades, social responsible investing (SRI) has rapidly grown to become a renowned investment strategy. Because of the contradictory findings on how successful this strategy is in terms of financial return, the aim of this thesis is to compare the performance of sustainable and conventional funds in four different geographical areas during the last three years. With the use of regression analysis, the correlation between the Portfolio Sustainability Score of a fund, which is a Morningstar-provided rating that represents how well a fund incorporates ESG, and its risk-adjusted return is determined.</p><p>The final results of this analysis varies among the four geographical regions. The correlation between the two variables is positive in USA and Asia ex-Japan, whereas a negative relationship is found in Europe and the Nordic region. However, the obtained findings are either not of statistical significance or not representative of the given data, implying that there is no difference between the risk-adjusted returns of sustainable versus conventional funds.</p>

Note - added missing text
----------------------------------------------------------------------
In diva2:820906 - error in title:
"A thesis submitted in fulfilment of the requirements for the degree of Masters of Mathematics"
==>
"Bijections on Catalan Structures"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:822466   - correct as is
----------------------------------------------------------------------
In diva2:608591   - correct as is
----------------------------------------------------------------------
In diva2:1807977 
abstract is: 
<p>Acoustic focusing of microscale protein crystals with acoustophoresis technology could reduce clogs during experiments with the scientific technique serial femtosecond x-ray crystallography (SFX). SFX determines molecular structures of proteins, these structures are valuable in drug discovery and fundamental biomedical research.</p><p>Lysozyme crystals were focused in their own mother liquor and dilutions with PBS buffer. The aim of these tests were to study how the acoustic contrast factor Φ changes with the medium. Recorded experiments were analyzed using the particle tracking software Trackmate to extract velocities and radii.</p><p>The lysozyme crystals changed morphologies in large dilutions of PBS buffert, they either became rounder or broke into fragments. The changed forms are likely caused by dissolution behaviors; some dilutions were unstable, but not unstable enough to dissolve the crystals completely. </p><p>Measured velocities during focusing of the crystals had large variance. Sinusoidal fits of the velocities had significant increases in amplitudes for larger dilutions of PBS. A change in acoustic contrast factor Φ could be the cause for the increased amplitudes, but the results do not rule out other causes. There are currently major knowledge gaps about using protein crystals as particles with acoustophoresis technologies, hence many ideas for future works have been proposed in this master thesis report.</p>

corrected abstract:
<p>Acoustic focusing of microscale protein crystals with acoustophoresis technology could reduce clogs during experiments with the scientific technique serial femtosecond x-ray crystallography (SFX). SFX determines molecular structures of proteins, these structures are valuable in drug discovery and fundamental biomedical research.</p><p>Lysozyme crystals were focused in their own mother liquor and dilutions with PBS buffer. The aim of these tests were to study how the acoustic contrast factor Φ changes with the medium. Recorded experiments were analyzed using the particle tracking software Trackmate to extract velocities and radii.</p><p>The lysozyme crystals changed morphologies in large dilutions of PBS buffert, they either became rounder or broke into fragments. The changed forms are likely caused by dissolution behaviors; some dilutions were unstable, but not unstable enough to dissolve the crystals completely.</p><p>Measured velocities during focusing of the crystals had large variance. Sinusoidal fits of the velocities had significant increases in amplitudes for larger dilutions of PBS. A change in acoustic contrast factor Φ could be the cause for the increased amplitudes, but the results do not rule out other causes. There are currently major knowledge gaps about using protein crystals as particles with acoustophoresis technologies, hence many ideas for future works have been proposed in this master thesis report.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1516090   - correct as is
----------------------------------------------------------------------
In diva2:1757072 
abstract is: 
<p>Despite much research indicating that acquisitions are unsatisfactory in generating value, in terms of stock market return, their continued and growing existence highlights that acquisitions play an essential role in the corporate landscape, and will only continue doing so moving forward. </p><p>This continuous undertaking in acquisitions despite a lacking performance inspired a thesis that is focused on viewing acquisitions through an operational perspective. The aim of the thesis is to answer the question: "What impacts the difference in operational efficiency between the group of companies, that is formed after an acquisition, and the separate companies, that operate before an acquisition?" A multiple linear regression model analysis is performed with explanatory regressors that are mainly based on the target-acquirer relation.</p><p>The final model's relatively low explanatory power combined with certain model assumption violations and a challenging sample of observations harms the reliability of the significant regressors in the final model. The significant regressors are the acquirer's average annual EBIT margin growth rate before the acquisition and the cross-border regressor. The two regressors are in line with acquisition and financial accounting theory. The conclusion, however, is that analyzing acquisitions in this regard is questionable and could very well not return informative results that can be applied to future acquisitions to improve the unsatisfactory returns.</p>

corrected abstract:
<p>Despite much research indicating that acquisitions are unsatisfactory in generating value, in terms of stock market return, their continued and growing existence highlights that acquisitions play an essential role in the corporate landscape, and will only continue doing so moving forward.</p><p>This continuous undertaking in acquisitions despite a lacking performance inspired a thesis that is focused on viewing acquisitions through an operational perspective. The aim of the thesis is to answer the question: ”What impacts the difference in operational efficiency between the group of companies, that is formed after an acquisition, and the separate companies, that operate before an acquisition?” A multiple linear regression model analysis is performed with explanatory regressors that are mainly based on the target-acquirer relation.</p><p>The final model's relatively low explanatory power combined with certain model assumption violations and a challenging sample of observations harms the reliability of the significant regressors in the final model. The significant regressors are the acquirer's average annual EBIT margin growth rate before the acquisition and the cross-border regressor. The two regressors are in line with acquisition and financial accounting theory. The conclusion, however, is that analyzing acquisitions in this regard is questionable and could very well not return informative results that can be applied to future acquisitions to improve the unsatisfactory returns.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:653296 
abstract is: 
<p>This work analyzes the implementation of an active train pantograph in a full finite element model in the program Ansys. As controller design method, the H∞ method was taken in order to cope with the different uncertainties in the given system. The focus lies on the contact force between the pantograph and the catenary. The goal was the reduction of the contact force standard deviation in order to allow higher train speeds on existing lines. An additional goal is the use of multi train configurations. This means that two coupled trains with a distance between the two pantographs of 100 meters can run with high speed on existing lines. Current regulations limit the distance to 200 meters. In addition to the active solutions, different modifications of the given pantograph were investigated.</p><p>The simulations showed that the desired speed of 280 km/h is achieved on existing lines in multi train configuration. For only one train, a speed of up to 300 km/h can be reached. More important, by using an estimator, the standard deviation values for these speeds were still below the limitations and hence, it is possible to implement this solution in a real system.</p>

corrected abstract:
<p>This work analyzes the implementation of an active train pantograph in a full finite element model in the program Ansys. As controller design method, the <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?\mathcal{H}"  title="\mathcal{H} " alt="LaTeX: \mathcal{H}" /><sub>∞</sub> method was taken in order to cope with the different uncertainties in the given system. The focus lies on the contact force between the pantograph and the catenary. The goal was the reduction of the contact force standard deviation in order to allow higher train speeds on existing lines. An additional goal is the use of multi train configurations. This means that two coupled trains with a distance between the two pantographs of 100 meters can run with high speed on existing lines. Current regulations limit the distance to 200 meters. In addition to the active solutions, different modifications of the given pantograph were investigated.</p><p>The simulations showed that the desired speed of 280 <sup>km</sup>/<sub>h</sub> is achieved on existing lines in multi train configuration. For only one train, a speed of up to 300 <sup>km</sup>/<sub>h</sub> can be reached.</p><p>More important, by using an estimator, the standard deviation values for these speeds were still below the limitations and hence, it is possible to implement this solution in a real system.</p>

Note the "<sup>km</sup>/<sub>h</sub>" and I added the mimtex to get the \mathcal{H}
----------------------------------------------------------------------
In diva2:1247182 
abstract is: 
<p>Master of Science thesis in Naval Architecture presents a study and the performance of an active seat suspension with the purpose to suppress shocks, caused by slamming in High Speed Crafts (HSCs). The system is modelled and simulated with the aid of the Mathworks software Simulink, with the main objective to evaluate if the active suspension seat has the potential to mitigate slamming impact loads to a larger extent compared to a passive suspension seat. The active suspension model is developed by adding a PD-controlled actuator in parallel with the spring and damper of a passive seat’s suspension. This paper presents the performance study of an active suspension seat where the seat is given a single impact load as input. The results are then compared to a comparable passive seat. The most promising results show that the active system can reduce the passenger seat’s acceleration response by roughly 30 %. This is achieved on the expense of an increased stroke length, from 30 mm for a comparable passive system, to 34 mm for the active system. To achieve this the actuator need to provide up to 900 N of force with a rise-time of 15 ms. During the assessment of the suspension seat performance four key performance indicators(KPI) were found to be of significance. Those are the seat response acceleration, seat displacement relative to the seat base, settling time and the zero crossing time. The seat acceleration is directly proportional to the load that the passenger is being subjected to. Hence, the acceleration is the property that needs to be reduced in order to decrease risk of injuries. The stroke length of the seat in relation to the seat base should be kept to a minimum for several reasons. One being the risk of bottoming out the suspension if the stroke length is too high, risking damage on equipment as well as injuries on passenger. Since the conditions on sea entail series of impact loads on the hull, the settling time need to be as short as possible to avoid accumulating the displacement. This is caused when the seat has not yet returned to its neutral position before next impact occurs. To define the response time of the system, the zero-crossing performance indicator was defined. Zero-crossing time is defined as the time from when the displacement of the seat starts (the suspension being compressed) until it returns and crosses the neutral position, regardless if the suspension stops at the neutral position or continue extending. A correlation was found between the zero-crossing time and the settling time. Both KPIs are dependent on the 𝑃𝐺𝑎𝑖𝑛 (the proportional part of the PD-control)and what is found is that a short zero-crossing time entail an increased overshoot, that in turn results in a longer settling time due to the seat’s oscillation about the neutral position. The active suspension seat model in this paper can be further developed and evaluated with respect to the performance indicators stated by (European Union, 2002) like VDV, RMS acceleration values etc.</p>

corrected abstract:
<p>This Master of Science thesis in Naval Architecture presents a study and the performance of an active seat suspension with the purpose to suppress shocks, caused by slamming in High Speed Crafts (HSCs). The system is modelled and simulated with the aid of the Mathworks software Simulink, with the main objective to evaluate if the active suspension seat has the potential to mitigate slamming impact loads to a larger extent compared to a passive suspension seat. The active suspension model is developed by adding a PD-controlled actuator in parallel with the spring and damper of a passive seat’s suspension.</p><p>This paper presents the performance study of an active suspension seat where the seat is given a single impact load as input. The results are then compared to a comparable passive seat. The most promising results show that the active system can reduce the passenger seat’s acceleration response by roughly 30 %. This is achieved on the expense of an increased stroke length, from 30 mm for a comparable passive system, to 34 mm for the active system. To achieve this the actuator need to provide up to 900 N of force with a rise-time of 15 ms.</p><p>During the assessment of the suspension seat performance four key performance indicators (KPI) were found to be of significance. Those are the seat response acceleration, seat displacement relative to the seat base, settling time and the zero crossing time. The seat acceleration is directly proportional to the load that the passenger is being subjected to. Hence, the acceleration is the property that needs to be reduced in order to decrease risk of injuries. The stroke length of the seat in relation to the seat base should be kept to a minimum for several reasons. One being the risk of bottoming out the suspension if the stroke length is too high, risking damage on equipment as well as injuries on passenger. Since the conditions on sea entail series of impact loads on the hull, the settling time need to be as short as possible to avoid accumulating the displacement. This is caused when the seat has not yet returned to its neutral position before next impact occurs. To define the response time of the system, the zero-crossing performance indicator was defined. Zero-crossing time is defined as the time from when the displacement of the seat starts (the suspension being compressed) until it returns and crosses the neutral position, regardless if the suspension stops at the neutral position or continue extending. A correlation was found between the zero-crossing time and the settling time. Both KPIs are dependent on the 𝑃<sub>𝐺𝑎𝑖𝑛</sub> (the proportional part of the PD-control) and what is found is that a short zero-crossing time entail an increased overshoot, that in turn results in a longer settling time due to the seat’s oscillation about the neutral position.</p><p>The active suspension seat model in this paper can be further developed and evaluated with respect to the performance indicators stated by (European Union, 2002) like VDV, RMS acceleration values etc.</p>

Note added subscript and missing paragraph breaks; also add the initial missing word
----------------------------------------------------------------------
In diva2:1816887   - correct as is
----------------------------------------------------------------------
In diva2:1259795   - correct as is
----------------------------------------------------------------------
In diva2:1780244   - correct as is
----------------------------------------------------------------------
In diva2:1843154   - correct as is
----------------------------------------------------------------------
In diva2:1356948   - correct as is
----------------------------------------------------------------------
In diva2:1066362 
abstract is: 
<p>In order for wind-farm operators to deal with challenges regarding their fleet management, it is useful for them to estimate their units’ performance for different conditions. To perform such estimations, Computational Fluid Dynamics (CFD) may be used.</p><p>This project focuses on the development of a CFD model for the aerodynamic analysis of wind turbine rotors, depending on their surface roughness. The work has been carried out in collaboration with the KTH Royal Institute of Technology and the Vattenfall AB R&amp;D department.</p><p>The open-source software OpenFOAM has been used to develop the desired model. A rigid body incompressible steady state, Reynolds-Averaged Navier-Stokes equations, <em>k – ω </em>SST CFD case has been set up. The NREL 5-MW rotor geometry has been used and the effect of four different surface roughness height values {1mm, 0.5mm, 100 μm, 30 μm} on its aerodynamic performance has been investigated for an incoming wind velocity of 10m/s. The referred roughness height values have been applied on the whole rotor surface. A 120° wedge type computational domain of unstructured mesh has been developed for the present simulations.</p><p>The results indicate that a roughness-height increase leads to earlier flow separation over the blade suction side and increases the turbulent area of the boundary layer. That leads to a decrease for the extracted <em>Torque </em>and the <em>Thrust </em>force on the wind turbine rotor. Moreover, it is concluded that the rotor aerodynamic performance is more sensitive to low roughness heights rather than to high ones.</p>

corrected abstract:
<p>In order for wind-farm operators to deal with challenges regarding their fleet management, it is useful for them to estimate their units’ performance for different conditions. To perform such estimations, Computational Fluid Dynamics (CFD) may be used.</p><p>This project focuses on the development of a CFD model for the aerodynamic analysis of wind turbine rotors, depending on their surface roughness. The work has been carried out in collaboration with the KTH Royal Institute of Technology and the Vattenfall AB R&amp;D department.</p><p>The open-source software OpenFOAM has been used to develop the desired model. A rigid body incompressible steady state, Reynolds-Averaged Navier-Stokes equations, <em>𝑘 – ω</em> SST CFD case has been set up. The NREL 5-MW rotor geometry has been used and the effect of four different surface roughness height values {1mm, 0.5mm, 100 μm, 30 μm} on its aerodynamic performance has been investigated for an incoming wind velocity of 10m/s. The referred roughness height values have been applied on the whole rotor surface. A 120° wedge type computational domain of unstructured mesh has been developed for the present simulations.</p><p>The results indicate that a roughness-height increase leads to earlier flow separation over the blade suction side and increases the turbulent area of the boundary layer. That leads to a decrease for the extracted <em>Torque</em> and the <em>Thrust</em> force on the wind turbine rotor. Moreover, it is concluded that the rotor aerodynamic performance is more sensitive to low roughness heights rather than to high ones.</p>

Note adjected the italics region and converted "k" to "𝑘"
----------------------------------------------------------------------
In diva2:757141   - correct as is
----------------------------------------------------------------------
In diva2:653238 - missing space in title:
"Aeroelastic instabilities simulations on turbofan HP compressorblisk at surge-like reversed flow conditions"
==>
"Aeroelastic instabilities simulations on turbofan HP compressor blisk at surge-like reversed flow conditions"

abstract is: 
<p>This paper presents the work done at the High Pressure Compressor design division of Safran Snecma to simulate the flutter behaviour of a compressor blisk at reversed flow conditions. The report states the preliminary phase of modelling the reversed flow on an entire compressor with a stationary simulation and the single blade aeroelastic study results obtained. One can then get interpretations on the blow down aerodynamics that can affect the mechanical behaviour of the blade. Eight simulations of a single blade model have been done on two aeroelastic configurations, each one on a rotor from two different compressor designs. By investigating the airflow around the blade profiles, it becomes possible to get values on the aerodynamic damping on the blade. As presented below, even with strong surge stresses, the compressor stage will remain flutter-free.</p>

corrected abstract:
<p>This paper presents the work done at the High Pressure Compressor design division of <em>Safran Snecma</em> to simulate the flutter behaviour of a compressor blisk at reversed flow conditions. The report states the preliminary phase of modelling the reversed flow on an entire compressor with a stationary simulation and the single blade aeroelastic study results obtained. One can then get interpretations on the blow down aerodynamics that can affect the mechanical behaviour of the blade. Eight simulations of a single blade model have been done on two aeroelastic configurations, each one on a rotor from two different compressor designs. By investigating the airflow around the blade profiles, it becomes possible to get values on the aerodynamic damping on the blade. As presented below, even with strong surge stresses, the compressor stage will remain flutter-free.</p>

Note - added italics
----------------------------------------------------------------------
In diva2:398320   - correct as is
----------------------------------------------------------------------
In diva2:1578571 
abstract is: 
<p>Growing environmental concerns are causing a large transformation within the energy industry. Within the gas turbine industry, there is a large drive to develop improved modern dry-low emission combustion systems. The aim is to enable gas turbines to run on green fuels like hydrogen, while still keeping emission as NOx down. To design these systems, a thorough understanding of the aerothermal and kinetic processes within the combustion system of a gas turbine is essential.</p><p>The goal of the thesis was to develop a one-dimensional general network model of the combustion system of Siemens Energy SGT-700, which accurately could predict pressure losses, mass flows, key temperatures, and emissions.</p><p>Three models were evaluated and a code that emulated some aspects of the control system was developed. The models and the code were evaluated and compared to each other and to test data from earlier test campaigns performed on SGT-700 and SGT-600. Simulations were also carried out with hydrogen as the fuel. </p><p>In the end, a model of the SGT-700 combustion chamber was developed and delivered to Siemens Energy. The model had been verified against test data and predictions made by other Siemens Energy thermodynamic calculation software, for a range of load conditions. The preforms of the model, when hydrogen was introduced into the fuel mixture, were also tested and compared to test data</p>

corrected abstract:
<p>Growing environmental concerns are causing a large transformation within the energy industry. Within the gas turbine industry, there is a large drive to develop improved modern dry-low emission combustion systems. The aim is to enable gas turbines to run on green fuels like hydrogen, while still keeping emission as NOx down. To design these systems, a thorough understanding of the aerothermal and kinetic processes within the combustion system of a gas turbine is essential.</p><p>The goal of the thesis was to develop a one-dimensional general network model of the combustion system of Siemens Energy SGT-700, which accurately could predict pressure losses, mass flows, key temperatures, and emissions.</p><p>Three models were evaluated and a code that emulated some aspects of the control system was developed. The models and the code were evaluated and compared to each other and to test data from earlier test campaigns performed on SGT-700 and SGT-600. Simulations were also carried out with hydrogen as the fuel.</p><p>In the end, a model of the SGT-700 combustion chamber was developed and delivered to Siemens Energy. The model had been verified against test data and predictions made by other Siemens Energy thermodynamic calculation software, for a range of load conditions. The preforms of the model, when hydrogen was introduced into the fuel mixture, were also tested and compared to test data.</p>

Note - small changes in text
----------------------------------------------------------------------
In diva2:1757040 
abstract is: 
<p>The purpose of this report is to, together with a Swedish bank, evaluate whether publicly owned companies within the Oil &amp; Gas industry are valued differently based on their ESG (Environmental, Social and Governance) performance. To examine if there is a correlation between ESG-factors and valuation, a mathematical model will be built with linear regression analysis. </p><p>The model that best describes the data does not give a satisfactory enough correlation to conclude that ESG-factors generally impact valuation of companies and it is therefore not possible to, with significance, conclude that companies with strong ESG-profiles enjoy a higher valuation than its peers. However, there is a significant correlation between one of the regressors in the model and valuation which could imply the existence of a correlation but it is not significant enough to model accurately. There is also the problem of lack of data, many companies do not report ESG-factors which complicates modeling. There seems to be a difference in ESG-reporting based on region and company size which could be used as motivation for future studies of even more homogenous companies. Finally, potential improvements of the model are discussed </p>

corrected abstract:
<p>The purpose of this report is to, together with a Swedish bank, evaluate whether publicly owned companies within the Oil &amp; Gas industry are valued differently based on their ESG (Environmental, Social and Governance) performance. To examine if there is a correlation between ESG-factors and valuation, a mathematical model will be built with linear regression analysis.</p><p>The model that best describes the data does not give a satisfactory enough correlation to conclude that ESG-factors generally impact valuation of companies and it is therefore not possible to, with significance, conclude that companies with strong ESG-profiles enjoy a higher valuation than its peers. However, there is a significant correlation between one of the regressors in the model and valuation which could imply the existence of a correlation but it is not significant enough to model accurately. There is also the problem of lack of data, many companies do not report ESG-factors which complicates modeling. There seems to be a difference in ESG-reporting based on region and company size which could be used as motivation for future studies of even more homogenous companies. Finally, potential improvements of the model are discussed.</p>

Note miniro adjustments to the text.
----------------------------------------------------------------------
In diva2:1450570   - correct as is
----------------------------------------------------------------------
In diva2:1449911   - correct as is
----------------------------------------------------------------------
In diva2:802097 
abstract is: 
<p>The condition of the roads is a factor that may not only affect the wear of a vehicle, car or truck, but as well may reduce fuel consumption, increase comfort, lower noise and maybe most importantly increase traffic safety. This gives a need of a system that can measure road quality and detect potholes, which could be of interest to haulers and to local road authorities that would get valuable information of road sections that are in need of maintenance.</p><p>In this Master Thesis different algorithms were developed, and tested, that could automatically detect different kind of road anomalies using only an three-axis accelerometer mounted on the chassis of heavy duty trucks from Scania. Data collection was performed using two different trucks and the road anomalies were noted by the co-driver using the keyboard of a laptop. This Master Thesis also explored the correlation between the acceleration levels on the chassis and high elongation values on the front leaf spring.</p><p>Using a developed evaluation framework, the anomaly detections from the different algorithms were compared to the test oracle to determine if the anomaly detection given by the algorithm was a true positive hit or a false positive. A great advantage of the developed evaluation framework is that additional algorithms could easily be added for evaluation. For the evaluation of the algorithms the statistical F-measure, which is the harmonic mean of the precision and sensitivity, was used for the test’s accuracy of the algorithms.</p><p>The two algorithms that had the best performance results regarding detection of road anomalies were Algorithm – T and Algorithm – SDT. These two algorithms had a F-measure score of 65% and 64% respectively when the precision and sensitivity were equally weighted.</p><p>For the correlation between acceleration levels and high elongation levels, Algorithm – SDT scored the highest F-measure value of 14%. This value is far from satisfying and a reason for the low value is that the algorithms were primarily developed for detection of road anomalies.</p>

corrected abstract:
<p>The condition of the roads is a factor that may not only affect the wear of a vehicle, car or truck, but as well may reduce fuel consumption, increase comfort, lower noise and maybe most importantly increase traffic safety. This gives a need of a system that can measure road quality and detect potholes, which could be of interest to haulers and to local road authorities that would get valuable information of road sections that are in need of maintenance.</p><p>In this Master Thesis different algorithms were developed, and tested, that could automatically detect different kind of road anomalies using only an three-axis accelerometer mounted on the chassis of heavy duty trucks from Scania. Data collection was performed using two different trucks and the road anomalies were noted by the co-driver using the keyboard of a laptop. This Master Thesis also explored the correlation between the acceleration levels on the chassis and high elongation values on the front leaf spring.</p><p>Using a developed evaluation framework, the anomaly detections from the different algorithms were compared to the test oracle to determine if the anomaly detection given by the algorithm was a true positive hit or a false positive. A great advantage of the developed evaluation framework is that additional algorithms could easily be added for evaluation. For the evaluation of the algorithms the statistical F-measure, which is the harmonic mean of the precision and sensitivity, was used for the test’s accuracy of the algorithms.</p><p>The two algorithms that had the best performance results regarding detection of road anomalies were Algorithm – T and Algorithm – SDT. These two algorithms had a F-measure score of 65% and 64% respectively when the precision and sensitivity were equally weighted. For the correlation between acceleration levels and high elongation levels, Algorithm – SDT scored the highest F-measure value of 14%. This value is far from satisfying and a reason for the low value is that the algorithms were primarily developed for detection of road anomalies.</p>

Note remove the unneeded paragraph break
----------------------------------------------------------------------
In diva2:1319888 
abstract is: 
<p>The equity option expiration effect is a well observed phenomenon and is explained by delta hedge rebalancing and pinning risk, which makes the strike price of an option work as a magnet for the underlying price. The FX option expiration effect has not previously been explored to the same extent. In this paper the FX option expiration effect is investigated with the aim of finding out whether it provides valuable information for predicting FX rate movements. New models are created based on the concept of the option relevance coefficient that determines which options are at higher risk of being in the money or out of the money at a specified future time and thus have an attraction effect. An algorithmic trading strategy is created to evaluate these models. The new models based on the FX option expiration effect strongly outperform time series models used as benchmarks. The best results are obtained when the information about the FX option expiration effect is included as an exogenous variable in a GARCH-X model. However, despite promising and consistent results, more scientific research is required to be able to draw significant conclusions.</p>

corrected abstract:
<p>The equity option expiration effect is a well observed phenomenon and is explained by delta hedge rebalancing and pinning risk, which makes the strike price of an option work as a magnet for the underlying price. The FX option expiration effect has not previously been explored to the same extent. In this paper the FX option expiration effect is investigated with the aim of finding out whether it provides valuable information for predicting FX rate movements. New models are created based on the concept of the <em>option relevance coefficient</em> that determines which options are at higher risk of being in the money or out of the money at a specified future time and thus have an attraction effect. An algorithmic trading strategy is created to evaluate these models. The new models based on the FX option expiration effect strongly outperform time series models used as benchmarks. The best results are obtained when the information about the FX option expiration effect is included as an exogenous variable in a GARCH-X model. However, despite promising and consistent results, more scientific research is required to be able to draw significant conclusions.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1219121 
abstract is: 
<p>Strassen’s algorithm was one of the breakthroughs in matrix analysis in 1968. In this report the thesis of Volker Strassen’s algorithm for matrix multipli- cations along with theories about precisions will be shown. The benefits of using this algorithm compared to naive matrix multiplication and its implica- tions, how its performance compare to the naive algorithm, will be displayed. Strassen’s algorithm will also be assessed on how the output differ when the matrix sizes grow larger, as well as how the theoretical complexity of the al- gorithm differs from the achieved complexity.</p><p>The studies found that Strassen’s algorithm outperformed the naive matrix multiplication at matrix sizes 1024 1024 and above. The achieved complex- ity was a little higher compared to Volker Strassen’s theoretical. The optimal precision for this case were the double precision, Float64.</p><p>How the algorithm is implemented in code matters for its performance. A number of techniques need to be considered in order to improve Strassen’s algorithm, optimizing its termination criterion, the manner by which it is padded in order to make it more usable for recursive application and the way it is implemented e.g. parallel computing. Even tough it could be proved that Strassen’s algorithm outperformed the Naive after reaching a certain matrix size, it is still not the most efficient one; e.g. as shown with Strassen-Winograd. One need to be careful of how the sub-matrices are being allocated, to not use unnecessary memory. For further reading one can study cache-oblivious and cache-aware algorithms.</p>

corrected abstract:
<p>Strassen’s algorithm was one of the breakthroughs in matrix analysis in 1968. In this report the thesis of Volker Strassen’s algorithm for matrix multiplications along with theories about precisions will be shown. The benefits of using this algorithm compared to naive matrix multiplication and its implications, how its performance compare to the naive algorithm, will be displayed. Strassen’s algorithm will also be assessed on how the output differ when the matrix sizes grow larger, as well as how the theoretical complexity of the algorithm differs from the achieved complexity.</p><p>The studies found that Strassen’s algorithm outperformed the naive matrix multiplication at matrix sizes 1024 × 1024 and above. The achieved complexity was a little higher compared to Volker Strassen’s theoretical. The optimal precision for this case were the double precision, Float64.</p><p>How the algorithm is implemented in code matters for its performance. A number of techniques need to be considered in order to improve Strassen’s algorithm, optimizing its termination criterion, the manner by which it is padded in order to make it more usable for recursive application and the way it is implemented e.g. parallel computing. Even tough it could be proved that Strassen’s algorithm outperformed the Naive after reaching a certain matrix size, it is still not the most efficient one; e.g. as shown with Strassen-Winograd. One need to be careful of how the sub-matrices are being allocated, to not use unnecessary memory. For further reading one can study cache-oblivious and cache-aware algorithms.</p>

Note added missing "×" and remove unnecessary hyphens
----------------------------------------------------------------------
In diva2:696780 
abstract is: 
<p>We use regime switching and regression tree methods to evaluate performance in the risk premia strategies provided by Deutsche Bank and constructed from U.S. research data from the Fama French library. The regime switching method uses the Baum-Welch algorithm at its core and splits return data into a normal and a turbulent regime. Each regime is independently evaluated for risk and the estimates are then weighted together according to the expected value of the proceeding regime. The regression tree methods identify macro-economic states in which the risk premia perform well or poorly and use these results to allocate between risk premia strategies. The regime switching method proves to be mostly unimpressive but has its results boosted by investing less into risky assets as the probability of an upcoming turbulent regime becomes larger. This proves to be highly effective for all time periods and for both data sources. The regression tree method proves the most effective when making the assumption that we know all macro-economic data the same month as it is valid for. Since this is an unrealistic assumption the best method seems to be to evaluate the performance of the risk premia strategy using macro-economic data from the previous quarter.</p>

corrected abstract:
<p>We use regime switching and regression tree methods to evaluate performance in the risk premia strategies provided by Deutsche Bank and constructed from U.S. research data from the Fama French library. The regime switching method uses the Baum-Welch algorithm at its core and splits return data into a normal and a turbulent regime. Each regime is independently evaluated for risk and the estimates are then weighted together according to the expected value of the proceeding regime. The regression tree methods identify macro-economic states in which the risk premia perform well or poorly and use these results to allocate between risk premia strategies.</p><p>The regime switching method proves to be mostly unimpressive but has its results boosted by investing less into risky assets as the probability of an upcoming turbulent regime becomes larger. This proves to be highly effective for all time periods and for both data sources. The regression tree method proves the most effective when making the assumption that we know all macro-economic data the same month as it is valid for. Since this is an unrealistic assumption the best method seems to be to evaluate the performance of the risk premia strategy using macro-economic data from the previous quarter.</p>

Note add missing paragraph break
----------------------------------------------------------------------
In diva2:1699856   - correct as is
----------------------------------------------------------------------
In diva2:1319481 
abstract is: 
<p>The aim of this thesis is to optimize hydro power plants with data generated from observations and field tests at the plants. The output is optimal production tables and curves in order to operate and plan hydro power plants in an optimized way concerning power output, efficiency and distribution of water. The thesis is performed in collaboration with Vattenfall AB, which currently use an internal optimization program called SEVAP. Two alternative methods have been selected, employed and compared with the current optimization program, these are Interior-Point Method and Sequential Quadratic Programming. Three start-point strategies are created to increase the probability of finding a global optima. A heuristic rule is used for selection of strategy in order to prevent rapid changes in load distribution for small variations in dispatched water. The optimization is performed at three plants in Sweden with different size and setup. The results of this evaluation showed marginally better results for the employed methods in comparison to the currently used optimization. Further, the developed program is more flexible and compatible to integrate with future digitalization projects.</p>

corrected abstract:
<p>The aim of this thesis is to optimize hydro power plants with data generated from observations and field tests at the plants. The output is optimal production tables and curves in order to operate and plan hydro power plants in an optimized way concerning power output, efficiency and distribution of water. The thesis is performed in collaboration with Vattenfall AB, which currently use an internal optimization program called SEVAP.</p><p>Two alternative methods have been selected, employed and compared with the current optimization program, these are Interior-Point Method and Sequential Quadratic Programming. Three start-point strategies are created to increase the probability of finding a global optima. A heuristic rule is used for selection of strategy in order to prevent rapid changes in load distribution for small variations in dispatched water. The optimization is performed at three plants in Sweden with different size and setup.</p><p>The results of this evaluation showed marginally better results for the employed methods in comparison to the currently used optimization. Further, the developed program is more flexible and compatible to integrate with future digitalization projects.</p>

Note - only change to add the missing paragraph breaks
----------------------------------------------------------------------
In diva2:942567   - correct as is
----------------------------------------------------------------------
In diva2:1078803   - correct as is
----------------------------------------------------------------------
In diva2:676730 - missing space in title:
"An Aeroelastic Implementation for YachtSails and Rigs"
==>
"An Aeroelastic Implementation for Yacht Sails and Rigs"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1211062 - missing space in title:
"An analysis of how variables andhome styling affect housing prices"
==>
"An analysis of how variables and home styling affect housing prices"

abstract is: 
<p>Based on the growing interest for home styling and earlier psychological scientific evidence, this study examines how home styling and other variables affect the final price of condominiums in Uppsala. Using multiple linear regression and different statistics, seven different models are analyzed in order to determine whether or not home styling is an influencing factor. To obtain a reliable result, nine other variables such as starting price, living area and floor level etc. are included in the initial model. In addition, these models are investigated statistically to determine if near linear dependence among the regressor variables exists or not. The results show that home styling have a positive impact on the final price of a condominium. The different analytical methods do not always agree, but if looking at the regression result and confidence interval it is obvious that home styling can help increase the final price. Using variable selection, home styling is only included in the model when allowing seven or more variables. The results and analysis from this report is not enough to determine exactly how much home styling affects the final price; since home styling is converted to a dummy variable in the study. The conclusion is that there is a correlation between the response, final price, and the regressor variable, home styling.</p>

corrected abstract:
<p>Based on the growing interest for home styling and earlier psychological scientific evidence, this study examines how home styling and other variables affect the final price of condominiums in Uppsala. Using multiple linear regression and different statistics, seven different models are analyzed in order to determine whether or not home styling is an influencing factor. To obtain a reliable result, nine other variables such as starting price, living area and floor level etc. are included in the initial model. In addition, these models are investigated statistically to determine if near linear dependence among the regressor variables exists or not. The results show that home styling have a positive impact on the final price of a condominium. The different analytical methods do not always agree, but if looking at the regression result and confidence interval it is obvious that home styling can help increase the final price. Using variable selection, home styling is only included in the model when allowing seven or more variables. The results and analysis from this report is not enough to determine exactly how much home styling affects the final price; since home styling is converted to a dummy variable in the study. The conclusion is that there is a correlation between the response, final price, and the regressor varible, home styling.</p>

Note spelling error:
"varible" should be "variable"
----------------------------------------------------------------------
In diva2:1450550 
abstract is: 
<p>The purpose of this thesis is to define which variables affect the average credit spread on the Swedish bond market. The study is conducted via the help of Enter Fonder, who contributes with data and insight into the Swedish corporate bond market. Earlier research has put a lot of weight on the connection between default risk and credit spread. The exact effect is however still debated and it is unclear which variables best describe the default risk. A multilinear regression analysis is conducted, studying the effect on the average credit spread in the NOMX-index (NOMXCRSP) with the following predictor variables: Treasury rate, Predicted EPS amongst OMXS30-companies, Change in net asset under management (AUM) of Swedish corporate bonds, The average credit spread on two European and two American counterparts to NOMX, D/E-ratio and EBITDA-margin amongst OMXS30-companies and finally PMI-index from both the industry and service sector. The regression analysis is based on 89 data points which were aggregated into an equivalent interval on a monthly basis. The final results presents a model of seven variables consisting of all the four international indexes, treasury rate, predicted EPS and change in net AUM, and was able to explain around 87% of the variance in the data.</p>

corrected abstract:
<p>The purpose of this thesis is to define which variables affect the average credit spread on the Swedish bond market. The study is conducted via the help of Enter Fonder, who contributes with data and insight into the Swedish corporate bond market. Earlier research has put a lot of weight on the connection between default risk and credit spread. The exact effect is however still debated and it is unclear which variables best describe the default risk. A multilinear regression analysis is conducted, studying the effect on the average credit spread in the NOMX-index (NOMXCRSP) with the following predictor variables: <em>Treasury rate</em>, <em>Predicted EPS amongst OMXS30-companies</em>, <em>Change in net asset under management (AUM) of Swedish corporate bonds</em>, <em>The average credit spread on two European and two American counterparts to NOMX</em>, <em>D/E-ratio and EBITDA-margin amongst OMXS30-companies<em> and finally <em>PMI-index from both the industry and service sector<em>. The regression analysis is based on 89 data points which were aggregated into an equivalent interval on a monthly basis. The final results presents a model of seven variables consisting of all the four international indexes, treasury rate, predicted EPS and change in net AUM, and was able to explain around 87% of the variance in the data.</p>

Note added italics to match original
----------------------------------------------------------------------
In diva2:942570 --- title does not match that of thesis:
"An analysis on the prices of French wines and on a business plan of creating a wine app"
==>
"An analysis on the precis of French wines and on a business plan of creating a wine app"

abstract is: 
<p>The purpose of this thesis is to evaluate the influences behind the pricing of French wines by the use of Applied Mathematics and statistical analysis. A regression analysis, based on a literature review of a mathematical compendium, was executed to estimate the coefficients of 21 characteristics of French wines. To perform such analysis, 490 bottles of wines were picked independently from the most famous wine guide of France, the Guide Hachette des Vins. The data was collected in order to make a representation of the French wine production distribution. The final regression equation was reduced to 11 variables and acquired a low goodness of fit. The result may be adequate but not capable of an accurate prediction of the average price of a bottle of wine. The result can be used as an example on the presence of influences behind the pricing of a wine.</p><p>By the use of regression analysis, a second research was shaped: the impact of a regression equation in the creation of a mobile wine application. With knowledge in Industrial Engineering and a literature study in the fields of e-business, SWOT-analysis, business planning and innovation strategy; a covering analysis was put together in hope to achieve a profitable business strategy. Enhancing a search engine in a wine app by the use of regression gives an edge against existing competitors. The rewards of a wine app are greater than the risks. Henceforth, the creation of a wine app is recommended. </p>

corrected abstract:
<p>The purpose of this thesis is to evaluate the influences behind the pricing of French wines by the use of Applied Mathematics and statistical analysis. A regression analysis, based on a literature review of a mathematical compendium, was executed to estimate the coefficients of 21 characteristics of French wines. To perform such analysis, 490 bottles of wines were picked independently from the most famous wine guide of France, the <em lang="fr">Guide Hachette des Vins</em>. The data was collected in order to make a representation of the French wine production distribution. The final regression equation was reduced to 11 variables and acquired a low goodness of fit. The result may be adequate but not capable of an accurate prediction of the average price of a bottle of wine. The result can be used as an example on the presence of influences behind the pricing of a wine.</p><p>By the use of regression analysis, a second research was shaped: the impact of a regression equation in the creation of a mobile wine application. With knowledge in Industrial Engineering and a literature study in the fields of e-business, SWOT-analysis, business planning and innovation strategy; a covering analysis was put together in hope to achieve a profitable business strategy. Enhancing a search engine in a wine app by the use of regression gives an edge against existing competitors. The rewards of a wine app are greater than the risks. Henceforth, the creation of a wine app is recommended.</p>

Note - removed the unnecessary space at the end of the final paragraph and added italics (and language tag)
----------------------------------------------------------------------
In diva2:1609976   - correct as is
----------------------------------------------------------------------
In diva2:1246222 
abstract is: 
<p>The demand for higher velocities and heavier axle loads for freight trains leads to higher forces on the railway wheels which in turn lead to an increase in stresses on and below the surface of the wheel-rail contact. By time, this induces wear on the wheels which consequently lead to higher maintenance costs and in some cases accidents. The ability to predict the evolution of wheel profiles due to uniform wear has been demonstrated with a rather accurate precision in most operational conditions. These wear models are based on wear coefficients and since they are not usually valid for real operational conditions, the models are generally calibrated against real-life scenarios in order to adjust the coefficients from test conditions to real-life lubrication conditions. This engineering approach can be useful in prediction of wear in systems where the materials and contact conditions do not vary. However, when addressing material development focused on reducing specific damage modes, the approach is of limited use because the obtained wear coefficients are not directly related to material properties. Therefore, attempts towards developing physical fracture propagation models that relates to the contact conditions and material properties have been made. The purpose has been to retrieve vital information about where a fracture initiates and how it propagates. In the long run, it is of great interest to be able to attain information about how a material particle is removed from the contact surface. Studies for this type of model was done in the 70’s and 80’s mainly with pin-disk experiments but has not been utilized in the specific field of wheel-rail contact. The thesis is part of the FR8RAIL project arranged by the European rail initiative Shift2Rail. Literature studies have been the basis for the thesis in order to gain vital insights into fracture mechanics and other related fields. The physical fracture propagation models have been constructed in the FE software Abaqus with the implementation of the XFEM. For the 2D model, the fracture initiates at the top of the implanted inclusion when the friction coefficient is  and propagates upwards a few elements. For , the fracture initiates at the right surface boundary where the pressure distribution and traction is applied. The fracture propagation angle increases relative to the surface as the friction coefficient value is increased. The fracture for the 3D model extends broader compared to the 2D model at the top of the inclusion in the case of . The fracture initiates at the same surface location as for the 2D model for . The fracture propagation is however non-existent due to convergence problems. The FE-models constructed are initial steps towards analysing the fracture propagation and closely related phenomena for a railway freight wheel in detail. At the end of the thesis, the simplified models give mainly information about the fracture initiation, propagation and its patterns. From this first phase, further adjustments and improvements can take place in order to eliminate the margins of error. In the long run, fully integrated models with further implementations such as detailed microstructure for the contact conditions, plastic behaviour for the material, and complete three-dimensional models can finally be employed.</p>

corrected abstract:
<p>The demand for higher velocities and heavier axle loads for freight trains leads to higher forces on the railway wheels which in turn lead to an increase in stresses on and below the surface of the wheel-rail contact. By time, this induces wear on the wheels which consequently lead to higher maintenance costs and in some cases accidents. The ability to predict the evolution of wheel profiles due to uniform wear has been demonstrated with a rather accurate precision in most operational conditions. These wear models are based on wear coefficients and since they are not usually valid for real operational conditions, the models are generally calibrated against real-life scenarios in order to adjust the coefficients from test conditions to real-life lubrication conditions. This engineering approach can be useful in prediction of wear in systems where the materials and contact conditions do not vary. However, when addressing material development focused on reducing specific damage modes, the approach is of limited use because the obtained wear coefficients are not directly related to material properties. Therefore, attempts towards developing physical fracture propagation models that relates to the contact conditions and material properties have been made. The purpose has been to retrieve vital information about where a fracture initiates and how it propagates. In the long run, it is of great interest to be able to attain information about how a material particle is removed from the contact surface. Studies for this type of model was done in the 70’s and 80’s mainly with pin-disk experiments but has not been utilized in the specific field of wheel-rail contact. The thesis is part of the FR8RAIL project arranged by the European rail initiative Shift2Rail. Literature studies have been the basis for the thesis in order to gain vital insights into fracture mechanics and other related fields. The physical fracture propagation models have been constructed in the FE software Abaqus with the implementation of the XFEM. For the 2D model, the fracture initiates at the top of the implanted inclusion when the friction coefficient is µ &le; 0.2 and propagates upwards a few elements. For µ &gt; 0.2, the fracture initiates at the right surface boundary where the pressure distribution and traction is applied. The fracture propagation angle increases relative to the surface as the friction coefficient value is increased. The fracture for the 3D model extends broader compared to the 2D model at the top of the inclusion in the case of µ &le; 0.2. The fracture initiates at the same surface location as for the 2D model for µ &gt; 0.2. The fracture propagation is however non-existent due to convergence problems. The FE-models constructed are initial steps towards analysing the fracture propagation and closely related phenomena for a railway freight wheel in detail. At the end of the thesis, the simplified models give mainly information about the fracture initiation, propagation and its patterns. From this first phase, further adjustments and improvements can take place in order to eliminate the margins of error. In the long run, fully integrated models with further implementations such as detailed microstructure for the contact conditions, plastic behaviour for the material, and complete three-dimensional models can finally be employed.</p>

Note - added the missing equations
----------------------------------------------------------------------
In diva2:1432664 
abstract is: 
<p>Mean variance optimization has shortcomings making the strategy far from optimal from an investor’s perspective. The purpose of the study is to conduct an empirical investigation as to how modern methods of portfolio optimization address the shortcomings associated with mean variance optimization. Equal risk contribution, the Most diversified portfolioand a modification of the Minimum variance portfolio are considered as alternatives to the mean variance model. Portfolio optimization models introduced are explained in detail and solved using the optimization algorithms Cyclical coordinate descent and Alternating direction method of multipliers. Through implementation and backtesting using a diverse set of indices representing various asset classes, the study shows that the mean variance model suffers from high turnover and sensitivity to input parameters in comparison to the modern alternatives. The sophisticated asset allocation models equal risk contribution and the most diversified portfolio do not rely on expected return as an input parameter, which is seen as an advantage, and are not affected to the same extent by the shortcomings associated with mean variance optimization. The paper concludes by discussing the findings critically and suggesting ideas for further research.</p>

corrected abstract:
<p><em>Mean variance optimization</em> has shortcomings making the strategy far from optimal from an investor’s perspective. The purpose of the study is to conduct an empirical investigation as to how modern methods of portfolio optimization address the shortcomings associated with mean variance optimization. <em>Equal risk contribution</em>, the <em>Most diversified portfolio</em> and a modification of the <em>Minimum variance portfolio</em> are considered as alternatives to the mean variance model. Portfolio optimization models introduced are explained in detail and solved using the optimization algorithms <em>Cyclical coordinate descent</em> and <em>Alternating direction method of multipliers</em>. Through implementation and backtesting using a diverse set of indices representing various asset classes, the study shows that the mean variance model suffers from high turnover and sensitivity to input parameters in comparison to the modern alternatives. The sophisticated asset allocation models equal risk contribution and the most diversified portfolio do not rely on expected return as an input parameter, which is seen as an advantage, and are not affected to the same extent by the shortcomings associated with mean variance optimization. The paper concludes by discussing the findings critically and suggesting ideas for further research.</p>

Note added italics to match original
----------------------------------------------------------------------
In diva2:1431031   - correct as is
----------------------------------------------------------------------
In diva2:1360615 
abstract is: 
<p>This master thesis report presents the modelling process of key elements of the Laser Interferometer Space Antenna mission (LISA mission) in a Modelbased systems engineering (MBSE) approach with SysML (Systems Modeling Language). The model implements a selected set of functions of the mission through executable graphical representations, called diagrams. It is shown how such diagrams can benefit the mission, by comparing this mean of information exchange to the traditional text- based systems engineering. The model represents the mission structure and behaviour through a system of nested layers. The deeper the layer is, the more it gives details on a system part. Each layer can be seen from different point of views, either focusing on the structure, the behaviour, or the performance of related system part.</p>

corrected abstract:
<p>This master thesis report presents the modelling process of key elements of the Laser Interferometer Space Antenna mission (LISA mission) in a Modelbased systems engineering (MBSE) approach with SysML (Systems Modeling Language). The model implements a selected set of functions of the mission through executable graphical representations, called diagrams. It is shown how such diagrams can benefit the mission, by comparing this mean of information exchange to the traditional text-based systems engineering. The model represents the mission structure and behaviour through a system of nested layers. The deeper the layer is, the more it gives details on a system part. Each layer can be seen from different point of views, either focusing on the structure, the behaviour, or the performance of related system part.</p>

Note - only change to remove the space after a hyphen
----------------------------------------------------------------------
In diva2:1320415 
abstract is: 
<p>In this thesis, various famous models have been investigated and compared to a custom model for people detection in low resolution video feeds. YOLOv3 and SSD in particular are famous models which have, at their time, produced state of the art results on competitions such as ImageNet and COCO. The performance of all models have been compared on speed and accuracy where it was found that YOLOv3 was the slowest and SSD was the fastest. The proposed model was superior in accuracy to both of the aforementioned architectures which can be attributed to addition of newer techniques from research such as leaving activations out and having a carefully balanced loss function. The results seem to suggest that the proposed model is implementable for real-time inference using cheap hardware such as a raspberry pi 3B+ coupled with one or more AI accelerator stickssuch as the Intel Neural Compute Stick 2 and that the networks are usable for detection even in bad video streams.</p>

corrected abstract:
<p>In this thesis, various famous models have been investigated and compared to a custom model for people detection in low resolution video feeds. YOLOv3 and SSD in particular are famous models which have, at their time, produced state of the art results on competitions such as ImageNet and COCO. The performance of all models have been compared on speed and accuracy where it was found that YOLOv3 was the slowest and SSD was the fastest. The proposed model was superior in accuracy to both of the aforementioned architectures which can be attributed to addition of newer techniques from research such as leaving activations out and having a carefully balanced loss function. The results seem to suggest that the proposed model is implementable for real-time inference using cheap hardware such as a raspberry pi 3B+ coupled with one or more AI accelerator sticks such as the Intel Neural Compute Stick 2 and that the networks are usable for detection even in bad videostreams.</p>

Note spelling error:
mc='stickssuch' c='sticks such'
"videostreams" is one word in the original
----------------------------------------------------------------------
In diva2:1130084   - correct as is
Note that 'diva2:1109070' is a duplicate
----------------------------------------------------------------------
In diva2:1391482   - correct as is
----------------------------------------------------------------------
In diva2:1183207 
abstract is: 
<p>With more and more digital text-valued data available, the need to be able to cluster, classify and study them arises. We develop in this thesis statistical tools to perform null hypothesis testing and clustering or classification on text-valued data in the framework of Object-Oriented Data Analysis.</p><p>The project includes research on semantic methods to represent texts, comparisons between representations, distances for such representations and performance of permutation tests. Main methods compared are Vector Space Model and topic model. More precisely, this thesis will provide an algorithm to compute permutation tests at document or sentence level to study the equality in terms of distribution of two texts for different representations and distances. Lastly, we describe the study of texts regarding a syntactic point of view and its structure with a tree representation.</p>

corrected abstract:
<p>With more and more digital text-valued data available, the need to be able to cluster, classify and study them arises. We develop in this thesis statistical tools to perform null hypothesis testing and clustering or classification on text-valued data in the framework of Object-Oriented Data Analysis.</p><p>The project includes research on semantic methods to represent texts, comparisons between representations, distances for such representations and performance of permutation tests. Main methods compared are Vector Space Model and topic model. More precisely, this thesis will provide an algorithm to compute permutation tests at document or sentence level to study the equality in terms of distribution of two texts for different representations and distances.</p><p>Lastly, we describe the study of texts regarding a syntactic point of view and its structure with a tree representation.</p>

Note - only change was to added the missing paragraph break
----------------------------------------------------------------------
In diva2:1120349   - correct as is
----------------------------------------------------------------------
In diva2:1751455   - correct as is
----------------------------------------------------------------------
In diva2:813411 
abstract is: 
<p>In this study a multiple linear regression was carried out in the interest of analysing a number of variables effect on the final prices of apartments in Stockholm’s inner districts. The result may be employed to predict and observe percentage changes on the final price of apartments in Stockholm in the future. Five models were constructed after which they were analysed and compared. The construction of these models were supported by data from the real estate agency <em>Erik Olsson</em>. The result of this study displays that living space have the highest positive influence on the final prices. Among all the inner city districts, Östermalm is the district that contributes the most to the final price growth. All five models had a coefficient of determination between 89%-94%.</p>

corrected abstract:
<p>In this study a multiple linear regression was carried out in the interest of analysing a number of variables effect on the final prices of apartments in Stockholm’s inner districts. The result may be employed to predict and observe percentage changes on the final price of apartments in Stockholm in the future. Five models were constructed after which they were analysed and compared. The construction of these models were supported by data from the real estate agency <em>Erik Olsson</em>.</p><p>The result of this study displays that living space have the highest positive influence on the final prices. Among all the inner city districts, Östermalm is the district that contributes the most to the final price growth. All five models had a coefficient of determination between 89% &mdash; 94%.</p>

Note - added paragraph break and changed the dash to a longer em-dash
----------------------------------------------------------------------
In diva2:1655678   - correct as is
Note spelling error:
w='degress' val={'c': 'degrees', 's': 'diva2:1655678', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:754040   - correct as is
----------------------------------------------------------------------
In diva2:408791   - correct as is
----------------------------------------------------------------------
In diva2:760099 
abstract is: 
<p>American football is a well-known sport in America. In addition to the design and rules of the game and the surface it is played on, there are also other differences that are not directly visible. One difference noted in this study is how the teams in the game provide players. Each team has a board of directors together with the team's leadership and they will determine which players are most suitable for the team. These players are recruited to the team after graduating from a U.S. college. This makes it possible to recruit new and young players to the squad. If the team wants other types of new players, there is also an opportunity to replace one or more players. This is seen as a trading, where players are replaced with other players between the two teams. These two opportunities to recruit new players to the team are called drafting, which is a process of recruiting players.</p><p>This study focuses on the recruitment of players from college. In America, it is a system used to recruit both new and old players to their respective teams in the National Football League, NFL. This study evaluates and analyzes the factors based on how each team in the NFL recruits players from the U.S. college league (NCAA). The factors analyzed were for instance each individual player’s performance during the college league, such as age, passing touchdowns and tackles. By using logistic analysis, these factors could then be determined by analyzing the odds and its change for each player's performance based on their positions in the team. This study shows that in every way a positional player performs during the college league, there will be important factors that contribute to recruitment to the NFL.</p>

corrected abstract:
<p>American football is a well-known sport in America. In addition to the design and rules of the game and the surface it is played on, there are also other differences that are not directly visible. One difference noted in this study is how the teams in the game provide players. Each team has a board of directors together with the team's leadership and they will determine which players are most suitable for the team. These players are recruited to the team after graduating from a U.S. college. This makes it possible to recruit new and young players to the squad. If the team wants other types of new players, there is also an opportunity to replace one or more players. This is seen as a trading, where players are replaced with other players between the two teams. These two opportunities to recruit new players to the team are called <em>drafting</em>, which is a process of recruiting players. This study focuses on the recruitment of players from college. In America, it is a system used to recruit both new and old players to their respective teams in the National Football League, NFL.</p><p>This study evaluates and analyzes the factors based on how each team in the NFL recruits players from the U.S. college league (NCAA). The factors analyzed were for instance each individual player’s performance during the college league, such as age, passing touchdowns and tackles. By using logistic analysis, these factors could then be determined by analyzing the odds and its change for each player's performance based on their positions in the team. This study shows that in every way a positional player performs during the college league, there will be important factors that contribute to recruitment to the NFL.</p>

Note moved the paragraph break to where it is in the original and added italics
----------------------------------------------------------------------
In diva2:838515   - correct as is
----------------------------------------------------------------------
In diva2:849700   - correct as is
----------------------------------------------------------------------
In diva2:1249683 
abstract is: 
<p>In this study the ISDA SIMM and the initial margin requirements for non-centrally cleared over the counter derivatives is investigated and compared with the traditional risk measure value-at-risk.</p><p>The empirical results suggest that for swap portfolios the ISDA SIMM achieves its set out purpose of being less volatile and more transparent than the 10-day value-at-risk on a 99% conﬁdence level. However, the SIMM framework will re-quire market participants to be continuously updated and provided calibration parameters which reﬂect current market conditions from ISDA.</p>

corrected abstract:
<p>In this study the ISDA SIMM and the initial margin requirements for non-centrally cleared over the counter derivatives is investigated and compared with the traditional risk measure value-at-risk.</p><p>The empirical results suggest that for swap portfolios the ISDA SIMM achieves its set out purpose of being less volatile and more transparent than the 10-day value-at-risk on a 99% confidence level. However, the SIMM framework will require market participants to be continuously updated and provided calibration parameters which reflect current market conditions from ISDA.</p>

Note - only change deleted an unnecessary hyphen in "re-quire"
----------------------------------------------------------------------
In diva2:1757070 
abstract is: 
<p>This study aims to find the best model to predict the outcome of football (1,X,2 - Home Win, Draw, Away Win) games by looking at match data. The data used is put together from the three highest football divisions in England and go back to the year 2005. Multinomial logistic regression is used to model the response variable from the regressors. A best subset regression is used to find the models with the lowest Akaike Information Criterion (AIC). By doing a multicollinearity analysis these models are further examined and the best one is chosen. </p><p>The results show both expected and unexpected effects that create foundation for future studies. Areas for model improvement include more variables, comparison with the bookmaker’s odds and tests on new test data. The application of the model is in sports betting where it can be used to value multi bets and live odds.</p>

corrected abstract:
<p>This study aims to find the best model to predict the outcome of football (<em>1,X,2 - Home Win, Draw, Away Win</em>) games by looking at match data. The data used is put together from the three highest football divisions in England and go back to the year 2005. Multinomial logistic regression is used to model the response variable from the regressors. A best subset regression is used to find the models with the lowest <em>Akaike Information Criterion (AIC)</em>. By doing a multicollinearity analysis these models are further examined and the best one is chosen.</p><p>The results show both expected and unexpected effects that create foundation for future studies. Areas for model improvement include more variables, comparison with the bookmaker’s odds and tests on new test data. The application of the model is in sports betting where it can be used to value multi bets and live odds.</p>

Note removed an unnecessary space at the end of a paragraph and added italics
----------------------------------------------------------------------
In diva2:1145323   - correct as is
----------------------------------------------------------------------
In diva2:813123 
abstract is: 
<p>This is a project that describes a part of the development of a tool intended for wheelchair users in their learning process of new and important movements in everyday life and in sports. The process began with an investigation, with the help of people with wheelchair experience, of which movements that were considered important and which were considered useful in sports and in everyday life. The project was first focused on improving the technical skills of athletes practicing wheelchair sports. After studies and discussions it however changed focus, to also deal with obstacles in everyday life and to help people using wheelchairs to learn more everyday movements. After practical workshops, a mechanical analysis of the wheelchair was made. Some types of movement patterns, such as balancing on the rear wheels and turning, was considered essential to study and the goal was also to get information about for example the speed and acceleration of the wheelchair. This was to be analysed in real time and then sonified in later steps. We developed a code torecognize and assess these movements, which is the product of the project.</p>

mc='torecognize' c='to recognize'

corrected abstract:
<p>This is a project that describes a part of the development of a tool intended for wheelchair users in their learning process of new and important movements in everyday life and in sports. The process began with an investigation, with the help of people with wheelchair experience, of which movements that were considered important and which were considered useful in sports and in everyday life. The project was first focused on improving the technical skills of athletes practicing wheelchair sports. After studies and discussions it however changed focus, to also deal with obstacles in everyday life and to help people using wheelchairs to learn more everyday movements. After practical workshops, a mechanical analysis of the wheelchair was made. Some types of movement patterns, such as balancing on the rear wheels and turning, was considered essential to study and the goal was also to get information about for example the speed and acceleration of the wheelchair. This was to be analysed in real time and then sonified in later steps. We developed a code to recognize and assess these movements, which is the product of the project.</p>
----------------------------------------------------------------------
In diva2:1701376 
abstract is: 
<p>Cell differentiation is the process of a cell developing from one cell type to another. It is of interest to analyse the differentiation from stem cells to different types of mature cells, and discover what genes are involved in regulating the differentiation to specific cells, for instance to get insights to what is causing certain diseases and find potential treatments. </p><p>In this project, two mathematical models are developed for analysing blood cell differentiation (haematopoiesis) with methods based on optimal transportation. Optimal transportation is about moving one mass distribution to another at minimal cost. Modelling a sample of cells as point masses placed in a space based on the cells' gene expressions, accessed by single-cell RNA sequencing, optimal transportation is used to find transitions between cells that costs the least in terms of changes in gene expression. With this, cell-to-cell trajectories, from haematopoietic stem cells to mature blood cells, are obtained.</p><p>With the first model, cells are divided into groups based on their maturity, which is determined by using diffusion pseudotime, and optimal transportation is preformed between groups. The resulting trajectories suggest that haematopoietic stem cells possibly can develop into the same mature cell type in different ways, and that the cell fate for some cell types is decided late on in development. In future work, the gene regulation along the obtained trajectories can be analysed. The second model is developed to be more general than the first, and not be dependent on a group division before preforming optimal transportation.</p>

corrected abstract:
<p>Cell differentiation is the process of a cell developing from one cell type to another. It is of interest to analyse the differentiation from stem cells to different types of mature cells, and discover what genes are involved in regulating the differentiation to specific cells, for instance to get insights to what is causing certain diseases and find potential treatments.</p><p>In this project, two mathematical models are developed for analysing blood cell differentiation (haematopoiesis) with methods based on optimal transportation. Optimal transportation is about moving one mass distribution to another at minimal cost. Modelling a sample of cells as point masses placed in a space based on the cells' gene expressions, accessed by single-cell RNA sequencing, optimal transportation is used to find transitions between cells that costs the least in terms of changes in gene expression. With this, cell-to-cell trajectories, from haematopoietic stem cells to mature blood cells, are obtained.</p><p>With the first model, cells are divided into groups based on their maturity, which is determined by using diffusion pseudotime, and optimal transportation is preformed between groups. The resulting trajectories suggest that haematopoietic stem cells possibly can develop into the same mature cell type in different ways, and that the cell fate for some cell types is decided late on in development. In future work, the gene regulation along the obtained trajectories can be analysed. The second model is developed to be more general than the first, and not be dependent on a group division before preforming optimal transportation.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1570309 
abstract is: 
<p>This thesis analyses four different optimization algorithms for training a convolutional neural network (CNN) using three different datasets. The algorithms studied were stochastic gradient descent (SGD), Polyak momentum, Nesterov momentum and Adaptive Moment Estimation (ADAM). The data sets used to train the algorithms were two image based datasets called Fashion MNIST and Cifar-10 as well as a multivariate data set containing information about flower species called Iris. The algorithm that reached the highest accuracy across all three data sets was the Adaptive Moment Estimation, therefore making it the most suitable algorithm for these datasets.</p>

corrected abstract:
<p>This thesis analyses four different optimization algorithms for training a convolutional neural network (CNN) using three different datasets. The algorithms studied were stochastic gradient descent (SGD), Polyak momentum, Nesterov momentum and Adaptive Moment Estimation (ADAM). The data sets used to train the algorithms were two image based data sets called Fashion MNIST and Cifar-10 as well as a multivariate data set containing information about flower species called Iris. The algorithm that reached the highest accuracy across all three data sets was the Adaptive Moment Estimation, therefore making it the most suitable algorithm for these datasets.</p>

Note - only change is to make "datasets" into "data sets" - as per the original
----------------------------------------------------------------------
In diva2:1800559   - correct as is
----------------------------------------------------------------------
In diva2:1268646 
abstract is: 
<p>The main aim of this thesis was to understand the mechanism behind the classical transition scenario inside the boundary layer over an airfoil and eventually attempting to control this transition utilizing passive devices for transition delay. The initial objective of analyzing the transition phenomenon based on TS wave disturbance growth was conducted at 90 Hz using LDV and CTA measurement techniques at two different angles of attack. This was combined with the studies performed on two other frequencies of 100 and 110 Hz, in order to witness its impact on the neutral stability curve behavior. The challenges faced in the next phase of the thesis while trying to control the transition location, was to understand and encompass the effect of adverse pressure gradient before setting up the passive control devices, which in this case was miniature vortex generators. Consequently, several attempts were made to optimize the parameters of the miniature vortex generators depending upon the streak strength and stability. Finally, for 90 Hz a configuration of miniature vortex generators have been found to successfully stabilize the TS wave disturbances below a certain forcing amplitude, which also led to transition delay.</p>
corrected abstract:
<p>The main aim of this thesis was to understand the mechanism behind the classical transition scenario inside the boundary layer over an airfoil and eventually attempting to control this transition utilizing passive devices for transition delay. The initial objective of analyzing the transition phenomenon based on TS wave disturbance growth was conducted at 90 Hz using LDV and CTA measurement techniques at two different angles of attack. This was combined with the studies performed on two other frequencies of 100 and 110 Hz, in order to witness its impact on the neutral stability curve behaviour.</p><p>The challenges faced in the next phase of the thesis while trying to control the transition location, was to understand and encompass the effect of adverse pressure gradient before setting up the passive control devices, which in this case was miniature vortex generators. Consequently, several attempts were made to optimize the parameters of the miniature vortex generators depending upon the streak strength and stability. Finally, for 90 Hz a configuration of miniature vortex generators have been found to successfully stabilize the TS wave disturbances below a certain forcing amplitude, which also led to transition delay.</p>

Note "behavior" should eb "behaviour" - as in the original
----------------------------------------------------------------------
In diva2:509618 
abstract is: 
<p>In this work, a flow reversal resonator fitted with a short-circuit duct connecting the inlet and outlet is analysed and used as a tuneable muffler element, aimed to be used use in a semi-active muffler on an IC-engine. The work done can be divided into 3 main parts. 1), a study of what type of valve that could be used to change the acoustical properties of the short-circuit duct. 2), Design of a flow reversal resonator with a controllable valve as the short-circuit. 3), experimental validation in a flow acoustic test rig. The flow reversal resonator with a controllable valve as short-circuit is successfully validated to work as tuneable muffler element during laboratory conditions. The same valve concept is simulated in a full scale concept but not validated experimentally on a running IC-engine. The theory used to describe the acoustics of a flow duct element is also presented together with three simulation techniques and the two microphone technique used to determine the acoustic properties of the investigated flow reversal resonator.</p>

corrected abstract:
<p>In this work, a flow reversal resonator fitted with a short-circuit duct connecting the inlet and outlet is analysed and used as a tuneable muffler element, aimed to be used use in a semi-active muffler on an IC-engine. The work done can be divided into 3 main parts. 1), a study of what type of valve that could be used to change the acoustical properties of the short-circuit duct. 2), Design of a flow reversal resonator with a controllable valve as the short-circuit. 3), experimental validation in a flow acoustic test rig.</p><p>The flow reversal resonator with a controllable valve as short-circuit is successfully validated to work as tuneable muffler element during laboratory conditions. The same valve concept is simulated in a full scale concept but not validated experimentally on a running IC-engine.</p><p>The theory used to describe the acoustics of a flow duct element is also presented together with three simulation techniques and the two microphone technique used to determine the acoustic properties of the investigated flow reversal resonator.</p>

Note - added the missing paragraph breaks
----------------------------------------------------------------------
In diva2:633167 - missing space in title:
"Analysis and implementation of anefficient solver for large-scalesimulations of neuronal systems"
==>
"Analysis and implementation of an efficient solver for large-scale simulations of neuronal systems"


abstract is: 
<p>Numerical integration methods exploiting the characteristics of neuronal equation systems were investigated. The main observations was a high stiffness and a quasi-linearity of the system. The latter allowed for decomposition into two smaller systems by using a block diagonal Jacobian approximation. The popular backwards differentiation formulas methods (BDF) showed performance degradation for this during first experiments. Linearly implicit peer methods (PeerLI), a new class of methods, did not show this degradation. Parameters for PeerLI were optimized by experimental means and then compared in performance to BDF. Models were simulated in both Matlab and NEURON, a neuron modelling package. For small models PeerLI was competitive with BDF, especially with a block diagonal Jacobian. In NEURON the performance of the block diagonal Jacobian did no longer degrade for BDF, but instead showed degradation for PeerLI, especially for large models. With full Jacobian PeerLI was competitive with BDF, but with block diagonal Jacobian an increase of ca.50% was seen in simulation time. Overall PeerLI methods were competitive for certain problems, but did not give the desired performance gain for block diagonal Jacobian for large problems. There is, however, still a lot of room for improvement, since parameters were only determined experimentally and tuned to small problems.</p>

corrected abstract:
<p>Numerical integration methods exploiting the characteristics of neuronal equation systems were investigated. The main observations was a high stiffness and a quasi-linearity of the system. The latter allowed for decomposition into two smaller systems by using a block diagonal Jacobian approximation. The popular backwards differentiation formulas methods (BDF) showed performance degradation for this during first experiments. Linearly implicit peer methods (PeerLI), a new class of methods, did not show this degradation. Parameters for PeerLI were optimized by experimental means and then compared in performance to BDF. Models were simulated in both Matlab and NEURON, a neuron modelling package. For small models PeerLI was competitive with BDF, especially with a block diagonal Jacobian. In NEURON the performance of the block diagonal Jacobian did no longer degrade for BDF, but instead showed degradation for PeerLI, especially for large models. With full Jacobian PeerLI was competitive with BDF, but with block diagonal Jacobian an increase of ca. 50% was seen in simulation time. Overall PeerLI methods were competitive for certain problems, but did not give the desired performance gain for block diagonal Jacobian for large problems. There is, however, still a lot of room for improvement, since parameters were only determined experimentally and tuned to small problems.</p>

Note added space after "ca." and before "50%"
----------------------------------------------------------------------
In diva2:1679315   - correct as is
----------------------------------------------------------------------
In diva2:1380194 
abstract is: 
<p>Aircraft design is an act of art requiring dedication and careful work to ensure good results. An essential tool in that work is a flight mechanics simulator. Such simulators are often built up of modules/models that are executed in a sequential order in each time iteration. This project aims to analyze potential improvements to the model execution order based on the dependency structure of one such simulator. The analysis method Design Structure Matrix (DSM), was used to define/map the dependencies and then Binary Linear Programming (BLP) was utilized to find five new potentially improved model orders to minimize the number of feedbacks from one iteration to the next one. Those five proposed execution orders were next compared and evaluated. The result is a model order that reduce the number of models receiving feedbacks from the previous iteration from 13 to 6, with insignificant changes in the precision of the simulator.</p>

corrected abstract:
<p>Aircraft design is an act of art requiring dedication and careful work to ensure good results. An essential tool in that work is a flight mechanics simulator. Such simulators are often built up of modules/models that are executed in a sequential order in each time iteration. This project aims to analyze potential improvements to the model execution order based on the dependency structure of one such simulator. The analysis method <em>Design Structure Matrix (DSM)</em>, was used to define/map the dependencies and then <em>Binary Linear Programming (BLP)</em> was utilized to find five new potentially improved model orders to minimize the number of feedbacks from one iteration to the next one. Those five proposed execution orders were next compared and evaluated. The result is a model order that reduce the number of models receiving feedbacks from the previous iteration from 13 to 6, with insignificant changes in the precision of the simulator.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1289344 
abstract is: 
<p>One of the tasks for the Swedish Transport Agency, Traﬁkverket, is to provide traﬃc forecasts. To do this, a number of diﬀerent forecast models are used, where Samgods is a nationally estimated model, where the quality of the results gets more unstable the more disaggregated level you are looking at. For rail this is handled with a model called Bangods. However, in Bangods the diﬀerence in geographical growth within each commodity group is lost.</p><p>This thesis examines whether it is possible to replace the national growth rates from Samgods with geographical disaggregated growth rates. The growth rates are calculated with a mathematical model based on the pivot point method (PPM). The model has been implemented in Python and is used to disaggregate the growth rates from Samgods to maintain the geographical growth. However, the data to the model comes from diﬀerent systems and models that use diﬀerent link formats. Therefore a link matching method is required that converts links from one system to another before using PPM.</p><p>The growth rates from the PPM and the link matching-method has been modelled for twelve commodity groups, 8 or 1417 geographic regions and with or without a train division with four train types. The best result was to used 96 growth rates divided into twelve commodity groups and eight geographical regions.</p>


corrected abstract:
<p>One of the tasks for the Swedish Transport Agency, Trafikverket, is to provide traffic forecasts. To do this, a number of different forecast models are used, where Samgods is a nationally estimated model, where the quality of the results gets more unstable the more disaggregated level you are looking at. For rail this is handled with a model called Bangods. However, in Bangods the difference in geographical growth within each commodity group is lost.</p><p>This thesis examines whether it is possible to replace the national growth rates from Samgods with geographical disaggregated growth rates. The growth rates are calculated with a mathematical model based on the pivot point method (PPM). The model has been implemented in Python and is used to disaggregate the growth rates from Samgods to maintain the geographical growth. However, the data to the model comes from different systems and models that use different link formats. Therefore a link matching method is required that converts links from one system to another before using PPM.</p><p>The growth rates from the PPM and the link matching-method has been modelled for twelve commodity groups, 8 or 1417 geographic regions and with or without a train division with four train types. The best result was to used 96 growth rates divided into twelve commodity groups and eight geographical regions.</p>


Note replaced ligatures with their expended evrsion
----------------------------------------------------------------------
In diva2:1578526 
abstract is: 
<p>In this study a finite element model has been created in ABAQUS/Standard in order to replicate impact tests performed by SSAB and to determine residual stresses in a plate. Uniaxial tensile tests have been done in order to determine the plastic behaviour of a high strength steel from the manufacturer SSAB and to determine whether the material can be assumed to be isotropic. Test specimens in the rolling and cross-rolling direction were cut out from an undeformed plate. The data was fit to a Voce hardening law which showed that the material is not isotropic due to different initial yield stresses. However, the hardening curve can be regarded as isotropic due to small differences between the rolling and cross-rolling direction.</p><p>Vickers hardness tests have been done on three test specimens cut out from a deformed plate which has been used in the impact tests at SSAB. These Vickers hardness tests have been done in order to verify the finite element model. The Vickers hardness determined from experiments was compared to the calculated Vickers hardness from the finite element simulation. The comparison showed good agreement between experiments and simulations seeing that they follow the same trend. This indicate that the finite element model is a good numerical method to determine residual stresses in high strength steels subjected to significant plastic deformation.</p>

corrected abstract:
<p>In this study a finite element model has been created in ABAQUS/Standard in order to replicate impact tests performed by SSAB and to determine residual stresses in a plate. Uniaxial tensile tests have been done in order to determine the plastic behaviour of a high strength steel from the manufacturer SSAB and to determine whether the material can be assumed to be isotropic. Test specimens in the rolling and cross-rolling direction were cut out from an undeformed plate. The data was fit to a Voce hardening law which showed that the material is not isotropic due to different initial yield stresses. However, the hardening curve can be regarded as isotropic due to small differences between the rolling and cross-rolling direction.</p><p>Vickers hardness tests have been done on three test specimens cut out from a deformed plate which has been used in the impact tests at SSAB. These Vickers hardness tests have been done in order to verify the finite element model. The Vickers hardness determined from experiments was compared to the calculated Vickers hardness from the finite element simulation. The comparison showed good agreement between experiments and simulations seeing that they follow the same trend. This indicate that the finite element model is a good numerical method to determine residual stresses in high strength steels subjected to significant plastic deformation.</p>

Note change in hyphens from "0x1d" to "0x2d"
----------------------------------------------------------------------
In diva2:1817060 
abstract is: 
<p>A reduction in the global use of fossil fuels is necessary when striving for a more sustainable future. One key strategy in the transition from fossil fuels is electrification. This strategy is particularly prominent within the transport sector, where more efficient ways to store electric energy are being pursued. Structural battery composites represent a promising technology. Being based on multifunctional composite materials that can carry mechanical loads and store electrical energy at the same time, they provide a ‘mass-less’ energy storage.</p><p>This work aims to develop a shape morphing structural battery capable of bending upwards and downwards in a cantilever setup. The structural battery is made from several constituents. Two outer layers of carbon fibers act as negative electrodes and a middle layer of aluminium foil coated with NMC622 on both sides acts as the positive electrodes. Additionally, a glass veil layer and a ceramic separator separate the positive and negative electrodes. A structural battery electrolyte is used to embed the laminate in order to provide load transfer and ion transfer. From this setup, it is possible to control the lithiation/delithiation of each carbon fiber layer independently and thereby bend the laminate in the desired direction. Subsequently, the system is modeled both analytically using Matlab and numerically using Comsol Multiphysics 6.1. </p><p>From the models it is found that the system is in theory capable of large deformations, showing promising results. However, the experimental laminates show low capacity upon cycling which would cause near to zero deformations. The poor performance of the system could be linked to incompatibility between the structural battery electrolyte and the NMC622.</p>

corrected abstract:
<p>A reduction in the global use of fossil fuels is necessary when striving for a more sustainable future. One key strategy in the transition from fossil fuels is electrification. This strategy is particularly prominent within the transport sector, where more efficient ways to store electric energy are being pursued. Structural battery composites represent a promising technology. Being based on multifunctional composite materials that can carry mechanical loads and store electrical energy at the same time, they provide a ‘mass-less’ energy storage.</p><p>This work aims to develop a shape morphing structural battery capable of bending upwards and downwards in a cantilever setup. The structural battery is made from several constituents. Two outer layers of carbon fibers act as negative electrodes and a middle layer of aluminium foil coated with NMC622 on both sides acts as the positive electrodes. Additionally, a glass veil layer and a ceramic separator separate the positive and negative electrodes. A structural battery electrolyte is used to embed the laminate in order to provide load transfer and ion transfer. From this setup, it is possible to control the lithiation/delithiation of each carbon fiber layer independently and thereby bend the laminate in the desired direction. Subsequently, the system is modeled both analytically using Matlab and numerically using Comsol Multiphysics 6.1.</p><p>From the models it is found that the system is in theory capable of large deformations, showing promising results. However, the experimental laminates show low capacity upon cycling which would cause near to zero deformations. The poor performance of the system could be linked to incompatibility between the structural battery electrolyte and the NMC<sub>622</sub>.</p>

Note eliminated an unnecessary space at the end of a paragraph and added subscripts
----------------------------------------------------------------------
In diva2:1279807 
abstract is: 
<p>In a gas turbine rotor, the Blade-Disc attachment is often a critical part. This part needs to endure both high contact stress and high temperature. Thus, the prediction of the life of the attachment becomes important.  Furthermore, the refinement of the Blade-Disc attachment is worth to study, to extend the lifetime of the turbine rotor. In this work, the Blade-Disc attachment is belonging to the gas turbine section.   </p><p>Geometry modifications based on a 2D FE-model of a blade-disc attachment are performed. Besides of flat contact surface, an alternative curved contact form which creates a barrel on the contact surface is studied. The effect of the different geometries, is studied with respect to high local stresses. The results are evaluated using different methods; deterministic LCF life using EVAL, probabilistic LCF life using ProbLCF and fretting damage parameter evaluation  using the Ruiz-Chen Model.   </p><p>During this work, geometry modifications based on a 2D Blade-Disc attachment is performed first to study the effect of the different modification results. Then a fretting damage parameter evaluation is carried out, to discuss the crack initiation position of the models with respect to different modification cases. Finally, low cycle fatigue tests are performed to analyse the life cycle of the models.   </p><p>The result of the work shows at manufactory tolerances can cause a maximum stress increase by at least 60% in the model, especially in the contact surfaces. Also, comparison results shows apply a barrel to the contact surface can effectively decrease the maximum stress in the model, and extend the cyclic life of the attachment. The most suitable barrel height for this work is between 0.05[mm] and 0.015 [mm].</p>

corrected abstract:
<p>In a gas turbine rotor, the Blade-Disc attachment is often a critical part. This part needs to endure both high contact stress and high temperature. Thus, the prediction of the life of the attachment becomes important.  Furthermore, the refinement of the Blade-Disc attachment is worth to study, to extend the lifetime of the turbine rotor. In this work, the Blade-Disc attachment is belonging to the gas turbine section.</p><p>Geometry modifications based on a 2D FE-model of a blade-disc attachment are performed. Besides of flat contact surface, an alternative curved contact form which creates a barrel on the contact surface is studied. The effect of the different geometries, is studied with respect to high local stresses. The results are evaluated using different methods; deterministic LCF life using EVAL, probabilistic LCF life using ProbLCF and fretting damage parameter evaluation using the Ruiz-Chen Model.</p><p>During this work, geometry modifications based on a 2D Blade-Disc attachment is performed first to study the effect of the different modification results. Then a fretting damage parameter evaluation is carried out, to discuss the crack initiation position of the models with respect to different modification cases. Finally, low cycle fatigue tests are performed to analyse the life cycle of the models.</p><p>The result of the work shows at manufactory tolerances can cause a maximum stress increase by at least 60% in the model, especially in the contact surfaces. Also, comparison results shows apply a barrel to the contact surface can effectively decrease the maximum stress in the model, and extend the cyclic life of the attachment. The most suitable barrel height for this work is between 0.05[mm] and 0.015 [mm].</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph and eliminate an unnecessary space
----------------------------------------------------------------------
In diva2:447365 
abstract is: 
<p>It is well known that boiling water reactors can experience inadvertent power oscillations. When such instability occurs the core can oscillate in two different modes (in phase mode and out of phase mode). In the late 90’s a stability benchmark was created using the stability data obtained from the experiments at the Swedish nuclear power plant of Ringhals-1. Data was collected from the cycles 14, 15 , 16 and 17. Later on, this data was used to validate the various models and codes with the aim of predicting the instability behavior of the core and understand the triggers of such oscillations. The current trend of increasing reactor power density and relying on natural circulation for core cooling may have consequences for the stability of modern BWR’s designs. The objective of this work is to find the most important parameters affecting the stability of the BWRs and propose alternative stability maps. For this purpose a TRACE/PARCS model of the Ringhals-1 NPP will be used. Afterwards a selection of possible parameters and dimensionless numbers will be made to study its effect on stability. Once those parameters are found they will be included in the stability maps to make them more accurate.</p>

corrected abstract:
<p>It is well known that boiling water reactors can experience inadvertent power oscillations. When such instability occurs the core can oscillate in two different modes (in phase mode and out of phase mode). In the late 90’s a stability benchmark was created using the stability data obtained from the experiments at the Swedish nuclear power plant of Ringhals-1. Data was collected from the cycles 14, 15 , 16 and 17. Later on, this data was used to validate the various models and codes with the aim of predicting the instability behavior of the core and understand the triggers of such oscillations. The current trend of increasing reactor power density and relying on natural circulation for core cooling may have consequences for the stability of modern BWR’s designs. The objective of this work is to find the most important parameters affecting the stability of the BWRs and propose alternative stability maps. For this purpose a TRACE/PARCS model of the Ringhals-1 NPP will be used. Afterwards a selection of possible parameters and dimensionless numbers will be made to study its effect on stability. Once those parameters are found they will be included in the stability maps to make them more accurate.</p>
----------------------------------------------------------------------
In diva2:810154 
abstract is: 
<p>Hydro power is the largest source for generation of electricity in the Nordic region today.</p><p>This production is heavily dependent on the weather since it dictates the terms for the availability and the amount of power to be produced. Vattenfall as a company has an incentive to avoid volatile revenue streams as it facilitates economic planning and induces a positive effect on its credit rating, thus also on its bottom line. Vattenfall is a large producer of hydro power with a possibility to move the power market which adds further complexity to the problem. In this thesis the authors develop new hedging strategies which will hedge more efficiently. With efficiency is meant the same risk, or standard deviation, at a lower cost or alternatively formulated lower risk for the same cost. In order to enable comparison and make claims about efficiency, a reference solution is developed that should reflect their current hedging strategy. To achieve higher efficiency we focus on finding dynamic hedging strategies. First a prototype model is suggested to facilitate the construction of the solution methods and if it is worthwhile to pursue a further investigation. As this initial prototype model results showed that there were substantial room for efficiency improvement, a larger main model with parameters estimated from data is constructed which encapsulate the real world scenario much better. Four different solutions methods are developed and applied to this main model setup. The results are then compared to reference strategy. We find that even though the efficiency was less then first expected from the prototype model results, using these new hedging strategies could reduce costs by 1.5 % - 5%. Although the final choice of the hedging strategy might be down to the end user we suggest the strategy called BW to reduce costs and improve efficiency. The paper also discusses among other things; the solution methods and hedging strategies, the term optimality and the impact of parameters in the model.</p>

corrected abstract:
<p>Hydro power is the largest source for generation of electricity in the Nordic region today. This production is heavily dependent on the weather since it dictates the terms for the availability and the amount of power to be produced. Vattenfall as a company has an incentive to avoid volatile revenue streams as it facilitates economic planning and induces a positive effect on its credit rating, thus also on its bottom line. Vattenfall is a large producer of hydro power with a possibility to move the power market which adds further complexity to the problem. In this thesis the authors develop new hedging strategies which will hedge more efficiently. With efficiency is meant the same risk, or standard deviation, at a lower cost or alternatively formulated lower risk for the same cost. In order to enable comparison and make claims about efficiency, a reference solution is developed that should reflect their current hedging strategy. To achieve higher efficiency we focus on finding dynamic hedging strategies. First a prototype model is suggested to facilitate the construction of the solution methods and if it is worthwhile to pursue a further investigation. As this initial prototype model results showed that there were substantial room for efficiency improvement, a larger main model with parameters estimated from data is constructed which encapsulate the real world scenario much better. Four different solutions methods are developed and applied to this main model setup. The results are then compared to reference strategy. We find that even though the efficiency was less then first expected from the prototype model results, using these new hedging strategies could reduce costs by 1.5 % - 5%. Although the final choice of the hedging strategy might be down to the end user we suggest the strategy called <em>BW</em> to reduce costs and improve efficiency. The paper also discusses among other things; the solution methods and hedging strategies, the term optimality and the impact of parameters in the model.</p>

Note removed the unnecessary paragraph break and added italics
----------------------------------------------------------------------
In diva2:1449088 
abstract is: 
<p>The performance of paperboard materials in packaging application has been investigated and evaluated for a long time. This is because it plays a decisive role for product protection and decoration in packaging applications. Potential damages during transportation sometimes affect the consistency of the performance. Therefore, the capability of the material to resist these external disturbances was of interest.</p><p>A multiply paperboard was chosen as the experimental material. The analysis conducted in this thesis aimed to reveal the tensile behavior in the cross-machine direction (CD) of the material against various kinds of local or global changes. The changes included global and local climate variations, cutouts, and regional weakening and strengthening, which were applied during the intervals between preloading and reloading. The digital image correlation (DIC) analysis computed the time-varying strain fields from the gray level information contained in the recorded videos of loading processes. </p><p>The generated strain fields were imported to post analysis. Comparison between comparable stages (two stages with the same average strain value from different loading sections) was considered as the scheme of isolating the influences of the changes and investigating them individually. The cosine image similarity method and the eigenface algorithm were used to validate this scheme, while the directional average calculation and the strain field compensation method were introduced to realize the isolation.</p><p>The differences between the front and back outer plies of the paperboard sheets were detected as individual. Moreover, both global and local climate changes were affecting the strain distributions of the specimens proportionally on account of the moisture ratio within the material. In addition, the invisible mechanical weakening and strengthening were captured evidently with the analysis, which caused strain concentrations due to the uneven distribution of expansion capability. The relaxation and bending in unloading processes were two of the primary disturbing factors within all the deformed specimens, which were related to time and bending direction, correspondingly.</p>

corrected abstract:
<p>The performance of paperboard materials in packaging application has been investigated and evaluated for a long time. This is because it plays a decisive role for product protection and decoration in packaging applications. Potential damages during transportation sometimes affect the consistency of the performance. Therefore, the capability of the material to resist these external disturbances was of interest.</p><p>A multiply paperboard was chosen as the experimental material. The analysis conducted in this thesis aimed to reveal the tensile behavior in the cross-machine direction (CD) of the material against various kinds of local or global changes. The changes included global and local climate variations, cutouts, and regional weakening and strengthening, which were applied during the intervals between preloading and reloading. The digital image correlation (DIC) analysis computed the time-varying strain fields from the gray level information contained in the recorded videos of loading processes.</p><p>The generated strain fields were imported to post analysis. Comparison between comparable stages (two stages with the same average strain value from different loading sections) was considered as the scheme of isolating the influences of the changes and investigating them individually. The cosine image similarity method and the eigenface algorithm were used to validate this scheme, while the directional average calculation and the strain field compensation method were introduced to realize the isolation.</p><p>The differences between the front and back outer plies of the paperboard sheets were detected as individual. Moreover, both global and local climate changes were affecting the strain distributions of the specimens proportionally on account of the moisture ratio within the material. In addition, the invisible mechanical weakening and strengthening were captured evidently with the analysis, which caused strain concentrations due to the uneven distribution of expansion capability. The relaxation and bending in unloading processes were two of the primary disturbing factors within all the deformed specimens, which were related to time and bending direction, correspondingly.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:708226   - correct as is
----------------------------------------------------------------------
In diva2:942695 
abstract is: 
<p>Student debt in the U.S has grown rapidly over the last decades. A common practice among lenders is to pool the loans into securities that are sold off and traded between institutional investors. Since these securities have no market price this thesis aims to develop a valuation model. A time discrete approach is used, based on the Hull-White short-rate model to create a trinomial interest rate tree. This tree serves as a basis for the discounting of future cash flows generated from a specific student loan asset-backed security. In order to assess the credit risk, the student loan market and potential speculative bubbles are discussed.</p><p>The model is applied on the ”Navient Student Loan Trust 2015-2” and each tranche’s intrinsic value and yield to maturity is calculated. Since the model lacks proper quantification of the credit risk, the result is a valuation model that is best used when valuing asset-backed securities that can be deemed risk- free.</p>

corrected abstract:
<p>Student debt in the U.S has grown rapidly over the last decades. A common practice among lenders is to pool the loans into securities that are sold off and traded between institutional investors. Since these securities have no market price this thesis aims to develop a valuation model. A time discrete approach is used, based on the Hull-White short-rate model to create a trinomial interest rate tree. This tree serves as a basis for the discounting of future cash flows generated from a specific student loan asset-backed security. In order to assess the credit risk, the student loan market and potential speculative bubbles are discussed.</p><p>The model is applied on the ”Navient Student Loan Trust 2015-2” and each tranche’s intrinsic value and yield to maturity is calculated. Since the model lacks proper quantification of the credit risk, the result is a valuation model that is best used when valuing asset-backed securities that can be deemed risk-free.</p>

Note - removed space after hypehn
----------------------------------------------------------------------
In diva2:1503976 
abstract is: 
<p>The electrification of the truck is crucial to meet the strategic vision of the European Union (EU) to contribute to net-zero greenhouse gas emissions for all sectors of the economy and society. The battery-electric truck is very efficient to reduce the emissions and has also a lower Total Cost of Ownership (TCO) compared to diesel trucks. Thus, the energy consumption of the battery-electric truck needs to be analysed in detail, and the differences in the conventional powertrain, recuperation by regenerative braking during driving and charging during standing, need to be considered. This master thesis aims to analyse the energy consumption of the battery-electric truck during driving and standing charging. For driving cycle simulation the Vehicle Energy Consumption calculation TOol (VECTO) and MATLAB are used. Different variations, such as payload, rolling resistance, air drag, and Power Take Off (PTO), are considered in the driving cycle simulation. The driving cycle simulation is verified by calculating the energy balance and compared with the on-road test results. For the standing charging simulation, MATLAB is used to analyse the charging loss with different battery packs and charging speeds. The results are shown with the Sankey diagram and other illustrative tools. Seen from the simulation results, the usable energy of the battery pack is enough for the truck to complete the designed driving cycle. The main loss in the powertrain is the Power Electronic Converter (PEC) and the electric machine. To increase the range and reduce energy loss, using a higher efficiency PEC and electric machine is an efficient method. For the charging simulation, the current Combined Charging System (CCS) standard charging station can charge the battery-electric truck with adequate voltage and reasonable charging time. The main loss during the charging comes from the charging station.</p>

corrected abstract:
<p>The electrification of the truck is crucial to meet the strategic vision of the European Union (EU) to contribute to net-zero greenhouse gas emissions for all sectors of the economy and society. The battery-electric truck is very efficient to reduce the emissions and has also a lower Total Cost of Ownership (TCO) compared to diesel trucks. Thus, the energy consumption of the battery-electric truck needs to be analysed in detail, and the differences in the conventional powertrain, recuperation by regenerative braking during driving and charging during standing, need to be considered.</p><p>This master thesis aims to analyse the energy consumption of the battery-electric truck during driving and standing charging. For driving cycle simulation the Vehicle Energy Consumption calculation TOol (VECTO) and MATLAB are used. Different variations, such as payload, rolling resistance, air drag, and Power Take Off (PTO), are considered in the driving cycle simulation. The driving cycle simulation is verified by calculating the energy balance and compared with the on-road test results. For the standing charging simulation, MATLAB is used to analyse the charging loss with different battery packs and charging speeds. The results are shown with the Sankey diagram and other illustrative tools.</p><p>Seen from the simulation results, the usable energy of the battery pack is enough for the truck to complete the designed driving cycle. The main loss in the powertrain is the Power Electronic Converter (PEC) and the electric machine. To increase the range and reduce energy loss, using a higher efficiency PEC and electric machine is an efficient method. For the charging simulation, the current Combined Charging System (CCS) standard charging station can charge the battery-electric truck with adequate voltage and reasonable charging time. The main loss during the charging comes from the charging station.</p>

Note added missing paragrap breaks
----------------------------------------------------------------------
In diva2:1351514 
abstract is: 
<p>In this study 3D CFD was used to study the mass flow distribution in a reheater in a nuclear power plant. The aim was to see if 3D-modeling provide different results than traditionally used 1D-analysis.</p><p>Models with detailed geometry were created for a section of the tube package in the reheater to obtain pressure drop coefficients. The set up of the model for pure cross-flow over the tubes was first validated against an experiment by Ward [33]. The model showed good agreement with the experimental data for pressure drop and heat transfer, with both unsteady simulations using LES and steady simulations using the SST <em>k </em><em>- w</em> model. It also showed that using the empirical correlations by Ward and Young [32] and Rabas et al. [22], without any experimental data for the tube bank, gave an overestimation of the pressure drop.</p><p>The pressure drop coefficients obtained from the tube package simulations were used to model the tube package in the reheater as a porous medium. A set of perforated plates in the reheater needed to be modeled as a porous medium as well. These plates were originally meant to be mounted on the inlet side of the tube package to even out the mass flow distribution. However, due to a manufacturing error they were now mounted on the outlet side. A set of simulations using the SST <em>k </em><em>- w</em> for a part of the plate geometry gave the required pressure drop coefficients.</p><p>With all coefficients for the porous medium obtained a full scale model of the reheater was created. In the 3D-model the IAPWS-IF97 formulation was used to model the pressure and temperature dependent properties of the steam. Both the SST <em>k </em><em>- w</em> model and DES were used for turbulence closure. A 1D-simulation of the reheater was also performed using RELAP5.</p><p>The 3D-simulations showed a larger difference in the mass flow distribution between the upper and lower part of the tube package than the 1D-model. The 1D-model showed a clear connection between the pressure drop over the perforated plates and the mass flow distribution. However, in the 3D-model the mass flow distribution appeared to also be affected by other properties.</p>

corrected abstract:
<p>In this study 3D CFD was used to study the mass flow distribution in a reheater in a nuclear power plant. The aim was to see if 3D-modeling provide different results than traditionally used 1D-analysis.</p><p>Models with detailed geometry were created for a section of the tube package in the reheater to obtain pressure drop coefficients. The set up of the model for pure cross-flow over the tubes was first validated against an experiment by Ward [33]. The model showed good agreement with the experimental data for pressure drop and heat transfer, with both unsteady simulations using LES and steady simulations using the SST 𝑘-<em>ω</em> model. It also showed that using the empirical correlations by Ward and Young [32] and Rabas et al. [22], without any experimental data for the tube bank, gave an overestimation of the pressure drop.</p><p>The pressure drop coefficients obtained from the tube package simulations were used to model the tube package in the reheater as a porous medium. A set of perforated plates in the reheater needed to be modeled as a porous medium as well. These plates were originally meant to be mounted on the inlet side of the tube package to even out the mass flow distribution. However, due to a manufacturing error they were now mounted on the outlet side. A set of simulations using the SST 𝑘-<em>ω</em> for a part of the plate geometry gave the required pressure drop coefficients.</p><p>With all coefficients for the porous medium obtained a full scale model of the reheater was created. In the 3D-model the IAPWS-IF97 formulation was used to model the pressure and temperature dependent properties of the steam. Both the SST 𝑘-<em>ω</em> model and DES were used for turbulence closure. A 1D-simulation of the reheater was also performed using RELAP5.</p><p>The 3D-simulations showed a larger difference in the mass flow distribution between the upper and lower part of the tube package than the 1D-model. The 1D-model showed a clear connection between the pressure drop over the perforated plates and the mass flow distribution. However, in the 3D-model the mass flow distribution appeared to also be affected by other properties.</p>

Note added paragraph breaks and fixed the 𝑘-ω
----------------------------------------------------------------------
In diva2:1658933 - note the double colon is probably unnecessary in the title
abstract is: 
<p>The purpose of this study is to identify what company specific parameters prior to an IPO have significant impact on share price performance one year after listing. This is done by analysing listings on the Stockholm Stock Exchange in the period 2014-2019. </p><p>The method which has been used is a multiple linear regression with adjusted share price as response variable and 7 specific company data points as independent variables. The share price development of companies is adjusted to the SIX Return Index and the 7 company variables cover size, growth, profitability and ownership.</p><p>The results from the study imply that the independent variables covering size and profitability have the highest impact on share price performance after listing and that ownership had the least impact. The final model with the independent variables that had the highest relevance still only display a small significant correlation with an adjusted R2 = 0.09, which is understandable due to the nature of share prices not being able to be predicted one year into the future. Furthermore, the stock market is a large and complex system of many unknowns, which aggravates the process of simplifying and quantifying data of only one source into a regression model with high predictability.</p>

corrected abstract:
<p>The purpose of this study is to identify what company specific parameters prior to an IPO have significant impact on share price performance one year after listing. This is done by analysing listings on the Stockholm Stock Exchange in the period 2014-2019.</p><p>The method which has been used is a multiple linear regression with adjusted share price as response variable and 7 specific company data points as independent variables. The share price development of companies is adjusted to the SIX Return Index and the 7 company variables covers size, growth, profitability and ownership.</p><p>The results from the study imply that the independent variables covering size and profitability have the highest impact on share price performance after listing and that ownership had the least impact. The final model with the independent variables that had the highest relevance still only display a small significant correlation with an adjusted R2 = 0.09, which is understandable due to the nature of share prices not being able to be predicted one year into the future. Furthermore, the stock market is a large and complex system of many unknowns, which aggravate the process of simplifying and quantifying data of only one source into a regression model with high predictability.</p>

Note small changes in text, also note that "R2" should be "R<sup>2</sup>" - error in original
----------------------------------------------------------------------
In diva2:1416550 
abstract is: 
<p>The tyre is an essential part of a road vehicle. It is in the contact between road and tyre that the forces that create the possibility for the driver to control the vehicle are generated. Tyres, however, wear down, which leads to both unhealthy wear particles and disposal of old tyres, both of which are harmful to the environment. If one could learn more about what causes wear, it might be possible to reduce tyre wear, which would be beneficial from both an economic and an ecological point of view. The aim of this thesis work is to develop a tyre model that can simulate tyre wear and take temperature, pressure and vehicle settings into account. Based on tyre brush theory, a tyre wear model has been developed which includes a thermal model, a pressure model and a friction model. Simulations and analysis of different cases has been performed. From the results, one can conclude the following: the tyre temperature and inflation pressure change with the distance the vehicle travels at the beginning and later become steady; higher external temperature will decrease tyre wear rate since the inflation pressure increases with the external temperature and the sliding friction decreases; higher vehicle speed leads to a higher tyre wear rate; the tyre temperature increases with increasing vehicle speed; the amount of tyre wear increases linearly with the normal load on the tyre; the tyre wear increases with the slip ratio exponentially due to both the siding distance and the sliding friction increasing with the slip ratio; the tyre wear increases exponentially with the slip angle. The complete model can estimate the tyre wear with different vehicle settings and external factors. More experiments are needed in the future to validate the complete model. In addition, since the heat transfer coefficient is changeable with temperature, the thermal model can be improved by introducing dynamic heat transfer coefficients. The Savkoor friction model used in the report can also be improved by tuning its parameters using more experimental data.</p>

corrected abstract:
<p>The tyre is an essential part of a road vehicle. It is in the contact between road and tyre that the forces that create the possibility for the driver to control the vehicle are generated. Tyres, however, wear down, which leads to both unhealthy wear particles and disposal of old tyres, both of which are harmful to the environment. If one could learn more about what causes wear, it might be possible to reduce tyre wear, which would be beneficial from both an economic and an ecological point of view.</p><p>The aim of this thesis work is to develop a tyre model that can simulate tyre wear and take temperature, pressure and vehicle settings into account.</p><p>Based on tyre brush theory, a tyre wear model has been developed which includes a thermal model, a pressure model and a friction model. Simulations and analysis of different cases has been performed. From the results, one can conclude the following: the tyre temperature and inflation pressure change with the distance the vehicle travels at the beginning and later become steady; higher external temperature will decrease tyre wear rate since the inflation pressure increases with the external temperature and the sliding friction decreases; higher vehicle speed leads to a higher tyre wear rate; the tyre temperature increases with increasing vehicle speed; the amount of tyre wear increases linearly with the normal load on the tyre; the tyre wear increases with the slip ratio exponentially due to both the siding distance and the sliding friction increasing with the slip ratio; the tyre wear increases exponentially with the slip angle.</p><p>The complete model can estimate the tyre wear with different vehicle settings and external factors. More experiments are needed in the future to validate the complete model. In addition, since the heat transfer coefficient is changeable with temperature, the thermal model can be improved by introducing dynamic heat transfer coefficients. The Savkoor friction model used in the report can also be improved by tuning its parameters using more experimental data.</p>

Note added paragraph breaks
----------------------------------------------------------------------
In diva2:1442611 
abstract is: 
<p>Due to their large torque-speed ratio and transmission efficiency, planetary gears are widely used in the automotive industry. However, high amplitude vibrations remain their critical weakness, which limits their usage especially when new strict noise legislations come into action.</p><p>A new approach to handle the instability problems of planetary gears encountered in real industrial context is presented in this work. First, the dynamic response of a planetary gear failing to pass the noise regulations is theoretically investigated through an analytical model. The equations of motion were solved using the Spectral Iterative Method. The observed experimental results correlated well with those from the developed model. In order to limit the resonance phenomena, impacts of different macro and micro-geometry modifications were analytically investigated: quadratic teeth profile, different planets positioning, different number of teeth and number of planets. Optimum modifications were retrieved and are expected to be tested experimentally on a test bench and on the truck.</p><p>Finally, the analytical model’s limits and sensitivity to different parameters were investigated in order to certify its reliability, and suggestions for improvements were presented.</p>

corrected abstract:
<p>Due to their large torque-speed ratio and transmission efficiency, planetary gears are widely used in the automotive industry. However, high amplitude vibrations remain their critical weakness, which limits their usage especially when new strict noise legislations come into action.</p><p>A new approach to handle the instability problems of planetary gears encountered in real industrial context is presented in this work. First, the dynamic response of a planetary gear failing to pass the noise regulations is theoretically investigated through an analytical model. The equations of motion were solved using the Spectral Iterative Method. The observed experimental results correlated well with those from the developed model. In order to limit the resonance phenomena, impacts of different macro and micro-geometry modifications were analytically investigated: quadratic teeth profile, different planets positioning, different number of teeth and number of planets. Optimum modifications were retrieved and are expected to be tested experimentally on a test bench and on the truck.</p><p>Finally, the analytical model’s limits and sensitivity to different parameters were investigated in order to certify its reliability, and suggestions for improvements were presented.</p>
----------------------------------------------------------------------
In diva2:737807   - correct as is
----------------------------------------------------------------------
In diva2:1232457 
abstract is: 
<p>Classical mechanics is the branch of physics concerned with describing the motion of bodies. The subject is based on three simple axioms relating forces and movement. These axioms were first postulated by Newton in the 17th century and are known as his three laws of motion.</p><p>Lagrangian mechanics is a restatement of the Newtonian formulation. It deals with energy quantities and paths-of-motion instead of forces. This often makes it simpler to use when working with non-trivial mechanical systems. In this thesis, we use the Lagrangian method to model two such systems; A rotating torus and a variant of the classical double pendulum.</p><p>It soon becomes clear that the complexity of these systems make them difficult to attack by hand. For this reason, we take a computer-based approach. We use a software-package called Sophia which is a plug-in to the computer algebra system Maple. Sofia was developed at the Department of Mechanics at KTH for the specific purpose of modeling mechanical problems using Lagrange’s method. We demonstrate that this method can be successfully applied to the analysis of motion of complex mechanical systems. The complete equations of motion are derived in a symbolic form and then integrated numerically. The motion of the system is finally visualized by means of 3D graphics software Blender.</p>

corrected abstract:
<p>Classical mechanics is the branch of physics concerned with describing the motion of bodies. The subject is based on three simple axioms relating forces and movement. These axioms were first postulated by Newton in the 17th century and are known as his three laws of motion.</p><p>Lagrangian mechanics is a restatement of the Newtonian formulation. It deals with energy quantities and paths-of-motion instead of forces. This often makes it simpler to use when working with non-trivial mechanical systems. In this thesis, we use the Lagrangian method to model two such systems; A rotating torus and a variant of the classical double pendulum.</p><p>It soon becomes clear that the complexity of these systems make them difficult to attack by hand. For this reason, we take a computer-based approach. We use a software-package called Sophia which is a plug-in to the computer algebra system Maple™. Sofia was developed at the Department of Mechanics at KTH for the specific purpose of modeling mechanical problems using Lagrange’s method. We demonstrate that this method can be successfully applied to the analysis of motion of complex mechanical systems. The complete equations of motion are derived in a symbolic form and then integrated numerically. The motion of the system is finally visualized by means of 3D graphics software Blender™.</p>

Note added the ™ markup
----------------------------------------------------------------------
In diva2:1186356   - correct as is
----------------------------------------------------------------------
In diva2:1349671   - correct as is
----------------------------------------------------------------------
In diva2:1210809 
abstract is: 
<p>The Tobii Pro Glasses 2 are used to record gaze data that is used for market research or scientific experiments. To make extraction of relevant statistics more efficient, the gaze points in the recorded video are mapped to a static snapshot with areas of interests (AOIs). The most important statistics revolve around fixations. A fixation is when a person is keeping his or her vision still for a short period of time. The method most used today is to manually map the gaze points. However, a faster method is automated mapping using the Real World Mapping (RWM) tool. In order to examine the reliability of RWM, the fixations from different recordings and projects were analyzed using Decision Trees. Further, a Random Forest (RF) model was constructed in order to predict if a gaze point was correctly or incorrectly mapped. It was shown that fixation classification on data from RWM performed significantly worse than when the same fixation classification on manually mapped data was run. It was shown that RWM works better when head movement is low and AOIs are set appropriately. This can guide researchers in set- ting up experiments, although major improvements of RWM is needed. The RF classifier showed promising results on several test sets for mapped gaze points. It also showed promising results for gaze points that were not mapped and were close in time to being mapped. In conclusion, the RF should replace current methods of estimating the quality of RWM gaze points. Gaze points that are classified as badly mapped can be manually remapped. If RWM fails to map large segments of gaze points to a snapshot, visually classifying these to be remapped is the preferred method.</p>

corrected abstract:
<p>The Tobii Pro Glasses 2 are used to record gaze data that is used for market research or scientific experiments. To make extraction of relevant statistics more efficient, the gaze points in the recorded video are mapped to a static snapshot with areas of interests (AOIs). The most important statistics revolve around fixations. A fixation is when a person is keeping his or her vision still for a short period of time. The method most used today is to manually map the gaze points. However, a faster method is automated mapping using the Real World Mapping (RWM) tool. In order to examine the reliability of RWM, the fixations from different recordings and projects were analyzed using Decision Trees. Further, a Random Forest (RF) model was constructed in order to predict if a gaze point was correctly or incorrectly mapped. It was shown that fixation classification on data from RWM performed significantly worse than when the same fixation classification on manually mapped data was run. It was shown that RWM works better when head movement is low and AOIs are set appropriately. This can guide researchers in setting up experiments, although major improvements of RWM is needed. The RF classifier showed promising results on several test sets for mapped gaze points. It also showed promising results for gaze points that were not mapped and were close in time to being mapped. In conclusion, the RF should replace current methods of estimating the quality of RWM gaze points. Gaze points that are classified as badly mapped can be manually remapped. If RWM fails to map large segments of gaze points to a snapshot, visually classifying these to be remapped is the preferred method.</p>


Note - eliminated an unnecessary hyphen
----------------------------------------------------------------------
In diva2:1347995 
abstract is: 
<p>In this thesis, the use of unsupervised and semi-supervised machine learning techniques was analyzed as potential tools for anomaly detection in the sensor network that the electrical system in a Scania truck is comprised of. The experimentation was designed to analyse the need for both point and contextual anomaly detection in this setting. For the point anomaly detection the method of Isolation Forest was experimented with and for contextual anomaly detection two different recurrent neural network architectures using Long Short Term Memory units was relied on. One model was simply a many to one regression model trained to predict a certain signal, while the other was an encoder-decoder network trained to reconstruct a sequence. Both models were trained in an semi-supervised manner, i.e. on data that only depicts normal behaviour, which theoretically should lead to a performance drop on abnormal sequences resulting in higher error terms. In both setting the parameters of a Gaussian distribution were estimated using these error terms which allowed for a convenient way of defining a threshold which would decide if the observation would be flagged as anomalous or not. Additional experimentation's using an exponential weighted moving average over a number of past observations to filter the signal was also conducted. The models performance on this particular task was very different but the regression model showed a lot of promise especially when combined with a filtering preprocessing step to reduce the noise in the data. However the model selection will always be governed by the nature the particular task at hand so the other methods might perform better in other settings.</p>

corrected abstract:
<p>In this thesis, unsupervised and semi-supervised machine learning techniques are analyzed as potential tools for anomaly detection in Scania truck sensor networks.  The thesis investigates the need for both point and contextual anomaly detection in this setting. For the point anomaly detection the method of Isolation forest was applied and for contextual anomaly detection two different recurrent neural network architectures using Long Short Term Memory units were used. One model was simply a many-to-one regression model trained to predict a certain signal, while the other was an encoder-decoder network trained to reconstruct a sequence. Both models were trained in an semi-supervised manner, i.e. on data that only depicts normal behaviour, which theoretically should lead to a performance drop on abnormal sequences resulting in higher error terms. In both settings the parameters of a Gaussian distribution were estimated using these error terms, which allowed for a convenient way of defining a threshold which would decide if the observation would be flagged as anomalous or not. Additional experiments using an exponential weighted moving average over a number of past observations to filter the signal was also conducted. The methods performance on this particular task were very different, but the regression model showed a lot of promise especially when combined with a filtering preprocessing step to reduce the noise in the data. However, the model selection will always be governed by the nature of the particular task at hand, so the other methods might perform better in other settings.</p>

Note there is a major difference between the DiVA and original abstract
----------------------------------------------------------------------
In diva2:1431644   - correct as is
----------------------------------------------------------------------
In diva2:1216876   - correct as is
----------------------------------------------------------------------
In diva2:1156318 - missing space in title:
"Ant Colony Algorithms andits applications to Autonomous Agents Systems"
==>
"Ant Colony Algorithms and its applications to Autonomous Agents Systems"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:233558
Note: no full text in DiVA

The abstract (currently) shown in DiVA does not match the txe that I earlier found for this DiVA_ID.
----------------------------------------------------------------------
In diva2:420111   - correct as is
----------------------------------------------------------------------
In diva2:911535   - correct as is
----------------------------------------------------------------------
In diva2:1320414   - correct as is
----------------------------------------------------------------------
In diva2:1212533   - correct as is
----------------------------------------------------------------------
In diva2:1212533 
abstract is: 
<p>This thesis is an implementation project of a portfolio optimization model, with the purpose of creating a decision support tool. It aims to provide quantitative input to the portfolio construction process at Handelsbanken Fonder, by applying Konno &amp; Yamazaki’s Mean Absolute Deviation method, with a Feinstein &amp; Thapa modification. Additionally, the Black-Litterman model is implemented to approximate the input of expected return. The linear optimization problem was then solved by the Simplex algorithm. The main deliverable is a model that can assist portfolio managers in making investment decisions. Back-testing of the model showed that it did not outperform the benchmark portfolios, which is likely a result of only allowing long positions in the model. Nevertheless, the model provides value by giving the user a second opinion on the efficient frontier, for any given investment decision.</p>

corrected abstract:
<p>This thesis is an implementation project of a portfolio optimization model, with the purpose of creating a decision support tool. It aims to provide quantitative input to the portfolio construction process at Handelsbanken Fonder, by applying Konno &amp; Yamazaki’s Mean Absolute Deviation method, with a Feinstein &amp; Thapa modification. Additionally, the Black-Litterman model is implemented to approximate the input of expected return. The linear optimization problem was then solved by the Simplex algorithm. The main deliverable is a model that can assist portfolio managers in making investment decisions. Back-testing of the model showed that it did not outperform the benchmark portfolios, which is likely a result of only allowing long positions in the model. Nevertheless, the model provides value by giving the user a second opinion on the efficient frontier, for any given investment decision.</p>
----------------------------------------------------------------------
In diva2:1298366   - correct as is
----------------------------------------------------------------------
In diva2:1442067 
abstract is: 
<p>Graph theory is a mathematical study of objects and their pairwise relations, known as nodes and edges respectively. The birth of graph theory is often considered to take place in 1736 when the Swiss mathematician Leonhard Euler tried to solve a routing problem involving seven bridges of Königsberg in Prussia. In more recent times, graph theory has caught the attention of companies from all types of industries due to its power of modelling and analysing exceptionally large networks.</p><p>This thesis investigates the usage of graph theory in the energy sector for a utility company, in particular Fortum whose activities consist of, but not limited to, production and distribution of electricity and heat. The output of the thesis is a wide overview of graph-theoretic concepts and their practical applications, as well as a study of a use-case where some concepts are put into deeper analysis. The chosen use-case within the scope of this thesis is feature selection - a process for reducing the number of features, also known as input variables, typically before a regression model is built to avoid overfitting and increase model interpretability.</p><p>Five graph-based feature selection methods with different points of view are studied. Experiments are conducted on realistic data sets with many features to verify the legitimacy of the methods. One of the data sets is owned by Fortum and used for forecasting the electricity price, among other important quantities. The obtained results look promising according to several evaluation metrics and can be used by Fortum as a support tool to develop prediction models. In general, a utility company can likely take advantage graph theory in many ways and add value to their business with enriched mathematical knowledge.</p>

corrected abstract:
<p>Graph theory is a mathematical study of objects and their pairwise relations, known as <em>nodes</em> and <em>edges</em> respectively. The birth of graph theory is often considered to take place in 1736 when the Swiss mathematician Leonhard Euler tried to solve a routing problem involving seven bridges of Königsberg in Prussia. In more recent times, graph theory has caught the attention of companies from all types of industries due to its power of modelling and analysing exceptionally large networks.</p><p>This thesis investigates the usage of graph theory in the energy sector for a utility company, in particular Fortum whose activities consist of, but not limited to, production and distribution of electricity and heat. The output of the thesis is a wide overview of graph-theoretic concepts and their practical applications, as well as a study of a use-case where some concepts are put into deeper analysis. The chosen use-case within the scope of this thesis is feature selection - a process for reducing the number of features, also known as input variables, typically before a regression model is built to avoid overfitting and increase model interpretability.</p><p>Five graph-based feature selection methods with different points of view are studied. Experiments are conducted on realistic data sets with many features to verify the legitimacy of the methods. One of the data sets is owned by Fortum and used for forecasting the electricity price, among other important quantities. The obtained results look promising according to several evaluation metrics and can be used by Fortum as a support tool to develop prediction models. In general, a utility company can likely take advantage graph theory in many ways and add value to their business with enriched mathematical knowledge.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1799803 
abstract is: 
<p>This thesis investigates applying the semiparametric method Peaks-Over-Threshold on data generated from a Monte Carlo simulation when estimating the financial risk measures Value-at-Risk and Expected Shortfall. The goal is to achieve a faster convergence than a Monte Carlo simulation when assessing extreme events that symbolise the worst outcomes of a financial portfolio. Achieving a faster convergence will enable a reduction of iterations in the Monte Carlo simulation, thus enabling a more efficient way of estimating risk measures for the portfolio manager. </p><p>The financial portfolio consists of US life insurance policies offered on the secondary market, gathered by our partner RessCapital. The method is evaluated on three different portfolios with different defining characteristics. </p><p>In Part I an analysis of selecting an optimal threshold is made. The accuracy and precision of Peaks-Over-Threshold is compared to the Monte Carlo simulation with 10,000 iterations, using a simulation of 100,000 iterations as the reference value. Depending on the risk measure and the percentile of interest, different optimal thresholds are selected. </p><p>Part II presents the result with the optimal thresholds from Part I. One can conclude that Peaks-Over-Threshold performed significantly better than a Monte Carlo simulation for Value-at-Risk with 10,000 iterations. The results for Expected Shortfall did not achieve a clear improvement in terms of precision, but it did show improvement in terms of accuracy. </p><p>Value-at-Risk and Expected Shortfall at the 99.5th percentile achieved a greater error reduction than at the 99th. The result therefore aligned well with theory, as the more "rare" event considered, the better the Peaks-Over-Threshold method performed. </p><p>In conclusion, the method of applying Peaks-Over-Threshold can be proven useful when looking to reduce the number of iterations since it do increase the convergence of a Monte Carlo simulation. The result is however dependent on the rarity of the event of interest, and the level of precision/accuracy required.</p>

corrected abstract:
<p>This thesis investigates applying the semiparametric method Peaks-Over-Threshold on data generated from a Monte Carlo simulation when estimating the financial risk measures Value-at-Risk and Expected Shortfall. The goal is to achieve a faster convergence than a Monte Carlo simulation when assessing extreme events that symbolise the worst outcomes of a financial portfolio. Achieving a faster convergence will enable a reduction of iterations in the Monte Carlo simulation, thus enabling a more efficient way of estimating risk measures for the portfolio manager.</p><p>The financial portfolio consists of US life insurance policies offered on the secondary market, gathered by our partner RessCapital. The method is evaluated on three different portfolios with different defining characteristics.</p><p>In Part I an analysis of selecting an optimal threshold is made. The accuracy and precision of Peaks-Over-Threshold is compared to the Monte Carlo simulation with 10,000 iterations, using a simulation of 100,000 iterations as the reference value. Depending on the risk measure and the percentile of interest, different optimal thresholds are selected.</p><p>Part II presents the result with the optimal thresholds from Part I. One can conclude that Peaks-Over-Threshold performed significantly better than a Monte Carlo simulation for Value-at-Risk with 10,000 iterations. The results for Expected Shortfall did not achieve a clear improvement in terms of precision, but it did show improvement in terms of accuracy.</p><p>Value-at-Risk and Expected Shortfall at the 99.5th percentile achieved a greater error reduction than at the 99th. The result therefore aligned well with theory, as the more ”rare” event considered, the better the Peaks-Over-Threshold method performed.</p><p>In conclusion, the method of applying Peaks-Over-Threshold can be proven useful when looking to reduce the number of iterations since it do increase the convergence of a Monte Carlo simulation. The result is however dependent on the rarity of the event of interest, and the level of precision/accuracy required.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph and fixing double quotes
----------------------------------------------------------------------
In diva2:1800537   - correct as is
----------------------------------------------------------------------
In diva2:1757027 
abstract is: 
<p>Investing in public real estate stocks can diversify a stock portfolio due to the nature of these companies. The industry is generally less sensitive to economic downturns and spikes in inflation are offset by increased real estate property and rent prices. Nevertheless, measures of the wider economy could be used as predictors of the real estate stock market. </p><p>This thesis attempts to model the Swedish real estate stock market with the index SX35PI (Stockholm Real Estate PI) using the fundamental economic factors and repo rate. Data was collected and formatted to a monthly interval for the period February 2012 to December 2021. This resulted in an exponential multiple regression model that used all the regressors that explained 95.7% of the variation in SX35PI, and an alternative autoregressive forecasting model that explained 82.3% of the variation in SX35PI.</p>

corrected abstract:
<p>Investing in public real estate stocks can diversify a stock portfolio due to the nature of these companies. The industry is generally less sensitive to economic downturns and spikes in inflation are offset by increased real estate property and rent prices. Nevertheless, measures of the wider economy could be used as predictors of the real estate stock market.</p><p>This thesis attempts to model the Swedish real estate stock market with the index SX35PI (Stockholm Real Estate PI) using the fundamental economic factors and repo rate. Data was collected and formatted to a monthly interval for the period February 2012 to December 2021. This resulted in an exponential multiple regression model that used all the regressors that explained 95.7% of the variation in SX35PI, and an alternative autoregressive forecasting model that explained 82.3% of the variation in SX35PI.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1871604   - correct as is
----------------------------------------------------------------------
In diva2:1851009   - correct as is
----------------------------------------------------------------------
In diva2:1450579   - correct as is
----------------------------------------------------------------------
In diva2:1440755 
abstract is: 
<p>This report sets out to implement and asses a one-dimensional loss model for centrifugal compressors. The loss model which has been evaluated is the model by Oh et al. (1997) [1]. The implementation was completed using iterative methods implemented in Matlab. It was shown that the performance prediction using the Oh et al method produced usable estimates in the higher rotational speed region used in this work, at design conditions. The pressure ratio estimates had a difference compared to the measured reference data of 5-10% while the isentropic efficiency had slightly higher differences. Furthermore, it was shown that the best estimates of the pressure ratio came from the lowest rotational speeds, and successively flattened out from the expected curvature as the speed increased. The isentropic efficiency did not have the same property, giving the best consistent estimate at a higher rotational speed of 143 kRPM. The conclusion which was drawn from this work was that the model by Oh et al. is a useful model for prediction of performance at design points in the lower RPM region, while requiring complementary calculations at off-design conditions at high RPM. It was also pointed out that there are several areas that require further work within performance prediction to make it more implementation-friendly.</p>

corrected abstract:
<p>This report sets out to implement and asses a one-dimensional loss model for centrifugal compressors. The loss model which has been evaluated is the model by Oh et al. (1997) [1]. The implementation was completed using iterative methods implemented in Matlab. It was shown that the performance prediction using the Oh et al method produced usable estimates in the higher rotational speed region used in this work, at design conditions. The pressure ratio estimates had a difference compared to the measured reference data of 5-10% while the isentropic efficiency had slightly higher differences. Furthermore, it was shown that the best estimates of the pressure ratio came from the lowest rotational speeds, and successively flattened out from the expected curvature as the speed increased. The isentropic efficiency did not have the same property, giving the best consistent estimate at a higher rotational speed of 143 kRPM. The conclusion which was drawn from this work was that the model by Oh et al. is a useful model for prediction of performance at design points in the lower RPM region, while requiring complementary calculations at off-design conditions at high RPM.</p><p>It was also pointed out that there are several areas that require further work within performance prediction to make it more implementation-friendly.</p>

Note added the missing paragraph break
----------------------------------------------------------------------
In diva2:1351695   - correct as is
----------------------------------------------------------------------
In diva2:1596353   - correct as is
----------------------------------------------------------------------
In diva2:1450000 
abstract is: 
<p>With the increasing demand for renewable energy sources, new systems are being developed to sustain future infrastructure, accommodating these new energy sources. One of the proposed solutions is to incorporate distributed energy resources to different households in order to provide local energy demands effectively. To enable large-scale integration of flexible energy resources, it is crucial to reduce end-user energy and power costs, which can be done by designing an optimization model objected to minimize the total electricity bill. In the scope of this Master thesis, the interest lies in investigating a control strategy to operate batteries, heat pumps, and other assets by producing the optimal setpoints using the designed optimization algorithm that takes, amongst others, market and weather data as well as customer behavior into account. The applied method for producing these setpoints is sensitivity analysis in linear programming, and heat pump scheduling has been investigated for performance evaluation of this technique.</p><p>The results show that applying this method produces the optimal setpoints over the non-controllable electricity load range by utilizing a low number of optimizations, i.e. high computation-efficiency, and high accuracy. Consequently, the controller by having the given setpoints as the input can easily adjust the heat pump output power based on the real-time non-controllable electricity load without creating any peaks and extra costs for the customers.</p>

corrected abstract:
<p>With increasing demand for renewable energy sources, new systems are being developed to sustain future infrastructure, accommodating these new energy sources. One of the proposed solutions is to incorporate distributed energy resources to different households in order to provide local energy demands effectively. To enable large-scale integration of flexible energy resources, it is crucial that to reduce end-user energy and power costs, which can be done by designing an optimization model objected to minimize the total electricity bill. In the scope of this Master thesis, the interest lies in investigating a control strategy to operate batteries, heat pumps, and other assets by producing the optimal setpoints using the designed optimization algorithm that takes, amongst others, market and weather data as well as customer behavior into account. The applied method for producing these setpoints is sensitivity analysis in linear programming, and heat pump scheduling has been investigated for performance evaluation of this technique.</p><p>The results show that applying this method produces the optimal setpoints over the non-controllable electricity load range by utilizing a low number of optimizations, i.e. high computation-efficiency, and high accuracy. Consequently, the controller by having the given setpoints as the input can easily adjust the heat pump output power based on the real-time non-controllable electricity load without creating any peaks and extra costs for the customers.</p>

Note small changes in the text
----------------------------------------------------------------------
In diva2:828122 
abstract is: 
<p>The electric vehicle (EV) fleet is expected to continue growing in the near future. The increasing electrification of the transportation sector is a promising solution to the global dependency on oil and is expected to drive investments in renewable and intermittent energy sources.</p><p>In order to facilitate the integration, utilize the potential of a growing EV fleet and to avoid unwanted effects on the electric grid, smart charging strategies will be necessary. The aspect of smart EV charging investigated in this work is the profitability of bidirectional energy transfer, often referred to as vehicle-to-grid (V2G), i.e. the possibility of using aggregated EV batteries as storage for energy which can be injected back to the grid.</p><p>A mixed integer linear problem (MILP) for minimizing energy costs and battery ageing costs for EV owners is formulated. The battery degradation due to charging and discharging is accounted for in the model used. A realistic case study of overnight charging of EVs in Sweden is constructed, and the results show that given current energy prices and battery costs, V2G is not profitable for EV owners. Further, a hypothetical case for lower battery costs is formulated to demonstrate the ability of our MILP model to treat a number of charging scenarios</p>

corrected abstract:
<p>The electric vehicle (EV) fleet is expected to continue growing in the near future. The increasing electrification of the transportation sector is a promising solution to the global dependency on oil and is expected to drive investments in renewable and intermittent energy sources.</p><p>In order to facilitate the integration, utilize the potential of a growing EV fleet and to avoid unwanted effects on the electric grid, smart charging strategies will be necessary.</p><p>The aspect of smart EV charging investigated in this work is the profitability of bidirectional energy transfer, often referred to as vehicle-to-grid (V2G), i.e. the possibility of using aggregated EV batteries as storage for energy which can be injected back to the grid.</p><p>A mixed integer linear problem (MILP) for minimizing energy costs and battery ageing costs for EV owners is formulated. The battery degradation due to charging and discharging is accounted for in the model used. A realistic case study of overnight charging of EVs in Sweden is constructed, and the results show that given current energy prices and battery costs, V2G is not profitable for EV owners. Further, a hypothetical case for lower battery costs is formulated to demonstrate the ability of our MILP model to treat a number of charging scenarios.</p>

Note added missing paragraph break and missing terminal period in last sentence
----------------------------------------------------------------------
In diva2:1149189 
abstract is: 
<p>In recent years, new regulations and stronger competition have further increased the importance of stochastic asset-liability management (ALM) models for life insurance firms. However, the often complex nature of life insurance contracts makes modeling to a challenging task, and insurance firms often struggle with models quickly becoming too complicated and inefficient. There is therefore an interest in investigating if, in fact, certain traits of financial ratios could be exposed through a more efficient model.</p><p>In this thesis, a discrete time stochastic model framework, for the simulation of simplified balance sheets of life insurance products, is proposed. The model is based on a two-factor stochastic capital market model, supports the most important product characteristics, and incorporates a reserve-dependent bonus declaration. Furthermore, a first approach to endogenously model customer transitions is proposed, where realized policy returns are used for assigning transition probabilities.</p><p>The model's sensitivity to different input parameters, and ability to capture the most important behaviour patterns, are demonstrated by the use of scenario and sensitivity analyses. Furthermore, based on the findings from these analyses, suggestions for improvements and further research are also presented.</p>

corrected abstract:
<p>In recent years, new regulations and stronger competition have further increased the importance of stochastic asset-liability management (ALM) models for life insurance firms. However, the often complex nature of life insurance contracts makes modeling to a challenging task, and insurance firms often struggle with models quickly becoming too complicated and inefficient. There is therefore an interest in investigating if, in fact, certain traits of financial ratios could be exposed through a more efficient model.</p><p>In this thesis, a discrete time stochastic model framework, for the simulation of simplified balance sheets of life insurance products, is proposed. The model is based on a two-factor stochastic capital market model, supports the most important product characteristics, and incorporates a reserve-dependent bonus declaration. Furthermore, a first approach to endogenously model customer transitions is proposed, where realized policy returns are used for assigning transition probabilities.</p><p>The model's sensitivity to different input parameters, and ability to capture the most important behaviour patterns, are demonstrated by the use of scenario and sensitivity analyses. Furthermore, based on the findings from these analyses, suggestions for improvements and further research are also presented.</p>
In diva2:1149189 
abstract is: 
<p>In recent years, new regulations and stronger competition have further increased the importance of stochastic asset-liability management (ALM) models for life insurance firms. However, the often complex nature of life insurance contracts makes modeling to a challenging task, and insurance firms often struggle with models quickly becoming too complicated and inefficient. There is therefore an interest in investigating if, in fact, certain traits of financial ratios could be exposed through a more efficient model.</p><p>In this thesis, a discrete time stochastic model framework, for the simulation of simplified balance sheets of life insurance products, is proposed. The model is based on a two-factor stochastic capital market model, supports the most important product characteristics, and incorporates a reserve-dependent bonus declaration. Furthermore, a first approach to endogenously model customer transitions is proposed, where realized policy returns are used for assigning transition probabilities.</p><p>The model's sensitivity to different input parameters, and ability to capture the most important behaviour patterns, are demonstrated by the use of scenario and sensitivity analyses. Furthermore, based on the findings from these analyses, suggestions for improvements and further research are also presented.</p>

corrected abstract:
<p>In recent years, new regulations and stronger competition have further increased the importance of stochastic asset-liability management (ALM) models for life insurance firms. However, the often complex nature of life insurance contracts makes modelling to a challenging task, and insurance firms often struggle with models quickly becoming too complicated and inefficient. There is therefore an interest in investigating if, in fact, certain traits of financial ratios could be exposed through a more efficient model.</p><p>In this thesis, a discrete time stochastic model framework, for the simulation of simplified balance sheets of life insurance products, is proposed. The model is based on a two-factor stochastic capital market model, supports the most important product characteristics, and incorporates a reserve-dependent bonus declaration. Furthermore, a first approach to endogenously model customer transitions is proposed, where realized policy returns are used for assigning transition probabilities.</p><p>The model's sensitivity to different input parameters, and ability to capture the most important behaviour patterns, are demonstrated by the use of scenario and sensitivity analyses. Furthermore, based on the findings from these analyses, suggestions for improvements and further research are also presented.</p>


Note - only one change "modeling" to "modelling" (as per the original)
-------------------------------------------------------------------------------
In diva2:1699780 
abstract is: 
<p>In this master thesis, different neural networks have investigated annotating objects in video streams with partially annotated data as input. Annotation in this thesis is referring to bounding boxes around the targeted objects. Two different methods have been used ROLO and GOTURN, object detection with tracking respective object tracking with pixels. The data set used for validation is surveillance footage consists of varying image resolution, image size and sequence length. Modifications of the original models have been executed to fit the test data. </p><p>Promising results for modified GOTURN were shown, where the partially annotated data was used as assistance in tracking. The model is robust and provides sufficiently accurate object detections for practical use. With the new model, human resources for image annotation can be reduced by at least half.</p>

corrected abstract:
<p>In this master thesis, different neural networks have investigated annotating objects in video streams with partially annotated data as input. Annotation in this thesis is referring to bounding boxes around the targeted objects. Two different methods have been used ROLO and GOTURN, object detection with tracking respective object tracking with pixels. The data set used for validation is surveillance footage consists of varying image resolution, image size and sequence length. Modifications of the original models have been executed to fit the test data.</p><p>Promising results for modified GOTURN were shown, where the partially annotated data was used as assistance in tracking. The model is robust and provides sufficiently accurate object detections for practical use. With the new model, human resources for image annotation can be reduced by at least half.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1149609   - correct as is
----------------------------------------------------------------------
In diva2:1341574 
abstract is: 
<p>We cover in this report the implementation of a reinforcement learning (RL) algorithm capable of learning how to play the game 'Breakout' on the Atari Learning Environment (ALE). The non-human player (agent) is given no prior information of the game and must learn from the same sensory input that a human would typically receive when playing the game. The aim is to reproduce previous results by optimizing the agent driven control of 'Breakout' so as to surpass a typical human score. To this end, the problem is formalized by modeling it as a Markov Decision Process. We apply the celebrated Deep Q-Learning algorithm with action masking to achieve an optimal strategy. We find our agent's average score to be just below the human benchmarks: achieving an average score of 20, approximately 65% of the human counterpart. We discuss a number of implementations that boosted agent performance, as well as further techniques that could lead to improvements in the future.</p>

corrected abstract:
<p>We cover in this report the implementation of a reinforcement learning (RL) algorithm capable of learning how to play the game <em>Breakout</em> on the Atari Learning Environment (ALE). The non-human player (agent) is given no prior information of the game and must learn from the same sensory input that a human would typically receive when playing the game.</p><p>The aim is to reproduce previous results by optimizing the agent driven control of <em>Breakout</em> so as to surpass a typical human score. To this end, the problem is formalized by modeling it as a Markov Decision Process. We apply the celebrated Deep Q-Learning algorithm with action masking to achieve an optimal strategy.</p><p>We find our agent's average score to be just below the human benchmarks: achieving an average score of 20, approximately 65% of the human counterpart. We discuss a number of implementations that boosted agent performance, as well as further techniques that could lead to improvements in the future.</p>

Note "Breakout" was not set in quotes but rather in italics. Also added the missing paragraph breaks
----------------------------------------------------------------------
In diva2:1499840 
abstract is: 
<p>The study of structure-borne vibrations caused by railway traffic is becoming increasingly relevant along with the expansions of cities, as building projects are forced towards more sensitive areas, e.g closer to railway tracks. With high enough vibration levels, there is a risk for human annoyance inside buildings. Knowledge of the attenuation of vibrations in the ground caused by railway traffic is in such cases of great importance. Where a railway switch is present, higher levels are likely to be generated as the train passes. In this study, measurements of vibrations from railway traffic as trains pass a railway switch was performed. Four accelerometers were placed at different distances away from the track to capture the attenuation. In the analysis of the measurements, the attenuation of train induced vibrations and the dominating response due to the presence of the railway switch is found. From the findings in the measurements, a model based on superpositioning of incoming waves from point sources is developed, that predicts the attenuation of vibrations over distance from a train passing over a railway switch.</p>

corrected abstract:
<p>The study of structure-borne vibrations caused by railway traffic is becoming increasingly relevant along with the expansions of cities, as building projects are forced towards more sensitive areas, e.g closer to railway tracks. With high enough vibration levels, there is a risk for human annoyance inside buildings. Knowledge of the attenuation of vibrations in the ground caused by railway traffic is in such cases of great importance. Where a railway switch is present, higher levels are likely to be generated as the train passes.</p><p>In this study, measurements of vibrations from railway traffic as trains pass a railway switch were performed. Four accelerometers were placed at different distances away from the track to capture the attenuation. In the analysis of the measurements, the attenuation of train induced vibrations and the dominating response due to the presence of the railway switch is found.</p><p>From the findings in the measurements, a model based on superpositioning of incoming waves from point sources is developed, that predicts the attenuation of vibrations over distance from a train passing over a railway switch.</p>

Note "was" to "were" and added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1653920 
abstract is: 
<p>This Master thesis report contains a complete attitude and orbit control analysis for solar sail spacecraft in the Circular-Restricted-3-Body-Reference frame. The overall goal of the project is to drive one solar sail from the edge of the Earth Sphere of Influence to a vicinity of the sub-Lagrange1 point. First, a trajectory control optimization has been performed using a direct transcription method to find an optimal control to guide the solar sail to a neighborhood of the sub-Lagrange 1 point. Then, the stability of the artificial equilibrium points in the Sun-Earth system has beeninvestigated. Two options are detailed in the following study: Halo orbits which correspond to periodic motion, and the implementation of a PD-controller on the solar sail normal vector to make any artificial equilibrium point asymptotically stable. An attitude control analysis study has been performed to estimate the needed torques for all the mission phases. This report is one of the first step for Space sunshade missions in the vicinity of the sub-Lagrange 1 point.</p>

corrected abstract:
<p>This Master thesis report contains a complete attitude and orbit control analysis for solar sail spacecraft in the Circular-Restricted-3-Body-Reference frame. The overall goal of the project is to drive one solar sail from the edge of the Earth Sphere of Influence to a vicinity of the sub-Lagrange 1 point. First, a trajectory control optimization has been performed using a direct transcription method to find an optimal control to guide the solar sail to a neighborhood of the sub-Lagrange 1 point. Then, the stability of the artificial equilibrium points in the Sun-Earth system has been investigated. Two options are detailed in the following study: Halo orbits which correspond to periodic motion, and the implementation of a PD-controller on the solar sail normal vector to make any artificial equilibrium point asymptotically stable. An attitude control analysis study has been performed to estimate the needed torques for all the mission phases. This report is one of the first step for Space sunshade missions in the vicinity of the sub-Lagrange 1 point.</p>

Note changes:
mc='beeninvestigated' c='been investigated'
"sub-Lagrange1" to "sub-Lagrange 1"
----------------------------------------------------------------------
In diva2:1227832   - correct as is
----------------------------------------------------------------------
In diva2:1816796 
abstract is: 
<p>This thesis deals with the scheduling problem for a constellation of Earth observation satellites, focusing on modelling the attitude dynamics to assess the tasking capabilities. A target selection algorithm is developed considering the time dependent manoeuvres between targets and the time-dependent value of the observed targets. Further, a closed-loop dynamics simulation is carried out to assess the agility of the 6U platform and verify the results of the algorithm. The work does not intend to present definitive numerical results, rather the goal is to develop a holistic framework that allows appraising the performance of a platform and the fulfilment of the mission objectives, aiming to maximise the collective value of the observed targets. Given the inputs in terms of platform, sensor, orbit and list of targets, this work serves to simulate the target selection and imaging at an arbitrary day and time for a chosen observation window.</p>

corrected abstract:
<p>This thesis deals with the scheduling problem for a constellation of Earth observation satellites, focusing on modelling the attitude dynamics to assess the tasking capabilities. A target selection algorithm is developed considering the time-dependent manoeuvres between targets and the time-dependent value of the observed targets. Further, a closed-loop dynamics simulation is carried out to assess the agility of the 6U platform and verify the results of the algorithm. The work does not intend to present definitive numerical results, rather the goal is to develop a holistic framework that allows appraising the performance of a platform and the fulfilment of the mission objectives, aiming to maximise the collective value of the observed targets. Given the inputs in terms of platform, sensor, orbit and list of targets, this work serves to simulate the target selection and imaging at an arbitrary day and time for a chosen observation window.</p>

Note added a hyphen in "time-dependent"
----------------------------------------------------------------------
In diva2:755070   - correct as is
----------------------------------------------------------------------
In diva2:753869   - correct as is
----------------------------------------------------------------------
In diva2:1215870 
abstract is: 
<p>In bridge design, a set of piles is referred to as a pile group. The design process of pile groups employed by many firms is currently manual, time consuming, and produces pile groups that are not robust against placement errors.</p><p>This thesis applies the metaheuristic method Genetic Algorithm to automate and improve the design of pile groups for bridge column foundations. A software is developed and improved by implementing modifications to the Genetic Algorithm. The algorithm is evaluated by the pile groups it produces, using the Monte Carlo method to simulate errors for the purpose of testing the robustness. The results are compared with designs provided by the consulting firm Tyrens AB.</p><p>The software is terminated manually, and generally takes less than half an hour to produce acceptable pile groups. The developed Genetic Algorithm Software produces pile groups that are more robust than the manually designed pile groups to which they are compared, using the Monte Carlo method. However, due to the visually disorganized designs, the pile groups produced by the algorithm may be di cult to get approved by Trafikverket. The software might require further modifications addressing this problem before it can be of practical use.</p>

corrected abstract:
<p>In bridge design, a set of piles is referred to as a pile group. The design process of pile groups employed by many firms is currently manual, time consuming, and produces pile groups that are not robust against placement errors.</p><p>This thesis applies the metaheuristic method Genetic Algorithm to automate and improve the design of pile groups for bridge column foundations. A software is developed and improved by implementing modifications to the Genetic Algorithm. The algorithm is evaluated by the pile groups it produces, using the Monte Carlo method to simulate errors for the purpose of testing the robustness. The results are compared with designs provided by the consulting firm Tyréns AB.</p><p>The software is terminated manually, and generally takes less than half an hour to produce acceptable pile groups. The developed Genetic Algorithm Software produces pile groups that are more robust than the manually designed pile groups to which they are compared, using the Monte Carlo method. However, due to the visually disorganized designs, the pile groups produced by the algorithm may be difficult to get approved by Trafikverket. The software might require further modifications addressing this problem before it can be of practical use.</p>

Note fixed the access in "Tyréns" and added the missing ligrature in "difficult"
----------------------------------------------------------------------
In diva2:1679036   - correct as is
----------------------------------------------------------------------
In diva2:1139765 
abstract is: 
<p>This paper is about automatizing parameter estimation of GARCH type conditional volatility models for the sake of using it in an automated risk monitoring system. Many challenges arise with this task such as guaranteeing convergence, being able to yield reasonable results regardless of the quality of the data, accuracy versus speed of the algorithm to name a few. These problems are investigated and a robust framework for an algorithm is proposed, containing dimension reducing and constraint relaxing parameter space transformations with robust initial values. The algorithm is implemented in java with two models, namely the GARCH and gjr-GARCH model. By using real market data, performance of the algorithm are tested with various in-sample and out-of-sample measures, including backtesting of the widely used risk measure Value-at-Risk. The empirical studies conclude that the more complex gjr-sGARCH model with the conditional student’s t distribution was found to yield the most accurate results. However for the purpose of this paper the GARCH orgjr-GARCH seems more appropriate.</p>

corrected abstract:
<p>This paper is about automatizing parameter estimation of GARCH type conditional volatility models for the sake of using it in an automated risk monitoring system. Many challenges arise with this task such as guaranteeing convergence, being able to yield reasonable results regardless of the quality of the data, accuracy versus speed of the algorithm to name a few. These problems are investigated and a robust framework for an algorithm is proposed, containing dimension reducing and constraint relaxing parameter space transformations with robust initial values. The algorithm is implemented in java with two models, namely the GARCH and gjr-GARCH model. By using real market data, performance of the algorithm are tested with various in-sample and out-of-sample measures, including backtesting of the widely used risk measure Value-at-Risk. The empirical studies conclude that the more complex gjr-sGARCH model with the conditional student’s t distribution was found to yield the most accurate results. However for the purpose of this paper the GARCH or gjr-GARCH seems more appropriate.</p>

Note spelling error:
mc='orgjr-GARCH' c='or gjr-GARCH'
----------------------------------------------------------------------
In diva2:1219103 
abstract is: 
<p> An important step towards making road transportation safer around the world is the development of autonomous vehicles. In this paper a controller for performing autonomous overtaking at highway speeds using Model Predictive Control (MPC) is designed. The MPC framework is designed and tested in a simulated environment in order to evaluate the performance of the controller. Four different MPC frameworks are developed for generating paths for autonomous overtaking and a Proportional Integral Derivative (PID) controller is formulated for a general comparison. The four MPC frameworks all plan trajectories by introducing constraints; they differ in the way they formulate said constraints. From the simulations we conclude that MPC is a better controller choice than PID for the application of controlling autonomous vehicles because of the usability of MPC, even though they might be equally fast. The benefits and drawbacks of the MPC implementations and their characteristics are discussed, and we conclude that the preferred implementation is a linear sloped edge dynamic constraint where a disallowed area around the leading vehicle is explicitly defined outside of the MPC framework.</p>

corrected abstract:
<p>An important step towards making road transportation safer around the world is the development of autonomous vehicles. In this paper a controller for performing autonomous overtaking at highway speeds using Model Predictive Control (MPC) is designed. The MPC framework is designed and tested in a simulated environment in order to evaluate the performance of the controller. Four different MPC frameworks are developed for generating paths for autonomous overtaking and a Proportional Integral Derivative (PID) controller is formulated for a general comparison. The four MPC frameworks all plan trajectories by introducing constraints; they differ in the way they formulate said constraints. From the simulations we conclude that MPC is a better controller choice than PID for the application of controlling autonomous vehicles because of the usability of MPC, even though they might be equally fast. The benefits and drawbacks of the MPC implementations and their characteristics are discussed, and we conclude that the preferred implementation is a linear sloped edge dynamic constraint where a disallowed area around the leading vehicle is explicitly defined outside of the MPC framework.</p>

Note simply eliminate the initial unnecessary space.
----------------------------------------------------------------------
In diva2:1441967 
abstract is: 
<p>The technical development has during the last few decades moved forward in a very high pace and modern technical solutions have become an even larger part of everyday life. Technical systems are becoming more and more advanced, and society is more influenced by autonomous solutions than before. The vehicle industry is no exception. One of the most important goals in development of autonomous vehicles is to reduce human errors, because most accidents are caused by humans. The objective in this thesis is to survey how far the development of autonomous vehicles have come and also in which areas this is happening. Based on a literature study of previous research on the subject, a judgement of future development in autonomous vehicles will be performed. The areas which are analyzed in this thesis are private cars, public transports, as well as warehouse and logistics. This thesis is first and foremost focused on how the situation in Sweden look. There are three major aspects of the development of autonomous vehicles that are analyzed namely, technical, legal and ethical. The analysis for future development is based on a SWOT-analysisto map out the different strengths, weaknesses, opportunities, and threats to this technology. The three aspects mentioned above constitutes the SWOT-analysis. This thesis shows that the development of autonomous vehicles is moving forward in all of the areas analyzed, where warehouse and logistics has the most developed vehicles. Public transport is also on its way to get implemented in real traffic, while private cars have a long way to go before commercial usage.</p>
mc='analysisto' c='analysis to'
mc='SWOT-analysisto' c='SWOT-analysis to'

partal corrected: diva2:1441967: <p>The technical development has during the last few decades moved forward in a very high pace and modern technical solutions have become an even larger part of everyday life. Technical systems are becoming more and more advanced, and society is more influenced by autonomous solutions than before. The vehicle industry is no exception. One of the most important goals in development of autonomous vehicles is to reduce human errors, because most accidents are caused by humans. The objective in this thesis is to survey how far the development of autonomous vehicles have come and also in which areas this is happening. Based on a literature study of previous research on the subject, a judgement of future development in autonomous vehicles will be performed. The areas which are analyzed in this thesis are private cars, public transports, as well as warehouse and logistics. This thesis is first and foremost focused on how the situation in Sweden look. There are three major aspects of the development of autonomous vehicles that are analyzed namely, technical, legal and ethical. The analysis for future development is based on a SWOT-analysis to map out the different strengths, weaknesses, opportunities, and threats to this technology. The three aspects mentioned above constitutes the SWOT-analysis. This thesis shows that the development of autonomous vehicles is moving forward in all of the areas analyzed, where warehouse and logistics has the most developed vehicles. Public transport is also on its way to get implemented in real traffic, while private cars have a long way to go before commercial usage.</p>

corrected abstract:
<p>The technical development has during the last few decades moved forward in a very high pace and modern technical solutions have become an even larger part of everyday life. Technical systems are becoming more and more advanced, and society is more influenced by autonomous solutions than before. The vehicle industry is no exception. One of the most important goals in development of autonomous vehicles is to reduce human errors, because most accidents are caused by humans.</p><p>The objective in this thesis is to survey how far the development of autonomous vehicles have come and also in which areas this is happening. Based on a literature study of previous research on the subject, a judgement of future development in autonomous vehicles will be performed. The areas which are analyzed in this thesis are private cars, public transports, as well as warehouse and logistics. This thesis is first and foremost focused on how the situation in Sweden look. There are three major aspects of the development of autonomous vehicles that are analyzed namely, technical, legal and ethical.</p><p>The analysis for future development is based on a SWOT-analysis to map out the different strengths, weaknesses, opportunities, and threats to this technology. The three aspects mentioned above constitutes the SWOT-analysis.</p><p>This thesis shows that the development of autonomous vehicles is moving forward in all of the areas analyzed, where warehouse and logistics has the most developed vehicles. Public transport is also on its way to get implemented in real traffic, while private cars have a long way to go before commercial usage.</p>

Note added the missigng paragraph breaks.
----------------------------------------------------------------------
In diva2:1583473 
abstract is: 
<p>Many satellites have an orbit of reference defined according to their mission. The satellites need therefore to navigate as close as possible to their reference orbit. However, due to external forces, the trajectory of a satellite is disturbed and actions need to be taken. For now, the trajectories of the satellites are monitored by the operations of satellites department which gives appropriate instructions of navigation to the satellites. These steps require a certain amount of time and involvement which could be used for other purposes. A solution could be to make the satellites autonomous. The satellites would take their own decisions depending on their trajectory. The navigation control would be therefore much more efficient, precise and quicker. Besides, the autonomous orbit control could be coupled with an avoidance collision risk management. The satellites would decide themselves if an avoidance maneuver needs to be considered. The alerts of collisions would be given by the ground segment. In order to advance in this progress, this internship enables to analyse the feasibility of the implementation of the two concepts by testing them on an experiments satellite. To do so, tests plans were defined, tests procedures were executed and post-treatment tools were developed for analysing the results of the tests. Critical computational cases were considered as well. The tests were executed in real operations conditions.</p>

corrected abstract:
<p>Many satellites have an orbit of reference defined according to their mission. The satellites need therefore to navigate as close as possible to their reference orbit. However, due to external forces, the trajectory of a satellite is disturbed and actions need to be taken. For now, the trajectories of the satellites are monitored by the operations of satellites department which gives appropriate instructions of navigation to the satellites. These steps require a certain amount of time and involvement which could be used for other purposes.</p><p>A solution could be to make the satellites autonomous. The satellites would take their own decisions depending on their trajectory. The navigation control would be therefore much more efficient, precise and quicker. Besides, the autonomous orbit control could be coupled with an avoidance collision risk management. The satellites would decide themselves if an avoidance maneuver needs to be considered. The alerts of collisions would be given by the ground segment. In order to advance in this progress, this internship enables to analyse the feasibility of the implementation of the two concepts by testing them on an experiments satellite.</p><p>To do so, tests plans were defined, tests procedures were executed and post-treatment tools were developed for analysing the results of the tests. Critical computational cases were considered as well. The tests were executed in real operations conditions.</p>
----------------------------------------------------------------------
In diva2:1442071 
abstract is: 
<p>We review recent research into trajectory planning for autonomous overtaking to understand existing challenges. Then, the recently developed framework Learning Model Predictive Control (LMPC) is presented as a suitable method to iteratively improve an overtaking manoeuvre each time it is performed. We present recent extensions to the LMPC framework to make it applicable to overtaking. Furthermore, we also present two alternative modelling approaches with the intention of reducing computational complexity of the optimization problems solved by the controller. All proposed frameworks are built from scratch in Python3 and simulated for evaluation purposes. Optimization problems are modelled and solved using the Gurobi 9.0 Python API gurobipy. The results show that LMPC can be successfully applied to the overtaking problem, with improved performance at each iteration. However, the first proposed alternative modelling approach does not improve computational times as was the intention. The second one does but fails in other areas.</p>

corrected abstract:
<p>We review recent research into trajectory planning for autonomous overtaking to understand existing challenges. Then, the recently developed framework <em>Learning Model Predictive Control</em> (LMPC) from [1] is presented as a suitable method to iteratively improve an overtaking manoeuvre each time it is performed. We present extensions to the LMPC framework from [2] and [3] to make it applicable to overtaking. Furthermore, we also present two alternative modelling approaches with the intention of reducing computational complexity of the optimization problems solved by the controller. All proposed frameworks are built from scratch in <span style="font-variant: small-caps;">Python3</span> and simulated for evaluation purposes. Optimization problems are modelled and solved using the <span style="font-variant: small-caps;">Gurobi 9.0</span> Python API <span style="font-variant: small-caps;">gurobipy</span>. The results show that LMPC can be successfully applied to the overtaking problem, with improved performance at each iteration. However, the first proposed alternative modelling approach does not improve computational times as was the intention. The second one does but fails in other areas.</p>

Note change in wording and added italics and small caps
----------------------------------------------------------------------
In diva2:1056983 
abstract is: 
<p>How to measure risk is an important question in finance and much work has been done on how to quantitatively measure risk. An important part of this measurement is evaluating the measurements against the outcomes a procedure known as backtesting. A common risk measure is Expected shortfall for which how to backtest has been debated. In this thesis we will compare four different proposed backtests and see how they perform in a realistic setting. The main finding in this thesis is that it is possible to find backtests that perform well but it is important to investigate them thoroughly as small errors in the model can lead to large errors in the outcome of the</p><p>backtest</p>

corrected abstract:
<p>How to measure risk is an important question in finance and much work has been done on how to quantitatively measure risk. An important part of this measurement is evaluating the measurements against the outcomes a procedure known as backtesting. A common risk measure is Expected shortfall for which how to backtest has been debated. In this thesis we will compare four different proposed backtests and see how they perform in a realistic setting. The main finding in this thesis is that it is possible to find backtests that perform well but it is important to investigate them thoroughly as small errors in the model can lead to large errors in the outcome of the >backtest</p>

Note remove the unnecessary paragraph break
----------------------------------------------------------------------
In diva2:848996   - correct as is
----------------------------------------------------------------------
In diva2:449167   - correct as is
----------------------------------------------------------------------
In diva2:1162961 
abstract is: 
<p>This report has been written during my internship/master thesis at Thales Alenia Space, Cannes, FRANCE. The subject of the thesis is ball bearing design, and is focused on the software RBSDyn. This software has been developed by CNES, the French Center for Space Studies, and is used to simulate bearings behaviors under various conditions. My mission was to verify, test and implement this software for the company. In order to do so, the first step was to understand the bearing theory, which is the first part of this report. The second step was to use the software and verify its results, which is presented in the second section. Eventually, the final goal of this internship was to create a sequence to help Thales</p><p>Alenia Space engineers to design and select bearings, using this software and an Excel tool that needed to be created. Note that for confidentiality reasons, the values and names used for internal TAS mechanisms have been removed of this document.</p>

corrected abstract:
<p>This report has been written during my internship/master thesis at Thales Alenia Space, Cannes, FRANCE. The subject of the thesis is ball bearing design, and is focused on the software RBSDyn. This software has been developed by CNES, the French Center for Space Studies, and is used to simulate bearings behaviors under various conditions. My mission was to verify, test and implement this software for the company. In order to do so, the first step was to understand the bearing theory, which is the first part of this report. The second step was to use the software and verify its results, which is presented in the second section. Eventually, the final goal of this internship was to create a sequence to help Thales Alenia Space engineers to design and select bearings, using this software and an Excel tool that needed to be created.</p><p>Note that for confidentiality reasons, the values and names used for internal TAS mechanisms have been removed of this document.</p>

Note removed unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1219078   - correct as is
----------------------------------------------------------------------
In diva2:1319947   - correct as is
----------------------------------------------------------------------
In diva2:818932 
abstract is: 
<p>This report presents a novel algorithm for hierarchical clustering called <em>Bayesian Sample Clustering </em>(BSC). BSC is a <em>single linkage </em>algorithm that uses data samples to produce a <em>predictive distribution </em>for each sample. The predictive distributions are compared using the <em>Chan-Darwiche distance</em>, a metric for finite probability distributions, to produce a hierarchy of samples. The implemented version of BSC is found at <em>https:</em><em>//</em><em>github.com</em><em>/</em><em>Skjulet</em><em>/</em><em>Bayesian Sample Clustering.</em></p><p><em> </em></p>

corrected abstract:
<p>This report presents a novel algorithm for hierarchical clustering called <em>Bayesian Sample Clustering</em> (BSC). BSC is a <em>single linkage</em> algorithm that uses data samples to produce a <em>predictive distribution</em> for each sample. The predictive distributions are compared using the <em>Chan-Darwiche distance</em>, a metric for finite probability distributions, to produce a hierarchy of samples. The implemented version of BSC is found at<br> <a href="https://github.com/Skjulet/Bayesian Sample Clustering">https://github.com/Skjulet/Bayesian Sample Clustering</a>.</p>

Note lots of changes
----------------------------------------------------------------------
In diva2:820529 
abstract is: 
<p>Under the Advanced Measurement Approach (AMA), banks must use four different sources of information to assess their operational risk capital requirement. The three main quantitative sources available to build the future loss distribution are internal loss data, external loss data and scenario analysis. The fourth source, business environment and internal control factors, is treated as an ex-post update to capital calculations and is not a subject of this thesis. Ap- proaches from Extreme Value Theory (EVT) have gained popularity in the area of operational risk in recent years, with its focus on the behaviour of processes at extreme levels making it a natural candidate for operational risk modelling. However, the adoption of EVT in operational risk modelling has encountered several obstacles with the main one being the scarcity of data leading to substantial statistical uncertainty for both parameter and capital estimates. This Master thesis evaluates Bayesian Inference approaches to extreme value estimation and implements a method to reduce these uncertainties. The results indicate that the Bayesian Inference approaches gives a significant reduction of the statistical uncertainties compared to more traditional estimators and also performs well when applied on real-world data sets. </p>

corrected abstract:
<p>Under the Advanced Measurement Approach (AMA), banks must use four different sources of information to assess their operational risk capital requirement. The three main quantitative sources available to build the future loss distribution are internal loss data, external loss data and scenario analysis. The fourth source, business environment and internal control factors, is treated as an ex-post update to capital calculations and is not a subject of this thesis. Approaches from Extreme Value Theory (EVT) have gained popularity in the area of operational risk in recent years, with its focus on the behaviour of processes at extreme levels making it a natural candidate for operational risk modelling. However, the adoption of EVT in operational risk modelling has encountered several obstacles with the main one being the scarcity of data leading to substantial statistical uncertainty for both parameter and capital estimates. This Master thesis evaluates Bayesian Inference approaches to extreme value estimation and implements a method to reduce these uncertainties. The results indicate that the Bayesian Inference approaches gives a significant reduction of the statistical uncertainties compared to more traditional estimators and also performs well when applied on real-world data sets.</p>

Note remove unnecessary hyphen
----------------------------------------------------------------------
In diva2:1320142 
abstract is: 
<p>Neural networks are powerful tools for modelling complex non-linear mappings, but they often suffer from overfitting and provide no measures of uncertainty in their predictions. Bayesian techniques are proposed as a remedy to these problems, as these both regularize and provide an inherent measure of uncertainty from their posterior predictive distributions. By quantifying predictive uncertainty, we attempt to improve a systematic trading strategy by scaling positions with uncertainty. Exact Bayesian inference is often impossible, and approximate techniques must be used. For this task, this thesis compares dropout, variational inference and Markov chain Monte Carlo. We find that dropout and variational inference provide powerful regularization techniques, but their predictive uncertainties cannot improve a systematic trading strategy. Markov chain Monte Carlo provides powerful regularization as well as promising estimates of predictive uncertainty that are able to improve a systematic trading strategy. However, Markov chain Monte Carlo suffers from an extreme computational cost in the high-dimensional setting of neural networks.</p>

corrected abstract:
<p>Neural networks are powerful tools for modelling complex non-linear mappings, but they often suffer from overfitting and provide no measures of uncertainty in their predictions. Bayesian techniques are proposed as a remedy to these problems, as these both regularize and provide an inherent measure of uncertainty from their posterior predictive distributions.</p><p>By quantifying predictive uncertainty, we attempt to improve a systematic trading strategy by scaling positions with uncertainty. Exact Bayesian inference is often impossible, and approximate techniques must be used. For this task, this thesis compares dropout, variational inference and Markov chain Monte Carlo. We find that dropout and variational inference provide powerful regularization techniques, but their predictive uncertainties cannot improve a systematic trading strategy. Markov chain Monte Carlo provides powerful regularization as well as promising estimates of predictive uncertainty that are able to improve a systematic trading strategy. However, Markov chain Monte Carlo suffers from an extreme computational cost in the high-dimensional setting of neural networks.</p>

Note add missing paragraph
----------------------------------------------------------------------
In diva2:1752040   - correct as is
----------------------------------------------------------------------
In diva2:1263411 
abstract is: 
<p>The lighter an object is the easier it is to send into space. This principle is what drives the never ending hunt for lighter structures in the space industry. One way to reduce weight is to replace existing materials with lighter ones. Polymer matrix composites are such materials, as their density is lower than both steel and aluminium. The company RUAG Space produces a payload separating system that operates by clamping the payload using a clamp band to the rocket and releasing the payload by releasing the tension in the band. The current band is made in aluminium but RUAG seeks to build them using carbon fiber reinforced epoxy instead. Earlier projects have shown that carbon fiber fulfills the basic requirements, but has insufficient bearing strength to handle the loads at the bolted joints to the release mechanism. Research suggests that making the individual layers of carbon fiber thinner will increase the bearing strength and so in this project test specimen have been manufactured using thick and thin carbon fiber layers. These specimen were then subjected to bearing loads and the response was observed. The result showed that the ultimate bearing strength only increased a small amount with thin plies, but the onset of damage came at 47% higher stress levels compared to thick plies, suggesting a more brittle behavior. Since the onset of damage is the most important factor for RUAG the use of thin plies produced very positive results and could be a viable solution to increase the bearing strength in the clamp band.</p>

corrected abstract:
<p>The lighter an object is the easier it is to send into space. This principle is what drives the never ending hunt for lighter structures in the space industry. One way to reduce weight is to replace existing materials with lighter ones. Polymer matrix composites are such materials, as their density is lower than both steel and aluminium. The company RUAG Space produces a payload separating system that operates by clamping the payload using a clamp band to the rocket and releasing the payload by releasing the tension in the band. The current band is made in aluminium but RUAG seeks to build them using carbon fiber reinforced epoxy instead. Earlier projects have shown that carbon fiber fulfills the basic requirements, but has insufficient bearing strength to handle the loads at the bolted joints to the release mechanism. Research suggests that making the individual layers of carbon fiber thinner will increase the bearing strength and so in this project test specimen have been manufactured using thick and thin carbon fiber layers. These specimen were then subjected to bearing loads and the response was observed. The result showed that the ultimate bearing strength only increased a small amount with thin plies, but the onset of damage came at 47% higher stress levels compared to thick plies, suggesting a more brittle behavior. Since the onset of damage is the most important factor for RUAG the use of thin plies produced very positive results and could be a viable solution to increase the bearing strength in the clamp band.</p>
----------------------------------------------------------------------
In diva2:1115832 
abstract is: 
<p>In this thesis various portfolio weighting strategies are tested. Their performance is determined by their average annual return, Sharpe ratio, tracking error, information ratio and annual standard deviation. The data used is provided by Öhman from Bloomberg and consists of monthly data between 1996-2016 of all stocks that were in the MSCI USA Index at any time between 2002-2016.For any given month we use the last five years of data as a basis for the analysis. Each time the MSCI USA Index changes portfolio constituents we update which constituents are in our portfolio.</p><p>The traditional weighting strategies used in this thesis are market capitalization, equal, risk-adjusted alpha, fundamental and minimum variance weighting. On top of that, the weighting strategies are used in a cluster framework where the clusters are constructed by using K-means clustering on the stocks each month. The clusters are assigned equal weight and then the traditional weighting strategies are applied within each cluster. Additionally, a GARCH-estimated covariance matrix of the clusters is used to determine the minimum variance optimized weights of the clusters where the constituents within each cluster are equally weighted.</p><p>We conclude in this thesis that the market capitalization weighting strategy is the one that earns the least of all traditional strategies. From the results we can conclude that there are weighting strategies with higher Sharpe ratio and lower standard deviation. The risk-adjusted alpha in a traditional framework performed best out of all strategies. All cluster weighting strategies with the exception of risk-adjusted alpha outperform their traditional counterpart in terms of return.</p>

corrected abstract:
<p>In this thesis various portfolio weighting strategies are tested. Their performance is determined by their average annual return, Sharpe ratio, tracking error, information ratio and annual standard deviation. The data used is provided by Öhman from Bloomberg and consists of monthly data between 1996-2016 of all stocks that were in the MSCI USA Index at any time between 2002-2016. For any given month we use the last five years of data as a basis for the analysis. Each time the MSCI USA Index changes portfolio constituents we update which constituents are in our portfolio.</p><p>The traditional weighting strategies used in this thesis are market capitalization, equal, risk-adjusted alpha, fundamental and minimum variance weighting. On top of that, the weighting strategies are used in a cluster framework where the clusters are constructed by using 𝐾-means clustering on the stocks each month. The clusters are assigned equal weight and then the traditional weighting strategies are applied within each cluster. Additionally, a GARCH-estimated covariance matrix of the clusters is used to determine the minimum variance optimized weights of the clusters where the constituents within each cluster are equally weighted.</p><p>We conclude in this thesis that the market capitalization weighting strategy is the one that earns the least of all traditional strategies. From the results we can conclude that there are weighting strategies with higher Sharpe ratio and lower standard deviation. The risk-adjusted alpha in a traditional framework performed best out of all strategies. All cluster weighting strategies with the exception of risk-adjusted alpha outperform their traditional counterpart in terms of return.</p>

Note replaced "K" with "𝐾"
----------------------------------------------------------------------
In diva2:1381279 
abstract is: 
<p>High-speed planing craft is designed to overcome conventional hull’s speed barrier associated with wave making resistance and high frictional forces. Despite being able to reach high speeds, some planing hull forms will develop large volumes of spray attached to the hull surface, which can account for a large proportion of the total resistance. In this study, an experimental evaluation of the novel spray deflector technology proposed by Petestep AB is carried out in model scale at the Davidson Laboratory towing tank. The spray deflectors are compared against a time-proven spray rails technology and bare hull configuration. A modular hull design was developed that allows for rapid conversion between the three hull configurations and for future modifications to the design. The calm water resistance tests have shown up to 9% resistance reduction for spray rails and up to 25.75% reduction for spray deflectors as compared to the bare hull configuration. The running position of the spray deflector configuration was affected by the selected deflector design and differed from the spray rail and bare hull configuration, making the direct comparison of the technologies inapplicable. The Irregular waves tests have shown that for the current deflector design, the significant accelerations are approximately the same for the spray rail and spray deflector configurations. Both the technologies have led to increased accelerations at the center of gravity as compared to the bare hull. The spray deflector configuration, however, experienced lower accelerations in the bow area. A number of improvements to the current model design were proposed for the next series of experiments.</p>

corrected abstract:
<p>High-speed planing craft is designed to overcome conventional hull’s speed barrier associated with wave making resistance and high frictional forces. Despite being able to reach high speeds, some planing hull forms will develop large volumes of spray attached to the hull surface, which can account for a large proportion of the total resistance. In this study, an experimental evaluation of the novel spray deflector technology proposed by Petestep AB is carried out in model scale at the Davidson Laboratory towing tank. The spray deflectors are compared against a time-proven spray rails technology and bare hull configuration. A modular hull design was developed that allows for rapid conversion between the three hull configurations and for future modifications to the design.</p><p>The calm water resistance tests have shown up to 9% resistance reduction for spray rails and up to 25.75% reduction for spray deflectors as compared to the bare hull configuration. The running position of the spray deflector configuration was affected by the selected deflector design and differed from the spray rail and bare hull configuration, making the direct comparison of the technologies inapplicable. The Irregular waves tests have shown that for the current deflector design, the significant accelerations are approximately the same for the spray rail and spray deflector configurations. Both the technologies have led to increased accelerations at the center of gravity as compared to the bare hull. The spray deflector configuration, however, experienced lower accelerations in the bow area. A number of improvements to the current model design were proposed for the next series of experiments.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1380187 
abstract is: 
<p>This report presents the bending modes study conducted on a heavy launcher. The controller of the launcher takes as inputs the attitude and attitude rate measurements given by the Inertial Measurement Unit (IMU). Since the bending modes generate measurement errors at the IMU location, the study of deformations due to these bending modes is critical to assess the stability of the launcher during the atmospheric flight phase. The goal of this master thesis project is to detect and then select the most excitable bending modes among the large number of modes provided by a detailed structural analysis of the launcher. Only these relevant modes will be later used to generate reduced dynamical models of the launcher in order to efficiently design an appropriate controller. Indeed, considering all the bending modes will dramatically increase the calculation time and will not significantly improve the representativeness of the model at the control law frequency range of interest. To reach this objective, an extended excitability (the maximum of the module of the transfer function between the effective deflection and the considered mode generalized coordinate transported at the IMU location) is defined and computed for each mode. A criterion has been implemented to choose only the relevant modes. The sensitivity study conducted during this master thesis project has shown that with around 20 modes over 200, one can reproduce the dynamic behavior of the complete system. </p>

corrected abstract:
<p>This report presents the bending modes study conducted on a heavy launcher. The controller of the launcher takes as inputs the attitude and attitude rate measurements given by the Inertial Measurement Unit (IMU). Since the bending modes generate measurement errors at the IMU location, the study of deformations due to these bending modes is critical to assess the stability of the launcher during the atmospheric flight phase. The goal of this master thesis project is to detect and then select the most excitable bending modes among the large number of modes provided by a detailed structural analysis of the launcher. Only these relevant modes will be later used to generate reduced dynamical models of the launcher in order to efficiently design an appropriate controller. Indeed, considering all the bending modes will dramatically increase the calculation time and will not significantly improve the representativeness of the model at the control law frequency range of interest. To reach this objective, an extended excitability (the maximum of the module of the transfer function between the effective deflection and the considered mode generalized coordinate transported at the IMU location) is defined and computed for each mode. A criterion has been implemented to choose only the relevant modes. The sensitivity study conducted during this master thesis project has shown that with around 20 modes over 200, one can reproduce the dynamic behavior of the complete system.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1701325 
abstract is: 
<p>Benford’s Law describes a profound behavior that the leading digits of many quantities arising from mathematics, physics, ﬁnance, and engineering exhibit. In this text we prove Benford’s Law for the absolute value of the characteristic polynomial det (U-λI) of the CUE(N) as N →∞.  Our analysis produces an integrable bound for the characteristic function of log | det (U - λI|.</p>

corrected abstract:
<p>Benford’s Law describes a profound behavior that the leading digits of many quantities arising from mathematics, physics, finance, and engineering exhibit. In this text we prove Benford’s Law for the absolute value of the characteristic polynomial det(𝑈-&#x1D706;𝐼) of the CUE(𝑁) as 𝑁 → ∞. Our analysis produces an integrable bound for the characteristic function of log|det(𝑈 - &#x1D706;𝐼)|.</p>

Note set equations using math symbols and added missing right parenthesis to last equation
----------------------------------------------------------------------
In diva2:813360 - original does not have an abstract in English
abstract is: 
<p>In a world with increasing energy consumption, and where a large part of the energy is based on non-environmental sources, renewable energy is a solution that could help to satisfy all needs. One kind of renewable energy is solar energy, but to obtain economic viability for solar plants in countries such as Sweden can be hard. This project is part of a larger project aiming to perform a cost-effectiveness analysis of an imaginary solar plant in Stockholm. The goal of this sub-project is to investigate how much electricity a solar plant at the Royal Tennis Hall in Stockholm can produce assuming that the plant is 100 % reliable. The sub-project also compares the potential revenue if the electricity is sold on the Nord Pool Spot with the potential savings from not having to buy the produced amount from electric companies. In the economic analysis, possibilities for hedging, uncertainties in future electricity market and various grants are studied. In order to estimate production and profit a literature study has been conducted. An overall cost benefit analysis has been performed together with the other sub-projects. The annual production of a plant with an installed capacity of 46.64 kW has been estimated to 68.6 MWh. If all the electricity produced is consumed instead of being sold, the electricity costs would be reduced by 57,150 SEK annually.</p>

corrected abstract:
<p>In a world with increasing energy consumption, and where a large part of the energy is based on non-environmental sources, renewable energy is a solution that could help to satisfy all needs. One kind of renewable energy is solar energy, but to obtain economic viability for solar plants in countries such as Sweden can be hard. This project is part of a larger project aiming to perform a cost-effectiveness analysis of an imaginary solar plant in Stockholm. The goal of this sub-project is to investigate how much electricity a solar plant at the Royal Tennis Hall in Stockholm can produce assuming that the plant is 100 % reliable. The sub-project also compares the potential revenue if the electricity is sold on the Nord Pool Spot with the potential savings from not having to buy the produced amount from electric companies. In the economic analysis, possibilities for hedging, uncertainties in future electricity market and various grants are studied. In order to estimate production and profit a literature study has been conducted. An overall cost benefit analysis has been performed together with the other sub-projects. The annual production of a plant with an installed capacity of 46.64 kW has been estimated to 68.6 MWh. If all the electricity produced is consumed instead of being sold, the electricity costs would be reduced by 57,150 SEK annually.</p>

Note that the English does not exactly match  translation of the Sweden:
----------------------------------------------------------------------
In diva2:612289 
abstract is: 
<p>This study treats the design of secondary structures for wheel-loaded decks. It concludes that significant savings in structural weight, overall cost and environmental impact can be obtained by an improved design. The rules of three classification societies are examined and their principle differences are discussed. Weight and cost optimal solutions of rule-based design are identified for a deck of a typical short-sea RoRo-vessel. The rule-optimal designs are assessed and further improved on the basis of FEcalculations and the economic and environmental benefits associated with the best solutions are approximated.</p>
mc='FEcalculations' c='FE calculations'

partal corrected: diva2:612289: <p>This study treats the design of secondary structures for wheel-loaded decks. It concludes that significant savings in structural weight, overall cost and environmental impact can be obtained by an improved design. The rules of three classification societies are examined and their principle differences are discussed. Weight and cost optimal solutions of rule-based design are identified for a deck of a typical short-sea RoRo-vessel. The rule-optimal designs are assessed and further improved on the basis of FE calculations and the economic and environmental benefits associated with the best solutions are approximated.</p>

corrected abstract:
<p>This study treats the design of secondary structures for wheel-loaded decks. It concludes that significant savings in structural weight, overall cost and environmental impact can be obtained by an improved design. The rules of three classification societies are examined and their principle differences are discussed. Weight and cost optimal solutions of rule-based design are identified for a deck of a typical short-sea RoRo-vessel. The rule-optimal designs are assessed and further improved on the basis of FE-calculations and the economic and environmental benefits associated with the best solutions are approximated.</p>

Note added hyphen to "FE-calculations"
----------------------------------------------------------------------
In diva2:1319939   - correct as is
----------------------------------------------------------------------
In diva2:813076 -- no English abstract in the full text
----------------------------------------------------------------------
In diva2:1374912   - correct as is
----------------------------------------------------------------------
In diva2:1057190 
abstract is: 
<p>Airplanes have a risk of encounter birds while flying, taking off or landing and to ensure a safe flight the engines have to sustain functionality after one or several bird strikes; one vital part in the engine is the first stage rotor and hence it has to withstand bird strikes. Commercial finite element codes with explicit time marching techniques are commonly used today but they can be time-consuming and therefore a faster analytical method is sought. An analytical method is suitable for comparison of rotor blades in an early design process.</p><p>The force during the bird strike was divided into two parts in accordance with a paper by Sinha et al. [1], a slicing force and a travelling force. Once the force was obtained it was transformed to a pressure over an area. The pressure and area was then used to perform a transient analysis in ANSYS. The analytical based results are then compared to results obtained in a simulation done in LS-DYNA. The force exerted on a rotor blade, analytical and in LS-DYNA, shows good agreement with respect to maximum force. Comparing the displacement of the rotor blade in ANSYS and LS-DYNA shows that the magnitude agrees well but the phase does not agree as well.</p><p>It is possible to make automated scripts with a minimum of input data required to perform the analysis. The slicing force shows a good estimate of the maximum force exerted on a single blade during a bird strike. The impulse of the slicing force can also be seen as a lower limit of the total impulse felt by a single blade. The travelling force based on the paper by Sinha et al. was found to have some insufficiency and the recommendation is therefore to exclude the travelling force, in current state, from the analysis.</p>

corrected abstract:
<p>Airplanes have a risk of encounter birds while flying, taking off or landing and to ensure a safe flight the engines have to sustain functionality after one or several bird strikes; one vital part in the engine is the first stage rotor and hence it has to withstand bird strikes. Commercial finite element codes with explicit time marching techniques are commonly used today but they can be time-consuming and therefore a faster analytical method is sought. An analytical method is suitable for comparison of rotor blades in an early design process.</p><p>The force during the bird strike was divided into two parts in accordance with a paper by Sinha et al. [1], a slicing force and a travelling force. Once the force was obtained it was transformed to a pressure over an area. The pressure and area was then used to perform a transient analysis in ANSYS. The analytical based results are then compared to results obtained in a simulation done in LS-DYNA.</p><p>The force exerted on a rotor blade, analytical and in LS-DYNA, shows good agreement with respect to maximum force. Comparing the displacement of the rotor blade in ANSYS and LS-DYNA shows that the magnitude agrees well but the phase does not agree as well.</p><p>It is possible to make automated scripts with a minimum of input data required to perform the analysis. The slicing force shows a good estimate of the maximum force exerted on a single blade during a bird strike. The impulse of the slicing force can also be seen as a lower limit of the total impulse felt by a single blade. The travelling force based on the paper by Sinha et al. was found to have some insufficiency and the recommendation is therefore to exclude the travelling force, in current state, from the analysis.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1777333   - correct as is
----------------------------------------------------------------------
In diva2:558015 
abstract is: 
<p>We study blow-ups and their relation to orders of vanishing in algebraic geometry. In particular, we study the exceptional divisor and the strict transform of a blow-up. We use the order of vanishing to measure the severity of singularities, and show that if we blow up a closed point on a hypersurface we obtain points of equal order above it.</p>

corrected abstract:
<p>We study blow-ups and their relation to orders of vanishing in algebraic geometry. In particular, we study the exceptional divisor and the strict transform of a blow-up. We use the order of vanishing to measure the severity of singularities, and show that if we blow up a closed point on a hypersurface we obtain points of equal or lower order above it.</p>

Note added missing words "or lower"
----------------------------------------------------------------------
In diva2:1630882   - correct as is
----------------------------------------------------------------------
In diva2:1680272   - correct as is
----------------------------------------------------------------------
In diva2:1776722   - correct as is
----------------------------------------------------------------------
In diva2:1145353 
abstract is: 
<p>Using a vessel for public transport can possibly save large amounts of time in a city as Stockholm. The transport is easy during the period of the year where there is no ice cover on the waters, however during the time when there is ice, the vessels used face more extreme conditions. Swedish Steel Yachts (SSY) now wants to have a design for their “Shuttle Ferry Concept” intended for operation all year round. SSY has developed a special way of designing a ship’s hull structure, using this design together with the super duplex stainless steel alloy, SAF2507, SSY hopes to revolutionize the ship building industry. The aim of this thesis is to deliver a bow design that is able to combine operation in brash ice with good performance in open water using the special SSY design together with the super duplex stainless steel.</p><p>This thesis presents to you basic knowledge regarding operation in ice, ice theory, the SSY design concept more in detail and finally a design development of a suitable structure. The results from the thesis are shown, mainly as preferred outer geometry and expected load cases from the ice in the operational area.</p>

corrected abstract:
<p>Using a vessel for public transport can possibly save large amounts of time in a city as Stockholm. The transport is easy during the period of the year where there is no ice cover on the waters, however during the time when there is ice, the vessels used face more extreme conditions. Swedish Steel Yachts (SSY) now wants to have a design for their “Shuttle Ferry Concept” intended for operation all year round.</p><p>SSY has developed a special way of designing a ship’s hull structure, using this design together with the super duplex stainless steel alloy, SAF2507, SSY hopes to revolutionize the ship building industry. The aim of this thesis is to deliver a bow design that is able to combine operation in brash ice with good performance in open water using the special SSY design together with the super duplex stainless steel.</p><p>This thesis presents to you basic knowledge regarding operation in ice, ice theory, the SSY design concept more in detail and finally a design development of a suitable structure.</p><p>The results from the thesis are shown, mainly as preferred outer geometry and expected load cases from the ice in the operational area.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1229270 
abstract is: 
<p>When leaving classical physics and entering the realm of quantum physics, there are many new concepts being introduced. One of the most fundamental ideas in quantum mechanics is that particles no longer have exact known positions, but instead expected values and prob- abilities. This leads to the phenomena of truly identical particles, since they no longer can be distinguished simply by their positions. An important property differentiating different kinds of particles is how a system behaves when two such identical particles are exchanged. Historically, this divided particles into bosons and fermions, corresponding to symmetry and antisymmetry under an exchange.</p><p>However, in two dimensions a new type of particle appears. These particles are called anyons, and behave differently when particles are exchanged. Anyons can be further divided into abelian and non-abelian anyons, of which this thesis will focus on the latter. The ex- changes can then be represented by the fundamental group of the configuration space of the particles, and in two dimensions this fundamental group is the braid group. Using rotors from a Clifford algebra and studying excitations of Majorana fermions, this thesis will show a way to calculate the exchange matrices of non-abelian anyons, and their corresponding eigenvalues. Furthermore, suggestions on a generalization of this framework along with areas where it can be applied are given.</p>

corrected abstract:
<p>When leaving classical physics and entering the realm of quantum physics, there are many new concepts being introduced. One of the most fundamental ideas in quantum mechanics is that particles no longer have exact known positions, but instead expected values and probabilities. This leads to the phenomena of truly identical particles, since they no longer can be distinguished simply by their positions. An important property differentiating different kinds of particles is how a system behaves when two such identical particles are exchanged. Historically, this divided particles into bosons and fermions, corresponding to symmetry and antisymmetry under an exchange.</p><p>However, in two dimensions a new type of particle appears. These particles are called anyons, and behave differently when particles are exchanged. Anyons can be further divided into abelian and non-abelian anyons, of which this thesis will focus on the latter. The exchanges can then be represented by the fundamental group of the configuration space of the particles, and in two dimensions this fundamental group is the braid group. Using rotors from a Clifford algebra and studying excitations of Majorana fermions, this thesis will show a way to calculate the exchange matrices of non-abelian anyons, and their corresponding eigenvalues. Furthermore, suggestions on a generalization of this framework along with areas where it can be applied are given.</p>

Note removed unnecessary hyphens
----------------------------------------------------------------------
In diva2:1695468 
abstract is: 
<p>Technological advancements have opened up the possibility of digitizing the pathological landscape, enabling deep learning-based methods to analyze digitized tissue samples, i.e., whole slide images (WSIs). Attention has recently shifted toward modeling WSIs as graphs since graph representations can capture dynamic relationships. This thesis investigates different graph construction techniques in conjunction with graph-based deep learning to classify WSIs as breast cancer histological grade 1 versus histological grade 3. To that extent, multiple graph representation techniques and two graph convolutional networks, GCN and GraphSAGE, were utilized. Finally, by evaluating the proposed models on an external test set originating from a separate cohort, it is clear that both models have the capacity for binary histological grading, yielding AUC scores of 0.791 (95% CI 0.756 − 0.825) and 0.838 (95% CI 0.808 − 0.869) for the GCN and GraphSAGE models. Modeling WSIs as graphs is an exciting and emerging field; however, further work is needed to evaluate alternative graph representation techniques and graph convolutional networks.</p>

corrected abstract:
<p>Technological advancements have opened up the possibility of digitizing the pathological landscape, enabling deep learning-based methods to analyze digitized tissue samples, i.e., whole slide images (WSIs). Attention has recently shifted toward modeling WSIs as graphs since graph representations can capture dynamic relationships. This thesis investigates different graph construction techniques in conjunction with graph-based deep learning to classify WSIs as breast cancer histological grade 1 versus histological grade 3. To that extent, multiple graph representation techniques and two graph convolutional networks, GCN and GraphSAGE, were utilized. Finally, by evaluating the proposed models on an external test set originating from a separate cohort, it is clear that both models have the capacity for binary histological grading, yielding AUC scores of 0.791 (95% CI 0.756 &ndash; 0.825) and 0.838 (95% CI 0.808 &ndash; 0.869) for the GCN and GraphSAGE models. Modeling WSIs as graphs is an exciting and emerging field; however, further work is needed to evaluate alternative graph representation techniques and graph convolutional networks.</p>

Note use an en dash  for the ranges
----------------------------------------------------------------------
In diva2:1880371   - correct as is
----------------------------------------------------------------------
In diva2:878311   - correct as is
----------------------------------------------------------------------
In diva2:1115298   - correct as is
----------------------------------------------------------------------
In diva2:1380062 
abstract is: 
<p>Since the world’s first fixed-wing scheduled aircraft took-off in 1914, with the development on commercial aircraft, the aviation industry has improved constantly in the following 104 years [1]. In 2017, over 4.1 billion of passengers were carried by about 36.8 million of flights by the world’s airlines. Statistic number also shows that about 2% of human-induced carbon dioxide emission should be responsible by the aviation industry [2].To protect the environment and reduce carbon dioxide emission, one important way is to reduce jet fuel consumption. Aircraft manufacturers has already employed many fuel saving methods such as improving aircraft aerodynamics and engine efficiency, and apply composite materials to reduce aircraft weight in recent years. For airlines, a suitable and economical flight plan is helpful to reduce fuel consumption. However, in addition to fuel consumption, time is another equally important factor for airlines at the same time.This thesis starts from the flight management point of view, based on dynamic programming method, establish a numerical simulation to calculate the most optimal vertical flight trajectory under ATC (Air Traffic Control) constrains and up-to-date high-resolution weather information.</p>

corrected abstract:
<p>Since the world’s first fixed-wing scheduled aircraft took-off in 1914, with the development on commercial aircraft, the aviation industry has improved constantly in the following 104 years [1]. In 2017, over 4.1 billion of passengers were carried by about 36.8 million of flights by the world’s airlines. Statistic number also shows that about 2% of human-induced carbon dioxide emission should be responsible by the aviation industry [2].</p><p>To protect the environment and reduce carbon dioxide emission, one important way is to reduce jet fuel consumption. Aircraft manufacturers has already employed many fuel saving methods such as improving aircraft aerodynamics and engine efficiency, and apply composite materials to reduce aircraft weight in recent years. For airlines, a suitable and economical flight plan is helpful to reduce fuel consumption. However, in addition to fuel consumption, time is another equally important factor for airlines at the same time.</p><p>This thesis starts from the flight management point of view, based on dynamic programming method, establish a numerical simulation to calculate the most optimal vertical flight trajectory under ATC (Air Traffic Control) constrains and up-to-date high-resolution weather information.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1147858   - correct as is
----------------------------------------------------------------------
In diva2:1334688 
abstract is: 
<p>During the last three years the Swedish stock market has showed a strong upwards movement from the lows of 2016. At the same time the IPO activity has been large and a lot of the offerings have had a positive return during the first day of trading in the market.</p><p>The goal of this study is to analyze if there is any particular IPO specific data that has a correlation with the first day return and if it can be used to predict the first day return for future IPO’s. If any regressors were shown to have correlation with the first day return, the goal is also to find a subset of regressors with even higher predictability. Then to classify which regressors show the highest correlation with a large positive return. The method which has been used is a multiple linear regression with IPO-data from the period 2017-2018.</p><p>The results from the study imply that none of the chosen regressors show any significant correlation with the first day return. It is a complicated process which might be difficult to simplify and quantify into a regression model, but further studies are needed to draw a conclusion if there are any other qualitative factors which correlate with the first day return.</p>

corrected abstract:
<p>During the last three years the Swedish stock market has showed a strong upwards movement from the lows of 2016. At the same time the IPO activity has been large and a lot of the offerings have had a positive return during the first day of trading in the market.</p><p>The goal of this study is to analyze if there is any particular IPO specific data that has a correlation with the first day return and if it can be used to predict the first day return for future IPO’s. If any regressors were shown to have any correlation with the first day return, the goal is also to find a subset of regressors with even higher predictability. Then to classify which regressors show the highest correlation with a large positive return. The method which has been used is a multiple linear regression with IPO-data from the period 2017-2018.</p><p>The results from the study imply that none of the chosen regressors show any significant correlation with the first day return. It is a complex process which might be difficult to simplify and quantify into a regression model, hence further studies are needed to draw a conclusion if there are any other qualitative factors which correlate with the first day return.</p>

Note corrected the text to match that in the original
----------------------------------------------------------------------
In diva2:1334677 - full text document is scanned
abstract is: 
<p>In this thesis a Markov chain model, which can be used for analysing students’ performance and their academic progress, is developed. Being able to evaluate students progress is useful for any educational system. It gives a better understanding of how students resonates and it can be used as support for important decisions and planning. Such a tool can be helpful for managers of the educational institution to establish a more optimal educational policy, which ensures better position in the educational market. To show that it is reasonable to use a Markov chain model for this purpose, a test for how well data fits such a model is created and used. The test shows that we cannot reject the hypothesis that the data can be fitted to a Markov chain model.</p>

corrected abstract:
<p>In this thesis a Markov chain model, which can be used for analysing students’ performance and their academic progress, is developed. Being able to evaluate students progress is useful for any educational system. It gives a better understanding of how students resonates and it can be used as support for important decisions and planning. Such a tool can be helpful for managers of the educational institution to establish a more optimal educational policy, which ensures better position in the educational market.</p><p>To show that it is reasonable to use a Markov chain model for this purpose, a test for how well data fits such a model is created and used. The test shows that we cannot reject the hypothesis that the data can be fitted to a Markov chain model.</p><p>The data used for the thesis contains information about 22551 students from LTH between the years 1993 &ndash; 2016 from 15 different programs, i.e all master of engineering programs offered.</p><p>The thesis will also contain an Industrial Engineering and Management part which looks into how management in educational organisations matter for educational performance.</p>

Note added missing final two paragraphs
----------------------------------------------------------------------
In diva2:1879553 - the English abstract - is actually in Swedish; there is no English abstract in the full texxt
The English title is present in DiVA, but not the Swedish title that is in the report.

Note as this thesis is from 2024 - it should have both an English and Swedish abstract in DiVA
----------------------------------------------------------------------
In diva2:605178 
abstract is: 
<p>The project has as task, if possible, to predict the stock market by assuming that it behaves like a Markov chain. To accomplish this, Markov chains of order one, two and three are used. All the observed stocks are considered as up- or downgoing at closing. This leaves the Markov chain with two possible states. Both daily and weekly reports occur. A program is constructed to be able to generate hints on how the stock price is going to change in the future. The program which does these calculations also tests itself to see the proportion of correct hints, i.e. how efficient the program is. The efficiency is measured in two made-up constants, the</p><p><em>GJ </em>constant and the <em>J </em>constant. The program in use is Matlab.</p><p>Keywords: Markov chain, higher order, two states, self-test, Matlab.</p>
mc='downgoing' c='down going'


corrected abstract:
<p>The project has as task, if possible, to predict the stock market by assuming that it behaves like a Markov chain. To accomplish this, Markov chains of order one, two and three are used. All the observed stocks are considered as up- or downgoing at closing. This leaves the Markov chain with two possible states. Both daily and weekly reports occur. A program is constructed to be able to generate hints on how the stock price is going to change in the future. The program which does these calculations also tests itself to see the proportion of correct hints, i.e. how efficient the program is. The efficiency is measured in two made-up constants, the 𝐺𝐽 constant and the 𝐽 constant. The program in use is Matlab.</p>

Note - removed keywords from abstract; added the math italic characters for the constants
----------------------------------------------------------------------
In diva2:1827834 
abstract is: 
<p>Åre ski resort is the largest and most renowned ski resort in Sweden, offering excellent skiing opportunities, restaurants, and nightlife in a prime location. Meanwhile, it is often subject to heavy traffic during the peak season and has earned a bad reputation for struggling with long lift queues. To address the issue, this paper aimed at analyzing the current capacity of the ski resort with the purpose of identifying areas and cost-efficient measures for improvement. It was done by modeling the ski system as a Jackson Network based on queuing theory, with relevant parameters extracted from actual skier data provided by the operating company Skistar. Several models were constructed to capture varying skiing patterns throughout the day and under different weather conditions. The models suggested that the lift queues first start to form at the lifts VM 8:an, Sadelexpressen, and Bräckeliften when the number of skiers in the system ranges from 3,700 to 6,200. Recommendations were then proposed to Skistar on how to resolve the identified bottlenecks and increase the resort’s capacity to a range of 6,000 to 8,400 skiers. Lastly, the models estimated that the resort could </p>

corrected abstract:
<p>Åre ski resort is the largest and most renowned ski resort in Sweden, offering excellent skiing opportunities, restaurants, and nightlife in a prime location. Meanwhile, it is often subject to heavy traffic during the peak season and has earned a bad reputation for struggling with long lift queues. To address the issue, this paper aimed at analyzing the current capacity of the ski resort with the purpose of identifying areas and cost-efficient measures for improvement. It was done by modeling the ski system as a Jackson Network based on queuing theory, with relevant parameters extracted from actual skier data provided by the operating company Skistar. Several models were constructed to capture varying skiing patterns throughout the day and under different weather conditions. The models suggested that the lift queues first start to form at the lifts VM 8:an, Sadelexpressen, and Bräckeliften when the number of skiers in the system range from 3,700 to 6,200. Recommendations were then proposed to Skistar on how to resolve the identified bottlenecks and increase the resort’s capacity to a range of 6,000 to 8,400 skiers. Lastly, the models estimated that the resort could reach a maximum capacity of 14,000 skiers by optimally utilizing all of its lifts.</p>

Note added missing text and fixed "ranges" to "range"
----------------------------------------------------------------------
In diva2:1645391 
abstract is: 
<p>The automotive and heavy-duty trucking industries are heading towards research and development of alternative powertrain solutions to meet the United Nations sustainability goals and cleaner solutions to aid climate change actions. This thesis project aligns with the vision of finding greener and sustainable modes of transport in the heavy long haulage trucking industry. This project aims to find and develop a method for creating drive cycles, getting the vehicular power requirements to drive on these selected routes and finally calculating the TCO of a vehicle.</p><p>The scripts for these mentioned steps are developed in MATLAB. The approach used in this work could help both the vehicle manufacturer and the vehicle operator to predict or cater to upcoming customer demand on, in our case, routes pan EU, to receive information about energy, power and vehicular configuration needed to fulfil the mission, and also, optimize the powertrain configuration in collaboration with a parallel thesis work done here at Scania, and finally calculate a somewhat simplified TCO of the vehicle. </p><p>In this work, two different driving conditions has been used; summer or winter, and two different payload conditions, as well as two types of vehicle powertrains; FCEV and BEV. Finally, a comparison regarding TCO for FCEV and BEV has been done.</p>

corrected abstract:
<p>The automotive and heavy-duty trucking industries are heading towards research and development of alternative powertrain solutions to meet the United Nations sustainability goals and cleaner solutions to aid climate change actions. This thesis project aligns with the vision of finding greener and sustainable modes of transport in the heavy long haulage trucking industry. This project aims to find and develop a method for creating drive cycles, getting the vehicular power requirements to drive on these selected routes and finally calculating the Total Cost of Ownership (TCO) of a vehicle.</p><p>The scripts for these mentioned steps are developed in MATLAB. The approach used in this work could help both the vehicle manufacturer and the vehicle operator to predict or cater to upcoming customer demand on, in our case, routes pan EU, to receive information about energy, power and vehicular configuration needed to fulfil the mission, and also, optimize the powertrain configuration in collaboration with a parallel thesis work done here at Scania [15], and finally calculate a somewhat simplified TCO of the vehicle.</p><p>In this work, two different driving conditions has been used; summer or winter, and two different payload conditions, as well as two types of vehicle powertrains; Fuel Cell Electric Vehicle (FCEV) and Battery Electric Vehicle (BEV). Finally, a comparison regarding TCO for FCEV and BEV has been done.</p>

Note added the missing text
----------------------------------------------------------------------
In diva2:1431032   - correct as is
----------------------------------------------------------------------
In diva2:1663244   - correct as is
----------------------------------------------------------------------
In diva2:1663200 
abstract is: 
<p>Despite having a philosophical grounding from empiricism that spans some centuries, the algorithmization of causal discovery started only a few decades ago. This formalization of studying causal relationships relies on connections between graphs and probability distributions. In this setting, the task of causal discovery is to recover the graph that best describes the causal structure based on the available data. A particular class of causal discovery algorithms, called constraint-based methods rely on Directed Acyclic Graphs (DAGs) as an encoding of Conditional Independence (CI) relations that carry some level of causal information. However, a CI relation such as X and Y being independent conditioned on Z assumes the independence holds for all possible values Z can take, which can tend to be unrealistic in practice where causal relations are often context-specific. In this thesis we aim to develop constraint-based algorithms to learn causal structure from Context-Specific Independence (CSI) relations within the discrete setting, where the independence relations are of the form X and Y being independent given Z and C = a for some a. This is done by using Context-Specific trees, or CStrees for short, which can encode CSI relations.</p>

corrected abstract:
<p>Despite having a philosophical grounding from empiricism that spans some centuries, the algorithmization of causal discovery started only a few decades ago. This formalization of studying causal relationships relies on connections between graphs and probability distributions. In this setting, the task of causal discovery is to recover the graph that best describes the causal structure based on the available data. A particular class of causal discovery algorithms, called constraint-based methods rely on Directed Acyclic Graphs (DAGs) as an encoding of Conditional Independence (CI) relations that carry some level of causal information. However, a CI relation such as 𝑋 and 𝑌 being independent conditioned on 𝑍 assumes the independence holds for all possible values 𝑍 can take, which can tend to be unrealistic in practice where causal relations are often context-specific. In this thesis we aim to develop constraint-based algorithms to learn causal structure from Context-Specific Independence (CSI) relations within the discrete setting, where the independence relations are of the form 𝑋 and 𝑌 being independent given 𝑍 and 𝐶 = 𝑎 for some 𝑎. This is done by using Context-Specific trees, or CStrees for short, which can encode CSI relations.</p>

Note changes equations to use math italic font
----------------------------------------------------------------------
In diva2:1040644 
abstract is: 
<p>Computing the loads for a train passing a tunnel requires to predict both the external and the internal pressure variations in time, both of which are strong and quick for a non-pressure tight train.</p><p>The key achievement of this work has been the development of 3D CFD Star-CCM+ overset mesh simulations capable of simulating the single train tunnel entry and tunnel passage as well as the two trains crossing inside the tunnel. Unfortunately it is not affordable to execute a 3D CFD study for the tunnel passage of each new train model, so 1D CFD codes have been employed, simplified predictive models have been developed and both have been compared to the 3D CFD results.</p><p>An important result has been identifying the influence of several parameters on the loads caused both by the travelling pressure waves generated when the train enters the tunnel and by the pressure disturbances due to train crossing inside the tunnel, using Star-CCM+ parameters sweeps simulations over train speed and train and tunnel geometrical parameters.</p><p>The main conclusion is that the internal pressure variation is particularly important to compute the loads, especially for non-tight trains. For this reason it is necessary to take into account both the carriage free length and the position along the carriage on which the loads are needed.</p>

corrected abstract:
<p>Computing the loads for a train passing a tunnel requires to predict both the external and the internal pressure variations in time, both of which are strong and quick for a non-pressure tight train.</p><p>The key achievement of this work has been the development of 3D CFD Star-CCM+ overset mesh simulations capable of simulating the single train tunnel entry and tunnel passage as well as the two trains crossing inside the tunnel. Unfortunately it is not affordable to execute a 3D CFD study for the tunnel passage of each new train model, so 1D CFD codes have been employed, simplified predictive models have been developed and both have been compared to the 3D CFD results.</p><p>An important result has been identifying the influence of several parameters on the loads caused both by the travelling pressure waves generated when the train enters the tunnel and by the pressure disturbances due to train crossing inside the tunnel, using Star-CCM+ parameters sweeps simulations over train speed and train and tunnel geometrical parameters.</p><p>The main conclusion is that the internal pressure variation is particularly important to compute the loads, especially for non-tight trains. For this reason it is necessary to take into account both the carriage free length and the position along the carriage on which the loads are needed.</p>
----------------------------------------------------------------------
In diva2:1306995 
abstract is: 
<p>High speed planing hulls are currently widely used for example in recreational and emergency vessel applications. However, very little CFD research has been done for planing vessels, especially for those with stepped hulls. A validated CFD method for planing stepped hulls could be a valuable improvement for the design phase of such hulls. In this thesis, a CFD method for stepped hulls, with a primary focus on two-step hulls, is developed using STAR-CCM+. As a secondary objective, porpoising instability of two-step hulls is investigated. The simulations</p><p>are divided into two parts: In the first part a method is developed and validated with existing experimental and numerical data for a simple model scale planing hull with one step. In the second part the method is applied for two two-step hulls provided with Hydrolift AS. A maximum two degrees of freedom, trim and heave, are used, as well as RANS based k-w SST turbulence model and Volume of Fluid (VOF) as a free surface model. The results for the one-step hull mostly corresponded well with the validation data. For the two-step hulls, validation data did not exists and they were first simulated with a fixed trim and sinkage and compered between each other. In the simulations with free trim and heave both hulls experienced unstable porpoising behavior.</p>

corrected abstract:
<p>High speed planing hulls are currently widely used for example in recreational and emergency vessel applications. However, very little CFD research has been done for planing vessels, especially for those with stepped hulls. A validated CFD method for planing stepped hulls could be a valuable improvement for the design phase of such hulls.</p><p>In this thesis, a CFD method for stepped hulls, with a primary focus on two-step hulls, is developed using STAR-CCM+. As a secondary objective, porpoising instability of two-step hulls is investigated. The simulations are divided into two parts: In the first part a method is developed and validated with existing experimental and numerical data for a simple model scale planing hull with one step. In the second part the method is applied for two two-step hulls provided with Hydrolift AS. A maximum two degrees of freedom, trim and heave, are used, as well as RANS based 𝑘-ω SST turbulence model and Volume of Fluid (VOF) as a free surface model.</p><p>The results for the one-step hull mostly corresponded well with the validation data. For the two-step hulls, validation data did not exists and they were first simulated with a fixed trim and sinkage and compered between each other. In the simulations with free trim and heave both hulls experienced unstable porpoising behavior.</p>

Note added missing paragraph breaks and fixed "k" to "𝑘" and "w" to "ω"
----------------------------------------------------------------------
In diva2:1788302   - correct as is
----------------------------------------------------------------------
In diva2:538302 
abstract is: 
<p>The objective of this project is to design, using computational fluid dynamics (CFD), a set of retention aid dosage nozzles that minimize shear levels during their operation. This includes the effect of dosage nozzle size, contour and dosage velocity - absolute and relative to the stock flow. As a starting point, the three different dosage nozzles currently implemented on the Innventia FEX paper-machine have been studied using CFD. Problem areas, defined as regions of high viscous and/or turbulent shear, with these designs should be identified, and solutions to their improvement have been realized. The computational models considered here include non Newtonian models of the retention aid solution, as well as turbulent modeling of the stock flow. Novel configurations have been implemented which attempt to minimize the strain rate and shear stress during dosage and at the same time improve the mixing quality of the retention aid polymers. While the velocity of the side jet is determined to be the main cause of high strain rates and shear stresses, a good mixing can be reached by varying the position of the nozzles and the diameter penetrating the stock flow. The best compromise of mixing and shear stress has been reached with a triple side-wall nozzles configuration.</p>

corrected abstract:
<p>The objective of this project is to design, using computational fluid dynamics (CFD), a set of retention aid dosage nozzles that minimize shear levels during their operation. This includes the effect of dosage nozzle size, contour and dosage velocity - absolute and relative to the stock flow. As a starting point, the three different dosage nozzles currently implemented on the Innventia FEX paper-machine have been studied using CFD. Problem areas, defined as regions of high viscous and/or turbulent shear, with these designs should be identified, and solutions to their improvement have been realized. The computational models considered here include non-Newtonian models of the retention aid solution, as well as turbulent modeling of the stock flow. Novel configurations have been implemented which attempt to minimize the strain rate and shear stress during dosage and at the same time improve the mixing quality of the retention aid polymers. While the velocity of the side jet is determined to be the main cause of high strain rates and shear stresses, a good mixing can be reached by varying the position of the nozzles and the diameter penetrating the stock flow. The best compromise of mixing and shear stress has been reached with a triple side-wall nozzles configuration.</p>

Note added hyphen in "non-Newtonian"
----------------------------------------------------------------------
In diva2:1720147   - correct as is
----------------------------------------------------------------------
In diva2:1392095 - note that the registered trademark symbol is missing from title:
"CFD Simulations of Unsteady LHA Ship Airwake in OpenFOAM"
==>
"CFD Simulations of Unsteady LHA Ship Airwake in OpenFOAM®"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1218975 
abstract is: 
<p>The investing market can be a cold ruthless place for the layman. In order to get the chance of making money in this business one must place countless hours on research, with many different parameters to handle in order to reach success. To reduce the risk, one must look to many different companies operating in multiple fields and industries. In other words, it can be a hard task to manage this feat. With modern technology, there is now lots of potential to handle this tedious analysis autonomously using machine learning and clever algorithms. With this approach, the amount of analyzes is only limited by the capacity of the computer. Resulting in a number far greater than if done by hand. This study aims at exploring the possibilities to modify and implement efficient algorithms in the field of finance. The study utilizes the power of kernel methods in order to algorithmically analyze the patterns found in financial data efficiently. By combining the powerful tools of change point detection and nonlinear regression the computer can classify the different trends and moods in the market. The study culminates to a tool for analyzing data from the stock market in a way that minimizes the influence from short spikes and drops, and instead is influenced by the underlying pattern. But also, an additional tool for predicting future movements in the price.</p>

corrected abstract:
<p>The investing market can be a cold ruthless place for the layman. In order to get the chance of making money in this business one must place countless hours on research, with many different parameters to handle in order to reach success. To reduce the risk, one must look to many different companies operating in multiple fields and industries. In other words, it can be a hard task to manage this feat.</p><p>With modern technology, there is now lots of potential to handle this tedious analysis autonomously using machine learning and clever algorithms. With this approach, the amount of analyzes is only limited by the capacity of the computer. Resulting in a number far greater than if done by hand.</p><p>This study aims at exploring the possibilities to modify and implement efficient algorithms in the field of finance. The study utilizes the power of kernel methods in order to algorithmically analyze the patterns found in financial data efficiently. By combining the powerful tools of change point detection and nonlinear regression the computer can classify the different trends and moods in the market.</p><p>The study culminates to a tool for analyzing data from the stock market in a way that minimizes the influence from short spikes and drops, and instead is influenced by the underlying pattern. But also, an additional tool for predicting future movements in the price.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1561661 
abstract is: 
<p>Pentameric ligand-gated ion channels (pLGICs) are membrane receptors that play a crucial role in every living organism. The pLGIC protein structure forms a pore through the membrane of a cell that can let specific ions pass through, upon activation by endogenous agonists. pLGICs are allosterically modulated by ligands binding at allosteric sites, that either stabilize a certain conformation or change the binding affinity of the endogenous agonist. However, much remains unknown about the exact way in which these modulators bind to and affect pLGICs. An increased understanding could help in the search for novel and/or more effective target drugs. With this masters thesis, I hope to contribute by investigating the modulatory effect of ethanol on the bacterial Gloeobacter ligand-gated ion channel (GLIC). This has been done by performing oocyte electrophysiology recordings and analysis of molecular dynamics simulations, both with and without ethanol, and of four separate variants of GLIC that are either potentiated or inhibited by ethanol. Two possible allosteric sites were discovered in a transmembraneintrasubunit pocket: a potentiating allosteric site close to the M2 helix and residue V242, as well as an inhibitory membrane- and M4 helix-close intrasubunit site. Finally, evidence was found that could support a previously suggested inhibitory allosteric site in the pore around the 9’ hydrophobic gate.</p>

corrected abstract:
<p>Pentameric ligand-gated ion channels (pLGICs) are membrane receptors that play a crucial role in every living organism. The pLGIC protein structure forms a pore through the membrane of a cell that can let specific ions pass through, upon activation by endogenous agonists. pLGICs are allosterically modulated by ligands binding at allosteric sites, that either stabilize a certain conformation or change the binding affinity of the endogenous agonist. However, much remains unknown about the exact way in which these modulators bind to and affect pLGICs. An increased understanding could help in the search for novel and/or more effective target drugs. With this masters thesis, I hope to contribute by investigating the modulatory effect of ethanol on the bacterial <em>Gloeobacter</em> ligand-gated ion channel (GLIC). This has been done by performing oocyte electrophysiology recordings and analysis of molecular dynamics simulations, both with and without ethanol, and of four separate variants of GLIC that are either potentiated or inhibited by ethanol. Two possible allosteric sites were discovered in a transmembrane intrasubunit pocket: a potentiating allosteric site close to the M2 helix and residue V242, as well as an inhibitory membrane- and M4 helix-close intrasubunit site. Finally, evidence was found that could support a previously suggested inhibitory allosteric site in the pore around the 9’ hydrophobic gate.</p>

Note - added missing italics
----------------------------------------------------------------------
In diva2:549834   - correct as is
----------------------------------------------------------------------
In diva2:1817011   - correct as is
----------------------------------------------------------------------
In diva2:1703975   - correct as is
----------------------------------------------------------------------
In diva2:1288436 
abstract is: 
<p>This report describes the structural design of a wing for a Vertical Take Off  and Landing drone, in which all the structure will be built by fused deposition modeling of polylactic acid (PLA). To perform this de-sign, the material used is first characterized in different orientations using tensile stress tests, Image Correlation and MATLAB. These properties are then input in a MATLAB program specially developed for this project to obtain the optimum skin and spar thickness in the wing for certain fight conditions. Results are finally verified with a 3D model in CAD and scaled wings in bending tests.</p>

corrected abstract:
<p>This report describes the structural design of a wing for a Vertical Take Off and Landing drone, in which all the structure will be built by fused deposition modeling of polylactic acid (PLA). To perform this design, the material used is first characterized in different orientations using tensile stress tests, Image Correlation and MATLAB. These properties are then input in a MATLAB program specially developed for this project to obtain the optimum skin and spar thickness in the wing for certain flight conditions. Results are finally verified with a 3D model in CAD and scaled wings in bending tests.</p>

Note - removed unnecessary hyphen and corrceed spelling of "flight"
----------------------------------------------------------------------
In diva2:550442   - perhaps a missing colon separating title and subtitle:
"Characterization of Track Irregularities With respect to vehicle response"
==>
"Characterization of Track Irregularities: With respect to vehicle response"

abstract correct as is
----------------------------------------------------------------------
In diva2:1082703 
abstract is: 
<p>The demands on fuel efficiency and environmental friendliness of cars have driven the automotive industry towards composite materials which reduce the weight compared to the traditional aluminum and steel solutions. The purpose of this master thesis is to evaluate the possibility and feasibility of redesigning a high volume metal chassis part in composite materials.  To accomplish this the thesis work was divided into two parts. The first part consists of a composite study which explores the available composite technologies in the industry such as implemented chassis components and available manufacturing methods. The composite study shows that almost no high volume chassis component in the market are made out of composites, with exception to leaf springs. In the industry there are many different composite manufacturing methods but in general the most ready for high volume production are Injection molding, compression molding and RTM. A method was also explored to efficiently evaluate different material and manufacturing methods against each other. By knowing the critical requirement both materials and manufacturing methods can be evaluated separately against each other. The second part consists of a design phase where the knowledge from the composite study was used to choose and redesign a chassis component in composite. A motor mount was chosen and redesigned using injection molding. The new design shows that a weight decrease of at least 38% is possible without significant cost differences. </p>

corrected abstract:
<p>The demands on fuel efficiency and environmental friendliness of cars have driven the automotive industry towards composite materials which reduce the weight compared to the traditional aluminum and steel solutions. The purpose of this master thesis is to evaluate the possibility and feasibility of redesigning a high volume metal chassis part in composite materials.  To accomplish this the thesis work was divided into two parts. The first part consists of a composite study which explores the available composite technologies in the industry such as implemented chassis components and available manufacturing methods. The composite study shows that almost no high volume chassis component in the market are made out of composites, with exception to leaf springs. In the industry there are many different composite manufacturing methods but in general the most ready for high volume production are Injection molding, compression molding and RTM. A method was also explored to efficiently evaluate different material and manufacturing methods against each other. By knowing the critical requirement both materials and manufacturing methods can be evaluated separately against each other. The second part consists of a design phase where the knowledge from the composite study was used to choose and redesign a chassis component in composite. A motor mount was chosen and redesigned using injection molding. The new design shows that a weight decrease of at least 38% is possible without significant cost differences.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1081928 
abstract is: 
<p>Autonomous driving might increase safety and profitability of trucks in many applications. The mining industry, with its enclosed and controlled areas, is ideal for early implementation of autonomous solutions. The possibility of increased productivity, profitability and safety for the mining industry and the mining area as a ground for development could, through collaboration, result in many benefits for both mining companies and truck manufactures. Scania must investigate how these autonomous vehicles should be constructed. The project goal is thereby to develop a chassis layout concept for an autonomous truck. The concept should improve profitability and safety for transportation of materials within the mining industry while minimizing the introduction of new components to Scania.</p><p>The chosen approach is based on the Ulrich &amp; Eppinger method for product development including generation and selection of concepts. Product requirements were specified from identified customer needs. The generated concepts were evaluated against these requirements and comparisons were performed with weighted matrices. Some benefits of the final chassis layout concept are a higher load carrying capacity, more robust component placement and higher ground clearance. The vehicle concept would also be able to operate in underground mines with low roof clearance which could open new market segments for Scania. However, the concept requires development to gain higher performance on load carrying components in the chassis front.</p><p>The suggested concept shows that Scania could build and deliver autonomous mining vehicles with optimized chassis layouts based on Scania’s existing components within a near future.</p>

corrected abstract:
<p>Autonomous driving might increase safety and profitability of trucks in many applications. The mining industry, with its enclosed and controlled areas, is ideal for early implementation of autonomous solutions. The possibility of increased productivity, profitability and safety for the mining industry and the mining area as a ground for development could, through collaboration, result in many benefits for both mining companies and truck manufactures.</p><p>Scania must investigate how these autonomous vehicles should be constructed. The project goal is thereby to develop a chassis layout concept for an autonomous truck. The concept should improve profitability and safety for transportation of materials within the mining industry while minimizing the introduction of new components to Scania.</p><p>The chosen approach is based on the Ulrich &amp; Eppinger method for product development including generation and selection of concepts. Product requirements were specified from identified customer needs. The generated concepts were evaluated against these requirements and comparisons were performed with weighted matrices.</p><p>Some benefits of the final chassis layout concept are a higher load carrying capacity, more robust component placement and higher ground clearance. The vehicle concept would also be able to operate in underground mines with low roof clearance which could open new market segments for Scania. However, the concept requires development to gain higher performance on load carrying components in the chassis front.</p><p>The suggested concept shows that Scania could build and deliver autonomous mining vehicles with optimized chassis layouts based on Scania’s existing components within a near future.</p>

Note - added missing paragrap breaks
----------------------------------------------------------------------
In diva2:1379746   - correct as is
----------------------------------------------------------------------
In diva2:1348448 
abstract is: 
<p>The divisible cross-country ski, an innovation by Cityski, created to facilitate practicing the sport in an urban setting. The parts are attached by a cone-shaped coupling in a fitted sleeve made of composite material and fastened with a lock. The aim of this thesis is to analyze the coupling and lock with mechanical models to calculate loads, deformations and stresses that may occur when the ski is used as well as giving suggestions for further improvement. The study concludes that the coupling in current state and material cannot be expected to stand anticipated loads during usage as the stresses exceed the tensile strength of the material with a factor of four. A study of materials showed that by using aluminum as an option to composite could reduce the factor considerably. Further, the result also showed that over 90 percent of the overall stresses the cone was subjected to were represented by bending stresses. To account for the stresses the coupling has been subjected to a parameter study in order to relate the geometry of the cone to the stresses. From this it was shown that by simply adjusting the placement of the cone from its centered position downwards the bending stresses could be reduced by approximately 9 percent. This combined with additional adjustments such as a more rigid locking mechanism and tighter lock could contribute to a maximum 26 percent reduction. To further establish the result from the mechanical modelling a numerical analysis with finite element method was conducted using the software ANSYS MECHANICAL. The outcomes confirmed earlier results by showing similar levels of stresses, even to a certain degree higher due to concentration in the area where the cone is mounted. This is a known weak zone from earlier testing of the ski. To enable suggestion for strengthening the area a method for topology optimization was done. Based on this a new model which make use of the material more effectively was designed. Additional suggestions for improvements discussed but not analyzed in depth are the possibility of a plate of a more rigid material on the bottom side of the cone to allow for a higher load before break or make the ski slightly higher at the coupling to lower the effects of bending stresses.</p>

corrected abstract:
<p>The divisible cross-country ski, an innovation by Cityski, created to facilitate practicing the sport in an urban setting. The parts are attached by a cone-shaped coupling in a fitted sleeve made of composite material and fastened with a lock. The aim of this thesis is to analyze the coupling and lock with mechanical models to calculate loads, deformations and stresses that may occur when the ski is used as well as giving suggestions for further improvement. The study concludes that the coupling in current state and material cannot be expected to stand anticipated loads during usage as the stresses exceed the tensile strength of the material with a factor of four. A study of materials showed that by using aluminum as an option to composite could reduce the factor considerably.</p><p>Further, the result also showed that over 90 percent of the overall stresses the cone was subjected to were represented by bending stresses. To account for the stresses the coupling has been subjected to a parameter study in order to relate the geometry of the cone to the stresses. From this it was shown that by simply adjusting the placement of the cone from its centered position downwards the bending stresses could be reduced by approximately 9 percent. This combined with additional adjustments such as a more rigid locking mechanism and tighter lock could contribute to a maximum 26 percent reduction.</p><p>To further establish the result from the mechanical modelling a numerical analysis with finite element method was conducted using the software ANSYS MECHANICAL. The outcomes confirmed earlier results by showing similar levels of stresses, even to a certain degree higher due to concentration in the area where the cone is mounted. This is a known weak zone from earlier testing of the ski. To enable suggestion for strengthening the area a method for topology optimization was done. Based on this a new model which make use of the material more effectively was designed. Additional suggestions for improvements discussed but not analyzed in depth are the possibility of a plate of a more rigid material on the bottom side of the cone to allow for a higher load before break or make the ski slightly higher at the coupling to lower the effects of bending stresses.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:852715 
abstract is: 
<p>Three methods for claims reserving are compared on two data sets. The first two methods are the commonly used chain ladder method that uses aggregated payments and the relatively new method, double chain ladder, that apart from the payments data also uses the number of reported claims. The third method is more advanced, data on micro-level is needed such as the reporting delay and the number of payment periods for every single claim. The two data sets that are used consist of claims with typically shorter and longer settlement time, respectively. The questions considered are if you can gain anything from using a method that is more advanced than the chain ladder method and if the gain differs from the two data sets. The methods are compared by simulating the reserves distributions as well as comparing the point estimates of the reserve with the real out-of-sample reserve. The results show that there is no gain in using the micro-level method considered. The double chain lad- der method on the other hand performs better than the chain ladder method. The difference between the two data sets is that the reserve in the data set with longer settlement times is harder to estimate, but no difference can be seen when it comes to method choice.</p>

corrected abstract:
<p>Three methods for claims reserving are compared on two data sets. The first two methods are the commonly used chain ladder method that uses aggregated payments and the relatively new method, double chain ladder, that apart from the payments data also uses the number of reported claims. The third method is more advanced, data on micro-level is needed such as the reporting delay and the number of payment periods for every single claim. The two data sets that are used consist of claims with typically shorter and longer settlement time, respectively. The questions considered are if you can gain anything from using a method that is more advanced than the chain ladder method and if the gain differs from the two data sets. The methods are compared by simulating the reserves distributions as well as comparing the point estimates of the reserve with the real out-of-sample reserve. The results show that there is no gain in using the micro-level method considered. The double chain ladder method on the other hand performs better than the chain ladder method. The difference between the two data sets is that the reserve in the data set with longer settlement times is harder to estimate, but no difference can be seen when it comes to method choice.</p>

Note - removed unnecessary hyphen
----------------------------------------------------------------------
In diva2:1215659   - correct as is
----------------------------------------------------------------------
In diva2:1595248 - PDF appears to have been printed as a bitmap
abstract is: 
<p>The primary objective of this thesis is to evaluate the prospect of machine learning methods being used to classify flying qualities based on simulator data (with the focus being on pitch maneuvers). If critical flying qualities could be identified earlier in the verification process, they can be further invested in and focused on with less cost for design changes of the flight control system.</p><p>Information from manned simulations with given flying quality levels are used to create a replication of the performed pitch maneuver in a desktop simulator. The generated flight data is represented by different measures in the classification to separately train and test the machine learning models against the given flying quality level. The models used are Logistic Regression, Support Vector Machines with radial basis functions (RBF), linear and polynomial kernels along with Artificial Neural Networks. </p><p>The results show that the classifiers correctly identify at least 80% of cases with critical flying qualities. The classification shows that the statistical measures of the time signals and first order time derivatives of pitch, roll and yaw rates are enough for classification within the scope of this thesis. The different machine learning models show no significant difference in performance in the scope of this thesis. In conclusion, machine learning methods show good potential for classification of flying qualities, and could become an important tool for evaluating flying qualities of large amounts of simulations, in addition to manned simulations.</p>

corrected abstract:
<p>The primary objective of this thesis is to evaluate the prospect of machine learning methods being used to classify flying qualities based on simulator data (with the focus being on pitch maneuvers). If critical flying qualities could be identified earlier in the verification process, they can be further invested in and focused on with less cost for design changes of the flight control system.</p><p>Information from manned simulations with given flying quality levels are used to create a replication of the performed pitch maneuver in a desktop simulator. The generated flight data is represented by different measures in the classification to separately train and test the machine learning models against the given flying quality level. The models used are Logistic Regression, Support Vector Machines with radial basis functions (RBF), linear and polynomial kernels along with Artificial Neural Networks.</p><p>The results show that the classifiers correctly identify at least 80% of cases with critical flying qualities. The classification shows that statistical measures of the time signals and first order time derivatives of pitch, roll and yaw rates are enough for classification within the scope of this thesis. The different machine learning models show no significant difference in performance in the scope of this thesis. In conclusion, machine learning methods shows good potential for classification of flying qualities, and could become an important tool for evaluating flying qualities of large amounts of simulations, in addition to manned simulations.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph and minor spelling corrceions
("the statistical" should eb "statistical" and "show" to "shows")
----------------------------------------------------------------------
In diva2:575902   - correct as is
----------------------------------------------------------------------
In diva2:1850032 
abstract is: 
<p>The Maximum Likelihood Degree (ML degree) of a statistical model is the number of complex critical points of the likelihood function. In this thesis we study this on Colored Gaussian Graphical Models, classifying the ML degree of colored graphs of order up to three. We do this by calculating the rational function degree of the gradient of the log- likelihood. Moreover we find that coloring a graph can lower the ML degree. Finally we calculate solutions to the homaloidal partial differential equation developed by Améndola et al. The code developed for these calculations can be used on graphs of higher orders.</p>

corrected abstract:
<p>The Maximum Likelihood Degree (ML degree) of a statistical model is the number of complex critical points of the likelihood function. In this thesis we study this on Colored Gaussian Graphical Models, classifying the ML degree of colored graphs of order up to three. We do this by calculating the rational function degree of the gradient of the log-likelihood. Moreover we find that coloring a graph can lower the ML degree. Finally we calculate solutions to the homaloidal partial differential equation developed by Améndola et al. The code developed for these calculations can be used on graphs of higher orders.</p>

Note - removed unnecessary space after hypen
----------------------------------------------------------------------
In diva2:1849053   - correct as is
----------------------------------------------------------------------
In diva2:1165806   - correct as is
----------------------------------------------------------------------
In diva2:1307352   - correct as is
----------------------------------------------------------------------
In diva2:1375293   - correct as is
----------------------------------------------------------------------
In diva2:1431629 
abstract is: 
<p>With the help of new technology it has become much easier to apply for a job. Reaching out to a larger audience also results in a lot of more applications to consider when hiring for a new position. This has resulted in that many big companies uses statistical learning methods as a tool in the first step of the recruiting process. Smaller companies that do not have access to the same amount of historical and big data sets do not have the same opportunities to digitalise their recruitment process. Using topological data analysis, this thesis explore how clustering methods can be used on smaller data sets in the early stages of the recruitment process. It also studies how the level of abstraction in data representation affects the results. The methods seem to perform well on higher level job announcements but struggles on basic level positions. It also shows that the representation of candidates and jobs has a huge impact on the results.</p>

corrected abstract:
<p>With the help of new technology it has become much easier to apply for a job. Reaching out to a larger audience also results in a lot of more applications to consider when hiring for a new position. This has resulted in that many big companies uses statistical learning methods as a tool in the first step of the recruiting process. Smaller companies that do not have access to the same amount of historical and big data sets do not have the same opportunities to digitalise their recruitment process. Using topological data analysis, this thesis explore how clustering methods can be used on smaller data sets in the early stages of the recruitment process. It also studies how the level of abstraction in data representation affects the results. The methods seem to perform well on higher level job announcements but struggles on basic level positions. It also shows that the choice of representation of candidates and jobs has a huge impact on the results.</p>

Note added missing word "choice"
----------------------------------------------------------------------
In diva2:1319859 
abstract is: 
<p>In this thesis a clustering of the Stockholm county housing market has been performed using different clustering methods. Data has been derived and different geographical constraints have been used. DeSO areas (Demographic statistical areas), developed by SCB, have been used to divide the housing market in to smaller regions for which the derived variables have been calculated. Hierarchical clustering methods, SKATER and Gaussian mixture models have been applied. Methods using different kinds of geographical constraints have also been applied in an attempt to create more geographically contiguous clusters. The different methods are then compared with respect to performance and stability. The best performing method is the Gaussian mixture model EII, also known as the K-means algorithm. The most stable method when applied to bootstrapped samples is the ClustGeo-method.</p>

corrected abstract:
<p>In this thesis a clustering of the Stockholm county housing market has been performed using different clustering methods. Data has been derived and different geographical constraints have been used. DeSO areas (Demographically statistical areas), developed by SCB, has been used to divide the housing market in to smaller regions for which the derived variables have been calculated. Hierarchical clustering methods, <em>SKATER</em> and Gaussian mixture models have been applied. Methods using different kinds of geographical constraints have also been applied in an attempt to create more geographically contiguous clusters. The different methods are then compared with respect to performance and stability. The best performing method is the Gaussian mixture model <em>EII</em>, also known as the 𝐾-<em>means</em> algorithm. The most stable method when applied to bootstrapped samples is the <em>ClustGeo-method</em>.</p>

Note added missing text, corrected some text, added italics and changed "K" to "𝐾"
----------------------------------------------------------------------
In diva2:633890 
abstract is: 
<p>Adaptive mesh refinement and coarsening methods are effective techniques to reduce the computation time of finite element based solvers. Parallel imple- mentations of such adaption routines, suitable for large scale computations on distributed memory machines, need additional care. In this thesis, a coarsening technique based on edge collapses is presented, its implementation and opti- mization for parallel computations explained and it is analyzed with respect to coarsening efficiency and performance. As a possible application the use of mesh coarsening in adaptive flow simulations is demonstrated</p>

corrected abstract:
<p>Adaptive mesh refinement and coarsening methods are effective techniques to reduce the computation time of finite element based solvers. Parallel implementations of such adaption routines, suitable for large scale computations on distributed memory machines, need additional care. In this thesis, a coarsening technique based on edge collapses is presented, its implementation and optimization for parallel computations explained and it is analyzed with respect to coarsening efficiency and performance. As a possible application the use of mesh coarsening in adaptive flow simulations is demonstrated.</p>

Note - removed unnecessary hpyehns and added terminal period to last sentence (as it is in the original)
----------------------------------------------------------------------
In diva2:1086408   - correct as is
----------------------------------------------------------------------
In diva2:1596326   - correct as is --- diva2:1656340 seems to be a duplicate
----------------------------------------------------------------------
In diva2:1739340   - correct as is
----------------------------------------------------------------------
In diva2:1441558   - correct as is
----------------------------------------------------------------------
In diva2:1796647 
abstract is: 
<p>This research study aims to investigate the capacity of single photons to carry information through polarization and time ordering and proposes a protocol called Beyond Pulse Position Modulation (BPPM) to improve photon-based communication reliability over longer distances with limited power. Such a protocol may be used in any communication scenario where energy efficiency is important, e.g., in satellite communication or where pulse position modulation (PPM) typically is used. The study compares various metrics such as information bits per symbol, photon, and time bin to evaluate the system’s efficiency and conducts a comparative analysis of BPPM, Pulse Position Modulation (PPM), On-Off Keying (OOK), andGeneral protocol’s effectiveness. (The simulations were conducted using the Python programming language with Visual Studio Code IDE.)</p>

mc='andGeneral' c='and General'

corrected abstract:
<p>This research study aims to investigate the capacity of single photons to carry information through polarization and time ordering and proposes a protocol called Beyond Pulse Position Modulation (BPPM) to improve photon-based communication reliability over longer distances with limited power. Such a protocol may be used in any communication scenario where energy efficiency is important, e.g., in satellite communication or where pulse position modulation (PPM) typically is used. The study compares various metrics such as information bits per symbol, photon, and time bin to evaluate the system’s efficiency and conducts a comparative analysis of BPPM, Pulse Position Modulation (PPM), On-Off Keying (OOK), and General protocol’s effectiveness. (The simulations were conducted using the Python programming language with Visual Studio Code IDE.)</p>

Note - separated the merged words
----------------------------------------------------------------------
In diva2:1255173   - correct as is

Note error in original "in-situ" should be "<em>in situ</em>"
----------------------------------------------------------------------
In diva2:872164   - correct as is
----------------------------------------------------------------------
In diva2:1107306 
abstract is: 
<p>This thesis compares two groups of features for short-term price predictions of futures contracts; fast- and slow-acting features. The fast-acting group are based on limit order book derived features and technical indicators that reacts to changes in price quickly. The slow-acting features constitute of technical indicators that reacts to changes in price slowly.</p><p>The comparison is done through two methods, group importance and a mean cost calculation. This is evaluated for different forecast horizons and contracts. Furthermore, two years of data was provided to do the analysis. Moreover, the comparison is modelled with an ensemble method called random forest. The response is constructed using rolling quantiles and a volume weighted price. </p><p>The finding implies that fast-acting features are superior at predicting price changes on smaller time scales, while long-acting features are better at predicting prices changes on larger time scales. Furthermore, the multivariate model results were similar to the univariate ones. However, the results are not clear-cut and more investigation ought to be done in order to confirm these results.</p>

corrected abstract:
<p>This thesis compares two groups of features for short-term price predictions of futures contracts; fast- and slow-acting features. The fast-acting group are based on limit order book derived features and technical indicators that reacts to changes in price quickly. The slow-acting features constitute of technical indicators that reacts to changes in price slowly.</p><p>The comparison is done through two methods, group importance and a mean cost calculation. This is evaluated for different forecast horizons and contracts. Furthermore, two years of data was provided to do the analysis. Moreover, the comparison is modelled with an ensemble method called random forest. The response is constructed using rolling quantiles and a volume weighted price.</p><p>The finding implies that fast-acting features are superior at predicting price changes on smaller time scales, while long-acting features are better at predicting prices changes on larger time scales. Furthermore, the multivariate model results were similar to the univariate ones. However, the results are not clear-cut and more investigation ought to be done in order to confirm these results.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1766767   - correct as is
----------------------------------------------------------------------
In diva2:917945   - correct as is
----------------------------------------------------------------------
In diva2:1528156 
abstract is: 
<p>Formation flying of satellites describes a mission in which a set of satellites arrange their position with respect to one another. In this paper, satellite formation flying guidance and control algorithms are investigated in terms of required velocity increment Delta-v, and tracking error for a Chief/Deputy satellite system. Different control methods covering continuous and impulsive laws are implemented and tested for Low Earth Orbit (LEO). Sliding Mode, Feedback Linearization and Model Predictive Controllers are compared to an Impulsive Feedback Law which tracks the mean orbital element differences. Sliding Mode and Feedback Linearization controllers use the same dynamic model which includes Earth Oblateness perturbations. On the other hand, Model Predictive Control with Multi-Objective Cost Function is based on the Clohessy–Wiltshire equations, which do not account for any perturbation and do not cover the eccentricity of the orbit. The comparison was done for two different missions both including Earth Oblateness effects only. A relative orbit mission, which was based on the Prisma Satellite Mission and a rendezvous mission, was implemented. The reference trajectory for the controllers was generated with Yamanaka and Ankersen’s state transition matrix, while a separate method was used for the Impulsive Law. In both of the missions, it was observed that the implemented Impulsive Law outperformed in terms of Delta-v, 1.2 to 3.5 times smaller than the continuous control approaches, while the continuous controllers had a smaller tracking error, 2 to 8.3 times less, both in terms of root mean square error and maximum error in the steady state. Finally, this study shows that the tracking error and Delta-v has inversely proportional relationship.</p>

corrected abstract:
<p>Formation flying of satellites describes a mission in which a set of satellites arrange their position with respect to one another. In this paper, satellite formation flying guidance and control algorithms are investigated in terms of required velocity increment ∆𝑣, and tracking error for a Chief/Deputy satellite system. Different control methods covering continuous and impulsive laws are implemented and tested for Low Earth Orbit (LEO). Sliding Mode, Feedback Linearization and Model Predictive Controllers are compared to an Impulsive Feedback Law which tracks the mean orbital element differences. Sliding Mode and Feedback Linearization controllers use the same dynamic model which includes Earth Oblateness perturbations. On the other hand, Model Predictive Control with Multi-Objective Cost Function is based on the Clohessy–Wiltshire equations, which do not account for any perturbation and do not cover the eccentricity of the orbit. The comparison was done for two different missions both including Earth Oblateness effects only. A relative orbit mission, which was based on the Prisma Satellite Mission and a rendezvous mission, was implemented. The reference trajectory for the controllers was generated with Yamanaka and Ankersen’s state transition matrix, while a separate method was used for the Impulsive Law. In both of the missions, it was observed that the implemented Impulsive Law outperformed in terms of ∆𝑣, 1.2 to 3.5 times smaller than the continuous control approaches, while the continuous controllers had a smaller tracking error, 2 to 8.3 times less, both in terms of root mean square error and maximum error in the steady state. Finally, this study shows that the tracking error and ∆𝑣 has inversely proportional relationship.</p>

Note - replaced "Delta-v" with "∆𝑣"
----------------------------------------------------------------------
In diva2:1567721   - correct as is
----------------------------------------------------------------------
In diva2:1429548 
abstract is: 
<p>The use of composite materials has been common in small craft boat building for a long time. In recent years, there has been a huge push in the development of diﬀerent types of appendages such as hydrofoils. These hydrofoils are commonly manufactured in carbon ﬁbre composites, due to high requirements in weight and stiﬀness. These appendages can be diﬃcult to develop and complex to manufacture since manufacturing methods for composites are complex. KTH Royal Institute of Technology is developing a hydrofoil concept for a small autonomous vessel. The hydrofoil is designed to be built in carbon ﬁbre composite. It requires to have control surfaces in order to maintain a stable ﬂight and the electrical propulsion is located on it as well. This makes this hydrofoil one of a kind and the parts that build up the hydrofoil have more speciﬁcations than just to be designed from a hydrodynamic and structural point of view like a conventional hydrofoil. This thesis investigated what manufacturing methods should be used when building a hydrofoil like this.</p><p>Existing manufacturing methods such as vacuum infusion and diﬀerent types of prepreg moulding have been reviewed and are presented early in the report. The methods have been analyzed from the perspective of the components of the hydrofoil, resulting in an initial manufacturing strategy for the diﬀerent components. The strategy includes everything from a 3D-model of the part to a ﬁnished product, including sheet design, mould manufacturing and moulding of the part.</p><p>Several tests were conducted before a component was successfully manufactured. Each test was evaluated and presented in such a manner that the reader can understand what is needed to be improved and why. The conclusions of each test lead to an improvement of the manufacturing technique and a new test until the ﬁnal result was acquired. The tests were examined with a microscope to verify the quality of the part. Then a weight fraction analysis was made on these parts. The ﬁnal conclusions of the thesis gave successful methods to manufacture the diﬀerent parts of the hydrofoil. A fast manufacturing method for product development of complex parts was achieved. The resulting parts from the tests show good quality from the analysis.</p>

corrected abstract:
<p>The use of composite materials has been common in small craft boat building for a long time. In recent years, there has been a huge push in the development of different types of appendages such as hydrofoils. These hydrofoils are commonly manufactured in carbon fibre composites, due to high requirements in weight and stiffness. These appendages can be difficult to develop and complex to manufacture since manufacturing methods for composites are complex. KTH Royal Institute of Technology is developing a hydrofoil concept for a small autonomous vessel. The hydrofoil is designed to be built in carbon fibre composite. It requires to have control surfaces in order to maintain a stable flight and the electrical propulsion is located on it as well. This makes this hydrofoil one of a kind and the parts that build up the hydrofoil have more specifications than just to be designed from a hydrodynamic and structural point of view like a conventional hydrofoil. This thesis investigated what manufacturing methods should be used when building a hydrofoil like this.</p><p>Existing manufacturing methods such as vacuum infusion and different types of prepreg moulding have been reviewed and are presented early in the report. The methods have been analyzed from the perspective of the components of the hydrofoil, resulting in an initial manufacturing strategy for the different components. The strategy includes everything from a 3D-model of the part to a finished product, including sheet design, mould manufacturing and moulding of the part.</p><p>Several tests were conducted before a component was successfully manufactured. Each test was evaluated and presented in such a manner that the reader can understand what is needed to be improved and why. The conclusions of each test lead to an improvement of the manufacturing technique and a new test until the final result was acquired. The tests were examined with a microscope to verify the quality of the part. Then a weight fraction analysis was made on these parts. The final conclusions of the thesis gave successful methods to manufacture the different parts of the hydrofoil. A fast manufacturing method for product development of complex parts was achieved. The resulting parts from the tests show good quality from the analysis.</p>

Note the corrected version has replaced the ligatures byt their equivalents
----------------------------------------------------------------------
In diva2:1693894 
abstract is: 
<p>The master thesis subject takes place in the automotive industry and specifically in the internal combustion engine area. The need of improving the efficiency of the engines leads to develop new technologies like turbo compressors. Some of the challenges to overcome are high rotational speed difficulties or extreme load and fatigue in the rotors. By design they are also prone to aerodynamic instabilities like compressor surge. These off design behaviors are not often studied by the manufacturers and therefore not so well known. </p><p>The aims are to understand, analyze and possible ameliorate the sources of compressor surge; to identify surge causes; to create a way to reproduce the phenomena with robustness and precision; to be able to study potential solutions to eliminate surge noises. A literature review has been carried out. This would give good metrics to identify surge cycles.</p><p>Based on the theory developed by Fink et al. (1992) a simulation model has been generated, followed by a process of calibration carried out using data acquired during field experiments. This method uses a fully modifiable simulation model in order to be able to be adapted to a wide range of turbo compressors.</p><p>The predicted data by the model shows a reasonable agreement with the experimental data. This allows to test control laws with a surge valve or a high pressure gas recirculating valve. The knowledge alongside the simulation would help the team to better apprehend the problem on the future engine generations and have means to avoid the unwanted surge phenomena to occur.</p>

corrected abstract:
<p>The master thesis subject takes place in the automotive industry and specifically in the internal combustion engine area. The need of improving the efficiency of the engines leads to develop new technologies like turbo compressors. Some of the challenges to overcome are high rotational speed difficulties or extreme load and fatigue in the rotors. By design they are also prone to aerodynamic instabilities like compressor surge. These off design behaviors are not often studied by the manufacturers and therefore not so well known.</p><p>The aims are to understand, analyze and possible ameliorate the sources of compressor surge; to identify surge causes; to create a way to reproduce the phenomena with robustness and precision; to be able to study potential solutions to eliminate surge noises. A literature review has been carried out. This would give good metrics to identify surge cycles.</p><p>Based on the theory developed by <em>Fink et al.</em> (1992) a simulation model has been generated, followed by a process of calibration carried out using data acquired during field experiments. This method uses a fully modifiable simulation model in order to be able to be adapted to a wide range of turbo compressors.</p><p>The predicted data by the model shows a reasonable agreement with the experimental data. This allows to test control laws with a surge valve or a high pressure gas recirculating valve. The knowledge alongside the simulation would help the team to better apprehend the problem on the future engine generations and have means to avoid the unwanted surge phenomena to occur.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph and added italics
----------------------------------------------------------------------
In diva2:604474 
abstract is: 
<p>This thesis details the refinement and numerical solution of a preexisting model for predicting the strengths and positions of so-called wake-vortices that are generated from the lift of heavy aircraft. The ultimate objective is to implement a numerical scheme for the model that is fast enough</p><p>to allow for probabilistic methods, such as Monte Carlosimulations, in order to deal with the inherent uncertainty in input parameters for wake-vortex predictions.</p><p>The differential equation system of the wake-vortex model is stated clearly, which has not been done before. The refinement consists in reducing the number of necessary state variables in the differential equation system.</p><p>A numerical algorithm based on the mathematical properties of the model is implemented and different ways of optimizing the computations are considered, e.g. through</p><p>parallelization.</p><p>Finally, a study will be made trying to assess the validity of the results through analyses of the accuracy and of the model’s sensitivity to small input parameter variations.</p>

mc='Carlosimulations' c='Carlo simulations'

corrected abstract:
<p>This thesis details the refinement and numerical solution of a preexisting model for predicting the strengths and positions of so-called wake-vortices that are generated from the lift of heavy aircraft. The ultimate objective is to implement a numerical scheme for the model that is fast enough to allow for probabilistic methods, such as Monte Carlo-simulations, in order to deal with the inherent uncertainty in input parameters for wake-vortex predictions.</p><p>The differential equation system of the wake-vortex model is stated clearly, which has not been done before. The refinement consists in reducing the number of necessary state variables in the differential equation system.</p><p>A numerical algorithm based on the mathematical properties of the model is implemented and different ways of optimizing the computations are considered, e.g. through parallelization.</p><p>Finally, a study will be made trying to assess the validity of the results through analyses of the accuracy and of the model’s sensitivity to small input parameter variations.</p>

Note - removed unnecessary paragraph breaks and added hypen
----------------------------------------------------------------------
In diva2:1705466 
abstract is: 
<p>Numerical simulations of large complex systems such as biomolecules often suffer from the full description of the system having too many dimensions for direct numerical calculations and Monte Carlo methods having trouble overcoming energy barriers. It is therefore desirable to formulate a description in lower dimension which captures the system’s macroscopic behaviour. Recently, Lindahl et al [1] proposed a metric, g(λ), on the extended space Λ based on the dynamics of the system to optimize Monte Carlo sampling within extended ensemble formalism. In this thesis, we formulate a low-dimensional effective coarse-grained dynamic on Λ as a diffusion process and ask if it is possible to use this metric to calculate thelocal effective diffusion matrix as D(λ) = g−1(λ). By testing various scenarios we conclude that computing D(λ) in this manner indeed gives a correct effective dynamic in most cases, where the scale of coarse-graining can be tuned. However, an incorrect dynamic is received for example when the scale of coarse-graining is comparable to the size of oscillations in the energy landscape.</p>

mc='thelocal' c='the local'

corrected abstract:
<p>Numerical simulations of large complex systems such as biomolecules often suffer from the full description of the system having too many dimensions for direct numerical calculations and Monte Carlo methods having trouble overcoming energy barriers. It is therefore desirable to formulate a description in lower dimension which captures the system’s macroscopic behaviour. Recently, Lindahl <em>et al</em> [1] proposed a metric, 𝑔(&#x1D706;), on the extended space Λ based on the dynamics of the system to optimize Monte Carlo sampling within extended ensemble formalism. In this thesis, we formulate a low-dimensional effective coarse-grained dynamic on Λ as a diffusion process and ask if it is possible to use this metric to calculate the local effective diffusion matrix as 𝐷(λ) = 𝑔<sup>−1</sup>(&#x1D706;). By testing various scenarios we conclude that computing 𝐷(&#x1D706;) in this manner indeed gives a correct effective dynamic in most cases, where the scale of coarse-graining can be tuned. However, an incorrect dynamic is received for example when the scale of coarse-graining is comparable to the size of oscillations in the energy landscape.</p>

Note error in oroginal, "et al" is missing a period and added math italics and superscript for equations
----------------------------------------------------------------------
In diva2:1438246 
abstract is: 
<p>In this thesis the eigenmodes and eigenvalues of three dimensional structures are analyzed using the Python environment FEniCS in combination with an implementation of the Arnoldi method in MATLAB for calculation of eigenpairs. This is done by considering separable solutions of the wave equation and subsequently expressing these as the solutions to an eigenvalue problem. The eigenvalue problem is then solved on two geometries inspired by two objects from Star Wars; the Death Star and a TIE fighter. To do this, the eigenvalue problem obtained from the wave equation is expressed in its weak form, also known as its variational form, and then discretized into a generalized eigenvalue problem. The eigenvalue problem is then solved approximately using the Arnoldi method, a method that can be used for finding approximate solutions to large and sparse eigenvalue problems. The main results are the plots of the eigenmodes of the two structures which are produced using the Python library vtkplotter.</p>

corrected abstract:
<p>In this thesis the eigenmodes and eigenvalues of three dimensional structures are analyzed using the Python environment FEniCS in combination with an implementation of the Arnoldi method in MATLAB for calculation of eigenpairs. This is done by considering separable solutions of the wave equation and subsequently expressing these as the solutions to an eigenvalue problem. The eigenvalue problem is then solved on two geometries inspired by two objects from Star Wars; the Death Star and a TIE fighter. To do this, the eigenvalue problem obtained from the wave equation is expressed in its weak form, also known as its variational form, and then discretized into a generalized eigenvalue problem. The eigenvalue problem is then solved approximately using the Arnoldi method, a method that can be used for finding approximate solutions to large and sparse eigenvalue problems. The main results are the plots of the eigenmodes of the two structures which are produced using the Python library vtkplotter.</p>
----------------------------------------------------------------------
In diva2:1435827 
abstract is: 
<p>The Bose-Einstein condensate is a phase of matter that arises when cooling gases of bosons to extremely low temperatures. When studying these condensates one may use the Gross-Pitaevskii equation, which is a non-linear variant of the Schrödinger equation. An interesting phenomenon that arises when rotating a Bose-Einstein condensate is the appearance of vortices. We implement a semi-implicit Euler scheme using spectral methods proposed in [1] to numerically calculate the ground state of a rotating Bose-Einstein condensate. We start with implementing a simpler iterative fixed-point method to solve the Euler scheme but show that this method fails to converge for large rotations. Because of this we implement multiple Krylov subspace solvers that in fact do converge for large rotations and show that the Preconditioned Conjugate Gradient method has better performance than the BiConjugate Gradient Stabilized method. After the implementation we briefly look at the performance of the method and improve it with simple tricks that do not compromise the accuracy or robustness and which reduce the computation time slightly. Lastly we look at the formation of vortices in 2-dimensional and 3-dimensional Bose-Einstein condensates. We show that the number of vortices increases exponentially for increasing angular velocity in 2D until the condensate breaks apart, but in 3D we ultimately find that the required computation time and RAM storage is too large to be able to analyze the vortices in a similar way on our personal computers.</p>

corrected abstract:
<p>The Bose-Einstein condensate is a phase of matter that arises when cooling gases of bosons to extremely low temperatures. When studying these condensates one may use the Gross-Pitaevskii equation, which is a non-linear variant of the Schrödinger equation. An interesting phenomenon that arises when rotating a Bose-Einstein condensate is the appearance of vortices.</p><p>We implement a semi-implicit Euler scheme using spectral methods proposed in [1] to numerically calculate the ground state of a rotating Bose-Einstein condensate. We start with implementing a simpler iterative fixed-point method to solve the Euler scheme but show that this method fails to converge for large rotations. Because of this we implement multiple Krylov subspace solvers that in fact do converge for large rotations and show that the Preconditioned Conjugate Gradient method has better performance than the BiConjugate Gradient Stabilized method. After the implementation we briefly look at the performance of the method and improve it with simple tricks that do not compromise the accuracy or robustness and which reduce the computation time slightly.</p><p>Lastly we look at the formation of vortices in 2-dimensional and 3-dimensional Bose-Einstein condensates. We show that the number of vortices increases exponentially for increasing angular velocity in 2D until the condensate breaks apart, but in 3D we ultimately find that the required computation time and RAM storage is too large to be able to analyze the vortices in a similar way on our personal computers.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1741196 
abstract is: 
<p>Planing hull is a common hull design concept which decreases the resistance while speed is increasing for a specific speed range. It is also suitable for hull modifications to achieve higher efficiency. Spray deflector is a promising hull modification which offers extra resistance decreasing and less vertical acceleration for planing hulls. Spray deflector technology can decrease the resistance up to 28% compared to the bare hull. However, the information on spray deflector design is strongly limited. In this study, there are two different types of spray deflector designs compared via CFD to achieve better design. Star CCM+ software was used to create CFD models with given numerical settings: 3-Dimensional, implicit unsteady, multiphase VOF, RANS based SST K-Omega turbulences model, all y+ Hybrid Wall Treatment while only considering heave and trim. Froude Number of the simulations ranges from 2 to 2.6. To improve the value of CFD models, mesh sensitivity study, time step study, y+ study, and alteration of prism layer number were conducted. The experimental base of this thesis is Molchanov’s “Experimental validation of spray deflectors' impact on the performance of high-speed planing” study from 2018. All CFD outcomes were evaluated according to these experiments. </p><p>There is a problem named numerical ventilation which downgrades the value of outcomes. Thus, three different methods were evaluated against numerical ventilation additionally to the spray deflector comparison. These methods are “Phase Replacement”, “Modified High-Resolution Interface Capturing Scheme”, and “Volume Fraction Source Term”. Application of Volume Fraction Source Term method gave the best achievements for the calculation of resistance with 0.35% error ratio, and trim angle 17% error ratio while causing 16% error ratio for heave. The modified HRIC scheme achieved a 1.4% error ratio for heave, 12.5% error ratio for resistance, and 20.4% error ratio for trim angle. The restrictions of these methods and their application ways are specified in this thesis.</p>

corrected abstract:
<p>Planing hull is a common hull design concept which decreases the resistance while speed is increasing for a specific speed range. It is also suitable for hull modifications to achieve higher efficiency. Spray deflector is a promising hull modification which offers extra resistance decreasing and less vertical acceleration for planing hulls. Spray deflector technology can decrease the resistance up to 28% compared to the bare hull. However, the information on spray deflector design is strongly limited. In this study, there are two different types of spray deflector designs compared via CFD to achieve better design. Star CCM+ software was used to create CFD models with given numerical settings: 3-Dimensional, implicit unsteady, multiphase VOF, RANS based SST K-Omega turbulences model, all y+ Hybrid Wall Treatment while only considering heave and trim. Froude Number of the simulations ranges from 2 to 2.6. To improve the value of CFD models, mesh sensitivity study, time step study, y+ study, and alteration of prism layer number were conducted. The experimental base of this thesis is Molchanov’s “Experimental validation of spray deflectors' impact on the performance of high-speed planing” study from 2018. All CFD outcomes were evaluated according to these experiments.</p><p>There is a problem named numerical ventilation which downgrades the value of outcomes. Thus, three different methods were evaluated against numerical ventilation additionally to the spray deflector comparison. These methods are “Phase Replacement”, “Modified High-Resolution Interface Capturing Scheme”, and “Volume Fraction Source Term”. Application of Volume Fraction Source Term method gave the best achievements for the calculation of resistance with 0.35% error ratio, and trim angle 17% error ratio while causing 16% error ratio for heave. The modified HRIC scheme achieved a 1.4% error ratio for heave, 12.5% error ratio for resistance, and 20.4% error ratio for trim angle. The restrictions of these methods and their application ways are specified in this thesis.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1720652   - correct as is
----------------------------------------------------------------------
In diva2:1127926   - correct as is
----------------------------------------------------------------------
In diva2:1572092 
abstract is: 
<p>In this report, the conceptual design of an unmanned aerial vehicle (UAV) and aerodynamic analysis is treated. The project was split into two groups, one group would do the aerodynamic analysis, and the other group would do the performance analysis. The plan was to create a UAV capable of surveying life stock, large farmlands, wildlife, and also reindeer husbandry. This demanded that the aircraft had to be able to easily launch from all types of locations. To solve this the plane was designed for vertical take-off and landing capabilities (VTOL). </p><p>The study includes the selection and performance testing of an airfoil, aerodynamic performance of the wing, and the wing's geometry. It also includes stability analysis, structural design, and CAD creation. The majority of this work was done by combining the usage of the design tool XFLR5 with CAD from Solid Edge and equations done in MATLAB. The aircraft accomplishes our goals to have it be VTOL functional. It has a flight time of over 2 hours and weighs less than 5 kg. Its cruise speed lies at 12 m/s. It is also possible to create a detailed design and to produce the aircraft with relative ease and low cost. Its dynamic stability is however not optimized and further work is needed if optimized stability is desired.</p>

corrected abstract:
<p>In this report, the conceptual design of an unmanned aerial vehicle (UAV) and aerodynamic analysis is treated. The project was split into two groups, one group would do the aerodynamic analysis, and the other group would do the performance analysis. The plan was to create a UAV capable of surveying life stock, large farmlands, wildlife, and also reindeer husbandry. This demanded that the aircraft had to be able to easily launch from all types of locations. To solve this the plane was designed for vertical take-off and landing capabilities (VTOL).</p><p>The study includes the selection and performance testing of an airfoil, aerodynamic performance of the wing, and the wing's geometry. It also includes stability analysis, structural design, and CAD creation. The majority of this work was done by combining the usage of the design tool XFLR5 with CAD from Solid Edge and equations done in MATLAB. The aircraft accomplishes our goals to have it be VTOL functional. It has a flight time of over 2 hours and weighs less than 5 kg. Its cruise speed lies at 12 m/s. It is also possible to create a detailed design and to produce the aircraft with relative ease and low cost. Its dynamic stability is however not optimized and further work is needed if optimized stability is desired.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1572108   - correct as is
----------------------------------------------------------------------
In diva2:1130018   - correct as is
----------------------------------------------------------------------
In diva2:1599577 
abstract is: 
<p>The objective of this study was to design a launch vehicle capable of deploying a nanosatellite into a Sun-synchronous orbit at 500 km orbital altitude from the JAS 39E/F Gripen fighter aircraft. This was achieved by first performing theoretical calculations for the required nozzles and solid propellant grain configurations for the first two solid stages, followed by the necessary liquid propellant configuration for the third stage. Lastly, two methods were investigated in solving the trajectory ascent problem for the launch vehicle design. First, by stating the trajectory problem as an initial value problem while guessing a Sigmoidal steering law. Secondly, by stating the trajectory problem as a boundary value problem. The latter was solved by transcribing the trajectory problem into a nonlinear program where a parametric steering law was derived using a Sequential quadratic programming algorithm.Ultimately, resulting in a launch vehicle design with a gross lift-off mass of 1,289 kg, capable of launching an 8.4 kg payload into the targeted orbit, with suggested modifications to increase the possible payload mass to 12.9 kg.</p>

mc='algorithm.Ultimately' c='algorithm. Ultimately'

corrected abstract:
<p>The objective of this study was to design a launch vehicle capable of deploying a nanosatellite into a Sun-synchronous orbit at 500 km orbital altitude from the JAS 39E/F Gripen fighter aircraft. This was achieved by first performing theoretical calculations for the required nozzles and solid propellant grain configurations for the first two solid stages, followed by the necessary liquid propellant configuration for the third stage. Lastly, two methods were investigated in solving the trajectory ascent problem for the launch vehicle design. First, by stating the trajectory problem as an initial value problem while guessing a Sigmoidal steering law. Secondly, by stating the trajectory problem as a boundary value problem. The latter was solved by transcribing the trajectory problem into a nonlinear program where a parametric steering law was derived using a Sequential quadratic programming algorithm. Ultimately, resulting in a launch vehicle design with a gross lift-off mass of 1,289 kg, capable of launching an 8.4 kg payload into the targeted orbit, with suggested modifications to increase the possible payload mass to 12.9 kg.</p>

Note change due to unmerging words
----------------------------------------------------------------------
In diva2:1740138   - correct as is
----------------------------------------------------------------------
In diva2:1360752 
abstract is: 
<p>The Swedish Navy has been using the AUV62-AT for submarine hunting training successfully for a while and it has been found that the biggest drawback is the transport which is very resource draining. The Swedish Defence Materiel Administration was given the task to solve this and a hydrofoiling-trailer was seen as a potential solution. This thesis will present the design process of the foil-trailer as well as issues found and how theses were mitigated, to produce a viable design. To develop the final design both current use of hydrofoil vessels and further adaptions needed for this applications were investigated. The project was done in collaboration with Simon Källerfelt Korall, who has during the project investigated in detail the foil-trailer’s roll stability, presents this in "The development of a foiling trailer for transport of the AUV62-AT"[1], which deals with how the experimental- and model results lines up. The result is a final design of a foil-trailer which is constituted of several sub-systems that has been found to improve the overall concept. It was concluded that the concept was viable and if further developed it can be used as a great method of saving time when deploying the AUV62-AT.</p>

corrected abstract:
<p>The Swedish Navy has been using the AUV62-AT for submarine hunting training successfully for a while and it has been found that the biggest drawback is the transport which is very resource draining. The Swedish Defence Materiel Administration was given the task to solve this and a hydrofoiling-trailer was seen as a potential solution.</p><p>This thesis will present the design process of the foil-trailer as well as issues found and how theses were mitigated, to produce a viable design. To develop the final design both current use of hydrofoil vessels and further adaptions needed for this applications were investigated. The project was done in collaboration with Simon Källerfelt Korall, who has during the project investigated in detail the foil-trailer’s roll stability, presents this in "The development of a foiling trailer for transport of the AUV62-AT"[1], which deals with how the experimental- and model results lines up.</p><p>The result is a final design of a foil-trailer which is constituted of several sub-systems that has been found to improve the overall concept. It was concluded that the concept was viable and if further developed it can be used as a great method of saving time when deploying the AUV62-AT.</p>

Note - added missing paragrap breaks
----------------------------------------------------------------------
In diva2:1499358   - correct as is
----------------------------------------------------------------------
In diva2:550530 - note title is missing registered trademark symbol:
"Conceptual Simulator Implementation of Flapping Wing Micro Air Vehicle Using FLAMES"
==>
"Conceptual Simulator Implementation of Flapping Wing Micro Air Vehicle Using FLAMES®"

abstract is: 
<p>The interest for Flapping Wing Micro Air Vehicles (FWMAV) is growing. With this comes a need for future users to test and evaluate these vehicles with simulations. This study presents a first iteration of an implementation of a FWMAV in FLAMES Simulation Framework. An aerodynamic model based on the widely used quasi-steady blade element approach is presented and a first linearised version is implemented in FLAMES. The simulation model is capable of both manual and autonomous flight. With the first person view a pilot can investigate buildings and objects. The study gives an idea of how FLAMES can be used for this type of simulations. FLAMES provides a good environment for testing the vehicle both individually and in a context with other units.</p>

corrected abstract:
<p>The interest for Flapping Wing Micro Air Vehicles (FWMAV) is growing. With this comes a need for future users to test and evaluate these vehicles with simulations. This study presents a first iteration of an implementation of a FWMAV in FLAMES Simulation Framework. An aerodynamic model based on the widely used quasi-steady blade element approach is presented and a first linearised version is implemented in FLAMES. The simulation model is capable of both manual and autonomous flight. With the first person view a pilot can investigate buildings and objects. The study gives an idea of how FLAMES can be used for this type of simulations. FLAMES provides a good environment for testing the vehicle both individually and in a context with other units.</p>
----------------------------------------------------------------------
In diva2:1900954 
abstract is: 
<p>This study presents a preliminary launch abort concept for the Nyx Earth Crew spacecraft developed at The Exploration Company. A requirements study of general human mission certification focusing on abort requirements is followed by a more detailed assessment of abort needs, interfaces and performance. Assuming launch with the Ariane 6 from French Guiana, a total of nine abort modes, three major abort criteria, seven success parameters, a layout of an abort control system and a mission sequence are defined, including an assessment of the most substantial risks during an abort manoeuvre. Five driving abort requirements were identified in the areas of safety, performance, reusability, in-house development and system complexity. Based on this extensive analysis, three different abort architectures with two design variations each are proposed: A "puller tower" similar to the one used on the Orion spacecraft, a "pusher engine" similar to the one used on the Crew Dragon spacecraft and an architecture using solid rocket boosters which does not yet exist for abort purposes. An architecture trade-off is performed by means of studying the components, accommodation, propulsion system and driving requirements of each architecture. The new hybrid booster concept is deemed the most feasible as it combines several advantages of both the pusher and puller system, namely a lightweight, jettisoned and simple solid propulsion system. Nonetheless, given that this concept has no abort heritage, several analyses have to be conducted to determine whether this concept is technically feasible, in particular in terms of continuous abort capability, booster jettison and potential reusability. The most feasible alternative to be further investigated was identified as the pusher engine, given potential synergies with the main propulsion system and advantages in terms of reusability. The puller tower is proposed to be discarded as a potential launch abort system for Nyx Earth Crew given its excessive mass and high sequential complexity during an abort. Based on this preliminary trade-off, a development plan is presented which spans six years from kick-off in 2025 to maiden launch end of 2030. The work breakdown structure includes preliminary design focusing on propellants and nozzles, detailed design with an engineering model containing all subsystems and qualification with two models used for pad and in-flight abort testing, leading up to the flight model in 2030. A cost analysis of this development cycle resulted in 180 - 200M € non-recurring development costs driven by qualification testing and 25 - 50M € recurring flight costs driven by the added launch mass.</p>


corrected abstract:
<p>This study presents a preliminary launch abort concept for the Nyx Earth Crew spacecraft developed at The Exploration Company. A requirements study of general human mission certification focusing on abort requirements is followed by a more detailed assessment of abort needs, interfaces and performance. Assuming launch with the Ariane 6 from French Guiana, a total of nine abort modes, three major abort criteria, seven success parameters, a layout of an abort control system and a mission sequence are defined, including an assessment of the most substantial risks during an abort manoeuvre. Five driving abort requirements were identified in the areas of safety, performance, reusability, in-house development and system complexity. Based on this extensive analysis, three different abort architectures with two design variations each are proposed: A ”puller tower” similar to the one used on the Orion spacecraft, a ”pusher engine” similar to the one used on the Crew Dragon spacecraft and an architecture using solid rocket boosters which does not yet exist for abort purposes. An architecture trade-off is performed by means of studying the components, accommodation, propulsion system and driving requirements of each architecture. The new hybrid booster concept is deemed the most feasible as it combines several advantages of both the pusher and puller system, namely a lightweight, jettisoned and simple solid propulsion system. Nonetheless, given that this concept has no abort heritage, several analyses have to be conducted to determine whether this concept is technically feasible, in particular in terms of continuous abort capability, booster jettison and potential reusability. The most feasible alternative to be further investigated was identified as the pusher engine, given potential synergies with the main propulsion system and advantages in terms of reusability. The puller tower is proposed to be discarded as a potential launch abort system for Nyx Earth Crew given its excessive mass and high sequential complexity during an abort. Based on this preliminary trade-off, a development plan is presented which spans six years from kick-off in 2025 to maiden launch end of 2030. The work breakdown structure includes preliminary design focusing on propellants and nozzles, detailed design with an engineering model containing all subsystems and qualification with two models used for pad and in-flight abort testing, leading up to the flight model in 2030. A cost analysis of this development cycle resulted in 180 - 200M € non-recurring development costs driven by qualification testing and 25 - 50M € recurring flight costs driven by the added launch mass.</p>

Note - only change was to make the double quotes match those of the original
----------------------------------------------------------------------
In diva2:1436882 
abstract is: 
<p>The Functional Materials Division of KTH conducts research on CDI technology for water desalination. A control unit is being designed and requires measurement of salinity which relates to electrolytic conductivity. Hence, a sensor developed by Innovative Sensor Technology (iST) for electrolytic conductivity was studied. In this report a starting point for integration of the sensor to the control unit is examined. The main focuses are sensor input voltage, output current, and circuit evaluation. Tests have been made in sodium chloride solution in salinity ranges of 700-2200 mg/L. The output current range was 590-2200 μA RMS with 15 % error. Reasonable input voltages could not be verified in tests. Exemplary circuits from iST were built and tested experimentally. They were demonstrated to work when the sensor was not connected. The circuit parts that were tested for accuracy, showed a signal transmission accuracy of around 95 %, within certain input ranges. Valid future work and further improvements are suggested.</p>

corrected abstract:
<p>The Functional Materials Division of KTH conducts research on CDI technology for water desalination. A control unit is being designed and requires measurement of salinity which relates to electrolytic conductivity. Hence, a sensor developed by Innovative Sensor Technology (iST) for electrolytic conductivity was studied.</p><p>In this report a starting point for integration of the sensor to the control unit is examined. The main focuses are sensor input voltage, output current, and circuit evaluation.</p><p>Tests have been made in sodium chloride solution in salinity ranges of 700-2200 mg/L. The output current range was 590-2200 µA RMS with 15 % error. Reasonable input voltages could not be verified in tests.</p><p>Exemplary circuits from iST were built and tested experimentally. They were demonstrated to work when the sensor was not connected. The circuit parts that were tested for accuracy, showed a signal transmission accuracy of around 95 %, within certain input ranges. Valid future work and further improvements are suggested.</p>

Note - added missing paragrap h breaks and change to use the "µ" in the original (i.e., the micro sign)
----------------------------------------------------------------------
In diva2:766649   - correct as is
----------------------------------------------------------------------
In diva2:1663235   - correct as is
----------------------------------------------------------------------
In diva2:1867423 
abstract is: 
<p>Ligand-gated ion channels play an important role in electrochemical signal transduction across diverse organisms, yet their structural and functional intricacies are not fully understood. Particularly lacking is the knowledge of their response to variations in pH, an aspect necessary for understanding their physiological relevance and potential therapeutic targeting in neurological diseases. In this thesis project, I have investigated the mechanistic response of sTeLIC, a recently reported prokaryotic member of the pentameric ligand-gated ion channel family, to different environmental conditions. Using molecular dynamics simulations, a total of 16 different environmental conditions have been explored including variations in pH (neutral and alkaline), the presence and absence of calcium, and the inclusion of an electric field acting as an external driving force on charged atoms. The results reveal a comprehensive pH-sensing and gating mechanism involving key residues, notably E106 (on the β6 strand) and E160 (on loop F), and their local microenvironments. Additionally, an inhibitory mechanism for calcium is proposed, with E160 playing an important role. The simulations including an electric field has provided support for a non-conventional ion pathway through the pore. Collectively, these results offer insights into a mechanistic framework that may extend to other physiologically relevant systems, providing a foundation for further investigations and potential future therapeutic intervention.</p>

corrected abstract:
<p>Ligand-gated ion channels play an important role in electrochemical signal transduction across diverse organisms, yet their structural and functional intricacies are not fully understood. Particularly lacking is the knowledge of their response to variations in pH, an aspect necessary for understanding their physiological relevance and potential therapeutic targeting in neurological diseases. In this thesis project, I have investigated the mechanistic response of sTeLIC, a recently reported prokaryotic member of the pentameric ligand-gated ion channel family, to different environmental conditions. Using molecular dynamics simulations, a total of 16 different environmental conditions have been explored including variations in pH (neutral and alkaline), the presence and absence of calcium, and the inclusion of an electric field acting as an external driving force on charged atoms. The results reveal a comprehensive pH-sensing and gating mechanism involving key residues, notably E106 (on the β6 strand) and E160 (on loop F), and their local microenvironments. Additionally, an inhibitory mechanism for calcium is proposed, with E160 playing an important role. The simulations including an electric field has provided support for a non-conventional ion pathway through the pore. Collectively, these results offer insights into a mechanistic framework that may extend to other physiologically relevant systems, providing a foundation for further investigations and potential future therapeutic interventions.</p>


Note spelling error:
'intervention' = => 'interventions'
----------------------------------------------------------------------
In diva2:1800496 - no absract in thesis! I'm not sure where the abstract in DiVA came from
----------------------------------------------------------------------
In diva2:1678955 
abstract is: 
<p>Dark matter, the unknown matter that constitutes 85% of all matter in the universe, is one of the greatest mysteries in fundamental physics. One theory that might explain dark matter predicts that there are long-lived particles known as dark pions. If these were created in a particle accelerator, they could decay inside the detector, resulting in particles that seemingly "emerge" from nothing. This phenomenon is known as emerging jets. In this study, emerging jets are simulated with various values of the dark pion average lifetime, dark pion mass, and mediator particle mass. These simulations are compared with a search for displaced vertices conducted by the ATLAS collaboration, allowing one to reinterpret the ATLAS results to constrain the parameter values that the emerging-jets model can have. This study simulates and constrains the allowed values for the dark pion mass, dark pion average life time and mediator mass with 95% confidence level. This is the first study to use results from the ATLAS experiment to constrain the emerging-jets model, as well as the first study to exclude this region of the parameter space.</p>

corrected abstract:
<p>Dark matter, the unknown matter that constitutes 85% of all matter in the universe, is one of the greatest mysteries in fundamental physics. One theory that might explain dark matter predicts that there are long-lived particles known as dark pions. If these were created in a particle accelerator, they could decay inside the detector, resulting in particles that seemingly “emerge” from nothing. This phenomenon is known as emerging jets. In this study, emerging jets are simulated with various values of the dark pion average lifetime, dark pion mass, and mediator particle mass. These simulations are compared with a search for displaced vertices conducted by the ATLAS collaboration, allowing one to reinterpret the ATLAS results to constrain the parameter values that the emerging-jets model can have. This study simulates and constrains the allowed values for the dark pion mass, dark pion average life time and mediator mass with 95% confidence level. This is the first study to use results from the ATLAS experiment to constrain the emerging-jets model, as well as the first study to exclude this region of the parameter space.</p>

Note - only change was to make the double quotation marks match the original
----------------------------------------------------------------------
In diva2:1658894 
abstract is: 
<p>Hedge funds use a variety of different financial instruments in order to try to achieve over-average returns without taking on excessive risk - options being one of the most common of these instruments. Basket options is a type of option that is written on several underlying assets that can be used to hedge risky positions. This project has been working together with the hedge fund Proxy P to develop software to construct basket options and to analyze their use as a hedging strategy.</p><p>Construction of basket options can be performed through the use of several different mathematical models. These models range from complex continuous models, such as Monte Carlo simulations, to simple discrete models, such as the binomial option pricing model. In this project, the binomial option pricing model was chosen as the main tool to determine some quantities of basket options. It can conveniently handle both European and American options, independently of whether these are put or call options. The quantities calculated, the option price and option Delta, are dependent on the volatility and the initial price of the underlying. When evaluating the basket option there are two key assumptions that need to be studied. These key assumptions are if the weights and the initial price of the underlying change with each time step, or if they are held constant. It was found that both the weights and the price of the underlying should change dynamically with each time step.</p><p>Furthermore, in order to evaluate the performance of the basket options used as a hedge, the project used historical data and measured how the options neutralized negative movements in the underlying. This was done through the use of the option Delta and the hedge ratio. What could be concluded was that the put basket option can serve as a relatively inexpensive hedge and minimize the risk on the downside in a sufficient matter.</p>

corrected abstract:
<p>Hedge funds use a variety of different financial instruments in order to try to achieve overaverage returns without taking on excessive risk - options being one of the most common of these instruments. Basket options is a type of option that is written on several underlying assets that can be used to hedge risky positions. This project has been working together with the hedge fund Proxy P to develop software to construct basket options and to analyze their use as a hedging strategy.</p><p>Construction of basket options can be performed through the use of several different mathematical models. These models range from complex continuous models, such as Monte Carlo simulations, to simple discrete models, such as the binomial option pricing model. In this project, the binomial option pricing model was chosen as the main tool to determine some quantities of basket options. It can conveniently handle both European and American options, independently of whether these are put or call options. The quantities calculated, the option price and option Delta, are dependent on the volatility and the initial price of the underlying. When evaluating the basket option there are two key assumptions that need to be studied. These key assumptions are if the weights and the initial price of the underlying change with each time step, or if they are held constant. It was found that both the weights and the price of the underlying should change dynamically with each time step.</p><p>Furthermore, in order to evaluate the performance of the basket options used as a hedge, the project used historical data and measured how the options neutralized negative movements in the underlying. This was done through the use of the option Delta and the hedge ratio. What could be concluded was that the put basket option can serve as a relatively inexpensive hedge and minimize the risk on the downside in a sufficient matter.</p>

Note handle word split at end of line with hyphenation:
'over-average' ==> 'overaverage'
----------------------------------------------------------------------
In diva2:1878793   - correct as is
----------------------------------------------------------------------
In diva2:1145360   - correct as is
----------------------------------------------------------------------
In diva2:448845 
abstract is: 
<p>Abstrakt:</p><p>This report examines different control design methods, linear as well as nonlinear, for a suborbital reusable launch vehicle. An investigation of the natural vehicle behavior is made, after which a baseline linear controller is constructed to fulfill certain handling quality criteria. Thereafter the nonlinear cascade control methods block backstepping and nonlinear dynamic inversion via time scale separation are examined and used to construct two nonlinear controllers for the vehicle. Optimal controllers, in terms of three different criteria, are found using the genetic optimization algorithm differential evolution. The optimal controllers are compared, and it is found that nonlinear dynamic inversion via time scale separation performs better than block backstepping with respect to the cases investigated. The results suggest control design by global optimization is a viable and promising complement to classical methods.</p>

corrected abstract:
<p>This report examines different control design methods, linear as well as nonlinear, for a suborbital reusable launch vehicle. An investigation of the natural vehicle behavior is made, after which a baseline linear controller is constructed to fulfill certain handling quality criteria. Thereafter the nonlinear cascade control methods block backstepping and nonlinear dynamic inversion via time scale separation are examined and used to construct two nonlinear controllers for the vehicle. Optimal controllers, in terms of three different criteria, are found using the genetic optimization algorithm differential evolution. The optimal controllers are compared, and it is found that nonlinear dynamic inversion via time scale separation performs better than block backstepping with respect to the cases investigated. The results suggest control design by global optimization is a viable and promising complement to classical methods.</p>

Note - only change was to remove "<p>Abstrakt:</p>"
----------------------------------------------------------------------
In diva2:1658831 
abstract is: 
<p>A numerical method for solving the buoyancy-driven magneto-convection equations in a rapidly rotating spherical shell is presented. The method is implemented through a FORTRAN 90 program, based on a FORTRAN 66 code written by Hollerbach [International Journal for Numerical Methods in Fluids, 32 (2000)] and partially translated in FORTRAN 90 by Riquier. The program uses the pseudo-spectral method and computes velocity as well as temperature fields in a rapidly rotating spherical shell. The code has been validated through comparisons with previous studies and parallelized using OpenMP. Comparisons with Hollerbach's method have been carried out and showed improvements in stability.</p>

corrected abstract:
<p>A numerical method for solving the buoyancy-driven magneto-convection equations in a rapidly rotating spherical shell is presented. The method is implemented through a FORTRAN 90 program, based on a FORTRAN 66 code written by Hollerbach [<em>International Journal for Numerical Methods in Fluids</em>, 32 (2000)] and partially translated in FORTRAN 90 by Riquier. The program uses the pseudo-spectral method and computes velocity as well as temperature fields in a rapidly rotating spherical shell. The code has been validated through comparisons with previous studies and parallelized using OpenMP. Comparisons with Hollerbach's method have been carried out and showed improvements in stability.</p>

Note - added italics
----------------------------------------------------------------------
In diva2:1431643 
abstract is: 
<p>This thesis evaluates how the evolutionary algorithm CMA-ES (Covariance Matrix Adaption Evolution Strategy) can be used for optimizing the total initial margin for a network of banks trading bilateral OTC derivatives. The algorithm is a stochastic method for optimization of non-linear and, but not limited to, non-convex functions. The algorithm searches for an optimum by generating normally distributed samples and iteratively updating the mean and covariance matrix of the search distribution using the best candidate solutions in the sampled population. In this thesis, feasible solutions are represented by the null space obtained from the constraint of keeping all banks' market exposure unchanged throughout the optimization, and the generated samples for each iteration correspond to linear combinations of the base vectors spanning this null space. In particular, this thesis investigates how different representations of this null space affect the convergence speed of the algorithm. By applying the algorithm to problems of varying sizes, using several different null space representations coming from different matrix decomposition methods, it is found that as long as an orthonormal representation is used it does not matter which matrix decomposition method it comes from. This is found to be because, given any orthonormal null space representation, the algorithm will at start generate a rotationally invariant sample space in its search for the optimal solution, independent of the specific null space representation. If the representation is not orthogonal, the initial sample will in contrast be in the shape of an ellipsoid and thus biased in certain directions, which in general affects the performance negatively. A non-orthonormal representation can converge faster in specific optimization problems, if the direction of the solution is known in advance and the sample space is pointed towards that direction. However, the benefit of this aspect is limited in a realistic scenario and an orthonormal representation is recommended. Furthermore, as it is shown that different orthonormal representations perform equally, it is implied that other characteristics can be considered when deciding which matrix decomposition method to use; such as the importance of fast computation or desire for a sparse representation.</p>

corrected abstract:
<p>This thesis evaluates how the evolutionary algorithm CMA-ES (<em>Covariance Matrix Adaption Evolution Strategy</em>) can be used for optimizing the total initial margin for a network of banks trading bilateral OTC derivatives. The algorithm is a stochastic method for optimization of non-linear and, but not limited to, non-convex functions. The algorithm searches for an optimum by generating normally distributed samples and iteratively updating the mean and covariance matrix of the search distribution using the best candidate solutions in the sampled population. In this thesis, feasible solutions are represented by the null space obtained from the constraint of keeping all banks' market exposure unchanged throughout the optimization, and the generated samples for each iteration correspond to linear combinations of the base vectors spanning this null space. In particular, this thesis investigates how different representations of this null space affect the convergence speed of the algorithm. By applying the algorithm to problems of varying sizes, using several different null space representations coming from different matrix decomposition methods, it is found that as long as an orthonormal representation is used it does not matter which matrix decomposition method it comes from. This is found to be because, given any orthonormal null space representation, the algorithm will at start generate a rotationally invariant sample space in its search for the optimal solution, independent of the specific null space representation. If the representation is not orthogonal, the initial sample will in contrast be in the shape of an ellipsoid and thus biased in certain directions, which in general affects the performance negatively. A non-orthonormal representation can converge faster in specific optimization problems, if the direction of the solution is known in advance and the sample space is pointed towards that direction. However, the benefit of this aspect is limited in a realistic scenario and an orthonormal representation is recommended. Furthermore, as it is shown that different orthonormal representations perform equally, it is implied that other characteristics can be considered when deciding which matrix decomposition method to use; such as the importance of fast computation or desire for a sparse representation.</p>

Note - added italics
----------------------------------------------------------------------
In diva2:1595241 
abstract is: 
<p>The thesis work described in this report is about simulation of the cooling of an electrical machine rotor. Limitations and simplifications were made on the CAD model of the rotor with the purpose of reducing the simulation time, for it to then be used for CFD-simulations using STAR-CCM+. This was done to see the temperature, as well as its distribution, in the model. By changing various parameters, one at the time whilst the rest were kept at their assigned standard values, the changes could be analysed and thereafter compared. The tests include smaller geometry changes, parameters of the coolant and its flow, parameters for the airgap and the materials in the laminates and the material around the magnets, as well as changes in loss values. The simulations for geometry changes involving the magnets and their surrounding material resulted in minor temperature increases. An inner rotor radius increase gave a relatively large temperature decrease (although this change would be more difficult to make in practice). Most of the mean values of the temperature changes in the regions of the model were within 10% from the standard simulation used. Increased thermal contact resistance between the Bakelite and the laminates, and increased losses had the worst impact on the cooling. Meanwhile the changes in coolant parameters (as well as the its inlet temperature and mass flow) and reduced losses had the best impact on the cooling. Generally, the temperature distributions looked similar for the different simulations. There were more differences in the distributions for the simulations with changed material properties or thermal contact resistance.</p>

corrected abstract:
<p>The thesis work described in this report is about simulation of the cooling of an electrical machine rotor. Limitations and simplifications were made on the CAD model of the rotor with the purpose of reducing the simulation time, for it to then be used for CFD-simulations using STAR-CCM+. This was done to see the temperature, as well as its distribution, in the model. By changing various parameters, one at the time whilst the rest were kept at their assigned standard values, the changes could be analysed and thereafter compared. The tests include smaller geometry changes, parameters of the coolant and its flow, parameters for the airgap and the materials in the laminates and the material around the magnets, as well as changes in loss values. The simulations for geometry changes involving the magnets and their surrounding material resulted in minor temperature increases. An inner rotor radius increase gave a relatively large temperature decrease (although this change would be more difficult to make in practice). Most of the mean values of the temperature changes in the regions of the model were within 10% from the standard simulation used. Increased thermal contact resistance between the Bakelite and the laminates, and increased losses had the worst impact on the cooling. Meanwhile the changes in coolant parameters (as well as the oil inlet temperature and mass flow) and reduced losses had the best impact on the cooling. Generally, the temperature distributions looked similar for the different simulations. There were a bit more differences in the distributions for the simulations with changed material properties or contact resistance.</p>

Note - some changes in wording to match the original
----------------------------------------------------------------------
In diva2:1800241 
abstract is: 
<p>This thesis treats the modelling of a high-dimensional data set of longitudinal binary responses. The data consists of default indicators from different nations around the world as well as some explanatory variables such as exposure to underlying assets. The data used for the modelling is an aggregated term which combines several of the default indicators in the data set into one. </p><p>The modelling sets out from a portfolio perspective and seeks to find underlying correlations between the nations in the data set as well as see the extreme values produced by a portfolio with assets in the nations in the data set. The modelling takes a copula approach which uses Gaussian copulas to first formulate several different models mathematically and then optimize the parameters in the models to best fit the data set. Models A and B are optimized using standard stochastic gradient ascent on the likelihood function while model C uses variational inference and stochastic gradient ascent on the evidence lower bound for optimization. Using the different Gaussian copulas obtained from the optimization process a portfolio simulation is then done to examine the extreme values.</p><p>The results show low correlations in models A and B while model C with it's additional regional correlations show slightly higher correlations in three of the subgroups. The portfolio simulations show similar tail behaviour in all three models, however model C produces more extreme risk measure outcomes in the form of higher VaR and ES.</p>

corrected abstract:
<p>This thesis treats the modelling of a high-dimensional data set of longitudinal binary responses. The data consists of default indicators from different nations around the world as well as some explanatory variables such as exposure to underlying assets. The data used for the modelling is an aggregated term which combines several of the default indicators in the data set into one.</p><p>The modelling sets out from a portfolio perspective and seeks to find underlying correlations between the nations in the data set as well as see the extreme values produced by a portfolio with assets in the nations in the data set. The modelling takes a copula approach which uses Gaussian copulas to first formulate several different models mathematically and then optimize the parameters in the models to best fit the data set. Models A and B are optimized using standard stochastic gradient ascent on the likelihood function while model C uses variational inference and stochastic gradient ascent on the evidence lower bound for optimization. Using the different Gaussian copulas obtained from the optimization process a portfolio simulation is then done to examine the extreme values.</p><p>The results show low correlations in models A and B while model C with it's additional regional correlations show slightly higher correlations in three of the subgroups. The portfolio simulations show similar tail behaviour in all three models, however model C produces more extreme risk measure outcomes in the form of higher VaR and ES.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1823877   - correct as is
----------------------------------------------------------------------
In diva2:1017681   - correct as is
----------------------------------------------------------------------
In diva2:1827798 
abstract is: 
<p>Parkinson's is a chronic, progressive, neurodegenerative disease. The most common treatment is levodopa/carbidopa, which suppresses the symptoms of the disease. In this report, a cost-utility analysis of the MyFID levodopa/carbidopa micro tablet dispenser has been conducted. The method used was a continuous-time Markov chain with states based on the score in MDS-UPDRS II and MDS-UPDRS III. The model was simulated over a time horizon of five years and was started at different disease severity levels. The results obtained from the simulations were a dominant ICER for all model versions except when the simulation was started in the earliest stages of the disease, where it was moderate to high. The conclusion was that the treatment method with the micro </p>

corrected abstract:
<p>Parkinson's is a chronic, progressive, neurodegenerative disease. The most common treatment is levodopa/carbidopa, which suppresses the symptoms of the disease. In this report, a cost-utility analysis of the MyFID levodopa/carbidopa micro tablet dispenser has been conducted. The method used was a continuous-time Markov chain with states based on the score in MDS-UPDRS II and MDS-UPDRS III. The model was simulated over a time horizon of five years and was started at different disease severity levels. The results obtained from the simulations were a dominant ICER for all model versions except when the simulation was started in the earliest stages of the disease, where it was moderate to high. The conclusion was that the treatment method with the micro tablet dispenser was cost-e↵ective in the moderate to later stages of the disease.</p>

Note - added missing text
----------------------------------------------------------------------
In diva2:1699445 
abstract is: 
<p>In calculating potential energy surfaces (PES) in quantum chemistry interpolation methods are sometimes used to get a sufficiently approximated surface. Kowalewski, Larsson and Heryudono (J Chem Phys, 148(8):084104) has developed an adaptive interpolation method to approximate a PES by polyharmonic splines interpolation. They show that this method is effective for this purpose, however in this method they did not use information about stationary points of the PES. The thesis will investigate how useful it is to incorporate stationary points in the interpolation scheme in one dimension. We start by going over the proposed method and some background in Hermite-Birkhoff interpolation. After this it is shown how the interpolation scheme is changed and how the stationary points are incorporated into it. In the result section we look at the Morse-potential and an approximate double-well potential. For some error tolerances we get a reduction in the amount of points needed to adequately calculate the functions with stationary points and Hermite-Birkhoff interpolation compared to without stationary points, showing an improvement in the method.</p>

corrected abstract:
<p>In calculating potential energy surfaces (PES) in quantum chemistry interpolation methods are sometimes used to get a sufficiently approximated surface. Kowalewski, Larsson and Heryudono (J Chem Phys, 148(8):084104) has developed an adaptive interpolation method to approximate a PES by polyharmonic splines interpolation. They show that this method is effective for this purpose, however in this method they did not use information about stationary points of the PES. This thesis will investigate how useful it is to incorporate stationary points in the interpolation scheme in one dimension. We start by going over the proposed method and some background in Hermite-Birkhoff interpolation. After this it is shown how the interpolation scheme is changed and how the stationary points are incorporated into it. In the result section we look at the Morse-potential and an approximate double-well potential. For some error tolerances we get a reduction in the amount of points needed to adequately calculate the functions with stationary points and Hermite-Birkhoff interpolation compared to without stationary points, showing an improvement in the method.</p>

Note corrected "The" to "This" - as per the original
----------------------------------------------------------------------
In diva2:1185917 
abstract is: 
<p>The following thesis contains an extensive account of the theory of class groups. First the form class group is introduced through equivalence classes of certain integral binary quadratic forms with a given discriminant. The sets of classes is then turned into a group through an operation referred to as "composition''. Then the ideal class group is introduced through classes of fractional ideals in the ring of integers of quadratic fields with a given discriminant. It is then shown that for negative fundamental discriminants, the ideal class group and form class group are isomorphic. Some concrete computations are then done, after which some of the most central conjectures concerning the average behaviour of class groups with discriminant less than $X$ -- the Cohen-Lenstra heuristics -- are stated and motivated. The thesis ends with a sketch of a proof by Bob Hough of a strong result related to a special case of the Cohen-Lenstra heuristics.</p>

corrected abstract:
<p>The following thesis contains an extensive account of the theory of class groups. First the form class group is introduced through equivalence classes of certain integral binary quadratic forms with a given discriminant. The sets of classes is then turned into a group through an operation referred to as “composition”. Then the ideal class group is introduced through classes of fractional ideals in the ring of integers of quadratic fields with a given discriminant. It is then shown that for negative fundamental discriminants, the ideal class group and form class group are isomorphic. Some concrete computations are then done, after which some of the most central conjectures concerning the average behaviour of class groups with discriminant less than 𝑋 &mdash; the Cohen-Lenstra heuristics &mdash; are stated and motivated. The thesis ends with a sketch of a proof by Bob Hough of a strong result related to a special case of the Cohen-Lenstra heuristics.</p>

Note - changed "X" to "𝑋" and changed "--" to "&mdash;".
----------------------------------------------------------------------
In diva2:1082698 
abstract is: 
<p>Marine vessels designed to be self-propelled are generally unstable when towed. Submarines are not an exception; holding a course while towing a surfaced submarine is a challenging operation that often requires several tug boats or special methods. The Swedish navy’s submarine rescue vehicle URF, for example, is directionally unstable when being towed at lower speeds, and this report examines methods of improving URF’s course stability under these circumstances. An experimental evaluation was conducted to assess the effect of static trim angle on URF’s course stability; by adjusting URF’s trim angle, the center of pressure can be shifted in a way that is favourable to course stability. A <em>1</em>:<em>19 </em>scale model was developed and towed in calm water at equivalent full-scale speeds of <em>2 </em>to <em>8 </em>knots and at trim angles between <em>0 </em>and <em>15 </em>degrees. Course stability was assessed on the basis of the model’s observed behaviour during towing, including the model’s maximum angle during wandering, stable angle, tendency to dive and behaviour upon sudden release of the towline.</p>

corrected abstract:
<p>Marine vessels designed to be self-propelled are generally unstable when towed. Submarines are not an exception; holding a course while towing a surfaced submarine is a challenging operation that often requires several tug boats or special methods. The Swedish navy’s submarine rescue vehicle URF, for example, is directionally unstable when being towed at lower speeds, and this report examines methods of improving URF’s course stability under these circumstances. An experimental evaluation was conducted to assess the effect of static trim angle on URF’s course stability; by adjusting URF’s trim angle, the center of pressure can be shifted in a way that is favourable to course stability. A 1:19 scale model was developed and towed in calm water at equivalent full-scale speeds of 2 to 8 knots and at trim angles between 0 and 15 degrees. Course stability was assessed on the basis of the model’s observed behaviour during towing, including the model’s maximum angle during wandering, stable angle, tendency to dive and behaviour upon sudden release of the towline.</p>

Note - I don't think that the numbers need to be in italics as the whole abstract is in italics, so there is not specific emphasis on the digits.
----------------------------------------------------------------------
In diva2:556595 
abstract is: 
<p>Abstract</p><p>With the ever-increasing complexity of financial markets and financial products, many investors now choose to benefit from a manager’s expertise by investing in a fund. This fueled a rapid growth of the fund industry over the past decades, and the recent emergence of complex derivatives products written on underlying funds. The diversity (hedge funds, mutual funds, funds of funds, managed accounts…) and the particularities (liquidity, specific risks) of funds call for adapted models and suited risk management. This thesis aims at understanding the issues and difficulties met when dealing with such products. In particular, we will deal in a great extent with CPPI (Constant Proportion Portfolio Insurance) structures written on funds, which combine the specificities of funds with particularities of such structures. Correctly assessing the corresponding market risks is a challenging issue, and is the subject of many investigations.</p>

corrected abstract:
<p>With the ever-increasing complexity of financial markets and financial products, many investors now choose to benefit from a manager’s expertise by investing in a fund. This fueled a rapid growth of the fund industry over the past decades, and the recent emergence of complex derivatives products written on underlying funds. The diversity (hedge funds, mutual funds, funds of funds, managed accounts…) and the particularities (liquidity, specific risks) of funds call for adapted models and suited risk management.</p><p>This thesis aims at understanding the issues and difficulties met when dealing with such products. In particular, we will deal in a great extent with CPPI (Constant Proportion Portfolio Insurance) structures written on funds, which combine the specificities of funds with particularities of such structures. Correctly assessing the corresponding market risks is a challenging issue, and is the subject of many investigations.</p>

Note - removed "<p>Abstract</p>"
added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1576953   - correct as is
----------------------------------------------------------------------
In diva2:751508   - correct as is
----------------------------------------------------------------------
In diva2:1642581   - correct as is
----------------------------------------------------------------------
In diva2:1319906 
abstract is: 
<p>In order to ensure solvency, financial institutions must evaluate their credit risk exposure and determine how much economic capital is required to hold as a cushion. This thesis compares three factor models, namely Asymptotic Single Risk Factor (“ASRF”), Inter-sector and Intra-sector factor models and evaluates how their different characteristics affect the economic capital outcomes. The thesis also investigates how these outcomes are affected when assuming asset dependency through a Student's-$t$ copula. Focus will also be put on how different types and levels of asset correlation affect the models' credit risk results. We use a fictitious loan portfolio consisting of 138 Swedish firms with equity data from between 2007 and 2019 in order to calculate asset correlations and economic capital. Our main findings are that the asset correlations severely impact the outcomes of the credit risk models and that practitioners must calibrate and stress test their models regularly with respect to how correlations vary between different firms. The thesis also finds that using copulas for credit portfolios provides more conservative risk outcomes but makes the models less sensitive to correlation level input.</p>

corrected abstract:
<p>In order to ensure solvency, financial institutions must evaluate their credit risk exposure and determine how much economic capital is required to hold as a cushion. This thesis compares three factor models, namely Asymptotic Single Risk Factor, Inter-sector and Intra-sector factor models and evaluates how their different characteristics affect the economic capital outcomes. The thesis also investigates how these outcomes are affected when assuming asset dependency through a Student's-𝑡 copula. Focus will also be put on how different types and levels of asset correlation affect the models' credit risk results. We use a fictitious loan portfolio consisting of 138 Swedish firms with equity data from between 2007 and 2019 in order to calculate asset correlations and economic capital. Our main findings are that the asset correlations severely impact the outcomes of the credit risk models and that practitioners must calibrate and stress test their models regularly with respect to how correlations vary between different firms. The thesis also finds that using copulas for credit portfolios provides more conservative risk outcomes but makes the models less sensitive to correlation level input.</p>

Note - removed '(“ASRF”)' as this is not in the original - replaced "t" with "𝑡"
----------------------------------------------------------------------
In diva2:852714 
abstract is: 
<p>In this thesis, the pricing of counterparty credit risk on an OTC plain vanilla interest rate swap is investigated. Counterparty credit risk can be defined as the risk that a counterparty in a financial contract might not be able or willing to fulfil their obligations. This risk has to be taken into account in the valuation of an OTC derivative. The market price of the counterparty credit risk is known as the Credit Value Adjustment (CVA). In a bilateral contract, such as a swap, the party’s own creditworthiness also has to be taken into account, leading to another adjustment known as the Debit Value Adjustment (DVA). Since 2013, the international accounting standards (IFRS) states that these adjustments have to be done in order to reflect the fair value of an OTC derivative.</p><p>A short background and the derivation of CVA and DVA is presented, including related topics like various risk mitigation techniques, hedging of CVA, regulations etc.. Four different pricing frameworks are compared, two more sophisticated frameworks and two approximative approaches. The most complex framework includes an interest rate model in form of the LIBOR Market Model and a credit model in form of the Cox-Ingersoll- Ross model. In this framework, the impact of dependencies between credit and market risk factors (leading to wrong-way/right-way risk) and the dependence between the default time of different parties are investigated.</p>

corrected abstract:
<p>In this thesis, the pricing of counterparty credit risk on an OTC plain vanilla interest rate swap is investigated. Counterparty credit risk can be defined as the risk that a counterparty in a financial contract might not be able or willing to fulfil their obligations. This risk has to be taken into account in the valuation of an OTC derivative. The market price of the counterparty credit risk is known as the Credit Value Adjustment (CVA). In a bilateral contract, such as a swap, the party’s own creditworthiness also has to be taken into account, leading to another adjustment known as the Debit Value Adjustment (DVA). Since 2013, the international accounting standards (IFRS) states that these adjustments have to be done in order to reflect the fair value of an OTC derivative.</p><p>A short background and the derivation of CVA and DVA is presented, including related topics like various risk mitigation techniques, hedging of CVA, regulations etc.. Four different pricing frameworks are compared, two more sophisticated frameworks and two approximative approaches. The most complex framework includes an interest rate model in form of the LIBOR Market Model and a credit model in form of the Cox-Ingersoll-Ross model. In this framework, the impact of dependencies between credit and market risk factors (leading to wrong-way/right-way risk) and the dependence between the default time of different parties are investigated.</p>

Note - removed unnecessary space after a hyphen
----------------------------------------------------------------------
In diva2:1827867   - correct as is
----------------------------------------------------------------------
In diva2:653271   - correct as is
----------------------------------------------------------------------
In diva2:1221512   - correct as is
----------------------------------------------------------------------
In diva2:302193   - correct as is
----------------------------------------------------------------------
In diva2:1057226 
abstract is: 
<p>This report is a Master’s Thesis in Aerospace Engineering, performed at the NASA Ames Research Center. It describes the development of the CubeSub, a submersible testbed compatible with the CubeSat form factor. The CubeSub will be used to mature technology and operational procedures to be used in space exploration, and possibly also as a tool for exploration of Earthly environments. CubeSats are carried as payloads, either containing technology to be tested or experiments and sensors for scientific use.</p><p>The CubeSub is designed to be built up by modules, which can be assembled in different configurations to fulfill different needs. Each module is powered individually and intermodular communication is wireless, reducing the need for wiring. The inside of the hull is flooded with ambient water to simplify the interaction between payloads and surrounding environment. The overall torpedo-like shape is similar to that of a conventional AUV, slender and smooth. This is to make for a low drag, reduce the risk of snagging on surrounding objects and make it possible to deploy through an ice sheet via a narrow borehole. Rapid prototyping is utilized wherever possible. Full-scale prototypes have been constructed through 3D-printing and using COTS (Commercial Off-The-Shelf) components. Arduino boards are used for control and internal communication.</p><p>Modules required for basic operation have been designed, manufactured and tested. Each module is described with regards to its function, design and manufacturability. By performing tests in a pool it was found that the basic concept is sound and that future improvements include better controllability, course stability and waterproofing of electrical components. Further development is needed to make the CubeSub usable for its intended purposes. The largest gains are expected to be found by developing the software and improving controllability.</p>

corrected abstract:
<p>This report is a Master’s Thesis in Aerospace Engineering, performed at the NASA Ames Research Center. It describes the development of the CubeSub, a submersible testbed compatible with the CubeSat form factor. The CubeSub will be used to mature technology and operational procedures to be used in space exploration, and possibly also as a tool for exploration of Earthly environments. CubeSats are carried as payloads, either containing technology to be tested or experiments and sensors for scientific use.</p><p>The CubeSub is designed to be built up by modules, which can be assembled in different configurations to fulfill different needs. Each module is powered individually and intermodular communication is wireless, reducing the need for wiring. The inside of the hull is flooded with ambient water to simplify the interaction between payloads and surrounding environment. The overall torpedo-like shape is similar to that of a conventional AUV, slender and smooth. This is to make for a low drag, reduce the risk of snagging on surrounding objects and make it possible to deploy through an ice sheet via a narrow borehole. Rapid prototyping is utilized wherever possible. Full-scale prototypes have been constructed through 3D-printing and using COTS (Commercial Off-The-Shelf) components. Arduino boards are used for control and internal communication.</p><p>Modules required for basic operation have been designed, manufactured and tested. Each module is described with regards to its function, design and manufacturability. By performing tests in a pool it was found that the basic concept is sound and that future improvements include better controllability, course stability and waterproofing of electrical components.</p><p>Further development is needed to make the CubeSub usable for its intended purposes. The largest gains are expected to be found by developing the software and improving controllability.</p>

Note - added missing paragraph break
----------------------------------------------------------------------
In diva2:622287   - correct as is
----------------------------------------------------------------------
In diva2:1229172   - correct as is
----------------------------------------------------------------------
In diva2:848322   - correct as is
----------------------------------------------------------------------
In diva2:1082032 
abstract is: 
<p>Nowadays, the developed diagnostics models and software are not capable of locating the root cause of an emerging malfunction, or in other words the responsible component, while the vehicle is up and running. In most cases they are solely able to provide the driver with indications that a fault has been detected within a group of components. Subsequently, it is unavoidable that the vehicle returns to the workshop for a number of standardized tests to be performed, in order to evaluate the condition of the potentially faulty components. The new era in combustion engines and the attempt to fully incorporate closed-loop combustion control can facilitate the diagnostics procedure and especially the process of fault isolation. By harnessing signals from both real and virtual sensors, it can be feasible to diagnose or even prognose faults, averting the return of the vehicle to the workshop. Moreover, the down-time of the vehicle, can be radically decreased, since there will be an indication on which components to focus. Taking into account the fastpace steps and improvements on the respective hardware, such as sensors, one can understand that this endeavour can actually be successful in the future. In the spectrum of this thesis it is assessed whether or not fault detection and isolation can be achieved, through comparison of sensors’ output signals for a number of engine parameters to a stored set of nominal values for these parameters (reference values). Towards that goal, virtual sensors have been developed with the aid of measurement data, in order to increase the reliability of the system.</p><p>Subsequently, a network of dependencies between parameter values and consequent malfunctions has been constructed, in the form of flowcharts, rudimental for fault isolation. In addition to that and despite the fact that no finalized production code for the model is provided, pseudocode charts have been created as well. Finally, significant effort was made to derive precise tolerances for the reference values, as this is of great importance for the results of the diagnostics model.</p>
mc='fastpace' c='fast pace'

corrected abstract:
<p>Nowadays, the developed diagnostics models and software are not capable of locating the root cause of an emerging malfunction, or in other words the responsible component, while the vehicle is up and running. In most cases they are solely able to provide the driver with indications that a fault has been detected within a group of components. Subsequently, it is unavoidable that the vehicle returns to the workshop for a number of standardized tests to be performed, in order to evaluate the condition of the potentially faulty components. The new era in combustion engines and the attempt to fully incorporate closed-loop combustion control can facilitate the diagnostics procedure and especially the process of fault isolation. By harnessing signals from both real and virtual sensors, it can be feasible to diagnose or even prognose faults, averting the return of the vehicle to the workshop. Moreover, the down-time of the vehicle, can be radically decreased, since there will be an indication on which components to focus. Taking into account the fast pace steps and improvements on the respective hardware, such as sensors, one can understand that this endeavour can actually be successful in the future. In the spectrum of this thesis it is assessed whether or not fault detection and isolation can be achieved, through comparison of sensors’ output signals for a number of engine parameters to a stored set of nominal values for these parameters (reference values). Towards that goal, virtual sensors have been developed with the aid of measurement data, in order to increase the reliability of the system.</p><p>Subsequently, a network of dependencies between parameter values and consequent malfunctions has been constructed, in the form of flowcharts, rudimental for fault isolation. In addition to that and despite the fact that no finalized production code for the model is provided, pseudocode charts have been created as well. Finally, significant effort was made to derive precise tolerances for the reference values, as this is of great importance for the results of the diagnostics model.</p>
----------------------------------------------------------------------
In diva2:901716   - correct as is
----------------------------------------------------------------------
In diva2:1111499   - correct as is
----------------------------------------------------------------------
In diva2:1721339 
abstract is: 
<p>Road compaction is the last and important stage in road construction. Both under-compaction and over-compaction are inappropriate and may lead to road failures. Intelligent compactors has enabled data gathering and edge computing functionalities, which introduces possibilities in data-driven compaction control. Compaction physical processes are complex and are material-dependent. In the road construction industry, material physical models, together with boundary conditions, can be used for modeling effects of compacting the underlying subgrade materials and the pavement (the most widely used is asphalt) itself on site, which can be computed using Finite Element (FE) methods. However, parametrizations of these physical models require large efforts, creating difficulties in using these models to optimize real-time compaction. Our research has, for the first time, bridged the gap between data-driven compaction control and physics by introducing the parameter identification pipeline. Two use cases are investigated, corresponding to offline learning and online learning of parameters. In offline learning, a sequence of actions is learned to maximally reduce parameters uncertainties without observing responses; in online learning, the decisions of actions are made and parameters are derived while sequential observations come in. The parameter identification pipeline developed in this thesis involves compaction simulation using a simple physical model, surrogate model development using Artificial Neural Network (ANN), and online/offline optimization procedure with Approximate Bayesian Computation (ABC). The developed procedure can successfully identify the parameters with low uncertainty for the case that the selected experiments supply enough information to theoretically identify the parameters. For the case of that parameters cannot be theoretically identified by certain experiments, the identified parameters have larger uncertainties.</p>

corrected abstract:
<p>Road compaction is the last and important stage in road construction. Both undercompaction and over-compaction are inappropriate and may lead to road failures. Intelligent compactors has enabled data gathering and edge computing functionalities, which introduces possibilities in data-driven compaction control. Compaction physical processes are complex and are material-dependent. In the road construction industry, material physical models, together with boundary conditions, can be used for modeling effects of compacting the underlying subgrade materials and the pavement (the most widely used is asphalt) itself on site, which can be computed using Finite Element (FE) methods. However, parametrizations of these physical models require large efforts, creating difficulties in using these models to optimize real-time compaction. Our research has, for the first time, bridged the gap between data-driven compaction control and physics by introducing the parameter identification pipeline. Two use cases are investigated, corresponding to offline learning and online learning of parameters. In offline learning, a sequence of actions is learned to maximally reduce parameters uncertainties without observing responses; in online learning, the decisions of actions are made and parameters are derived while sequential observations come in. The parameter identification pipeline developed in this thesis involves compaction simulation using a simple physical model, surrogate model development using Artificial Neural Network (ANN), and online/offline optimization procedure with Approximate Bayesian Computation (ABC). The developed procedure can successfully identify the parameters with low uncertainty for the case that the selected experiments supply enough information to theoretically identify the parameters. For the case of that parameters cannot be theoretically identified by certain experiments, the identified parameters have larger uncertainties.</p>

Note - "under-compaction" to "undercompaction" - it can be a single word
----------------------------------------------------------------------
In diva2:1570234   - correct as is
----------------------------------------------------------------------
In diva2:562202 - mossing ligature in title:
"Day-of-the-week eects in stock market data"
==>
"Day-of-the-week effects in stock market data"

abstract is: 
<p>The purpose of this thesis is to investigate day-of-the-week effects for stock index returns. The investigations include analysis of means and variances as well as return-distribution properties such as skewness and tail behavior. Moreover, the existences of conditional day-of-the-week effects, depending on the outcome of returns from the previous week, are analyzed. Particular emphasis is put on determining useful testing procedures for differences in variance in return data from different weekdays. Two time series models, AR and GARCH(1,1), are used to find out if any weekday's mean return is different from other days. The investigations are repeated for two-day re- turns and for returns of diversified portfolios made up of several stock index returns.</p>

corrected abstract:
<p>The purpose of this thesis is to investigate day-of-the-week effects for stock index returns. The investigations include analysis of means and variances as well as return-distribution properties such as skewness and tail behavior. Moreover, the existences of conditional day-of-the-week effects, depending on the outcome of returns from the previous week, are analyzed. Particular emphasis is put on determining useful testing procedures for differences in variance in return data from different weekdays. Two time series models, AR and GARCH(1,1), are used to find out if any weekday's mean return is different from other days. The investigations are repeated for two-day returns and for returns of diversified portfolios made up of several stock index returns.</p>

Note - removed unnecessary hyphen
----------------------------------------------------------------------
In diva2:1450568   - correct as is
----------------------------------------------------------------------
In diva2:1537642   - correct as is
----------------------------------------------------------------------
In diva2:1610014   - correct as is - diva2:1596321 seems to be a duplicate
----------------------------------------------------------------------
In diva2:1107427 
abstract is: 
<p>Deep learning and neural networks has recently become a powerful tool to solve complex problem due to improvements in training algorithms. Examples of successful application can be found in speech recognition and machine translation. There exist relative few finance articles were deep learning have been applied, but existing articles indicate that deep learning can be successfully applied to problems in finance. </p><p>This thesis studies forecasting of financial price movements using two types of neural networks, namely; feedforward and recurrent networks. For the feedforward neural networks we considered non-deep networks with more neurons and deep networks with fewer neurons. In addition to the comparison between feedforward and recurrent networks, a comparison between deep and non-deep networks will be made. The recurrent architecture consists of a recurrent layer mapping into a feedforward layer followed by an output layer. The networks are trained with two different feature setups, one less complex and one more complex.</p><p>The findings for non-deep vs. deep feedforward neural networks imply that there does not exist any general pattern whether deep or non-deep networks are preferable. The findings for recurrent neural networks vs. feedforward neural networks imply that recurrent neural networks do not necessarily outperform feedforward neural networks even though financial data in general are time-dependent. In some cases, adding batch normalization can improve the accuracy for the feedforward neural networks. This can be preferable instead of using more complex models, such as a recurrent neural networks. Moreover, there are significant differences in accuracies between using the two different feature setups. The highest accuracy for all networks are 52.82%, which is significantly better than the simple benchmark.</p>

corrected abstract:
<p>Deep learning and neural networks has recently become a powerful tool to solve complex problem due to improvements in training algorithms. Examples of successful application can be found in speech recognition and machine translation. There exist relative few finance articles were deep learning have been applied, but existing articles indicate that deep learning can be successfully applied to problems in finance.</p><p>This thesis studies forecasting of financial price movements using two types of neural networks, namely; feedforward and recurrent networks. For the feedforward neural networks we considered non-deep networks with more neurons and deep networks with fewer neurons. In addition to the comparison between feedforward and recurrent networks, a comparison between deep and non-deep networks will be made. The recurrent architecture consists of a recurrent layer mapping into a feedforward layer followed by an output layer. The networks are trained with two different feature setups, one less complex and one more complex.</p><p>The findings for non-deep vs. deep feedforward neural networks imply that there does not exist any general pattern whether deep or non-deep networks are preferable. The findings for recurrent neural networks vs. feedforward neural networks imply that recurrent neural networks do not necessarily outperform feedforward neural networks even though financial data in general are time-dependent. In some cases, adding batch normalization can improve the accuracy for the feedforward neural networks. This can be preferable instead of using more complex models, such as a recurrent neural networks. Moreover, there are significant differences in accuracies between using the two different feature setups. The highest accuracy for all networks are 52.82%, which is significantly better than the simple benchmark.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1218590 
abstract is: 
<p>Reinforcement learning has recently become a promising area of machine learning with significant achievements in the subject. Recent successes include surpassing human experts on Atari games and also AlphaGo becoming the first computer ranked on the highest professional level in the game Go, to mention a few. This project aims to apply Policy Gradient Methods (PGM) in a multi agent environment. PGM are widely regarded as being applicable to more problems than for instance Deep Q-Learning but have a tendency to converge upon local optimums. In this report we aim to explore if an optimal policy is achievable with PGM in a multi-agent framework. Numerical simulations implementing the aforementioned method in an environment with up to 4 agents and moving obstacles showed a convergence and the efficiency of the approach. A relatively small amount of collisions took place once the learnt agents were tested. These result differed when changing some parameters such as learning rates and number of neurons in the neural network. The conclusion was that a very fast convergence upon at least a local optimal policy was achieved in this setting.</p>

corrected abstract:
<p>Reinforcement learning has recently become a promising area of machine learning with significant achievements in the subject. Recent successes include surpassing human experts on Atari games and also AlphaGo becoming the first computer ranked on the highest professional level in the game Go, to mention a few.</p><p>This project aims to apply Policy Gradient Methods (PGM) in a multi agent environment. PGM are widely regarded as being applicable to more problems than for instance Deep Q-Learning but have a tendency to converge upon local optimums. In this report we aim to explore if an optimal policy is achievable with PGM in a multi-agent framework.</p><p>Numerical simulations implementing the aforementioned method in an environment with up to 4 agents and moving obstacles showed a convergence and the efficiency of the approach. A relatively small amount of collisions took place once the learnt agents were tested. These result differed when changing some parameters such as learning rates and number of neurons in the neural network.</p><p>The conclusion was that a very fast convergence upon at least a local optimal policy was achieved in this setting.</p>
----------------------------------------------------------------------
In diva2:1680315 - custom coding - 
abstract   - correct as is
----------------------------------------------------------------------
In diva2:1432666 
abstract is: 
<p>The goal of this thesis is to explore a new clustering algorithm, VAE-Clustering, and examine if it can be applied to find differences in the distribution of stock returns and augment the distribution of a current portfolio of stocks and see how it performs in different market conditions.</p><p>The VAE-clustering method is as mentioned a newly introduced method and not widely tested, especially not on time series. The first step is therefore to see if and how well the clustering works. We first apply the algorithm to a dataset containing monthly time series of the power demand in Italy. The purpose in this part is to focus on how well the method works technically. When the model works well and generates proper results with the Italian Power Demand data, we move forward and apply the model on stock return data. In the latter application we are unable to find meaningful clusters and therefore unable to move forward towards the goal of the thesis.</p><p>The results shows that the VAE-clustering method is applicable for time series. The power demand have clear differences from season to season and the model can successfully identify those differences. When it comes to the financial data we hoped that the model would be able to find different market regimes based on time periods. The model is though not able distinguish different time periods from each other. We therefore conclude that the VAE-clustering method is applicable on time series data, but that the structure and setting of the financial data in this thesis makes it to hard to find meaningful clusters.</p><p>The major finding is that the VAE-clustering method can be applied to time series. We highly encourage further research to find if the method can be successfully used on financial data in different settings than tested in this thesis.  </p>

corrected abstract:
<p>The goal of this thesis is to explore a new clustering algorithm, VAE-Clustering, and examine if it can be applied to find differences in the distribution of stock returns and augment the distribution of a current portfolio of stocks and see how it performs in different market conditions.</p><p>The VAE-clustering method is as mentioned a newly introduced method and not widely tested, especially not on time series. The first step is therefore to see if and how well the clustering works. We first apply the algorithm to a dataset containing monthly time series of the power demand in Italy. The purpose in this part is to focus on how well the method works technically. When the model works well and generates proper results with the Italian Power Demand data, we move forward and apply the model on stock return data. In the latter application we are unable to find meaningful clusters and therefore unable to move forward towards the goal of the thesis.</p><p>The results shows that the VAE-clusterinng method is applicable for time series. The power demand have clear differences from season to season and the model can successfully identify those differences. When it comes to the financial data we hoped that the model would be able to find different market regimes based on time periods. The model is though not able distinguish different time periods from each other. We therefore conclude that the VAE-clustering method is applicable on time series data, but that the structure and setting of the financial data in this thesis makes it to hard to find meaningful clusters.</p><p>The major finding is that the VAE-clustering method can be applied to time series. We highly encourage further research to find if the method can be successfully used on financial data in different settings than tested in this thesis.</p>

Note error:
w='VAE-clusterinng' val={'c': 'VAE-clustering', 's': 'diva2:1432666', 'n': 'error in original'}
and eliminated unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1114442 
abstract is: 
<p>This thesis was conducted to investigate what factors are important for a financial institute when predicting the risk of default for a Swedish mortgage portfolio. The applied method was logistic regression analysis and the data used in the thesis was received from a Swedish financial institute. Many of the conducted studies assessing the risk of default only considers five to ten covariates. This thesis started by 29 covariates, ending up in a final model with 16 covariates included. The most important covariates were shown to be pressure of payments, the sum of assets and the time as customer at the financial institute. The derived final model showed a high predictive ability and provides insight of significant drivers of default for a Swedish mortgage portfolio. Considering the alarming housing market in Sweden, and the subprime crisis in the U.S 2008, the subject is highly relevant. </p>

corrected abstract:
<p>This thesis was conducted to investigate what factors are important for a financial institute when predicting the risk of default for a Swedish mortgage portfolio. The applied method was logistic regression analysis and the data used in the thesis was received from a Swedish financial institute. Many of the conducted studies assessing the risk of default only considers five to ten covariates. This thesis started by 29 covariates, ending up in a final model with 16 covariates included. The most important covariates were shown to be pressure of payments, the sum of assets and the time as customer at the financial institute. The derived final model showed a high predictive ability and provides insight of significant drivers of default for a Swedish mortgage portfolio. Considering the alarming housing market in Sweden, and the subprime crisis in the U.S 2008, the subject is highly relevant.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1679042   - correct as is
----------------------------------------------------------------------
In diva2:1334640 
abstract is: 
<p>The mental health has increased in Sweden, which besides the personal suffering affects both the society and economy. The reason behind the increase does not have any definite explanation but the answer may, at least partly, be found in macroeconomic and socioeconomic factors. This report will therefore investigate if there exists a relationship between mental health problems and macroeconomic and socioeconomic factors. An analysis of how these factors may explain the increase of mental health problems is also performed. To see if a relationship exists, a multivariable regression analysis is performed, where the dependent variable is defined as severe problems with anxiety and worry. The regression variables are education level, GDP per capita, the households disposable income and unemployment. The analysis is performed on the groups; women, men and total population and the data is collected over the years 2002 to 2017.</p><p>The analysis indicates a certain relationship between the different macro and socioeconomic variables and mental health problems. For the total population, education level is the most significant. For women, education level, GDP per capita and the households disposable income are most important. For men, unemployment and disposable income are the strongest correlated variables.</p><p>The models approximately fulfills the assumptions for the least square method and have multicollinearity present, which in total makes them less reliable. Further research to validate these relationships and to contribute to explanations of potential causality is needed.</p>

corrected abstract:
<p>The mental health has increased in Sweden, which besides the personal suffering affects both the society and economy. The reason behind the increase does not have any definite explanation but the answer may, at least partly, be found in macroeconomic and socioeconomic factors. This report will therefore investigate if there exists a relationship between mental health problems and macroeconomic and socioeconomic factors. An analysis of how these factors may explain the increase of mental health problems is also performed. To see if a relationship exists, a multivariable regression analysis is performed, where the dependent variable is defined as severe problems with anxiety and worry. The regression variables are education level, GDP per capita, the households disposable income and unemployment. The analysis is performed on the groups; women, men and total population and the data is collected over the years 2002 to 2017. The analysis indicates a certain relationship between the different macro and socioeconomic variables and mental health problems. For the total population, education level is the most significant. For women, education level, GDP per capita and the households disposable income are most important. For men, unemployment and disposable income are the strongest correlated variables.</p><p>The models approximately fulfills the assumptions for the least square method and have multicollinearity present, which in total makes them less reliable. Further research to validate these relationships and contribute to explanations of potential causality is needed.</p>

Note - removed unnecessary paragraph break and rmoved "to" (before "contribute") as it was not in the original
----------------------------------------------------------------------
In diva2:1888605 
abstract is: 
<p>A 2018 United Nations report stated that limiting global warming to below 1.5°C is crucial to avoid the worst impacts of climate change. There are numerous ways to do this, one being through geoengineering using solar sails to shade Earth from approximately 2% of the incident sunlight. As part of the Sunshade project at KTH Space Center, this master thesis set out to design an attitude and orbital control system (AOCS) that could sustain a solar sail in the vicinity of the sub-Lagrange 1 (sub-L1) point. Two options were investigated: employing a PD-controller (Proportional-Derivative controller) to asymptotically stabilize the solar sail at any artificial equilibrium in the neighborhood of sub-L1, and designing a controller through Lyapunov’s Direct Method to sustain periodic motion around sub-L1. The study found that the PD-controller managed to track its prescribed reference trajectory to a satisfactory level but required five times more torque than the 0.06 Nmm available with the current solar sail design. The controller derived through Lyapunov’s Direct Method managed to track regular halo orbits. However, it failed to stay on course when applied to more extreme trajectories, even though it was capable of tracking the required attitude to a high degree. All in all, the study concluded that it is feasible to station-keep a solar sail in a Halo orbit around sub-L1, but that asymptotic stabilization at an artificial equilibrium requires too much torque in its current iteration.</p>

corrected abstract:
<p>A 2018 United Nations report stated that limiting global warming to below 1.5°C is crucial to avoid the worst impacts of climate change. There are numerous ways to do this, one being through geoengineering using solar sails to shade Earth from approximately 2% of the incident sunlight. As part of the Sunshade project at KTH Space Center, this master thesis set out to design an attitude and orbital control system that could sustain a solar sail in the vicinity of the sub-Lagrange 1 (sub-L1) point. Two options were investigated: Employing a PD-controller (Proportional Derivative controller) to asymptotically stabilize the solar sail at any artificial equilibrium in the neighborhood of sub-L1 and designing a controller through Lyapunov’s Direct Method to sustain periodic motion around sub-L1. The study found that the PD-controller managed to track its prescribed reference trajectory to a satisfactory level but in turn required five times more torque than the 0.06 Nmm available with the current solar sail design. The controller derived through Lyapunov’s Direct Method managed to track regular halo orbits. However, it failed to stay on course when applied to more extreme trajectories even though it was capable of tracking the required attitude to a high degree. All in all, the study came to the conclusion that it is feasible to station-keep a solar sail in a Halo orbit around sub-L1 but that asymptotic stabilization at an artificial equilibrium requires too much torque in its current iteration.</p>

Note changed the text to match the original
----------------------------------------------------------------------
In diva2:1817013   - correct as is
----------------------------------------------------------------------
In diva2:1586506 
abstract is: 
<p>To develop turbomachines aerodynamic testing is an important part of the process. The performance testing this thesis will focus on is pressure measurements, which often is done using pneumatic probes. The pneumatic probes used in this project were a five-hole probe, a pitot probe and two different types of Kiel probes. GKN Aerospace had a five-hole probe and a pitot probe that was to be used. The two Kiel probes, however, needed to be designed and fabricated. It was interesting to design an open shielded Kiel probe and a close shielded Kiel probe. The open shielded Kiel probe is more insensitive to the flow angle than the close shielded Kiel probe, but the close shielded Kiel probe could be useful when a rake or grid of probes is needed. In this thesis the total pressure and the sensitivity to the flow angle will be compared between the four pneumatic probes. It will give details of which probes could be useful in which situations. To get all the data necessary there is also other equipment that are needed. The equipment necessary for this project are two servo motors are used to move the probes, scanners are used to measure the pressures, a NI Compact RIO is used as a remote system, an EtherCAT is used to transfer the signals from the computer to the Compact RIO and Labview is used to create the program to control all the components. This equipment was then used to calibrate the five-hole probe and for all the tests in the wind tunnel. The total pressure and the insensitivity to the flow angle could then be compared between the probes. This resulted in an analysis that presented the five-hole probe as a good choice for when the flow angle is unknown. The open Kiel probe is a good choice when a high insensitivity to the flow angle is required and the closed Kiel probe is a good choice when a rake or grid of total pressure probes is required. The pitot probe is a good choice when a cheap probe with low insensitivity to the flow angle is needed.</p>

corrected abstract:
<p>To develop turbomachines aerodynamic testing is an important part of the process. The performance testing this thesis will focus on is pressure measurements, which often is done using pneumatic probes. The pneumatic probes used in this project were a five-hole probe, a pitot probe and two different types of Kiel probes. GKN Aerospace had a five-hole probe and a pitot probe that was to be used. The two Kiel probes, however, needed to be designed and fabricated. It was interesting to design an open shielded Kiel probe and a close shielded Kiel probe. The open shielded Kiel probe is more insensitive to the flow angle than the close shielded Kiel probe, but the close shielded Kiel probe could be useful when a rake or grid of probes is needed.</p><p>In this thesis the total pressure and the sensitivity to the flow angle will be compared between the four pneumatic probes. It will give details of which probes could be useful in which situations. To get all the data necessary there is also other equipment that are needed. The equipment necessary for this project are two servo motors are used to move the probes, scanners are used to measure the pressures, a NI Compact RIO is used as a remote system, an EtherCAT is used to transfer the signals from the computer to the Compact RIO and Labview is used to create the program to control all the components. This equipment was then used to calibrate the five-hole probe and for all the tests in the wind tunnel. The total pressure and the insensitivity to the flow angle could then be compared between the probes. This resulted in an analysis that presented the five-hole probe as a good choice for when the flow angle is unknown. The open Kiel probe is a good choice when a high insensitivity to the flow angle is required and the closed Kiel probe is a good choice when a rake or grid of total pressure probes is required. The pitot probe is a good choice when a cheap probe with low insensitivity to the flow angle is needed.</p>

Note added the missing paragraph break
----------------------------------------------------------------------
In diva2:917988 
abstract is: 
<p>This master thesis describes the design and construction of a prototype for the Moon House project. The goal was to develop a structural concept which ultimately will allow a 2 × 2.5 × 3 m3 house to be deployed on the surface of the Moon as an art installation. A 1 to 5 scale model was built and tested. Provided is background information on lightweight and inﬂatable technology for space applications. This is then reviewed together with earlier work related to the Moon House project in order to come up with a feasible design. The structure consists of a frame made out of plain-weave glass ﬁber tape springs. These are joined with plastic connectors and the frame is covered in a thin rip-stop polyester ﬁlm. Elastic folds and pin-jointed hinges allow the structure to be folded, thus reducing its stowed volume. Deployment of the house is achieved with a combination of pressurization and elastically stored strain energy in the tape springs from folding of the structure. The tape springs have been tailored using speciﬁc lay-up and geometry to achieve an efﬁcient folding scheme. The ﬁnal structure was designed in Solid Edge and connectors were 3D-printed in plastic material. Deployment tests have been performed with partial success. Points of improvement have been identiﬁed and recommendations are made for future work.</p>

corrected abstract:
<p>This master thesis describes the design and construction of a prototype for the Moon House project. The goal was to develop a structural concept which ultimately will allow a 2 × 2.5 × 3 m<sup>3</sup> house to be deployed on the surface of the Moon as an art installation. A 1 to 5 scale model was built and tested. Provided is background information on lightweight and inflatable technology for space applications. This is then reviewed together with earlier work related to the Moon House project in order to come up with a feasible design. The structure consists of a frame made out of plain-weave glass fiber tape springs. These are joined with plastic connectors and the frame is covered in a thin rip-stop polyester film. Elastic folds and pin-jointed hinges allow the structure to be folded, thus reducing its stowed volume. Deployment of the house is achieved with a combination of pressurization and elastically stored strain energy in the tape springs from folding of the structure. The tape springs have been tailored using specific lay-up and geometry to achieve an efficient folding scheme. The final structure was designed in Solid Edge and connectors were 3D-printed in plastic material. Deployment tests have been performed with partial success. Points of improvement have been identified and recommendations are made for future work.</p>

Note added the superscript
----------------------------------------------------------------------
In diva2:1503984   - correct as is
----------------------------------------------------------------------
In diva2:1817027 
abstract is: 
<p>Rocket Factory Augsburg (RFA) a German New Space Startup is developing a three-stage rocket launcher aiming at LEO/SSO orbits. A fundamental responsibility of the GNC team is the development of the rocket navigation algorithm to estimate the attitude, position, and velocity allowing the guidance and control loops to autonomously steer the rocket. This thesis focuses on the analysis and design of a Hybrid Navigation system able to satisfy the various necessities of a launch vehicle, such as delay compensation and GNSS outages. The navigation architecture was chosen to be a Closed Loop, Loosely Coupled, Delayed Error State Kalman Filter thanks to the proven capability of COTS receivers to autonomously provide a consistent PVT solution throughout the flight. A preliminary analysis used a reference trajectory to evaluate the effect of the sensor grade on inertial performances and choose an appropriate integration scheme. The filter’s system model was explored using approximate analytical results on observability. The developed navigation module was then tested within a Monte Carlo simulation environment by perturbating the sensor parameter in accordance with the sensor datasheet. As a further verification, the modeled IMU output was compared to the engineering model, to assure that the simulation result would yield conservative errors. Due to concern over the visibility of GNSS satellites during flight, a simplified Almanac-based GPS model has been developed, proving that enough satellite visibility is available along the trajectory. The estimation error was compared with the filter’s estimated covariance and found well within the bounds. Through the study of the covariance evolution, it was determined that given the reference dynamics, the sensor misalignments are the least observable states. Realistic signal outages were introduced in the most critical flight intervals. The filter was indeed found to be robust and the tuning proved to be adequate to capture the dead reckoning drift. Finally, the entire navigation module was deployed onto the avionics engineering model, including the flight computer, IMU, GNSS, and antennas, in a configuration equivalent to flight. The navigation module was then tested to ensure that the execution was in performance under severe multipath errors and prolonged GNSS outages with the covariance estimates correctly covering the uncertainty.</p>

corrected abstract:
<p>Rocket Factory Augsburg (RFA) a German New Space Startup is developing a three-stage rocket launcher aiming at LEO/SSO orbits. A fundamental responsibility of the GNC team is the development of the rocket navigation algorithm to estimate the attitude, position, and velocity allowing the guidance and control loops to autonomously steer the rocket. This thesis focuses on the analysis and design of a Hybrid Navigation system able to satisfy the various necessities of a launch vehicle, such as delay compensation and GNSS outages. The navigation architecture was chosen to be a <em>Closed Loop</em>, <em>Loosely Coupled</em>, <em>Delayed Error State Kalman Filter</em> thanks to the proven capability of COTS receivers to autonomously provide a consistent PVT solution throughout the flight. A preliminary analysis used a reference trajectory to evaluate the effect of the sensor grade on inertial performances and choose an appropriate integration scheme. The filter’s system model was explored using approximate analytical results on observability. The developed navigation module was then tested within a Monte Carlo simulation environment by perturbating the sensor parameter in accordance with the sensor datasheet. As a further verification, the modeled IMU output was compared to the engineering model, to assure that the simulation result would yield conservative errors. Due to concern over the visibility of GNSS satellites during flight, a simplified Almanac-based GPS model has been developed, proving that enough satellite visibility is available along the trajectory. The estimation error was compared with the filter’s estimated covariance and found well within the bounds. Through the study of the covariance evolution, it was determined that given the reference dynamics, the sensor misalignments are the least observable states. Realistic signal outages were introduced in the most critical flight intervals. The filter was indeed found to be robust and the tuning proved to be adequate to capture the dead reckoning drift. Finally, the entire navigation module was deployed onto the avionics engineering model, including the flight computer, IMU, GNSS, and antennas, in a configuration equivalent to flight. The navigation module was then tested to ensure that the execution was in performance under severe multipath errors and prolonged GNSS outages with the covariance estimates correctly covering the uncertainty.</p>
----------------------------------------------------------------------
In diva2:805332   - correct as is
----------------------------------------------------------------------
In diva2:458105 
abstract is: 
<p>This master thesis is about designing and dimensioning a boat for transport of caravan, car, trailer or ATV. Buoyancy and stability are important properties as the pontoonboat should also be able to serve as a workboat. Many of the vessels on the market today that is being used for similar purpose are stable enough, but their hull is often similar with barges and thus too slow. This work has been to construct a boat that has good stability but also has a hull that enables it to move faster, is more maneuverable and having a more attracting design than today’s barges. This study includes requirement analysis, production of general arrangement, selection of appropriate material, design analysis and hydrostatic analysis. The hull design is based on DNV’s regulatory Standard for Certification of Craft. As the boat should be adaptable to accommodate customer requirements, it is fitted with outboard motors. This allows the user to easily change motors to fit the speed requirement of the boat. Sufficient power to meet speed requirement is decided after tests with prototype. The transom is designed with enough space for two engines when this allows for improved maneuverability. The work resulted in a boat that is 11.5 m long and 3.8 m wide. Aluton 38' is made entirely of aluminum and can take loads up to 3 tons.</p>
mc='pontoonboat' c='pontoon boat'

partal corrected: diva2:458105: <p>This master thesis is about designing and dimensioning a boat for transport of caravan, car, trailer or ATV. Buoyancy and stability are important properties as the pontoon boat should also be able to serve as a workboat. Many of the vessels on the market today that is being used for similar purpose are stable enough, but their hull is often similar with barges and thus too slow. This work has been to construct a boat that has good stability but also has a hull that enables it to move faster, is more maneuverable and having a more attracting design than today’s barges. This study includes requirement analysis, production of general arrangement, selection of appropriate material, design analysis and hydrostatic analysis. The hull design is based on DNV’s regulatory Standard for Certification of Craft. As the boat should be adaptable to accommodate customer requirements, it is fitted with outboard motors. This allows the user to easily change motors to fit the speed requirement of the boat. Sufficient power to meet speed requirement is decided after tests with prototype. The transom is designed with enough space for two engines when this allows for improved maneuverability. The work resulted in a boat that is 11.5 m long and 3.8 m wide. Aluton 38' is made entirely of aluminum and can take loads up to 3 tons.</p>

corrected abstract:
<p>This master thesis is about designing and dimensioning a boat for transport of caravan, car, trailer or ATV. Buoyancy and stability are important properties as the pontoon boat should also be able to serve as a workboat. Many of the vessels on the market today that is being used for similar purpose are stable enough, but their hull is often similar with barges and thus too slow. This work has been to construct a boat that has good stability but also has a hull that enables it to move faster, is more maneuverable and having a more attracting design than today’s barges. This study includes requirement analysis, production of general arrangement, selection of appropriate material, design analysis and hydrostatic analysis. The hull design is based on DNV’s regulatory Standard for Certification of Craft. As the boat should be adaptable to accommodate customer requirements, it is fitted with outboard motors. This allows the user to easily change motors to fit the speed requirement of the boat. Sufficient power to meet speed requirement is decided after tests with prototype. The transom is designed with enough space for two engines when this allows for improved maneuverability. The work resulted in a boat that is 11.5 m long and 3.8 m wide. Aluton 38' is made entirely of aluminum and can take loads up to 3 tons.</p>
In diva2:458105 
abstract is: 
<p>This master thesis is about designing and dimensioning a boat for transport of caravan, car, trailer or ATV. Buoyancy and stability are important properties as the pontoonboat should also be able to serve as a workboat. Many of the vessels on the market today that is being used for similar purpose are stable enough, but their hull is often similar with barges and thus too slow. This work has been to construct a boat that has good stability but also has a hull that enables it to move faster, is more maneuverable and having a more attracting design than today’s barges. This study includes requirement analysis, production of general arrangement, selection of appropriate material, design analysis and hydrostatic analysis. The hull design is based on DNV’s regulatory Standard for Certification of Craft. As the boat should be adaptable to accommodate customer requirements, it is fitted with outboard motors. This allows the user to easily change motors to fit the speed requirement of the boat. Sufficient power to meet speed requirement is decided after tests with prototype. The transom is designed with enough space for two engines when this allows for improved maneuverability. The work resulted in a boat that is 11.5 m long and 3.8 m wide. Aluton 38' is made entirely of aluminum and can take loads up to 3 tons.</p>
mc='pontoonboat' c='pontoon boat'

partal corrected: diva2:458105: <p>This master thesis is about designing and dimensioning a boat for transport of caravan, car, trailer or ATV. Buoyancy and stability are important properties as the pontoon boat should also be able to serve as a workboat. Many of the vessels on the market today that is being used for similar purpose are stable enough, but their hull is often similar with barges and thus too slow. This work has been to construct a boat that has good stability but also has a hull that enables it to move faster, is more maneuverable and having a more attracting design than today’s barges. This study includes requirement analysis, production of general arrangement, selection of appropriate material, design analysis and hydrostatic analysis. The hull design is based on DNV’s regulatory Standard for Certification of Craft. As the boat should be adaptable to accommodate customer requirements, it is fitted with outboard motors. This allows the user to easily change motors to fit the speed requirement of the boat. Sufficient power to meet speed requirement is decided after tests with prototype. The transom is designed with enough space for two engines when this allows for improved maneuverability. The work resulted in a boat that is 11.5 m long and 3.8 m wide. Aluton 38' is made entirely of aluminum and can take loads up to 3 tons.</p>

corrected abstract:
<p>This master thesis is about designing and dimensioning a boat for transport of caravan, car, trailer or ATV. Buoyancy and stability are important properties as the pontoonboat should also be able to serve as a workboat. Many of the vessels on the market today that is being used for similar purpose are stable enough, but their hull is often similar with barges and thus too slow. This work has been to construct a boat that has good stability but also has a hull that enables it to move faster, is more maneuverable and having a more attracting design than today’s barges.</p><p>This study includes requirement analysis, production of general arrangement, selection of appropriate material, design analysis and hydrostatic analysis. The hull design is based on DNV’s regulatory Standard for Certification of Craft.</p><p>As the boat should be adaptable to accommodate customer requirements, it is fitted with outboard motors. This allows the user to easily change motors to fit the speed requirement of the boat. Sufficient power to meet speed requirement is decided after tests with prototype. The transom is designed with enough space for two engines when this allows for improved maneuverability.</p><p>The work resulted in a boat that is 11.5 m long and 3.8 m wide. Aluton 38' is made entirely of aluminum and can take loads up to 3 tons.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1360204 
abstract is: 
<p>Nowadays manufacturing processes are rapidly developing. Salt-bath dip brazing is a conventional manufacturing method commonly used by Saab AB to fuse aluminium components in a high temperature salt bath. However conventional manufacturing methods have shown some limitations. Additive Manufacturing, or 3D printing, is a newer technology which has become very popular in the industry offering competitive advantages regarding production time and size, and structural complexity of the components among other aspects. In this work, Additive Manufacturing is investigated to assess if the performance of heat sinks can be increased compared to the salt-bath dip brazing method.</p><p>Geometrical shapes of heat sink-fins were studied by empirical research to compare their characteristics in air-flow, convection and pressure drop. Eight different geometrical shapes have been analyzed using Additive Manufacturing, and the control plate fins was used as a reference for comparison with salt-bath dip brazing. It was found out that the NACA 0010 fins and Square Grid fins geometries gave the best performance with a 63% and 64% decrease in pressure drop per diverted energy compared to the control plate fins, respectively.</p>

corrected abstract:
<p>Nowadays manufacturing processes are rapidly developing. Salt-bath dip brazing is a conventional manufacturing method commonly used by Saab AB to fuse aluminium components in a high temperature salt bath. However conventional manufacturing methods have shown some limitations. Additive Manufacturing, or 3D printing, is a newer technology which has become very popular in the industry offering competitive advantages regarding production time and size, and structural complexity of the components among other aspects. In this work, Additive Manufacturing is investigated to assess if the performance of heat sinks can be increased compared to the salt-bath dip brazing method. Geometrical shapes of heat sink-fins were studied by empirical research to compare their characteristics in air-flow, convection and pressure drop. Eight different geometrical shapes have been analyzed using Additive Manufacturing, and the control plate fins was used as a reference for comparison with salt-bath dip brazing. It was found out that the NACA 0010 fins and Square Grid fins geometries gave the best performance with a 63% and 64% decrease in pressure drop per diverted energy compared to the control plate fins, respectively.</p>

Note - removed unnecessary paragraph break
----------------------------------------------------------------------
In diva2:846122   - correct as is
----------------------------------------------------------------------
In diva2:1720815 
abstract is: 
<p>Segmented traveling wave electro-absorption modulators (TWEAMs) can provide a modulation speed above 100 Gb/s, which is needed in new optical communication networks. In the EU-project TWILIGHT, KTH continues the design work of TWEAMs in the EU-projects IPHOBAC and HECTO with the aim to design a new optical modulator to be integrated with an electronic layer. The structure of the modulator has to be changed in order to make it compatible with the integration technology that will be used in the project. Like in previous projects, the PSPICE software is an important tool for the design work. The past studies left a large amount of different models and test data. This thesis has been focused on studying these models and data to see if they can help the future work in the project. This includes the comparison of the geometrical dimensions, the microwave properties, and the simulation results with the fabricated modulators in the past projects. The electrical parameters are also critical for the design work, especially the capacitance which determines the impedance of the modulator. Since the TWILIGHT project needs a new design of the modulator, the calculation of these parameters will change. This thesis contains a study how this can be done and discusses the parameters for the new TWEAM design.</p>

corrected abstract:
<p>Segment traveling wave electro-absorption modulators (TWEAM) can provide a modulation speed above 100 Gb/s, which play important roles in the optical communication. In the EUproject TWILIGHT, KTH continues its design work in EU-project IPHOBAC and HECTO for TWEAM, which plans to design a new optical modulator integrated with electronic layer, and the structure of the modulator will also be changed.</p><p>Like the project before, the PSPICE software is an important tool for the design work. The past study left a large amount of PSPICE models and their test data. This thesis will focus on study these models and data to see if they can help the future work in the project. Including compare the geometrical dimensions, the microwave property, and the simulation results with the fabricated modulator in the past project.</p><p>The electric parameters are also critical for the design work, especially the capacitance which decide the impedance of the modulator. Since the TWILIGHT project have a new design for the modulator, the calculation of its parameter changes. This thesis will study their calculation and discusses the parameters for the new designed TWEAM.</p>

Note the DiVA version is quite different from what is in the original
----------------------------------------------------------------------
In diva2:1206994 - missing space in title:
"Design of a cargo load carryingstructure for a hybrid air vehicle"
==>
"Design of a cargo load carrying structure for a hybrid air vehicle"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1486585   - correct as is
----------------------------------------------------------------------
In diva2:1528155 
abstract is: 
<p>The work summarized in this report aims to investigate how a drone airplane design can be optimized to create a safer and more efficient sea rescue by providing staff with an early picture, performing search missions and aiding communication through visual contact. A flying wing is in theory one of the most efficient designs for a fixed wing aircraft, at the same time as it also offers high structural efficiency for its given size. In this report, an overview of aerodynamics, stability and flying quality for a flying wing is discussed and analysed. XFLR5 was used for this project, and a comparison between the analytical results and wind tunnel test data for a prototype was conducted. A strong correlation was found between the theoretical analyses and the wind tunnel data. A simple control solution using only one set of elevons has been proposed and simulated, resulting in Level 1 dynamic stability for all modes except Dutch-roll (where the drone’s damping is 𝜁𝑑𝑟=0.07 and the requirement for Level 1 is 𝜁𝑑𝑟=0.08). For the range of angle of attack used, the autopilot system will have to trim the drone in flight to achieve stability. As the drone only has one set of control surfaces there will be a loss of efficiency in this scenario, meaning that 𝐶𝐿/𝐶𝐷 = 15.7 for loiter speed of 15 𝑚/𝑠 and 7.9 for full speed at 35 𝑚/𝑠. In regular flight, with a total mass &lt;1 𝑘𝑔, the drone is able to fly at full speed for 214 𝑘𝑚 or loiter for 6.3 ℎ with a battery package of 130 𝑊ℎ. As such, the objective of this project was achieved, and the proposed design met the given requirements.</p>

corrected abstract:
<p>The work summarized in this report aims to investigate how a drone airplane design can be optimized to create a safer and more efficient sea rescue by providing staff with an early picture, performing search missions and aiding communication through visual contact. A flying wing is in theory one of the most efficient designs for a fixed wing aircraft, at the same time as it also offers high structural efficiency for its given size. In this report, an overview of aerodynamics, stability and flying quality for a flying wing is discussed and analysed. XFLR5 was used for this project, and a comparison between the analytical results and wind tunnel test data for a prototype was conducted. A strong correlation was found between the theoretical analyses and the wind tunnel data. A simple control solution using only one set of elevons has been proposed and simulated, resulting in Level 1 dynamic stability for all modes except Dutch-roll (where the drone’s damping is &#x1D701;<sub>𝑑𝑟</sub>=0.07 and the requirement for Level 1 is &#x1D701;<sub>𝑑𝑟</sub>=0.08). For the range of angle of attack used, the autopilot system will have to trim the drone in flight to achieve stability. As the drone only has one set of control surfaces there will be a loss of efficiency in this scenario, meaning that 𝐶<sub>𝐿</sub>/𝐶<sub>𝐷</sub> = 15.7 for loiter speed of 15 𝑚/𝑠 and 7.9 for full speed at 35 𝑚/𝑠. In regular flight, with a total mass &lt; 1 𝑘𝑔, the drone is able to fly at full speed for 214 𝑘𝑚 or loiter for 6.3 ℎ with a battery package of 130 𝑊ℎ. As such, the objective of this project was achieved, and the proposed design met the given requirements.</p>

Note - fixed some of the equations to have the correct subscripts
----------------------------------------------------------------------
In diva2:893785   - correct as is
----------------------------------------------------------------------
In diva2:891479   - correct as is
----------------------------------------------------------------------
In diva2:1719738 
abstract is: 
<p>Satellite constellations intended for communications services are becoming increasingly relevant with multiple companies such as Starlink and OneWeb launching constellations consisting of hundreds or thousands of satellites. This thesis investigated how such a constellation can be designed for a small user terminal with a diameter of approximately 15 cm. Four constellations, two at 8 500 km altitude and two at 1 200 km altitude, were proposed. Methods for systematic placement of satellites in orbital planes, aspects going into the link budget, and relevant regulations on the international level were investigated. It was found that the most favourable constellation was a medium Earth orbit constellation with a minimum elevation of 30°. The primary reason for this choice was the limited budget which did not allow for a large number of satellites being launched. Finally, the concept of a hybrid constellation with both geostationary satellites and non-geostationary satellites was considered.</p>

corrected abstract:
<p>Satellite constellations intended for communications services are becoming increasingly relevant with multiple companies such as Starlink and OneWeb launching constellations consisting of hundreds or thousands of satellites. This thesis investigated how such a constellation can be designed for a small user terminal with a diameter of approximately 15 cm. Four constellations, two at 8&ThinSpace;500 km altitude and two at 1&ThinSpace;200 km altitude, were proposed. Methods for systematic placement of satellites in orbital planes, aspects going into the link budget, and relevant regulations on the international level were investigated. It was found that the most favourable constellation was a medium Earth orbit constellation with a minimum elevation of 30°. The primary reason for this choice was the limited budget which did not allow for a large amount of satellites being launched. Finally, the concept of a hybrid constellation with both geostationary satellites and non-geostationary satellites was considered.</p>

Note - small change in worging and spacing of the 4 digit numbers (to use &ThinSpace;) - to match the original
----------------------------------------------------------------------
In diva2:441454   - correct as is
----------------------------------------------------------------------
In diva2:1145330 
abstract is: 
<p>Today, ergonomic adjusting capabilities are lacking on centre consoles of small high-speed crafts with respect to the crew height. The centre consoles are normally constructed to fit the mean height of the operators to cover as many end users as possible. Issues might therefore arise for the operators who deviate from the mean height. This master thesis report covers the design, implementation and evaluation of a set of mechanisms which are to enable the operator work space of an 8-meter high speed craft to be longitudinally movable as well as ergonomically adjustable for operators with a length 160 to 200 cm, which include more than 90 % of the Swedish population. The resulting design is to make out a part of the Aevotec 800 which is an 8-meter high-speed craft which is being designed by Poseidon Konsult.</p><p>The development during the project included several phases; a concept phase, feasibility phase and the main design phase. The main design phase contains several cycles which were evaluated based by peak load calculations, fatigue analyses, ISO design calculations, design review/feedback sessions, anthropometrical measurement and mock-up/end-user review. Based on the executed evaluations of the project, a feasible system has been designed capable of handling the requirements set for the project. The superstructure has been successfully designed by ISO 12215-5 and fulfils parts of the DNV-GL regulations regarding ergonomics for high-speed light crafts.</p>

corrected abstract:
<p>Today, ergonomic adjusting capabilities are lacking on centre consoles of small high-speed crafts with respect to the crew height. The centre consoles are normally constructed to fit the mean height of the operators to cover as many end users as possible. Issues might therefore arise for the operators who deviate from the mean height.</p><p>This master thesis report covers the design, implementation and evaluation of a set of mechanisms which are to enable the operator work space of an 8-meter high speed craft to be longitudinally movable as well as ergonomically adjustable for operators with a length 160 to 200 cm, which include more than 90 % of the Swedish population. The resulting design is to make out a part of the Aevotec 800 which is an 8-meter high-speed craft which is being designed by Poseidon Konsult.</p><p>The development during the project included several phases; a concept phase, feasibility phase and the main design phase. The main design phase contains several cycles which were evaluated based by peak load calculations, fatigue analyses, ISO design calculations, design review/feedback sessions, anthropometrical measurement and mock-up/end-user review. Based on the executed evaluations of the project, a feasible system has been designed capable of handling the requirements set for the project. The superstructure has been successfully designed by ISO 12215-5 and fulfils parts of the DNV-GL regulations regarding ergonomics for high-speed light crafts.</p>

Note - adding missing paragraph break
----------------------------------------------------------------------
In diva2:1645435 
abstract is: 
<p>Scania works continuously to develop internal combustion engines and after treatment systems which can achieve low pollutant emissions and high efficiency. A major principal that Scania adopted for this goal is the Simulation Driven Development (SDD) process. Here computer aided simulations aid to develop designs and improve characteristics all the while reducing the iterative prototyping and testing process. Currently, parametric modelling and Design of Experiments (DoE) is a proven and major mode of exploring designs within the various stakeholder fields of development; namely structural, fluid mechanics and acoustics. </p><p>Topology optimisation in fluid flow is a new field which promises quick exploration of design spaces. The result of topology optimisation are unintuitive designs that could serve as baseline designs which can further reduce the design process.</p><p>The objective of the thesis was to explore topology optimisation and investigate a way to incorporate topology optimisation in the design process at Scania CV AB. For this task Tosca Fluid by Dassault Systèmes was chosen for its optimisation capabilities, which uses back-flow as the criteria of optimisation. The case study was conducted based on the MTX diesel one box inlet end-plate which was used as reference. The dimensional constraints of the reference product were used to model and utilise the developed workflow. Since this task involved the use of substrates for exhaust gas filtration, it was imperative to explore uniformity of flow over the substrate as an additional optimisation criterion. The project studied modelling design spaces to satisfy the design criteria and the shortcoming the software currently poses have been documented in this report. The report also mentions the tasks involved in setting up simulation cases to work well with Tosca Fluid. In the current stage of the thesis work it was not possible to incorporate uniformity as an additional criterion and hence fails in using Tosca Fluid to optimise topology for turn volumes involving the use of substrates.</p>

corrected abstract:
<p>Scania works continuously to develop internal combustion engines and after treatment systems which can achieve low pollutant emissions and high efficiency. A major principal that Scania adopted for this goal is the Simulation Driven Development (SDD) process. Here computer aided simulations aid to develop designs and improve characteristics all the while reducing the iterative prototyping and testing process. Currently, parametric modelling and Design of Experiments (DoE) is a proven and major mode of exploring designs within the various stakeholder fields of development; namely structural, fluid mechanics and acoustics.</p><p>Topology optimisation in fluid flow is a new field which promises quick exploration of design spaces. The result of topology optimisation are unintuitive designs that could serve as baseline designs which can further reduce the design process.</p><p>The objective of the thesis was to explore topology optimisation and investigate a way to incorporate topology optimisation in the design process at Scania CV AB. For this task Tosca Fluid by Dassault Systèmes was chosen for its optimisation capabilities, which uses back-flow as the criteria of optimisation. The case study was conducted based on the MTX diesel one box inlet end-plate which was used as reference. The dimensional constraints of the reference product were used to model and utilise the developed workflow. Since this task involved the use of substrates for exhaust gas filtration, it was imperative to explore uniformity of flow over the substrate as an additional optimisation criterion. The project studied modelling design spaces to satisfy the design criteria and the shortcoming the software currently poses have been documented in this report. The report also mentions the tasks involved in setting up simulation cases to work well with Tosca Fluid. In the current stage of the thesis work it was not possible to incorporate uniformity as an additional criterion and hence fails in using Tosca Fluid to optimise topology for turn volumes involving the use of substrates.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1590726 - missing subscript in title:
"Design of passively loaded specimen for constant KI during crack growth"
==>
"Design of passively loaded specimen for constant K<sub>I</sub> during crack growth"

abstract is: 
<p>Passive loading of a specimen is a relatively cheap method to use in fracture mechanical testing compared to an actively loaded specimen. For stress corrosion cracking testing it is easier to use a passively loaded specimen since the specimen easily can be placed in a specific corrosive environment. The passive method lacks information about the crack growth over time and the load can not be regulated during the test to ensure crack growth. This thesis work was mainly about finding a specimen with a region of constant KI to ensure crack growth without the need of controlling the load and to find a way to estimate the crack growth over time. The work is based on Linear Elastic Fracture Mechanics and the Finite Element Method. The thesis work resulted in a specimen with constant KI in the region 23/50 ≤ a/W ≤ 33/50 of crack growth and an equation was found to describe the relation between the crack propagation and the strain measured on the specimens back face.</p>

corrected abstract:
<p>Passive loading of a specimen is a relatively cheap method to use in fracture mechanical testing compared to an actively loaded specimen. For stress corrosion cracking testing it is easier to use a passively loaded specimen since the specimen easily can be placed in a specific corrosive environment. The passive method lacks information about the crack growth over time and the load can not be regulated during the test to ensure crack growth. This thesis work was mainly about finding a specimen with a region of constant 𝐾<sub>𝐼<sub> to ensure crack growth without the need of controlling the load and to find a way to estimate the crack growth over time. The work is based on Linear Elastic Fracture Mechanics and the Finite Element Method. The thesis work resulted in a specimen with constant 𝐾<sub>𝐼<sub> in the region 23/50 ≤ a/W ≤ 33/50 of crack growth and an equation was found to describe the relation between the crack propagation and the strain measured on the specimens back face.</p>

Note - fixed the equation and added subscript, i.e., 𝐾<sub>𝐼<sub> 
----------------------------------------------------------------------
In diva2:430662 - other than printing - all operations on the PDF file are not permited
abstract   - correct as is
----------------------------------------------------------------------
In diva2:752131   - correct as is
----------------------------------------------------------------------
In diva2:753832   - correct as is
----------------------------------------------------------------------
In diva2:1843151   - correct as is
----------------------------------------------------------------------
In diva2:1137877   - correct as is
----------------------------------------------------------------------
In diva2:927736 
abstract is: 
<p>Unsupervised pre-training has recently emerged as a method for initializing super- vised machine learning methods. Foremost it has been applied to artificial neural networks (ANN). Previous work has found unsupervised pre-training to increase accuracy and be an effective method of initialization for ANNs[2].</p><p>This report studies the effect of unsupervised pre-training when detecting Twit- ter trends. A Twitter trend is defined as a topic gaining popularity.</p><p>Previous work has studied several machine learning methods to analyse Twitter trends. However, this thesis studies the efficiency of using a multi-layer percep- tron classifier (MLPC) with and without Bernoulli restricted Boltzmann machine (BRBM) as an unsupervised pre-training method. Two relevant factors studied are the number of hidden layers in the MLPC and the size of the available dataset for training the methods.</p><p>This thesis has implemented a MLPC that can detect trends at an accuracy of 85%. However, the experiments conducted to test the effect of unsupervised pre-training were inconclusive. No benefit could be concluded when using BRBM pre-training for the Twitter time series data. </p>

corrected abstract:
<p>Unsupervised pre-training has recently emerged as a method for initializing supervised machine learning methods. Foremost it has been applied to artificial neural networks (ANN). Previous work has found unsupervised pre-training to increase accuracy and be an effective method of initialization for ANNs[2].</p><p>This report studies the effect of unsupervised pre-training when detecting Twitter trends. A Twitter trend is defined as a topic gaining popularity.</p><p>Previous work has studied several machine learning methods to analyse Twitter trends. However, this thesis studies the efficiency of using a multi-layer perceptron classifier (MLPC) with and without Bernoulli restricted Boltzmann machine (BRBM) as an unsupervised pre-training method. Two relevant factors studied are the number of hidden layers in the MLPC and the size of the available dataset for training the methods.</p><p>This thesis has implemented a MLPC that can detect trends at an accuracy of 85%. However, the experiments conducted to test the effect of unsupervised pre-training were inconclusive. No benefit could be concluded when using BRBM pre-training for the Twitter time series data.</p>

Note removed unnecessary hyphens and eliminated an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1849176   - correct as is
----------------------------------------------------------------------
In diva2:1514599 
abstract is: 
<p>This project studies the signal detection and identification problem in high-dimensional noisy data and the possibility of using it on microbiome data. An extensive simulation study was performed on generated data using as well as a microbiome dataset collected on patients with Parkinson's disease, using Donoho and Jin's Higher criticism, Jager and Wellner's phi-divergence-based goodness-of-fit-test and Stepanova and Pavlenko's CsCsHM statistic . We present some novel approaches based on established theory that perform better than existing methods and show that it is possible to use the signal identification framework to detect differentially abundant features in microbiome data. Although the novel approaches produce good results, they lack substantial mathematical foundations and should be avoided if theoretical rigour is needed. We also conclude that while we have found that it is possible to use signal identification methods to find abundant features in microbiome data, further refinement is necessary before it can be properly used in research.</p>

corrected abstract:
<p>This project studies the signal detection and identification problem in high-dimensional noisy data and the possibility of using it on microbiome data. An extensive simulation study was performed on generated data using as well as a microbiome dataset collected on patients with Parkinson's disease, using Donoho and Jin's Higher criticism, Jager and Wellner's phi-divergence-based goodness-of-fit-test [26] and Stepanova and Pavlenko's CsCsHM statistic  [37]. We present some novel approaches based on established theory that perform better than existing methods and show that it is possible to use the signal identification framework to detect differentially abundant features in microbiome data. Although the novel approaches produce good results, they lack substantial mathematical foundations and should be avoided if theoretical rigour is needed. We also conclude that while we have found that it is possible to use signal identification methods to find abundant features in microbiome data, further refinement is necessary before it can be properly used in research.</p>

Note - added the citations as per the original
----------------------------------------------------------------------
In diva2:867032   - correct as is
----------------------------------------------------------------------
In diva2:1213813 
abstract is: 
<p>In this study, the Botswanan currency Pula is analyzed and factors affecting the exchange rate between the Pula and the US Dollar are examined. Using a multiple linear regression analysis and the Cochrane-Orcutt transformation, an explanatory model is developed. Factors are chosen based on economic theory and are evaluated empirically through statistical methods. The final model shows that The Botswana Stock Index, The Real Interest Rate Spread between Botswana and USA, Corruption Index, Business Confidence Index, Diamond Price Index and Unemployment Rate, are relevant and significant in determining changes in the exchange rate. Some factors were significant, but had to be removed from the model due to multicollinearity.</p>

corrected abstract:
<p>In this study, the Botswanan currency Pula is analysed and factors affecting the exchange rate between the Pula and the US Dollar are examined. Using a multiple linear regression analysis and the Cochrane-Orcutt transformation, an explanatory model is developed. Factors are chosen based on economic theory and are evaluated empirically through statistical methods. The final model shows that The Botswana Stock Index, The Real Interesst Rate Spread Between Botswana and USA, Corruption Index, Business Confidence Index, Diamond Price Index and Unemployment Rate, are relevant and significant in determining changes in the exchange rate. Some factors were significant, but had to be removed from the model due to multicollinearity.</p>

Note minor changes to text to match the original
w='Interesst' val={'c': 'Interest', 's': 'diva2:1213813', 'n': 'error in original'}
w='analyzed' val={'c': 'analysed', 's': 'diva2:1213813', 'n': 'error in original'}
In original capitalized as "Between"
----------------------------------------------------------------------
In diva2:942593 
abstract is: 
<p>This thesis in applied statistics and industrial economics examined which factors and strategies that had a statistically significant impact on profitability, within the business to consumer dietary supplement market. The data this thesis was based on consisted of several annual reports from the year 2011 to 2015ånd other strategic information. The data included 19 different dietary supplement retailers on the Swedish market. In order to establish which factors had a significant impact on profitability, a linear regression was used. The result was an identified linear relationship between Operating Margin and the covariates Solidity, Average Salary, Only Own Brand, Free Returns, Student Discounts and Chat. A market analysis was then performed using Porter's five forces and a PEST analysis. The analysis concluded that the market is attractive but there are a few uncertainties surrounding the future development of the dietary supplement retailing market.</p>

corrected abstract:
<p>This thesis in applied statistics and industrial economics examined which factors and strategies that had a statistically significant impact on profitability, within the business to consumer dietary supplement market. The data this thesis was based on consisted of several annual reports from the year 2011 to 2015 and other strategic information. The data included 19 different dietary supplement retailers on the Swedish market. In order to establish which factors had a significant impact on profitability, a linear regression was used. The result was an identified linear relationship between <em>Operating Margin</em> and the covariates <em>Solidity</em>, <em>Average Salary</em>, <em>Only Own Brand</em>, <em>Free Returns</em>, <em>Student Discounts</em> and <em>Chat</em>. A market analysis was then performed using Porter's five forces and a PEST analysis. The analysis concluded that the market is attractive but there are a few uncertainties surrounding the future development of the dietary supplement retailing market.</p>

Note - added italics
----------------------------------------------------------------------
In diva2:1652392   - correct as is
----------------------------------------------------------------------
In diva2:1879547   - correct as is
----------------------------------------------------------------------
In diva2:1362800   - correct as is
----------------------------------------------------------------------
In diva2:1440102   - correct as is
----------------------------------------------------------------------
In diva2:1780183 
abstract is: 
<p>Quantum key distribution (QKD) is the idea of using quantum systems to securely communicate a shared encryption key between two parties. In contrast to classical methods of encryption, QKD utilizes fundamental quantum properties such as superposition and entanglement to encode information in a way that guarantees security. Most QKD systems are based on sending photons in an optical fiber where the polarisation of the photons is the quantum property used to encode information. The different algorithms used to do this are referred to as QKD protocols. This thesis aimed to construct an educational tool to simulate simple QKD systems using four common QKD protocols, where the user can vary system parameters and study its effect on the results. Furthermore, the aim was to be able to produce simulation results that are accurate enough to provide a first approximation of how a real experimental setup would perform. The program was built in Python using the Qiskit library and all the desired features were implemented in a graphical interface. For one of the implemented protocols (BB84) the simulation results were compared to experimental data from a QKD experiment in Copenhagen, which indicated that the program is able to produce a useful first approximation of a real experimental setup. The program could be further improved by allowing for simulations of more complex systems.</p>


corrected abstract:
<p>Quantum key distribution (QKD) is the idea of using quantum systems to securely communicate a shared encryption key between two parties. In contrast to classical methods of encryption, QKD utilizes fundamental quantum properties such as superposition and entanglement to encode information in a way that guarantees security. Most QKD systems are based on sending photons in an optical fiber where the polarisation of the photons is the quantum property used to encode information. The different algorithms used to do this are referred to as QKD protocols. This thesis aimed to construct an educational tool to simulate simple QKD systems using four common QKD protocols, where the user can vary system parameters and study its effect on the results. Furthermore, the aim was to be able to produce simulation results that are accurate enough to provide a first approximation of how a real experimental setup would perform. The program was built in Python using the Qiskit library and all the desired features were implemented in a graphical interface. For one of the implemented protocols (BB84) the simulation results were compared to experimental data from three QKD experiments, which indicated that the program is able to produce a useful first approximation of a real experimental setup. The program could be further improved by allowing for simulations of more complex systems.</p>

Note - minor changes in wording
----------------------------------------------------------------------
In diva2:1255165 
abstract is: 
<p>My Master Thesis takes place in the context of the MicroCarb mission. The goal of this mission is to identify the sinks and the sources of carbon dioxide on Earth in order to map them and to improve the knowledge of its cycle. To fulfill this mission, some particular guidance modes must be implemented in order to study their feasibility. My thesis consisted in defining and enriching the algorithms used to define the guidance laws, by implementing new tools and a new guidance law, and studying the induced performances in terms of data acquisition and with respect to the constraints related to the satellite. Alongside with this mission, the implementation of those elements support the development of the guidance library POLARIS, actual in its early phase, which is at first only dedicated to MicroCarb but which is intended to become multimissions. First, I describe the CNES as well as the guidance team I worked in. Then, the context of the Master Thesis is introduced. Once the context is established we will focus on the first elements I have been working on, as part of the Dazzling studies. Indeed, the spectrometer used in MicroCarb is very sensitive and has to be maintained at very low temperature. Thus the passive cooling mechanism must be protected from the Sunlight and from the light reflected by the Earth. I had to use a class of the Space mechanics library PATRIUS, called Assembly, in order to materialize the satellite and its numerous parts. Once implemented, I was able to perform some Dazzling Studies, highlighting some issues with the various strategies that were considered, and opening new perspectives. Moreover, a problem was detected on a crucial function of the guidance laws calculator. Once a new function was compiled, I had to made a cross validation using Scilab, and results were positive. This part will end with a Geometric Cape study, realized in order to quantify the influence of the satellite, and the MCV roll, over the Geometric Shifting. In the second part, we will introduce a guidance law which was not implemented initially, and on which I had to work during the last weeks of the thesis: The City mode. Although this mode is similar to an existing calibration mode, it has its own characteristics I had to take into account. The code for this acquisition mode worked well, but the results were not satisfying, considering the Dazzling problem and the kinematic constraints. Thus new strategies had to be considered, and more particularly the 2-scans mode. This mode brought a lot of satisfactions, but there is still more work to be done. This report ends with a general conclusion about my work and some perspectives which could be considered for future studies. I also present my personal contribution and some encountered difficulties I had to deal with.</p>

corrected abstract:
<p>My Master Thesis takes place in the context of the MicroCarb mission. The goal of this mission is to identify the sinks and the sources of carbon dioxide on Earth in order to map them and to improve the knowledge of its cycle. To fulfill this mission, some particular guidance modes must be implemented in order to study their feasibility. My thesis consisted in defining and enriching the algorithms used to define the guidance laws, by implementing new tools and a new guidance law, and studying the induced performances in terms of data acquisition and with respect to the constraints related to the satellite. Alongside with this mission, the implementation of those elements support the development of the guidance library POLARIS, actual in its early phase, which is at first only dedicated to MicroCarb but which is intended to become multimissions.</p><p>First, I describe the CNES as well as the guidance team I worked in. Then, the context of the Master Thesis is introduced. Once the context is established we will focus on the first elements I have been working on, as part of the Dazzling studies. Indeed, the spectrometer used in MicroCarb is very sensitive and has to be maintained at very low temperature. Thus the passive cooling mechanism must be protected from the Sunlight and from the light reflected by the Earth. I had to use a class of the Space mechanics library PATRIUS, called Assembly, in order to materialize the satellite and its numerous parts. Once implemented, I was able to perform some Dazzling Studies, highlighting some issues with the various strategies that were considered, and opening new perspectives. Moreover, a problem was detected on a crucial function of the guidance laws calculator. Once a new function was compiled, I had to made a cross validation using Scilab, and results were positive. This part will end with a Geometric Cape study, realized in order to quantify the influence of the satellite, and the MCV roll, over the Geometric Shifting.</p><p>In the second part, we will introduce a guidance law which was not implemented initially, and on which I had to work during the last weeks of the thesis: The City mode. Although this mode is similar to an existing calibration mode, it has its own characteristics I had to take into account. The code for this acquisition mode worked well, but the results were not satisfying, considering the Dazzling problem and the kinematic constraints. Thus new strategies had to be considered, and more particularly the 2-scans mode. This mode brought a lot of satisfactions, but there is still more work to be done.</p><p>This report ends with a general conclusion about my work and some perspectives which could be considered for future studies. I also present my personal contribution and some encountered difficulties I had to deal with.</p>

Note - added the missing paragraph breaks
----------------------------------------------------------------------
In diva2:1821625 
abstract is: 
<p>Calcium, a pivotal ion in various human signaling pathways, holds significant relevance in cancer research and treatment. Investigating the transmission of spontaneous calcium signals between cells and understanding how they are influenced by different treatments is therefore an interesting area of study.</p><p>This master's thesis presents a newly developed tool designed for analyzing intracellular calcium concentration [Ca2+] fluctuations. The tool encompasses a cell segmentation model, tracking algorithms, nearest neighbor identification, mean intensity measurement, and calculations of dominant frequency of [Ca2+] oscillations and signal correlation among neighboring cells.</p><p>The tool was applied to fluorescence microscopy images of Madin-Darby Canine Kidney II cells expressing GCaMP6m and either treated with the cardiac glycoside ouabain, a combination of ouabain and the gap junction blocker heptanol, or left untreated for comparison. Additionally, an exploration into the impact of connexin 43 (Cx43) transfection on [Ca2+] fluctuations was undertaken.</p><p>The transfection rate for Cx43 did not reach a sufficient level, impeding a thorough analysis of its influence on [Ca2+] fluctuations. Furthermore, the investigation into dominant frequencies and signal correlation did not yield conclusive findings regarding the effects of ouabain and heptanol. Nevertheless, a foundational tool for analysis has been developed, with the potential for expansion to analyze more aspects calcium signaling and application to additional cell types and treatments.</p>

corrected abstract:
<p>Calcium, a pivotal ion in various human signaling pathways, holds significant relevance in cancer research and treatment. Investigating the transmission of spontaneous calcium signals between cells and understanding how they are influenced by different treatments is therefore an interesting area of study.</p><p>This master's thesis presents a newly developed tool designed for analyzing intracellular calcium concentration [Ca<sup>2+</sup>]<sub>i</sub> fluctuations. The tool encompasses a cell segmentation model, tracking algorithms, nearest neighbor identification, mean intensity measurement, and calculations of dominant frequency of [Ca<sup>2+</sup>]<sub>i</sub> oscillations and signal correlation among neighboring cells.</p><p>The tool was applied to fluorescence microscopy images of Madin-Darby Canine Kidney II cells expressing GCaMP6m and either treated with the cardiac glycoside ouabain, a combination of ouabain and the gap junction blocker heptanol, or left untreated for comparison. Additionally, an exploration into the impact of connexin 43 (Cx43) transfection on [Ca<sup>2+</sup>]<sub>i</sub> fluctuations was undertaken.</p><p>The transfection rate for Cx43 did not reach a sufficient level, impeding a thorough analysis of its influence on [Ca<sup>2+</sup>]<sub>i</sub> fluctuations. Furthermore, the investigation into dominant frequencies and signal correlation did not yield conclusive findings regarding the effects of ouabain and heptanol. Nevertheless, a foundational tool for analysis has been developed, with the potential for expansion to analyze more aspects calcium signaling and application to additional cell types and treatments.</p>

Note - added superscripts and subscripts to match original
----------------------------------------------------------------------
In diva2:1219164   - correct as is
----------------------------------------------------------------------
In diva2:1739361 
abstract is: 
<p>Wiener Linien’s tram network is one of the largest and busiest of its kind worldwide. Maintaining and improving the Level of Service (LoS) is one of the major tasks of the operations division. To direct these improvements efficiently to lines and sections of lines in need, tram line performance needs to be assessed. In this master thesis a Python-based model is developed to assess tram line performance using ideal operational constraints. Furthermore, the model is capable of computing the energy consumption for this optimal case. The tool computes Undisturbed Optimal Travel Times (UOTTs) which serve as a benchmark for tram line performance. Therefor it builds on track alignment data (curves and radii, gradients and switches), speed restriction data and a set of optimal parameters (no traffic, constant acceleration, etc.). Furthermore, train resistance, curve speeds and vehicle type are taken into consideration. These parameters are investigated and selected based on literature studies, interviews with employees of Wiener Linien, as well as field tests. Finally, the model results are compared to real world data for evaluation. A discussion of the results regarding further applications and improvements is performed.</p>

corrected abstract:
<p>Wiener Linien’s tram network is one of the largest and busiest of its kind worldwide. Maintaining and improving the Level of Service (LoS) is one of the major tasks of the operations division. To direct these improvements efficiently to lines and sections of lines in need, tram line performance needs to be assessed. In this master thesis a Python-based model is developed to assess tram line performance using ideal operational constraints. Furthermore, the model is capable of computing the energy consumption for this optimal case.</p><p>The tool computes Undisturbed Optimal Travel Times (UOTTs) which serve as a benchmark for tram line performance. Therefor it builds on track alignment data (curves and radii, gradients and switches), speed restriction data and a set of optimal parameters (no traffic, constant acceleration, etc.). Furthermore, train resistance, curve speeds and vehicle type are taken into consideration. These parameters are investigated and selected based on literature studies, interviews with employees of Wiener Linien, as well as field tests.</p><p>Finally, the model results are compared to real world data for evaluation. A discussion of the results regarding further applications and improvements is performed.</p>

Note added the missing paragraph breaks
----------------------------------------------------------------------
In diva2:660594   - correct as is
----------------------------------------------------------------------
In diva2:1737474   - correct as is
----------------------------------------------------------------------
In diva2:1599586   - correct as is
----------------------------------------------------------------------
In diva2:1329559   - correct as is
----------------------------------------------------------------------
In diva2:1698409   - correct as is
----------------------------------------------------------------------
In diva2:1714428   - correct as is
----------------------------------------------------------------------
In diva2:1145346 - missing spaces in title:
"Development of a Sonar forUnderwater Sensor Platformsand Surface Vehicles"
==>
"Development of a Sonar for Underwater Sensor Platforms and Surface Vehicles"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:509606 
abstract is: 
<p>Automotive turbo compressors generate high frequency noise in the air intake system. This sound generation is of importance for the perceived sound quality of luxury cars and may need to be controlled by the use of silencers. The silencers usually contain resonators with slits, perforates and cavities. The purpose of the work reported is to develop acoustic models for these resonators where relevant effects such as the effect of realistic mean flow on losses and possibly 3D effects are considered. An experimental campaign has been undertaken where the two-port matrices and transmission loss of four sample resonators has been measured without flow and for two different mean flow speeds (M=0.05 &amp; M=0.1) using two source location technique. Models for the four resonators have been developed using a 1D linear acoustic code (SIDLAB) and a FEM code (COMSOL Multi-physics). Different models, from the literature, for including the effect of mean flow on the acoustic losses at slits and perforates have been discussed. Correct modeling of acoustic losses for resonators with complicated geometry is important for the simulation and development of new and improved silencers, and the present work contributes to this understanding. The measured acoustic properties compared well with the simulated model for almost all the cases.</p>

corrected abstract:
<p>Automotive turbo compressors generate high frequency noise in the air intake system. This sound generation is of importance for the perceived sound quality of luxury cars and may need to be controlled by the use of silencers. The silencers usually contain resonators with slits, perforates and cavities. The purpose of the work reported is to develop acoustic models for these resonators where relevant effects such as the effect of realistic mean flow on losses and possibly 3D effects are considered.</p><p>An experimental campaign has been undertaken where the two-port matrices and transmission loss of four sample resonators has been measured without flow and for two different mean flow speeds (M=0.05 &amp; M=0.1) using two source location technique.</p><p>Models for the four resonators have been developed using a 1D linear acoustic code (SIDLAB) and a FEM code (COMSOL Multi-physics). Different models, from the literature, for including the effect of mean flow on the acoustic losses at slits and perforates have been discussed.</p><p>Correct modeling of acoustic losses for resonators with complicated geometry is important for the simulation and development of new and improved silencers, and the present work contributes to this understanding.</p><p>The measured acoustic properties compared well with the simulated model for almost all the cases.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1381283 
abstract is: 
<p>The sales of all-terrain vehicles (ATVs) are increasing year by year, especially in countries like Sweden, Australia and New Zealand. With the increase in sales, a proportional rise in number of accidents involving all-terrain vehicles is also evident. Of these accidents the major cause was identified as rollover occurrence. While there are some protection devices, the lack of rollover prevention devices available is glaring. In addition to that, ATVs do not have many control units or electronic systems compared to modern-day cars. This makes it difficult to implement complicated computer technology to improve vehicle stability or act as a prevention system. In this thesis, a couple of active safety systems to prevent rollover have been proposed. Lateral Load Transfer Ratio (LLTR) is used as the primary parameter to analyse and signify rollover. First, an alarm has been proposed based on LLTR and roll angle. Two different types of alarms have been analysed, one based on real-time values and the other on predicted values. The prediction-based alarm shows better performance over the other alarm by giving the driver more time to take action. Second, to remove the flaws of the alarms and take away control from the driver, in case of impending rollover, an active braking system has been proposed. Multiple braking strategies have been simulated. The strategy where the brakes are applied on the outer wheels was found to be most effective.</p>

corrected abstract:
<p>The sales of all-terrain vehicles (ATVs) are increasing year by year, especially in countries like Sweden, Australia and New Zealand. With the increase in sales, a proportional rise in number of accidents involving all-terrain vehicles is also evident. Of these accidents the major cause was identified as rollover occurrence.</p><p>While there are some protection devices, the lack of rollover prevention devices available is glaring. In addition to that, ATVs do not have many control units or electronic systems compared to modern-day cars. This makes it difficult to implement complicated computer technology to improve vehicle stability or act as a prevention system. In this thesis, a couple of active safety systems to prevent rollover have been proposed. Lateral Load Transfer Ratio (LLTR) is used as the primary parameter to analyse and signify rollover.</p><p>First, an alarm has been proposed based on LLTR and roll angle. Two different types of alarms have been analysed, one based on real-time values and the other on predicted values. The prediction-based alarm shows better performance over the other alarm by giving the driver more time to take action. Second, to remove the flaws of the alarms and take away control from the driver, in case of impending rollover, an active braking system has been proposed. Multiple braking strategies have been simulated. The strategy where the brakes are applied on the outer wheels was found to be most effective.</p>

Note - added missing paragraph break
----------------------------------------------------------------------
In diva2:1360256   - correct as is
----------------------------------------------------------------------
In diva2:1057198   - correct as is
----------------------------------------------------------------------
In diva2:1152943   - correct as is
----------------------------------------------------------------------
In diva2:1528056   - correct as is
----------------------------------------------------------------------
In diva2:805330   - correct as is
----------------------------------------------------------------------
In diva2:1817479   - correct as is
----------------------------------------------------------------------
In diva2:1360554   - correct as is
----------------------------------------------------------------------
In diva2:1590118 
abstract is: 
<p>Tissue paper is a type of soft, absorbent, and lightweight paper with several applications for hygiene and kitchen use. Embossing is an operation during the converting stage of the tissue production process which creates relief designs on tissue. Other than producing designs for aesthetic purposes, embossing increases bulk which improves absorbency and softness but reduces mechanical strength and stiffness.</p><p>A computational model using the finite element method is developed to simulate the embossing of tissue paper. A tool for fitting an appropriate material model for tissue to its experimental test data is implemented. The material model is subjected to a verification test and it works sufficiently well to model the in-plane elastic and plastic anisotropic behavior of tissue. Two validation tests are conducted to check the embossing model against experimental test data where it is observed that the model works well. Firstly, it provides an idea about the amount of pressure required to be applied during loading to reach a certain embossing level. Secondly, it predicts the tensile strength of embossed tissue sheets although it provides a slight underestimate. Potential reasons for the shortcomings in the tensile strength are suggested and recommendations for further improving the model are provided. Lastly, parametric studies are conducted to investigate the influence of embossing pattern geometry on the mechanical performance of embossed tissue. </p><p>After passing the verification and validation stages, the model is ready to serve as a convenient, less time-consuming, and cost-effective alternative to experimental testing to study the embossing process. It can also be used as a tool to examine the effect of one or more model parameters on embossing by simply changing them and studying the new results.</p>

corrected abstract:
<p>Tissue paper is a type of soft, absorbent, and lightweight paper with several applications for hygiene and kitchen use. Embossing is an operation during the converting stage of the tissue production process which creates relief designs on tissue. Other than producing designs for aesthetic purposes, embossing increases bulk which improves absorbency and softness but reduces mechanical strength and stiffness.</p><p>A computational model using the finite element method is developed to simulate the embossing of tissue paper. A tool for fitting an appropriate material model for tissue to its experimental test data is implemented. The material model is subjected to a verification test and it works sufficiently well to model the in-plane elastic and plastic anisotropic behavior of tissue. Two validation tests are conducted to check the embossing model against experimental test data where it is observed that the model works well. Firstly, it provides an idea about the amount of pressure required to be applied during loading to reach a certain embossing level. Secondly, it predicts the tensile strength of embossed tissue sheets although it provides a slight underestimate. Potential reasons for the shortcomings in the tensile strength are suggested and recommendations for further improving the model are provided. Lastly, parametric studies are conducted to investigate the influence of embossing pattern geometry on the mechanical performance of embossed tissue.</p><p>After passing the verification and validation stages, the model is ready to serve as a convenient, less time-consuming, and cost-effective alternative to experimental testing to study the embossing process. It can also be used as a tool to examine the effect of one or more model parameters on embossing by simply changing them and studying the new results.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1707826   - correct as is
----------------------------------------------------------------------
In diva2:770275 - missing spaces in title:
"Development of waste heatrecovery systems for mobileheavy duty applications"
==>
"Development of waste heat recovery systems for mobile heavy duty applications"

abstract is: 
<p>The focus of today’s automotive industry is to reduce emissions and fuel consumption of all vehicles.</p><p>Concentrating on the truck industry, the last 20 years have focused largely on cutting emissions of particulate matter and nitrogen oxides. For the future, attention will be focused on fuel consumption and emissions of carbon dioxide. Waste heat recovery appears to be a very promising concept for fuel economy on long haul heavy duty Diesel trucks.</p><p>After a general introduction on the concept of waste heat recovery and the Rankine cycle, this thesis work shows how to model and calibrate a cooling system circuit for a heavy duty Diesel engine equipped with a waste heat recovery system. Then an overview of the current transmission systems that are suitable to transfer energy from the waste heat recovery expander to the engine shaft is presented. For all transmission architectures, input speed range, speed ratio range, transmission efficiency as well as weight and size are detailed and compared to each other. Finally, these systems are modeled and integrated to a complete vehicle Simulink simulation platform and simulations are run on two highway driving cycles. Resulting average recovered powers and fuel consumptions are compared and the analysis finally shows that a gear train transmission has the best performance for this kind of driving cycle.</p>

corrected abstract:
<p>The focus of today’s automotive industry is to reduce emissions and fuel consumption of all vehicles. Concentrating on the truck industry, the last 20 years have focused largely on cutting emissions of particulate matter and nitrogen oxides. For the future, attention will be focused on fuel consumption and emissions of carbon dioxide. Waste heat recovery appears to be a very promising concept for fuel economy on long haul heavy duty Diesel trucks.</p><p>After a general introduction on the concept of waste heat recovery and the Rankine cycle, this thesis work shows how to model and calibrate a cooling system circuit for a heavy duty Diesel engine equipped with a waste heat recovery system. Then an overview of the current transmission systems that are suitable to transfer energy from the waste heat recovery expander to the engine shaft is presented. For all transmission architectures, input speed range, speed ratio range, transmission efficiency as well as weight and size are detailed and compared to each other. Finally, these systems are modeled and integrated to a complete vehicle Simulink simulation platform and simulations are run on two highway driving cycles. Resulting average recovered powers and fuel consumptions are compared and the analysis finally shows that a gear train transmission has the best performance for this kind of driving cycle.</p>

Note - removed unnecessary paragraph break
----------------------------------------------------------------------
 diva2:1119939 
abstract is: 
<p>In this report it is analysed if there are any differences in car preferences between genders and if so, what they are and what is being done today. Different studies, analyses and articles are presented and discussed. Most of them are found through KTH’s library and the internet. A lot of differences were found, men and women preferred the complete different driving feels when testing a car with adjustable variables. Their preference for car types were also found to be different, with women preferring for example small cars and SUVs. Sports cars and luxury cars are mostly bought by men. It was also found that extravagant colors like orange and yellow was preferred mostly by men, with women dominating discrete colors like silver and gold. A surprising finding was that women are less acceptive toward self driving cars than men, citing safety as their main concern, but when studying the acceptance of different advanced driver assistance systems such as lane departure warning and front collision warning, women liked those systems more than men. So there are a lot of differences between the genders and women are becoming a larger part of the car buyer market, with the US already having more female driving licence holders than the male equivalent so gender is definitely something to consider when designing and manufacturing a new car.</p>

corrected abstract:
<p>In this report it is analysed if there are any differences in car preferences between genders and if so, what they are and what is being done today. Different studies, analyses and articles are presented and discussed. Most of them are found through KTH’s library and the internet.</p><p>A lot of differences were found, men and women preferred the complete different driving feels when testing a car with adjustable variables. Their preference for car types were also found to be different, with women preferring for example small cars and SUVs. Sports cars and luxury cars are mostly bought by men. It was also found that extravagant colors like orange and yellow was preferred mostly by men, with women dominating discrete colors like silver and gold. A surprising finding was that women are less acceptive toward self driving cars than men, citing safety as their main concern, but when studying the acceptance of different advanced driver assistance systems such as lane departure warning and front collision warning, women liked those systems more than men.</p><p>So there are a lot of differences between the genders and women are becoming a larger part of the car buyer market, with the US already having more female driving licence holders than the male equivalent so gender is definitely something to consider when designing and manufacturing a new car.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1558807 
abstract is: 
<p>Grip stiffness is an important property of carton board packaging. The structure of the packaging needs to withstand the handling of one or many consumers. A carton board packaging that feels stiff when handled conveys a sense of luxury to the consumer, one example of this is the packages containing new flagship smart phones. If a package is more or less stiff is at this moment subjectively interpreted by test panels. An objective method of measuring grip stiffness is sought after for repeat ability and speed. This master thesis investigates the influence of geometry and material parameters on the grip stiffness of a carton board packages. The purpose is to determine how the measured point loads differ between different geometries for one particular material. </p><p>Measurements are conducted using a tensile tester and a sensory device, Syntouch Biotac. Only two of the 19 sensors are analyzed in this thesis. Ther data analyzed in this thesis comes from experiments conducted by the author and experiments conducted during a bachelors thesis at Örebro University. The same equipment was used for both experiments. The authors experiments were aimed at finding how a rotation of the packaging would affect the results whereas no rotations were made during experiments not conducted by the author. The authors own experiments were also made using an actual human finger, but only on packages that were not rotated. A finite element study was performed to validate the results from the Syntouch Biotac. X-ray computed tomography was used to investigate if the damage done to the carton board material by a human finger was similar to the damage done to the carton board material by the Syntouch Biotac. </p><p>Results from the Syntouch Biotac show that it is possible to tell where along and how close to the edge of the packaging the Syntouch Biotac is touching the packaging and that it is possible to discern between materials if the surface weights are different enough. The X-ray computed tomography show that damages done to the carton material cone by either the Syntouch Biotac or a human finger, are not possible to tell apart with the method of analysis used in this thesis.</p>

corrected abstract:
<p>Grip stiffness is an important property of carton board packaging. The structure of the packaging needs to withstand the handling of one or many consumers. A carton board packaging that feels stiff when handled conveys a sense of luxury to the consumer, one example of this is the packages containing new flagship smart phones. If a package is more or less stiff is at this moment subjectively interpreted by test panels. An objective method of measuring grip stiffness is sought after for repeat ability and speed. This master thesis investigates the influence of geometry and material parameters on the grip stiffness of a carton board packages. The purpose is to determine how the measured point loads differ between different geometries for one particular material.</p><p>Measurements are conducted using a tensile tester and a sensory device, Syntouch Biotac. Only two of the 19 sensors are analyzed in this thesis. Ther data analyzed in this thesis comes from experiments conducted by the author and experiments conducted during a bachelors thesis at Örebro University. The same equipment was used for both experiments. The authors experiments were aimed at finding how a rotation of the packaging would affect the results whereas no rotations were made during experiments not conducted by the author. The authors own experiments were also made using an actual human finger, but only on packages that were not rotated. A finite element study was performed to validate the results from the Syntouch Biotac. X-ray computed tomography was used to investigate if the damage done to the carton board material by a human finger was similar to the damage done to the carton board material by the Syntouch Biotac.<br>’</p><p>Results from the Syntouch Biotac show that it is possible to tell where along and how close to the edge of the packaging the Syntouch Biotac is touching the packaging and that it is possible to discern between materials if the surface weights are different enough. The X-ray computed tomography show that damages done to the carton material cone by either the Syntouch Biotac or a human finger, are not possible to tell apart with the method of analysis used in this thesis.</p><p lang="sv">I den här masteruppsatsen undersöktes geometri- och materialparametrars påverkan på kartongförpackningars greppstyvhet. Syftet var att avgöra hur de olika punktlasterna skiljer sig åt mellan olika geometrier för ett givet material.</p>

Note inserted the missing last paragraph in Swedish and the "'" that appears following the 2nd paragraph
----------------------------------------------------------------------
In diva2:1237803 
abstract is: 
<p>High-chromium cast irons are used in certain applications where the demand on abrasion resistance is high. Such applications can be found in the milling industry and in pumps for transport of abrasive particles in liquid suspension. Soft annealed high-chromium cast iron containing 2.6 % C and 24.7 % Cr was supplied by Xylem Water Solutions, Sundbyberg, and investigated by dilatometry. The heat treatments were inspired by induction hardening procedures. The purpose of the investigation was to evaluate the effect of maximum temperature reached during heat treatment on the final length of the test specimen. The aim with this was to find the treatment yielding the maximum possible length which should be profitable to create desirable compressive stresses in the surface hardened area. The experimental results were used to create a finite element model in COMSOL Multiphysics accommodating for the maximum temperature, simulating the phase changes occurring in a geometry based on the experimental test specimen. The experimental results did not reveal any clear correlation between the maximum temperature and the final length change. The hardness, however, increased with the increasing temperature in the treatment interval 900-1150 °C. The, by light optical microscopy, observed amount of secondary precipitated carbides decreased with increasing temperature. Martensite transformation was also affected; the transformation temperature decreased for increased treatment temperatures. From dilatometry it was also seen that the thermal strains were greatly affected by the direction of which the material was cut from the original cast material. Samples taken perpendicular to the mainly investigated direction showed lower coefficients of thermal expansion and the final strain was clearly positive compared to the slightly negative values found for the main direction. This phenomenon could possibly be explained by different macrostructures created during solidification of the melt causing anisotropy in the eutectic. The implementation in COMSOL by describing the phase transformation as ordinary differential equations did show partially good results in the simulation of thermal expansion. The difference in original material is noticeable in the dilatometry and the simulated martensite transformation deviates from the experimental results. The model needs to be validated against new intermediate test temperatures and the martensite transformation kinetics must be investigated further to yield better results to be able to combine the phase transformations with mechanical calculations.</p>

corrected abstract:
<p>High-chromium cast irons are used in certain applications where the demand on abrasion resistance is high. Such applications can be found in the milling industry and in pumps for transport of abrasive particles in liquid suspension. Soft annealed high-chromium cast iron containing 2.6 % C and 24.7 % Cr was supplied by Xylem Water Solutions, Sundbyberg, and investigated by dilatometry. The heat treatments were inspired by induction hardening procedures. The purpose of the investigation was to evaluate the effect of maximum temperature reached during heat treatment on the final length of the test specimen. The aim with this was to find the treatment yielding the maximum possible length which should be profitable to create desirable compressive stresses in the surface hardened area. The experimental results were used to create a finite element model in COMSOL Multiphysics accommodating for the maximum temperature, simulating the phase changes occurring in a geometry based on the experimental test specimen.</p><p>The experimental results did not reveal any clear correlation between the maximum temperature and the final length change. The hardness, however, increased with the increasing temperature in the treatment interval 900-1150 °C. The, by light optical microscopy, observed amount of secondary precipitated carbides decreased with increasing temperature. Martensite transformation was also affected; the transformation temperature decreased for increased treatment temperatures. From dilatometry it was also seen that the thermal strains were greatly affected by the direction of which the material was cut from the original cast material. Samples taken perpendicular to the mainly investigated direction showed lower coefficients of thermal expansion and the final strain was clearly positive compared to the slightly negative values found for the main direction. This phenomenon could possibly be explained by different macrostructures created during solidification of the melt causing anisotropy in the eutectic. The implementation in COMSOL by describing the phase transformation as ordinary differential equations did show partially good results in the simulation of thermal expansion. The difference in original material is noticeable in the dilatometry and the simulated martensite transformation deviates from the experimental results. The model needs to be validated against new intermediate test temperatures and the martensite transformation kinetics must be investigated further to yield better results to be able to combine the phase transformations with mechanical calculations.</p>

Note - added missing paragraph break
----------------------------------------------------------------------
In diva2:1244672 
abstract is: 
<p>The goal with this report is to design a rear underrun protection for Scania trucks that accommodate new regulations regarding loads from collision with other vehicles on the underrun protection. The new regulations state that the rear underrun protection have to withstand approximately twice the load that the rear underrun protection can withstand today. The new regulations are to be put into force in 2021.</p><p>The first part of the report consists of a dimensioning of an exsisting rear underrun protection, so that it may withstand the heavier loads from a collision. The dimensioning process was performed by firstly making a simple FEM-model based on beam elements, that could serve as a guide for what dimensions which needed to change. After the simplified model was made, a more detailed non-linear FEM-model was made and analyzed in Ansys Workbench 17. In order to withstand the larger forces, the new design had larger dimensions and the underrun protection's weight increased, from 55,6 kg to 66,8 kg. In the second part of the report, new concept designs have been created with the purpose of saving weight. This was done by creating completely new designs, but also by making smaller modifications on the existing dimensioned underrun protection. The maximum weight reduction that could be made, and still meet the demands, was 14,2 kg, from 66,8 kg to 52,2 kg. If Scania chooses to adopt the created concept, the company will accommodate the future regulations and still reduce weight on its trucks compared with today.</p>
w='exsisting' val={'c': 'existing', 's': 'diva2:1244672', 'n': 'error in original'}

corrected abstract:
<p>The goal with this report is to design a rear underrun protection for Scania trucks that accommodate new regulations regarding loads from collision with other vehicles on the underrun protection. The new regulations state that the rear underrun protection have to withstand approximately twice the load that the rear underrun protection can withstand today. The new regulations are to be put into force in 2021.</p><p>The first part of the report consists of a dimensioning of an exsisting rear underrun protection, so that it may withstand the heavier loads from a collision. The dimensioning process was performed by firstly making a simple FEM-model based on beam elements, that could serve as a guide for what dimensions which needed to change. After the simplified model was made, a more detailed non-linear FEM-model was made and analyzed in Ansys Workbench 17. In order to withstand the larger forces, the new design had larger dimensions and the underrun protection's weight increased, from 55,6 kg to 66,8 kg.</p><p>In the second part of the report, new concept designs have been created with the purpose of saving weight. This was done by creating completely new designs, but also by making smaller modifications on the existing dimensioned underrun protection. The maximum weight reduction that could be made, and still meet the demands, was 14,2 kg, from 66,8 kg to 52,2 kg. If Scania chooses to adopt the created concept, the company will accommodate the future regulations and still reduce weight on its trucks compared with today.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1751952 
abstract is: 
<p>This thesis work is performed in collaboration with Syntell AB and a client company interested in assistance with charging infrastructure dimensioning. The aim of this thesis is to develop an executable, generalizable model that can aid decision making regarding charging infrastructure. Furthermore, this is done within a Model-Based Systems Engineering (MBSE) context, which enables representation of the system as a model. </p><p>As the data and model concerning the client company is classified, it is not presented in this report. Instead, to further enhance the aim of developing a generalizable model, a test case is produced for this project work. This case consists of passenger electric vehicles and chargers in a metropolitan setting, where data is gathered from public sources. </p><p>The results show that the model is executable and flexible to fit any type of electric vehicle and different specifications of chargers. Using an MBSE approach enables the project owner to customize the model development for the specific use case. Additionally, defining a system in focus establishes what the system uptime is, enabling calculations of availability. The results for this specific use case are interpreted to show how the model can be used to aid the dimensioning of charging infrastructure using the model output. To further verify the model representation of the system, the model can be run in live-mode, where vehicles and chargers can be added while the model is running to instantly examine the system dynamics. </p><p>Concluding, the method for utilizing the model to evaluate systems availability is described. The model output, as well as the thorough description of the model, can be used to increase the knowledge within MBSE for executable modeling.</p>

corrected abstract:
<p>This thesis work is performed in collaboration with Syntell AB and a client company interested in assistance with charging infrastructure dimensioning. The aim of this thesis is to develop an executable, generalizable model that can aid decision making regarding charging infrastructure. Furthermore, this is done within a Model-Based Systems Engineering (MBSE) context, which enables representation of the system as a model.</p><p>As the data and model concerning the client company is classified, it is not presented in this report. Instead, to further enhance the aim of developing a generalizable model, a test case is produced for this project work. This case consists of passenger electric vehicles and chargers in a metropolitan setting, where data is gathered from public sources.</p><p>The results show that the model is executable and flexible to fit any type of electric vehicle and different specifications of chargers. Using an MBSE approach enables the project owner to customize the model development for the specific use case. Additionally, defining a system in focus establishes what the system uptime is, enabling calculations of availability. The results for this specific use case are interpreted to show how the model can be used to aid the dimensioning of charging infrastructure using the model output. To further verify the model representation of the system, the model can be run in <em>live-mode</em>, where vehicles and chargers can be added while the model is running to instantly examine the system dynamics.</p><p>Concluding, the method for utilizing the model to evaluate systems availability is described. The model output, as well as the thorough description of the model, can be used to increase the knowledge within MBSE for executable modeling.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph, added missing paragraph breaks, and added italics
----------------------------------------------------------------------
In diva2:1320133 
abstract is: 
<p>Recently geometric deep learning introduced a new way for machine learning algorithms to tackle point cloud data in its raw form. Pioneers like PointNet and many architectures building on top of its success realize the importance of invariance to initial data transformations. These include shifting, scaling and rotating the point cloud in 3D space. Similarly to our desire for image classifying machine learning models to classify an upside down dog as a dog, we wish geometric deep learning models to succeed on transformed data. As such, many models employ an initial data transform in their models which is learned as part of a neural network, to transform the point cloud into a global canonical space. I see weaknesses in this approach as they are not guaranteed to perform completely invariant to input data transformations, but rather approximately. To combat this I propose to use local deterministic transformations which do not need to be learned. The novelty layer of this project builds upon Edge Convolutions and is thus dubbed DirEdgeConv, with the directional invariance in mind. This layer is slightly altered to introduce another layer by the name of DirSplineConv. These layers are assembled in a variety of models which are then benchmarked against the same tasks as its predecessor to invite a fair comparison. The results are not quite as good as state of the art results, however are still respectable. It is also my belief that the results can be improved by improving the learning rate and its scheduling. Another experiment in which ablation is performed on the novel layers shows that the layers  main concept indeed improves the overall results.</p>

corrected abstract:
<p>Recently geometric deep learning introduced a new way for machine learning algorithms to tackle point cloud data in its raw form. Pioneers like PointNet and many architectures building on top of its success realize the importance of invariance to initial data transformations. These include shifting, scaling and rotating the point cloud in 3D space. Similarly to our desire for image classifying machine learning models to classify an upside down dog as a dog, we wish geometric deep learning models to succeed on transformed data. As such, many models employ an initial data transform in their models which is learned as part of a neural network, to transform the point cloud into a global canonical space. I see weaknesses in this approach as they are not guaranteed to perform completely invariant to input data transformations, but rather approximately. To combat this I propose to use local deterministic transformations which do not need to be learned. The novelty layer of this project builds upon Edge Convolutions and is thus dubbed <strong><span style="font-family: 'courier new', courier;">DirEdgeConv</span></strong>, with the directional invariance in mind. This layer is slightly altered to introduce another layer by the name of <strong><span style="font-family: 'courier new', courier;">DirSplineConv</span></strong>. These layers are assembled in a variety of models which are then benchmarked against the same tasks as its predecessor to invite a fair comparison. The results are not quite as good as state of the art results, however are still respectable. It is also my belief that the results can be improved by improving the learning rate and its scheduling. Another experiment in which ablation is performed on the novel layers shows that the layers main concept indeed improves the overall results.</p>

Note - removed an unnecessary space and set the names in bold courier
----------------------------------------------------------------------
In diva2:720238   - correct as is
----------------------------------------------------------------------
In diva2:458870 
abstract is: 
<p>In the present paper, a study on the directivity of sound from a wind turbine has been conducted. The aim of the study is to investigate the horizontal sound radiation pattern through a field study compared to a noise prediction. The benefit of the results may be used to optimize the output effect from the wind turbine while the guidelines for noise levels at nearby residential areas still are met.</p>
<p>The complete directivity pattern around the wind turbine was investigated by performing emission measurements around the wind turbine from a method described in IEC 61400-11</p>
<p><em><em>Wind turbine generator systems – Part 11: Acoustic noise measurement technique. </em></em>Furthermore, the dominant sound source from the wind turbine, the turbulent boundary layer trailing edge noise, and the frequency range where it is dominating has also been scrutinized. The results show that the dipole character of the trailing edge noise has an impact on the entire horizontal radiation pattern from the wind turbine.</p>
<p>From a field study it was found that there was a distinguishable directivity of the sound. On a distance of 125 m from the wind turbine the sound pressure level in the crosswind direction of the wind turbine is close to 3 dBA less than the sound pressure level in the downwind direction of the wind turbine when the wind speed is 8 m/s at a height of 10 m. The difference between other directions compared to the downwind direction is less significant. This could be utilized to optimize the power output, however the difference in sound level is relatively small but the advantage for power output have to be quantified before a conclusion of the benefits can be made.</p>

corrected abstract:
<p>In the present paper, a study on the directivity of sound from a wind turbine has been conducted. The aim of the study is to investigate the horizontal sound radiation pattern through a field study compared to a noise prediction. The benefit of the results may be used to optimize the output effect from the wind turbine while the guidelines for noise levels at nearby residential areas still are met.</p>
<p>The complete directivity pattern around the wind turbine was investigated by performing emission measurements around the wind turbine from a method described in IEC 61400-11 <em>Wind turbine generator systems – Part 11: Acoustic noise measurement technique</em>. Furthermore, the dominant sound source from the wind turbine, the turbulent boundary layer trailing edge noise, and the frequency range where it is dominating has also been scrutinized. The results show that the dipole character of the trailing edge noise has an impact on the entire horizontal radiation pattern from the wind turbine.</p>
<p>From a field study it was found that there was a distinguishable directivity of the sound. On a distance of 125 m from the wind turbine the sound pressure level in the crosswind direction of the wind turbine is close to 3 dBA less than the sound pressure level in the downwind direction of the wind turbine when the wind speed is 8 m/s at a height of 10 m. The difference between other directions compared to the downwind direction is less significant.</p><p>This could be utilized to optimize the power output, however the difference in sound level is relatively small but the advantage for power output have to be quantified before a conclusion of the benefits can be made.</p>

Note added missing paragraph breaks and fixed italics
----------------------------------------------------------------------
In diva2:1341339   - correct as is
----------------------------------------------------------------------
In diva2:1800541 
abstract is: 
<p>Since the introduction of derivatives to the financial markets, volatility trading has emerged as a method for investors to make money in every market condition. In parallel with introducing derivatives to the financial markets, hedging methods have emerged and are today essential instruments for the liquidity providers active in the markets. The most commonly used hedging method is delta hedging which cancels out the directional risk in the option. Hedging the vega risk with dispersion trading seems to be both a profitable and accurate hedging method. This thesis examines the effectiveness of dispersion trading for reducing the vega risk in OMXS30 options. This is investigated by backtesting a strategy based on going short OMXS30 index volatility and long volatility on a tracking portfolio with a zero net vega. This investigation aims to determine if the dispersion trading strategy can be a reliable risk management tool. It was found that vega could accurately be hedged using dispersion trading. However, when considering the bid-ask spread, the strategy did not show profitability over the simulated period. Weighting the portfolio more in favour of companies with smaller bid-ask spreads did not show improved profitability.</p>

corrected abstract:
<p>Since the introduction of derivatives to the financial markets, volatility trading has emerged as a method for investors to make money in every market condition. Hedging methods have become crucial tools for liquidity providers operating in financial markets, emerging alongside the introduction of derivatives. Today, they play a parallel role alongside derivatives, ensuring the necessary risk mitigation and liquidity in the markets. The most commonly used hedging method is delta hedging which effectively eliminates the directional risk associated with options. Dispersion trading appears to be a profitable and precise method for hedging vega risk, offering an effective approach to manage and mitigate volatility-related concerns. The aim of this thesis is to investigate the effectiveness of dispersion trading in mitigating vega risk specifically in OMXS30 options. The approach involves implementing a backtesting strategy that encompasses taking a short position in OMXS30 index volatility while simultaneously adopting a long volatility stance on a tracking portfolio. This strategy ensures a net vega of zero. This investigation aims to determine if the dispersion trading strategy can be a reliable risk management tool. It was found that vega could accurately be hedged using dispersion trading. However, when considering the bid-ask spread, the strategy did not show profitability over the simulated period. Weighting the portfolio more in favour of companies with smaller bid-ask spreads did not show improved profitability.</p>

Note - replaced text with that found in the original
----------------------------------------------------------------------
In diva2:753691 
abstract is: 
<p>The amount of information stored on the internet grows daily and naturally the requirements on the systems used to search for and analyse information increases. As a part in meeting the raised requirements this study investigates if it is possible for a automatised text analysis system to distinguish certain groups and categories of words in a text, and more specifically investigate if it is possible to distinguish words with a high information value from words with a low information value. This is important to enable optimizations of systems for global surveillance and information retrieval. The study is carried out using word spaces, which are often used in text analysis to model language. The distributional character of certain categories of words is examined by studying the intrinsic dimensionality of the space, locally around different words. Based on the result from the study of the intrinsic dimensionality, where there seems to be differences in the distributional character between categories of words, an algorithm is implemented for classifying words based on the dimensionality data. The classification algorithm is tested for different categories. The result strengthens the thesis that there could exist useful differences between the distributional character of different categories of words.</p>

corrected abstract:
<p>The amount of information stored on the internet grows daily and naturally the requirements on the systems used to search for and analyse information increases. As a part in meeting the raised requirements this study investigates if it is possible for a automatised text analysis system to distinguish certain groups and categories of words in a text, and more specifically investigate if it is possible to distinguish words with a high information value from words with a low information value. This is important to enable optimizations of systems for global surveillance and information retrieval. The study is carried out using word spaces, which are often used in text analysis to model language. The distributional character of certain categories of words is examined by studying the intrinsic dimensionality of the space, locally around different words. Based on the result from the study of the intrinsic dimensionality, where there seems to be differences in the distributional character between categories of words, an algorithm is implemented for classifying words based on the dimensionality data. The classification algorithm is tested for different categories. The result strengthens the thesis that there could exist useful differences between the distributional character of different categories of words.</p>
----------------------------------------------------------------------
In diva2:825411   - correct as is
----------------------------------------------------------------------
In diva2:1893985   - correct as is
----------------------------------------------------------------------
In diva2:1436772 
abstract is: 
<p>Quantitative approaches to achieving excess return are becoming increasingly popular as computational capabilities increase. Today, the main issue at hand is the development of accurate and reliable models for predicting the return of individual instruments. Index tracking mutual funds are however seldom able to select specific instruments to invest in, but rather select individual instruments in an index not to invest in. This study seeks to investigate whether downside deviation is a suitable risk measure for identifying instruments that underperform among a collection of instruments, and thereby the possibility of using downside deviation in a stop loss algorithm. That is, an algorithm that analyzes a portfolio and indicates which instruments might underperform compared to the benchmark, helping the portfolio manager to avoid realizing negative returns. In order to simulate the suggested model, daily pricing data between 2009-12-30 and 2018-12-28 for the large cap list on OMX Stockholm per 2009-12-30 is used. A model generating excess return was indeed found using downside deviation as the performance measure. The model yielded an average excess return of 3.27% over 1000 simulations with p &lt; .00001 for the unregulated case. When regulations such as UCITS IV was implemented, the statistically significant excess return turn to statistically significant deficit return.</p>

corrected abstract:
<p>Quantitative approaches to achieving excess return are becoming increasingly popular as computational capabilities increase. Today, the main issue at hand is the development of accurate and reliable models for predicting the return of individual instruments. Index tracking mutual funds are however seldom able to select specific instruments to invest in, but rather select individual instruments in an index <strong><em>not</em></strong> to invest in. This study seeks to investigate whether downside deviation is a suitable risk measure for identifying instruments that underperform among a collection of instruments, and thereby the possibility of using downside deviation in a stop loss algorithm. That is, an algorithm that analyzes a portfolio and indicates which instruments might underperform compared to the benchmark, helping the portfolio manager to avoid realizing negative returns. In order to simulate the suggested model, daily pricing data between 2009-12-30 and 2018-12-28 for the large cap list on OMX Stockholm per 2009-12-30 is used. A model generating excess return was indeed found using downside deviation as the performance measure. The model yielded an average excess return of 3.27% over 1000 simulations with 𝑝 &lt; .00001 for the unregulated case. When regulations such as UCITS IV was implemented, the statistically significant excess return turn to statistically significant deficit return.</p>

Note replaced "p" with "𝑝" and applied bold italics to "not"
----------------------------------------------------------------------
In diva2:754054   - correct as is
----------------------------------------------------------------------
In diva2:1823748 
abstract is: 
<p>With a high increase of users in the world's ever expanding sharing economy, Airbnb has become a customary solution in short term rentals of accommodations. In this market, it is the host's job to choose a pricing which sufficiently corresponds to what tenants are willing to pay. There can be multiple methods of choosing the price but this study aims to determine and evaluate which factors have a significant impact on short term rental pricing of housing and to what degree. By modelling this issue, the reader can make a better understanding of what to pay or charge for an accommodation. This study also serves as ground work for further investigations exploring nested and multi-leveled factors.</p><p>The study is limited to the Spanish short term rental market, taking a more in-depth look at the cities of Barcelona, Madrid and Palma. Moreover, listings between 2015 and 2017 are considered in the study. In the end, factors identified as significant on accommodation pricing were Entire Home, Accommodates, Bathrooms, Review Scores Rating etc.. Some of the factors are interchangeable as they have a miniscule effect on the accommodation pricing. Conversely, Entire Home and Accommodates is seen as absolute necessities for the model as they, together, explain two-thirds of the variations in price.</p>

corrected abstract:
<p>With a high increase of users in the world's ever expanding sharing economy, Airbnb has become a customary solution in short term rentals of accommodations. In this market, it is the host's job to choose a pricing which sufficiently corresponds to what tenants are willing to pay. There can be multiple methods of choosing the price but this study aims to determine and evaluate which factors have a significant impact on short term rental pricing of housing and to what degree. By modelling this issue, the reader can make a better understanding of what to pay or charge for an accommodation. This study also serves as ground work for further investigations exploring nested and multi-leveled factors.</p><p>The study is limited to the Spanish short term rental market, taking a more in-depth look at the cities of Barcelona, Madrid and Palma. Moreover, listings between 2015 and 2017 are considered in the study. In the end, factors identified as significant on accommodation pricing were <em>Entire Home</em>, <em>Accommodates</em>, <em>Bathrooms</em>, <em>Review Scores Rating</em> etc.. Some of the factors are interchangeable as they have a miniscule effect on the accommodation pricing. Conversely, <em>Entire Home</em> and <em>Accommodates</em> is seen as absolute necessities for the model as they, together, explain two-thirds of the variations in price.</p>

Note - added italics
----------------------------------------------------------------------
In diva2:1450559 
abstract is: 
<p>Finding a way to forecast what characteristics make a fast growing company would be useful, both for companies trying to succeed and for investment companies wanting to make successful investments. This thesis aims to develop a model describing the relationship between 9 chosen characteristics, based on real data from 2015 concerning companies that were rewarded with a DI Gasell in 2018. The final result show that half of the variables chosen to form the model have little to no relationship with the response variable EBIT margin. However, the final model consists of four variables that with statistic significance correlates with the response variable. The explanatory level is low and implies that forecasting companies growth probably can’t be done using this model. The four regressors that correlate with EBIT margin are Year of Incorporation, Operatingrevenue, Number of subsidiaries &amp; SNI code. Although a forecast can’t be performed other insight are obtained from the research. Companies with SNI code 4, which corresponds to operating in the economic sector, affects EBIT margin in a more positive way than other sectors. Number of subsidiaries correlates fairly linearly with the response variable. Contradictory to previous research CEO characteristics are shown to be the least important factor contributing to profitability.</p>

corrected abstract:
<p>Finding a way to forecast what characteristics make a fast growing company would be useful, both for companies trying to succeed and for investment companies wanting to make successful investments. This thesis aims to develop a model describing the relationship between 9 chosen characteristics, based on real data from 2015 concerning companies that were rewarded with a DI Gasell in 2018.</p><p>The final result show that half of the variables chosen to form the model have little to no relationship with the response variable EBIT margin. However, the final model consists of four variables that with statistic significance correlates with the response variable. The explanatory level is low and implies that forecasting companies growth probably can’t be done using this model. The four regressors that correlate with EBIT margin are Year of Incorporation, Operating revenue, Number of subsidiaries &amp; SNI code. Although a forecast can’t be performed other insight are obtained from the research. Companies with SNI code 4, which corresponds to operating in the economic sector, affects EBIT margin in a more positive way than other sectors. Number of subsidiaries correlates fairly linearly with the response variable. Contradictory to previous research CEO characteristics are shown to be the least important factor contributing to profitability.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1527802   - correct as is
----------------------------------------------------------------------
In diva2:1431618   - correct as is
----------------------------------------------------------------------
In diva2:1891374   - correct as is
----------------------------------------------------------------------
In diva2:938902   - correct as is
----------------------------------------------------------------------
In diva2:1737475 
abstract is: 
<p>The transport sector plays a critical role in meeting the targets of achieving net-zero emissions. In comparison to traditional fuel vehicles (ICEVs), new energy vehicles, such as electric vehicles (EVs), have advantages of energy-saving and emission reduction. Despite policy plans for this transition, Sweden’s vehicle fleet is still predominantly composed by ICEVs. In order to enable an effective transition, models that create understanding of the dynamics of the transport system are necessary. This study aim's to simulate the dynamics of transitioning Sweden’s transport system in means of how the uptake of two different propulsion technologies might evolve. Specifically, the transition from the fossil-fuel dependent transport system, predominantly by ICEVs, towards an electrified transport system, consisting of EVs. </p><p>Considering the competition and substitution situation between ICEVs and EVs, a System Dynamic model is proposed to simulate the plausible trend of Sweden’s transport system based on the Lotka-Volterra population model. In addition to this, scenario analysis of the key model parameters is performed and their presumed effects is evaluated. The results of the plausible trend of Sweden’s transport system indicates that the model can be applied to reflect on the vehicle fleet evolution's and have potential to demonstrate trends and behaviour of the transport system. Nevertheless, in the scope of this thesis, the established system model includes some limitations and is not able to reflect on the complexity of the transport system fully - but this thesis, and the established system model can generate intuition on how the modelling approach can be applied and how factors can be integrated by which could explain the vehicle fleet evolution's.</p>

corrected abstract:
<p>The transport sector plays a critical role in meeting the targets of achieving net-zero emissions. In comparison to traditional fuel vehicles (ICEVs), new energy vehicles, such as electric vehicles electric vehicles (EVs), have advantages of energy-saving and emission reduction. Despite policy plans for this transition, Sweden’s vehicle fleet is still predominantly composed by ICEVs. In order to enable an effective transition, models that create understanding of the dynamics of the transport system are necessary. This study aim's to simulate the dynamics of transitioning Sweden’s transport system in means of how the uptake of two different propulsion technologies might evolve. Specifically, the transition from the fossil-fuel dependent transport system, predominantly by ICEVs, towards an electrified transport system, consisting of EVs.</p><p>Considering the competition and substitution situation between ICEVs and EVs, a System Dynamic model is proposed to simulate the plausible trend of Sweden’s transport system based on the Lotka-Volterra population model. In addition to this, scenario analysis of the key model parameters is performed and their presumed effects is evaluated. The results of the plausible trend of Sweden’s transport system indicates that the model can be applied to reflect on the vehicle fleet evolution's and have potential to demonstrate trends and behaviour of the transport system. Nevertheless, in the scope of this thesis, the established system model includes some limitations and is not able to reflect on the complexity of the transport system fully - but this thesis, and the established system model can generate intuition on how the modelling approach can be applied and how factors can be integrated by which could explain the vehicle fleet evolution's.</p>

Note - repeating text error in original "electric vehicles electric vehicles"
----------------------------------------------------------------------
In diva2:1279822 
abstract is: 
<p>To meet the need of lightweight chassis in the near future, a technological step of introducing anisotropic materials like Carbon Fibre Reinforced Plastics (CFRP) in structural parts of cars is a possible way ahead. Though there are commercially available tools to find suitability of Fibre Reinforced Plastics (FRPs) and their orientations, they depend on numerical optimization and complexity increases with the size of the model. Nevertheless, the user has a very limited control of intermediate steps. To understand the type of material system that can be used in different regions for a lightweight chassis, especially during the initial concept phase, a more simplified, yet reliable tool is desirable. The thesis aims to provide a framework for determining fibre orientations according to the most-ideal loading path to achieve maximum advantage from FRP-materials. This has been achieved by developing algorithms to find best-fit material orientations analytically, which uses principal stresses and their orientations in a finite element originating from multiple load cases. This thesis takes inspiration from the Durst criteria (2008) which upon implementation provides information on how individual elements must be modelled in a component subjected to multiple load cases. This analysis pre-evaluates the potential of FRP-suitable parts. Few modifications have been made to the existing formulations by the authors which have been explained in relevant sections. The study has been extended to develop additional MATLAB subroutines which finds the type of laminate design (uni-directional, bi-axial or quasi-isotropic) that is suitable for individual elements.  Several test cases have been run to check the validity of the developed algorithm. Finally, the algorithm has been implemented on a Body-In-White subjected to two load cases. The thesis gives an idea of how to divide the structure into sub-components along with the local fibre directions based on the fibre orientations and an appropriate laminate design based on classical laminate theory.</p>

corrected abstract:
<p>To meet the need of lightweight chassis in the near future, a technological step of introducing anisotropic materials like Carbon Fibre Reinforced Plastics (CFRP) in structural parts of cars is a possible way ahead. Though there are commercially available tools to find suitability of Fibre Reinforced Plastics (FRPs) and their orientations, they depend on numerical optimization and complexity increases with the size of the model. Nevertheless, the user has a very limited control of intermediate steps. To understand the type of material system that can be used in different regions for a lightweight chassis, especially during the initial concept phase, a more simplified, yet reliable tool is desirable.</p><p>The thesis aims to provide a framework for determining fibre orientations according to the most-ideal loading path to achieve maximum advantage from FRP-materials. This has been achieved by developing algorithms to find best-fit material orientations analytically, which uses principal stresses and their orientations in a finite element originating from multiple load cases. This thesis takes inspiration from the Durst criteria (2008) which upon implementation provides information on how individual elements must be modelled in a component subjected to multiple load cases. This analysis pre-evaluates the potential of FRP-suitable parts. Few modifications have been made to the existing formulations by the authors which have been explained in relevant sections.</p><p>The study has been extended to develop additional MATLAB subroutines which finds the type of laminate design (uni-directional, bi-axial or quasi-isotropic) that is suitable for individual elements.</p><p>Several test cases have been run to check the validity of the developed algorithm. Finally, the algorithm has been implemented on a Body-In-White subjected to two load cases. The thesis gives an idea of how to divide the structure into sub-components along with the local fibre directions based on the fibre orientations and an appropriate laminate design based on classical laminate theory.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1505242 
abstract is: 
<p>Sustainability thinking and environmental questions are often in the center of news today. Our planet is changing and us humans need to change our lifestyle in order to avoid potentially devastating events. New laws and regulations, along with new sustainability goals, are constantly arriving around the world. Within the maritime industry, multiple emission goals have recently been implemented which puts a lot of pressure on shipping companies around the world. This thesis introduces a new sales-tool, The Ecolometer, which purpose is to support sales-processes within Volvo Penta. The tool uses a Product Environmental Footprint methodology to quantify different environmental impact categories based on the Nine Planetary Boundaries model. Based on the Planetary Boundaries model, some focus categories have been selected which has been set as requirements for the new tool. The tool have been validated and tested on some cases, where different propulsion systems where investigated. The results shows that the method and tool is useful and applicable to the intended purpose. The tool provides an efficient and easy way to quantify and optimize environmental impact throughout the lifecycle of a vessel, while in the meantime making sustainability thinking accessible, and popular within the industry. The thesis also shows how necessary it is for Volvo Penta the implement this tool, in order to ensure that their sustainability investments is going in the right direction.</p>

corrected abstract:
<p>Sustainability thinking and environmental questions are often in the center of news today. Our planet is changing and us humans need to change our lifestyle in order to avoid potentially devastating events. New laws and regulations, along with new sustainability goals, are constantly arriving around the world. Within the maritime industry, multiple emission goals have recently been implemented which puts a lot of pressure on shipping companies around the world.</p><p>This thesis introduces a new sales-tool, <em>The Ecolometer</em>, which purpose is to support sales-processes within Volvo Penta. The tool uses a <em>Product Environmental Footprint</em> methodology to quantify different environmental impact categories based on the <em>Nine Planetary Boundaries</em> model. Based on the Planetary Boundaries model, some focus categories have been selected which has been set as requirements for the new tool.</p><p>The tool have been validated and tested on some cases, where different propulsion systems where investigated. The results shows that the method and tool is useful and applicable to the intended purpose. The tool provides an efficient and easy way to quantify and optimize environmental impact throughout the lifecycle of a vessel, while in the meantime making sustainability thinking accessible, and popular within the industry.</p><p>The thesis also shows how necessary it is for Volvo Penta the implement this tool, in order to ensure that their sustainability investments is going in the right direction.</p>

Note - added missing paragraph breaks and added italics
----------------------------------------------------------------------
In diva2:1082720   - correct as is
----------------------------------------------------------------------
In diva2:1817153 
abstract is: 
<p>GKN Aerospace AB, Sweden (GAS) is one of the leading companies taking up the charge in manufacturing components using Additive Manufacturing(ed) (AM) techniques in the aerospace sector. They are a hub of engineering and they are a supplier of engine and engine components to the world’s leading aero-engine manufacturers, and airframes to civil and military aircraft manufacturers. A phenomenon that is of interest to designers at GAS is the effects of dwell times on high temperature fatigue, especially how this phenomenon affects the fatigue properties of Laser Powder Bed Fusion (LPBF) Inconel 718 (IN718). IN718 is a versatile alloy that can be used at relatively high temperatures and has excellent weldability and is one of the newer materials replacing expensive materials such as Titanium (and its alloys) in the aerospace industry. The aerospace industry has been pushing for an increase in parts manufactured using AM processes because of the advantage the AM process grants to the production process, however a new manufacturing process for an industry needs to be studied and researched from a failure perspective, i.e. the prominent mode of failure for components manufactured using AM and the underlying factors influencing the failure mechanism must be studied. This thesis explores a solution to predict the life of components based on experimental crack propagation tests wherein the test specimens were subjected to the phenomenon mentioned above. A literature survey was conducted researching ways to model this phenomenon and the factors affecting it. The methods found in the literature survey were far too complex to model for the purposes of this thesis, additionally the methods described in the literature were empirical methods describing the phenomenon, rather than a fundamental study of factors causing the phenomenon and ways to model their influence on the life of the component.</p><p>Hence, a simple method based on the Palmgren-Miner linear damage summation rule which was coded in the form of a FORTRAN code was utilized to compute the life of the components. Software runs predicting life of physical experiments were conducted and inferences about the predictive method were drawn. The limitations of this method were understood and possible solutions were explored, based on which conclusions were drawn regarding the method’s efficacy in predicting the life of the specimens that underwent dwell loading during fatigue cycling. Finally, the method was applied to a case study to understand the effectiveness of the method.</p>

corrected abstract:
<p>GKN Aerospace AB, Sweden (GAS) is one of the leading companies taking up the charge in manufacturing components using Additive Manufacturing(ed) (AM) techniques in the aerospace sector. They are a hub of engineering and they are a supplier of engine and engine components to the world’s leading aero-engine manufacturers, and airframes to civil and military aircraft manufacturers.</p><p>A phenomenon that is of interest to designers at GAS is the effects of dwell times on high temperature fatigue, especially how this phenomenon affects the fatigue properties of Laser Powder Bed Fusion (LPBF) Inconel 718 (IN718). IN718 is a versatile alloy that can be used at relatively high temperatures and has excellent weldability and is one of the newer materials replacing expensive materials such as Titanium (and its alloys) in the aerospace industry.</p><p>The aerospace industry has been pushing for an increase in parts manufactured using AM processes because of the advantage the AM process grants to the production process, however a new manufacturing process for an industry needs to be studied and researched from a failure perspective, i.e. the prominent mode of failure for components manufactured using AM and the underlying factors influencing the failure mechanism must be studied.</p><p>This thesis explores a solution to predict the life of components based on experimental crack propagation tests wherein the test specimens were subjected to the phenomenon mentioned above. A literature survey was conducted researching ways to model this phenomenon and the factors affecting it. The methods found in the literature survey were far too complex to model for the purposes of this thesis, additionally the methods described in the literature were empirical methods describing the phenomenon, rather than a fundamental study of factors causing the phenomenon and ways to model their influence on the life of the component.</p><p>Hence, a simple method based on the Palmgren-Miner linear damage summation rule which was coded in the form of a FORTRAN code was utilized to compute the life of the components. Software runs predicting life of physical experiments were conducted and inferences about the predictive method were drawn.</p><p>The limitations of this method were understood and possible solutions were explored, based on which conclusions were drawn regarding the method’s efficacy in predicting the life of the specimens that underwent dwell loading during fatigue cycling.</p><p>Finally, the method was applied to a case study to understand the effectiveness of the method.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1779320   - correct as is
----------------------------------------------------------------------
In diva2:1255159 
abstract is: 
<p>This thesis investigated different configurations of the attitude determination and control system (ADCS) for the MIST satellite, to find a satisfying trade-off between computational demand and estimation/pointing accuracy. A model of the satellite dynamics was developed and used in a simulation. The designed ADCS consists of a discrete extended Kalman filter (EKF) and a model predictive control (MPC) controller tunable in different ways. The filter works with a linearization of the spacecraft dynamics model which is performed about the last attitude estimate and it is also capable of estimating the residual magnetic moment of the spacecraft without any initial guess. Three different models were used with the MPC and compared: a linear-like, state-dependent model, a model linearized around a fixed equilibrium point, and a model linearized around the last attitude estimate. The simulation, developed with Simulink, served as a testbed for the different tunings. From the simulation results, the filter proved to be capable of estimating the residual magnetic moment of the satellite with satisfying accuracy. Estimation and pointing requirements were met on average with a mean absolute estimation error of 0.8 deg and a mean absolute pointing error of 3.5 deg. This performance was achieved in face of measurement and model uncertainty.</p>

corrected abstract:
<p>This thesis investigated different configurations of the attitude determination and control system (ADCS) for the MIST satellite, to find a satisfying trade-off between computational demand and estimation/pointing accuracy. A model of the satellite dynamics was developed and used in a simulation. The designed ADCS consists of a discrete extended Kalman filter (EKF) and a model predictive control (MPC) controller tunable in different ways. The filter works with a linearization of the spacecraft dynamics model which is performed about the last attitude estimate and it is also capable of estimating the residual magnetic moment of the spacecraft without any initial guess. Three different models were used with the MPC and compared: a linear-like, state-dependent model, a model linearized around a fixed equilibrium point, and a model linearized around the last attitude estimate. The simulation, developed with Simulink®, served as a testbed for the different tunings. From the simulation results, the filter proved to be capable of estimating the residual magnetic moment of the satellite with satisfying accuracy. Estimation and pointing requirements were met on average with a mean absolute estimation error of 0.8º  and a mean absolute pointing error of 3.5º. This performance was achieved in face of measurement and model uncertainty.</p>

Note corrected the degree symbols "º" and added the registered trademark symbol ("®") to match the original
----------------------------------------------------------------------
In diva2:1319685 
abstract is: 
<p>In this paper, Monte Carlo simulation for CCR (Counterparty Credit Risk) modeling is investigated. A jump-diffusion model, Bates' model, is used to describe the price process of an asset, and the counterparty default probability is described by a stochastic intensity model with constant intensity. In combination with Monte Carlo simulation, the variance reduction technique importance sampling is used in an attempt to make the simulations more efficient. Importance sampling is used for simulation of both the asset price and, for CVA (Credit Valuation Adjustment) estimation, the default time. CVA is simulated for both European and Bermudan options. It is shown that a significant variance reduction can be achieved by utilizing importance sampling for asset price simulations. It is also shown that a significant variance reduction for CVA simulation can be achieved for counterparties with small default probabilities by employing importance sampling for the default times. This holds for both European and Bermudan options. Furthermore, the regression based method least squares Monte Carlo is used to estimate the price of a Bermudan option, resulting in CVA estimates that lie within an interval of feasible values. Finally, some topics of further research are suggested.</p>

corrected abstract:
<p>In this paper, Monte Carlo simulation for <em>CCR</em> (Counterparty Credit Risk) modeling is investigated. A jump-diffusion model, Bates' model, is used to describe the price process of an asset, and the counterparty default probability is described by a stochastic intensity model with constant intensity. In combination with Monte Carlo simulation, the variance reduction technique <em>importance sampling</em> is used in an attempt to make the simulations more efficient. Importance sampling is used for simulation of both the asset price and, for <em>CVA</em> (Credit Valuation Adjustment) estimation, the default time. CVA is simulated for both European and Bermudan options. It is shown that a significant variance reduction can be achieved by utilizing importance sampling for asset price simulations. It is also shown that a significant variance reduction for CVA simulation can be achieved for counterparties with small default probabilities by employing importance sampling for the default times. This holds for both European and Bermudan options. Furthermore, the regression based method <em>least squares Monte Carlo</em> is used to estimate the price of a Bermudan option, resulting in CVA estimates that lie within an interval of feasible values. Finally, some topics of further research are suggested.</p>

Note - added italics
----------------------------------------------------------------------
In diva2:1721352   - correct as is
----------------------------------------------------------------------
In diva2:460393   - correct as is
----------------------------------------------------------------------
In diva2:1328897   - correct as is
----------------------------------------------------------------------
In diva2:1348450 
abstract is: 
<p>An abdominal aorta aneurysm is a very complicated condition with limited medical treatments. A better understanding of the disease is therefore vital for development of new treatment methods. This report covers the ability to print models of an aortic aneurysm with the same material properties as the real ones. Such models would be of great benefit as it would allow scientists and doctors to both easily and cheaply produce viable models for their research. The printer tested was a Fused Filament Fabrication printer with TPU 95A polymer as choice of material. This was done with a two-step process. The first step being material testing, producing stress-strain curves of the polymer and evaluating the limitations of FFF printing. The second being a finite element analysis of an aortic aneurysm from a CT angiography scan. The results from the two approaches then were compared. The material testing gave TPU 95A an elongation of 2-4 % which was deemed to stiff when compared to the FEM-analysis, that had an elongation approximately five times larger. If an elastic material to satisfy the required elongation were used, the FFF printing method still would have to deal with print stability problems and an increased need of support structures that could block the polymer blood vessel. As such FFF printing was seemed inadequate for printing elastic equivalent aortic aneurysm models unless for specific tests with small deformations.</p>

corrected abstract:
<p>An abdominal aorta aneurysm is a very complicated condition with limited medical treatments. A better understanding of the disease is therefore vital for development of new treatment methods. This report covers the ability to print models of an aortic aneurysm with the same material properties as the real ones. Such models would be of great benefit as it would allow scientists and doctors to both easily and cheaply produce viable models for their research.</p><p>The printer tested was a Fused Filament Fabrication printer with TPU 95A polymer as choice of material. This was done with a two-step process. The first step being material testing, producing stress-strain curves of the polymer and evaluating the limitations of FFF printing. The second being a finite element analysis of an aortic aneurysm from a CT angiography scan. The results from the two approaches then were compared.</p><p>The material testing gave TPU 95A an elongation of 2-4 % which was deemed to stiff when compared to the FEM-analysis, that had an elongation approximately five times larger. If an elastic material to satisfy the required elongation were used, the FFF printing method still would have to deal with print stability problems and an increased need of support structures that could block the polymer blood vessel. As such FFF printing was seemed inadequate for printing elastic equivalent aortic aneurysm models unless for specific tests with small deformations.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1876090 
abstract is: 
<p>This work aims to develop a model capable of calculating the energy consumption of an electric car. The goal is a method of calculating the energy needs for a certain trip and presenting recommendations for a more efficient driving mode. The purpose of the work is to create a tool to help drivers use less energy and identify problems when developing such a tool. The model is based on a power equation which considers air resistance, rolling resistance, height differences and acceleration. A model of regenerative brakes is also developed in order to account for regained kinetic energy. Using map data, a routing tool is developed to allow input of a trip which the model can calculate energy needs for. The model shows for an example car Porsche Taycan 4S over a specific test-trip an energy consumption of 128.65 Wh/km and a 3% energy savings in relation to energy efficient driving. When evaluating the model against the Urban Dynamometer Driving Schedule (UDDS) and the New European Driving Cycle (NEDC) estimated driving ranges corresponding to 493.3 km and 521.2 km respectively, in relation to official statistics of 396 km. Finally, findings during development, problems with the model and recommendations are discussed. </p>

corrected abstract:
<p>This work aims to develop a model capable of calculating the energy consumption of an electric car. The goal is a method of calculating the energy needs for a certain trip and presenting recommendations for a more efficient driving mode. The purpose of the work is to create a tool to help drivers use less energy and identify problems when developing such a tool. The model is based on a power equation which considers air resistance, rolling resistance, height differences and acceleration. A model of regenerative brakes is also developed in order to account for regained kinetic energy. Using map data, a routing tool is developed to allow input of a trip which the model can calculate energy needs for. The model shows for an example car Porsche Taycan 4S over a specific test-trip an energy consumption of 128.65 Wh/km and a 3% energy savings in relation to energy efficient driving. When evaluating the model against the <em>Urban Dynamometer Driving Schedule (UDDS)</em> and the <em>New European Driving Cycle (NEDC)</em> estimated driving ranges corresponding to 493.3 km and 521.2 km respectively, in relation to official statistics of 396 km. Finally, findings during development, problems with the model and recommendations are discussed.</p>

Note eliminated an unnecessary space at the end of a paragraph and added italics
----------------------------------------------------------------------
In diva2:706733 
abstract is: 
<p>With today’s increase in rail traffic the impact of noise on the people near railway lines is increasing. To control this problem European Union regulations, including <em>TSI (Technical Specifications for Interoperability) Noise, </em>has come into force by implementing many strict norms on new railway vehicles put on the market. One <em>TSI </em>regulation is limiting the acceleration noise, which in turn calls for low noise solutions for drive systems which typically governs the vehicle noise at low speeds, up to around <em>80 Km/hr</em>. This regulation on railway noise has become a major challenge for many train manufacturers. This calls for electromagnetic-Vibration-Acoustics multi physics analysis of <em>the motor. </em></p><p>The thesis mainly focuses on numerical modelling of the electromagnetic-vibro-acoustics system. Ideally such models can compute the audible magnetic sound power radiated from the motor as a function of its speed, PWM strategy applied and geometry. Here a pulse modulated permanent magnet traction motor was modelled with finite elements (FE), using a commercial software. Thereafter, the dynamic characteristics of the motor was analysed by FE modal analysis (eigen-frequencies and eigen-modes) as well as by point force frequency response analysis. The FE model was validated by comparing the Eigen-modes, Eigen-frequencies and point accelerance with the experimental modal analysis results. Thereafter, frequency response analysis was performed to calculate the vibration velocities on the surface of the stator frame using realistic electromagnetic forces. These forces were calculated with a dedicated motor analysis tool (FLUX). Moreover, the vibration velocities calculated were used in boundary element model to calculate the radiated sound power and the directivity. Finally the calculated vibration and sound power levels were compared with measurement results.</p>

corrected abstract:
<p>With today’s increase in rail traffic the impact of noise on the people near railway lines is increasing. To control this problem European Union regulations, including <em>TSI (Technical Specifications for Interoperability) Noise,</em> has come into force by implementing many strict norms on new railway vehicles put on the market. One <em>TSI</em> regulation is limiting the acceleration noise, which in turn calls for low noise solutions for drive systems which typically governs the vehicle noise at low speeds, up to around <em>80 Km/hr</em>. This regulation on railway noise has become a major challenge for many train manufacturers. This calls for electromagnetic-Vibration-Acoustics multi physics analysis of <em>the motor</em>.</p><p>The thesis mainly focuses on numerical modelling of the electromagnetic-vibro-acoustics system. Ideally such models can compute the audible magnetic sound power radiated from the motor as a function of its speed, PWM strategy applied and geometry. Here a pulse modulated permanent magnet traction motor was modelled with finite elements (FE), using a commercial software. Thereafter, the dynamic characteristics of the motor was analysed by FE modal analysis (eigen-frequencies and eigen-modes) as well as by point force frequency response analysis. The FE model was validated by comparing the Eigen-modes, Eigen-frequencies and point accelerance with the experimental modal analysis results. Thereafter, frequency response analysis was performed to calculate the vibration velocities on the surface of the stator frame using realistic electromagnetic forces. These forces were calculated with a dedicated motor analysis tool (FLUX). Moreover, the vibration velocities calculated were used in boundary element model to calculate the radiated sound power and the directivity. Finally the calculated vibration and sound power levels were compared with measurement results.</p>

Note - adjusted the start and end of italics
----------------------------------------------------------------------
In diva2:753961   - correct as is
----------------------------------------------------------------------
In diva2:1450554 - no English abstract in full text
----------------------------------------------------------------------
In diva2:1335191 
abstract is: 
<p>Brain-computer interfaces (BCIs) are systems, communication pathways that enables users to interact with exterior devices without relying on bodily movements. This interaction between human and machine is possible by allowing the system access to read the brains electrical activity, often via electroencephalograms (EEG). The data is afterwards utilized as training sets to teach the BCI to map and predict selective brain patterns to certain commands. This papers main aim is to investigate the accuracy of two classification methods, linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) from two aspects, single-subject cross-validation and cross-subjects generalization. The data consisted of 30 subjects, two sessions for a binary classification task, that being, closing right/left hand. The results show that the accuracy of the classifiers LDA and QDA are relatively similar. The accuracy is defined as the number of correctly classified tasks in relation to the number of classes. For the result one-way Analysis of Variance (ANOVA) was utilizes, with the null-hypothesis that the mean-accuracy are the same for LDA and QDA. The mean-value results for single-subject cross- validation were between 57-59% . The accuracy for cross-subject generalizations were between 49-51 % . Both classifiers performed similarly, and the null hypothesis could not be rejected. However there are differences in performance when comparing single-subject cross-validation and cross-subject generalization. This paper further discuss factors that provided the results such as classification models LDA vs QDA, subjects and other causes.</p>

corrected abstract:
<p>Brain-computer interfaces (BCIs) are systems, communication pathways that enables users to interact with exterior devices without relying on bodily movements. This interaction between human and machine is possible by allowing the system access to read the brains electrical activity, often via electroencephalograms (EEG). The data is afterwards utilized as training sets to teach the BCI to map and predict selective brain patterns to certain commands.</p><p>This papers main aim is to investigate the accuracy of two classification methods, linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) from two aspects, single-subject cross-validation and cross-subjects generalization. The data consisted of 30 subjects, two sessions for a binary classification task, that being, closing right/left hand.</p><p>The results show that the accuracy of the classifiers LDA and QDA are relatively similar. The accuracy is defined as the number of correctly classified tasks in relation to the number of classes. For the result one-way Analysis of Variance (ANOVA) was utilizes, with the null-hypothesis that the mean-accuracy are the same for LDA and QDA. The mean-value results for single-subject cross-validation were between 57-59% . The accuracy for cross-subject generalizations were between 49-51 %. Both classifiers performed similarly, and the null hypothesis could not be rejected. However there are differences in performance when comparing single-subject cross-validation and cross-subject generalization. This paper further discuss factors that provided the results such as classification models LDA vs QDA, subjects and other causes.</p>

Note added missing paragraph breaks and eliminated unnecessary space after a hyphen and before a period
----------------------------------------------------------------------
In diva2:1130075 
abstract is: 
<p>The aim of this report is to design a transport aircraft capable of carrying supplies for 5 000 people, to last for a week, from anywhere in Europe to the equator. The design process is to be based on existing procedures that are being used by the aviation industries.An initial idea was conceived for a jet-powered subsonic carrier capable of carrying 70 ton of supplies 8 600 nautical miles. Calculations were made to determine factors such as weight, wing area, number of engines and various important coefficients. During the calculation phase a decision was made to change the initial concept entirely, to instead replace the single aircraft with two supersonic jets capable of carrying half the payload of the subsonic jet. Once the new concept had been defined calculations were done once again. The results of the design process have produced a supersonic jet capable of carrying 35 metric ton of payload at Mach 2 for 8 600 nautical miles. Such an aircraft can provide relief to disaster zones in less than 3 hours, a vast improvement compared to the eight hours it would take for the initial concept airplane to carry the needed supplies the same distance.</p>

mc='industries.An' c='industries. An'

corrected abstract:
<p>The aim of this report is to design a transport aircraft capable of carrying supplies for 5 000 people, to last for a week, from anywhere in Europe to the equator. The design process is to be based on existing procedures that are being used by the aviation industries.</p><p>An initial idea was conceived for a jet-powered subsonic carrier capable of carrying 70 ton of supplies 8 600 nautical miles. Calculations were made to determine factors such as weight, wing area, number of engines and various important coefficients. During the calculation phase a decision was made to change the initial concept entirely, to instead replace the single aircraft with two supersonic jets capable of carrying half the payload of the subsonic jet. Once the new concept had been defined calculations were done once again.</p><p>The results of the design process have produced a supersonic jet capable of carrying 35 metric ton of payload at Mach 2 for 8 600 nautical miles. Such an aircraft can provide relief to disaster zones in less than 3 hours, a vast improvement compared to the eight hours it would take for the initial concept airplane to carry the needed supplies the same distance.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:942668 
abstract is: 
<p>This thesis within mathematical statistics and industrial economics forms a foundation which explains how Sweden and other innovation-driven countries can act to achieve an efficient school. Multiple linear regression is used to identify explanatory variables that affect the results on the PISA test and factors that foster innovation are examined via a literature review. The included variables in the regression analysis were chosen out of four perspectives: general, school, teaching and student. The result showed that significant variables were found in all four perspectives. Moreover, the literature review was based on an individual perspective and an organisational perspective. The results are presented as an extensive overview of factors that affect innovation. A conclusion that higher teacher salaries and better quality of equipment in schools lead to more efficient learning is drawn. However, smaller class sizes do not affect quality of learning and more time spent in school by students affects the quality of learning negatively. Further conclusions are that schools should focus on group-based work instead of individual work. Students should also be taught to redefine problems and combine knowledge from different subjects to a greater extent.</p>


corrected abstract:
<p>This thesis within mathematical statistics and industrial economics forms a foundation which explains how Sweden and other innovation-driven countries can act to achieve an efficient school. Multiple linear regression is used to identify explanatory variables that affect the results on the PISA test and factors that foster innovation are examined via a literature review. The included variables in the regression analysis were chosen out of four perspectives: general, school, teaching and student. The result showed that significant variables were found in all four perspectives. Moreover, the literature review was based on an individual perspective and an organisational perspective. Result was presented as an extensive overview of factors that affect innovation. A conclusion that higher teacher salaries and better quality of equipment in schools lead to more efficient learning was drawn. However, smaller class sizes do not affect quality of learning and more time spent in school by students affects quality of learning negatively. Further conclusions were that schools should focus on group-based work instead of individual work. Students should also be taught to redefine problems and combine knowledge from different subjects to a greater extent.</p>

Note correctedd the wording so that it matches the original
----------------------------------------------------------------------
In diva2:942572 
abstract is: 
<p>This study aims to analyse the market for structured products in general and the pricing of equity­linked bonds and warrants in particular. The price of the equity­linked bonds is calculated combining the Black­Scholes­Merton’s model for pricing options with bond theory and the pricing of warrants is also modeled with Black­Scholes­Merton’s model. Thereafter the study examines whether the four major Swedish banks’ stated arrangement fees are consistent with the true estimated fees according to financial theory. The result indicates on a general level that the true fees exceed the fees that are stated in the term sheets of the products. The study further connects the result to the banks’ way of conducting business on the financial market for structured products, viewed from a CSR perspective, and analyses how fair the market is for private investors. The conclusion is that the banks have made progress with regards to CSR in recent years, with new laws and regulations in place. However there is still aspects that can be improved for the market of structured products to become more transparent and fair.</p>

corrected abstract:
<p>This study aims to analyse the market for structured products in general and the pricing of equity-linked bonds and warrants in particular. The price of the equity-linked bonds is calculated combining the Black-Scholes-Merton’s model for pricing options with bond theory and the pricing of warrants is also modeled with Black-Scholes-Merton’s model. Thereafter the study examines whether the four major Swedish banks’ stated arrangement fees are consistent with the true estimated fees according to financial theory. The result indicates on a general level that the true fees exceed the fees that are stated in the term sheets of the products.</p><p>The study further connects the result to the banks’ way of conducting business on the financial market for structured products, viewed from a CSR perspective, and analyses how fair the market is for private investors. The conclusion is that the banks have made progress with regards to CSR in recent years, with new laws and regulations in place. However there is still aspects that can be improved for the market of structured products to become more transparent and fair.</p>

Note - added missing paragraph break
----------------------------------------------------------------------
In diva2:1341326   - correct as is
----------------------------------------------------------------------
In diva2:406386 
abstract is: 
<p>Today the environment is a topic discussed eagerly by both the government, media and businesses. Just to do a search on the internet on the word environment provides more than 6 million hits and the first thing that comes up is most often linked to environmental threats, climate changes or how the municipalities / companies are working with environmental sustainability programs. As said before, the environment is a hot topic today, but how much are we humans affected by the information we receive? And what do we make of the knowledge that we receive? This was one of the starting points for this thesis that came to serve as motivation for the pre-investigation that took place in Axfoods stores. A pre-investigation which would be the basis for how to create a Web-based education in the Department of Environment for the employees of Axfood. An education that should be able to bring up the level of knowledge, awareness and motivation of the employees of Axfood, so that they will start thinking more about the environment, both at their work but also outside of it. The questions for this report are: What are the different meanings that can be identified in the employees and customers perception of the concept of environment? What are the underlying factors to the employees and customers behavior when it comes to acting for the environment? What does employees and customers in Axfood consider to be the crucial step for humans to become more engaged and motivated to act for a better environment? The empirical material is based on qualitative and quantitative interviews of employees and customers within Axfood enterprises. A material that according to an inductive method has been compiled and analyzed and which forms the basis for the conclusions reached in the report. The survey showed that it today is relatively equally between our positive and negative attitude towards the field environment, but one person who does not carry any previous experience on the topic and is now fed with a variety of negative impressions is showed to have a negative attitude to the subject environment. Similarly, a person who also carries the negative experiences from their childhood will today have a negative attitude. While a person who carries the positive experiences from their childhood have a positive attitude to the subject environment. It turns out that this can be crucial for how we act today to the environment. Negative experiences have a tendency to prevent us from working. The material shows that the staff and customers who participated in the investigation has a increased demand of an education in the department of Environment that will provide the foundations needed for a continued participation in the future work for sustainability. Where the focus of the study shows that an education like this one should help the employees with motivation, simplicity of contribution and to find out how I im fakt are with and are affecting my surroundings. Based on this material, several reasons on why one doesn´t do anything today for the environment is presented and also what would change this behavior. The biggest reason of why we today are not acting on the environment proved to be convenience, participants have lifestyles and routine patterns that are difficult to break or cut down on. This can also manifest itself in combination with the feeling of uncertainty about the subject Environment in connection with the lack of knowledge, while you at the same don’t get the motivation that is needed from media which leads to the feeling of powerlessness among the participants. The top priority today for bringing involvement among the participants is the education that was created by Axfood in the spring and summer of 2010.</p>

corrected abstract:
<p>Today the environment is a topic discussed eagerly by both the government, media and businesses. Just to do a search on the internet on the word environment provides more than 6 million hits and the first thing that comes up is most often linked to environmental threats, climate changes or how the municipalities / companies are working with environmental sustainability programs. As said before, the environment is a hot topic today, but how much are we humans affected by the information we receive? And what do we make of the knowledge that we receive? This was one of the starting points for this thesis that came to serve as motivation for the pre-investigation that took place in Axfoods stores. A pre-investigation which would be the basis for how to create a Web-based education in the Department of Environment for the employees of Axfood. An education that should be able to bring up the level of knowledge, awareness and motivation of the employees of Axfood, so that they will start thinking more about the environment, both at their work but also outside of it. The questions for this report are: What are the different meanings that can be identified in the employees and customers perception of the concept of environment? What are the underlying factors to the employees and customers behavior when it comes to acting for the environment? What does employees and customers in Axfood consider to be the crucial step for humans to become more engaged and motivated to act for a better environment?</p><p>The empirical material is based on qualitative and quantitative interviews of employees and customers within Axfood enterprises. A material that according to an inductive method has been compiled and analyzed and which forms the basis for the conclusions reached in the report.</p><p>The survey showed that it today is relatively equally between our positive and negative attitude towards the field environment, but one person who does not carry any previous experience on the topic and is now fed with a variety of negative impressions is showed to have a negative attitude to the subject environment. Similarly, a person who also carries the negative experiences from their childhood will today have a negative attitude. While a person who carries the positive experiences from their childhood have a positive attitude to the subject environment. It turns out that this can be crucial for how we act today to the environment. Negative experiences have a tendency to prevent us from working.</p><p>The material shows that the staff and customers who participated in the investigation has a increased demand of an education in the department of Environment that will provide the foundations needed for a continued participation in the future work for sustainability. Where the focus of the study shows that an education like this one should help the employees with motivation, simplicity of contribution and to find out how I im fakt are with and are affecting my surroundings. Based on this material, several reasons on why one doesn´t do anything today for the environment is presented and also what would change this behavior.</p><p>The biggest reason of why we today are not acting on the environment proved to be convenience, participants have lifestyles and routine patterns that are difficult to break or cut down on. This can also manifest itself in combination with the feeling of uncertainty about the subject Environment in connection with the lack of knowledge, while you at the same don’t get the motivation that is needed from media which leads to the feeling of powerlessness among the participants. The top priority today for bringing involvement among the participants is the education that was created by Axfood in the spring and summer of 2010.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1360663 
abstract is: 
<p>In this report, a Systems Engineering work is discussed, where an investigation has been carried out on the possibility of docking an autonomous underwater vessel with the new generation submarine A26. In the work, the focus has been on the early steps of the Systems Engineering discipline. A literature review of existing research and work in the field has been carried out in order to identify</p><p>possible technical solutions accessible today. Stakeholders have been identified and people with key positions in each area have been interviewed to be able to compile the requirement of needs. Based on the needs, abilities that the system needs to meet have been mapped. In order to facilitate the analysis of the docking system, a definition as well as a zoning of the various stages of the docking process have been performed. A description of different technologies for underwater communication is shown and discussed. An evaluation and risk analysis of a docking system has been carried out to illustrate the pros and cons of the various communication technologies during a docking procedure. Finally, two mechanical systems for the final phase of a docking have been compared to each other.</p>

corrected abstract:
<p>In this report, a Systems Engineering work is discussed, where an investigation has been carried out on the possibility of docking an autonomous underwater vessel with the new generation submarine A26. In the work, the focus has been on the early steps of the Systems Engineering discipline. A literature review of existing research and work in the field has been carried out in order to identify possible technical solutions accessible today. Stakeholders have been identified and people with key positions in each area have been interviewed to be able to compile the requirement of needs. Based on the needs, abilities that the system needs to meet have been mapped. In order to facilitate the analysis of the docking system, a definition as well as a zoning of the various stages of the docking process have been performed. A description of different technologies for underwater communication is shown and discussed. An evaluation and risk analysis of a docking system has been carried out to illustrate the pros and cons of the various communication technologies during a docking procedure. Finally, two mechanical systems for the final phase of a docking have been compared to each other.</p>

Note removed unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1341330   - correct as is
Note that the thesis is written in English, hence the title and alternate title are backwards
----------------------------------------------------------------------
In diva2:1375078 
abstract is: 
<p>A reinforced fiber-glass model of a NACA 4412 wing profile is designed and set-up in the Minimum Turbulence Level (MTL) wind tunnel facility at KTH. The model has 65 pressure taps orifices, and the set-up includes two mounting panels designed to allow for particle image velocimetry (PIV) and hot wire anemometry (HWA) measurements of the boundary layer (to be performed in a future campaign). In a first experimental campaign pressure scans are conducted at three angles of attack of interest (5,10 and 12 degrees), and at four different Reynolds numbers based on chord length and inflow velocity (200,000, 400,000, 1,000,000, and 1,640,000). The preliminary results show good agreement with DNS and LES data, however, the effective angle of attack of the wing is affected by the interference of the test section. In order to obtain proper flow conditions for future campaigns inside the test section, wall inserts are designed using 2D k-omega SST simulations. The side-walls are streamlined and the final geometry is corrected to account for the boundary-layer growth over them. The inserts are shown to avoid early separation near the trailing edge at higher angles of attack (10 and 12 degrees), but the 2D simulations fail to capture the aforementioned angle-of-attack issue affecting the pressure distributions. Future extensions of the present insert design should include both 3D simulations of the test-section and a robust optimization procedure to prescribe the resulting pressure distribution.</p>

corrected abstract:
<p>A reinforced fiber-glass model of a NACA 4412 wing profile is designed and set-up in the Minimum Turbulence Level (MTL) wind tunnel facility at KTH. The model has 65 pressure tap orifices, and the set-up includes two mounting panels designed to allow for particle image velocimetry (PIV) and hot wire anemometry (HWA) measurements of the boundary layer (to be performed in a future campaign). In a first experimental campaign pressure scans are conducted at three angles of attack of interest (5, 10 and 12 degrees), and at four different Reynolds numbers based on chord length and inflow velocity (200,000, 400,000, 1,000,000, and 1,640,000). The preliminary results show good agreement with DNS and LES data, however, the effective angle of attack of the wing is affected by the interference of the test section. In order to obtain proper flow conditions for future campaigns inside the test section, wall inserts are designed using 2D k-omega SST simulations. The side-walls are streamlined and the final geometry is corrected to account for the boundary-layer growth over them. The inserts are shown to avoid early separation near the trailing edge at higher angles of attack (10 and 12 degrees), but the 2D simulations fail to capture the aforementioned angle-of-attack issue affecting the pressure distributions. Future extensions of the present insert design should include both 3D simulations of the test-section and a robust optimization procedure to prescribe the resulting pressure distribution.</p>

Note "taps" changed to "tap" as in the original and added a space after "5,"
----------------------------------------------------------------------
In diva2:753944 
abstract is: 
<p>Today's high oil prices and more stringent environmental requirements have led to that manufacturers of mobile construction equipment try to make their products more energy efficient. With this in mind a project about hybridization of hydraulic systems has been formed. The purpose of this project was to analyze the savings that can be achieved by hybridization of hydraulic systems. A reach stacker truck (Kalmar DRD450-80S4XS) were simulated in some load cases with conventional hydraulic systems and with three types of hybridized hydraulic systems. Different parts of the hydraulic systems were also analyzed separately. The analysis has shown that significant energy savings can be achieved by hybridizing hydraulic systems. The analysis showed that between 9-30% energy savings could be achieved by hybridizing the truck's various subsystems.</p>

corrected abstract:
<p>Today's high oil prices and more stringent environmental requirements have led to that manufacturers of mobile construction equipment try to make their products more energy efficient. With this in mind a project about hybridization of hydraulic systems has been formed. The purpose of this project was to analyze the savings that can be achieved by hybridization of hydraulic systems.</p><p>A reach stacker truck (Kalmar DRD450-80S4XS) were simulated in some load cases with conventional hydraulic systems and with three types of hybridized hydraulic systems. Different parts of the hydraulic systems were also analyzed separately.</p><p>The analysis has shown that significant energy savings can be achieved by hybridizing hydraulic systems. The analysis showed that between 9-30 % energy savings could be achieved by hybridizing the truck's various subsystems.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1285840 
abstract is: 
<p>Arctic Trucks have been modifying vehicles such as the Toyota Hilux, Toyota Land Cruiser, Isuzu D-Max, Nissan Patrol and Nissan Navara for over 25 years for special projects as well as for recreational purposes both in Iceland and other countries. Arctic Trucks started up as a sub-division for Toyota Iceland but became an independent company in 2005. Their capability to make off-road vehicles is well known, the Toyota Hilux AT38 being their flagship. It has been driven to both the magnetic north pole, the South Pole and various other remote places and is widely used for logistics in Icelandic highlands as well as other places both during summer and winter time. This M.Sc. project in vehicle engineering covers measurements of energy consumption of a modified Toyota Hilux AT38 2017 in order to determine if some improvements are possible when it comes to fuel consumption and the vehicles environmental impact by hybridisation or full electrification of the vehicle. Fuel consumption is measured in various on-road and off-road conditions (gravel, snow and asphalt). Calculations are made to estimate the effect on fuel consumption of the climate control in the vehicle cabin. Air drag coefficient and friction coefficient are estimated based on coast-down tests in real-life conditions. These factors are necessary to evaluate the total running resistance of the vehicle. These fuel consumption measurements show that the fuel consumption for off-road driving is quite high and since this type of vehicle also needs to be light, the advantages of hybridisation or full electrification need to be examined further. For highway and city driving, hybridisation might be feasible but many factors need to be looked at for that case as well. As of now, battery technology and lack of infrastructure are standing in the way of this type of electric or plugin hybrid vehicles, since these vehicles are used in environments where electricity or even fossil fuel is hard and expensive to reach.</p>

corrected abstract:
<p>Arctic Trucks have been modifying vehicles such as the Toyota Hilux, Toyota Land Cruiser, Isuzu D-Max, Nissan Patrol and Nissan Navara for over 25 years for special projects as well as for recreational purposes both in Iceland and other countries. Arctic Trucks started up as a sub-division for Toyota Iceland but became an independent company in 2005. Their capability to make off-road vehicles is well known, the Toyota Hilux AT38 being their flagship. It has been driven to both the magnetic north pole, the south pole and various other remote places and is widely used for logistics in Icelandic highlands as well as other places both during summer and winter time.</p><p>This M.Sc. project in vehicle engineering covers measurements of energy consumption of a modified Toyota Hilux AT38 2017 in order to determine if some improvements are possible when it comes to fuel consumption and the vehicles environmental impact by hybridisation or full electrification of the vehicle.</p><p>Fuel consumption is measured in various on-road and off-road conditions (gravel, snow and asphalt). Calculations are made to estimate the effect on fuel consumption of the climate control in the vehicle cabin. Air drag coefficient and friction coefficient are estimated based on coast-down tests in real-life conditions. These factors are necessary to evaluate the total running resistance of the vehicle.</p><p>These fuel consumption measurements show that the fuel consumption for off-road driving is quite high and since this type of vehicle also needs to be light, the advantages of hybridisation or full electrification need to be examined further. For highway and city driving, hybridisation might be feasible but many factors need to be looked at for that case as well. As of now, battery technology and lack of infrastructure are standing in the way of this type of electric or plugin hybrid vehicles, since these vehicles are used in environments where electricity or even fossil fuel is hard and expensive to reach.</p>

Note spelling error:
w='coefficent' val={'c': 'coefficient', 's': 'diva2:1285840', 'n': 'error in original'}

Alao added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1247381 
abstract is: 
<p>This master thesis investigates and analyzes the energy use for traction and auxiliary equipment in passenger rail vehicles. It covers both the train service with passengers and when the trains are going through other stages in the everyday operation. The operational cycle and associated operational situations are introduced as a way of describing the varying use of a train over time. The descriptions focus on the most common activities and situations, such as stabling and parking, regular cleaning, inspections and maintenance. Also how these situations affect energy use by their need for different auxiliary systems to be active.</p><p>An energy model is developed based on the operational cycle as a primary input, together with relevant vehicle parameters and climate conditions. The latter proving to be a major influence on the energy used by the auxiliary equipment. The model is applied in two case studies, on SJ's X55 and Västtrafik's X61 trains. Both are modern electric multiple units equipped with energy meters. Model input is gathered from available technical documentation, previous studies and by measurements and parameter estimations. Operational cycle input is collected through different planning systems and rolling stock rosters. Climate input is finally compiled from open meteorological data banks.</p><p>The results of the case studies show that the method and models are useful for studying the energy used by the trains in their operational cycles. With the possibility to distinguish the energy used by the auxiliary equipment, both during and outside the time the trains are in service with passengers. With this it's also possible to further investigate and study potential energy saving measures for the auxiliary equipment. Simulations of new ventilation control functions and improved use of existing operating modes on the trains show that considerable energy savings could be achieved with potentially very small investments or changes to the trains.</p><p>The results generally show the importance of a continued investigation of the auxiliary equipment's energy use, as well as how the different operational situations other than the train service affect the total energy use.</p>

corrected abstract:
<p>This master thesis investigates and analyzes the energy use for traction and auxiliary equipment in passenger rail vehicles. It covers both the train service with passengers and when the trains are going through other stages in the everyday operation. The <em>operational cycle</em> and associated <em>operational situations</em> are introduced as a way of describing the varying use of a train over time. The descriptions focus on the most common activities and situations, such as stabling and parking, regular cleaning, inspections and maintenance. Also how these situations affect energy use by their need for different auxiliary systems to be active.</p><p>An energy model is developed based on the operational cycle as a primary input, together with relevant vehicle parameters and climate conditions. The latter proving to be a major influence on the energy used by the auxiliary equipment. The model is applied in two case studies, on SJ's X55 and Västtrafik's X61 trains. Both are modern electric multiple units equipped with energy meters. Model input is gathered from available technical documentation, previous studies and by measurements and parameter estimations. Operational cycle input is collected through different planning systems and rolling stock rosters. Climate input is finally compiled from open meteorological data banks.</p><p>The results of the case studies show that the method and models are useful for studying the energy used by the trains in their operational cycles. With the possibility to distinguish the energy used by the auxiliary equipment, both during and outside the time the trains are in service with passengers. With this it's also possible to further investigate and study potential energy saving measures for the auxiliary equipment. Simulations of new ventilation control functions and improved use of existing operating modes on the trains show that considerable energy savings could be achieved with potentially very small investments or changes to the trains.</p><p>The results generally show the importance of a continued investigation of the auxiliary equipment's energy use, as well as how the different operational situations other than the train service affect the total energy use.</p>

Note - added missing  italics
----------------------------------------------------------------------
In diva2:1385199   - correct as is
----------------------------------------------------------------------
In diva2:1849108   - correct as is
----------------------------------------------------------------------
In diva2:1869718 
abstract is: 
<p>This thesis investigates the application of TrueBalance, a conversational agent designed to support young adults vulnerable to eating disorders (EDs). TrueBalance integrates Cognitive Behavioral Therapy (CBT) techniques with biomedical insights, including genetic and neurobiological factors, to provide a more personalized and scientifically grounded support system. It addresses limitations in existing dietary monitoring tools that usually focus on calorie tracking and food intake, often neglecting the nuanced needs of specific groups like young females and elite athletes, who are particularly vulnerable to EDs and disordered eating behaviors. </p><p>The study addresses how biomedical determinants can be integrated into a conversational agent, how these agents can utilize CBT principles to support individuals vulnerable to EDs, and what challenges and opportunities arise from the user’s perspective when using such a dialogue model. The research strives to bridge the gap in current dietary self-monitoring tools by offering a more robust and empathetic support system for individuals struggling with EDs.</p><p>Through iterative development and user testing, TrueBalance has demonstrated its potential as an engaging educational tool. Feedback from both therapists and users has highlighted the tool’s utility in real-world settings. It has led to suggestions for enhancements in personalizing interactions and making response systems more adaptive. The findings suggest conversational agents like TrueBalance have potential in non-clinical support environments for individuals with EDs and function as a potential informative, supportive tool for therapists’ education.</p>

corrected abstract:
<p>This thesis investigates the application of TrueBalance, a conversational agent designed to support young adults vulnerable to eating disorders (EDs). TrueBalance integrates Cognitive Behavioral Therapy (CBT) techniques with biomedical insights, including genetic and neurobiological factors, to provide a more personalized and scientifically grounded support system. It addresses limitations in existing dietary monitoring tools that usually focus on calorie tracking and food intake, often neglecting the nuanced needs of specific groups like young females and elite athletes, who are particularly vulnerable to EDs and disordered eating behaviors.</p><p>The study addresses how biomedical determinants can be integrated into a conversational agent, how these agents can utilize CBT principles to support individuals vulnerable to EDs, and what challenges and opportunities arise from the user’s perspective when using such a dialogue model. The research strives to bridge the gap in current dietary self-monitoring tools by offering a more robust and empathetic support system for individuals struggling with EDs.</p><p>Through iterative development and user testing, TrueBalance has demonstrated its potential as an engaging educational tool. Feedback from both therapists and users has highlighted the tool’s utility in real-world settings. It has led to suggestions for enhancements in personalizing interactions and making response systems more adaptive. The findings suggest conversational agents like TrueBalance have potential in non-clinical support environments for individuals with EDs and function as a potential informative, supportive tool for therapists’ education.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:408786 
abstract is: 
<p>This is a study that aims to help create a better world. As big as it sounds, it is the truth. Every day municipality officers in Bosnia and Herzegovina work for a better environment, a better world. Despite their efforts the work they do is not as efficient as it could be and the support available for them is meagre. For this reason a programme called Municipality Environmental Infrastructure is under development in cooperation between the University of Sarajevo (Bosnia and Herzegovina) and the Royal Institute of Technology (Sweden) with financing from the Swedish International Development Cooperation Agency (Sida). This thesis is a part of the work to make the programme successful. Competence needs for municipality officers will be identified through a needs analysis based on interviews with different stakeholders. The answers provided will create a picture of the needs that is both univocal and diverse with competences in identifying and handling environmental threats as well as managing infrastructure projects. The thesis will also look at what pedagogical methods the teachers at the programme plan to use and how this affects the programme. Since the programme is held in a formal setting but intends practical use of the knowledge this leads to high demands on the pedagogical methods. The programme syllabus will be fount to not entirely encompass all competence needs but suggestions will be made as to how to include the identified needs.</p>

corrected abstract:
<p>This is a study that aims to help create a better world. As big as it sounds, it is the truth. Every day municipality officers in Bosnia and Herzegovina work for a better environment, a better world. Despite their efforts the work they do is not as efficient as it could be and the support available for them is meagre. For this reason a programme called Municipality Environmental Infrastructure is under development in cooperation between the University of Sarajevo (Bosnia and Herzegovina) and the Royal Institute of Technology (Sweden) with financing from the Swedish International Development Cooperation Agency (Sida). This thesis is a part of the work to make the programme successful.</p><p>Competence needs for municipality officers will be identified through a needs analysis based on interviews with different stakeholders. The answers provided will create a picture of the needs that is both univocal and diverse with competences in identifying and handling environmental threats as well as managing infrastructure projects.</p><p>The thesis will also look at what pedagogical methods the teachers at the programme plan to use and how this affects the programme. Since the programme is held in a formal setting but intends practical use of the knowledge this leads to high demands on the pedagogical methods. The programme syllabus will be fount to not entirely encompass all competence needs but suggestions will be made as to how to include the identified needs.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1816764 
abstract is: 
<p>The disturbance caused by railway noise has been investigated in this paper. A total of 127residents living near the Västra Stambanan were interviewed, and the noise levels from NordiskaBeräkningsmodellen were compared with the perceived disturbance. The results of the studyindicate a correlation between indoor noise levels and disturbance. The activities that peoplereported as being affected were meals, relaxation, sleep, and conversations in decreasing order.</p>

mc='peoplereported' c='people reported'
mc='studyindicate' c='study indicate'

corrected abstract:
<p>The disturbance caused by railway noise has been investigated in this paper. A total of 127 residents living near the Västra Stambanan were interviewed, and the noise levels from <span lang="sv">Nordiska Beräkningsmodellen</span> were compared with the perceived disturbance. The results of the study indicate a correlation between indoor noise levels and disturbance. The activities that people reported as being affected were meals, relaxation, sleep, and conversations in decreasing order.</p>

Note the added spaces and a language take for the model
----------------------------------------------------------------------
In diva2:828718   - correct as is
----------------------------------------------------------------------
In diva2:1255660 
abstract is: 
<p>This thesis focuses on investigating the usage of statistical models for estimating fuel consumption of heavy duty vehicles. Several statistical models are assessed, along with machine learning using artiﬁcial neural networks.</p><p>Data recorded by sensors on board trucks in the EU describe the operational usage of the vehicle. The usage of this data for estimating the fuel consumption is assessed, and several variables originating from the operational data is modelled and tested as possible input parameters.</p><p>The estimation model for real world fuel consumption uses 8 parameters describing the operational usage of the vehicles, and 8 parameters describing the vehicles themselves. The operational parameters describe the average speed, topography, variation of speed, idling, and more. This model has an average relative error of 5.75%, with a prediction error less than 11.14% for 95% of all tested vehicles.</p><p>When only vehicle parameters are considered, it is possible to make predictions with an average relative error of 9.30%, with a prediction error less than 19.50% for 95% of all tested vehicles.</p><p>Furthermore, a computer software called Vehicle Energy Consumption Calculation tool(VECTO) must be used to simulate the fuel consumption for all heavy duty vehicles, according to legislation by the EU. Running VECTO is a slow process, and this thesis also investigates how well statistical models can be used to quickly estimate the VECTO fuel consumption. The model estimates VECTO fuel consumption with an average relative error of 0.32%and with a prediction error less than 0.65% for 95% of all tested vehicles</p>

corrected abstract:
<p>This thesis focuses on investigating the usage of statistical models for estimating fuel consumption of heavy duty vehicles. Several statistical models are assessed, along with machine learning using artificial neural networks.</p><p>Data recorded by sensors on board trucks in the EU describe the operational usage of the vehicle. The usage of this data for estimating the fuel consumption is assessed, and several variables originating from the operational data is modelled and tested as possible input parameters.</p><p>The estimation model for real world fuel consumption uses 8 parameters describing the operational usage of the vehicles, and 8 parameters describing the vehicles themselves. The operational parameters describe the average speed, topography, variation of speed, idling, and more. This model has an average relative error of 5.75%, with a prediction error less than 11.14% for 95% of all tested vehicles.</p><p>When only vehicle parameters are considered, it is possible to make predictions with an average relative error of 9.30%, with a prediction error less than 19.50% for 95% of all tested vehicles.</p><p>Furthermore, a computer software called Vehicle Energy Consumption Calculation tool(VECTO) must be used to simulate the fuel consumption for all heavy duty vehicles, according to legislation by the EU. Running VECTO is a slow process, and this thesis also investigates how well statistical models can be used to quickly estimate the VECTO fuel consumption. The model estimates VECTO fuel consumption with an average relative error of 0.32% and with a prediction error less than 0.65% for 95% of all tested vehicles.</p>

Note added missing terminal period in last sentence (it is in the original) and added a space after "%" and before "and"
----------------------------------------------------------------------
In diva2:1799805   - correct as is
----------------------------------------------------------------------
In diva2:821274   - correct as is
----------------------------------------------------------------------
In diva2:721674 
abstract is: 
<p>It is well known that the inflation linked breakeven inflation, defined as the difference between a nominal yield and an inflation linked yield, sometimes is used as an approximation of the market’s inflation expectation. D’Amico et al. (2009, [5]) show that this is a poor approximation for the US market. Based on their work, this thesis shows that the approximation also is poor for the Swedish bond market. This is done by modelling the Swedish bond market using a five-factor latent variable model, where an inflation linked bond specific premium is introduced. Latent variables and parameters are estimated using a Kalman filter and a maximum likelihood estimation. The conclusion is drawn that the modelling was successful and that the model implied outputs gave plausible results.</p>

corrected abstract:
<p>It is well known that the inflation linked breakeven inflation, defined as the difference between a nominal yield and an inflation linked yield, sometimes is used as an approximation of the market’s inflation expectation. D’Amico <em>et al.</em> (2009, [5]) show that this is a poor approximation for the US market. Based on their work, this thesis shows that the approximation also is poor for the Swedish bond market. This is done by modelling the Swedish bond market using a five-factor latent variable model, where an inflation linked bond specific premium is introduced. Latent variables and parameters are estimated using a Kalman filter and a maximum likelihood estimation. The conclusion is drawn that the modelling was successful and that the model implied outputs gave plausible results.</p>

Note added missing italics
----------------------------------------------------------------------
In diva2:1739392 
abstract is: 
<p>In the modern world, vehicle manufacturers are not the sole producer of every component in a vehicle but instead assign different subsystems to various OEMs (Original Equipment Manufacturer). Subsystems such as Rear Drive Units (RDU), Power Transfer Units (PTU), E-drives etc. are developed separately and then integrated into the main vehicle. Integration of such subsystems with the main vehicle requires to be tested each time in order to measure its effects on the overall vehicle. This process highly inhibits the development of such individual subsystems. Hence, in order to reduce testing time while maximizing development time, the component-based TPA (Transfer Path Analysis) is used to predict the effects of each subsystem on the overall vehicle without actual assembly. This method makes use of the “transfer path” which is the path along which a source excites a receiver. In this experiment, the source (active component) is a Rear Drive Unit (RDU) while the receiver (passive component) is a subframe. In this case, the forces exerted by the RDU to the subframe at the contact points are calculated using the FRF of the combined source-receiver system with the measured responses on the subframe. Furthermore, these calculated forces are purely a characteristic of the source (RDU) and entirely independent of what receiver the source may be attached to. These forces are then used to predict the responses on any receiving structure attached to the same source. These characteristic forces calculated from the RDU are called blocked forces. These forces do not require any previous knowledge of the receiving structure. Hence, the primary aim of this report is to estimate and use blocked forces exerted by the RDU to predict responses on the subframe. An additional task was the comparison of predicted responses and measured responses on the subframe.</p>

corrected abstract:
<p>In the modern world, vehicle manufacturers are not the sole producer of every component in a vehicle but instead assign different subsystems to various OEMs (Original Equipment Manufacturer). Subsystems such as Rear Drive Units (RDU), Power Transfer Units (PTU), E-drives etc. are developed separately and then integrated into the main vehicle.</p><p>Integration of such subsystems with the main vehicle must be tested each time in order to measure its effects on the overall vehicle, this process highly inhibits the development of individual subsystems. Hence, in order to reduce testing time while maximizing development time. The component-based TPA (Transfer Path Analysis) is used to predict the effects of each subsystem on the overall vehicle without actual assembly.</p><p>In this case, the forces exerted by the RDU at the contact points are calculated and used to predict the responses on the receiving test rig structure. The forces estimated from the RDU are called blocked forces. These forces are a characteristic of the source and is independent of the vehicle design. Hence, it does not require previous knowledge of the receiving structure. The aim of this report is to use the component-based TPA method to estimate the blocked forces exerted by the RDU, using a combination of operational and passive measurements, to predict the responses on the receiving structure.</p>

Note major change in wording between DiVA abstract and the oroginal
----------------------------------------------------------------------
In diva2:1319717   - correct as is
----------------------------------------------------------------------
In diva2:1432609   - correct as is
----------------------------------------------------------------------
In diva2:1082719 
abstract is: 
<p>In certiﬁcation of new rail vehicles with respect to running characteristics, a wide variety of operating conditions needs to be considered. Behaviour on straight and curved tracks, including twisted tracks at various speeds, is of great importance. However, the wheel-rail friction should always be high corresponding to dry conditions. It means that the certiﬁcation tests have to be carried out during dry weather conditions and unlubricated rails. But measuring the friction at test conditions is a great challenge. Therefore, in a recent work (Petrov et al.), an algorithm was proposed for the continuous estimation of wheel-rail friction along both rails. The algorithm is based on wheel-rail forces in all three directions (Y, Q, X ) for both wheels in a wheelset, lateral contact position on its wheels and wheelset angle of attack. The algorithm was evaluated with a ﬁctional vehicle with vehicle-track dynamics simulations on various tracks (straight and curves) and track irregularities. In cooperation with SNC Lavalin (formerly Interﬂeet Technology), an opportunity arose to get the required data from on-track tests. In this way, all nine quantities above were measured during test runs of a new vehicle, so the algorithm could be evaluated under realistic conditions. The tests in tight curves of radius 150 m are used in the present work for this purpose. The measured data and the algorithm were processed in a Matlab program to get the friction estimate. Apart from the friction, the creepages and spin are also estimated with the aim to serve as quality indicators of the estimated friction. As the vehicle measurements include noise, errors and uncertainties, a statistical tool was introduced. Moreover, a sensitivity analysis was performed. It was observed that in these tight curves, the friction estimation on the outer wheel is poor, but a phenomenon arose that with increasing spin corresponding to even higher contact angle the friction might be estimated again. However, small spin gives a good friction estimation provided the total creep is high enough. Therefore, sharp curves, traction/braking or large track irregularities are necessary to estimate the friction well.In order to continue the work deeper into the area, other tests with the above scenarios would be useful</p>

mc='well.In' c='well. In'

corrected abstract:
<p>In certification of new rail vehicles with respect to running characteristics, a wide variety of operating conditions needs to be considered. Behaviour on straight and curved tracks, including twisted tracks at various speeds, is of great importance. However, the wheel-rail friction should always be high corresponding to dry conditions. It means that the certification tests have to be carried out during dry weather conditions and unlubricated rails. But measuring the friction at test conditions is a great challenge.</p><p>Therefore, in a recent work (Petrov et al.), an algorithm was proposed for the continuous estimation of wheel-rail friction along both rails. The algorithm is based on wheel-rail forces in all three directions (Y, Q, X ) for both wheels in a wheelset, lateral contact position on its wheels and wheelset angle of attack. The algorithm was evaluated with a fictional vehicle with vehicle-track dynamics simulations on various tracks (straight and curves) and track irregularities.</p><p>In cooperation with SNC Lavalin (formerly Interfleet Technology), an opportunity arose to get the required data from on-track tests. In this way, all nine quantities above were measured during test runs of a new vehicle, so the algorithm could be evaluated under realistic conditions. The tests in tight curves of radius 150 m are used in the present work for this purpose. The measured data and the algorithm were processed in a Matlab program to get the friction estimate. Apart from the friction, the creepages and spin are also estimated with the aim to serve as quality indicators of the estimated friction.</p><p>As the vehicle measurements include noise, errors and uncertainties, a statistical tool was introduced. Moreover, a sensitivity analysis was performed. It was observed that in these tight curves, the friction estimation on the outer wheel is poor, but a phenomenon arose that with increasing spin corresponding to even higher contact angle the friction might be estimated again. However, small spin gives a good friction estimation provided the total creep is high enough. Therefore, sharp curves, traction/braking or large track irregularities are necessary to estimate the friction well.</p><p>In order to continue the work deeper into the area, other tests with the above scenarios would be useful.</p>

Note added missing paragraph breaks and added missing terminal period in last sentence (it is in the original)
----------------------------------------------------------------------
In diva2:1137975   - correct as is
----------------------------------------------------------------------
In diva2:1334603   - correct as is
----------------------------------------------------------------------
In diva2:1214786 
abstract is: 
<p>This thesis is one in a group of several theses that are researching different subjects in the development of a new cryptocurrency. For a few years now, the cryptocurrency market has grown dramatically, in the lead of the original cryptocurrency Bitcoin. Today, most cryptocurrencies' validation-technology, including Bitcoin's, are based on Proof-of-Work (PoW), i.e., a system where transaction validation is made by servers calculating mathematical problems. PoW results in high energy consumption and slow transaction speed. In this cryptocurrency, the validation mechanism will build on a technology called Proof-of-Stake (PoS). PoS does not yield as high energy consumption and often leads to faster transaction speed. The specific technique for validation in this system is that validators bet their coins to validate transactions and get rewards in the form of transaction fees if they end up conforming the transactions that reach consensus among the validators. In particular, the purpose of this report is to research the risk and reward for validators in the betting process and from this develop a reward policy which yields a fast and secure validation. The methods used for solving the problems are simulations based on Monte Carlo methods. From the simulations, the results are discussed and compared. Also, this report will cover economic theories behind cryptocurrencies, mainly focusing on monetary policy and the transaction markets. The findings of this report are several risk functions for different topologies and winning conditions considered during the development of the cryptocurrency. Further, a conclusion was that the expected value of profit for validators need to be constant, independent of when the bets are made with regard to previous bets. From this, a reward function which distributes rewards between winning validators was formed. Another, economical conclusion from this was that, in the long run, the expected value of profit of betting should converge to zero due to a perfect competition market.</p>

corrected abstract:
<p>This thesis is one in a group of several theses that are researching different subjects in the development of a new cryptocurrency. For a few years now, the cryptocurrency market has grown dramatically, in the lead of the original cryptocurrency Bitcoin. Today, most cryptocurrencies' validation-technology, including Bitcoin's, are based on Proof-of-Work (PoW), i.e., a system where transaction validation is made by servers calculating mathematical problems. PoW results in high energy consumption and slow transaction speed. In this cryptocurrency, the validation mechanism will build on a technology called Proof-of-Stake (PoS). PoS does not yield as high energy consumption and often leads to faster transaction speed. The specific technique for validation in this system is that validators bet their coins to validate transactions and get rewards in the form of transaction fees if they end up confirming the transactions that reach consensus among the validators. In particular, the purpose of this report is to research the risk and reward for validators in the betting process and from this develop a reward policy which yields a fast and secure validation. The methods used for solving the problems are simulations based on Monte Carlo methods. From the simulations, the results are discussed and compared. Also, this report will cover economic theories behind cryptocurrencies, mainly focusing on monetary policy and the transaction markets. The findings of this report are several risk functions for different topologies and winning conditions considered during the development of the cryptocurrency. Further, a conclusion was that the expected value of profit for validators need to be constant, independent of when the bets are made with regard to previous bets. From this, a reward function which distributes rewards between winning validators was formed. Another, economical conclusion from this was that, in the long run, the expected value of profit of betting should converge to zero due to a perfect competition market.</p>

Note spelling error:
 'conforming' should be 'confirming' - correct in original
----------------------------------------------------------------------
In diva2:1656038   - correct as is
----------------------------------------------------------------------
In diva2:1746374 
abstract is: 
<p>In the printing industry quality is key, this is because a product is printed for a better appearance or to convey information. If the packaging for a product looks bad customers will get a bad first impression of the product.</p><p>The quality of the print can be affected by many different factors. The material properties of the paperboard such as stiffness, surface roughness, and thickness. It could also be affected by the coating of the paperboard, the properties of the ink, the material properties of the printing form, etc. </p><p>In this thesis, the effect of the contact time and the maximum pressure in the nip of a flexographic printing press was studied. To separate these variables six different desirable pressure pulses were decided upon. Three of them where the contact time remains constant while the maximum pressure changes, and three other pulses where the maximum pressure is constant while the contact time changes. The printing was done in an IGT F1 laboratory printing press. The different pressure pulses were achieved by using three different printing forms and varying the force settings in the printing press.</p><p>The mottle results from the mottle analysis do not show a clear reliance on the contact time or maximum pressure. Instead, the stiffness of the printing forms influences the results to such an extent that it overshadows any effect that the contact time or maximum pressure has. In the case of dot gain, it can be determined that a larger impression leads to more dot gain. However here the stiffness of the printing plays a large role, and no clear conclusions can be drawn when strictly comparing contact time or maximum pressure.</p>

corrected abstract:
<p>In the printing industry quality is key, this is because a product is printed for a better appearance or to convey information. If the packaging for a product looks bad customers will get a bad first impression of the product.</p><p>The quality of the print can be affected by many different factors. The material properties of the paperboard such as stiffness, surface roughness, and thickness. It could also be affected by the coating of the paperboard, the properties of the ink, the material properties of the printing form, etc.</p><p>In this thesis, the effect of the contact time and the maximum pressure in the nip of a flexographic printing press was studied. To separate these variables six different desirable pressure pulses were decided upon. Three of them where the contact time remains constant while the maximum pressure changes, and three other pulses where the maximum pressure is constant while the contact time changes. The printing was done in an IGT F1 laboratory printing press. The different pressure pulses were achieved by using three different printing forms and varying the force settings in the printing press.</p><p>The mottle results from the mottle analysis do not show a clear reliance on the contact time or maximum pressure. Instead, the stiffness of the printing forms influences the results to such an extent that it overshadows any effect that the contact time or maximum pressure has. In the case of dot gain, it can be determined that a larger impression leads to more dot gain. However here the stiffness of the printing plays a large role, and no clear conclusions can be drawn when strictly comparing contact time or maximum pressure.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1894653 
abstract is: 
<p>Low-cost sensors (LCS) are becoming increasingly popular for measuring the level of particulate matter (PM) in the air. However, issues with reliability require calibrations before the sensors can be used in regulatory settings. In this paper, the performance of an LCS is evaluated in relation to a reference grade monitor. Furthermore, a calibration model for the LCS is created and evaluated using maximum likelihood. </p><p>Central to this paper is the issue of autocorrelation in time series data. When performing calibrations using autocorrelated data, it’s important to consider this correlation when choosing the method for building the calibration model. This paper presents one method for dealing with calibration using highly autocorrelated data, namely restricted maximum likelihood.</p><p>Contrary to other existing studies on sensor calibration, this study does not employ the ordinary least-squares regression model to generate the calibration models, since the autocorrelation in the data violates the fundamental assumptions of independent and identically distributed residuals. Here, the maximum likelihood (ML) method is used to account for the autocorrelation in the data. Later on, the Akaike information criterion (AIC) is used to select the regressor variables which have the biggest impact on the model. </p><p>Finally, the results are presented and discussed. The results show that, for the specific data, the attempted models don’t produce satisfactory results. Several reasons for this are discussed, shedding light on the intricacies and challenges of modeling PM measurements using autocorrelated data.</p>

corrected abstract:
<p>Low-cost sensors (LCS) are becoming increasingly popular for measuring the level of particulate matter (PM) in the air. However, issues with reliability require calibrations before the sensors can be used in regulatory settings. In this paper, the performance of an LCS is evaluated in relation to a reference grade monitor. Furthermore, a calibration model for the LCS is created and evaluated using maximum likelihood.</p><p>Central to this paper is the issue of autocorrelation in time series data. When performing calibrations using autocorrelated data, it’s important to consider this correlation when choosing the method for building the calibration model. This paper presents one method for dealing with calibration using highly autocorrelated data, namely restricted maximum likelihood.</p><p>Contrary to other existing studies on sensor calibration, this study does not employ the ordinary least-squares regression model to generate the calibration models, since the autocorrelation in the data violates the fundamental assumptions of independent and identically distributed residuals. Here, the maximum likelihood (ML) method is used to account for the autocorrelation in the data. Later on, the Akaike information criterion (AIC) is used to select the regressor variables which have the biggest impact on the model.</p><p>Finally, the results are presented and discussed. The results show that, for the specific data, the attempted models don’t produce satisfactory results. Several reasons for this are discussed, shedding light on the intricacies and challenges of modeling PM measurements using autocorrelated data.</p>

Note only change was to eliminate an unnecessary space at the end of paragraphs
----------------------------------------------------------------------
In diva2:1464116 
abstract is: 
<p>As axle loads and speeds constantly increase in rail transport, new track systems are being developed. One such development is the ballastless track system. Today there are several types and variations of slab tracks, but how do they differ, and which one is the best? This thesis aims to answer these questions for given scenarios as each system has its unique set of strengths and therefore performs differently compared to the other systems for different projects. In this thesis, several existing ballastless track solutions have been studied. This was done viaballastless system manufacturer websites, brochures, other notable literature as well as multiple meetings with each of the system manufacturers. As a result, a descriptive list of nine different systems has been developed as well as a more detailed comparison in the shape of a table. To find out which one should be used and when, a model was developed for comparison of them. This model is based on a Multiple-criteria decision analysis (MCDA). This is a tool that can be used to compare different alternatives, based on several, often conflicting criteria. In the end, the VIKOR method was chosen. The choice was based on VIKOR’s user-friendliness, as well as implementation of auxiliary features, such as regret-value and compromise solutions. The MCDA based model was built in Excel and MATLAB and is expandable to the needs of the user. To test the model and whether it contains any bias, a sensitivity study was carried out. Ten hypothetical scenarios were set up and corresponding importance weights were assigned accordingly. The results were mixed and sparse for the different hypothetical scenarios and showed that no, or little, inherent biases were present in the model. Thus, the model proved to be successful in the end, and can therefore be a good addition to the selection process of a ballastless system alongside other studies, such as Life-cycle cost analysis (LCCA). There is however still some more development that could be done to improve the model. Finally, to demonstrate how the model is implemented for a rail project, a case study was carried out. The case study was conducted for a single hypothetical tunnel close to a city, assumed to be in Sweden. The background conditions were described, and the weighting process was illustrated and inserted to the model. For this particular case the ÖBB-Porr system from the Porr group proved to be the most suitable choice.</p>


corrected abstract:
<p>As axle loads and speeds constantly increase in rail transport, new track systems are being developed. One such development is the ballastless track system. Today there are several types and variations of slab tracks, but how do they differ, and which one is the best? This thesis aims to answer these questions for given scenarios as each system has its unique set of strengths and therefore performs differently compared to the other systems for different projects.</p><p>In this thesis, several existing ballastless track solutions have been studied. This was done via ballastless system manufacturer websites, brochures, other notable literature as well as multiple meetings with each of the system manufacturers. As a result, a descriptive list of nine different systems has been developed as well as a more detailed comparison in the shape of a table.</p><p>To find out which one should be used and when, a model was developed for comparison of them. This model is based on a Multiple-criteria decision analysis (MCDA). This is a tool that can be used to compare different alternatives, based on several, often conflicting criteria. In the end, the VIKOR method was chosen. The choice was based on VIKOR’s user-friendliness, as well as implementation of auxiliary features, such as regret-value and compromise solutions. The MCDA based model was built in Excel and MATLAB and is expandable to the needs of the user.</p><p>To test the model and whether it contains any bias, a sensitivity study was carried out. Ten hypothetical scenarios were set up and corresponding importance weights were assigned accordingly. The results were mixed and sparse for the different hypothetical scenarios and showed that no, or little, inherent biases were present in the model. Thus, the model proved to be successful in the end, and can therefore be a good addition to the selection process of a ballastless system alongside other studies, such as Life-cycle cost analysis (LCCA). There is however still some more development that could be done to improve the model.</p><p>Finally, to demonstrate how the model is implemented for a rail project, a case study was carried out. The case study was conducted for a single hypothetical tunnel close to a city, assumed to be in Sweden. The background conditions were described, and the weighting process was illustrated and inserted to the model. For this particular case the ÖBB-Porr system from the Porr group proved to be the most suitable choice.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1229321 
abstract is: 
<p>An ultrasound modeling tool, from the software newFASANT, which uses a ray- tracing technique is analyzed, tested and evaluated for industrial purpose in the automotive industry in collaboration with Volvo Car Corporation. The specific application for which it is evaluated is to simulate the sensor surrounding (traffic environment) for ultrasound sensors on cars. This is used in traffic simulators for testing assisted and automated parking functions in a virtual environment.</p><p>A theoretical study is carried out to analyze the method used in the tool. Ray-tracing and its background assumptions are compared to numerical simulation methods by an in-depth study of the main properties and advantages, disadvantages, and a computational complexity investigation. The results of this study deem the method advantageous from a computational speed point of view and complete enough in its results for the specific automotive application.</p><p>Tests are carried out on the newFASANT US module by comparing simple cases such as reflection from a disk to known theoretical results to establish the accuracy of the tool. More complex scenarios, with diffraction through narrow slits, are tested to investigate the limits of the software and its method (ray tracing). In these tests the tool is compared to another one, MATLAB toolbox, which makes use of a numerical method. Lastly an evaluation of performance is made with a common application scenario. The results of the tests were close to the theoretical predictions and the assumptions coming from the theoretical study of the methods. The modeling tool is then considered a valid option for simulating ultrasound propagation in car simulation programs.</p>

corrected abstract:
<p>An ultrasound modeling tool, from the software newFASANT, which uses a ray-tracing technique is analyzed, tested and evaluated for industrial purpose in the automotive industry in collaboration with Volvo Car Corporation. The specific application for which it is evaluated is to simulate the sensor surrounding (traffic environment) for ultrasound sensors on cars. This is used in traffic simulators for testing assisted and automated parking functions in a virtual environment.</p><p>A theoretical study is carried out to analyze the method used in the tool. Ray-tracing and its background assumptions are compared to numerical simulation methods by an in-depth study of the main properties and advantages, disadvantages, and a computational complexity investigation. The results of this study deem the method advantageous from a computational speed point of view and complete enough in its results for the specific automotive application.</p><p>Tests are carried out on the newFASANT US module by comparing simple cases such as reflection from a disk to known theoretical results to establish the accuracy of the tool. More complex scenarios, with diffraction through narrow slits, are tested to investigate the limits of the software and its method (ray tracing). In these tests the tool is compared to another one, MATLAB toolbox, which makes use of a numerical method. Lastly an evaluation of performance is made with a common application scenario. The results of the tests were close to the theoretical predictions and the assumptions coming from the theoretical study of the methods. The modeling tool is then considered a valid option for simulating ultrasound propagation in car simulation programs.</p>

Note removed unnecessary space after hyphen
----------------------------------------------------------------------
In diva2:1900936   - correct as is
----------------------------------------------------------------------
In diva2:1670582   - correct as is
----------------------------------------------------------------------
In diva2:867738   - correct as is
----------------------------------------------------------------------
In diva2:1715085 
abstract is: 
<p>The focus of this thesis was to increase the knowledge in the area of "dynamic friction" during oblique helmet impacts, both experimentally and numerically. Physical experiments have been performed with multiple helmets, different angles of the anvil and different surface materials, with results of impact forces from the anvil and accelerations of the head. By the use of the Coulomb friction model the friction, over time, during the impact of the head and helmet has been calculated. Finite element method has then been used in LS-DYNA to try and replicate the physical results with the goal of creating an accurate friction model. There has been previous work done where the commonly used abrasive paper has been compared to asphalt as ground material in the drop tests. A similar study has been performed in this project which has been compared to the previous work. The results when comparing asphalt to abrasive paper and stainless steel shows that abrasive paper has a higher friction and rotational acceleration of the head compared to asphalt. Stainless steel however displays similar characteristics as asphalt in both friction and accelerations. The load cell used during the experimental testing has been examined carefully since the value of the friction coefficient has differed depending on the angle and the impact location on the anvil. There are still uncertainties surrounding the reliability of the results from the load cell. LS-DYNA has two different ways of modelling friction, one where LS-DYNA uses a modified equation of the Coulomb friction model and another one where the user can insert a table of values on coefficient of friction and relative velocity. Both methods have been used, however the latter method has proven to be more suitable for the kind of tests used in this project.</p>

corrected abstract:
<p>The focus of this thesis was to increase the knowledge in the area of ”dynamic friction” during oblique helmet impacts, both experimentally and numerically. Physical experiments have been performed with multiple helmets, different angles of the anvil and different surface materials, with results of impact forces from the anvil and accelerations of the head. By the use of the Coulomb friction model the friction, over time, during the impact of the head and helmet has been calculated. Finite element method has then been used in LS-DYNA to try and replicate the physical results with the goal of creating an accurate friction model. There has been previous work done where the commonly used abrasive paper has been compared to asphalt as ground material in the drop tests. A similar study has been performed in this project which has been compared to the previous work. The results when comparing asphalt to abrasive paper and stainless steel shows that abrasive paper has a higher friction and rotational acceleration of the head compared to asphalt. Stainless steel however displays similar characteristics as asphalt in both friction and accelerations. The load cell used during the experimental testing has been examined carefully since the value of the friction coefficient has differed depending on the angle and the impact location on the anvil. There are still uncertainties surrounding the reliability of the results from the load cell. LS-DYNA has two different ways of modelling friction, one where LS-DYNA uses a modified equation of the Coulomb friction model and another one where the user can insert a table of values on coefficient of friction and relative velocity. Both methods have been used, however the latter method has proven to be more suitable for the kind of tests used in this project.</p>

Note only change to make the double quotes the same as in the original
----------------------------------------------------------------------
In diva2:957678   - correct as is
----------------------------------------------------------------------
In diva2:1449680 
abstract is: 
<p>This study aims to evaluate to which degree CFD can complement traditional towing-tank experiments, and to develop a computationally cheap and robust methodology for these type of simulations. Two radically different surface ship hulls were chosen for the trials: a capesize bulk carrier and a fast displacement hull. A bare submarine hull was also used to benchmark the software in the early stages of the study. All simulations were Reynolds–Averaged Navier–Stokes (RANS) simulations using the <em>k-w-</em>SST turbulence model. The chosen software was OpenFOAM 5.x and foam-extend 4.1 coupled with the commercial expansion Naval Hydro Pack, which is supposed to handle high Courant numbers well.</p><p>Producing a high-quality mesh which is able to capture both the free surface and the boundary layers proved to be of utmost importance and the meshing procedure is thus discussed in detail. A majority of the surface ship simulations suffered from a phenomenon known as numerical ventilation. The effect seemed to be much worse for the fast-displacement hull and the drag estimates for this hull deviated around 16.1% from experimental data. The bulk carrier was less affected and the drag estimates for this hull only deviated around 6.3% from experimental data. In order to reduce the computational cost, all surface ship simulations were run with a maximum Courant number of 25 and some consequences of this are also discussed.</p>

corrected abstract:
<p>This study aims to evaluate to which degree CFD can complement traditional towing-tank experiments, and to develop a computationally cheap and robust methodology for these type of simulations. Two radically different surface ship hulls were chosen for the trials: a capesize bulk carrier and a fast displacement hull. A bare submarine hull was also used to benchmark the software in the early stages of the study. All simulations were Reynolds–Averaged Navier–Stokes (RANS) simulations using the 𝑘-<em>ω</em>-SST turbulence model. The chosen software was OpenFOAM 5.x and foam-extend 4.1 coupled with the commercial expansion Naval Hydro Pack, which is supposed to handle high Courant numbers well.</p><p>Producing a high-quality mesh which is able to capture both the free surface and the boundary layers proved to be of utmost importance and the meshing procedure is thus discussed in detail. A majority of the surface ship simulations suffered from a phenomenon known as numerical ventilation. The effect seemed to be much worse for the fast-displacement hull and the drag estimates for this hull deviated around 16.1% from experimental data. The bulk carrier was less affected and the drag estimates for this hull only deviated around 6.3% from experimental data. In order to reduce the computational cost, all surface ship simulations were run with a maximum Courant number of 25 and some consequences of this are also discussed.</p>

Note - replaced "k" by "𝑘" and "w" by "ω"
----------------------------------------------------------------------
In diva2:1244661 
abstract is: 
<p>Better Shelter is a company, developing and providing module housing units for refugees in crisis areas. These module housing units are kept in place against wind and harsh conditions by 10 ground anchors, that are buried and tied to the frame. This bachelor thesis study and evaluate the mechanical behavior of these anchors, investigate the possibility to replace the current material from an aluminum alloy to a glass fibre-reinforced plastic and present a new and improved design. Evaluation of the ground anchor wire is not relevant in this report.</p><p>The results are based on four simulations executed in the FEM program, ANSYS Workbench, including provided and custom made CAD models of the anchor for both materials. From relevant loading cases, static and alternating, with an amplitude force of 2000 N , relevant information was collected and analyzed. Relevant data in this case include equivalent and maximum principal stress as well as the safety factor and cyclic fatigue life.</p><p>The simulations showed that minor plasticity and damage will occur to the current anchor in both materials. This is debatable since the computer simulation is based on models that are often an unreal representation of reality. As an example in this case, the stress levels in sharp corners of the CAD model highly exceeds the levels that would be considered realistic. However, great improvements could be observed with the new anchor design in both materials regarding strength and reduced volume. This concludes that a change of design and material is motivated mechanically and economically. Improvements include lowering cost to 4.44 SEK per unit which is a reduction by 54.8% as well as reducing the weight from 29 g to 11.8 g per unit.</p>

corrected abstract:
<p>Better Shelter is a company, developing and providing module housing units for refugees in crisis areas. These module housing units are kept in place against wind and harsh conditions by 10 ground anchors, that are buried and tied to the frame. This bachelor thesis study and evaluate the mechanical behavior of these anchors, investigate the possibility to replace the current material from an aluminum alloy to a glass fibre-reinforced plastic and present a new and improved design. Evaluation of the ground anchor wire is not relevant in this report.</p><p>The results are based on four simulations executed in the FEM program, ANSYS Workbench, including provided and custom made CAD models of the anchor for both materials. From relevant loading cases, static and alternating, with an amplitude force of 2000 𝑁, relevant information was collected and analyzed. Relevant data in this case include equivalent and maximum principal stress as well as the safety factor and cyclic fatigue life.</p><p>The simulations showed that minor plasticity and damage will occur to the current anchor in both materials. This is debatable since the computer simulation is based on models that are often an unreal representation of reality. As an example in this case, the stress levels in sharp corners of the CAD model highly exceeds the levels that would be considered realistic. However, great improvements could be observed with the new anchor design in both materials regarding strength and reduced volume. This concludes that a change of design and material is motivated mechanically and economically. Improvements include lowering cost to 4.44 <em>SEK</em> per unit which is a reduction by 54.8% as well as reducing the weight from 29 𝑔 to 11.8 𝑔 per unit.</p>

Note replace "N" by "𝑁", set SEK in italics, and replaced "g" by "𝑔"
----------------------------------------------------------------------
In diva2:1052984   - correct as is
----------------------------------------------------------------------
In diva2:1818047   - correct as is
----------------------------------------------------------------------
In diva2:1719774   - correct as is
----------------------------------------------------------------------
In diva2:1514597   - correct as is
----------------------------------------------------------------------
In diva2:940431   - correct as is
----------------------------------------------------------------------
In diva2:1756999   - correct as is
----------------------------------------------------------------------
In diva2:1714255 
abstract is: 
<p>The through-thickness mechanical behavior of paperboard is fundamental for converting operations and many end-use situations of paper-based products. High quality and uniformity of the through-thickness properties is ensured during paperboard manufacturing by use of on-line and laboratory quality control methods. One important test method used for quantifying the delamination strength of paperboard is the Scott bond type test [1]. In the Scott bond test, a swinging pendulum impacts an aluminum angle which is adhered to a paper sample. The loss of potential energy of the pendulum due to the rupturing of the paper piece is interpreted as the delamination strength of the paper material. Deviations and scatter of Scott bond values steer the production to maintain the desired strength of the material.</p><p>The objective of this thesis is to minimize the scatter of data obtained from Scott bond type testing by building a better understanding of the method and the interaction between paperboard properties and Scott bond values.</p><p>Scott bond testing of three-ply paperboard and splitted paperboard sheets were performed to enable testing of the whole construction as well as testing of the interfaces and the weakest ply separately. The Scott bond results were normalized with respect to the true delaminated surface of ruptured specimens to reduce the scatter of the results and to obtain a more accurate measurement of the delamination resistance of the material. The Scott bond sequence was also photographed using high-speed photography to explore the activated mechanisms during delamination of specimens. In addition, the 180 degree peel test and z-directional tensile test were performed to study the correlation between Scott bond values and the results from other quality control methods developed to measure the delamination strength of paperboard.</p><p>The results show that normalizing the Scott bond data, by accounting for the true delaminated surface, and rejecting specimens that are impacted twice by the pendulum during the Scott bond sequence reduces the scatter of the data significantly. It also improves the correlation with the true delamination strength of the material measured using splitted paperboard sheets. Furthermore, the same procedure improves the correlation between Scott bond data and data obtained from peel testing and z-direction tensile testing. High-speed photography of the Scott bond sequence showed that variations of the delamination mechanism causes scatter of Scott bond data, which overestimates the measured delamination resistance of paperboard.</p><p>The main conclusions from the thesis are that the scatter of Scott bond data can be minimized by normalizing Scott bond values with respect to true delaminated surface of ruptured specimens and by rejecting data from specimens that are impacted twice by the pendulum. The scatter of the data can also be reduced by being consistent with the chosen direction of the specimen by facing either the top ply or bottom ply upwards on the sample stage.</p>

corrected abstract:
<p>The through-thickness mechanical behavior of paperboard is fundamental for converting operations and many end-use situations of paper-based products. High quality and uniformity of the through-thickness properties is ensured during paperboard manufacturing by use of on-line and laboratory quality control methods. One important test method used for quantifying the delamination strength of paperboard is the Scott bond type test [1]. In the Scott bond test, a swinging pendulum impacts an aluminum angle which is adhered to a paper sample. The loss of potential energy of the pendulum due to the rupturing of the paper piece is interpreted as the delamination strength of the paper material. Deviations and scatter of Scott bond values steer the production to maintain the desired strength of the material.</p><p>The objective of this thesis is to minimize the scatter of data obtained from Scott bond type testing by building a better understanding of the method and the interaction between paperboard properties and Scott bond values.</p><p>Scott bond testing of three-ply paperboard and splitted paperboard sheets were performed to enable testing of the whole construction as well as testing of the interfaces and the weakest ply separately. The Scott bond results were normalized with respect to the true delaminated surface of ruptured specimens to reduce the scatter of the results and to obtain a more accurate measurement of the delamination resistance of the material. The Scott bond sequence was also photographed using high-speed photography to explore the activated mechanisms during delamination of specimens. In addition, the 180º peel test and z-directional tensile test were performed to study the correlation between Scott bond values and the results from other quality control methods developed to measure the delamination strength of paperboard.</p><p>The results show that normalizing the Scott bond data, by accounting for the true delaminated surface, and rejecting specimens that are impacted twice by the pendulum during the Scott bond sequence reduces the scatter of the data significantly. It also improves the correlation with the true delamination strength of the material measured using splitted paperboard sheets. Furthermore, the same procedure improves the correlation between Scott bond data and data obtained from peel testing and z-direction tensile testing. High-speed photography of the Scott bond sequence showed that variations of the delamination mechanism causes scatter of Scott bond data, which overestimates the measured delamination resistance of paperboard.</p><p>The main conclusions from the thesis are that the scatter of Scott bond data can be minimized by normalizing Scott bond values with respect to true delaminated surface of ruptured specimens and by rejecting data from specimens that are impacted twice by the pendulum. The scatter of the data can also be reduced by being consistent with the chosen direction of the specimen by facing either the top ply or bottom ply upwards on the sample stage.</p>

Note replaced "degree" with "º" as in the original
----------------------------------------------------------------------
In diva2:1341290 
abstract is: 
<p>The objective of this project in aeronautical engineering is to conceptually design an electric aircraft for service in the year 2030. The mission profile of this aircraft is to fly in urban environments with electric vertical take-off and landing (eVTOL) by utilizing tiltrotors. Flying trips within or between cities, the eVTOL aircraft combines the take-off advantages of helicopters with the cruise speed and efficiency of airplanes, making for a versatile aircraft suitable for the urban environment. With a capacity of one pilot and three passengers with luggage the range is 315 km with a cruise speed of 100 m/s at an altitude of 500 m. With a wingspan of 10 m and a fuselage length of 9.2 m the aircraft has a take-off weight of 2 metric tonnes out of which 350 kg are battery packs. The eVTOL has two contra-rotating propellers on the tips of the forward swept wing and two single propellers on a V-tail to a total of six 194 kW electric motors in four tilting assemblies providing an operational peak thrust-to-weight ratio of 1.39.</p>

corrected abstract:
<p>The objective of this project in aeronautical engineering is to conceptually design an electric aircraft for service in the year 2030. The mission profile of this aircraft is to fly in urban environments with electric vertical take-off and landing (eVTOL) by utilizing tiltrotors. Flying trips within or between cities, the eVTOL aircraft combines the take-off advantages of helicopters with the cruise speed and efficiency of airplanes, making for a versatile aircraft suitable for the urban environment.</p><p>With a capacity of one pilot and three passengers with luggage the range is 315 km with a cruise speed of 100 m/s at an altitude of 500 m. With a wingspan of 10 m and a fuselage length of 9.2 m the aircraft has a take-off weight of 2 metric tonnes out of which 350 kg are battery packs.</p><p>The eVTOL has two contra-rotating propellers on the tips of the forward swept wing and two single propellers on a V-tail to a total of six 194 kW electric motors in four tilting assemblies providing an operational peak thrust-to-weight ratio of 1.39.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1198967 
abstract is: 
<p>The thesis examines propeller configurations for the propulsion system for the water mode in the amphibious craft concept Newt, which also includes contra-rotating propellers. Newt is a research project on innovative commuter vehicles carried out by the naval architecture department at KTH. The project aims to enable new commuter routes for both transportation by land and water do to the properties of the vehicle. The thesis focuses on the propulsion for the water mode, where three design requirements for different speeds must be considered. The vehicle shall:</p><p>1.be able to overcome the resistance until the foils lift the hull out of the water;</p><p>2.achieve high efficiency at cruising speed; and</p><p>3.enable transport speeds over 25 knots.</p><p>Different concepts have been evaluated where limitations in space and available subcomponents has guided the choice selection of the this. The propellers have been modeled with combination of the momentum theory and the blade element theory. The propellers have also been modeled with the commercial software Helicilel. The design of the propellers has been based on literature studies and different combinations of torque, rotation speed, cord length, NACA profiles, pitch and inflow rates have been tested. The system is dimensioned against the resistance which is based on small-scale model test that have been carried out. The result is four propellers that is placed as two pairs either as two contra rotating propeller pods or as four standalone pods. Dimensioning has focused on overcoming the planning threshold as well archive high efficiency at cruising speed. The top speed is estimated to 29 knots.</p>

corrected abstract:
<p>The thesis examines propeller configurations for the propulsion system for the water mode in the amphibious craft concept Newt, which also includes contra-rotating propellers. Newt is a research project on innovative commuter vehicles carried out by the naval architecture department at KTH. The project aims to enable new commuter routes for both transportation by land and water do to the properties of the vehicle. The thesis focuses on the propulsion for the water mode, where three design requirements for different speeds must be considered. The vehicle shall:</p><ol><li>be able to overcome the resistance until the foils lift the hull out of the water;</li><li>achieve high efficiency at cruising speed; and</li><li>enable transport speeds over 25 knots.</li></ol><p>Different concepts have been evaluated where limitations in space and available subcomponents has guided the choice selection of the this. The propellers have been modeled with combination of the momentum theory and the blade element theory. The propellers have also been modeled with the commercial software Helicilel. The design of the propellers has been based on literature studies and different combinations of torque, rotation speed, cord length, NACA profiles, pitch and inflow rates have been tested. The system is dimensioned against the resistance which is based on small-scale model test that have been carried out. The result is four propellers that is placed as two pairs either as two contra rotating propeller pods or as four standalone pods. Dimensioning has focused on overcoming the planning threshold as well archive high efficiency at cruising speed. The top speed is estimated to 29 knots.</p>

Note added the HTML for the numbered list
----------------------------------------------------------------------
In diva2:1215662 
abstract is: 
<p>A telecommunications network is divided into cells, which have radio properties to lessen interference. Users move between these cells with their equipment.  If the equipment is actively used, it goes through a process called handover when it moves between cells, this creates sequences of visited cells. This thesis investigates these handovers and the corresponding sequences of visited cells.</p><p>In this thesis there are two objectives related to the handovers between cells. The first is to determine if different types of sequences have different proportions of unwanted behaviour, the second is to develop a method to detect changes in the patterns of the handovers, between different time periods.</p><p>For both objectives it is examined if the sequences of visited cells can be modelled as r-order Markov chains. For the first objective, it is examined if there are different proportions of unwanted behaviour for the r most recently visited cells, using a Markov chain approach. The sequences are also examined as a whole with a clustering method using dissimilarity matrices. For the second objective, it is first examined if it is possible to model the sequences of visited cells from different time periods as Markov chains and then perform a homogeneity test between them. After that it is examined if dissimilarity metrics could be used to detect changes between time periods, this is done using dissimilarity matrices.</p><p>In the end it can be concluded that different types of sequences have different proportions of unwanted behaviour. Furthermore, it can be concluded that the approach of modelling the sequences as Markov chains in order to detect changes in handover behaviour between time periods, does not work. Finally, it is concluded that dissimilarity metrics could be used to detect changes between time periods, and additionally, some suitable dissimilarity metrics are presented.</p>

corrected abstract:
<p>A telecommunications network is divided into cells, which have radio properties to lessen interference. Users move between these cells with their equipment.  If the equipment is actively used, it goes through a process called handover when it moves between cells, this creates sequences of visited cells. This thesis investigates these handovers and the corresponding sequences of visited cells.</p><p>In this thesis there are two objectives related to the handovers between cells. The first is to determine if different types of sequences have different proportions of unwanted behaviour, the second is to develop a method to detect changes in the patterns of the handovers, between different time periods.</p><p>For both objectives it is examined if the sequences of visited cells can be modelled as r-order Markov chains. For the first objective, it is examined if there are different proportions of unwanted behaviour for the 𝑟 most recently visited cells, using a Markov chain approach. The sequences are also examined as a whole with a clustering method using dissimilarity matrices. For the second objective, it is first examined if it is possible to model the sequences of visited cells from different time periods as Markov chains and then perform a homogeneity test between them. After that it is examined if dissimilarity metrics could be used to detect changes between time periods, this is done using dissimilarity matrices.</p><p>In the end it can be concluded that different types of sequences have different proportions of unwanted behaviour. Furthermore, it can be concluded that the approach of modelling the sequences as Markov chains in order to detect changes in handover behaviour between time periods, does not work. Finally, it is concluded that dissimilarity metrics could be used to detect changes between time periods, and additionally, some suitable dissimilarity metrics are presented.</p>

Note - replaced "r" with "𝑟"
----------------------------------------------------------------------
In diva2:1800497 
abstract is: 
<p>In today's society sustainability has become an important subject and has an impact on various sectors. Corporations include sustainability in their corporate strategy, which further affects the field of corporate finance. This has lead to a new insight among investors to include a sustainability criterion in their investment processes. This research has investigated how Investor AB could optimize their portfolio by including sustainability criterion (ESG) and how different portfolio setups will differ from each other. </p><p>The research has been conducted utilizing Markowitz portfolio optimization described by Markowitz theory. The application of the theory has been extended with a third criterion of a weighted ESG score rating where the optimal solutions were found using the notion of Pareto optimality and quadratic programming. Different cases have been created to find how more sustainable portfolios can differ from each other.</p><p>The research shows that portfolios consisting of companies with higher ESG rating do not significantly decrease the expected return but can suffer from higher standard deviation, which indicates that it is driven by assets with higher ESG score rating.</p><p>The obtained results show that the portfolios obtained including the third criterion will not always obtain a value of Jensen's Alpha above zero (0) and are therefore not optimal strategies to outperform the benchmark index, SIX Return Index. A portfolio that consists of non-sustainable and sustainable assets has performed better than other portfolios that under- or overperform from the perspective of sustainability. </p><p>The conclusion is that an investor must sacrifice a higher weighted ESG score rating of its portfolio to obtain a higher expected return and less risk. An investor that aims for higher return, must exclude the sustainability criterion.</p>

corrected abstract:
<p>In today's society sustainability has become an important subject and has an impact on various sectors. Corporations include sustainability in their corporate strategy, which further affects the field of corporate finance. This has lead to a new insight among investors to include a sustainability criterion in their investment processes. This research has investigated how Investor AB could optimize their portfolio by including sustainability criterion (ESG) and how different portfolio setups will differ from each other.</p><p>The research has been conducted utilizing Markowitz portfolio optimization described by Markowitz theory. The application of the theory has been extended with a third criterion of a weighted ESG score rating where the optimal solutions were found using the notion of Pareto optimality and quadratic programming. Different cases have been created to find how more sustainable portfolios can differ from each other.</p><p>The research shows that portfolios consisting of companies with higher ESG rating do not significantly decrease the expected return but can suffer from higher standard deviation, which indicates that it is driven by assets with higher ESG score rating. The obtained results show that the portfolios obtained including the third criterion will not always obtain a value of Jensen's Alpha above zero (0) and are therefore not optimal strategies to outperform the benchmark index, SIX Return Index. A portfolio that consists of non-sustainable and sustainable assets has performed better than other portfolios that under- or overperform from the perspective of sustainability.</p><p>The conclusion is that an investor must sacrifice a higher weighted ESG score rating of its portfolio to obtain a higher expected return and less risk. An investor that aims for higher return, must exclude the sustainability criterion.</p>

Note eliminate an unnecessary space at the end of paragraphs, added missing paragraph break, and remove unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1334823   - correct as is
----------------------------------------------------------------------
In diva2:1568234   - correct as is
----------------------------------------------------------------------
In diva2:641449 
abstract is: 
<p>Several alternative ways of producing energy came up as the world took conscience of the finite availability of fossil fuels and the environmental consequences of its use and processing. Wave and tidal energy are among the so called green energies. Wave energy converters have been under research for the past two decades and yet there hasn’t been one technology that gathered everyone’s acceptance as being the most suitable one. The present work is focused on a self-rectifying turbine for wave energy harnessing. It’s a self-rectifying biplane Wells with an intermediate stator. The main goal is to evaluate the performance of such a turbine. Two different analyses were performed: experimental and computational. The experimental tests were made so that efficiency, velocity profiles and loss coefficients could be calculated. To do so scaled-down prototypes were built from scratch and tested experimentally. The 3D numerical analysis was possible by using a CFD commercial code: Fluent 6.3. Several simulations were performed for different flow coefficients. Three different degrees of mesh refinement were applied and k-ε turbulence model was the one chosen to simulate the viscous behavior of the flow through the turbine. A steady-state analysis is due and two mixing planes were used at the interfaces between the rotors and the stator. In the end comparisons are made between the experimental and numerical results</p>

corrected abstract:
<p>Several alternative ways of producing energy came up as the world took conscience of the finite availability of fossil fuels and the environmental consequences of its use and processing. Wave and tidal energy are among the so called green energies. Wave energy converters have been under research for the past two decades and yet there hasn’t been one technology that gathered everyone’s acceptance as being the most suitable one.</p><p>The present work is focused on a self-rectifying turbine for wave energy harnessing. It’s a self-rectifying biplane Wells with an intermediate stator. The main goal is to evaluate the performance of such a turbine. Two different analyses were performed: experimental and computational. The experimental tests were made so that efficiency, velocity profiles and loss coefficients could be calculated. To do so scaled-down prototypes were built from scratch and tested experimentally. The 3D numerical analysis was possible by using a CFD commercial code: Fluent 6.3. Several simulations were performed for different flow coefficients. Three different degrees of mesh refinement were applied and 𝑘-ε turbulence model was the one chosen to simulate the viscous behavior of the flow through the turbine. A steady-state analysis is due and two mixing planes were used at the interfaces between the rotors and the stator.</p><p>In the end comparisons are made between the experimental and numerical results</p>

Note Note only change was to eliminate an unnecessary space at the end of a paragraph
Note also that the final sentence is lacking terminal punctuation in the original.
Also replaced "k" by "𝑘".
----------------------------------------------------------------------
In diva2:867125 - - unnessary period at end of title:
"Experimental and Numerical Analysis of the Shear Ring."
==>
"Experimental and Numerical Analysis of the Shear Ring"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1356870   - correct as is
----------------------------------------------------------------------
In diva2:1145357   - correct as is
----------------------------------------------------------------------
In diva2:1595588   - correct as is
----------------------------------------------------------------------
In diva2:1437661 
abstract is: 
<p>The precise interpretation of quantum mechanics is still open to discussion. A number of non-intercompatible interpretations, with a not unsignificant respective following, are at the time of writing still viable. When Eugene Wigner stated his titular thought experiment he managed to highlight how varying assumptions and accompanying mechanisms can have implications even on a macro-scale. Caslav Brukner expands the thought experiment of Wigner's Friend in his recent paper and derives a theorem using the setup. In our paper, we empirically validate the theorem derived in Brukner's paper, employing a setup based on his extended Wigner's Friend experiment. More precisely, we realise this setup on a 5-qubit Quantum Computer and achieve a violation of a Bell-type inequality, close to what is theoretically possible. This seems to imply that local observer-independent facts are not possible within the current framework of quantum mechanics.</p>

corrected abstract:
<p>The precise interpretation of quantum mechanics is still open to discussion. A number of non-intercompatible interpretations, with a not unsignificant respective following, are at the time of writing still viable. When Eugene Wigner stated his titular thought experiment he managed to highlight how varying assumptions and accompanying mechanisms can have implications even on a macro-scale. Časlav Brukner expands the thought experiment of Wigner's Friend in his recent paper and derives a theorem using the setup. In our paper, we empirically validate the theorem derived in Brukner's paper, employing a setup based on his extended Wigner's Friend experiment. More precisely, we realise this setup on a 5-qubit Quantum Computer and achieve a violation of a Bell-type inequality, close to what is theoretically possible. This seems to imply that local observer-independent facts are not possible within the current framework of quantum mechanics.</p>

Note "C" replaced by "Č"
----------------------------------------------------------------------
In diva2:558442 
abstract is: 
<p>In this report different measurement methods were evaluated for determination of acoustic parameters of porous absorbents by request of Scania CV AB. The purpose of these measurements is that the data could be used in a computer program to calculate acoustic behavior using different configurations and kinds of absorbents in the driver’s cabin of a truck.</p><p>The method used for these measurements is called the four microphone- and transfer matrix-method. This method requires a test tube with four microphone positions, a sound source capable of emitting broad band noise and an anechoic termination. With this test rig the transfer functions between a reference signal and the four microphones can be measured and then evaluated in a Matlab</p><p>®program to determine the different parameters.</p><p>The data from these measurements produced results with large errors for almost all the different kinds of absorbents. This is due to the fact that almost all of the test objects either did not match the assumptions made in the model or had flawed geometry. If more suitable test objects were used, better results could be achieved.</p>

corrected abstract:
<p>In this report different measurement methods were evaluated for determination of acoustic parameters of porous absorbents by request of Scania CV AB. The purpose of these measurements is that the data could be used in a computer program to calculate acoustic behavior using different configurations and kinds of absorbents in the driver’s cabin of a truck.</p><p>The method used for these measurements is called the four microphone- and transfer matrix-method. This method requires a test tube with four microphone positions, a sound source capable of emitting broad band noise and an anechoic termination. With this test rig the transfer functions between a reference signal and the four microphones can be measured and then evaluated in a Matlab® program to determine the different parameters.</p><p>The data from these measurements produced results with large errors for almost all the different kinds of absorbents. This is due to the fact that almost all of the test objects either did not match the assumptions made in the model or had flawed geometry. If more suitable test objects were used, better results could be achieved.</p>

Note removed unnecessary paragraph
----------------------------------------------------------------------
In diva2:1335357 - extra spaces in Swedish title:
"Experimentell prövning av Mermin- Peres Magiska Kvadrat på IBMs 5- kvantbitsdator"
==>
"Experimentell prövning av MerminPeres Magiska Kvadrat pa IBMs 5-kvantbitsdator"

The report is written in English hence the title and alternative title should be reversed.

abstract is: 
<p>The validity of the theory of quantum mechanics has been a major topic of discussion for the past century. Einstein, Podolsky, and Rosen famously attempted to show the incompleteness of quantum mechanics, their main criticism hinging on the inherent nonlocality of quantum mechanical phenomena. So-called Hidden-variable theories were proposed as a more fundamental description of reality. The Bell and Kochen-Specker theorems, developed later on, placed limitations on the nature of such Hidden-variable theories. The Kochen-Specker theorem states that no non-contextual Hidden-variable theory is able to reproduce the predictions of quantum mechanics. In this paper an experimental proof of the Kochen-Specker theorem in Hilbert space of dimension four -- known as the Mermin-Peres Magic Square -- is implemented on IBM's publicly available five qubit quantum computer. It is shown that the resulting data cannot be explained by a model assuming measurement results can be predetermined, i.e. a model assuming realism.</p>

corrected abstract:
<p>The validity of the theory of quantum mechanics has been a major topic of discussion for the past century. Einstein, Podolsky, and Rosen famously attempted to show the incompleteness of quantum mechanics, their main criticism hinging on the inherent <em>nonlocality</em> of quantum mechanical phenomena. So-called <em>Hidden-variable theories</em> were proposed as a more fundamental description of reality. The Bell and Kochen-Specker theorems, developed later on, placed limitations on the nature of such Hidden-variable theories. The Kochen-Specker theorem states that no <em>non-contextual</em> Hidden-variable theory is able to reproduce the predictions of quantum mechanics. In this paper an experimental proof of the Kochen-Specker theorem in Hilbert space of dimension four &mdash; known as the <em>Mermin-Peres Magic Square</em> &mdash; is implemented on IBM's publicly available five qubit quantum computer. It is shown that the resulting data cannot be explained by a model assuming measurement results can be predetermined, i.e. a model assuming realism.</p>

Note replaced the -- with &mdash;
Also added the italics
----------------------------------------------------------------------
In diva2:1823775   - correct as is
----------------------------------------------------------------------
In diva2:1341307 
abstract is: 
<p>In molecular dynamics, mathematical models of metallic systems should in general have the temperature of the system as a dependent variable. In particular, the potential energy term of the Hamiltonian function of the interaction model should be dependent on temperature in addition to interparticular distances. This puts the Hamiltonian function on a form which is generally non-separable. Conventional explicit numerical methods which are symplectic when used to integrate the equations of motion of systems with separable Hamiltonians are not in general symplectic when used to integrate the equations of motion of systems with a non-separable Hamiltonian. Hence, an integrator which sustains symplecticity when used in a system with non-separable Hamiltonian is sought. A family of explicit integrators which are symplectic when integrating systems with a non-separable Hamiltonian are shown to exhibit similar or superior performance to the Velocity Verlet and fourth-order Runge-Kutta schemes, albeit with the drawback of numerical instability when used on a system where forces depend exponentially on the inverted interparticular distances. To the knowledge of the authors, this study is the first time this family of integrators is applied in the context of molecular dynamics. The results of this study provide a first indication that a comprehensive solution to the problem of integrating the equations of motion of a system with a non-separable Hamiltonian explicitly and symplectically is not provided by the considered family of integrators. However, further investigations into using this family of integrators in other molecular dynamics systems than those investigated here are needed to provide a more definitive conclusion.</p>

corrected abstract:
<p>In molecular dynamics, mathematical models of metallic systems should in general have the temperature of the system as a dependent variable [1]. In particular, the potential energy term of the Hamiltonian function of the interaction model should be dependent on temperature in addition to interparticular distances. This puts the Hamiltonian function on a form which is generally non-separable. Conventional explicit numerical methods which are symplectic when used to integrate the equations of motion of systems with separable Hamiltonians are not in general symplectic when used to integrate the equations of motion of systems with a non-separable Hamiltonian. Hence, an integrator which sustains symplecticity when used in a system with non-separable Hamiltonian is sought. A family of explicit integrators which are symplectic when integrating systems with a non-separable Hamiltonian are shown to exhibit similar or superior performance to the Velocity Verlet and fourth-order Runge-Kutta schemes, albeit with the drawback of numerical instability when used on a system where forces depend exponentially on the inverted interparticular distances. To the knowledge of the authors, this study is the first time this family of integrators is applied in the context of molecular dynamics. The results of this study provide a first indication that a comprehensive solution to the problem of integrating the equations of motion of a system with a non-separable Hamiltonian explicitly and symplectically is not provided by the considered family of integrators. However, further investigations into using this family of integrators in other molecular dynamics systems than those investigated here are needed to provide a more definitive conclusion.</p>

Note only change was to add the missing citation
----------------------------------------------------------------------
In diva2:854670   - correct as is
----------------------------------------------------------------------
In diva2:1334775   - correct as is
----------------------------------------------------------------------
In diva2:1438887 
abstract is: 
<p>Gamma-ray bursts (GRB) are extremely high energy pulses that are connected to supernovae and black hole formation. They are divided into two forms, long and short GRBs. The emission of a GRB are split into two phases, the prompt emission primarily observed in gamma-rays and the afterglow which is synchrotron radiation sent out when the relativistic jet collides with the circumstellar medium. It is known that the X-ray emission is absorbed, however we can not say how this absorption is distributed between the host galaxy and the Intergalactic medium (IGM). In this paper we will use X-ray spectral analysis of long GRBs at different redshifts using X-ray data from the Swift telescope to investigate the time evolution of the absorption. Constant evolution points to the majority of the absorption taking place in the IGM and for decreasing, close to the GRB. Spectra will be fitted with an absorbed power-law using chi-square statistics. The results will show one clear case of constant behaviour and two GRBs that seem constant but with bad statistics. One GRB shows a clear decreasing behaviour. Two GRBs show nonphysical behaviour which indicates that the absorbed power-law might not be the correct model for these GRBs. Given the uncertainties in the analysis we may conclude that the majority of the GRBs we analysed exhibit a constant behaviour which would indicate a dominant portion of the absorption occurring within the IGM.</p>

corrected abstract:
<p>Gamma-ray bursts (GRB) are extremely high energy pulses that are connected to supernovae and black hole formation. They are divided into two forms, long and short GRBs. The emission of a GRB are split into two phases, the prompt emission primarily observed in gamma-rays and the afterglow which is synchrotron radiation sent out when the relativistic jet collides with the circumstellar medium. It is known that the X-ray emission is absorbed, however we can not say how this absorption is distributed between the host galaxy and the Intergalactic medium (IGM). In this paper we will use X-ray spectral analysis of long GRBs at different redshifts using X-ray data from the Swift telescope to investigate the time evolution of the absorption. Constant evolution points to the majority of the absorption taking place in the IGM and for decreasing, close to the GRB. Spectra will be fitted with an absorbed power-law using χ<sup>2</sup> statistics. The results will show one clear case of constant behaviour and two GRBs that seem constant but with bad statistics. One GRB shows a clear decreasing behaviour. Two GRBs show nonphysical behaviour which indicates that the absorbed power-law might not be the correct model for these GRBs. Given the uncertainties in the analysis we may conclude that the majority of the GRBs we analysed exhibit a constant behaviour which would indicate a dominant portion of the absorption occurring within the IGM.</p>

Note replaced "chi-squared" with "χ<sup>2</sup>" as in the original
----------------------------------------------------------------------
In diva2:1781513   - correct as is
----------------------------------------------------------------------
In diva2:1894688   - correct as is
----------------------------------------------------------------------
In diva2:1894670   - correct as is
----------------------------------------------------------------------
In diva2:1220074   - correct as is
----------------------------------------------------------------------
In diva2:852161   - correct as is
----------------------------------------------------------------------
In diva2:1833668   - correct as is
----------------------------------------------------------------------
In diva2:1214294 
abstract is: 
<p>Using logistic regression, we aim to construct a model to examine the factors that are most influential in affecting user behavior on the flight comparison site flygresor.se. The factors examined were number of adults, number of children, number of stops on the inbound trip, number of stops on the outbound trip, number of days between the search date and the departure date and number of search results displayed for the user. The data sample, collected during a one-week period, was taken from Flygresor and consisted of trips to or from Sweden, made within Europe, excluding Nordic countries, and made more than six days before departure. To find the variables which best explain the user behavior, variable selection methods were used along with hypothesis testing. Also, multicollinearity analysis and residual analysis were performed to evaluate the final model. The result showed that the factor number of children had no significant impact on the conversion rate, while the remaining factors had a high impact. The final model has a high predictive ability on the user's propensity to select a certain flight.</p>

corrected abstract:
<p>Using logistic regression, we aim to construct a model to examine the factors that are most influential in affecting user behavior on the flight comparison site flygresor.se. The factors examined were <em>number of adults</em>, <em>number of children</em>, <em>number of stops on the inbound trip</em>, <em>number of stops on the outbound trip</em>, <em>number of days between the search date and the departure date</em> and <em>number of search results displayed for the user</em>. The data sample, collected during a one-week period, was taken from Flygresor and consisted of trips to or from Sweden, made within Europe, excluding Nordic countries, and made more than six days before departure. To find the variables which best explain the user behavior, variable selection methods were used along with hypothesis testing. Also, multicollinearity analysis and residual analysis were performed to evaluate the final model. The result showed that the factor <em>number of children</em> had no significant impact on the conversion rate, while the remaining factors had a high impact. The final model has a high predictive ability on the user's propensity to select a certain flight.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1450297   - correct as is
----------------------------------------------------------------------
In diva2:942686   - correct as is
----------------------------------------------------------------------
In diva2:1114102 
abstract is: 
<p>Digitization has changed the way people access the internet. Smartphones is soon to be the preferred internet access device leading us into a new generation of e-commerce, namely mobile commerce or m-commerce. The on-going transition, from desktop to smartphone has led to an uprising problem for companies within the area of e-commerce. Visitors coming from a smartphone device tend to not go through with the purchase. With this transition in mind, the thesis aimed to identify the factors that affect the proportion of smartphone visitors on a website, more specifically at the flight comparison site Flygresor.se. The method used was multiple linear regression analysis. To see whether the chosen factors affected the proportion of smartphone transactions or just the proportion of smartphone sessions two regression were performed. One with response variable Sessions and one with response variable Transactions, where Sessions refer to the number of visitors on the website and Transactions refer to the number of visitors moving on to the final booking website. The explanatory variables used were divided into four categories; Marketing, Channels, Season and Other, where the category Other contained the variables Total number of visitors and Amount of MB used per smartphone subscription. The study showed that all categories contained variables with significant impact on both of the response variables. There was only one variable that had different impact on the models, namely the Total number of visitors. The result indicates that smartphone users tend to, in comparison with desktop users, to a less extent continue to the final booking website. Since there were no other variables that only had an impact on Transactions it was assumed that there exist other factors which have a greater impact on smartphone users tendency to finalize a booking.</p>

corrected abstract:
<p>Digitization has changed the way people access the internet. Smartphones is soon to be the preferred internet access device leading us into a new generation of e-commerce, namely mobile commerce or m-commerce. The on-going transition, from desktop to smartphone has led to an uprising problem for companies within the area of e-commerce. Visitors coming from a smartphone device tend to not go through with the purchase. With this transition in mind, the thesis aimed to identify the factors that affect the proportion of smartphone visitors on a website, more specifically at the flight comparison site Flygresor.se. The method used was multiple linear regression analysis. To see whether the chosen factors affected the proportion of smartphone transactions or just the proportion of smartphone sessions two regression were performed. One with response variable <em>Sessions</em> and one with response variable <em>Transactions</em>, where <em>Sessions</em> refer to the number of visitors on the website and <em>Transactions</em> refer to the number of visitors moving on to the final booking website. The explanatory variables used were divided into four categories; <em>Marketing</em>, <em>Channels</em>, <em>Season</em> and <em>Other</em>, where the category <em>Other</em> contained the variables <em>Total number of visitors</em> and <em>Amount of MB used per smartphone subscription</em>. The study showed that all categories contained variables with significant impact on both of the response variables. There was only one variable that had different impact on the models, namely the <em>Total number of visitors</em>. The result indicates that smartphone users tend to, in comparison with desktop users, to a less extent continue to the final booking website. Since there were no other variables that only had an impact on <em>Transactions</em> it was assumed that there exist other factors which have a greater impact on smartphone users tendency to finalize a booking.</p>


Note - added italics
----------------------------------------------------------------------
In diva2:1214615   - correct as is
----------------------------------------------------------------------
In diva2:1757018 
abstract is: 
<p>A popular type of investments are financial investments. Even though the Swedish society aspires for equality, there are still financial differences between the sexes. This thesis project focuses on what factors that affect the amount that women and men invest in stocks. The aim is to obtain a deeper knowledge about the investment market and how it appears for women and men. The reason for this is to raise awareness about the inequality issue when it comes to the low representation of females in the investment market. </p><p>The objective for this project is to produce two models where one of them is for women and the other one for men. Those models have equal regressors: inflation, GDP growth, OMX Stockholm Price Index, average income per year and eight different age groups. The response variable is the average portfolio value for a specific age group at respective model. The data is taken mainly from Swedish Statistical Central Bureau, but is also gathered from NASDAQ, as well as the World Bank. The data is collected between the years 2000 to 2020. </p><p>The models are firstly evaluated at their full model, meaning that all regressors are included. The women's model shows a considerably good fitting, since almost all regressors are significant with low p-values along with a Multiple R-squared at 0.773 and Adjusted R-squared at 0.757. However, improvements can be made since outliers need to be removed, and the regressor income hold multicollinearity. Men's full model has a poor performance with fewer significant regressors and lower Multiple and Adjusted R-squared. Both models are then transformed, with applied square root of inflation at both models, and the square of income at men's model. </p><p>To determine the reduced model, variable selection is implemented. With Best Subset Selection, women's model includes OMX, GDP, income, age group 1, age group 6, age group 7 and age group 8. On the other hand, men's model has the regressors GDP, income, age group 1, age group 2, age group 3, age group 4, age group 5 and age group 7. Both these models perform much better where all the regressors had significant p-values and satisfactory Model Assumptions plots. In addition, no multicollinearity exists in the models. In conclusion, both the reduced models are chosen for final models. </p><p>Proposed further research within this topic is to include more appropriate regressors that may affect the portfolio value as well as to compare with other countries and include real investment instead of just financial investments.</p>

corrected abstract:
<p>A popular type of investments are financial investments. Even though the Swedish society aspires for equality, there are still financial differences between the sexes. This thesis project focuses on what factors that affect the amount that women and men invest in stocks. The aim is to obtain a deeper knowledge about the investment market and how it appears for women and men. The reason for this is to raise awareness about the inequality issue when it comes to the low representation of females in the investment market.</p><p>The objective for this project is to produce two models where one of them is for women and the other one for men. Those models have equal regressors: inflation, GDP growth, OMX Stockholm Price Index, average income per year and eight different age groups. The response variable is the average portfolio value for a specific age group at respective model. The data is taken mainly from Swedish Statistical Central Bureau, but is also gathered from NASDAQ, as well as the World Bank. The data is collected between the years 2000 to 2020.</p><p>The models are firstly evaluated at their full model, meaning that all regressors are included. The women's model shows a considerably good fitting, since almost all regressors are significant with low p-values along with a Multiple 𝑅<sup>2</sup> at 0.773 and Adjusted 𝑅<sup>2</sup> at 0.757. However, improvements can be made since outliers need to be removed, and the regressor income hold multicollinearity. Men's full model has a poor performance with fewer significant regressors and lower Multiple and Adjusted 𝑅<sup></sup>. Both models are then transformed, with applied square root of inflation at both models, and the square of income at men's model.</p><p>To determine the reduced model, variable selection is implemented. With Best Subset Selection, women's model includes OMX, GDP, income, age group 1, age group 6, age group 7 and age group 8. On the other hand, men's model has the regressors GDP, income, age group 1, age group 2, age group 3, age group 4, age group 5 and age group 7. Both these models perform much better where all the regressors had significant p-values and satisfactory Model Assumptions plots. In addition, no multicollinearity exists in the models. In conclusion, both the reduced models are chosen for final models.</p><p>Proposed further research within this topic is to include more appropriate regressors that may affect the portfolio value as well as to compare with other countries and include real investment instead of just financial investments.</p>

Note eliminated unnecessary space at the end of paragraphs, replacws R-square with "𝑅<sup></sup>"
----------------------------------------------------------------------
In diva2:1334384 - unnecessary repeatition in title:
"Factors that influence condominium pricing in Stockholm: A regression analysis: A regression analysis"
==>
"Factors that influence condominium pricing in Stockholm: A regression analysis"

Note that the abstract was scanned - hence there is no text to select.

abstract is: 
<p>This thesis aims to examine which factors that are of significance when forecasting the selling price of condominiums in Stockholm city. Through the use of multiple linear regression, response variable transformation, and a multitude of methods for refining the model fit, a conclusive, out of sample validated model with a confidence level of 95% was obtained. To conduct the statistical methods, the software R was used.</p><p>This study is limited to the districts of inner city Stockholm with the postal codes 112-118, and the final model can only be applied to this area as the postal codes are included as regressors in the model. The time period in which the selling price was analyzed varied between January 2014 and April 2019, in which the volatility of the time value of money has not been taken into account for the time period. The final model included the following variables as the ones having an impact on the selling price: floor, living area, monthly fee, construction year, district of the city.</p>

corrected abstract:
<p>This thesis aims to examine which factors that are of significance when forecasting the selling price of condominiums in Stockholm city. Through the use of multiple linear regression, response variable transformation, and a multitude of methods for refining the model fit, a conclusive, out of sample validated model with a confidence level of 95% was obtained. To conduct the statistical methods, the software R was used.</p><p>This study is limited to the districts of inner city Stockholm with the postal codes 112-118, and the final model can only be applied to this area as the postal codes are included as regressors in the model. The time period in which the selling price was analyzed varied between January 2014 and April 2019, in which the volatility of the time value of money has not been taken into account for the time period. The final model included the following variables as the ones having an impact on the selling price: <em>floor</em>, <em>living area</em>, <em>monthly fee</em>, <em>construction year</em>, <em>district of the city</em>.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1652650 
abstract is: 
<p>This report investigates how geographic position, physical attributes of the property and form of ownership affect the price of houses in Stockholm County. The research goal is to develop a model, based on the factors with the strongest impact on house prices, which can be used to estimate the value of a property. The investigation is based on data of sold houses in Stockholm County in 2020. The dataset was obtained from Booli. After relevant adjustments of the dataset have been made, in order to satisfy the assumptions of multiple linear regression, multiple linear regression was applied. Thereafter, forward selection was used to determine which factors to include in the final model. The results indicate that living area is the factor with the highest impact on property prices. Furthermore, other important factors are distance to water, construction year and which municipality the property is situated in. Finally, the importance of the investigation and the model for larger property owners is discussed.</p>

corrected abstract:
<p>This report investigates how geographic position, physical attributes of the property and form of ownership affect the price of houses in Stockholm County. The research goal is to develop a model, based on the factors with the strongest impact on house prices, which can be used to estimate the value of a property. The investigation is based on data of sold houses in Stockholm County in 2020. The dataset was obtained from Booli. After relevant adjustments of the dataset had been made (in order to satisfy the assumptions of multiple linear regression), multiple linear regression was applied. Thereafter, forward selection was used to determine which factors to include in the final model. The results indicate that living area is the factor with the highest impact on property prices. Furthermore, other important factors are distance to water, construction year and which municipality the property is situated in. Finally, the importance of the investigation and the model for larger property owners is discussed.</p>

Note fixed wording and parenthetical to be the same as the original
----------------------------------------------------------------------
In diva2:1230037 
abstract is: 
<p>This thesis concerns the implementation of an pre-analysis tool in orienteering for the problem of finding the optimal route choice on a leg in orienteering. The problem is modelled by the framework of level sets and an anisotropic travel time equation is proposed. The analysis tool designed in this thesis is based on solving an anisotropic travel time equation, based on a digital elevation model and symbols imported from an orienteering map, using the fast sweeping method.</p>

corrected abstract:
<p>This thesis concerns the implementation of a pre-analysis tool in orienteering for the problem of finding the optimal route choice on a leg in orienteering. The problem is modelled by the theory of level sets methods, and an anisotropic travel time equation is proposed. The analysis tool designed in this thesis is based on solving an anisotropic travel time equation, based on a digital elevation model and symbols imported from an orienteering map, using the fast sweeping method.</p>

Note changed wording to match the original
----------------------------------------------------------------------
In diva2:1018712 
abstract is: 
<p>In this master thesis, methods for fatigue analysis of front engine brackets subjected to road induced gravity loads (g-loads) are studied. The objective of the thesis is to investigate the possibility to improve simulation and test analysis for the components. The powertrain is modeled with varying degrees of complexity and the different models are compared to each other and to Scania's models for analysis of the engine suspension. The analysis begins with g-loads and proceeds with time-dependent loads. It is investigated how simulated strains in the cylinder block correspond to measured strains from the test track at Scania. Finally, it is investigated how component tests corresponds to actual loads by comparing the results.</p><p>The results from the first part of the thesis indicate that worst load case is loading in the negative z -direction and the model of the powertrain with isolators modelled as spring elements is the best for g-loads lower than -3g and the model is sufficient for loads lower than -8g. The results from the second part of the thesis indicate that the simulated strains generally correspond to the measured strains, but with a slight difference in strain amplitude.</p>

corrected abstract:
<p>In this master thesis, methods for fatigue analysis of front engine brackets subjected to road induced gravity loads (𝑔-loads) are studied. The objective of the thesis is to investigate the possibility to improve simulation and test analysis for the components. The powertrain is modeled with varying degrees of complexity and the different models are compared to each other and to Scania's models for analysis of the engine suspension. The analysis begins with 𝑔-loads and proceeds with time-dependent loads. It is investigated how simulated strains in the cylinder block correspond to measured strains from the test track at Scania. Finally, it is investigated how component tests corresponds to actual loads by comparing the results.</p><p>The results from the first part of the thesis indicate that worst load case is loading in the negative z -direction and the model of the powertrain with isolators modelled as spring elements is the best for 𝑔-loads lower than -3g and the model is sufficient for loads lower than -8g. The results from the second part of the thesis indicate that the simulated strains generally correspond to the measured strains, but with a slight difference in strain amplitude.</p>

Note replaced "g" with "𝑔"
----------------------------------------------------------------------
In diva2:872206 - unnecessary after hyphe space in title:
"Fatigue analysis of two wheel‐ mounted brake disc designs"
==>
"Fatigue analysis of two wheel‐mounted brake disc designs"

abstract is: 
<p>Due to a need of more compact bogies, the brake discs can be mounted on the railway wheels, bolted through the wheel web. Thus, the wheels are drilled and have multiple areas of contact with the brake discs. To establish maintenance procedures that will be applied to the  wheels,  SNCF  used  the  feedback  from  experience  (as  with  the  train  AGC)  which  gives  perfect  performance  in  terms  of  safety.  However,  to  optimize  the  maintenance  process, numerical  simulations  may  be  preferred  since  they  are  less  conservative.  This  report  describes  the  numerical  simulations,  based  on  the  finite  element  method,  that  were conducted to determine if the Régiolis wheel complies with the standard EN 13979-­‐1 from a mechanical  fatigue  point  of  view.  In  addition,  it  provides  additional  insights  regarding  the  loads and damage suffered by the wheel, which are not taken into account in the standard: the  damage  induced  by  disc  braking  and  the  fretting  that  may  occur  at  the  contact  interfaces. This study has been used as a decision support for the first inspection intervals of the Régiolis wheels.</p>

corrected abstract:
<p>Due to a need of more compact bogies, the brake discs can be mounted on the railway wheels, bolted through the wheel web. Thus, the wheels are drilled and have multiple areas of contact with the brake discs. To establish maintenance procedures that will be applied to the wheels, SNCF used the feedback from experience (as with the train AGC) which gives perfect performance in terms of safety. However, to optimize the maintenance process, numerical simulations may be preferred since they are less conservative. This report describes the numerical simulations, based on the finite element method, that were conducted to determine if the Régiolis wheel complies with the standard EN 13979-1 from a mechanical fatigue point of view. In addition, it provides additional insights regarding the loads and damage suffered by the wheel, which are not taken into account in the standard: the damage induced by disc braking and the fretting that may occur at the contact interfaces. This study has been used as a decision support for the first inspection intervals of the Régiolis wheels.</p>


Note replace "-­‐" with "-", also removed the double spaces
----------------------------------------------------------------------
In diva2:1900921   - correct as is
----------------------------------------------------------------------
In diva2:784006   - correct as is
----------------------------------------------------------------------
In diva2:1816733 
abstract is: 
<p>Additive manufacturing methods has been prevailing for several decades and the recent technological advancements brings in the flexibility and consideration for large-scale production in the industries. The components manufactured with these methods have wide variety of applications and therefore, it is crucial to investigate the mechanical performance of the printed parts. There have been many researches done to investigate the mechanical behaviour of polymer material but the studies are limited when it comes to the fatigue performance of the polymer parts printed using multi-jet fusion technology.</p><p>The aim of the master thesis is to investigate the fatigue behaviour of polyamide12 (PA12) material with the components manufactured using HP’s multi-jet fusion 3D printing machine. Fatigue life is influenced by several factors such as the loading condition, the topology of the specimen, material properties, print quality and the environmental conditions. It is therefore essential to consider all these factors when developing the experiments for fatigue life prediction.</p><p>The master thesis work can be divided into three sections. The first section focuses on evaluating the mechanical properties of polyamide12. This includes the quasi-static test for determining the tensile properties of specimens with the geometrical influence, the difference in properties in relation to the print directions, the influence of humidity and porosity over the mechanical performance of the material and finally the effect of internal heat generation and the surrounding temperature. The results show that the temperature and the quality of the specimens are the two major factors affecting the mechanical and fatigue performance of PA12. That being said, the next section focuses on setting up the fatigue experiments based on the data obtained from the static tests. When carrying out the experiment, both the test frequency and the surrounding temperature were foundto have a greater impact over the fatigue results. High test frequency showed a dramatic increase in the temperature of the specimen which caused an early failure. Hence, the experiments were developed in such a way that the influence of the thermal fatigue can be ignored by controlling the temperature of the specimen through a compressed air cooling system. The final section presents the findings, the conclusions about the material behaviour and the development of a finite element model to predict the fatigue life of a topology optimised demonstrator part using the data gathered from the experiments.</p>

corrected abstract:
<p>Additive manufacturing methods has been prevailing for several decades and the recent technological advancements brings in the flexibility and consideration for large-scale production in the industries. The components manufactured with these methods have wide variety of applications and therefore, it is crucial to investigate the mechanical performance of the printed parts. There have been many researches done to investigate the mechanical behaviour of polymer material but the studies are limited when it comes to the fatigue performance of the polymer parts printed using multi-jet fusion technology.</p><p>The aim of the master thesis is to investigate the fatigue behaviour of polyamide12 (PA12) material with the components manufactured using HP’s multi-jet fusion 3D printing machine. Fatigue life is influenced by several factors such as the loading condition, the topology of the specimen, material properties, print quality and the environmental conditions. It is therefore essential to consider all these factors when developing the experiments for fatigue life prediction.</p><p>The master thesis work can be divided into three sections. The first section focuses on evaluating the mechanical properties of polyamide12. This includes the quasi-static test for determining the tensile properties of specimens with the geometrical influence, the difference in properties in relation to the print directions, the influence of humidity and porosity over the mechanical performance of the material and finally the effect of internal heat generation and the surrounding temperature. The results show that the temperature and the quality of the specimens are the two major factors affecting the mechanical and fatigue performance of PA12. That being said, the next section focuses on setting up the fatigue experiments based on the data obtained from the static tests. When carrying out the experiment, both the test frequency and the surrounding temperature were found to have a greater impact over the fatigue results. High test frequency showed a dramatic increase in the temperature of the specimen which caused an early failure. Hence, the experiments were developed in such a way that the influence of the thermal fatigue can be ignored by controlling the temperature of the specimen through a compressed air cooling system. The final section presents the findings, the conclusions about the material behaviour and the development of a finite element model to predict the fatigue life of a topology optimised demonstrator part using the data gathered from the experiments.</p>

Note o missing space was added "foundto" ==> "found to"
----------------------------------------------------------------------
In diva2:1817091 
abstract is: 
<p>In this thesis work a literature survey is done to collect published data for T-welded joints with a thickness of 5-12 mm, produced in high strength steel and treated with HFMI. In addition to this, large structures are researched. The data was evaluated in nominal stress and effective notch stress and compared with the recommended FAT value from the International Institute of Welding (IIW) for post-weld improved joints. It was concluded that not sufficient data exist for T-joints in high strength steel, especially steels of grades with yield strength greater than 450 MPa, and thicknesses of 5-12mm, neither does enough data exist for large structures. The recommended fatigue strength values from the IIW were compared.</p>

corrected abstract:
<p>In this thesis work a literature survey is done to collect published data for T-welded joints with a thickness of 5-12 mm, produced in high strength steel and treated with HFMI. In addition to this, large structures are researched. The data was evaluated in nominal stress and effective notch stress and compared with the recommended FAT value from the International Institute of Welding (<em>IIW</em>) for post-weld improved joints. It was concluded that not sufficient data exist for T-joints in high strength steel, especially steels of grades with yield strength greater than 450 MPa, and thicknesses of 5-12mm, neither does enough data exist for large structures. The recommended fatigue strength values from the <em>IIW</em> were compared.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1757007   - correct as is
----------------------------------------------------------------------
In diva2:1466892 
abstract is: 
<p>Creasing is often performed on paperboard to lower the bending stiﬀness in a region, making the paperboard easier to fold. Packsize AB uses rolls to crease corrugated paperboard. When using rolls for creasing double-walled corrugated paperboard in the machine direction, fracture occasionally occur on the top liner. This master thesis investigates the initiation of these surface cracks for a BC type corrugated paperboard. Experiments and FE-analysis are performed to study the magnitude of inﬂuence for various creasing roll geometries, under various hydraulic base pressures and changes in the material properties. The report includes a discussion of which material parameters that are the most sensitive with respect to surface crack initi-ation. Comparisons between FE-analysis and experiments are made to validate the FE-modeling. The experimental results show that the creasing roll geometries signif-icantly aﬀects the percentage of failure. The best geometry lowered the percentage of failure from 36 % to 0 %. Furthermore, the most sensitive material parameters con-cerning the Tsai-Wu failure criterion used in the numerical simulations are Young’s modulus of the liners and the ultimate strength in the cross direction of the top liner. A variation of 5 % of these material properties yields a change of the maximum Tsai-Wu strength index with 1.5 % and 5 %, respectively. However, further investigations with focus on validation and comparison of FE-analysis and experiments are needed to ensure a reliable model where various types and sizes of corrugated paperboard can be analyzed. This would need measured material properties as a part of the experimental study. Also, the performance of the yielded creasing lines from the experiments are not included in this thesis as well as the criticality of the top surface cracks.</p>

corrected abstract:
<p>Creasing is often performed on paperboard to lower the bending stiffness in a region, making the paperboard easier to fold. Packsize AB uses rolls to crease corrugated paperboard. When using rolls for creasing double-walled corrugated paperboard in the machine direction, fracture occasionally occur on the top liner. This master thesis investigates the initiation of these surface cracks for a BC type corrugated paperboard. Experiments and FE-analysis are performed to study the magnitude of influence for various creasing roll geometries, under various hydraulic base pressures and changes in the material properties. The report includes a discussion of which material parameters that are the most sensitive with respect to surface crack initiation. Comparisons between FE-analysis and experiments are made to validate the FE-modeling. The experimental results show that the creasing roll geometries significantly affects the percentage of failure. The best geometry lowered the percentage of failure from 36 % to 0 %. Furthermore, the most sensitive material parameters concerning the Tsai-Wu failure criterion used in the numerical simulations are Young’s modulus of the liners and the ultimate strength in the cross direction of the top liner. A variation of 5 % of these material properties yields a change of the maximum Tsai-Wu strength index with 1.5 % and 5 %, respectively. However, further investigations with focus on validation and comparison of FE-analysis and experiments are needed to ensure a reliable model where various types and sizes of corrugated paperboard can be analyzed. This would need measured material properties as a part of the experimental study. Also, the performance of the yielded creasing lines from the experiments are not included in this thesis as well as the criticality of the top surface cracks.</p>


Note - removed unnecessary hyphens
----------------------------------------------------------------------
In diva2:1817048   - correct as is
----------------------------------------------------------------------
In diva2:1670577 
abstract is: 
<p>In this report I present a feasibility study of using open astronomical data to make Initial Orbit Determination (IOD) for Resident Space Objects (RSO) appearing as streaks in telescope images. The purpose is to contribute to Space Surveillance and Tracking (SST) for maintaining Space Situation Awareness (SSA). Data from different wide-field survey telescopes were considered but due to availability constraints only mask images from Zwicky Transient Facility (ZTF) survey were chosen for the analysis. An algorithm was developed to detect streaks in the mask images and match them to RSO known to be within the Field of View (FoV) at the observation time. Further, the IOD was made with angles-only Laplace’s method and the state vectors calculated for the streaks from the IOD were compared to those from the TLE for the matching RSO. The algorithm was tested with 6 different image fields acquired between the 14th to the 16th December 2019, of which 4 are characterised as non-crowded and 2 as crowded. The streak finding algorithm has a better precision and sensitivity for the non-crowded field, with an F1-score of 0.65, but is worse for the crowded fields with an F1-score of 0.035. In the non-crowded fields 95% of all streak and object matches are true matches to unique RSO, while for the crowded field only 10% are true matches. It was found that the 1''/pixel resolution in the images is too low for doing an IOD with Laplace’s method, despite how well the streak finding algorithm performs. However, with some improvements, the method is suitable as a cost effective way to verify known RSO in catalogues.</p>

corrected abstract:
<p>In this report I present a feasibility study of using open astronomical data to make Initial Orbit Determination (IOD) for Resident Space Objects (RSO) appearing as streaks in telescope images. The purpose is to contribute to Space Surveillance and Tracking (SST) for maintaining Space Situation Awareness (SSA). Data from different wide-field survey telescopes were considered but due to availability constraints only mask images from Zwicky Transient Facility (ZTF) survey were chosen for the analysis. An algorithm was developed to detect streaks in the mask images and match them to RSO known to be within the Field of View (FoV) at the observation time. Further, the IOD was made with angles-only Laplace’s method and the state vectors calculated for the streaks from the IOD were compared to those from the TLE for the matching RSO. The algorithm was tested with 6 different image fields acquired between the 14th to the 16th December 2019, of which 4 are characterised as non-crowded and 2 as crowded. The streak finding algorithm has a better precision and sensitivity for the non-crowded field, with an F1-score of 0.65, but is worse for the crowded fields with an F1-score of 0.035. In the non-crowded fields 95% of all streak and object matches are true matches to unique RSO, while for the crowded field only 10% are true matches. It was found that the 1″/pixel resolution in the images is too low for doing an IOD with Laplace’s method, despite how well the streak finding algorithm performs. However, with some improvements, the method is suitable as a cost effective way to verify known RSO in catalogues.</p>

Note fixed double prime
----------------------------------------------------------------------
In diva2:1679333 
abstract is: 
<p>In biology, it is common to study cultured cells (in vitro) with fluorescence time-lapse microscopy. The cells are recorded for longer period of time and can later be viewed at an accelerated speed. During the acquisition some live cells tend to migrate. This can be a problem if the cell’s migration speed is high enough to move outside the field of view (FOV) during the acquisition time. The cells that moves outside the FOV can no longer be recorded and the information about them will be lost. This thesis presents scripts that have been developed for ZEN (blue) to be able to track a specific migrating cell of interest in real-time with automated control of imaging parameters. The microscope stage position is modified on-the-fly to have the tracked cell in the center of the FOV for the whole experiment. Three different types of experiments to track migrating NK cells were performed with the scripts. The results show that the scripts were able to track one NK cell for more than 1 hour in both conventional wide-field and lattice light-sheet microscopy. The segmentation was inaccurate when one or more objects were in close proximity to the tracked cell. By applying a watershed algorithm the segmentation result can be improved in some cases.</p>

corrected abstract:
<p>In biology, it is common to study cultured cells (<em>in vitro</em>) with fluorescence time-lapse microscopy. The cells are recorded for longer period of time and can later be viewed at an accelerated speed. During the acquisition some live cells tend to migrate. This can be a problem if the cell’s migration speed is high enough to move outside the field of view (FOV) during the acquisition time. The cells that moves outside the FOV can no longer be recorded and the information about them will be lost. This thesis presents scripts that have been developed for ZEN (blue) to be able to track a specific migrating cell of interest in real-time with automated control of imaging parameters. The microscope stage position is modified on-the-fly to have the tracked cell in the center of the FOV for the whole experiment. Three different types of experiments to track migrating NK cells were performed with the scripts. The results show that the scripts were able to track one NK cell for more than 1 hour in both conventional wide-field and lattice light-sheet microscopy. The segmentation was inaccurate when one or more objects were in close proximity to the tracked cell. By applying a watershed algorithm the segmentation result can be improved in some cases.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1794954   - correct as is
----------------------------------------------------------------------
In diva2:1713305 
abstract is: 
<p>The work aims to provide a finite element model using Ansys Workbench for realistically simulating paperboard packages. The geometrical properties of a package and the loading conditions were altered to investigate their influence on the box behavior and the applicability of Ansys as the finite element solver for paperboard material. The package was based on the material and geometrical properties of a previous study by Marin et al. [1]. The box was loaded between two steel plates on which the forces were applied. The anisotropic behavior of paperboard was introduced by defining local coordinate systems for each surface that aligns with the MD-CD directions. Analysis of a box in compression, where the creases were represented by crease elements, was done and verified by experimental results. The simulations at compressive and tensile loadings show how the height of the box influences the failure force, as well as where stress concentrations arise. A sweep of box height in shear loading found a transition point in deformation mechanism for this particular box, where the force-displacement curve changed from convex to concave. Ansys was found to be an adequate tool for simulating paperboard materials and packages. Thus, simulations in torsional shear and other conditions that paperboard packages might have in practice is advisable</p><p>[1] Marin, G., Nygårds, M., Östlund, S., Srinivasa, P. (2021) "Experimental and finite element simulated box compression tests on paperboard packages at different moisture levels", Packaging Technology and Science 34(4), 229-243.</p>

corrected abstract:
<p>The work aims to provide a finite element model using Ansys Workbench for realistically simulating paperboard packages. The geometrical properties of a package and the loading conditions were altered to investigate their influence on the box behavior and the applicability of Ansys as the finite element solver for paperboard material. The package was based on the material and geometrical properties of a previous study by Marin et al. <a href="#fn1" id="ref1">[1]</a>. The box was loaded between two steel plates on which the forces were applied. The anisotropic behavior of paperboard was introduced by defining local coordinate systems for each surface that aligns with the MD-CD directions. Analysis of a box in compression, where the creases were represented by crease elements, was done and verified by experimental results. The simulations at compressive and tensile loadings show how the height of the box influences the failure force, as well as where stress concentrations arise. A sweep of box height in shear loading found a transition point in deformation mechanism for this particular box, where the force-displacement curve changed from convex to concave. Ansys was found to be an adequate tool for simulating paperboard materials and packages. Thus, simulations in torsional shear and other conditions that paperboard packages might have in practice is advisable</p>
<div id="footnotes">
    <p id="fn1" style="padding-left: 20px;">[1] Marin, G., Nygårds, M., Östlund, S., Srinivasa, P. (2021) "Experimental and finite element simulated box compression tests on paperboard packages at different moisture levels", Packaging Technology and Science 34(4), 229-243. <a href="#ref1" aria-label="Back to reference">↩</a></p>
</div>


Added the footnote formatting
----------------------------------------------------------------------
In diva2:1087210   - correct as is
----------------------------------------------------------------------
In diva2:1707748 
----------------------------------------------------------------------
In diva2:1652376 - full text does not have an abstract in either language, but yet they are in DiVA!
----------------------------------------------------------------------
In diva2:1215541   - correct as is
----------------------------------------------------------------------
In diva2:1225438   - correct as is
----------------------------------------------------------------------
In diva2:560435 
abstract is: 
<p>Some cases of infant skull fracture fall under the category of forensic study where it is not obvious whether the head trauma happened due to an accident or abuse. To be able to determine the cause of the head trauma with sufficient accuracy, biomechanical analysis using finite element modeling of the infant cranium has been established. By simulating the trauma, one may be able to obtain the fracture propagation of the skull and from it determine if the scenario narrative is plausible. Geometry of skull, sutures, scalp and brain of a 2 month old infant head was obtained using CT images and meshed using voxel hexahedral meshing. Simulation of an impact to the head from a fall of 0.82 m height, to a rigid floor, was carried out in the non-linear finite element program LS-Dyna. Two scenarios were simulated: an impact to the occipitalparietal bones and an impact to the right parietal bone. The fracture propagation was obtained using the Chang-Chang Composite Failure Model as a constitutive model for the skull bones. The amount of material parameters gathered in the present study to predict fracture of the infant skull has not been obtained before, to the best knowledge of author.</p><p>Validation of the models’ ability to show relatively correct fracture propagation was carried out by comparing the obtained fracture pattern from the parietal-occipital impact against published fracture patterns of infant PMHS skulls from a free fall onto a hard surface. The fracture pattern was found to be in good compliance with the published data. The fracture pattern in the parietal bone from the impact was compared against a fracture pattern from a previously constructed model at STH. The patterns of the models show some similarities but improvements to the model and further validations need to be carried out.</p>

corrected abstract:
<p>Some cases of infant skull fracture fall under the category of forensic study where it is not obvious whether the head trauma happened due to an accident or abuse. To be able to determine the cause of the head trauma with sufficient accuracy, biomechanical analysis using finite element modeling of the infant cranium has been established. By simulating the trauma, one may be able to obtain the fracture propagation of the skull and from it determine if the scenario narrative is plausible. Geometry of skull, sutures, scalp and brain of a 2 month old infant head was obtained using CT images and meshed using voxel hexahedral meshing. Simulation of an impact to the head from a fall of 0.82 m height, to a rigid floor, was carried out in the non-linear finite element program LS-Dyna. Two scenarios were simulated: an impact to the occipital-parietal bones and an impact to the right parietal bone. The fracture propagation was obtained using the Chang-Chang Composite Failure Model as a constitutive model for the skull bones. The amount of material parameters gathered in the present study to predict fracture of the infant skull has not been obtained before, to the best knowledge of author.</p><p>Validation of the models’ ability to show relatively correct fracture propagation was carried out by comparing the obtained fracture pattern from the parietal-occipital impact against published fracture patterns of infant PMHS skulls from a free fall onto a hard surface. The fracture pattern was found to be in good compliance with the published data. The fracture pattern in the parietal bone from the impact was compared against a fracture pattern from a previously constructed model at STH. The patterns of the models show some similarities but improvements to the model and further validations need to be carried out.</p>

Note spelling error:
w='occipitalparietal' val={'c': 'occipital-parietal', 's': 'diva2:560435', 'n': 'correct in original'}
----------------------------------------------------------------------
In diva2:1644863 
abstract is: 
<p>A finite element model of the CubeSat MIST was created, in order to assess the stresses that occur in the satellite in response to loads during its launch. Due to size limits of thesoftware used, simplifications had to be made to the geometry of the model. The loads assessed were quasi-static accelerations, random vibrations, shock loads, as well as a combined quasi-static acceleration and random vibration case. The study assumed the worst possible loads from a list of different potential launch vehicles for the satellite. Non-linear boundary conditions could not be modelled, and instead different linear boundary condition combinations were assessed. The results showed that the satellite showed positive margins of safety for the quasi-static loads. The lowest natural frequency for the satellite was above 130 Hz. For the random vibration loads, positive margins of safety could be shown if adverse stresses attributed to the boundary conditions inthe worst case were ignored. The model proved too conservative to qualify the satellite for the shock loads. Shock testing is therefore recommended for future work, unless requirements for waiving the shock testing can be met. The random vibration and combined loads analysis showed that the −X shear panel experienced high stresses in the corners of its windows, and the part should be inspected once environmental tests are conducted. The −X shear panel only showed adverse stresses in the most extreme boundary condition case, where its deformation was deemed unrealistic.</p>

mc='thesoftware' c='the software'

corrected abstract:
<p>A finite element model of the CubeSat MIST was created, in order to assess the stresses that occur in the satellite in response to loads during its launch. Due to size limits of the software used, simplifications had to be made to the geometry of the model. The loads assessed were quasi-static accelerations, random vibrations, shock loads, as well as a combined quasi-static acceleration and random vibration case. The study assumed the worst possible loads from a list of different potential launch vehicles for the satellite. Non-linear boundary conditions could not be modelled, and instead different linear boundary condition combinations were assessed. The results showed that the satellite showed positive margins of safety for the quasi-static loads. The lowest natural frequency for the satellite was above 130 Hz. For the random vibration loads, positive margins of safety could be shown if adverse stresses attributed to the boundary conditions in the worst case were ignored. The model proved too conservative to qualify the satellite for the shock loads. Shock testing is therefore recommended for future work, unless requirements for waiving the shock testing can be met. The random vibration and combined loads analysis showed that the −X shear panel experienced high stresses in the corners of its windows, and the part should be inspected once environmental tests are conducted. The −X shear panel only showed adverse stresses in the most extreme boundary condition case, where its deformation was deemed unrealistic.</p>

Note added space for two different sets of merged words
----------------------------------------------------------------------
In diva2:1719736   - correct as is
----------------------------------------------------------------------
In diva2:1803740 
abstract is: 
<p>The thermal conductivity and electrical resistivity of Zr, ZrC, and ZrN were calculated using first-principles density functional theory (DFT) and the Boltzmann transport equation. The electron-phonon scattering was modeled via the self-energy relaxation time approximation (SERTA), and the phonon-phonon scattering via the analogous single-mode relaxation time approximation (SMRTA). The results obtained from Abinit's electron-phonon coupling code EPH is in good agreement with experimental reference data for Zr and ZrN. Notably, the calculated electrical resistivity of ZrC was found to be significantly lower than the available reference data, likely due to deviations from a perfect Zr/C stoichiometric ratio in the experimental samples. Additionally, it was observed that the calculated lattice thermal conductivity was overestimated at low temperatures, possibly attributed to the neglect of electron-phonon scattering that otherwise appears in metallic systems.</p>

corrected abstract:
<p>The thermal conductivity and electrical resistivity of Zr, ZrC, and ZrN were calculated using first-principles density functional theory (DFT) and the Boltzmann transport equation. The electron-phonon scattering was modeled via the self-energy relaxation time approximation (SERTA), and the phonon-phonon scattering via the analogous single-mode relaxation time approximation (SMRTA).</p><p>The results obtained from Abinit's electron-phonon coupling code EPH is in good agreement with experimental reference data for Zr and ZrN. Notably, the calculated electrical resistivity of ZrC was found to be significantly lower than the available reference data, likely due to deviations from a perfect Zr/C stoichiometric ratio in the experimental samples. Additionally, it was observed that the calculated lattice thermal conductivity was overestimated at low temperatures, possibly attributed to the neglect of electron-phonon scattering that otherwise appears in metallic systems.</p>

Note - added missing paragraph break
----------------------------------------------------------------------
In diva2:1751613   - correct as is
----------------------------------------------------------------------
In diva2:1148274
abstract is: 
<p>Besides financial analysis, quantitative tools play a major role in asset management. By managing the aggregation of large amount of historical and prospective data on different asset classes, it can give portfolio allocation solution with respect to risk and regulatory constraints.</p><p>Asset class modeling requires three main steps, the first one is to assess the product features (risk premium and risks) by considering historical and prospective data, which in the case of fixed income depends on spread and default levels. The second is choosing the quantitative model, in this study we introduce a new credit model, which unlike equity like models, model default as a main feature of fixed income performance. The final step consists on calibrating the model.</p><p>We start in this study with the modeling of bond classes and study its behavior in asset allocation, we than model the capital solution transaction as an example of a fixed income structured product.</p>

corrected abstract:
<p>Besides financial analysis, quantitative tools play a major role in asset management. By managing the aggregation of large amount of historical and prospective data on different asset classes, it can give portfolio allocation solution with respect to risk and regulatory constraints.</p><p>Asset class modeling requires three main steps, the first one is to assess the product features (risk premium and risks) by considering historical and prospective data, which in the case of fixed income depends on spread and default levels. The second is choosing the quantitative model , in this study we introduce a new credit model , which unlike equity like models, model default as a main feature of fixed income performance. The final step consists on calibrating the model.</p><p>We start in this study with the modeling of bond classes and study its behavior in asset allocation, we than model the capital solution transaction as an example of a fixed income structured product.</p>

Note the original has a space after "model" and before the ","
----------------------------------------------------------------------
In diva2:1817022   - correct as is
----------------------------------------------------------------------
In diva2:1739387 
abstract is: 
<p>The geostationary orbit has many applications for Earth observation and telecommunications. Like all satellites, spacecraft in this type of orbit are affected by perturbations. These perturbations can modify the satellites trajectories and, without control, deviate them from their target position. In order for these spacecraft to keep their target orbit, station keeping manoeuvres are required. These orbital manoeuvres are performed by thrusters − electrical or chemical − and are thus platform dependent. As part of the ground segments, Flight Dynamics Software programs are used to plan these manoeuvres, taking into account perturbations and orbital propagation – which is the prediction of the orbital evolution of the satellites – as well as the specific spacecraft configuration. Telecommands are then sent periodically to the spacecraft through the ground stations to execute the manoeuvres. Part of the manoeuvres goal is to control the inclination drift, but during the end-of-life of the spacecraft it is also possible to save some fuel − and thus increase the mission lifetime − by not controlling the inclination anymore. In this paper, inclined orbit station keeping strategy and algorithms are developed to an industrial level for a specific spacecraft platform: the SpaceBus Neo, a geostationary platform developed by Thales Alenia Space within ESA’s Neosat program. Additionally, some inclined orbits produced by the developed software are presented and analysed.</p>
skipping mc='keepin'

partal corrected: diva2:1739387: <p>The geostationary orbit has many applications for Earth observation and telecommunications. Like all satellites, spacecraft in this type of orbit are affected by perturbations. These perturbations can modify the satellites trajectories and, without control, deviate them from their target position. In order for these spacecraft to keep their target orbit, station keeping manoeuvres are required. These orbital manoeuvres are performed by thrusters − electrical or chemical − and are thus platform dependent. As part of the ground segments, Flight Dynamics Software programs are used to plan these manoeuvres, taking into account perturbations and orbital propagation – which is the prediction of the orbital evolution of the satellites – as well as the specific spacecraft configuration. Telecommands are then sent periodically to the spacecraft through the ground stations to execute the manoeuvres. Part of the manoeuvres goal is to control the inclination drift, but during the end-of-life of the spacecraft it is also possible to save some fuel − and thus increase the mission lifetime − by not controlling the inclination anymore. In this paper, inclined orbit station keeping strategy and algorithms are developed to an industrial level for a specific spacecraft platform: the SpaceBus Neo, a geostationary platform developed by Thales Alenia Space within ESA’s Neosat program. Additionally, some inclined orbits produced by the developed software are presented and analysed.</p>

corrected abstract:
<p>The geostationary orbit has many applications for Earth observation and telecommunications. Like all satellites, spacecraft in this type of orbit are affected by perturbations. These perturbations can modify the satellites trajectories and, without control, deviate them from their target position. In order for these spacecraft to keep their target orbit, station keeping manoeuvres are required. These orbital manoeuvres are performed by thrusters &mdash; electrical or chemical &mdash; and are thus platform dependent. As part of the ground segments, Flight Dynamics Software programs are used to plan these manoeuvres, taking into account perturbations and orbital propagation &mdash; which is the prediction of the orbital evolution of the satellites &mdash; as well as the specific spacecraft configuration. Telecommands are then sent periodically to the spacecraft through the ground stations to execute the manoeuvres. Part of the manoeuvres goal is to control the inclination drift, but during the end-of-life of the spacecraft it is also possible to save some fuel &mdash; and thus increase the mission lifetime &mdash; by not controlling the inclination anymore. In this paper, inclined orbit station keeping strategy and algorithms are developed to an industrial level for a specific spacecraft platform: the SpaceBus Neo, a geostationary platform developed by Thales Alenia Space within ESA’s Neosat program. Additionally, some inclined orbits produced by the developed software are presented and analysed.</p>

Note used &mdash; for the parentheticals
----------------------------------------------------------------------
In diva2:1583510   - correct as is
----------------------------------------------------------------------
In diva2:738385 - unnessary period at end of title:
"Fluid Dynamics of Phonation."
==>
"Fluid Dynamics of Phonation"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1639320   - correct as is
----------------------------------------------------------------------
In diva2:448529 - there is no abstract in the full text - the "abstract" comes from the "Introduction"
----------------------------------------------------------------------
In diva2:1247296   - correct as is
----------------------------------------------------------------------
In diva2:1163147 
abstract is: 
<p>The purpose of this project is to design and simulate a transport aircraft that the specifications of which are determined by the intended mission. The requirements of the project are that the designed aircraft should be able to transport necessities from Europe to any country in Africa without supplying it with any fuel during the journey. Based on these requirements, further conditions have been imposed on the aircraft. Italy has specifically been selected as the starting destination and Gabon as the final destination, providing a total range of, 9 503 km (5 131 nm), with return included. The designed aircraft will be able to transport a total load of necessities in terms of food and tents weighing 99 208 lb and crew consisting of 6 persons with an approximate weight of 1 102 lb. The aircraft should have a cruise speed of 0.77 M, an altitude of 30 000 ft and a climbing rate of 1 774 ft/min. The supplies will be loaded on 10 pallets where each pallet is released with each parachute, the model of the parachutes are JPADS10K. The airport requires a minimum takeoff distance of 11 700 ft. During the design phase the focus was placed on the airplane's weights and wings. The results that were obtained was that the aircraft's total takeoff weight is 748 702 lb and the wing area (wing reference area) is 5 950 ft2. The remaining necessary calculations that gave these results are presented in the report. In order to perform the project in an efficient and correct manner, a number of seminars and several literature recommendations were given, which gave better understanding of the subject and facilitated the work.</p>

corrected abstract:
<p>The purpose of this project is to design and simulate a transport aircraft that the specifications of which are determined by the intended mission. The requirements of the project are that the designed aircraft should be able to transport necessities from Europe to any country in Africa without supplying it with any fuel during the journey. Based on these requirements, further conditions have been imposed on the aircraft. Italy has specifically been selected as the starting destination and Gabon as the final destination, providing a total range of, 9 503 km (5 131 nm), with return included. The designed aircraft will be able to transport a total load of necessities in terms of food and tents weighing 99 208 lb and crew consisting of 6 persons with an approximate weight of 1 102 lb. The aircraft should have a cruise speed of 0.77 M, an altitude of 30 000 ft and a climbing rate of 1 774 ft/min. The supplies will be loaded on 10 pallets where each pallet is released with each parachute, the model of the parachutes are JPADS10K. The airport requires a minimum takeoff distance of 11 700 ft.</p><p>During the design phase the focus was placed on the airplane's weights and wings. The results that were obtained was that the aircraft's total takeoff weight is 748 702 lb and the wing area (wing reference area) is 5 950 ft<sup>2</sup>. The remaining necessary calculations that gave these results are presented in the report.</p><p>In order to perform the project in an efficient and correct manner, a number of seminars and several literature recommendations were given, which gave better understanding of the subject and facilitated the work.</p>

Note added missing paragraph breaks and fixed superscript
----------------------------------------------------------------------
In diva2:1185828 
abstract is: 
<p>In this thesis, we apply unsupervised and supervised statistical learning methods on the high-yield corporate bond market with the goal of predicting its future excess return. We analyse the excess return of industry based indices of high-yield corporate bonds belonging to the Chemical, Metals, Paper, Building Materials, Packaging, Telecom, and Electric Utility industry. To predict the excess return of these high-yield corporate bond industry indices we utilised externally given market-observable financial time series from 96 different asset and indices that we believe to be of predictive value for the excess return. These input time series covers assets and indices of major equity indices, corporate credit spreads, FX-currencies, stock-, bond-, and FX volatility, swap rates, swap spreads, certain commodities, and macro economic surprise indices. After pre-processing the input data we arrive at 154 predictors that are used in a two-phase implementation procedure consisting of an unsupervised time series Agglomerative Hierarchical clustering and a supervised Random Forest regression model. We use the Hierarchical time series clustering and the Random Forest unbiased variable importance estimates as means to reduce our input predictor space to the ten most influential predictor variables for each industry. These ten most influential predictors are then used in a Random Forest regression model to predict [1, 3, 5, 10] day future cumulative excess return. To accommodate for the characteristics of sequential time series data we also apply a sliding window method to the input predictors and the response variable in our Random Forest model.</p><p>The results show that excess returns in the various industries under study are predictable using Random Forest regression with our market-observable input data. The out-of-sample coefficient of determination <em>R</em>²out is in majority of the cases statistically significant at 0.01 level. The predictability varies across the industries and is in some cases dependent on whether we apply the sliding window method or not. Furthermore, applying the sliding window method on the predictors and the response variable showed in majority of the cases statistically significant improvements in the mean-squared prediction error. The variable importance estimates from such models show that the excess return time series exhibit some degree of autocorrelation.</p>

corrected abstract:
<p>In this thesis, we apply unsupervised and supervised statistical learning methods on the high-yield corporate bond market with the goal of predicting its future excess return. We analyse the excess return of industry based indices of high-yield corporate bonds belonging to the Chemical, Metals, Paper, Building Materials, Packaging, Telecom, and Electric Utility industry. To predict the excess return of these high-yield corporate bond industry indices we utilised externally given market-observable financial time series from 96 different asset and indices that we believe to be of predictive value for the excess return. These input time series covers assets and indices of major equity indices, corporate credit spreads, FX-currencies, stock-, bond-, and FX volatility, swap rates, swap spreads, certain commodities, and macro economic surprise indices. After pre-processing the input data we arrive at 154 predictors that are used in a two-phase implementation procedure consisting of an unsupervised time series Agglomerative Hierarchical clustering and a supervised Random Forest regression model. We use the Hierarchical time series clustering and the Random Forest unbiased variable importance estimates as means to reduce our input predictor space to the ten most influential predictor variables for each industry. These ten most influential predictors are then used in a Random Forest regression model to predict [1, 3, 5, 10] day future cumulative excess return. To accommodate for the characteristics of sequential time series data we also apply a sliding window method to the input predictors and the response variable in our Random Forest model.</p><p>The results show that excess returns in the various industries under study are predictable using Random Forest regression with our market-observable input data. The out-of-sample coefficient of determination 𝑅<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>2</sup><sub>out</sub></span></span> is in majority of the cases statistically significant at 0.01 level. The predictability varies across the industries and is in some cases dependent on whether we apply the sliding window method or not. Furthermore, applying the sliding window method on the predictors and the response variable showed in majority of the cases statistically significant improvements in the mean-squared prediction error. The variable importance estimates from such models show that the excess return time series exhibit some degree of autocorrelation.</p>

Note - fixed the R^2_out
----------------------------------------------------------------------
In diva2:1113475   - correct as is
----------------------------------------------------------------------
In diva2:705996   - correct as is
----------------------------------------------------------------------
In diva2:1663251 
abstract is: 
<p>To extract usable information from financial data the prices of financial instruments must be summarized in an efficient manner. Typically price quotes are sampled at discrete and equidistant points in time to create a time series of prices at fixed times. However, alternative methods that instead utilize certain changes in the price data, such as price changes or drawdowns, could potentially create time series with more relevant information. This thesis builds upon previous research on so called ”directional changes” to establish scaling laws using such alternative sampling methods. This has been studied extensively for foreign exchange rates, and some of those results are replicated in this thesis. But here we also extend the results to a new domain of instruments, namely futures. In addition, data sampled with different methods is investigated for predictability using a simple classifier for forecasting trend direction. The main findings are that the aforementioned scaling laws hold for the time period investigated (2016-2020), and that using other methods than the typical discrete time method yields a more predictable time series when it comes to price trend.</p>

corrected abstract:
<p>To extract usable information from financial data the prices of financial instruments must be summarized in an efficient manner. Typically price quotes are sampled at discrete and equidistant points in time to create a time series of prices at fixed time. However, alternative methods that instead utilize certain changes in the price data, such as price changes or drawdowns, could potentially create time series with more relevant information. This thesis builds upon previous research on so called ”directional changes” to establish scaling laws using such alternative sampling methods. This has been studied extensively for foreign exchange rate, and some of those results are replicated in this thesis. But here we also extend the results to a new domain of instruments, namely futures. In addition, data sampled with different methods is investigated for predictability using a simple classifier for forecasting trend direction. The main findings are that the aforementioned scaling laws hold for the time period investigated (2016-2020), and that using other methods than the typical discrete time method yields a more predictable time series when it comes to price trend.</p>

Note changed "times" to "time" and "rates" to "rate" as per the original
----------------------------------------------------------------------
In diva2:1596315 
abstract is: 
<p>During the COVID-19 pandemic, customer shopping habits have changed. Some industries experienced an abrupt shift during the pandemic outbreak while others navigate in new normal states. For some merchants, the highly-uncertain new phenomena of COVID-19 expresses as outliers in time series of volume of sales. As forecasting models tend to replicate past behavior of a series, outliers complicates the procedure of forecasting; the abnormal events tend to unreliably replicate in forecasts of the subsequent year(s). In this thesis, we investigate how to forecast volume of sales during the abnormal time period of COVID-19, where the classical ARIMA family of models produce unreliable forecasts. The research revolved around three time series exhibiting three types of outliers: a level shift, a transient change and an additive outlier. Upon detecting the time period of the abnormal behavior in each series, two experiments were carried out as attempts for increasing the predictive accuracy for the three extreme cases. The first experiment was related to imputing the abnormal data in the series and the second was related to using a combined model of a pre-pandemic and a post-abnormal forecast. The results of the experiments pointed at significant improvement of the mean absolute percentage error at significance level alpha=0.05 for the level shift when using a combined model compared to the pre-pandemic best-fit SARIMA model. Also, at significant improvement for the additive outlier when using a linear impute. For the transient change, the results pointed at no significant improvement in the predictive accuracy of the experimental models compared to the pre-pandemic best-fit SARIMA model. For the purpose of generalizing to large-scale conclusions of methods' superiority or feasibility for particular abnormal behaviors, empirical evaluations are required. The proposed experimental models were discussed in terms of reliability, validity and quality. By residual diagnostics, it was argued that the models were valid; however, that further improvements can be made. Also, it was argued that the models fulfilled desired attributes of simplicity, scaleability and flexibility. Due to the uncertain phenomena of the COVID-19 pandemic, it was suggested not to take the outputs as long-term reliable solutions. Rather, as temporary solutions requiring more frequent updating of forecasts.</p>

corrected abstract:
<p>During the COVID-19 pandemic, customer shopping habits have changed. Some industries experienced an abrupt shift during the pandemic outbreak while others navigate in new normal states. For some merchants, the highly-uncertain new phenomena of COVID-19 expresses as outliers in time series of volume of sales. As forecasting models tend to replicate past behavior of a series, outliers complicates the procedure of forecasting; the abnormal events tend to unreliably replicate in forecasts of the subsequent year(s).</p><p>In this thesis, we investigate how to forecast volume of sales during the abnormal time period of COVID-19, where the classical ARIMA family of models produce unreliable forecasts. The research revolved around three time series exhibiting three types of outliers: a level shift, a transient change and an additive outlier. Upon detecting the time period of the abnormal behavior in each series, two experiments were carried out as attempts for increasing the predictive accuracy for the three extreme cases. The first experiment was related to imputing the abnormal data in the series and the second was related to using a combined model of a pre-pandemic and a post-abnormal forecast.</p><p>The results of the experiments pointed at significant improvement of the mean absolute percentage error at significance level α = 0.05 for the level shift when using a combined model compared to the pre-pandemic best-fit SARIMA model. Also, at significant improvement for the additive outlier when using a linear impute. For the transient change, the results pointed at no significant improvement in the predictive accuracy of the experimental models compared to the pre-pandemic best-fit SARIMA model. For the purpose of generalizing to large-scale conclusions of methods' superiority or feasibility for particular abnormal behaviors, empirical evaluations are required.</p><p>The proposed experimental models were discussed in terms of reliability, validity and quality. By residual diagnostics, it was argued that the models were valid; however, that further improvements can be made. Also, it was argued that the models fulfilled desired attributes of simplicity, scaleability and flexibility. Due to the uncertain phenomena of the COVID-19 pandemic, it was suggested not to take the outputs as long-term reliable solutions. Rather, as temporary solutions requiring more frequent updating of forecasts.</p>

Note added missing paragraph breaks and turned "alpha" into "α"
----------------------------------------------------------------------
In diva2:1113118 
abstract is: 
<p>The rapid development of computational capacity in recent years has expanded the possibilities of digital modelling in architectural design. Parametric design has emerged from these possibilities with a capacity to generate complex geometries which call for advanced structural systems. Especially for form found structures, where the geometry is determined by structural mechanics, collaboration between architects and structural engineers</p><p>is crucial in early design.</p><p>In this master thesis a form finding algorithm for grid shells has been developed. The algorithm is based on dynamic relaxation with kinetic damping coupled with a structural evaluation by the finite element method. The algorithm is applied to steel and glass grid shells, first with arbitrary boundary geometry and then in case studies where two shells are analysed in greater detail.</p><p>The algorithm is capable to form find grid shells with arbitrary boundary geometry. Convergence of dynamic relaxation is studied to ensure that a structure with a high degree of membrane action is found. Verification of the case study forms show that the algorithm produces grid shells with sufficient structural performance as long as the grid is sparse. However, the algorithm fails to provide structural stability for denser grid patterns, as</p><p>shown by geometrically nonlinear analysis. Deviations from the conditions used to form find the grid shells are shown to adversely affect structural performance. Finally, an attempt to reduce the number of unique elements in grid shells is made using prescriptive dynamic relaxation. This is shown to be a difficult task for the case of complex geometry.</p><p>Structurally meaningful geometry can be achieved by a parametric modelling approach where constraints limit the solution space even if multiple parameters are included. The direct link between computational procedures and software commonly used in architectural design enables successful collaboration between architect and structural engineer.</p>

corrected abstract:
<p>The rapid development of computational capacity in recent years has expanded the possibilities of digital modelling in architectural design. Parametric design has emerged from these possibilities with a capacity to generate complex geometries which call for advanced structural systems. Especially for form found structures, where the geometry is determined by structural mechanics, collaboration between architects and structural engineers is crucial in early design.</p><p>In this master thesis a form finding algorithm for grid shells has been developed. The algorithm is based on dynamic relaxation with kinetic damping coupled with a structural evaluation by the finite element method. The algorithm is applied to steel and glass grid shells, first with arbitrary boundary geometry and then in case studies where two shells are analysed in greater detail.</p><p>The algorithm is capable to form find grid shells with arbitrary boundary geometry. Convergence of dynamic relaxation is studied to ensure that a structure with a high degree of membrane action is found. Verification of the case study forms show that the algorithm produces grid shells with sufficient structural performance as long as the grid is sparse. However, the algorithm fails to provide structural stability for denser grid patterns, as shown by geometrically nonlinear analysis. Deviations from the conditions used to form find the grid shells are shown to adversely affect structural performance. Finally, an attempt to reduce the number of unique elements in grid shells is made using prescriptive dynamic relaxation. This is shown to be a difficult task for the case of complex geometry.</p><p>Structurally meaningful geometry can be achieved by a parametric modelling approach where constraints limit the solution space even if multiple parameters are included. The direct link between computational procedures and software commonly used in architectural design enables successful collaboration between architect and structural engineer.</p>

Note removed unnecessaru paragraph breaks
----------------------------------------------------------------------
In diva2:1740945 
abstract is: 
<p>The phase field method is a versatile tool to study crack initiation and propagation in systems with complex geometries. Based on a variational formulation of the equilibrium equations, the sharp crack topology is regularized by a crack with diffusive edges and the damage is described by a continuous phase field variable. In this work, a phase field model is developed, first in the framework of linear elasticity and then of hyperelasticity. The performance of the model is evaluated on a tensile test of a 2D square pre-cracked at half height. Nevertheless, convergence problems appears for the hyperelastic phase field model, due to the nonlinearity of the problem coupled with the singularity at the crack tip. The influence of many parameters on the results and the numerical convergence is investigated. Very small displacement increments and a large regularization length do not solve the difficulties, but a sufficient value of the numerical viscosity parameter allows to obtain a total propagation of the crack in small deformations. If the convergence issues are increased in large deformations, promising results are obtained on tests without pre-cracking. More sophisticated numerical methods will be needed to improve the robustness of the model. Tensile tests were also performed on rubbers (pre-cracked or not) to guide the choice of a better and more realistic hyperelastic model, to fit the material parameters and to validate the developed model in further works.</p>

corrected abstract:
<p>The phase field method is a versatile tool to study crack initiation and propagation in systems with complex geometries. Based on a variational formulation of the equilibrium equations, the sharp crack topology is regularized by a crack with diffusive edges and the damage is described by a continuous phase field variable. In this work, a phase field model is developed, first in the framework of linear elasticity and then of hyperelasticity. The performance of the model is evaluated on a tensile test of a 2D square pre-cracked at half height. Nevertheless, convergence problems appears for the hyperelastic phase field model, due to the nonlinearity of the problem coupled with the singularity at the crack tip. The influence of many parameters on the results and the numerical convergence is investigated. Very small displacement increments and a large regularization length &lscr;&cscr; do not solve the difficulties, but a sufficient value of the numerical parameter η<sub>𝑛𝑢𝑚</sub> allows to obtain a total propagation of the crack in small deformations. If the convergence issues are increased in large deformations, promising results are obtained on tests without pre-cracking. More sophisticated numerical methods will be needed to improve the robustness of the model. Tensile tests were also performed on rubbers (pre-cracked or not) to guide the choice of a better and more realistic hyperelastic model, to fit the material parameters and to validate the developed model in further works.</p>

Note fixed the equations
----------------------------------------------------------------------
In diva2:854637   - correct as is
----------------------------------------------------------------------
In diva2:1478040   - correct as is
----------------------------------------------------------------------
In diva2:1057263   - correct as is
----------------------------------------------------------------------
In diva2:1655867 
abstract is: 
<p>We present a rigorous yet accessible introduction to structures on finite sets foundational for a formal study of complex networks. This includes a thorough treatment of binary relations, distance spaces, their properties and similarities. Correspondences between relations and graphs are given and a brief introduction to graph theory is followed by a more detailed study of cohesiveness and centrality. We show how graph degeneracy is equivalent to the concept of k-cores, which give a measure of the cohesiveness or interconnectedness of a subgraph. We then further extend this to d-cores of directed graphs. After a brief introduction to topology, focusing on topological spaces from distances, we present a historical discussion on the early developments of algebraic topology. This is followed by a more formal introduction to simplicial homology where we define the homology groups. In the context of algebraic topology, the d-cores of a digraph give rise to a partially ordered set of subgraphs, leading to a set of filtrations that is two-dimensional in nature. Directed clique complexes of digraphs are defined in order to encode the directionality of complete subdigraphs. Finally, we apply these methods to the neuronal network of C.elegans. Persistent homology with respect to directed core filtrations as well as robustness of homology to targeted edge percolations in different directed cores is analyzed. Much importance is placed on intuition and on unifying methods of such dispersed disciplines as sociology and network neuroscience, by rooting them in pure mathematics.</p>

corrected abstract:
<p>We present a rigorous yet accessible introduction to structures on finite sets foundational for a formal study of complex networks. This includes a thorough treatment of binary relations, distance spaces, their properties and similarities. A correspondence between relations and graphs is given and a brief introduction to graph theory is followed by a more detailed study of cohesiveness and centrality. We show how graph degeneracy is equivalent to the concept of k-cores, which give a measure of the cohesiveness or interconnectedness of a subgraph. We then further extend this to d-cores of directed graphs. After a brief introduction to topology, focusing on topological spaces from distances, we present a historical discussion on the early developments of algebraic topology. This is followed by a more formal introduction to homology where we define the homology groups. In the context of algebraic topology, the d-cores of a digraph give rise to a partially ordered set of subgraphs, leading to a set of filtrations that is two dimensional in nature. Directed clique complexes of digraphs are defined in order to encode the directionality of complete subdigraphs. Persistent homology with respect to directed core filtrations as well as robustness of homology to targeted edge percolations in different directed cores is then analyzed. Much importance is placed on intuition and on unifying methodologies of such dispersed disciplines as sociology and network neuroscience, by rooting them in pure mathematics.</p>

Note changed the wording to match the original
----------------------------------------------------------------------
In diva2:1114365 
abstract is: 
<p>This thesis work is based on a dynamic programming solution of a fuel optimizing cruise controller that was developed at Scania CV AB last year. Known data of the road ahead, mainly the slope, is used to continuously calculate the optimal torque and gear choices of a given moving vehicle for a certain horizon. The optimization calculations are based on fuel consumption and the vehicle's arrival time to the final destination.</p><p>This report has been focused on achieving better "driveability" of the cruise controller while still maintaining the good fuel saving qualities that is already there. Simulation is used to evaluate the cruise controller on roads where the wanted data is known. The result is smaller speed variations on at road segments, which will improve a driver's impression of the cruise controller. The great fuel benefits of using roll-techniques in hilly areas is maintained from the previous implementation. The key to the optimal balance between these two behaviors is found using a method that limits the torque usage of the truck to a certain speed interval and then finds exception areas where the torque usage should be unlimited.</p>

corrected abstract:
<p>This thesis work is based on a dynamic programming solution of a fuel optimizing cruise controller that was developed at Scania CV AB last year. Known data of the road ahead, mainly the slope, is used to continuously calculate the optimal torque and gear choices of a given moving vehicle for a certain horizon. The optimization calculations are based on fuel consumption and the vehicle's arrival time to the final destination.</p><p>This report has been focused on achieving better ”driveability” of the cruise controller while still maintaining the good fuel saving qualities that is already there. Simulation is used to evaluate the cruise controller on roads where the wanted data is known. The result is smaller speed variations on flat road segments, which will improve a driver's impression of the cruise controller. The great fuel benefits of using roll-techniques in hilly areas is maintained from the previous implementation. The key to the optimal balance between these two behaviors is found using a method that limits the torque usage of the truck to a certain speed interval and then finds exception areas where the torque usage should be unlimited.</p>

Note fixed missing ligrature "flat" and fixed double quotes to match the original
----------------------------------------------------------------------
In diva2:867867 
abstract is: 
<p>The characterization of the vibromixer principles, in particular FUNDAMIX® technology produced by the Swiss company Dr.Mueller AG, is the focus of this study. Tests varying the vibration’s frequencies and amplitudes, as well as the mixing plate geometry, in terms of number of holes and their diameter, are done.</p><p>Interesting results regarding these parameters are obtained, proving problem complexity and previous experience. Higher amplitudes and frequencies result in a better fluid dynamic performance of the vibromixer, i.e. flow rate formed due to pumping capacity of the plate and creating the liquid recirculation.</p><p>The available total area of the holes should be limited too. Different fluid viscosities (up to 1212mPa/s) are tested and possible carbon fiber improvements in the shaft production briefly discussed. Finally, a Computational Fluid Dynamic approach is done and possible further researches are covered.</p>

corrected abstract:
<p>The characterization of the vibromixer principles, in particular FUNDAMIX® technology produced by the Swiss company Dr.Mueller AG, is the focus of this study. Tests varying the vibration’s frequencies and amplitudes, as well as the mixing plate geometry, in terms of number of holes and their diameter, are done. Interesting results regarding these parameters are obtained, proving problem complexity and previous experience. Higher amplitudes and frequencies result in a better fluid dynamic performance of the vibromixer, i.e. flow rate formed due to pumping capacity of the plate and creating the liquid recirculation. The available total area of the holes should be limited too. Different fluid viscosities (up to 1212mPa/s) are tested and possible carbon fiber improvements in the shaft production briefly discussed. Finally, a Computational Fluid Dynamic approach is done and possible further researches are covered.</p>

Note removed unnecessary paragraph breaks
----------------------------------------------------------------------
In diva2:845399 
abstract is: 
<p>In 1967 Furstenberg proved that the set {2n3mα(mod 1) | n, m ∈<strong>N</strong>} is dense in the circle for any irrational α. He also made the following famous measure rigidity conjecture: the only ergodic measures on the circle invariant under both x —&gt; 2x and x —&gt; 3x are the Lebesgue measure and measures supported on a finite set. In this thesis we discuss both Furstenberg’s theorem and his conjecture, as well as the partial solution of the latter given by Rudolph. Following Matheus’presentation of Avila’s ideas for a proof of a weak version of Rudolph’s theorem, we prove a result on extending measure preservation from a semigroup action to a larger semigroup action. Using this result we obtain restrictions on the set of invariant measures for certain classes of non-abelian affine actions on tori. We also study some general properties of affine abelian and non-abelian actions and we show that analogues of Furstenberg’s theorem hold for affine actions on the circle.</p>
mc='Matheus’presentation' c='Matheus’ presentation'


corrected abstract:
<p>In 1967 Furstenberg proved that the set {2<sup>𝑛</sup>3<sup>𝑚</sup>&#x1D6FC; (mod 1) | 𝑛, 𝑚 ∈ ℕ} is dense in the circle for any irrational &#x1D6FC;. He also made the following famous measure rigidity conjecture: the only ergodic measures on the circle invariant under both 𝑥 ↦ 2𝑥 and 𝑥 ↦ 3𝑥 are the Lebesgue measure and measures supported on a finite set. In this thesis we discuss both Furstenberg’s theorem and his conjecture, as well as the partial solution of the latter given by Rudolph. Following Matheus’ presentation of Avila’s ideas for a proof of a weak version of Rudolph’s theorem, we prove a result on extending measure preservation from a semigroup action to a larger semigroup action. Using this result we obtain restrictions on the set of invariant measures for certain classes of non-abelian affine actions on tori. We also study some general properties of affine abelian and non-abelian actions and we show that analogues of Furstenberg’s theorem hold for affine actions on the circle.</p>

Note - fixed equations; note that "&#x1D6FC;" is Mathematical Italic Small Alpha
----------------------------------------------------------------------
In diva2:458090 
abstract is: 
<p>This report, along with Laurent Gourc’s and Ben Marchant’s reports, presents the work done on the development of the new AcBuilder, realized for CEASIOM. CEASIOM is a package of different modules, developed as part of the SimSAC project, which aims to <strong>Sim</strong>ulate <strong>S</strong>tability A<strong>nd <strong>C</strong>ontrol Characteristics for Use in Conceptual Design. </strong>First, the CEASIOM software is introduced in the context of the SimSAC project and, to know where the development of the aircraft builder tool (AcBuilder) is, an overview of the previous version is shown. Then, based on the issues noticed by the users and the programmers of CEASIOM, the goals of the project are presented in listing some modifications and improvements to bring to the software. Secondly, the document treats about the requirements for the new AcBuilder development in order to reach the goals of the project. Those requirements come from both the programming languages used (Matlab and Java) and from the technical parts of the project (geometrical</p>
<p>construction of aircraft). Finally, this report presents the new AcBuilder tool, its new design interface, its new functionalities and the remaining improvements to implement in order to make the module compatible with the changes brought by the new requirements.</p>

corrected abstract:
<p>This report, along with Laurent Gourc’s and Ben Marchant’s reports, presents the work done on the development of the new AcBuilder, realized for CEASIOM. CEASIOM is a package of different modules, developed as part of the SimSAC project, which aims to <strong>Sim</strong>ulate <strong>S</strong>tability A<strong>nd <strong>C</strong>ontrol Characteristics for Use in Conceptual Design.</p><p>First, the CEASIOM software is introduced in the context of the SimSAC project and, to know where the development of the aircraft builder tool (AcBuilder) is, an overview of the previous version is shown. Then, based on the issues noticed by the users and the programmers of CEASIOM, the goals of the project are presented in listing some modifications and improvements to bring to the software.</p><p>Secondly, the document treats about the requirements for the new AcBuilder development in order to reach the goals of the project. Those requirements come from both the programming languages used (Matlab and Java) and from the technical parts of the project (geometrical construction of aircraft).</p><p>Finally, this report presents the new AcBuilder tool, its new design interface, its new functionalities and the remaining improvements to implement in order to make the module compatible with the changes brought by the new requirements.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1246622 
abstract is: 
<p>This report presents the work conducted at Safran Electronics &amp; Defense in the context of my</p><p>master thesis-internship. Vision-hybridisation with inertial systems has great potential in navigation systems. This master thesis investigates this potential and evaluates it on real data of trajectories for one specific hybridisation: loosely-coupled via an Extended Kalman Filter. The vision hybridisation is developed in order to limit the drift of position of the state-of-the-art GPS-hybridisation when the GPS is not available. This thesis presents hybridisation tests on different trajectories that were either simulated from scratch or reconstructed from real vehicle route (stimulation). Using real data for research work brings technical challenges. Some are presented, as well as the proposed solutions to tackle them.</p>

corrected abstract:
<p>This report presents the work conducted at Safran Electronics &amp; Defense in the context of my master thesis-internship. Vision-hybridisation with inertial systems has great potential in navigation systems. This master thesis investigates this potential and evaluates it on real data of trajectories for one specific hybridisation: loosely-coupled via an Extended Kalman Filter. The vision hybridisation is developed in order to limit the drift of position of the state-of-the-art GPS-hybridisation when the GPS is not available. This thesis presents hybridisation tests on different trajectories that were either simulated from scratch or reconstructed from real vehicle route (stimulation). Using real data for research work brings technical challenges. Some are presented, as well as the proposed solutions to tackle them.</p>

Note removed unnecessary paragraph break
----------------------------------------------------------------------
In diva2:644139   - correct as is
----------------------------------------------------------------------
In diva2:408812 
abstract is: 
<p>Design for Six Sigma (DFSS) is a further development of the Six Sigma draft to carry through preventing quality measures in developing and designing of new products and processes. With increasing demands on enterprises development processes to be more safety and faster the need of DFSS arise as a product, process and service development concept in the organisations. Design for Six Sigma aims to improve and structure the development process of an organisation and consider the application of the innovation problem solving. The concept aims to leave the structure of the traditional Six Sigma concept and instead, from an innovative perspective, search for new solutions to existing problems. The concept constitute of a toolbox with a lot of quality tools that are divided into phases in a development model. Combined with the organizations project model the Design for Six Sigma concept can give rise to excellent results in the work with developing and designing of new products and processes. The condition to succeed is that the draft has been established and accepted with the organizations collaborator. This thesis work summarizes the contents of the Design for Six Sigma concept. The concentration of the performance has been the underlying theory behind the DFSS draft together with the contents of the methodology and the included tools.</p>

corrected abstract:
<p>Design for Six Sigma (DFSS) is a further development of the Six Sigma draft to carry through preventing quality measures in developing and designing of new products and processes. With increasing demands on enterprises development processes to be more safety and faster the need of DFSS arise as a product, process and service development concept in the organisations.</p><p>Design for Six Sigma aims to improve and structure the development process of an organisation and consider the application of the innovation problem solving. The concept aims to leave the structure of the traditional Six Sigma concept and instead, from an innovative perspective, search for new solutions to existing problems.</p><p>The concept constitute of a toolbox with a lot of quality tools that are divided into phases in a development model. Combined with the organizations project model the Design for Six Sigma concept can give rise to excellent results in the work with developing and designing of new products and processes. The condition to succeed is that the draft has been established and accepted with the organizations collaborator.</p><p>This thesis work summarizes the contents of the Design for Six Sigma concept. The concentration of the performance has been the underlying theory behind the DFSS draft together with the contents of the methodology and the included tools.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1218184 
abstract is: 
<p>Financial ratios are used to compare the performance on individual stocks, in order to accurately value them. The price of a stock will however fluctuate over time and ever so often suffer from dizzying peaks or devastating slumps, as did during the financial crisis of 2008. This report seeks to answer whether investors valued stocks differently, with regards to key ratios, before and after the financial crisis, by using multiple linear regression analysis. The S&amp;P 500 companies classified as "Industrials" are chosen to form the basis of this analysis. The results cannot with statistical significance attest to there being a difference in stock valuation using key ratios in a linear regression setting.</p>

corrected abstract:
<p>Financial ratios are used to compare the performance on individual stocks, in order to accurately value them. The price of a stock will however fluctuate over time and ever so often suffer from dizzying peaks or devastating slumps, as did during the financial crisis of 2008. This report seeks to answer whether investors valued stocks differently, with regards to key ratios, before and after the financial crisis, by using multiple linear regression analysis. The S&amp;P 500 companies classified as ”Industrials” are chosen to form the basis of this analysis. The results cannot with statistical significance attest to there being a difference in stock valuation using key ratios in a linear regression setting.</p>

Note fixed double quotes to match original
----------------------------------------------------------------------
In diva2:516099 
abstract is: 
<p>Stockholm Gas AB sells essentially two different products. One is the town gas and the other vehicle gas. Both products will eventually have substantially the biogas as a feedstock. In the current situation though, the city gas consists mainly of natural gas mixed with air. Stockholm Gas AB has undergone major changes and facing new ones. The year 2011 marked a major step in Stockholm Gas climate and environmentally friendly development. The former town gas is based on light petroleum was replaced with a town gas quality consisting of natural gas and / or biogas is mixed with air. In addition to this change, the Stockholm Gas also developed its role as a supplier of biogas as vehicle fuel. New mains for the supply of biogas are under construction to link up biogas production locations with different filling stations. Stockholm Gas AB being a relatively young company, combined with the significant changes mentioned above, Stockholm Gas AB have a need to develop their communication to their customers. This study is a part of that development of communication. The study's overall purpose is to provide Stockholm Gas AB with a basis to develop and improve its communications with its various customer segments in terms of both information channels and the information contents regarding Stockholm Gas AB's both products and the raw material biogas. The aim is that the report should serve as a basis and a guide to Stockholm Gas AB in the preparation of information material relating to its products and raw material biogas, as well as helping Stockholm Gas AB's marketing department to better communicate with the Stockholm Gas AB's customers. Central to this study is to determine what type of information that Stockholm Gas AB's customers are interested in and their channels of choice, and compare this with the views of the staff at Stockholm Gas AB. The customers have in the study been divided into four segments. These segments are vehicle gas key accounts, (mainly taxi companies), vehicle gas private customers, town gas key accounts (primarily chefs) and town gas private customers. The study was conducted through qualitative interviews with employees at the Stockholm Gas AB, and with each of the customer segments. Interview responses to each question have been categorized and from this conclusions have been drawn and results been designed. A result is presented for each of the segments in the form of a Venn diagrams, and in a more deliberative part of the result.</p>

corrected abstract:
<p>Stockholm Gas AB sells essentially two different products. One is the town gas and the other vehicle gas. Both products will eventually have substantially the biogas as a feedstock. In the current situation though, the city gas consists mainly of natural gas mixed with air. Stockholm Gas AB has undergone major changes and facing new ones. The year 2011 marked a major step in Stockholm Gas climate and environmentally friendly development. The former town gas is based on light petroleum was replaced with a town gas quality consisting of natural gas and / or biogas is mixed with air. In addition to this change, the Stockholm Gas also developed its role as a supplier of biogas as vehicle fuel. New mains for the supply of biogas are under construction to link up biogas production locations with different filling stations. Stockholm Gas AB being a relatively young company, combined with the significant changes mentioned above, Stockholm Gas AB have a need to develop their communication to their customers. This study is a part of that development of communication.</p><p>The study's overall purpose is to provide Stockholm Gas AB with a basis to develop and improve its communications with its various customer segments in terms of both information channels and the information contents regarding Stockholm Gas AB's both products and the raw material biogas. The aim is that the report should serve as a basis and a guide to Stockholm Gas AB in the preparation of information material relating to its products and raw material biogas, as well as helping Stockholm Gas AB's marketing department to better communicate with the Stockholm Gas AB's customers. Central to this study is to determine what type of information that Stockholm Gas AB's customers are interested in and their channels of choice, and compare this with the views of the staff at Stockholm Gas AB. The customers have in the study been divided into four segments. These segments are vehicle gas key accounts, (mainly taxi companies), vehicle gas private customers, town gas key accounts (primarily chefs) and town gas private customers. The study was conducted through qualitative interviews with employees at the Stockholm Gas AB, and with each of the customer segments. Interview responses to each question have been categorized and from this conclusions have been drawn and results been designed. A result is presented for each of the segments in the form of a Venn diagrams, and in a more deliberative part of the result.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:653724 - missing spaces in title:
"Gaussian fluctuations of singleeigenvalues intime-dependent GUE"
==>
"Gaussian fluctuations of single eigenvalues in time-dependent GUE"


abstract is: 
<p>This thesis investigates the fluctuations of the eigenvalues of the Gaussian Unitary Ensemble on the basis of numerical simulations performed in the programming environment MATLAB. As is well known: if a Wiener process is defined on the space of Hermitian matrices then the eigenvalues describe a stochastic process known as Dyson Brownian motion. Thus, given a realisation of Dyson Brownian motion, the aim of this thesis is to investigate the distribution of a certain random vector pertaining to fluctuations of the evolution of the eigenvalues using statistical hypothesis tests such as the Kolmogorov-Smirnov test and Mardia’s test for multivariate normality. The results seem to indicate a Gaussian distribution but due to the nature of statistical hypothesis testing the results should be interpreted with caution and as indicative of the underlying distribution.</p>

corrected abstract:
<p>This thesis investigates the fluctuations of the eigenvalues of the Gaussian Unitary Ensemble on the basis of numerical simulations performed in the programming environment MATLAB. As is well known: if a Wiener process is defined on the space of Hermitian matrices then the eigenvalues describe a stochastic process known as Dyson Brownian motion.</p><p>Thus, given a realisation of Dyson Brownian motion, the aim of this thesis is to investigate the distribution of a certain random vector pertaining to fluctuations of the evolution of the eigenvalues using statistical hypothesis tests such as the Kolmogorov-Smirnov test and Mardia’s test for multivariate normality.</p><p>The results seem to indicate a Gaussian distribution but due to the nature of statistical hypothesis testing the results should be interpreted with caution and as indicative of the underlying distribution.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1334734 
abstract is: 
<p>The possible effects of the General Data Protections Regulations (GDPR) have been widely discussed among policymakers, stakeholders and ordinary people who are the objective for data collection. The purpose of GDPR is to protect people’s integrity and increase transparency for how personal data is used. Up until May 25th, 2018 personal data could be sampled and used without consent from users. Many argue that the introduction of GDPR is good, others are reluctant and argue that GDPR may harm data-driven companies.</p><p>The report aims to answer how GDPR affects sales at the flight search engine Flygresor.se. By examining how and to what extent these regulations impact revenue, it is hoped for that these findings will lead to a deeper understanding of how these regulations affect businesses. Multiple linear regression analysis was used as the framework to answer the research question. Numerous models were constructed based on data provided by Flygresor.se. The models mostly included categorical variables representing time indicators such as month, weekday, etc. After carefully performing data modifications, variable selections and model evaluation tests three final models were obtained. After performing statistical inference tests and multicollinearity diagnostics on the models it could be concluded that an effect from GDPR could not be statistically proven. However, this does not mean that an actual effect of GDPR did not occur, only that it could not be isolated and proven. Thus, the extent of the effect of GDPR is statistically inconclusive.</p>

corrected abstract:
<p>The possible effects of the General Data Protections Regulations (GDPR) have been widely discussed among policymakers, stakeholders and ordinary people who are the objective for data collection. The purpose of GDPR is to protect peoples integrity and increase transparency for how personal data is used. Up until May 25th, 2018 personal data could be sampled and used without consent from users. Many argue that the introduction of GDPR is good, others are reluctant and argue that GDPR may harm data driven companies.</p><p>The report aims to answer how GDPR affects sales at the flight search engine Flygresor.se. By examining how and to what extent these regulations impact revenue, it is hoped for that these findings will lead to a deeper understanding of how these regulations affect businesses. Multiple linear regression analysis was used as the framework to answer the research question. Numerous models were constructed based on data provided by Flygresor.se. The models mostly included categorical variables representing time indicators such as month, weekday, etc.</p><p>After carefully performing data modifications, variable selections and model evaluation tests the final three models that were obtained were:

<table border="1">
    <thead>
        <tr>
            <th style="text-align: center;" colspan="2">Final Models</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="width: 25%;">Model (M1)</td>
            <td style="width: 75%;">𝑦 = 𝐼<sub>𝐺𝐷𝑃𝑅</sub> + 𝑥<sub>𝐷𝑎𝑦𝑠</sub> + 𝐼<sub>𝑀𝑜𝑛𝑡ℎ</sub> + 𝐼<sub>𝑊𝑒𝑒𝑘𝑑𝑎𝑦</sub> + 𝐼<sub>𝐻𝑜𝑙𝑖𝑑𝑎𝑦</sub></td>
        </tr>
        <tr>
            <td style="width: 25%;">Model (M2)</td>
            <td style="width: 75%;">𝑦 = 𝐼<sub>𝐺𝐷𝑃𝑅</sub> + 𝐼<sub>𝑌𝑒𝑎𝑟</sub> + 𝐼<sub>𝑀𝑜𝑛𝑡ℎ</sub> + 𝐼<sub>𝑊𝑒𝑒𝑘𝑑𝑎𝑦</sub> + 𝐼<sub>𝐻𝑜𝑙𝑖𝑑𝑎𝑦</sub></td>
        </tr>
        <tr>
            <td style="width: 25%;">Model (M3)</td>
            <td style="width: 75%;">𝑦 = 𝐼<sub>𝐺𝐷𝑃𝑅</sub> &middot; 𝑥<sub>𝐷𝑎𝑦𝑠</sub> + 𝑥<sub>𝐷𝑎𝑦𝑠</sub> + 𝐼<sub>𝑀𝑜𝑛𝑡ℎ</sub> + 𝐼<sub>𝑊𝑒𝑒𝑘𝑑𝑎𝑦</sub> + 𝐼<sub>𝐻𝑜𝑙𝑖𝑑𝑎𝑦</sub></td>
        </tr>
    </tbody>
    <caption>Table 0.1</caption>
</table>

<p>After performing statistical inference tests and multicollinearity diagnostics on the models it could be concluded that an affect from GDPR could not be statistically proven. However, this does not mean that an actual effect of GDPR did not occur, only that it could not be isolated and proven. Thus, the extent of the effect of GDPR is statistically inconclusive.</p>

Note fixed table
----------------------------------------------------------------------
In diva2:1776910 
abstract is: 
<p>Three-dimensional particle tracking is a valuable tool in microfluidics for obtaining information about a system. General Defocus Particle Tracking (GDPT) is a straightforward method of 3D particle tracking that only requires a single-camera plane, making it applicable to existing equipment in a laboratory. </p><p>This project's aim was to evaluate the open-source module DefocusTracker which uses GDPT. DefocusTracker was used to track particles that were levitated in a microchip using ultrasonic standing waves. </p><p>The effects of different calibration methods used and the evaluation of the acoustic energy density over an active part of a piezo on an microchip device were investigated. Different procedures to generate a depth model from the calibration images showed that the choice of step length affects the accuracy of the depth model. A depth model created from the middle part of the field of view provides more accurate results compared to one made from the edge. Levitation experiments demonstrate that higher applied voltages result in a stronger acoustic energy density field over the field of view. The acoustic energy density field and pressure amplitude field show variations across the active piezo on the device, potentially due to a non-uniform thickness of the fluid layer and variations in energy delivery from the piezo.</p><p>Overall, GDPT proves to be a useful method for evaluating unknown aspects of a microfluidic system under the influence of ultrasonic standing waves.</p>


corrected abstract:
<p>Three-dimensional particle tracking is a valuable tool in microfluidics for obtaining information about a system. General Defocus Particle Tracking (GDPT) is a straightforward method of 3D particle tracking that only requires a single-camera plane, making it applicable to existing equipment in a laboratory.</p><p>This project's aim was to evaluate the open-source module DefocusTracker which uses GDPT. DefocusTracker was used to track particles that were levitated in a microchip using ultrasonic standing waves.</p><p>The effects of different calibration methods used and the evaluation of the acoustic energy density over an active part of a piezo on an microchip device were investigated. Different procedures to generate a depth model from the calibration images showed that the choice of step length affects the accuracy of the depth model. A depth model created from the middle part of the field of view provides more accurate results compared to one made from the edge. Levitation experiments demonstrate that higher applied voltages result in a stronger acoustic energy density field over the field of view. The acoustic energy density field and pressure amplitude field show variations across the active piezo on the device, potentially due to a non-uniform thickness of the fluid layer and variations in energy delivery from the piezo.</p><p>Overall, GDPT proves to be a useful method for evaluating unknown aspects of a microfluidic system under the influence of ultrasonic standing waves.</p>

Note only change was to eliminate an unnecessary space at the end of paragraphs
----------------------------------------------------------------------
In diva2:1360650 
abstract is: 
<p>The next generation of Autonomous Underwater Vehicles (AUV) can impact our observation of the world. The flight simulation and full-envelope hydrodynamics modeling can improve the performance of AUVs in terms of control, navigation and positioning. In order to achieve agile maneuverability, a more accurate database of full-envelope hydrodynamic coefficients is supposed to be generated.</p><p>Two semi-empirical methods, Jorgensen and DATCOM, and two numerical method, Computational Fluid Dynamics (CFD) and XFLR5 are used to push the boundaries of hydrodynamic coefficients: lift, drag and moment coefficients for flight-style AUVs at the Swedish Maritime Robotics Center (SMaRC). A comparison of different approaches and tools, and an analysis of the most appropriate approaches for different regions of a defined maneuver has been conducted in this thesis. A data confidence level was proposed as a way to estimate the accuracy of the data and a structured database was built in terms of data confidence level. Different components of the AUV such as the hull body and wings were analyzed separately. The new database is input to a 3DOF Simulink model and the 6DOF SMaRC hydrobatics simulator for flight dynamics simulations. Simulations show that the new database has a good applicability.</p>

corrected abstract:
<p>The next generation of Autonomous Underwater Vehicles (AUV) can impact our observation of the world. The flight simulation and full-envelope hydrodynamics modeling can improve the performance of AUVs in terms of control, navigation and positioning. In order to achieve agile maneuverability, a more accurate database of full-envelope hydrodynamic coefficients is supposed to be generated. Two semi-empirical methods, Jorgensen and DATCOM, and two numerical method, Computational Fluid Dynamics (CFD) and XFLR5 are used to push the boundaries of hydrodynamic coefficients: lift, drag and moment coefficients for flight-style AUVs at the Swedish Maritime Robotics Center (SMaRC). A comparison of different approaches and tools, and an analysis of the most appropriate approaches for different regions of a defined maneuver has been conducted in this thesis. A data confidence level was proposed as a way to estimate the accuracy of the data and a structured database was built in terms of data confidence level. Different components of the AUV such as the hull body and wings were analyzed separately. The new database is input to a 3DOF Simulink model and the 6DOF SMaRC hydrobatics simulator for flight dynamics simulations. Simulations show that the new database has a good applicability.</p>

Note removed unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1215620   - correct as is
----------------------------------------------------------------------
In diva2:1477976 
abstract is: 
<p>The aorta, the main and largest artery in the human body, is susceptible for many types of problems. One of the most common aortic disease is the formation of an aneurysm. Endovascular aortic aneurysm repair (EVAR) is a minimally invasive treatment option for aortic aneurysms, involving the deployment of an expandable stent graft within the aorta without operating the aneurysm directly. With 1.5 to 43 % of EVAR patients having postoperative complications, research to help predict these complications of EVAR is of essence. In this study, the deformations of the aorta induced by a deployed stent graft have been investigated and visualized in order to aid understanding of the geometrical behaviour of the aorta post EVAR. This has been carried out by the development and analysis of patient-specific aortic 3D reconstruction models, 3D printed physical models and FE simulation models. A qualitative assessment of the deformations was achieved by superimposing reconstructed geometries, revealing a light straightening of the aorta and iliac vessels, as well as anterior movement of the iliac branches. Based on the good agreement between the simulated and reconstructed geometries, the findings suggest that such deformations could be derived from the pressure being removed from the aneurysm due to the deployed stent graft, in combination with stent radial forces from the proximal and distal landing zones. Despite that the simulation seemed to underestimate distal movement of the iliac vessel, this study emphasizes the potential of 3D printing and FE analysis as promising tools for planning and research of EVAR.</p>

corrected abstract:
<p>The aorta, the main and largest artery in the human body, is susceptible for many types of problems. One of the most common aortic disease is the formation of an aneurysm. Endovascular aortic aneurysm repair (EVAR) is a minimally invasive treatment option for aortic aneurysms, involving the deployment of an expandable stent graft within the aorta without operating the aneurysm directly.</p><p>With 1.5 to 43 % of EVAR patients having postoperative complications, research to help predict these complications of EVAR is of essence. In this study, the deformations of the aorta induced by a deployed stent graft have been investigated and visualized in order to aid understanding of the geometrical behaviour of the aorta post EVAR. This has been carried out by the development and analysis of patient-specific aortic 3D reconstruction models, 3D printed physical models and FE simulation models.</p><p>A qualitative assessment of the deformations was achieved by superimposing reconstructed geometries, revealing a light straightening of the aorta and iliac vessels, as well as anterior movement of the iliac branches. Based on the good agreement between the simulated and reconstructed geometries, the findings suggest that such deformations could be derived from the pressure being removed from the aneurysm due to the deployed stent graft, in combination with stent radial forces from the proximal and distal landing zones. Despite that the simulation seemed to underestimate distal movement of the iliac vessel, this study emphasizes the potential of 3D printing and FE analysis as promising tools for planning and research of EVAR.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:676695   - correct as is

Note last sentence does not have a terminal punctuation in the original
----------------------------------------------------------------------
In diva2:1711769   - correct as is
----------------------------------------------------------------------
In diva2:1642676   - correct as is
----------------------------------------------------------------------
In diva2:688968 
abstract is: 
<p>A GPU Monte Carlo code for x-ray photon transport has been implemented and extensively tested. The code is intended for scatter compensation of cone beam computed tomography images.</p><p>The code was tested to agree with other well known codes within 5% for a set of simple scenarios. The scatter compensation was also tested using an artificial head phantom. The errors in the reconstructed Hounsfield values were reduced by approximately 70%.</p><p>Several variance reduction methods have been tested, although most were found infeasible on GPUs. The code is nonetheless fast, and can simulate approximately 3 ·10<sup>9</sup> photons per minute on a NVIDIA Quadro 4000 graphics card. With the use of appropriate filtering methods, the code can be used to calculate patient specific scatter distributions for a full CBCT scan in approximately one minute, allowing scatter reduction in clinical applications.</p>

corrected abstract:
<p>A GPU Monte Carlo code for x-ray photon transport has been implemented and extensively tested. The code is intended for scatter compensation of cone beam computed tomography images.</p><p>The code was tested to agree with other well known codes within 5% for a set of simple scenarios. The scatter compensation was also tested using an artificial head phantom. The errors in the reconstructed Hounsfield values were reduced by approximately 70%.</p><p>Several variance reduction methods have been tested, although most were found infeasible on GPUs. The code is nonetheless fast, and can simulate approximately 3 · 10<sup>9</sup> photons per minute on a NVIDIA Quadro 4000 graphics card. With the use of appropriate filtering methods, the code can be used to calculate patient specific scatter distributions for a full CBCT scan in approximately one minute, allowing scatter reduction in clinical applications.</p>

Note - minor change - added a space after the center dot
----------------------------------------------------------------------
In diva2:1211267   - correct as is
----------------------------------------------------------------------
In diva2:1432668 
abstract is: 
<p>This work addresses the following research question: Can we detect videogame glitches using Convolutional Neural Networks? Focusing on the most common types of glitches, texture glitches (Stretched, Lower Resolution, Missing, and Placeholder). We first systematically generate a dataset with both images with texture glitches and normal samples. </p><p>To detect the faulty images we try both Classification and Semantic Segmentation approaches, with a clear focus on the former. The best setting in classification uses a ShuffleNetV2 architecture and obtains precisions of 80.0%, 64.3%, 99.2%, and 97.0% in the respective glitch classes Stretched, Lower Resolution, Missing, and Placeholder. All of this with a low false positive rate of 6.7%.</p><p>To complement this study, we also discuss how the models extrapolate to different graphical environments, which are the main sources of confusion for the model, how to estimate the confidence of the network, and ways to interpret the internal behavior of the models.</p>

corrected abstract:
<p>This work addresses the following research question: <em>Can we detect videogame glitches using Convolutional Neural Networks?</em> Focusing in the most common types of glitches, texture glitches (Stretched, Lower Resolution, Missing and Placeholder). We first systematically generate a dataset with both images with texture glitches and normal samples.</p><p>To detect the faulty images we try both Classification and Semantic Segmentation approaches, with a clear focus on the former. The best setting in classification uses a ShuffleNetV2 architecture and obtains precisions of 80.0%, 64.3%, 99.2%, and 97.0% in the respective glitch classes Stretched, Lower Resolution, Missing, and Placeholder. All of this with a low false positive rate of 6.7%.</p><p>To complement this study, we also discuss how the models extrapolate to different graphical environments, which are the main sources of confusion for the model, how to estimate the confidence of the network, and ways to interpret the internal behavior of the models.</p>

Note eliminated an unnecessary space at the end of a paragraph, changed text to match the original, added italics
----------------------------------------------------------------------
In diva2:753585   - correct as is
----------------------------------------------------------------------
In diva2:1210580 
abstract is: 
<p>This thesis sets out to examine if and how private equity funds should hedge foreign exchange exposure. To our knowledge the field of foreign exchange hedging within private equity, from the private equity firms’ point of view, is vastly unexplored scientifically. The subject is important since foreign ex-change risk has a larger impact on private equity returns now than historically due to increased competition, cross-boarder investments and foreign exchange volatility. In order to answer the research question a simulation model is constructed and implemented under different scenarios. Foreign exchange rates are simulated and theoretical private equity funds are investigated and com-pared under different performance measures. The underlying mathematical theory originates from the work of Black and Scholes.</p><p>The main result of this thesis is that private equity funds cannot achieve a higher internal rate of return on average through hedging of foreign exchange exposure independent of the slope of the foreign exchange forward curve. However, hedging strategies yielding the same mean internal rate of return but performing better in terms of performance measures accounting for volatility of returns have been found. Furthermore, we found that the conclusions are independent of whether the current or forward foreign exchange rate is a better approximation for the future foreign exchange rate.</p>

corrected abstract:
<p>This thesis sets out to examine if and how private equity funds should hedge foreign exchange exposure. To our knowledge the field of foreign exchange hedging within private equity, from the private equity firms’ point of view, is vastly unexplored scientifically. The subject is important since foreign exchange risk has a larger impact on private equity returns now than historically due to increased competition, cross-boarder investments and foreign exchange volatility. In order to answer the research question a simulation model is constructed and implemented under different scenarios. Foreign exchange rates are simulated and theoretical private equity funds are investigated and compared under different performance measures. The underlying mathematical theory originates from the work of Black and Scholes.</p><p>The main result of this thesis is that private equity funds cannot achieve a higher internal rate of return on average through hedging of foreign exchange exposure independent of the slope of the foreign exchange forward curve. However, hedging strategies yielding the same mean internal rate of return but performing better in terms of performance measures accounting for volatility of returns have been found. Furthermore, we found that the conclusions are independent of whether the current or forward foreign exchange rate is a better approximation for the future foreign exchange rate.</p>

Note removed unnecessary hyphens
----------------------------------------------------------------------
In diva2:821940   - correct as is
----------------------------------------------------------------------
In diva2:1780176 
abstract is: 
<p>Vehicle Routing Problems are optimization problems centered around determining optimal travel routes for a fleet of vehicles to visit a set of nodes. Optimality is evaluated with regard to some desired quality of the solution, such as time-minimizing or cost-minimizing. There are many established solution methods which makes it meaningful to compare their performance. This thesis aims to investigate how the performances of various solution methods is affected by varying certain problem parameters. Problem characteristics such as the number of customers, vehicle capacity, and customer demand are investigated. The aim was approached by dividing the problem into two subproblems: distributing the nodes into suitable clusters, and finding the shortest route within each cluster. Results were produced by solving simulated sets of customers for different parameter values with different clustering methods, namely sweep, k-means and hierarchical clustering. Although the model required simplifications to facilitate the implementation, theresults provided some significant findings. The thesis concludes that for large vehicle capacity in relation to demand, sweep clustering is the preferred method. Whereas for smaller vehicles, the other two methods perform better.</p>

corrected abstract:
<p>Vehicle Routing Problems are optimization problems centered around determining optimal travel routes for a fleet of vehicles to visit a set of nodes. Optimality is evaluated with regard to some desired quality of the solution, such as time-minimizing or cost-minimizing. There are many established solution methods which makes it meaningful to compare their performance. This thesis aims to investigate how the performances of various solution methods is affected by varying certain problem parameters. Problem characteristics such as the number of customers, vehicle capacity, and customer demand are investigated. The aim was approached by dividing the problem into two subproblems: distributing the nodes into suitable clusters, and finding the shortest route within each cluster. Results were produced by solving simulated sets of customers for different parameter values with different clustering methods, namely sweep, k-means and hierarchical clustering. Although the model required simplifications to facilitate the implementation, the results provided some significant findings. The thesis concludes that for large vehicle capacity in relation to demand, sweep clustering is the preferred method. Whereas for smaller vehicles, the other two methods perform better.</p>

Note changed "theresults" to "the results"
----------------------------------------------------------------------
In diva2:1210846   - correct as is
----------------------------------------------------------------------
In diva2:461185 -- the full text contails the "Examensarbetesansökan"!!!
----------------------------------------------------------------------
In diva2:725544   - correct as is
----------------------------------------------------------------------
In diva2:813795   - correct as is
----------------------------------------------------------------------
In diva2:1680705 
abstract is: 
<p>We introduce localization and sheaves to define projective schemes, and in particular the projective n-space. Afterwards, we define closed subschemes of projective space and show that they arise from quotients of graded rings by homogeneous ideals. We then define the Hilbert function and Hilbert polynomial to determine several invariants of closed subschemes of projective space: their degree, dimension, and arithmetic genus. Finally, we provide numerous examples with explicit computations, finding the invariants of hypersurfaces, curves, the twisted cubic and more.</p>

corrected abstract:
<p>We introduce localization and sheaves to define projective schemes, and in particular the projective 𝑛-space. Afterwards, we define closed subschemes of projective space and show that they arise from quotients of graded rings by homogeneous ideals. We then define the Hilbert function and Hilbert polynomial to determine several invariants of closed subschemes of projective space: their degree, dimension, and arithmetic genus. Finally, we provide numerous examples with explicit computations, finding the invariants of hypersurfaces, curves, the twisted cubic and more.</p>

Note replaced "n" with "𝑛" in n-space
----------------------------------------------------------------------
In diva2:1816805   - correct as is
----------------------------------------------------------------------
In diva2:556631   - correct as is
----------------------------------------------------------------------
In diva2:1114446   - correct as is
----------------------------------------------------------------------
In diva2:1210760   - correct as is
----------------------------------------------------------------------
In diva2:1432493   - correct as is
----------------------------------------------------------------------
In diva2:1833732 -- unnecessary double colon in title:
"How Unlucky People Continue to be Unlucky:: A Study of the Predictive Capabilities of Insurance Claim Data"
==>
"How Unlucky People Continue to be Unlucky: A Study of the Predictive Capabilities of Insurance Claim Data"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:408825 - 'diva2:408824' seems to be a duplicate
abstract is: 
<p>This thesis analyzes the existing course evaluation system at VägsektornsUtbildningscentrum (VUC). To be able to study this we have used focus groups, interviews and auscultation. The study shows that there is adiscrepancy on what VUC says about the course evaluation and how the course evaluation is being used today. One conclusion is that that the information that is generated by the course evaluation corresponds to VUC goal with the course evaluation. Another conclusion is that the course evaluation not is usable for developing the course. With relative small means like asking the right question in the course evaluation could change this, and involve the teachers in the course evaluation system.</p>


corrected abstract:
<p>This thesis analyzes the existing course evaluation system at VägsektornsUtbildningscentrum (VUC).</p><p>To be able to study this we have used focus groups, interviews and auscultation. The study shows that there is a discrepancy on what VUC says about the course evaluation and how the course evaluation is being used today.</p><p>One conclusion is that that the information that is generated by the course evaluation corresponds to VUC goal with the course evaluation. Another conclusion is that the course evaluation not is usable for developing the course. With relative small means like asking the right question in the course evaluation could change this, and involve the teachers in the course evaluation system.</p>

Note added missing paragraph breaks and corrected merged word:
mc='adiscrepancy' c='a discrepancy'

Also "VägsektornsUtbildningscentrum" should be "Vägsektorns Utbildningscentrum" - error in original (it is correct in the Swedish abstract)
----------------------------------------------------------------------
In diva2:725634   - correct as is
----------------------------------------------------------------------
In diva2:1380827 
abstract is: 
<p>Optimal orbital trajectories are obtained through the solution of highly nonlinear large scale problems. In the case of low-thrust propulsion applications, the spacecraft benefits from high specific impulses and, hence, greater payload mass. However, these missions require a high count of orbital revolutions and, therefore, display augmented sensitivity to many disturbances. Solutions to such problems can be tackled via a discrete approach, using optimal feedback control laws. Historically, differential dynamic programming (DDP) has shown outstanding results in tackling these problems. A state of the art software that implements a variation of DDP has been developed by Whiffen and it is used by NASA’s DAWN mission [Mystic: Implementation of the Static Dynamic Optimal Control Algorithm for High-Fidelity, Low-Thrust Trajectory Design" , AAS/AIAA Astrodynamics Specialist Conference, (Keystone, Colorado), American Institute of Aeronautics and Astronautics, Aug. 21, 2006]. One of the latest techniques implemented to deal with these discrete constrained optimizations is the Hybrid Differential Dynamic Programming (HDDP) algorithm, introduced by Lantoine and Russell in [A Hybrid Differential Dynamic Programming Algorithm for Constrained Optimal Control Problems. Part 1: Theory", Journal of Optimization Theory and Applications, vol. 154, pp. 382-417, issue 2, Aug. 1, 2012]. This method complements the reliability and efficiency of classic nonlinear programming techniques with the robustness to poor initial guesses and the reduced computational effort of DDP. The key feature of the algorithm is the exploitation of a second order state transition matrix procedure to propagate the needed partials, decoupling the dynamics from the optimization. In doing so, it renders the integration of dynamical equations suitable for parallelization. Together with the possibility to treat constrained problems, this represents the greatest improvement of classic DDP. Nevertheless, the major limitation of this approach is the high computational cost to evaluate the required state transition matrices. Analytical derivatives, when available, have shown a significant reduction in the computational cost and time for HDDP application. This work applies differential algebra to HDDP to cope with this limitation. In particular, differential algebra is introduced to obtain state transition matrices as polynomial maps. These maps come directly from the integration of the dynamics of the system, removing the dedicated algorithmic step and reducing its computational cost. Moreover, by operating on polynomial maps, all the solutions of local optimization problems are treated through differential algebraic techniques. This approach allows us to deal with higher order expansions of the cost, without modifying the algorithm. The leading assumption of this work is that, treating higher than second order expansions, grants larger radii of convergence for the algorithm, improved robustness to initial guesses, hence faster rates of convergence. Examples are presented in this thesis to assess the performance of the newly constructed algorithm and to test the assumptions.​</p>

corrected abstract:
<p>Optimal orbital trajectories are obtained through the solution of highly nonlinear large scale problems. In the case of low-thrust propulsion applications, the spacecraft benefits from high specific impulses and, hence, greater payload mass. However, these missions require a high count of orbital revolutions and, therefore, display augmented sensitivity to many disturbances. Solutions to such problems can be tackled via a discrete approach, using optimal feedback control laws. Historically, differential dynamic programming (DDP) has shown outstanding results in tackling these problems. A state of the art software that implements a variation of DDP has been developed by Whiffen and it is used by NASA’s DAWN mission: ["Mystic: Implementation of the Static Dynamic Optimal Control Algorithm for High-Fidelity, Low-Thrust Trajectory Design" , AAS/AIAA Astrodynamics Specialist Conference, (Keystone, Colorado), American Institute of Aeronautics and Astronautics, Aug. 21, 2006]. One of the latest techniques implemented to deal with these discrete constrained optimizations is the Hybrid Differential Dynamic Programming (HDDP) algorithm, introduced by Lantoine and Russell in ["A Hybrid Differential Dynamic Programming Algorithm for Constrained Optimal Control Problems. Part 1: Theory", Journal of Optimization Theory and Applications, vol. 154, pp. 382-417, issue 2, Aug. 1, 2012]. This method complements the reliability and efficiency of classic nonlinear programming techniques with the robustness to poor initial guesses and the reduced computational effort of DDP. The key feature of the algorithm is the exploitation of a second order state transition matrix procedure to propagate the needed partials, decoupling the dynamics from the optimization. In doing so, it renders the integration of dynamical equations suitable for parallelization. Together with the possibility to treat constrained problems, this represents the greatest improvement of classic DDP. Nevertheless, the major limitation of this approach is the high computational cost to evaluate the required state transition matrices. Analytical derivatives, when available, have shown a significant reduction in the computational cost and time for HDDP application.</p><p>This work applies differential algebra to HDDP to cope with this limitation. In particular, differential algebra is introduced to obtain state transition matrices as polynomial maps. These maps come directly from the integration of the dynamics of the system, removing the dedicated algorithmic step and reducing its computational cost. Moreover, by operating on polynomial maps, all the solutions of local optimization problems are treated through differential algebraic techniques. This approach allows us to deal with higher order expansions of the cost, without modifying the algorithm. The leading assumption of this work is that, treating higher than second order expansions, grants larger radii of convergence for the algorithm, improved robustness to initial guesses, hence faster rates of convergence. Examples are presented in this thesis to assess the performance of the newly constructed algorithm and to test the assumptions.​</p>

Note added missing paragraph break, fixed some missing opening douple quotes, and added italics
----------------------------------------------------------------------
In diva2:440403   - correct as is
----------------------------------------------------------------------
In diva2:1381816   - correct as is
----------------------------------------------------------------------
In diva2:919659   - correct as is
----------------------------------------------------------------------
In diva2:1680206 
abstract is: 
<p>Ocean noise pollution is an invisible but growing threat. There are many sources of sound in the ocean but human underwater radiated noise, in particular from shipping is one of the most prominent one. Ocean noise pollution can interfere or sometimes even directly harm marine life. </p><p>This thesis is in collaboration with Kongsberg Maritime which aims to develop an underwater radiated noise prediction method for the ELegance pod system. In particular, the focus is on the noise generated as a direct effect of the permanent magnet motor vibrations. Kongsberg wants to be able to calculate the underwater radiated noise for different pod geometries and engine configurations in order to find an optimal operating speed of the electric motor.</p><p>The underwater radiated noise prediction is carried out using two methods. The first one is a 2-way coupled fluid-structure interaction harmonic response model, dealing with the vibrations. In addition, the flow induced noise is evaluated using CFD combined with Ffowcs-Williams Hawkings acoustic analogy. </p><p>The harmonic response model is used to calculate the sound in terms of a frequency response, which can be translated to revolutions per minute of the rotor. This allows Kongsberg to identify rotor speeds where the operation may or may not be optimal. The flow induced noise is investigated for a typical transit speed. The results show this noise is multiple orders of magnitude smaller than the sound caused by the vibrations. This together with the fact that the computational cost of CFD is large suggests that the flow induced noise is not something Kongsberg needs to consider at an early design stage.</p><p>Neither the propeller nor cavitation is considered in this thesis, due to the limited computational resources but also that Kongsberg designs propellers that are vessel specific. These sources of sound become important when considering the full acoustic profile of a propulsion unit of this type.</p>

corrected abstract:
<p>Ocean noise pollution is an invisible but growing threat. There are many sources of sound in the ocean but human underwater radiated noise, in particular from shipping is one of the most prominent one. Ocean noise pollution can interfere or sometimes even directly harm marine life.</p><p>This thesis is in collaboration with Kongsberg Maritime which aims to develop an underwater radiated noise prediction method for the ELegance pod system. In particular, the focus is on the noise generated as a direct effect of the permanent magnet motor vibrations. Kongsberg wants to be able to calculate the underwater radiated noise for different pod geometries and engine configurations in order to find an optimal operating speed of the electric motor.</p><p>The underwater radiated noise prediction is carried out using two methods. The first one is a 2-way coupled fluid-structure interaction harmonic response model, dealing with the vibrations. In addition, the flow induced noise is evaluated using CFD combined with Ffowcs-Williams Hawkings acoustic analogy.</p><p>The harmonic response model is used to calculate the sound in terms of a frequency response, which can be translated to revolutions per minute of the rotor. This allows Kongsberg to identify rotor speeds where the operation may or may not be optimal. The flow induced noise is investigated for a typical transit speed. The results show this noise is multiple orders of magnitude smaller than the sound caused by the vibrations. This together with the fact that the computational cost of CFD is large suggests that the flow induced noise is not something Kongsberg needs to consider at an early design stage.</p><p>Neither the propeller nor cavitation is considered in this thesis, due to the limited computational resources but also that Kongsberg designs propellers that are vessel specific. These sources of sound become important when considering the full acoustic profile of a propulsion unit of this type.</p>

Note only change was to eliminate an unnecessary space at the end of paragraphs
----------------------------------------------------------------------
In diva2:1044882   - correct as is
----------------------------------------------------------------------
In diva2:854634   - correct as is
----------------------------------------------------------------------
In diva2:796768   - correct as is
----------------------------------------------------------------------
In diva2:942565 
abstract is: 
<p>This thesis, in mathematical statistics and Industrial engineering and Management, consists of two parts. The main purpose of the first part is to conduct a multiple regression and create a prediction model for football transfers. The data for this thesis is gathered from the German website transfermarkt.co.uk and the data are just from the league’s Premier League, La Liga and Serie A. The final prediction model that was taken out from the regression model has an explanation degree of 47.1 percent and the players transfer fee could be explained by nine independent variables.</p><p>he purpose of the second part of the thesis is to identify and analyse revenue strategies that football clubs in Premier League, La Liga och Serie A are using and also investigate the revenue growth for the clubs. In this thesis, three clubs are chosen for analysing where all the clubs differ in economic and supporter size. The clubs are Real Madrid, Juventus and Everton. These clubs are examined in terms of their revenue models that consist three categories: Commercial revenue, Broadcasting revenue and Matchday revenue. </p><p>The result of this study shows that beyond Commercial revenue and Matchday revenue are the only parts of the revenue model a club can influence by implementing a new strategy. The results of the study shows that in addition to good sporting performances, also Commercial revenue and Matchday revenue are the categories in the revenue models that the clubs can affect with help of new business strategies. For a club to achieve higher growth within Commercial revenue and Matchday revenue shows the results that the clubs should increase the exposure of the club and have a privately owned stadium.</p>

corrected abstract:
<p>This thesis, in mathematical statistics and Industrial engineering and Management, consists of two parts. The main purpose of the first part is to conduct a multiple regression and create a prediction model for football transfers. The data for this thesis is gathered from the German website transfermarkt.co.uk and the data are just from the league’s Premier League, La Liga and Serie A. The final prediction model that was taken out from the regression model has an explanation degree of 47.1 percent and the players transfer fee could be explained by nine independent variables.</p><p>The purpose of the second part of the thesis is to identify and analyse revenue strategies that football clubs in Premier League, La Liga och Serie A are using and also investigate the revenue growth for the clubs. In this thesis, three clubs are chosen for analysing where all the clubs differ in economic and supporter size. The clubs are Real Madrid, Juventus and Everton. These clubs are examined in terms of their revenue models that consist three categories: Commercial revenue, Broadcasting revenue and Matchday revenue.</p><p>The result of this study shows that beyond Commercial revenue and Matchday revenue are the only parts of the revenue model a club can influence by implementing a new strategy. The results of the study shows that in addition to good sporting performances, also Commercial revenue and Matchday revenue are the categories in the revenue models that the clubs can affect with help of new business strategies. For a club to achieve higher growth within Commercial revenue and Matchday revenue shows the results that the clubs should increase the exposure of the club and have a privately owned stadium.</p>

Note missing "T" and eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1781244 - beta in title should be β:
"Identifying effects of adrenaline and dopamine binding on the beta2-adrenergic receptor structure and function using machine learning"
==>
"Identifying effects of adrenaline and dopamine binding on the  β2-adrenergic receptor structure and function using machine learning"

abstract is: 
<p>The beta2-adrenergic receptor is a G-protein coupled receptor, involved in several physiological processes, which enables signaling through the cell membrane. To study the effect of dopamine and adrenaline binding on the receptor structure and function, we used machine learning methods applied to data from molecular dynamics simulations. We found that the three machine learning methods Random Forest, Kullback-Leibler divergence, and Principal Component Analysis generated results that correspond to previous studies. When comparing the active state of the receptor with or without a ligand bound, we found that residues around Ser203 and Asn301 of the orthosteric binding pocket and residues around Ala91 of the TM2 differed. When instead comparing the active state of the receptor with adrenaline or dopamine bound, we found that residues around Thr68 differed. Additionally, we also found that adrenaline and dopamine cause different structural changes in the intracellular parts of TM5 and TM6. These findings indicate ligand-specific effects on the receptor, providing potentially useful information for the understanding of the interaction of adrenaline and dopamine with the beta2-adrenergic receptor.</p>

corrected abstract:
<p>The β<sub>2z/sub>-adrenergic receptor is a G-protein coupled receptor, involved in several physiological processes, which enables signaling through the cell membrane. To study the effect of dopamine and adrenaline binding on the receptor structure and function, we used machine learning methods applied to data from molecular dynamics simulations. We found that the three machine learning methods Random Forest, Kullback-Leibler divergence, and Principal Component Analysis generated results that correspond to previous studies. When comparing the active state of the receptor with or without a ligand bound, we found that residues around Ser203 and Asn301 of the orthosteric binding pocket and residues around Ala91 of the TM2 differed. When instead comparing the active state of the receptor with adrenaline or dopamine bound, we found that residues around Thr68 differed. Additionally, we also found that adrenaline and dopamine cause different structural changes in the intracellular parts of TM5 and TM6. These findings indicate ligand-specific effects on the receptor, providing potentially useful information for the understanding of the interaction of adrenaline and dopamine with the β<sub>2z/sub>-adrenergic receptor.</p>

Note fixed "beta2" to be "β<sub>2z/sub>"
Also, it shuold probably be this in the tifle - error in original
----------------------------------------------------------------------
In diva2:1761883   - correct as is
----------------------------------------------------------------------
In diva2:1342449   - correct as is
----------------------------------------------------------------------
In diva2:1335194   - correct as is
----------------------------------------------------------------------
In diva2:1722077 
abstract is: 
<p>This thesis presents studies on III-V semiconductor waveguides with particular emphasis on second-order optical nonlinearity. The nonlinear processes that were investigated in this thesis are the Second Harmonic Generation (SHG) and the Spontaneous Parametric Down-Conversion (SPDC). The optical waveguides are made of InGaP and the waveguide design includes tapered parts for in- and out-coupling of guided light. Simulation of light propagation and modal solutions were done using Lumerical MODE, FDTD, and COMSOL Multiphysics software. The in- and outcoupling for the design of tapered waveguide that utilize the bulk non-linearity is 65 % when the waveguide is 145 nm thick and 2.60 μm wide having PMMA as top cladding. The SHG conversion efficiency for this configuration when the waveguide length is 2 μm long, is found 31 %/W. Three cases of the utilization of the surface non-linearity are proposed too. Preliminary steps toward the fabrication of the waveguide structures are also reported. The particular mesa-isolated substrates are fabricated having a side wall with a negative angle profile that result to a significant undercut. InGaP waveguides were transferred to the target substrates successfully and the process that was used can enable heterogeneous integration of InGaP and SOI platform.</p>

corrected abstract:
<p>This thesis presents studies on III-V semiconductor waveguides with particular emphasis on second-order optical nonlinearity. The nonlinear processes that were investigated in this thesis are the Second Harmonic Generation (SHG) and the Spontaneous Parametric Down-Conversion (SPDC). The optical waveguides are made of InGaP and the waveguide design includes tapered parts for in- and out-coupling of guided light. Simulation of light propagation and modal solutions were done using Lumerical MODE, FDTD, and COMSOL Multiphysics software. The in- and out- coupling for the design of tapered waveguide that utilize the bulk non-linearity is 65% when the waveguide is 145 nm thick and 2.60 µm wide having PMMA as top cladding. The SHG conversion efficiency for this configuration when the waveguide length is 2 µm long, is found 31 % W<sup>-1</sup>. Three cases of the utilization of the surface non-linearity are proposed too. Preliminary steps toward the fabrication of the waveguide structures are also reported. The particular mesa-isolated substrates are fabricated having a side wall with a negative angle profile that result to a significant undercut. InGaP waveguides were transferred to the target substrates successfully and the process that was used can enable heterogeneous integration of InGaP and SOI platform.</p>

Note the "/W" should be "W<up>-1</sup>" to match the original
Also "μ" should be "µ" (micro symbol)
----------------------------------------------------------------------
In diva2:1465554   - correct as is
----------------------------------------------------------------------
In diva2:1811693   - correct as is
----------------------------------------------------------------------
In diva2:1237807 
abstract is: 
<p>Today the European energy grid contains more renewable energy sources than ever before, yet there is little to no research on how the increased amount affects the cost structure of the remaining energy sources of the grid. A consequence of phasing more renewable energy sources into the power grid, is a reduction of the overall balancing capacity of the grid. Thus, the demand for balancing services from the remaining dispatchable energy sources increases. Hydropower is currently frequently used to balance the grid, and thus, the increased demand for balancing services offers a large opportunity for the hydropower segment. Furthermore, as the operating patterns become increasingly aggressive, the structural integrity is reduced, and the maintenance costs are increased. This thesis finds and elucidates the magnitude of the reduced lifetime and increased maintenance costs. This master's thesis finds that the reduction of the structural integrity comes at a large cost, and greatly impacts the overall financial feasibility. In that regard, it is presented market solutions that further incentives balancing services. Balancing services are in markets sold as system services, which includes frequency response, black start capacity, reactive power, and reserve capacity. The thesis presents operating patterns for Francis turbines that seek to fulfill the various system services. The thesis predicts the lifetime of five unique operating patterns, one is assumingly the status quo of operations today, and another is an analogy of operating the turbine like a battery. The results show that low part load and startup are the most damaging operating points, and that the lifetime is lower for flexible operations, than the currently expected lifetime. Despite greatly reducing the lifetime of the turbine, the evaluated cases are financially feasible if they are adequately rewarded. The exact power price that provides adequate rewards differs for all five cases. Financially feasible power prices are in the interval 0.257 to 0.0533 NOK/kWh, where the lowest price refer to current operations and the highest price refers to the aggressive extreme case. The analyses conducted in this thesis are utilizing the numerical software ANSYS mechanical to predict the stress state, the Palmgren-Miner method to predict the lifetime, the rothalpy relationship to predict the pressure in the runner and net present value calculations to evaluate the financial feasibility. In addition, the thesis utilizes, and post-processes previously conducted numerical fluid analyses and pressure measurements from the Waterpower Laboratory at NTNU.</p>

corrected abstract:
<p>Today the European energy grid contains more renewable energy sources than ever before, yet there is little to no research on how the increased amount affects the cost structure of the remaining energy sources of the grid. A consequence of phasing more renewable energy sources into the power grid, is a reduction of the overall balancing capacity of the grid. Thus, the demand for balancing services from the remaining dispatchable energy sources increases. Hydropower is currently frequently used to balance the grid, and thus, the increased demand for balancing services offers a large opportunity for the hydropower segment. Furthermore, as the operating patterns become increasingly aggressive, the structural integrity is reduced, and the maintenance costs are increased. This thesis finds and elucidates the magnitude of the reduced lifetime and increased maintenance costs.</p><p>This master's thesis finds that the reduction of the structural integrity comes at a large cost, and greatly impacts the overall financial feasibility. In that regard, it is presented market solutions that further incentives balancing services. Balancing services are in markets sold as system services, which includes frequency response, black start capacity, reactive power, and reserve capacity. The thesis presents operating patterns for Francis turbines that seek to fulfill the various system services.</p><p>The thesis predicts the lifetime of five unique operating patterns, one is assumingly the status quo of operations today, and another is an analogy of operating the turbine like a battery. The results show that low part load and startup are the most damaging operating points, and that the lifetime is lower for flexible operations, than the currently expected lifetime.</p><p>Despite greatly reducing the lifetime of the turbine, the evaluated cases are financially feasible if they are adequately rewarded. The exact power price that provides adequate rewards differs for all five cases. Financially feasible power prices are in the interval 0.257 to 0.0533 NOK/kWh, where the lowest price refer to current operations and the highest price refers to the aggressive extreme case.</p><p>The analyses conducted in this thesis are utilizing the numerical software ANSYS mechanical to predict the stress state, the Palmgren-Miner method to predict the lifetime, the rothalpy relationship to predict the pressure in the runner and net present value calculations to evaluate the financial feasibility. In addition, the thesis utilizes, and post-processes previously conducted numerical fluid analyses and pressure measurements from the Waterpower Laboratory at NTNU.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1878500 
abstract is: 
<p>Electric vehicles aim to carry the torch into a sustainable future. An optimized cooling system is crucial to an Electric Drive Unit (EDU). A smartly designed cooling system will deliver high-performance, efficient and long-lasting EDUs at lower costs. One way to achieve that is to have an integrated cooling system. When the electric motor and transmission share a common oil, the oil returning from the transmission side is aerated due to spraying and splashing. This aeration affects the pump performance and may reduce the cooling performance of the oil. Thus, this thesis is initiated to understand the impact of aeration on heat transfer. </p><p>   Oil aeration is the presence of air in oil. This aeration depends on the air content and bubble sizes mixed in the oil. Typically, there is also some amount of dissolved air in any oil. Depending on the type of aeration, the oil will appear lighter than its usual colour and have a very foamy texture, showing a change in the properties of the fluid, for example density, viscosity and heat transfer.</p><p>   An experimental setup is built in order to replicate and study the effect of aeration on local heat transfer. A flat channel with rectangular cross-section is designed with three parts – a bottom plate, a flow spacer channel and a top transparent plate. The oil and air are mixed before they enter the channel and then heated using thin film heaters. A groove within the bottom plate houses an insulating material, the thin film heater, a thermocouple touching the heater and a thermochromic liquid crystal sheet facing the fluid mixture. The thermocouple gives temperature readings from a single point between the heater and the insulating material. Meanwhile, the liquid crystal sheets come in different desired temperature ranges and change in colour from red to blue to show the surface temperatures over an area. So, the surface temperature of the mixed fluid flow can be recorded visually over an area with the thin film heater under it to calculate the heat transfer coefficients accordingly.</p><p>   The drop in Nusselt number and heat transfer rates with increased aeration in the working fluid is the main highlight and result. The size of the air bubbles in the channel also determine how fast the heat transfer rate drops.</p>

corrected abstract:
<p>Electric vehicles aim to carry the torch into a sustainable future. An optimized cooling system is crucial to an Electric Drive Unit (EDU). A smartly designed cooling system will deliver high-performance, efficient and long-lasting EDUs at lower costs. One way to achieve that is to have an integrated cooling system. When the electric motor and transmission share a common oil, the oil returning from the transmission side is aerated due to spraying and splashing. This aeration affects the pump performance and may reduce the cooling performance of the oil. Thus, this thesis is initiated to understand the impact of aeration on heat transfer.</p><p>Oil aeration is the presence of air in oil. This aeration depends on the air content and bubble sizes mixed in the oil. Typically, there is also some amount of dissolved air in any oil. Depending on the type of aeration, the oil will appear lighter than its usual colour and have a very foamy texture, showing a change in the properties of the fluid, for example density, viscosity and heat transfer.</p><p>An experimental setup is built in order to replicate and study the effect of aeration on local heat transfer. A flat channel with rectangular cross-section is designed with three parts – a bottom plate, a flow spacer channel and a top transparent plate. The oil and air are mixed before they enter the channel and then heated using thin film heaters. A groove within the bottom plate houses an insulating material, the thin film heater, a thermocouple touching the heater and a thermochromic liquid crystal sheet facing the fluid mixture. The thermocouple gives temperature readings from a single point between the heater and the insulating material. Meanwhile, the liquid crystal sheets come in different desired temperature ranges and change in colour from red to blue to show the surface temperatures over an area. So, the surface temperature of the mixed fluid flow can be recorded visually over an area with the thin film heater under it to calculate the heat transfer coefficients accordingly.</p><p>The drop in Nusselt number and heat transfer rates with increased aeration in the working fluid is the main highlight and result. The size of the air bubbles in the channel also determine how fast the heat transfer rate drops.</p>

Note eliminated unnecessary space at the end or start of paragraphs
----------------------------------------------------------------------
In diva2:651663 
abstract is: 
<p>Nuclear waste is a highly debated issue of nuclear power. The long-term storage is not considered to be sustainable, thus other possibilities are sought after. One such possibility could be to recycle the nuclear waste in nuclear reactors, such as fast breeder reactors. The possibility to recycle americium, which is a nuclear waste, is the reason of this study.</p><p>This report studies how the amount of americium in the fuel of the reactor ELECTRA affects the temperature coefficient. It was also examined how the size of the reactor affects the reactivity factor, k</p><p>eff. The purpose was to see how much americium that could be added to the fuel before the temperature coefficient becomes positive, and the reactor becomes unstable.</p><p>The results first obtained in the study were not as expected. The expected result was that with an increased amount of americium the temperature coefficient would also increase. Instead the value of the temperature coefficient remained at an almost constant level. This is assumed to be because the reactor is designed to have a high amount of neutron leakage, which dominates the impact on the temperature coefficient.</p><p>To decrease the impact of the neutron leakage and to maintain criticality, the height of the reactor core was increased. This lead to the expected results of an increased temperature coefficient with an increased amount of americium in the fuel. The study also shows that more fuel is needed to maintain criticality when more americium is added.</p>

corrected abstract:
<p>Nuclear waste is a highly debated issue of nuclear power. The long-term storage is not considered to be sustainable, thus other possibilities are sought after. One such possibility could be to recycle the nuclear waste in nuclear reactors, such as fast breeder reactors. The possibility to recycle americium, which is a nuclear waste, is the reason of this study.</p><p>This report studies how the amount of americium in the fuel of the reactor ELECTRA affects the temperature coefficient. It was also examined how the size of the reactor affects the reactivity factor, k<sub>eff</sub>. The purpose was to see how much americium that could be added to the fuel before the temperature coefficient becomes positive, and the reactor becomes unstable.</p><p>The results first obtained in the study were not as expected. The expected result was that with an increased amount of americium the temperature coefficient would also increase. Instead the value of the temperature coefficient remained at an almost constant level. This is assumed to be because the reactor is designed to have a high amount of neutron leakage, which dominates the impact on the temperature coefficient.</p><p>To decrease the impact of the neutron leakage and to maintain criticality, the height of the reactor core was increased. This lead to the expected results of an increased temperature coefficient with an increased amount of americium in the fuel. The study also shows that more fuel is needed to maintain criticality when more americium is added.</p>

Note fixed subscript
----------------------------------------------------------------------
In diva2:1699816   - correct as is
----------------------------------------------------------------------
In diva2:1800245 
abstract is: 
<p>Irradiation of a reactor pressure vessel (RPV) causes a shift of the ductile to brittle transition region towards higher temperature regions. In the event of a pressurized thermal shock (PTS), where the temperature drops drastically, the ductile to brittle transition region might be entered for irradiated ferritic steel. Hence, there is a risk of brittle cleavage fracture. Cleavage fracture is a transgranular unstable fracture initiated by cracked second phase particles and rapidly propagated over grain boundaries.</p><p>The warm pre-stressing (WPS) effect can be helpful as it increases the apparent fracture toughness of ferritic steel pre-loaded in the ductile temperature region, which is the case for a PTS. This effect has been proven effective for virgin material, but the impact of residual stress fields on the WPS effect have not been investigated thoroughly. Utilizing a finite element model of notched three-point bending specimens and a non-local probabilistic model for fracture prediction the effect of residual stresses on the WPS effect was investigated in this thesis.</p><p>Regarding the crack tip state, expressed as J, the probability of fracture was alike for both material with and without residual stresses, however a significant loss of load bearing capacity was found comparing them two. The magnitude of this loss depends on pre-load level as well as specimen size. This loss however, was also found when not considering the WPS effect.</p>

corrected abstract:
<p>Irradiation of a Reactor Pressure Vessel (RPV) causes a shift of the ductile to brittle transition region towards higher temperature regions. In the event of a Pressurized Thermal Shock (PTS), where the temperature drops drastically, the ductile to brittle transition region might be entered for irradiated ferritic steel. Hence, there is a risk of brittle cleavage fracture. Cleavage fracture is a transgranular unstable fracture initiated by cracked second phase particles and rapidly propagated over grain boundaries.</p><p>The warm pre-stressing (WPS) effect can be helpful as it increases the apparent fracture toughness of ferritic steel pre-loaded in the ductile temperature region, which is the case for a PTS. This effect has been proven effective for virgin material, but the impact of residual stress fields on the WPS effect have not been investigated thoroughly. Utilizing a finite element model of notched three-point bending specimens and a non-local probabilistic model for fracture prediction the effect of residual stresses on the WPS effect was investigated in this thesis.</p><p>Regarding the crack tip state, expressed as J, the probability of fracture was alike for both material with and without residual stresses, however a significant loss of load bearing capacity was found comparing them two. The magnitude of this loss depends on pre-load level as well as specimen size. This loss however, was also found when not considering the WPS effect.</p>

Note correct capitalication to match original
----------------------------------------------------------------------
In diva2:1288441 
abstract is: 
<p>This thesis investigated the impact performance of CFRP products within the sports industry. The primary aim of this thesis was to evaluate different configurations, matrix system, and technologies to find the best performing solutions for impact. During this work, an extensive literature study was conducted and various solutions were reviewed. Further on, several tubes were manufactured, impacted and put through a 2 point bending test to find out the residual strength. It was found that TeXtreme R fabrics positively affected the impact performance when compared to conventional fabrics and UD depending on the placement location. Thin plies proved to be better than conventional plies. Newer technologies such as CNT stitching requires further investigation before it can be qualitatively assessed.</p>

corrected abstract:
<p>This thesis investigated the impact performance of CFRP products within the sports industry. The primary aim of this thesis was to evaluate different configurations, matrix system, and technologies to find the best performing solutions for impact.</p><p>During this work, an extensive literature study was conducted and various solutions were reviewed. Further on, several tubes were manufactured, impacted and put through a 2 point bending test to find out the residual strength.</p><p>It was found that TeXtreme® fabrics positively affected the impact performance when compared to conventional fabrics and UD depending on the placement location. Thin plies proved to be better than conventional plies. Newer technologies such as CNT stitching requires further investigation before it can be qualitatively assessed.</p>

Note added "®" and added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1816873   - correct as is
----------------------------------------------------------------------
In diva2:1570253 
abstract is: 
<p>As this project covers an implementation and optimisation of a neural network, it covers most of the essentials within the field of neural networks and its place within machine learning. The focus, however, lay on studying how stepping schedules affect learning. A stepping schedule is a predetermined way to decrease the learning rate over the training process. It turns out that for the networks implemented, stepping schedules do not seem to make a significant difference in performance, although slight improvements from constant learning rates could be found in some cases. Even when different depths and widths of the neural network were studied, a stepping schedule did not seem to have much of an impact. This result is somewhat surprising, as we in theory would expect to see a rise in performance when the learning rate is gradually decreased over learning. We can conclude that other correctly chosen parameters play a much more significant role in facilitating an effective learning process. The code of the implementation is found through the following link https://github.com/axeboii/Neural-Network.</p>


corrected abstract:
<p>As this project covers an implementation and optimisation of a neural network, it covers most of the essentials within the field of neural networks and its place within machine learning. The focus, however, lay on studying how stepping schedules affect learning. A stepping schedule is a predetermined way to decrease the learning rate over the training process. It turns out that for the networks implemented, stepping schedules do <em>not</em> seem to make a significant difference in performance, although slight improvements from constant learning rates could be found in some cases. Even when different depths and widths of the neural network were studied, a stepping schedule did not seem to have much of an impact. This result is somewhat surprising, as we in theory would expect to see a rise in performance when the learning rate is gradually decreased over learning. We can conclude that other correctly chosen parameters play a much more significant role in facilitating an effective learning process. The code of the implementation is found through the following link <a href="https://github.com/axeboii/Neural-Network">https://github.com/axeboii/Neural-Network</a>.</p>

Note added italics and link
----------------------------------------------------------------------
In diva2:1114359 - missing spaces in title:
"Implementation and Validation of Algorithm forEstimating the Possible Power of Curtailed Wind Turbines Exposed to WakeEffects"
==>
"Implementation and Validation of Algorithm for Estimating the Possible Power of Curtailed Wind Turbines Exposed to Wake Effects"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1450298   - correct as is
----------------------------------------------------------------------
In diva2:1057181   - correct as is
----------------------------------------------------------------------
In diva2:555813 
abstract is: 
<p>Abstract</p><p>In recent years we have witnessed how distress can spread quickly through the financial system and threaten financial stability. Hence there has been increased focus on developing systemic risk indicators that can be used by central banks and others as a monitoring tool. For Sveriges Riksbank it is of great value to be able to quantify the risks that can threaten the Swedish financial system CoVaR is a systemic risk measure implemented here with that with that purpose. CoVaR, which stands for conditional Value at Risk, measures a financial institutions contribution to systemic risk and its contribution to the risk of other financial institutions. The conclusion is that CoVaR can together with other systemic risk indicators help get a better understanding of the risks threatening the stability of the Swedish financial system.</p>

corrected abstract:
<p>In recent years we have witnessed how distress can spread quickly through the financial system and threaten financial stability. Hence there has been increased focus on developing systemic risk indicators that can be used by central banks and others as a monitoring tool. For Sveriges Riksbank it is of great value to be able to quantify the risks that can threaten the Swedish financial system. CoVaR is a systemic risk measure implemented here with that with that purpose. CoVaR, which stands for conditional Value at Risk, measures a financial institutions contribution to systemic risk and its contribution to the risk of other financial institutions. The conclusion is that CoVaR can together with other systemic risk indicators help get a better understanding of the risks threatening the stability of the Swedish financial system.</p>

Note removed "<p>Abstract</p>" and added a period after "system"
----------------------------------------------------------------------
In diva2:1167206 
abstract is: 
<p>This thesis extended the order of the reconstruction of state for convective fluxes used by Finite Volume (FV) algorithm in DLR’s next-generation CFD solver: Flucs, from constant and linear to quadratic and cubic. Two approaches for calculating derivatives were implemented in Flucs and some test cases were tried. To allow for integration of moments within each cell and a higher-order integration of fluxes, the mesh used by Discontinuous Galerkin (DG) was fed to the FV algorithm. Insufficient geometric treatment of the boundary cells and the dummy cells in FV is believed to be detrimental to the order of error reduction in NACA0012 case and the smooth bump case. In the smooth bump case, the FV algorithms failed to show higher than second order error reduction because of this reason. The order of the schemes away from the boundaries was verified with the Ehrenfried Vortex test case. For at least structured meshes and unstructured meshes with quads, schemes of order k approached k + 1 order accuracy on sufficiently fine meshes. The original goal of this thesis was partly accomplished and some further work in the code is expected.</p>

corrected abstract:
<p>This thesis extended the order of the reconstruction of state for convective fluxes used by Finite Volume (FV) algorithm in DLR’s next-generation CFD solver: Flucs, from constant and linear to quadratic and cubic. Two approaches for calculating derivatives were implemented in Flucs and some test cases were tried. To allow for integration of moments within each cell and a higher-order integration of fluxes, the mesh used by Discontinuous Galerkin (DG) was fed to the FV algorithm. Insufficient geometric treatment of the boundary cells and the dummy cells in FV is believed to be detrimental to the order of error reduction in NACA0012 case and the smooth bump case. In the smooth bump case, the FV algorithms failed to show higher than second order error reduction because of this reason. The order of the schemes away from the boundaries was verified with the Ehrenfried Vortex test case. For at least structured meshes and unstructured meshes with quads, schemes of order 𝑘 approached 𝑘 + 1 order accuracy on sufficiently fine meshes. The original goal of this thesis was partly accomplished and some further work in the code is expected.</p>

Note replace "k" with "𝑘"
----------------------------------------------------------------------
In diva2:1740133 
abstract is: 
<p>The main objective of this thesis project is to explore the possibility of using machine learning as a way of categorizing flight simulation outcomes. If such a possibility exists, the process of analyzing whether a new configuration stays within given boundaries can be streamlined. At least provide a first indication of the behaviour of the aircraft purely based on previous verified knowledge.</p><p>A large quantity of flight simulations are performed to create known data for training the machine learning models. Focusing on the boundaries for angle of attack (α) vs side slip angle (β), the machine learning models are trained on the parameters mach number, altitude, external load configuration and maneuver and classified based on the α and β output received from the simulation. A total of 6 machine learning algorithms have been implemented and compared to each other in terms of performance. The machine learning models are developed on an insensitive aircraftsimulation model and then tested on a simulation model based on a real aircraft. Different ways of improving the performance of the machine learning models are explored.</p><p>The results indicates that machine learning have potential when it comes to analyzing flight simulation data. In terms of total accuracy and precision there are models which perform incredibly well on the vast majority of the data. However, it is often the outliers and rare cases, which are of interest during analysis of simulation data, where the approach taken in this project lacks in performance.</p>

mc='aircraftsimulation' c='aircraft simulation'

corrected abstract:
<p>The main objective of this thesis project is to explore the possibility of using machine learning as a way of categorizing flight simulation outcomes. If such a possibility exists, the process of analyzing whether a new configuration stays within given boundaries can be streamlined. At least provide a first indication of the behaviour of the aircraft purely based on previous verified knowledge.</p><p>A large quantity of flight simulations are performed to create known data for training the machine learning models. Focusing on the boundaries for angle of attack (&#x1D6FC;) vs side slip angle (β), the machine learning models are trained on the parameters mach number, altitude, external load configuration and maneuver and classified based on the &#x1D6FC; and β output received from the simulation. A total of 6 machine learning algorithms have been implemented and compared to each other in terms of performance. The machine learning models are developed on an insensitive aircraft simulation model and then tested on a simulation model based on a real aircraft. Different ways of improving the performance of the machine learning models are explored.</p><p>The results indicates that machine learning have potential when it comes to analyzing flight simulation data. In terms of total accuracy and precision there are models which perform incredibly well on the vast majority of the data. However, it is often the outliers and rare cases, which are of interest during analysis of simulation data, where the approach taken in this project lacks in performance.</p>

Note replaced "α" by &#x1D6FC;  and replaced "β" by &#x1D6FD; - these are the Mathematical Italic Small Alpha and Beta as opposed to the Greek letters
----------------------------------------------------------------------
In diva2:950752   - correct as is
----------------------------------------------------------------------
In diva2:1465545 
abstract is: 
<p>Internal combustion engines, electric motors and batteries generate a significant amount of heat during operation that needs to be extracted by cooling systems. A cooling system is designed and installed to extract the generated heat and maintain the system temperature in an optimal range. Overheating has several unfavorable outcomes such as less durability and lower energy efficiency. The cooling system consists of several components such as hoses, flow splitters, valves, heat exchangers, coolant, pump, etc. The coolant, as the working fluid, is pumped to different heat generator component to enable the cooling down process. Computational Fluid Dynamics (CFD) is a powerful and cost efficient tool to simulate the cooling processes, design, and evaluate the performance of a cooling system. Generally, one dimensional CFD is a common approach to interpret and explain the cooling processes in the automotive industry due to its high flexibility and computational cost efficiency. Also, three dimensional CFD is employed whenever it is required to study complex physical phenomena and provide detailed information. Additionally, it is possible to couple one dimensional and three dimensional CFD approaches to simulate cooling processes. Not only is the coupled 1D-3D CFD approach able to capture complicated physical processes but also is flexible and cost efficient. The objective of this master thesis is to implement 1D-3D CFD coupled simulation on internal combustion engines’ cooling system and evaluate the advantages and disadvantages of this method. The performance of this method is examined in different case studies with different flow and geometrical characteristics. The effect of various turbulence models and numerical settings are investigated on the quality of the coupled simulations’ results. The coupled simulations are carried out using GT-SUITE and STAR-CCM+ software. The performed simulations show that the coupling method is a convenient approach which is able to capture detailed physics with high precision requiring reasonable computational costs. The results of the coupled simulations depict agreement with the uncoupled 1D CFD simulations, although some discrepancies are observed in complex case studies. Also, it is shown that the coupled simulations are sensitive to numerical settings and physical models, consequently, the case setup should be optimized carefully.</p>
skipping mc='isa'

partal corrected: diva2:1465545: <p>Internal combustion engines, electric motors and batteries generate a significant amount of heat during operation that needs to be extracted by cooling systems. A cooling system is designed and installed to extract the generated heat and maintain the system temperature in an optimal range. Overheating has several unfavorable outcomes such as less durability and lower energy efficiency. The cooling system consists of several components such as hoses, flow splitters, valves, heat exchangers, coolant, pump, etc. The coolant, as the working fluid, is pumped to different heat generator component to enable the cooling down process. Computational Fluid Dynamics (CFD) is a powerful and cost efficient tool to simulate the cooling processes, design, and evaluate the performance of a cooling system. Generally, one dimensional CFD is a common approach to interpret and explain the cooling processes in the automotive industry due to its high flexibility and computational cost efficiency. Also, three dimensional CFD is employed whenever it is required to study complex physical phenomena and provide detailed information. Additionally, it is possible to couple one dimensional and three dimensional CFD approaches to simulate cooling processes. Not only is the coupled 1D-3D CFD approach able to capture complicated physical processes but also is flexible and cost efficient. The objective of this master thesis is to implement 1D-3D CFD coupled simulation on internal combustion engines’ cooling system and evaluate the advantages and disadvantages of this method. The performance of this method is examined in different case studies with different flow and geometrical characteristics. The effect of various turbulence models and numerical settings are investigated on the quality of the coupled simulations’ results. The coupled simulations are carried out using GT-SUITE and STAR-CCM+ software. The performed simulations show that the coupling method is a convenient approach which is able to capture detailed physics with high precision requiring reasonable computational costs. The results of the coupled simulations depict agreement with the uncoupled 1D CFD simulations, although some discrepancies are observed in complex case studies. Also, it is shown that the coupled simulations are sensitive to numerical settings and physical models, consequently, the case setup should be optimized carefully.</p>

corrected abstract:
<p>Internal combustion engines, electric motors and batteries generate a significant amount of heat during operation that needs to be extracted by cooling systems. A cooling system is designed and installed to extract the generated heat and maintain the system temperature in an optimal range. Overheating has several unfavorable outcomes such as less durability and lower energy efficiency. The cooling system consists of several components such as hoses, flow splitters, valves, heat exchangers, coolant, pump etc. The coolant, as the working fluid, is pumped to different heat generator component to enable the cooling down process.</p><p>Computational Fluid Dynamics (CFD) is a powerful and cost efficient tool to simulate the cooling processes, design and evaluate the performance of a cooling system. Generally, one dimensional CFD is a common approach to interpret and explain the cooling processes in the automotive industry due to its high flexibility and computational cost efficiency. Also, three dimensional CFD is employed whenever it is required to study complex physical phenomena and provide detailed information. Additionally, it is possible to couple one dimensional and three dimensional CFD approaches to simulate cooling processes. Not only is the coupled 1D-3D CFD approach able to capture complicated physical processes but also is flexible and cost efficient.</p><p>The objective of this master thesis is to implement 1D-3D CFD coupled simulation on internal combustion engines’ cooling system and evaluate the advantages and disadvantages of this method. The performance of this method is examined in different case studies with different flow and geometrical characteristics. The effect of various turbulence models and numerical settings are investigated on the quality of the coupled simulations’ results. The coupled simulations are carried out using GT-SUITE and STAR-CCM+ software.</p><p>The performed simulations show that the coupling method is a convenient approach which is able to capture detailed physics with high precision requiring reasonable computational costs. The results of the coupled simulations depict agreement with the uncoupled 1D CFD simulations, although some discrepancies are observed in complex case studies. Also, it is shown that the coupled simulations are sensitive to numerical settings and physical models, consequently, the case setup should be optimized carefully.</p>

Note removed some commas that are not in the original and added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1877614   - correct as is
----------------------------------------------------------------------
In diva2:1033615   - correct as is
----------------------------------------------------------------------
In diva2:1478674 
abstract is: 
<p>The Integrated Vehicle Dynamics Control (IVDC) concept can influence the vehicle behaviour both longitudinally and laterally with just one upper level control concept and further lower level controllers. This demands for state estimation of the vehicle which also includes estimating parameters of interest for the vehicle dynamicist. The approach to this research is firstly in developing a robust unscented Kalman filter (UKF) estimator for the vehicle side slip tracking and also for cornering stiffness estimation which is then fed to the existing model predictive control allocation (MPCA) controller to enhance the lateral stability of the vehicle for the different manoeuvres studied. Based on these developments, two types of filters are created. One with adaption of distance between center of gravity (COG) and roll center height and another without adaption. The key factor in the estimator development is the time adaptive process covariance matrix for the cornering stiffnesses, with which only the initial values have to be parameterised. Combining this research encompasses effective and adaptive method for a better quality of estimation with a kinematic vehicle model which behaves like a real world vehicle, at least virtually.This study is carried out with the understanding of various optimal estimators, parametric sensitivity analysis and statistical inferences, facilitating a base for robust estimation. Keywords: kalametric, state estimation, design matrix, aliasing, kalman filter, projection algorithm, resolution</p>

mc='virtually.This' c='virtually. This'
mc='.This' c='. This'

corrected abstract:
<p>The Integrated Vehicle Dynamics Control (IVDC) concept can influence the vehicle behaviour both longitudinally and laterally with just one upper level control concept and further lower level controllers. This demands for state estimation of the vehicle which also includes estimating parameters of interest for the vehicle dynamicist.</p><p>The approach to this research is firstly in developing a robust unscented Kalman filter (UKF) estimator for the vehicle side slip tracking and also for cornering stiffness estimation which is then fed to the existing model predictive control allocation (MPCA) controller to enhance the lateral stability of the vehicle for the different manoeuvres studied. Based on these developments, two types of filters are created. One with adaption of distance between center of gravity (COG) and roll center height and another without adaption. The key factor in the estimator development is the time adaptive process covariance matrix for the cornering stiffnesses, with which only the initial values have to be parameterised.</p><p>Post the filter development, the parameters are identified based on the real-time vehicle usage factors like trailer towing, tyre pressure, etc. A statistical analysis of variance (ANOVA) is performed to know the influential factors amongst the group. A parametric optimisation is performed to improve the estimation quality.</p><p>Combining this research encompasses effective and adaptive method for a better quality of estimation with a kinematic vehicle model which behaves like a real world vehicle, at least virtually. This study is carried out with the understanding of various optimal estimators, parametric sensitivity analysis and statistical inferences, facilitating a base for robust estimation.</p>

Note added missing text and added missing paragraph breaks
----------------------------------------------------------------------
In diva2:491859 
abstract is: 
<p>The ACM 2 spot weld model is the currently most widely used spot weld model in industry. It is designed to be tuned using parameters to represent the stiffness characteristics of a spot weld in a suitable way. The choice of these parameters is however often associated with uncertainties. Therefore, the set of parameters with best performance is determined by employing an updating algorithm. Further, the result is compared to a new category of models, the so called Spider models, which intend to overcome mesh dependence in the spot weld area by re-meshing the structures locally. The results are formulated as guidelines for the optimum implementation of these spot weld models for structural dynamics. The studies are performed using two cut-out parts from a Volvo FH truck cabin. In addition, a new analytical approach for updating material data of isotropic shell structures is derived and the equations are verified using the same test pieces.</p>

corrected abstract:
<p>The ACM 2 spot weld model is the currently most widely used spot weld model in industry. It is designed to be tuned using parameters to represent the stiffness characteristics of a spot weld in a suitable way. The choice of these parameters is however often associated with uncertainties. Therefore, the set of parameters with best performance is determined by employing an updating algorithm. Further, the result is compared to a new category of models, the so called Spider models, which intend to overcome mesh dependence in the spot weld area by re-meshing the structures locally. The results are formulated as guidelines for the optimum implementation of these spot weld models for structural dynamics. The studies are performed using two cut-out parts from a Volvo FH truck cabin.</p><p>In addition, a new analytical approach for updating material data of isotropic shell structures is derived and the equations are verified using the same test pieces.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1499343 
abstract is: 
<p>A conceptual design of an innovative two-axle lightweight railway vehicle for commuter services is carried out at KTH Railway Group. An active wheelset steering is introduced to improve the curving performance of the vehicle, which is one of the critical performance requirements. This thesis aims to improve the steering performance of the active wheelset steering. Look-up tables for estimating time-varying wheel-rail contact parameters are introduced to supervise a simple PID controller of the active steering system in order to improve steering performance. The look-up table (LUT) estimation is focused on time-varying wheel-rail contact parameters, including creep coefficients and contact patch variables due to their direct influence on curving performance and lateral stability of the wheelset. As a result, the estimated longitudinal unit creep forces (UCF) have the potential to supervise the gains determination of PID controller because it can appropriately distinguish running conditions. The estimation of longitudinal UCF is achieved by the combination of the results from the LUT of creep coefficients and the LUT of contact patch variables. The result from longitudinal unit creep force estimation is shifted to the first quadrant to use as critical gain in the Ziegler-Nichols tuning method for the PID controller. The critical oscillation period for PID tuning can be expressed as a function of vehicle speed. Consequently, the PID controller for the active steering system uses time-varying gains with real-time tuning. The proposed control system for active wheelset steering is validated with nine running conditions using SIMPACK and MATLAB/Simulink co-simulation. The proposed control system provides a stable wheelset lateral displacement control regardless of the running condition. The active steering system significantly reduces wheel-rail wear, which demonstrates the effectiveness of the proposed active steering system.</p>

corrected abstract:
<p>A conceptual design of an innovative two-axle lightweight railway vehicle for commuter services is carried out at KTH Railway Group. An active wheelset steering is introduced to improve the curving performance of the vehicle, which is one of the critical performance requirements. This thesis aims to improve the steering performance of the active wheelset steering. Look-up tables for estimating time-varying wheel-rail contact parameters are introduced to supervise a simple PID controller of the active steering system in order to improve steering performance.</p><p>The look-up table (LUT) estimation is focused on time-varying wheel-rail contact parameters, including creep coefficients and contact patch variables due to their direct influence on curving performance and lateral stability of the wheelset. As a result, the estimated longitudinal unit creep forces (UCF) have the potential to supervise the gains determination of PID controller because it can appropriately distinguish running conditions. The estimation of longitudinal UCF is achieved by the combination of the results from the LUT of creep coefficients and the LUT of contact patch variables. The result from longitudinal unit creep force estimation is shifted to the first quadrant to use as critical gain in the Ziegler-Nichols tuning method for the PID controller. The critical oscillation period for PID tuning can be expressed as a function of vehicle speed. Consequently, the PID controller for the active steering system uses time-varying gains with real-time tuning.</p><p>The proposed control system for active wheelset steering is validated with nine running conditions using SIMPACK and MATLAB/Simulink co-simulation. The proposed control system provides a stable wheelset lateral displacement control regardless of the running condition. The active steering system significantly reduces wheel-rail wear, which demonstrates the effectiveness of the proposed active steering system.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1698400   - correct as is
----------------------------------------------------------------------
In diva2:1766559   - correct as is
----------------------------------------------------------------------
In diva2:1865478 
abstract is: 
<p>Tau neutrinos are one of the least understood elementary particles, as they are difficult to detect. One of the few detectors that can detect them is the IceCube neutrino observatory, located on the south pole. When tau neutrinos interact with a nucleus in the ice at IceCube, a tau lepton can form, which decays almost instantaneously. The decay products of the tau lepton will then emit light, which can be detected at IceCube. To gain a better understanding of tau neutrinos, it is therefore crucial to accurately simulate tau lepton decays.</p><p>In this thesis, the tau lepton decays within the IceCube simulation framework are evaluated by comparing the simulations to TAUOLA, a high-precision tau decay simulation library, Through this work, several issues with the current simulations were discovered. Specifically, the polarization of the tau lepton is neglected, hadronic resonances are not implemented for tau decays, and several decay modes are missing. These issues result in an overestimate of the energy of the visible decay products, which means that the light emitted from the tau decay is dimmer in reality than what is predicted by the current IceCube simulations. To address this, TAUOLA was integrated into the IceCube simulations, thereby enabling high-precision tau decay simulations for future studies.</p>

corrected abstract:
<p>The tau neutrino is one of the least understood elementary particles, as it is difficult to detect. One of the few detectors that can detect it is the IceCube neutrino observatory, located on the south pole. When a tau neutrino interacts with an atomic nucleus in the ice at IceCube, a tau lepton can form, which then decays almost instantaneously. The decay products of the tau lepton will then emit light, which can be detected at IceCube. To gain a better understanding of tau neutrinos, it is therefore crucial to accurately simulate tau lepton decays.</p><p>In this thesis, simulated tau decays in the IceCube simulation framework are evaluated and compared to tau  decay simulations in <span style="font-variant: small-caps;">Tauola</span>, a high-precision tau decay simulation library. Through this work, several issues with the current simulations were discovered. Specifically, the polarization of the tau lepton is neglected, hadronic resonances are not implemented for tau decays, and several decay modes are missing. These issues result in an overestimate of the energy of the visible decay products, which means that the light emitted from the tau decay is dimmer in reality than what is predicted by the current IceCube simulations. To address this, <span style="font-variant: small-caps;">Tauola</span> was integrated into the IceCube simulations, thereby enabling high-precision tau decay simulations for future studies.</p>

Note changed text to match the original
----------------------------------------------------------------------
In diva2:1269703   - correct as is
----------------------------------------------------------------------
In diva2:1816845 
abstract is: 
<p>This thesis focuses on the study of water transportation on vehicle surfaces, which is crucial for ensuring the unobstructed operation of sensors and cameras in autonomous vehicles. The research aims to develop and validate experimental and simulation methods to enhance the understanding of water droplet behaviour and to create accurate models for computational fluid dynamics (CFD) simulations. The primary objective is to investigate the feasibility of simulating water droplets using CFD. The study examines the behaviour of water droplets on a lacquered metal sheet and a glass surface. Physical experiments and CFD simulations are conducted to analyse droplet movement under the influence of gravity and airflow. The findings provide insights into the factors influencing droplet behaviour and validate the accuracy of the simulation models through physical tests. The research also discusses the limitations of the study and the implications for Volvo Cars, aiming to improve their ability to predict water droplet movement on their vehicles.</p>

corrected abstract:
<p>This thesis focuses on the study of water transportation on vehicle surfaces, which is crucial for ensuring the unobstructed operation of sensors and cameras in autonomous vehicles. The research aims to develop and validate experimental and simulation methods to enhance the understanding of water droplet behaviour and to create accurate models for computational fluid dynamics (CFD) simulations. The primary objective is to investigate the feasibility of simulating water droplets using CFD. The study examines the behaviour of water droplets on a lacquered metal sheet and a glass surface. Physical experiments and CFD simulations are conducted to analyse droplet movement under the influence of gravity and airflow. The findings provide insights into the factors influencing droplet behaviour and validate the accuracy of the simulation models through physical tests. The research also discusses the limitations of the study and the implications for Volvo Cars, aiming to improve their ability to predict water droplet movement on their vehicles.</p><p>The results indicate that water droplet behaviour can be accurately studied and simulated using a combination of experimental and CFD approaches. The findings of this study provide a good foundation for Volvo cars to reach their ultimate goal of using simulations to study water transportation exclusively.</p>

Note added missing text.
----------------------------------------------------------------------
In diva2:756318 - missing italics in title:
"In silico deconvolution of small cancer cell ratios using transcriptomics gene signatures"
==>
"<em>In silico</em> deconvolution of small cancer cell ratios using transcriptomics gene signatures"

abstract is: 
<p>In this work we suggest a deconvolution method, based on a convex optimization problem, to calculate the cancer amount from heterogeneous cell type gene expression profiles generated <em>in silico</em>.</p><p>Expression profiling is a technique for identifying global expression patterns within cellular groups, its multiple purposes may include the identification of disease biomarkers and the basic understanding of cellular processes. Given the necessity for understanding complex biological processes such as development and carcinogenesis, it is of main importance to distinguish between contributions to gene expression profiles from either regulation processes or abundance of cellular groups. Unfortunately, many biological samples contain mixtures of cell-types. This severely limits the conclusions that can be made about the specificity of gene expression in the cell-type of interest.</p><p>We describe a model to estimate the proportions of cell types in a given test data set based on a gene expression profile derived from transcriptomics. Our model is based on least squares estimation and the solution of a convex optimization problem. The technical aim is to solve an undetermined system of linear equations, which must satisfy several constraints and under a particular sparsity assumption.</p><p>Cell type mixtures were simulated <em>in silico </em>using a special procedure based on mean and standard deviations. Variable selection was performed by Analysis of Variance (ANOVA) using “cell type” as main factor and genes were ranked by F-statistics. We tested our model in breast and liver tissues, employing four cell types (three normal and one cancerous). We also performed a bootstrap procedure to test the robustness of our method concluding that our method is stable and accurate enough to estimate cancer portions of at least 10%.</p>

corrected abstract:
<p>In this work we suggest a deconvolution method, based on a convex optimization problem, to calculate the cancer amount from heterogeneous cell type gene expression profiles generated <em>in silico</em>.</p><p>Expression profiling is a technique for identifying global expression patterns within cellular groups, its multiple purposes may include the identification of disease biomarkers and the basic understanding of cellular processes. Given the necessity for understanding complex biological processes such as development and carcinogenesis, it is of main importance to distinguish between contributions to gene expression profiles from either regulation processes or abundance of cellular groups. Unfortunately, many biological samples contain mixtures of cell-types. This severely limits the conclusions that can be made about the specificity of gene expression in the cell-type of interest.</p><p>We describe a model to estimate the proportions of cell types in a given test data set based on a gene expression profile derived from transcriptomics. Our model is based on least squares estimation and the solution of a convex optimization problem. The technical aim is to solve an undetermined system of linear equations, which must satisfy several constraints and under a particular sparsity assumption.</p><p>Cell type mixtures were simulated <em>in silico</em> using a special procedure based on mean and standard deviations. Variable selection was performed by Analysis of Variance (ANOVA) using “cell type” as main factor and genes were ranked by F-statistics. We tested our model in breast and liver tissues, employing four cell types (three normal and one cancerous). We also performed a bootstrap procedure to test the robustness of our method concluding that our method is stable and accurate enough to estimate cancer portions of at least 10%.</p>

Note fixed italics
----------------------------------------------------------------------
In diva2:725042 - missing space in title:
"In times of regional geopolitical turmoil – Why do some equity funds performbetter than others?"
==>
"In times of regional geopolitical turmoil – Why do some equity funds perform better than others?"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:439876   - correct as is
----------------------------------------------------------------------
In diva2:1219174 
abstract is: 
<p>The very long times required to converge to optimal policies is a problem affecting machine learning and reinforcement learning in particular. Real-time solution on complex learning problems are necessary to expand the field into new domains where machine learning has previously been unfeasible. In this paper we introduce a novel method for training deep q-learning agents in an environment where the size can be dynamically scaled, in order to improve learning time. In this framework the agent starts in a very small environment where it can quickly experience different situations in a small scale and learn to handle them properly. As the agent learns the environment enough to reach certain predefined performance goals, the environment is expanded to increase complexity. The agent should then not have to relearn the environment completely, but simply adapt to the larger environment. This Incrementally Expanding Environment (IEE) method was compared to the conventional, deep q-learning method of training the agent on the full environment size from the beginning. Results showed that in some situations the methods performed identically, where in some situations the IEE method performed better. Particularly in scenarios with higher learning rates our framework improved its policy noticeably faster than the conventional method. The conventional method also notably never performed better than the IEE method. We thus conclude that the proposed framework is superior the the conventional one, being more robust to parameter choice and performing as well or better in all observed cases.</p>

corrected abstract:
<p>The very long times required to converge to optimal policies is a problem affecting machine learning and reinforcement learning in particular. Real-time solution on complex learning problems are necessary to expand the field into new domains where machine learning has previously been unfeasible. In this paper we introduce a novel method for training deep q-learning agents in an environment where the size can be dynamically scaled, in order to improve learning time. In this framework the agent starts in a very small environment where it can quickly experience different situations in a small scale and learn to handle them properly. As the agent learns the environment enough to reach certain predefined performance goals, the environment is expanded to increase complexity. The agent should then not have to relearn the environment completely, but simply adapt to the larger environment.</p><p>This Incrementally Expanding Environment (IEE) method was compared to the conventional, deep q-learning method of training the agent on the full environment size from the beginning. Results showed that in some situations the methods performed identically, where in some situations the IEE method performed better. Particularly in scenarios with higher learning rates our framework improved its policy noticeably faster than the conventional method. The conventional method also notably never performed better than the IEE method. We thus conclude that the proposed framework is superior the the conventional one, being more robust to parameter choice and performing as well or better in all observed cases.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1800502 
abstract is: 
<p>The fixed income market is not as exploited as other markets and has a more complex structure compared with the equity market. On the other hand, it has been seen that demand for research for the fixed income market has increased, which in turn has created greater interest in studying the characteristics of holdings in the market. This work studies whether it is possible to replicate indices through requirements for credit rating, sectors and mathematical key figures such as Duration, convexity, duration time spread (DTS) and option adjusted spread (OAS). Replication is made through linear programming in the program Python. By implementing lasso regression, this study examines whether it is possible to exceed the return by reducing the requirements for key figures that are not selected efter selection of variables in the regression. The investment company Alfred Berg has provided relevant data for this report. The data consists of information on all assets included in the index EUR Investment grade (ER00) over the period 2017-2021. The result of the replication follows the index returns, with small deviations, and the lasso regression selects the key figures DTS and OAS in its model. It is difficult to excess index return by focusing only on the key figures DTS and OAS. Analysis of other key figures and variables selected by the lasso regression can possibly create better results, as a suggestion for further work.</p>

corrected abstract:
<p>The fixed income market is not as exploited as other markets and has a more complex structure compared with the equity market. On the other hand, it has been seen that demand for research for the fixed income market has increased, which in turn has created greater interest in studying the characteristics of holdings in the market.</p><p>This thesis studies whether it is possible to replicate indices through requirements for credit rating, sectors and mathematical key figures: duration, convexity, duration times spread (DTS) and option adjusted spread (OAS). Replication is made through linear programming in the program Python. By implementing lasso regression, this study examines whether it is possible to exceed the return by reducing the requirements for key figures that are not selected after selection of variables in the regression.</p><p>The investment company Alfred Berg has provided relevant data for this report. The data consists of information on all assets included in the index EUR Investment grade (ER00) over the period 2017-2021.</p><p>The result of the replication follows the index returns, with small deviations, and the lasso regression selects the key figures DTS and OAS in its model. It is difficult to excess index return by focusing on the key figures DTS and OAS. Analysis of other key figures and variables selected by the lasso regression can possibly create better results, as a suggestion for further work.</p>

Note added missing paragraph breaks and some fixes to make the text the same as the original
----------------------------------------------------------------------
In diva2:877139   - correct as is
----------------------------------------------------------------------
In diva2:1878844 
abstract is: 
<p>This bachelor thesis investigates inequalities present within the elementary schools of Helsingborg during the period from 2013 to 2023. Through statistical analysis, it explores various dimensions of difference among students, considering factors such as gender, school affiliation, academic performance, and whether or not one recently arrived in the country. The study relies on data sourced from the Helsingborg municipality and addresses two main questions: the extent and nature of inequalities in Helsingborg’s schools and how these inequalities have evolved over the specified period. Two definitions of inequality are utilised: “Differences between groups” (e.g., between boys and girls) and “Differences within the same group” (indicated by high standard deviation within a group). These differences are quantified using three metrics: the sum of students’ grades (merit value), the number of classes passed, and whether or not the student qualified for further education (gymnasium). The methodology includes nonparametric hypothesis tests and regression analysis. The tests employed are Kruskal-Wallis’ test, Dunn’s test, χ2 test for independence, and Fisher’s exact test, with appropriate corrections. Standard linear regression is also applied. The findings highlight disparities in academic achievement and access to higher education. Notable results include a significant dichotomy between newly arrived students and other students across all performance metrics. There were no significant changes in inequality over the time period.</p>

corrected abstract:
<p>This bachelor thesis investigates inequalities present within the elementary schools of Helsingborg during the period from 2013 to 2023. Through statistical analysis, it explores various dimensions of difference among students, considering factors such as gender, school affiliation, academic performance, and whether or not one recently arrived in the country. The study relies on data sourced from the Helsingborg municipality and addresses two main questions: the extent and nature of inequalities in Helsingborg’s schools and how these inequalities have evolved over the specified period. Two definitions of inequality are utilised: “Differences between groups” (e.g., between boys and girls) and “Differences within the same group” (indicated by high standard deviation within a group). These differences are quantified using three metrics: the sum of students’ grades (merit value), the number of classes passed, and whether or not the student qualified for further education (gymnasium). The methodology includes nonparametric hypothesis tests and regression analysis. The tests employed are Kruskal-Wallis’ test, Dunn’s test, χ<sup>2</sup> test for independence, and Fisher’s exact test, with appropriate corrections. Standard linear regression is also applied.</p><p>The findings highlight disparities in academic achievement and access to higher education. Notable results include a significant dichotomy between newly arrived students and other students across all performance metrics. There were no significant changes in inequality over the time period.</p>

Note fixed superscript and added missing paragraph break
----------------------------------------------------------------------
In diva2:1757074   - correct as is
----------------------------------------------------------------------
In diva2:1355025 
abstract is: 
<p>This Document is a Master Thesis report as part of the cycle of the ENSIMAG and KTH engineer. The internship linked to this report took place at Nexialog Consulting, Paris.This document deals with the inference and ﬁltration by a counting process of a hidden parameter. The theory developed here is applied to Credit risk and specially to Migration Matrices of Rating.</p>
mc='Paris.This' c='Paris. This'
mc='.This' c='. This'

corrected abstract:
<p>This Document is a Master Thesis report as part of the cycle of the ENSIMAG and KTH engineer. The internship linked to this report took place at Nexialog Consulting, Paris.</p><p>This document deals with the inference and filtration by a counting process of a hidden parameter. The theory developed here is applied to Credit risk and specially to Migration Matrices of Rating.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1615850 
abstract is: 
<p>This report aims to give predictions of Stockholm’s region climate (especially temperature and precipitations) by 2040 and describe the consequences of possible climate changes on the building effluent water temperature and flowrate. First, I did an extensive review of the literature about climate change and climate prediction in Stockholm’s area in order to suggest probable climate scenarios for 2040, using climatic data from the reference period 1960-1990. Then the method of Heating and Cooling Degree Days was used to evaluate the consequences of climate change on household habits, and thus on building effluent water characteristics. It appears that Stockholm’s climate in 2040 will be different than in 1960-1990 with a really high probability. The most probable scenario indicates moderate warming (+1.5°C annually) and an increase in precipitations (+5% annually), with a strong seasonal heterogeneity: most warming and increase in precipitations happens during winter, while summer should become dryer. The climate is also predicted to become more variable, with increased extreme events. These changes have an impact on household habits: heating demand should decrease (by approximately 12% in a moderate warming scenario), and cooling demand increase remarkably (by more than 100% even in a moderate warming scenario). Thus, building effluent water temperature will decrease significantly (about 1°C to 2°C depending on the climate scenario), mostly due to the decrease in household heating.</p>

corrected abstract:
<p>This report aims to give predictions of Stockholm’s region climate (especially temperature and precipitations) by 2040 and describe the consequences of possible climate changes on the building effluent water temperature and flowrate.</p><p>First, I did an extensive review of the literature about climate change and climate prediction in Stockholm’s area in order to suggest probable climate scenarios for 2040, using climatic data from the reference period 1960-1990. Then the method of Heating and Cooling Degree Days was used to evaluate the consequences of climate change on household habits, and thus on building effluent water characteristics.</p><p>It appears that Stockholm’s climate in 2040 will be different than in 1960-1990 with a really high probability. The most probable scenario indicates moderate warming (+1.5°C annually) and an increase in precipitations (+5% annually), with a strong seasonal heterogeneity: most warming and increase in precipitations happens during winter, while summer should become dryer. The climate is also predicted to become more variable, with increased extreme events. These changes have an impact on household habits: heating demand should decrease (by approximately 12% in a moderate warming scenario), and cooling demand increase remarkably (by more than 100% even in a moderate warming scenario). Thus, building effluent water temperature will decrease significantly (about 1°C to 2°C depending on the climate scenario), mostly due to the decrease in household heating.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1241359   - correct as is
----------------------------------------------------------------------
In diva2:839895   - correct as is
----------------------------------------------------------------------
In diva2:1714317   - correct as is
----------------------------------------------------------------------
In diva2:1110793 
abstract is: 
<p>The purpose of this this report was to find the sources that led to the deviation between the simulation and measurement on the ride comfort evaluation of a high-speed train. This report consists of a literature study, introduction of the train where the measurement was taken on, result analysis and sensitivity test of the secondary suspension. The literature study focused on the modelling of secondary suspensions and the simulation of rail vehicles. The predicted results were compared with the measured results. Furthermore, comparison was carried out among three different secondary suspension concepts. The secondary suspensions went through a sensitivity test to see how the parameters influence the ride comfort evaluation of the rail vehicle.It was figured out that the main deviation between the simulation and the measurement was focused on a carbody where a hydraulic damper was introduced. The difference was mainly at 1.3Hz and between 7.5Hz and 9Hz.With the main deviations figured out, the sources that might influence the ride comfort evaluation was tested. It showed that the detail of track measurement had influence on the ride comfort evaluation. More detailed measurement should be carried out if higher agreement is wanted. The secondary suspensions went through a sensitivity test. The key parameters and their influence on ride comfort evaluation was pointed out. This report can be a guidance if further tuning on the parameters of the secondary suspensions are needed.</p>

mc='vehicle.It' c='vehicle. It'

corrected abstract:
<p>The purpose of this this report was to find the sources that led to the deviation between the simulation and measurement on the ride comfort evaluation of a high-speed train. This report consists of a literature study, introduction of the train where the measurement was taken on, result analysis and sensitivity test of the secondary suspension. The literature study focused on the modelling of secondary suspensions and the simulation of rail vehicles. The predicted results were compared with the measured results. Furthermore, comparison was carried out among three different secondary suspension concepts. The secondary suspensions went through a sensitivity test to see how the parameters influence the ride comfort evaluation of the rail vehicle.</p><p>It was figured out that the main deviation between the simulation and the measurement was focused on a carbody where a hydraulic damper was introduced. The difference was mainly at 1.3Hz and between 7.5Hz and 9Hz.</p><p>With the main deviations figured out, the sources that might influence the ride comfort evaluation was tested. It showed that the detail of track measurement had influence on the ride comfort evaluation. More detailed measurement should be carried out if higher agreement is wanted. The secondary suspensions went through a sensitivity test. The key parameters and their influence on ride comfort evaluation was pointed out. This report can be a guidance if further tuning on the parameters of the secondary suspensions are needed.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1722680   - correct as is
----------------------------------------------------------------------
In diva2:1320117 
abstract is: 
<p>With the current proliferation of cyber attacks, safeguarding internet facing assets from network intrusions, is becoming a vital task in our increasingly digitalised economies. Although recent successes of machine learning (ML) models bode the dawn of a new generation of intrusion detection systems (IDS); current solutions struggle to implement these in an efficient manner, leaving many IDSs to rely on rule-based techniques. In this paper we begin by reviewing the different approaches to feature construction and attack source identification employed in such applications. We refer to these steps as the framework within which models are implemented, and use it as a prism through which we can identify the challenges different solutions face, when applied in modern network traffic conditions. Specifically, we discuss how the most popular framework -- the so called flow-based approach -- suffers from significant overhead being introduced by its resource heavy pre-processing step. To address these issues, we propose the Information Theoretic Framework for Network Anomaly Detection (ITF-NAD); whose purpose is to facilitate online application of statistical learning models onto high-speed network links, as well as provide a method of identifying the sources of traffic anomalies. Its development was inspired by previous work on information theoretic-based anomaly and outlier detection, and employs modern techniques of entropy estimation over data streams. Furthermore, a case study of the framework's detection performance over 5 different types of Denial of Service (DoS) attacks is undertaken, in order to illustrate its potential use for intrusion detection and mitigation. The case study resulted in state-of-the-art performance for time-anomaly detection of single source as well as distributed attacks, and show promising results regarding its ability to identify underlying sources.</p>

corrected abstract:
<p>With the current proliferation of cyber attacks, safeguarding internet facing assets from network intrusions, is becoming a vital task in our increasingly digitalised economies. Although recent successes of machine learning (ML) models bode the dawn of a new generation of intrusion detection systems (IDS); current solutions struggle to implement these in an efficient manner, leaving many IDSs to rely on rule-based techniques. In this paper we begin by reviewing the different approaches to feature construction and attack source identification employed in such applications. We refer to these steps as the <em>framework</em> within which models are implemented, and use it as a prism through which we can identify the challenges different solutions face, when applied in modern network traffic conditions. Specifically, we discuss how the most popular framework &mdash; the so called <em>flow</em>-based approach &mdash; suffers from significant overhead being introduced by its resource heavy pre-processing step. To address these issues, we propose the Information Theoretic Framework for Network Anomaly Detection (ITF-NAD); whose purpose is to facilitate online application of statistical learning models onto high-speed network links, as well as provide a method of identifying the sources of traffic anomalies. Its development was inspired by previous work on information theoretic-based anomaly and outlier detection, and employs modern techniques of entropy estimation over data streams. Furthermore, a case study of the framework's detection performance over 5 different types of Denial of Service (DoS) attacks is undertaken, in order to illustrate its potential use for intrusion detection and mitigation. The case study resulted in state-of-the-art performance for time-anomaly detection of single source as well as distributed attacks, and show promising results regarding its ability to identify underlying sources.</p>

Note added italics and changed -- to &mdash;
----------------------------------------------------------------------
In diva2:1714291   - correct as is
----------------------------------------------------------------------
In diva2:1569744 
abstract is: 
<p>A dynamical system originally invented by Boltzmann has had recent developments. The system consists of a particle in a gravitational potential with an added centrifugal force, which is subject to reflection against a wall that separates the system from the gravitational center. The recent developments are with regards to the integrability of the system in the special case of vanishing centrifugal term. The purpose of this essay is to explicate these developments.</p>

corrected abstract:
<p>A dynamical system originally invented by Boltzmann has had recent developments [1] [2]. The system consists of a particle in a gravitational potential with an added centrifugal force, which is subject to reflection against a wall that separates the system from the gravitational center. The recent developments are with regards to the integrability of the system in the special case of vanishing centrifugal term. The purpose of this essay is to explicate these developments.</p>

Note adding missing citations
----------------------------------------------------------------------
In diva2:1033497   - correct as is
----------------------------------------------------------------------
In diva2:753608 
abstract is: 
<p>SCRAP is a student experiment that aims to validate theories on electron density fluctuations induced by the presence of dust particles connected to the phenomenon of polar mesospheric summer echoes (PMSE). Radar echoes are to be measured from a dust cloud of metallic microparticles released into the mesosphere above northern Sweden using a sounding rocket in spring 2015. This report presents a subproject of selecting the best size, amount and material of the dust particles to be used.</p><p>A collisionless model describing the charging of and wave backscattering from the dust cloud has been compiled and applied for typical mesospheric conditions. The results indicate that the charging is dominated by photoelectric emission, leading to a positive steady-state charge number between about 10 and 1000 in less than 24 s. The particle characteristics are shown to be measurable by computing the scattering cross-section originating in the propagation of dust acoustic waves. The choice of optimal particle parameters depends on two competitive effects. For large particles of high density and low photoelectric yield, the measured signals will be stronger but also broader in the frequency spectrum. Of the alternatives considered, silver particles of 0.1 <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cmu" />m radius are found to be the best choice.</p>

corrected abstract:
<p>SCRAP is a student experiment that aims to validate theories on electron density fluctuations induced by the presence of dust particles connected to the phenomenon of polar mesospheric summer echoes (PMSE). Radar echoes are to be measured from a dust cloud of metallic microparticles released into the mesosphere above northern Sweden using a sounding rocket in spring 2015. This report presents a subproject of selecting the best size, amount and material of the dust particles to be used.</p><p>A collisionless model describing the charging of and wave backscattering from the dust cloud has been compiled and applied for typical mesospheric conditions. The results indicate that the charging is dominated by photoelectric emission, leading to a positive steady-state charge number between about 10 and 1000 in less than 24 s. The particle characteristics are shown to be measurable by computing the scattering cross-section originating in the propagation of dust acoustic waves. The choice of optimal particle parameters depends on two competitive effects. For large particles of high density and low photoelectric yield, the measured signals will be stronger but also broader in the frequency spectrum. Of the alternatives considered, silver particles of 0.1 µm radius are found to be the best choice.</p>

Note insert the "µ" rather than using mimetex
----------------------------------------------------------------------
In diva2:1442653 - no abstract in full text, the text is from "Introduction"
abstract is: 
<p>Cleavage fracture is a catastrophic phenomenon as it is a sudden structural collapse occurring to structures made of insufficiently ductile steels working at lower temperatures. In this study, the focus is on the material used in reactor pressure vessels (RPV) in nuclear power plants. The steel is exposed to radiation that by time causes embrittlement of the material. Safe operation of the power plant with today's engineering standards is possible by taking advantage of warm pre-stressing (WPS) effects.</p><p>Welding is an inevitable part of manufacturing; therefore, it is important to investigate the influence of residual stresses (RS) due to welding on the load-bearing capacity of cracked components. Additionally, these stresses can also affect the benefits of WPS cycles which as mentioned earlier improves the structure behavior.</p><p>In this study, the effects of two mechanisms in WPS on a RS field will be studied and compared. As the focus of the study is on the brittle fracture, a probabilistic model made by Kroon and Faleskog, will be used. To get an overview of the thesis, a summary of each topic will follow.</p>

corrected abstract:
<p>Cleavage fracture is a catastrophic phenomenon as it is a sudden structural collapse occurring to structures made of insufficiently ductile steels working at lower temperatures[1]. In this study, the focus is on the material used in reactor pressure vessels (RPV) in nuclear power plants. The steel is exposed to radiation that by time causes embrittlement of the material. Safe operation of the power plant with today's engineering standards is possible by taking advantage of warm pre-stressing (WPS) effects.</p><p>Welding is an inevitable part of manufacturing; therefore, it is important to investigate the influence of residual stresses (RS) due to welding on the load-bearing capacity of cracked components. Additionally, these stresses can also affect the benefits of WPS cycles which as mentioned earlier improves the structure behavior.</p><p>In this study, the effects of two mechanisms in WPS on a RS field will be studied and compared. As the focus of the study is on the brittle fracture, a probabilistic model made by Kroon and Faleskog[4], will be used. To get an overview of the thesis, a summary of each topic will follow.</p>

Note added missing citations
----------------------------------------------------------------------
In diva2:725006 
abstract is: 
<p>This thesis provides a thorough analysis of the covered- and uncovered interest parity conditions (CIP, UIP) as well as the forward rate unbiasedness hypothesis (FRUH) for Sweden and the European Economic and Monetary Union (EMU). By studying data on interbank rates in Sweden (STIBOR) and the EMU (EURIBOR) as well as the corresponding spot- and forward exchange rates, monetary integration and country-specific risks are determined and analyzed with direct applications to the potential entry of Sweden into the EMU. As interest rate parity in general gives insight into market efficiency and frictions between the chosen regions, such points are discussed in addition to EMU entry. Drawing on past studies that mainly studied one condition in isolation, a nested formulation of interest rate parity is instead derived and tested using cointegration and robust estimation methods. The results point to a strict rejection of the FRUH for all horizons except the shortest and a case where CIP only holds for the 6-month horizon and partially over one year. This implies, based on the nested formulation, that UIP is rejected for all horizons as well. Ultimately, the study concludes that a Swedish entry into the EMU is not motivated given the lackluster results on UIP and due to the lack of monetary integration. </p>

corrected abstract:
<p>This thesis provides a thorough analysis of the covered- and uncovered interest parity conditions (CIP, UIP) as well as the forward rate unbiasedness hypothesis (FRUH) for Sweden and the European Economic and Monetary Union (EMU). By studying data on interbank rates in Sweden (STIBOR) and the EMU (EURIBOR) as well as the corresponding spot- and forward exchange rates, monetary integration and country-specific risks are determined and analyzed with direct applications to the potential entry of Sweden into the EMU. As interest rate parity in general gives insight into market efficiency and frictions between the chosen regions, such points are discussed in addition to EMU entry. Drawing on past studies that mainly studied one condition in isolation, a nested formulation of interest rate parity is instead derived and tested using cointegration and robust estimation methods. The results point to a strict rejection of the FRUH for all horizons except the shortest and a case where CIP only holds for the 6-month horizon and partially over one year. This implies, based on the nested formulation, that UIP is rejected for all horizons as well. Ultimately, the study concludes that a Swedish entry into the EMU is not motivated given the lackluster results on UIP and due to the lack of monetary integration.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:857210   - correct as is
----------------------------------------------------------------------
In diva2:1149190 
abstract is: 
<p>In May 2009 the European Commission decided on new regulations regarding solvency among insurance firms, the Solvency II Directive. The directive aims to strengthen the connection between the requirement of solvency and risks for insurance firms. The directive partly consists of a market risk module, in which a credit spread risk is a sub category.</p><p>In this thesis a model for credit spread risk is implemented. The model is an extended version of the Jarrow, Lando and Turnbull model (A Markov Model for theTerm Structure of Credit Risk Spreads, 1997) as proposed by Dubrana (A Stochastic Model for Credit Spreads under a Risk-Neutral Framework through the use of an Extended Version of the Jarrow, Lando and Turnbull Model, 2011). The implementation includes the calibration of a stochastic credit risk driver as well as a simulation of bond returns with the allowance of credit transitions and defaults.</p><p>The modeling will be made with the requirements of the Solvency II Directive in mind. Finally, the result will be compared with the Solvency II standard formula for the spread risk sub-module.</p>

mc='theTerm' c='the Term'

corrected abstract:
<p>In May 2009 the European Commission decided on new regulations regarding solvency among insurance firms, the Solvency II Directive. The directive aims to strengthen the connection between the requirement of solvency and risks for insurance firms. The directive partly consists of a market risk module, in which a credit spread risk is a sub category.</p><p>In this thesis a model for credit spread risk is implemented. The model is an extended version of the Jarrow, Lando and Turnbull model  [19] as proposed by Dubrana [8]. The implementation includes the calibration of a stochastic credit risk driver as well as a simulation of bond returns with the allowance of credit transitions and defaults.</p><p>The modeling will be made with the requirements of the Solvency II Directive in mind. Finally, the result will be compared with the Solvency II standard formula for the spread risk sub-module.</p>

Note replaced the text with citation - as per the original
----------------------------------------------------------------------
In diva2:1214240   - correct as is
----------------------------------------------------------------------
In diva2:1372261 
abstract is: 
<p>Couette, pipe, channel, and zero-pressure gradient (ZPG) turbulent boundary layer (TBL) flows have classically been considered as canonical wall-bounded turbulent flows since their near-wall behavior is generally considered to be universal, i.e. invariant of the flow case and the Reynolds number. Nevertheless, the idea that large-scale motions, being dominant in regions further away from the wall, might interact with and influence small-scale fluctuations close to the wall has not been disregarded. This view was mainly motivated due to the observed failure of collapse of the Reynolds normal stresses in viscous scaling. While this top-down influence has been studied extensively over the last decade, the idea of a bottom-up influence (backward energy transfer) is less examined. One exception was the recent experimental work on a Couette flow by Kawata, T. &amp; Alfredsson, P. H. (Phys. Rev. Lett. 120, 244501, 2018). In the present work, a spectral representation of the Reynolds Stress transport equation is used to perform a scale-by-scale analysis of the terms in the equation. Two flow cases were studied: first, a Direct Numerical Simulation (DNS) of a Couette flow at a similar Reynolds number as Kawata and Alfredsson. The Reynolds number was <em>ReT</em> = 120, viscosity <em>v</em>. Second, a Large Eddy Simulation (LES) of a ZPG TBL at <em>ReT</em> = 730, 1270, and 2400. For both cases the classic interscale transport or turbulent kinetic energy was observed. However, also an inverse interscale transport of Reynolds shear stress was observed for both cases.</p>

corrected abstract:
<p>Couette, pipe, channel, and zero-pressure gradient (ZPG) turbulent boundary layer (TBL) flows have classically been considered as canonical wall-bounded turbulent flows since their near-wall behavior is generally considered to be universal, i.e. invariant of the flow case and the Reynolds number. Nevertheless, the idea that large-scale motions, being dominant in regions further away from the wall, might interact with and influence small-scale fluctuations close to the wall has not been disregarded. This view was mainly motivated due to the observed failure of collapse of the Reynolds normal stresses in viscous scaling. While this top-down influence has been studied extensively over the last decade, the idea of a bottom-up influence (backward energy transfer) is less examined. One exception was the recent experimental work on a Couette flow by Kawata, T. &amp; Alfredsson, P. H. (Phys. Rev. Lett. 120, 244501, 2018). In the present work, a spectral representation of the Reynolds Stress transport equation is used to perform a scale-by-scale analysis of the terms in the equation. Two flow cases were studied: first, a Direct Numerical Simulation (DNS) of a Couette flow at a similar Reynolds number as Kawata and Alfredsson. The Reynolds number was 𝑅𝑒<sub>&#x1D70F;</sub> = 120, defined with friction viscosity &uscr;<sub>&#x1D70F;</sub>, channel half-height &hscr;, and kinematic viscosity &vscr;. Second, a Large Eddy Simulation (LES) of a ZPG TBL at 𝑅𝑒<sub>&#x1D70F;</sub> = 730, 1270, and 2400. For both cases the classic interscale transport of turbulent kinetic energy was observed. However, also an inverse interscale transport of Reynolds shear stress was observed for both cases.</p>

Note fixed text to match original, fixed equations
----------------------------------------------------------------------
In diva2:1584041 
abstract is: 
<p>The problem of causal discovery is to learn the true causal relations among a system of random variables based on the available data. Learning the true causal structure of p variables can sometimes be difficult, but it is crucial in many fields of science, such as biology, sociology and artificial intelligence. Classically, it is assumed that the true causal relations are completely encoded via a directed acyclic graph (DAG), and there are numerous algorithms for estimating a DAG representative of a causal system from data. Assuming it is feasible, the most effective way of learning the true causal structure is through interventional experiments. Eberhardt et al. identified the sufficient and in the worst case necessary number of interventions needed to learn a DAG, and then studied this problem from a game theory perspective, providing an upper bound on the expected number of experiments needed to identify the causal DAG. Here, we consider more general causal models, the CStrees, which allow for the true causal relations to be context-specific. We extend the results of Eberhardt to the family of CStrees by finding the sufficient and in the worst case necessary number of experiments the Scientist must perform in order to discover the true CStree among p variables. We generalize the game theoretic approach presented in Eberhardt's paper, to the CStrees with a specified causal ordering. We also give a geometric description of context-specific hard interventions in CStrees, through a bijection between the stages of the CStree and the faces of a polytope.</p>

corrected abstract:
<p>The problem of causal discovery is to learn the true causal relations among a system of random variables based on the available data. Learning the true causal structure of 𝑝 variables can sometimes be difficult, but it is crucial in many fields of science, such as biology, sociology and artificial intelligence. Classically, it is assumed that the true causal relations are completely encoded via a directed acyclic graph (DAG), and there are numerous algorithms for estimating a DAG representative of a causal system from data. Assuming it is feasible, the most effective way of learning the true causal structure is through interventional experiments. Eberhardt <em>et al.</em> identified the sufficient and in the worst case necessary number of interventions needed to learn a DAG, and then studied this problem from a game theory perspective, providing an upper bound on the expected number of experiments needed to identify the causal DAG. Here, we consider more general causal models, the CStrees, which allow for the true causal relations to be context-specific. We extend the results of Eberhardt to the family of CStrees by finding the sufficient and in the worst case necessary number of experiments the Scientist must perform in order to discover the true CStree among 𝑝 variables. We generalize the game theoretic approach presented in Eberhardt's paper, to the CStrees with a specified causal ordering. We also give a geometric description of context-specific hard interventions in CStrees, through a bijection between the stages of the CStree and the faces of a polytope.</p>

Note replace "p" with "𝑝" and added italics
----------------------------------------------------------------------
In diva2:839176 
abstract is: 
<p>The purpose of this study is to create a model of prediction for the volume distribution. Due to the lack of previous studies on the subject, an exploratory approach is used, with the purpose of serving as a proof of concept for further research. By looking at all market data from the Stockholm stock exchange the volume distribution of individual order books are matched with a mixed beta distribution and scaled by a prediction based on a linear regression. The model provided in this study outperforms the floating mean by quite a good margin. Some days are, almost by definition, impossible to get an accurate prediction on. Intraday news with a big impact have a tendency to skew the results away from the predicted value. To remedy this the initial model is expanded by using intraday data to catch up on trends</p>

corrected abstract:
<p>The purpose of this study is to create a model of prediction for the volume distribution. Due to the lack of previous studies on the subject, an exploratory approach is used, with the purpose of serving as a proof of concept for further research. By looking at all market data from the Stockholm stock exchange the volume distribution of individual order books are matched with a mixed beta distribution and scaled by a prediction based on a linear regression. The model provided in this study outperforms the floating mean by quite a good margin. Some days are, almost by definition, impossible to get an accurate prediction on. Intraday news with a big impact have a tendency to skew the results away from the predicted value. To remedy this the initial model is expanded by using intraday data to catch up on trends.</p>

Note added terminal period to last sentence - as per original
----------------------------------------------------------------------
In diva2:1219132   - correct as is
----------------------------------------------------------------------
In diva2:1568288   - correct as is
----------------------------------------------------------------------
In diva2:1736965   - correct as is
----------------------------------------------------------------------
In diva2:942671 
abstract is: 
<p>This study analyses the possibility to find an investment strategy that with a high probability will generate a positive return by using the technical instruments: RSI, Stochastic Oscillator and SMA. For determination of the probability and how high the return would be, a regression analysis is performed. Risk-adjusted return (Sharpe ratio) will then be calculated and compared with Swedish equity funds.</p><p>The result implies that it is possible using the selected parameters to find investment opportunities that provides higher risk-adjusted return than the Swedish equity funds, based on that the investor acquires a stock one day, and sells it the following trading day.</p>

corrected abstract:
<p>This study analyses the possibility to find an investment strategy that with a high probability will generate a positive return by using the technical instruments: RSI, Stochastic Oscillator and SMA. For determination of the probability and how high the return would be, a regression analysis is performed. Risk-adjusted return (Sharpe ratio) will then be calculated and compared with Swedish equity funds.</p><p>The result implies that it is possible using the selected parameters to find investment opportunities that provides higher risk-adjusted return than the Sweidsh equity funds, based on that the investor acquires a stock one day, and sells it the following trading day.</p>


Note error in oroginal
w='"Sweidsh' val={'c': 'Swedish', 's': 'diva2:942671', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:813895   - correct as is
----------------------------------------------------------------------
In diva2:1658656   - correct as is
----------------------------------------------------------------------
In diva2:1680232 
abstract is: 
<p>This paper has investigated if utilising Ion-Propulsion on a solar sunshade would lead to a reduction in total mass. The mechanism leading to this effect is that by using IP, the sunshade could be positioned closer to Earth. Positioning the sunshade closer to Earth would in turn allow for a smaller absolute size in the sunshade (whilst remaining a fixed angular size), and thus a decrease in total mass. This would lead to a decrease in launch costs and manufacturing costs. </p><p>The results indicate that fitting satellites with Ion-Propulsion would lead to a reduction in the total mass for the sunshade. More specifically they showcase the extent of which using IP has an effect on total mass, for example certain minimum ISP values that are needed to be reached in order for significant reductions of mass to arise. In a similar manner, the duration of which the satellites can remain in orbit also heavily impacts the total mass of the sunshade.</p><p>If Ion-Propulsion where to be utilised it is quintessential to understand how the technology impacts the total lifetime of the satellites. Additional investigation may also be made in investigating if Ion-Propulsion could be used in such a way where the satellites do not remain stationary in respect to the distance between Earth and the Sun, perhaps this could lead to additional reductions of mass.</p>

corrected abstract:
<p>This paper studies the feasibility of deploying a large planetary sunshade at the sub-L1 point as a geoengineering measure to counteract rising mean temperatures on Earth. More specifically, this paper studies if Ion-Propulsion could be utilised to decrease the total mass of such a sunshade. Ion-Propulsion could allow for the sunshade to be positioned closer to planet Earth, thus decreasing the required size of the sunshade, and consequently lead to lower total costs. The findings in this paper indicate that the utilisation of Ion-Propulsion would lead to a decrease in the total mass of the sunshade.  This would in turn lead to decreased deployment costs and increase the feasibility of the project.</p>

Note the original is very different from the DiVA abstract
----------------------------------------------------------------------
In diva2:1139440   - correct as is
----------------------------------------------------------------------
In diva2:666911   - correct as is
----------------------------------------------------------------------
In diva2:1221520 
abstract is: 
<p>Games can be used to model different autonomous decision problems to find strategies ac- cording to which the involved agents should act. When the agents in these problems cannot differentiate between certain states, for example due to a damaged sensor, the modeled game is said to be of imperfect information. The strategies in these types of games are much harder to find compared to games of perfect information, and it becomes even harder to find strategies when several agents are cooperating to solve a specific task in a so-called multi-player game of imperfect information.</p><p>We study a multi-player variant of a well known knowledge-based subset construction for games of imperfect information, which now considers a coalition of players working towards a common goal. This generalized construction reduces uncertainty in a multi-player game of imperfect information by creating a new, knowledge-based game. This reduction is useful since winning strategies found in the knowledge-based game can be translated back into winning strategies in the original game. The properties of this multi-player knowledge-based subset construction (MKBSC) are still not fully known. Unlike the single-player knowledge-based subset construction, the MKBSC is not guaranteed to give knowledge-based games of perfect information, which complicates the strategy synthesis. It is possible to apply the construction to the new game, creating another knowledge-based game of increased epistemic depth. It- erating the construction on the knowledge-based games may cause an unbounded increase in the number of states, causing the game to diverge.</p><p>We investigate the MKBSC by applying the construction to different multi-player games of imperfect information and classify the properties in the original game that may cause the constructed game to behave in certain ways. We observe that games which do not contain any observation overlaps or cycles within the game graph always stabilize when the construction is iterated on the game. These types of games are preferred when using the construction to create games which simplify strategy synthesis, since the number and size of created knowledge-based games is limited.</p>

corrected abstract:
<p>Games can be used to model different autonomous decision problems to find strategies according to which the involved agents should act. When the agents in these problems cannot differentiate between certain states, for example due to a damaged sensor, the modeled game is said to be of imperfect information. The strategies in these types of games are much harder to find compared to games of perfect information, and it becomes even harder to find strategies when several agents are cooperating to solve a specific task in a so-called multi-player game of imperfect information.</p><p>We study a multi-player variant of a well known knowledge-based subset construction for games of imperfect information, which now considers a coalition of players working towards a common goal. This generalized construction reduces uncertainty in a multi-player game of imperfect information by creating a new, knowledge-based game. This reduction is useful since winning strategies found in the knowledge-based game can be translated back into winning strategies in the original game. The properties of this multi-player knowledge-based subset construction (MKBSC) are still not fully known. Unlike the single-player knowledge-based subset construction, the MKBSC is not guaranteed to give knowledge-based games of perfect information, which complicates the strategy synthesis. It is possible to apply the construction to the new game, creating another knowledge-based game of increased epistemic depth. Iterating the construction on the knowledge-based games may cause an unbounded increase in the number of states, causing the game to diverge.</p><p>We investigate the MKBSC by applying the construction to different multi-player games of imperfect information and classify the properties in the original game that may cause the constructed game to behave in certain ways. We observe that games which do not contain any observation overlaps or cycles within the game graph always stabilize when the construction is iterated on the game. These types of games are preferred when using the construction to create games which simplify strategy synthesis, since the number and size of created knowledge-based games is limited.</p>

Note removed unnecesary hyphens
----------------------------------------------------------------------
In diva2:1189605 
abstract is: 
<p>This master thesis work was performed in the Scania RECT group. The purpose of this study was to investigate a new test method and to examine if it was possible to perform a vibration test on two alternators at the same time where one would act as a motor and the other as a conventional alternator. That all electric machines can be run both as a motor and a generator was nothing new. But the question was whether it was possible to put a load on the motor with enough good results to perform a vibration test. Among other things, in order to verify that subcontractors conform to Scania’s life length requirements, these types of vibration tests are routinely performed. Previous tests have shown that there have been some problems to perform a vibration test during operation. This due to low belt tension and unwanted resonances in the system. After thorough analysis of different control systems a controller to try to get the generator to rotate was purchased. First tests showed that it was possible to operate the alternator as an electric motor. Thereafter the new electric motor was paired with a similar alternator via a belt to start loading the original alternator. Two alternator types were investigated, one that worked significantly better in terms of testability than the other and the decision was taken to only use the better during the final vibration test. To the final test a completely new test rig was constructed to run new converted alternators in the future. Also, a fixture compatible with the bottom plate where the shaker test is performed was designed for future tests of alternators. Final vibration test resulted in several interruptions where the tested alternator broke down several times at the given PSD level and the final test could not be completed. However, it could be concluded that the new test method worked very well and can be used for future vibration tests of alternators.</p>

corrected abstract:
<p>This master thesis work was performed in the Scania RECT group. The purpose of this study was to investigate a new test method and to examine if it was possible to perform a vibration test on two alternators at the same time where one would act as a motor and the other as a conventional alternator. That all electric machines can be run both as a motor and a generator was nothing new. But the question was whether it was possible to put a load on the motor with enough good results to perform a vibration test. Among other things, in order to verify that subcontractors conform to Scania’s life length requirements, these types of vibration tests are routinely performed. Previous tests have shown that there have been some problems to perform a vibration test during operation. This due to low belt tension and unwanted resonances in the system. After thorough analysis of different control systems a controller to try to get the generator to rotate was purchased. First tests showed that it was possible to operate the alternator as an electric motor. Thereafter the new electric motor was paired with a similar alternator via a belt to start loading the original alternator. Two alternator types were investigated, one that worked significantly better in terms of testability than the other and the decision was taken to only use the better during the final vibration test. To the final test a completely new test rig was constructed to run new converted alternators in the future. Also, a fixture compatible with the bottom plate where the shaker test is performed was designed for future tests of alternators.</p><p>Final vibration test resulted in several interruptions where the tested alternator broke down several times at the given PSD level and the final test could not be completed. However, it could be concluded that the new test method worked very well and can be used for future vibration tests of alternators.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1083080 
abstract is: 
<p>This thesis aims at presenting Computational Fluid Dynamics studies conducted on an axisymmetric model of the Siemens SGT-800 burner using Ansys Fluent, Ansys CFX and Ansys ICEM. The goal is to perform a mesh study and turbulence model study for isothermal flow. The result will show the differences observed while using the two solvers by Ansys, Fluent and CFX. Two different meshes, A, coarse and B, optimal have been used for the mesh study. This will reveal the mesh dependency of the different parameters and if any differences are observed between the solver’s convergence and mesh independency performance. To further validate the mesh independency, a simplified test case is simulated for turbulent flow for 32 different cases testing the numerical algorithms and spatial discretization available in Ansys Fluent and finding the optimal method to achieve convergence and reliable results. Turbulence model study has been performed where k-ε, k-ω and k-ω Shear Stress Transport (SST) model have been simulated and the results between solvers and models are compared to see if the solvers’ way of handling the different models varies.Studies from this thesis suggest that both solvers implement the turbulence models differently. Out of the three models compared, k-ω SST is the model with least differences between solvers. The solution looks alike and therefore it could be suggested to use this model, whenever possible, for future studies when both solvers are used. For the models k-ε and k-ω significant differences were found between the two solvers when comparing velocity, pressure and turbulence kinetic energy. Different reasons for its occurrence are discussed in the thesis and also attempts have been made to rule out few of the reasons to narrow down the possible causes. One of the goals of the thesis was to also discuss the differences in user-interface and solver capabilities which have been presented in the conclusions and discussions section of the report. Questions that still remain unanswered after the thesis are why these differences are present between solvers and which of the solvers’ results are more reliable when these differences have been found.</p>
mc='varies.Studies' c='varies. Studies'

partal corrected: diva2:1083080: <p>This thesis aims at presenting Computational Fluid Dynamics studies conducted on an axisymmetric model of the Siemens SGT-800 burner using Ansys Fluent, Ansys CFX and Ansys ICEM. The goal is to perform a mesh study and turbulence model study for isothermal flow. The result will show the differences observed while using the two solvers by Ansys, Fluent and CFX. Two different meshes, A, coarse and B, optimal have been used for the mesh study. This will reveal the mesh dependency of the different parameters and if any differences are observed between the solver’s convergence and mesh independency performance. To further validate the mesh independency, a simplified test case is simulated for turbulent flow for 32 different cases testing the numerical algorithms and spatial discretization available in Ansys Fluent and finding the optimal method to achieve convergence and reliable results. Turbulence model study has been performed where k-ε, k-ω and k-ω Shear Stress Transport (SST) model have been simulated and the results between solvers and models are compared to see if the solvers’ way of handling the different models varies. Studies from this thesis suggest that both solvers implement the turbulence models differently. Out of the three models compared, k-ω SST is the model with least differences between solvers. The solution looks alike and therefore it could be suggested to use this model, whenever possible, for future studies when both solvers are used. For the models k-ε and k-ω significant differences were found between the two solvers when comparing velocity, pressure and turbulence kinetic energy. Different reasons for its occurrence are discussed in the thesis and also attempts have been made to rule out few of the reasons to narrow down the possible causes. One of the goals of the thesis was to also discuss the differences in user-interface and solver capabilities which have been presented in the conclusions and discussions section of the report. Questions that still remain unanswered after the thesis are why these differences are present between solvers and which of the solvers’ results are more reliable when these differences have been found.</p>

corrected abstract:
<p>This thesis aims at presenting Computational Fluid Dynamics studies conducted on an axisymmetric model of the Siemens SGT-800 burner using Ansys Fluent, Ansys CFX and Ansys ICEM. The goal is to perform a mesh study and turbulence model study for isothermal flow. The result will show the differences observed while using the two solvers by Ansys, Fluent and CFX. Two different meshes, A, coarse and B, optimal have been used for the mesh study. This will reveal the mesh dependency of the different parameters and if any differences are observed between the solver’s convergence and mesh independency performance. To further validate the mesh independency, a simplified test case is simulated for turbulent flow for 32 different cases testing the numerical algorithms and spatial discretization available in Ansys Fluent and finding the optimal method to achieve convergence and reliable results. Turbulence model study has been performed where k-ε, k-ω and k-ω Shear Stress Transport (SST) model have been simulated and the results between solvers and models are compared to see if the solvers’ way of handling the different models varies.</p><p>Studies from this thesis suggest that both solvers implement the turbulence models differently. Out of the three models compared, k-ω SST is the model with least differences between solvers. The solution looks alike and therefore it could be suggested to use this model, whenever possible, for future studies when both solvers are used. For the models k-ε and k-ω significant differences were found between the two solvers when comparing velocity, pressure and turbulence kinetic energy. Different reasons for its occurrence are discussed in the thesis and also attempts have been made to rule out few of the reasons to narrow down the possible causes. One of the goals of the thesis was to also discuss the differences in user-interface and solver capabilities which have been presented in the conclusions and discussions section of the report. Questions that still remain unanswered after the thesis are why these differences are present between solvers and which of the solvers’ results are more reliable when these differences have been found.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1355274 
abstract is: 
<p>The work deals with an investigation of steady, laminar and incompressible flow inside a toroidal pipe. In particular, our aim is to investigate the effects of curvature and the Reynolds-number on the flow features. Numerical simulation are performed with the open-source CFD code <em>OpenFOAM</em> and the results are compared with the available data in the literature [<em>Canton et. Al 2017, Int. J. Heat Fluid Flow</em>]. The analysis shows that by increasing the curvature, the flow features are altered. This change in flow features becomes more important as the curvature increases. A complete description of friction factor and secondary flow quantities is provided, and a comparison with reference data allows to assess the reliability of OpenFOAM for this study case by providing results in an easier way than with more advanced codes such as Nek5000.</p>

corrected abstract:
<p>The work deals with an investigation of steady, laminar and incompressible flow inside a toroidal pipe. In particular, our aim is to investigate the effects of curvature and the Reynolds-number on the flow features. Numerical simulation are performed with the open-source CFD code <em>OpenFOAM</em> and the results are compared with the available data in the literature [<em>Canton et al. 2017, Int. J. Heat Fluid Flow</em>]. The analysis shows that by increasing the curvature, the flow features are altered. This change in flow features becomes more important as the curvature increases. A complete description of friction factor and secondary flow quantities is provided, and a comparison with reference data allows to assess the reliability of OpenFOAM for this study case by providing results in an easier way than with more advanced codes such as Nek5000.</p>

Note change in italics
----------------------------------------------------------------------
In diva2:612207 
abstract is: 
<p>The ventilation systems are becoming more compact so as to save buildings space. In return, the velocity, in ventilation units, is increased and the air flows are also more turbulent. The hotwire investigation aims to correlate this increase of turbulence with the increase of the noise level. The investigation was done for industrials purposes and is based on empirical researches completed with theoretical knowledge. For the investigation, the hotwire sensor is chosen for its ability to detect small velocity fluctuations at high frequency. Two different prototypes are designed in order to highlight the influence of the turbulence level in the sound generation, especially at ventilation outlets. A procedure is also introduced, in which the hotwire is used for the turbulence measures and a reverberation room for the sound measurement. General conclusions are finally identified and explain the influence of the turbulence in the sound generation mechanisms. The influence of the prototypes geometries, on both the sound and the turbulence, is analyzed and the master thesis describes how the air flow velocity in ducts and the static pressure could modify both the turbulence and the sound levels.</p><p>The stated conclusions imply that the designers of new ventilation systems should take into account the turbulence generated by their experimental product if they want to conserve good sound properties.</p>

corrected abstract:
<p>The ventilation systems are becoming more compact so as to save buildings space. In return, the velocity, in ventilation units, is increased and the air flows are also more turbulent. The hotwire investigation aims to correlate this increase of turbulence with the increase of the noise level.</p><p>The investigation was done for industrials purposes and is based on empirical researches completed with theoretical knowledge.</p><p>For the investigation, the hotwire sensor is chosen for its ability to detect small velocity fluctuations at high frequency. Two different prototypes are designed in order to highlight the influence of the turbulence level in the sound generation, especially at ventilation outlets. A procedure is also introduced, in which the hotwire is used for the turbulence measures and a reverberation room for the sound measurement.</p><p>General conclusions are finally identified and explain the influence of the turbulence in the sound generation mechanisms. The influence of the prototypes geometries, on both the sound and the turbulence, is analyzed and the master thesis describes how the air flow velocity in ducts and the static pressure could modify both the turbulence and the sound levels.</p><p>The stated conclusions imply that the designers of new ventilation systems should take into account the turbulence generated by their experimental product if they want to conserve good sound properties.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1183299   - correct as is
----------------------------------------------------------------------
In diva2:1599583 
abstract is: 
<p>Due to the increase in demand on freight transportation it becomes necessary to avoid delays to ensure that the goods reach its destination on time. The main factors causing disturbances in the traffic on the mainline is the breakdown of vehicles mainly due to damaged wheels. The damaged wheels are identified through the wheel-rail impact force measurements provided by the wheel impact load detectors (WILD). This calls for the optimal schedule of maintenance of wheelsets and wagons in general. During the maintenance, the officials manually check for defective wheels and the exchange of wheelsets is performed based on the type of damage. The classification of wheel damages plays a vital role in providing ease of damage identification and insights to deduce a strategy for wheelset exchange. In this study, an attempt to classify the damaged wheelsets is done by analysing the wheel-rail impact force data from the wayside detectors. The data from the detectors are acquired from PredgeAB, a Luleå based startup pioneering in providing decision support on optimal maintenance schedules and predictive maintenance of rail wheels. Through their detection and prediction solutions it was observed that of all the wheels marked as damaged by the detectors 10% were undamaged. The source of the deviation in the impact force readings could help Predge make better estimations in damage detection and prediction. In this study, the sources contributing to the deviation is studied using multi-body simulations in GENSYS. A new method for modelling wheel damage is developed to overcome the shortcomings of the software. The findings of this study can then be used appropriately to make classifications of wheel damages.</p>

corrected abstract:
<p>Due to the increase in demand on freight transportation it becomes necessary to avoid delays to ensure that the goods reach its destination on time. The main factors causing disturbances in the traffic on the mainline is the break down of vehicles mainly due to damaged wheels. The damaged wheels are identified through the wheel-rail impact force measurements provided by the Wheel impact load detectors (WILD). This calls for the optimal schedule of maintenance of wheelsets and wagons in general. During the maintenance, the officials manually check for defective wheels and the exchange of wheelsets is performed based on the type of damage. The classification of wheel damages plays a vital role in providing ease of damage identification and insights to deduce a strategy for wheelset exchange. In this study, an attempt to classify the damaged wheelsets is done by analysing the wheel-rail impact force data from the wayside detectors. The data from the detectors are acquired from Predge AB, a Luleå based startup pioneering in providing decision support on optimal maintenance schedules and predictive maintenance of rail wheels. Through their detection and prediction solutions it was observed that of all the wheels marked as damaged by the detectors 10% were undamaged. The source of the deviation in the impact force readings could help Predge make better estimations in damage detection and prediction. In this study, the sources contributing to the deviation is studied using multi-body simulations in GENSYS. A new method for modelling wheel damage is developed to overcome the shortcomings of the software. The findings of this study can then be used appropriately to make classifications of wheel damages.</p>

Note some spaces added and a change in capitalization
----------------------------------------------------------------------
In diva2:447049   - correct as is
----------------------------------------------------------------------
In diva2:1286935 
abstract is: 
<p>In this thesis, support structures of a polymer powder based process called XXXXXXXX™ are examined. These structures are crucial for most additive manufacturing processes. The effects of several factors on five industrially important characteristics of support structures are examined by use of the Design of Experiment (DoE) method. It describes the planning as well as the analysis of the experiments. The experiments are planned in a fractional factorial 211-5 design with 64 specimens, resulting in a resolution of IV. The analysis of the data is done by use of the ANOVA method, with which the significance of effects and interaction effects are checked.</p>

corrected abstract:
<p>In this thesis, support structures of a polymer powder based process called XXXXXXXX™ are examined. These structures are crucial for most additive manufacturing processes. The effects of several factors on five industrially important characteristics of support structures are examined by use of the Design of Experiment (DoE) method. It describes the planning as well as the analysis of the experiments. The experiments are planned in a fractional factorial 2<sup>11-5</sup> design with 64 specimens, resulting in a resolution of IV. The analysis of the data is done by use of the ANOVA method, with which the significance of effects and interaction effects are checked.</p>

Note corrction in superscript
----------------------------------------------------------------------
In diva2:1803687   - correct as is
----------------------------------------------------------------------
In diva2:1670572 - full text fails to load - file is truncated
----------------------------------------------------------------------
In diva2:802072   - correct as is
----------------------------------------------------------------------
In diva2:1739031   - correct as is
----------------------------------------------------------------------
In diva2:1189533 
abstract is: 
<p>An investigation on noise transmission through the steering gear system is focused on the area of vehicle NVH (Noise Vibration and Harshness). From previous investigations it is well known that noise transmission through the steering system sometimes has a significant influence referring to tire road noise. In these cases, the interface force between steering gear and vehicle subframe is usually of interest during early stages of vehicle development. The target of the current work is to validate an approach based on a method, commonly known as the “Blocked force method” for noise and vibration testing, and check if it is possible to estimate the interface forces based on this method. This method became popular for mechanical testing in the middle 1900s [1] [2] [3], because of its easy applicability. The basic idea is to fix the component on a rigid plate and measure the interface force at the coupling points with force transducers inserted in between. It has a considerable potential in practical use, because if the blocked force could provide a good estimation for the interface force in noise transmission, it enables an efficient measurement of critical NVH behaviors without having to resort to the complete vehicle.However intuitively, the blocked force data could not be used directly. As a result, a coupled system equation is used for the coupling of the steering gear and the car body. Both the inertance of car body and steering system are taken into account, and thus introduces a compensation for the difference between vehicle and test bench. The method is validated against the transfer function measured on the full vehicle as the reference and compare this with the result calculated with the measured “Blocked force” data. Siemens Test.Lab is used for data acquisition and first data processing. Matlab is used for data post processing. Matrix symmetrization and singular value truncation is used to deal with the ill-conditioned data of steering gear inertance. A hybrid model with Finite Element data is proposed. Two test conditions are investigated: one set is with rigid coupling and the other is decoupled with rubber.</p>

corrected abstract:
<p>An investigation on noise transmission through the steering gear system is focused on the area of vehicle NVH (Noise Vibration and Harshness). From previous investigations it is well known that noise transmission through the steering system sometimes has a significant influence referring to tire road noise. In these cases, the interface force between steering gear and vehicle subframe is usually of interest during early stages of vehicle development. The target of the current work is to validate an approach based on a method, commonly known as the “Blocked force method” for noise and vibration testing, and check if it is possible to estimate the interface forces based on this method. This method became popular for mechanical testing in the middle 1900s [1] [2] [3], because of its easy applicability. The basic idea is to fix the component on a rigid plate and measure the interface force at the coupling points with force transducers inserted in between. It has a considerable potential in practical use, because if the blocked force could provide a good estimation for the interface force in noise transmission, it enables an efficient measurement of critical NVH behaviors without having to resort to the complete vehicle.</p><p>However intuitively, the blocked force data could not be used directly. As a result, a coupled system equation is used for the coupling of the steering gear and the car body. Both the inertance of car body and steering system are taken into account, and thus introduces a compensation for the difference between vehicle and test bench. The method is validated against the transfer function measured on the full vehicle as the reference and compare this with the result calculated with the measured “Blocked force” data. Siemens Test.Lab is used for data acquisition and first data processing. Matlab is used for data post processing. Matrix symmetrization and singular value truncation is used to deal with the ill-conditioned data of steering gear inertance. A hybrid model with Finite Element data is proposed. Two test conditions are investigated: one set is with rigid coupling and the other is decoupled with rubber.</p>

Note added missing paragraph break
Also, note that "Test.Lab" is one word
----------------------------------------------------------------------
In diva2:687488   - correct as is
----------------------------------------------------------------------
In diva2:1038792   - correct as is
----------------------------------------------------------------------
In diva2:1431651   - correct as is
----------------------------------------------------------------------
In diva2:971500 
abstract is: 
<p>The delay Lyapunov equation is a matrix boundary value problem arising in the characterization of many properties of time-delay systems, for example stability analysis. Its numerical treatment is challenging. For the special case of single-delay systems, a new algorithm based on a delay free formulation has recently been proposed. Using this formulation it is possible to obtain a linear system of equations with an equivalent solution. This linear system can be solved with GMRES or a similar iterative method, thus allowing to efficiently solve large-scale problems. In addition to the preconditioner proposed in the literature, on the basis of solving a T-Sylvester equation, a new preconditioner for this iterative method is derived here. It uses the diagonals of the time-delay system’s n × n state matrices to compute an approximation of the action of the n² × n² matrix associated to the linear system. Computational cost and convergence of this new preconditioner are investigated and proved. Examples for its efficiency under certain conditions are given and it is compared to the preconditioner from the literature. A pseudospectral analysis of the corresponding operators is conducted to get a better understanding of the convergence of both preconditioners. Several ways to obtain pseudospectra based convergence estimates are presented and their descriptiveness for different types of problems is discussed.</p>

corrected abstract:
<p>The delay Lyapunov equation is a matrix boundary value problem arising in the characterization of many properties of time-delay systems, for example stability analysis. Its numerical treatment is challenging. For the special case of single-delay systems, a new algorithm based on a delay free formulation has recently been proposed. Using this formulation it is possible to obtain a linear system of equations with an equivalent solution. This linear system can be solved with GMRES or a similar iterative method, thus allowing to efficiently solve large-scale problems. In addition to the preconditioner proposed in the literature, on the basis of solving a T-Sylvester equation, a new preconditioner for this iterative method is derived here. It uses the diagonals of the time-delay system’s 𝑛 × 𝑛 state matrices to compute an approximation of the action of the 𝑛<sup>2</sup> × 𝑛<sup>2</sup> matrix associated to the linear system. Computational cost and convergence of this new preconditioner are investigated and proved. Examples for its efficiency under certain conditions are given and it is compared to the preconditioner from the literature. A pseudospectral analysis of the corresponding operators is conducted to get a better understanding of the convergence of both preconditioners. Several ways to obtain pseudospectra based convergence estimates are presented and their descriptiveness for different types of problems is discussed.</p>

Note replaced "n" by "𝑛" and fixed superscripts
----------------------------------------------------------------------
In diva2:1751409 
abstract is: 
<p>This master thesis treats the numerical simulation of shaped charge jets and its fragmentation process. Shaped charges is a method to concentrate the effect of an explosive charge and penetrate deeply into a target due to a the formation of a jet with great penetration capabilities. The jet's penetration capability is limited by the eventual axial breakup and an understanding of the fragmentation process is great importance. </p><p>A literature review on the existing methods for studying the fragmentation process is presented. A physical model and its governing equations are thereafter derived based on the review. A Lagrangian approach is used to model the jet and equations based on conservation laws coupled with a constitutive relationship yielding a system of nonlinear partial differential equations. </p><p>Moreover, an analysis of the well-posedness of a simplified problem is investigated and its derived conditions are consistent with the physically expected.</p><p>The flight and initial breakup of the jet is then studied numerically by employing a method of lines. The implemented numerical model's stability is investigated empirically and the theoretically expected rate of convergence is confirmed. The theoretical conditions for well-posedness are also confirmed numerically. </p><p>The derived model and its implementation is tested for a real charge and its results are compared with and found consistent with more advanced simulations. Furthermore, the jets physical properties are also investigated and the existence of a critical wavelength is shown. </p><p>The resulting model and its implementation is capable of calculating position, velocity and geometry at fragmentation. It can also be used to investigate the calculated fragmentation's dependency on different parameters and constitutive equation. The numerical simulation can therefore be used to increase the understanding under which conditions the jet breakup and which material- and geometry properties that dominates the rate in the fragmentation process. Possible future use is also as the foundation of a tool that can be used to evaluate analytical models.</p>

corrected abstract:
<p>This master thesis treats the numerical simulation of shaped charge jets and its fragmentation process. Shaped charges is a method to concentrate the effect of an explosive charge and penetrate deeply into a target due to a the formation of a jet with great penetration capabilities. The jet's penetration capability is limited by the eventual axial breakup and an understanding of the fragmentation process is great importance.</p><p>A literature review on the existing methods for studying the fragmentation process is presented. A physical model and its governing equations are thereafter derived based on the review. A Lagrangian approach is used to model the jet and equations based on conservation laws coupled with a constitutive relationship yielding a system of nonlinear partial differential equations.</p><p>Moreover, an analysis of the well-posedness of a simplified problem is investigated and its derived conditions are consistent with the physically expected.</p><p>The flight and initial breakup of the jet is then studied numerically by employing a method of lines. The implemented numerical model's stability is investigated empirically and the theoretically expected rate of convergence is confirmed. The theoretical conditions for well-posedness are also confirmed numerically.</p><p>The derived model and its implementation is tested for a real charge and its results are compared with and found consistent with more advanced simulations. Furthermore, the jets physical properties are also investigated and the existence of a critical wavelength is shown.</p><p>The resulting model and its implementation is capable of calculating position, velocity and geometry at fragmentation. It can also be used to investigate the calculated fragmentation's dependency on different parameters and constitutive equation. The numerical simulation can therefore be used to increase the understanding under which conditions the jet breakup and which material- and geometry properties that dominates the rate in the fragmentation process. Possible future use is also as the foundation of a tool that can be used to evaluate analytical models.</p>

Note Note only change was to eliminate an unnecessary space at the end of a paragraph and fixed hyphen space
----------------------------------------------------------------------
In diva2:1285486 - missing space in title:
"Jet Stream Velocity fromAzipod on Stadsgården: A Litterature Study of PIANC W.G. 180Application for Stadsgården"
==>
"Jet Stream Velocity from Azipod on Stadsgården: A Litterature Study of PIANC W.G. 180 Application for Stadsgården"

Note "Litterature" is an error in the original

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1207110   - correct as is
----------------------------------------------------------------------
In diva2:1437505   - correct as is
----------------------------------------------------------------------
In diva2:1751410   - correct as is
----------------------------------------------------------------------
In diva2:1441634   - correct as is
----------------------------------------------------------------------
In diva2:619635 
abstract is: 
<p>On the one hand, airlines order new planes and the worldwide fleet increases, while, on the other hand, the market pressure, the rise of fuel prices and other factors contribute to regular changes in the technology. These drivers may impact maintenance activities and support to operators, and the number of issues occurring on in-service aircraft. In-service and production queries are a specific type of support activities followed-up by propulsion systems integration engineers from the aircraft manufacturer, such as Airbus. These technical questions can address any of the engine’s systems and must usually be answered to within a short timeframe as they might delay a flight or the delivery of an airplane. In the global scope of knowledge management inside the company, these engineers realized their loss of not capitalizing these activities and promoted this project. An adapted application has been developed to share the experience among programs and support the engineers for the treatment of such queries. As the focus of the project was put on assessing the actual need of the future users to provide an adapted tool, the database should prove its performance over the long term. This paper details the different steps of the project: analysis of the need, specifications, programming and testing, that led to meeting this specific need for capitalization.</p>

corrected abstract:
<p>This report details my work as the endpoint of the master’s program in aerospace engineering that I attended at the Aeronautical and Vehicle Engineering Department of Kungliga Tekniska Högskolan, Sweden. It is as well the conclusion of my internship in the Propulsion Systems Integration domain (EPT3) of Airbus Operation SAS, France.</p><p>On the one hand, airlines order new planes and the worldwide fleet increases, while, on the other hand, the market pressure, the rise of fuel prices and other factors contribute to regular changes in the technology. These drivers may impact maintenance activities and support to operators, and the number of issues occurring on in-service aircraft. In-service and production queries are a specific type of support activities followed-up by propulsion systems integration engineers from the aircraft manufacturer, such as Airbus. These technical questions can address any of the engine’s systems and must usually be answered to within a short timeframe as they might delay a flight or the delivery of an airplane. In the global scope of knowledge management inside the company, these engineers realized their loss of not capitalizing these activities and promoted this project. An adapted application has been developed to share the experience among programs and support the engineers for the treatment of such queries. As the focus of the project was put on assessing the actual need of the future users to provide an adapted tool, the database should prove its performance over the long term. This paper details the different steps of the project: analysis of the need, specifications, programming and testing, that led to meeting this specific need for capitalization.</p>

Note that the DiVA version is missing the first paragraph of text.
----------------------------------------------------------------------
In diva2:1342388   - correct as is
----------------------------------------------------------------------
In diva2:852528   - correct as is
----------------------------------------------------------------------
In diva2:1450306   - correct as is
----------------------------------------------------------------------
In diva2:1437558   - correct as is
----------------------------------------------------------------------
In diva2:1894665  - correct as is
-- uses custom encoding - so one must print and OCR
----------------------------------------------------------------------
In diva2:408807 
abstract is: 
<p>The role of the quality manager has evolved from being a quality controller to work with a more global business. Within many organizations in Sweden, however, the traditional role of the quality manager's remains and the tasks categorized as quality control still takes up most of the quality manager's time. The survey conducted as part of this master thesis shows that the quality manager in most businesses has multiple roles in addition to the role of quality manager, which can create a role conflict. One consequence of this is that each improvement is not given priority due to time constraints. Although the importance of a clear and committed leadership is the key for succeeding as the quality manager. The quality manager’s educational background can vary between different activities. Most quality managers have a basic training in business development and quality, but many do not have deep expertise in the area. The quality manager wants more education and is looking for deeper skills for living in a constantly learning. The master thesis identifies the quality manager's role, tasks and education. It is hoped that in the long run from this study could support the quality manager's role and future development in the debate.</p>

corrected abstract:
<p>The role of the quality manager has evolved from being a quality controller to work with a more global business. Within many organizations in Sweden, however, the traditional role of the quality manager's remains and the tasks categorized as quality control still takes up most of the quality manager's time. The survey conducted as part of this master thesis shows that the quality manager in most businesses has multiple roles in addition to the role of quality manager, which can create a role conflict. One consequence of this is that each improvement is not given priority due to time constraints. Although the importance of a clear and committed leadership is the key for succeeding as the quality manager.</p><p>The quality manager’s educational background can vary between different activities. Most quality managers have a basic training in business development and quality, but many do not have deep expertise in the area. The quality manager wants more education and is looking for deeper skills for living in a constantly learning.</p><p>The master thesis identifies the quality manager's role, tasks and education. It is hoped that in the long run from this study could support the quality manager's role and future development in the debate.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1341314   - correct as is
----------------------------------------------------------------------
In diva2:1894675   - correct as is
----------------------------------------------------------------------
In diva2:942711 
abstract is: 
<p>This thesis presents a study in mathematical optimization of the inventory routine at the company Aktiebolaget Kronborsten. The thesis establishes a general optimization problem identified at Kronborstens inventory routine. The identified problem is to find the optimal mix between products in the finished goods inventory, which minimizes the expected time until delivery.</p><p>The proposed model assumes that orders and manufacturing follow a stochastic process. With these assumptions the inventory and manufacturing are represented as several independent Markov processes. From the stationary distribution of these processes a function was identified for the expected time until delivery for a given solution. The identified function had convex properties which made it possible to solve the optimization problem using the marginal allocation algorithm.</p><p>The mathematical problem is followed by a chapter about the costs related to storage. The purpose of this chapter is to help Kronborsten to valuate their options and consequences of strategical decisions about the inventory levels. </p>

corrected abstract:
<p>This thesis presents a study in mathematical optimization of the inventory routine at the company Aktiebolaget Kronborsten. The thesis establish a general optimization problem identified at Kronborstens inventory routine. The identified problem is to find the optimal mix between products in the finished goods inventory, which minimizes the expected time until delivery.</p><p>The proposed model assumes that orders and manufacturing follow a stochastic process. With these assumptions the inventory and manufacturing are represented as several independent markov processes. From the stationary distribution of these processes a function was identified for the expected time until delivery for a given solution. The identified function had convex properties which made it possible to solve the optimization problem using the marginal allocation algorithm.</p><p>The mathematical problem is followed by a chapter about the costs related to storage. The purpose of this chapter is to help Kronborsten to valuate their options and consequences of strategical decisions about the inventory levels.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph

Errors in original:
w='markov' val={'c': 'Markov', 's': 'diva2:942711', 'n': 'error in original'}
w='establish' val={'c': 'establishes', 's': 'diva2:942711', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:1756997   - correct as is
 -- uses custom encoding
----------------------------------------------------------------------
In diva2:451204 
abstract is: 
<p>In this diploma project the so-called fixed beam moving stage (FBMS) module in the Raith 150 electron beam lithography system has been evaluated for making large area zone plate exposures. The project goal, besides the evaluation of the module, has been to find an exposure recipe for exposing zone plates with diameter up to 500 μm. The zone plates fabricated with this method will be used for synchrotron and x-ray free electron laser applications.</p>
<p>The thesis starts with a short introduction to zone plate properties and fabrication procedures. Then the work where FBMS exposed zone plates are compared with normal write field exposures of 75 μm diameter zone plates is described. The conclusion is that for these small diameters, major problems with wobbly zones occur for the FBMS patterns. However, for larger diameters the pattern typically looks better. The final result with large area exposures are excellent zone plate patterns with 500 μm diameter and 100 nm outermost zone width. The total exposure time was 2 h 15 min. This relatively short time indicate that it will be practically possible to use the Raith system for these large area exposures.</p>

corrected abstract:
<p>In this diploma project the so-called fixed beam moving stage (FBMS) module in the Raith 150 electron beam lithography system has been evaluated for making large area zone plate exposures. The project goal, besides the evaluation of the module, has been to find an exposure recipe for exposing zone plates with diameter up to 500 µm. The zone plates fabricated with this method will be used for synchrotron and x-ray free electron laser applications.</p>
<p>The thesis starts with a short introduction to zone plate properties and fabrication procedures. Then the work where FBMS exposed zone plates are compared with normal write field exposures of 75 µm diameter zone plates is described. The conclusion is that for these small diameters, major problems with wobbly zones occur for the FBMS patterns. However, for larger diameters the pattern typically looks better. The final result with large area exposures are excellent zone plate patterns with 500 µm diameter and 100 nm outermost zone width. The total exposure time was 2 h 15 min. This relatively short time indicate that it will be practically possible to use the Raith system for these large area exposures.</p>

Note "μ" (the Greek mu) should be replaced by "µ" (micron).
----------------------------------------------------------------------
In diva2:743430 -- missing spaces in title:
"Large-scale dynamic optimizationusing code generation and parallelcomputing"
==>
"Large-scale dynamic optimization using code generation and parallel computing"


abstract is: 
<p>Complex physical models are becoming increasingly used in industry for simulation and optimization. Modeling languages such as Modelica allow creating model libraries of physical components, which in turn can be used to compose system models of, e.g., vehicle systems, power plants and electronic systems. JModelica.org is an open source tool suite for Modelica. It includes a compiler for Modelica and for the language extension Optimica, which is used to formulate dynamic optimization problems based on Modelica models.</p><p>Direct collocation methods are used in JModelica.org to transcribe a dynamic optimization problem into a large-scale nonlinear program with sparse structure. This structure can be exploited for parallel solution of the linear <em>Karush Kuhn Tucker </em>KKT system solved in each step of an interior point method. Currently, the non-linear programming solvers available in JModelica.org do not support parallel algorithms for the solution of the structured problem.</p><p>The optimization platform in JModelica.org was modified to generate efficient C-code that could be linked with an external non-linear programming solver with parallel computation capability. CasADi, JModelica. org’s third party software, is used to generate C-code from the model equations, after collocation has been applied. Speed up factors over 20 were obtained for the compilation of the generated files, and the problem of non-compilable files was overcome.</p><p>A C++ interface was implemented to link the generated files with the parallel framework that solves non-linear problems with an interior point algorithm.A large-scale trajectory optimization problem, such as the start-up optimization of a power cycle model, was used to evaluate the performance of the interface and the parallel algorithm. Speedup of over 2.5 was obtained with 4 processors in a shared memory architecture.</p>

corrected abstract:
<p>Complex physical models are becoming increasingly used in industry for simulation and optimization. Modeling languages such as Modelica allow creating model libraries of physical components, which in turn can be used to compose system models of, e.g., vehicle systems, power plants and electronic systems. JModelica.org is an open source tool suite for Modelica. It includes a compiler for Modelica and for the language extension Optimica, which is used to formulate dynamic optimization problems based on Modelica models.</p><p>Direct collocation methods are used in JModelica.org to transcribe a dynamic optimization problem into a large-scale nonlinear program with sparse structure. This structure can be exploited for parallel solution of the linear <em>Karush Kuhn Tucker</em> KKT system solved in each step of an interior point method. Currently, the non-linear programming solvers available in JModelica.org do not support parallel algorithms for the solution of the structured problem.</p><p>The optimization platform in JModelica.org was modified to generate efficient C-code that could be linked with an external non-linear programming solver with parallel computation capability. CasADi, JModelica.org’s third party software, is used to generate C-code from the model equations, after collocation has been applied. Speed up factors over 20 were obtained for the compilation of the generated files, and the problem of non-compilable files was overcome.</p><p>A C++ interface was implemented to link the generated files with the parallel framework that solves non-linear problems with an interior point algorithm.A large-scale trajectory optimization problem, such as the start-up optimization of a power cycle model, was used to evaluate the performance of the interface and the parallel algorithm. Speedup of over 2.5 was obtained with 4 processors in a shared memory architecture.</p>

Note error:
w='algorithm.A' val={'c': 'algorithm. A', 's': 'diva2:743430', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:651381   - correct as is
----------------------------------------------------------------------
In diva2:1286102 - missing space oin title:
"Latent Task Embeddings forFew-Shot Function Approximation"
==>
"Latent Task Embeddings for Few-Shot Function Approximation"

abstract is: 
<p>Approximating a function from a few data points is of great importance in fields where data is scarce, like, for example, in robotics applications. Recently, scalable and expressive parametric models like deep neural networks have demonstrated superior performance on a wide variety of function approximation tasks when plenty of data is available –however, these methods tend to perform considerably worse in low-data regimes which calls for alternative approaches. One way to address such limitations is by leveraging prior information about the function class to be estimated when such data is available. Sometimes this prior may be known in closed mathematical form but in general it is not. This the-sis is concerned with the more general case where the prior can only be sampled from, such as a black-box forward simulator. To this end, we propose a simple and scalable approach to learning a prior over functions by training a neural network on data from a distribution of related functions. This steps amounts to building a so called latent task embedding where all related functions (tasks) reside and which later can be efficiently searched at task-inference time - a process called fine-tuning. The pro-posed method can be seen as a special type of auto-encoder and employs the same idea of encoding individual data points during training as the recently proposed Conditional Neural Processes. We extend this work by also incorporating an auxiliary task and by providing additional latent space search methods for increased performance after the initial training step. The task-embedding framework makes finding the right function from a family of related function quick and generally requires only a few informative data points from that function. We evaluate the method by regressing onto the harmonic family of curves and also by applying it to two robotic systems with the aim of quickly identifying and controlling those systems.</p>

corrected abstract:
<p>Approximating a function from a few data points is of great importance in fields where data is scarce, like, for example, in robotics applications. Recently, scalable and expressive parametric models like deep neural networks have demonstrated superior performance on a wide variety of function approximation tasks when plenty of data is available – however, these methods tend to perform considerably worse in low-data regimes which calls for alternative approaches. One way to address such limitations is by leveraging prior information about the function class to be estimated when such data is available. Sometimes this prior may be known in closed mathematical form but in general it is not. This thesis is concerned with the more general case where the prior can only be sampled from, such as a black-box forward simulator. To this end, we propose a simple and scalable approach to learning a prior over functions by training a neural network on data from a distribution of related functions. This steps amounts to building a so called latent task embedding where all related functions (tasks) reside and which later can be efficiently searched at task-inference time - a process called fine-tuning. The proposed method can be seen as a special type of auto-encoder and employs the same idea of encoding individual data points during training as the recently proposed <em>Conditional Neural Processes</em>. We extend this work by also incorporating an auxiliary task and by providing additional latent space search methods for increased performance after the initial training step. The task-embedding framework makes finding the right function from a family of related function quick and generally requires only a few informative data points from that function. We evaluate the method by regressing onto the harmonic family of curves and also by applying it to two robotic systems with the aim of quickly identifying and controlling those systems.</p>

Note removed unnecessary hyphens, added space betwen hyphen and "however", and added italics
----------------------------------------------------------------------
In diva2:1817230 
abstract is: 
<p>The automotive industry has been involved in making vehicles autonomous to different levels in the past decade rapidly. Particularly in the commercial vehicle market, there is a significant necessity to make trucks have a certain level of automation to help reduce dependence on human efforts to drive. This could help in reducing several accidents caused by human error. Interestingly there are several challenges and solutions in achieving and implementing autonomous driving for trucks. First, a benchmark of different control architectures that can make a truck drive autonomously are explored. The chosen controllers (Pure Pursuit, Stanley, Linear Quadratic Regulator, Sliding Mode Control and Model Predictive Control) vary in their simplicity in implementation and versatility in handling different vehicle parameters and constraints. A thorough comparison of these path tracking controllers are performed using several metrics. Second, a collision avoidance system based on cubic polynomials, inspired by rapidly exploring random tree (RRT) is presented. Some of the path tracking controllers are limited by their ability and hence a standalone collision avoidance system is needed to provide safe maneuvering. Simulations are performed for different test cases with and without obstacles. These simulations help compare safety margin and driving comfort of each path tracking controller that are integrated with the collision avoidance system. Third, different performance metrics like change in acceleration input, change in steering input, error in path tracking, deviation from base frame of track file and lateral and longitudinal margin between ego and target vehicle are presented. To conclude, a set of suitable controllers for heavy articulated vehicles are developed and benchmarked.</p>

corrected abstract:
<p>The automotive industry has been involved in making vehicles autonomous to various levels in the past decade. Particularly in the commercial vehicle market, there is a significant need to equip trucks with a certain level of automation to reduce dependence on human efforts for driving. This could help reduce accidents caused by human error. Interestingly, there are several challenges and solutions in achieving and implementing autonomous driving for trucks. First, a comparison of different control architectures that can enable a truck to drive autonomously is explored.  The selected controllers (Pure Pursuit, Stanley, Linear Quadratic Regulator, Sliding Mode Control, and Model Predictive Control) vary in simplicity of implementation and versatility in handling various vehicle parameters and constraints. A thorough comparison of these path-tracking controllers is conducted using several metrics. Second, a collision avoidance system based on cubic polynomials, inspired by rapidly exploring random trees (RRT), is presented. Some of the path-tracking controllers have limitations, necessitating a standalone collision avoidance system to ensure safe maneuvering. Simulations are performed for different test cases with and without obstacles. These simulations help compare the safety margin and driving comfort of each path-tracking controller integrated with the collision avoidance system. Third, various performance metrics such as changes in acceleration input, changes in steering input, path tracking errors, deviations from the base frame of the track file, and lateral and longitudinal margins between the ego vehicle and the target vehicle are presented. In conclusion, a set of suitable controllers for heavy articulated vehicles is developed and benchmarked.</p>

Note major change in wording in the original
----------------------------------------------------------------------
In diva2:1699454 
abstract is: 
<p>Computed Tomography (CT) is a non-invasive x-ray imaging method capable of reconstructing highly detailed cross-sectional interior maps of an object. CT is used in a range of medical applications such as detection of skeletal fractures, organ trauma and artery calcification. Reconstructing CT images requires the use of a forward operator, which is essentially a simulation of the scanning process. Photon-Counting CT is a rapidly developing alternative to conventional CT that promises higher spatial resolution, more accurate material separation and more robust reconstructions. A major difficulty in Photon-Counting CT is to model cross-talk between detectors. One way is to incorporate a wide point-spread function into the forward operator. Although this method works, it drastically slows down the reconstruction process. </p><p>In this thesis, we accelerate image reconstruction tasks for photon-counting CT by approximating the cross-talk component of the forward operator with a deep neural network, resulting in a learned forward operator. The learned operator reduces reconstruction error by an order of magnitude at the cost of a 20% increase in computation time, compared to ignoring cross-talk altogether. Furthermore, it generalises well to both unseen data and unseen detector settings. Our results indicate that a learned forward operator is a suitable way of approximating the forward operator in photon-counting CT.</p>

corrected abstract:
<p>Computed Tomography (CT) is a non-invasive x-ray imaging method capable of reconstructing highly detailed cross-sectional interior maps of an object. CT is used in a range of medical applications such as detection of skeletal fractures, organ trauma and artery calcification. Reconstructing CT images requires the use of a forward operator, which is essentially a simulation of the scanning process. Photon-Counting CT is a rapidly developing alternative to conventional CT that promises higher spatial resolution, more accurate material separation and more robust reconstructions. A major difficulty in Photon-Counting CT is to model cross-talk between detectors. One way is to incorporate a wide point-spread function into the forward operator. Although this method works, it drastically slows down the reconstruction process.</p><p>In this thesis, we accelerate image reconstruction tasks for photon-counting CT by approximating the cross-talk component of the forward operator with a deep neural network, resulting in a learned forward operator. The learned operator reduces reconstruction error by an order of magnitude at the cost of a 20% increase in computation time, compared to ignoring cross-talk altogether. Furthermore, it generalises well to both unseen data and unseen detector settings. Our results indicate that a learned forward operator is a suitable way of approximating the forward operator in photon-counting CT.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1583517   - correct as is
----------------------------------------------------------------------
In diva2:1372108 
abstract is: 
<p>Generating summaries of long text articles is a common application in natural language processing. Automatic text summarization models often find themselves generating summaries that don’t resemble the quality of human written text, even though they preserve factual accuracy. In this thesis, a method to improve quality of summaries is created by combining loss functions from an existing baseline competitive model (Pointer Generator Networks) for abstractive text summarization with SeqGAN - a successful text generation algorithm based on Generative Adversarial Networks. The model is tested on the CNN/Daily Mail dataset of news articles. The results show that the summaries generated by the model are more human-like in quality as well as accurate in content than the baseline model.</p>

corrected abstract:
<p>Generating summaries of long text articles is a common application in natural language processing. Automatic text summarization models tend to find themselves generating summaries that don’t resemble the quality of human written text, even though they preserve factual accuracy. In this thesis, we propose a quality metric using convolutional neural networks to evaluate the quality of text produced. Using this metric, we finetune a baseline generative model (Pointer Generator Networks) in a GAN-based approach by optimizing a combined loss function - one from the baseline generative model and another from the discriminative model. The discriminator’s loss serves as a qualitative training signal for finetuning the generator. The model is tested on the CNN / Daily Mail dataset of news articles. The results show that the summaries generated by the model are more well-written as well as accurate in content.</p>

Note major difference in wording in the original
----------------------------------------------------------------------
In diva2:796751   - correct as is
-- The text comes from the "Executive summary"
----------------------------------------------------------------------
In diva2:1499401 
abstract is: 
<p>Fiber reinforced plastics are composite materials that offer a lower weight, while still mechanically perform at least as good as conventional materials such as steel. This makes them attractive for the automotive industry since the implementation of them in e.g. a car frame would enable the manufacturers to sell a more fuel efficient vehicle to the customer. The manufacturing of composites is however more energy intense than for steel and the recycling capabilities are limited. This encourages the car designer to regard the product from a macro-perspective, spanning from the extraction of the resources needed to produce the material, to the phase where the product which the material constitutes is disposed. By analyzing such a macro-perspective, the life cycle energy of a product system can be estimated. Since the life cycle energy is correlated to the component design, an optimization problem can be established where the objective function to be minimized is the total life cycle energy. The component design can be expressed in terms of optimization design variables, yielding that the minimum energy is achieved by the optimal design. This methodology is called life cycle energy optimization (LCEO). The aim of this thesis is to apply this method and present a comparison between different materials and recycling strategies for a load carrying frame component provided by Volvo Cars. The materials studied are carbon fiber reinforced plastics (CFRP), glass fiber sheet moulding compound (GF-SMC) and conventional steel. A Python model consisting of five life cycle phases where each phase was described by a function was implemented. Each function uses the component geometry and material properties as an input and gives the energy of the phase as an output. By summing the outputted energies, the life cycle energy is obtained. The distribution of the results is visualized with bar plots. The results show that the least energy demanding option is to manufacture the component in GF-SMC and process the end-of-life product mechanically. If the fiber degradation is taken into account, the most efficient strategy is to manufacture the component in CFRP and recycle it using solvolysis. This thesis shows that the LCEO methodology can be used as a tool for designers to include the recyclability in an early phase of the product development. Future challenges concern the development of industrial recycling of fiber reinforced plastics where the fiber degradation is minimized.</p>

corrected abstract:
<p>Fiber reinforced plastics are composite materials that offer a lower weight, while still mechanically perform at least as good as conventional materials such as steel. This makes them attractive for the automotive industry since the implementation of them in e.g. a car frame would enable the manufacturers to sell a more fuel efficient vehicle to the customer. The manufacturing of composites is however more energy intense than for steel and the recycling capabilities are limited. This encourages the car designer to regard the product from a macro-perspective, spanning from the extraction of the resources needed to produce the material, to the phase where the product which the material constitutes is disposed. By analyzing such a macro-perspective, the life cycle energy of a product system can be estimated.</p><p>Since the life cycle energy is correlated to the component design, an optimization problem can be established where the objective function to be minimized is the total life cycle energy. The component design can be expressed in terms of optimization design variables, yielding that the minimum energy is achieved by the optimal design. This methodology is called life cycle energy optimization (LCEO). The aim of this thesis is to apply this method and present a comparison between different materials and recycling strategies for a load carrying frame component provided by Volvo Cars. The materials studied are carbon fiber reinforced plastics (CFRP), glass fiber sheet moulding compound (GF-SMC) and conventional steel.</p><p>A Python model consisting of five life cycle phases where each phase was described by a function was implemented. Each function uses the component geometry and material properties as an input and gives the energy of the phase as an output. By summing the outputted energies, the life cycle energy is obtained. The distribution of the results is visualized with bar plots. The results show that the least energy demanding option is to manufacture the component in GF-SMC and process the end-of-life product mechanically. If the fiber degradation is taken into account, the most efficient strategy is to manufacture the component in CFRP and recycle it using solvolysis. This thesis shows that the LCEO methodology can be used as a tool for designers to include the recyclability in an early phase of the product development. Future challenges concern the development of industrial recycling of fiber reinforced plastics where the fiber degradation is minimized.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1380123 
abstract is: 
<p>In order to reduce fuel consumption of aircraft engines, components are constantly optimized. This improvement is pushing component’s limits to the farthest they can handle. That’s why engine manufacturers have to guarantee a minimum engine lifespan. The evaluation of these lifespans is made on each component. This document highlights how the modeling of frictional contact can directly impact the predicted lifespan inside turbine’s bolted connections of an aircraft engine. Research study on finite element allowed a better modeling of contacts inside models. The understanding of phenomenon involves in contact problem increased with the improvement of these modeling progress. Results extracted from this document show the complexity of a modeling a problem containing multiple frictional contacts.</p>

corrected abstract:
<p>In order to reduce fuel consumption of aircraft engines, components are constantly optimized. This improvement is pushing component’s limits to the farthest they can handle. That’s why engine manufacturers have to guarantee a minimum engine lifespan. The evaluation of these lifespans is made on each component. This document highlights how the modeling of frictional contact can directly impact the predicted lifespan inside turbine’s bolted connections of an aircraft engine.</p><p>Research study on finite element allowed a better modeling of contacts inside models. The understanding of phenomenon involves in contact problem increased with the improvement of these modeling progress.</p><p>Results extracted from this document show the complexity of a modeling a problem containing multiple frictional contacts.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1040703 
abstract is: 
<p>In the thesis, a lifting body has been designed aiming to generate lift force for the pentacopter, called TILT LR (Long Range), at higher velocities during flights to improve the aerodynamic performances. The configuration, which is used as the skeleton of the long range drone for up to 75 kilometers flights, is based upon a tilting system allowing the rotors to rotate around their own axis in both pitch and roll angles. This offers the possibility to the TILT LR flying withoutany vertical excess thrust at a proper angle of attack and velocity. This new drone can be directly applied to missions require long flight time or cover long distance, such as Search &amp; Rescue(SAR), power lines and off-shore structures inspection, fire monitoring or surveillance. Several main CAD models have been created during the process of design and presented in the report together with the final design. For each model in the process, CFD simulations have been applied to observe the behaviors of the flows around the surfaces of the body during steady flights, followed by a brief analysis for further modification. A series of simulations with varying velocities and angle of attack have been performed for the final design, analyzing its performances under different air conditions. Flight envelope of the design has been presented also, together with some ideas of possible further studies on the pentacopter.</p>
mc='withoutany' c='without any'

partal corrected: diva2:1040703: <p>In the thesis, a lifting body has been designed aiming to generate lift force for the pentacopter, called TILT LR (Long Range), at higher velocities during flights to improve the aerodynamic performances. The configuration, which is used as the skeleton of the long range drone for up to 75 kilometers flights, is based upon a tilting system allowing the rotors to rotate around their own axis in both pitch and roll angles. This offers the possibility to the TILT LR flying without any vertical excess thrust at a proper angle of attack and velocity. This new drone can be directly applied to missions require long flight time or cover long distance, such as Search &amp; Rescue(SAR), power lines and off-shore structures inspection, fire monitoring or surveillance. Several main CAD models have been created during the process of design and presented in the report together with the final design. For each model in the process, CFD simulations have been applied to observe the behaviors of the flows around the surfaces of the body during steady flights, followed by a brief analysis for further modification. A series of simulations with varying velocities and angle of attack have been performed for the final design, analyzing its performances under different air conditions. Flight envelope of the design has been presented also, together with some ideas of possible further studies on the pentacopter.</p>

corrected abstract:
<p>In the thesis, a lifting body has been designed aiming to generate lift force for the pentacopter, called TILT LR (Long Range), at higher velocities during flights to improve the aerodynamic performances. The configuration, which is used as the skeleton of the long range drone for up to 75 kilometers flights, is based upon a tilting system allowing the rotors to rotate around their own axis in both pitch and roll angles. This offers the possibility to the TILT LR flying without any vertical excess thrust at a proper angle of attack and velocity. This new drone can be directly applied to missions require long flight time or cover long distance, such as Search &amp; Rescue (SAR), power lines and off-shore structures inspection, fire monitoring or surveillance.</p><p>Several main CAD models have been created during the process of design and presented in the report together with the final design. For each model in the process, CFD simulations have been applied to observe the behaviors of the flows around the surfaces of the body during steady flights, followed by a brief analysis for further modification. A series of simulations with varying velocities and angle of attack have been performed for the final design, analyzing its performances under different air conditions. Flight envelope of the design has been presented also, together with some ideas of possible further studies on the pentacopter.</p>

Note added missing paragraph breaks and added space before "(SAR)"
----------------------------------------------------------------------
In diva2:1679027 
abstract is: 
<p>Metal oxide semiconductor (MOS) gas sensors have proven to be useful in many applications, ranging from detection of hazardous gases to monitoring of air quality. The demand for power efficient and high performance gas sensors has seen an increase in situations facing contemporary society. Currently it is common for sensors to employ an energy inefficient heater to provide for the optimal working temperature of the sensor. Light activation has been proposed as an alternative that could possibly improve modern gas sensors by decreasing energy utilization as well as increasing sensitivity and selectivity. The purpose of the following project is to explore the mechanisms and characteristics of light activated gas sensing using cuprous oxide (Cu2O), such that the findings may contribute to the development of power efficient gas sensors able to distinguish between gases at low concentrations. Several Cu2O-sensors with thicknesses of 300, 500 and 700 nm were examined, many of which also were doped with materials such as silver, graphene and titanium. Multiple types of measurements were performed where the sensors were exposed to nitrogen and carbon dioxide gas under illumination from one of three distinct light sources. The results show that conditions such as low light intensities, doping the sensors and air as the operating environment (compared to nitrogen gas) are beneficial for the carbon dioxide response under light activation. However, these findings are only indications and would need confirmation by additional measurements, both in terms of variation and repetition, under improved conditions.</p>

corrected abstract:
<p>Metal oxide semiconductor (MOS) gas sensors have proven to be useful in many applications, ranging from detection of hazardous gases to monitoring of air quality. The demand for power efficient and high performance gas sensors has seen an increase in situations facing contemporary society. Currently it is common for sensors to employ an energy inefficient heater to provide for the optimal working temperature of the sensor. Light activation has been proposed as an alternative that could possibly improve modern gas sensors by decreasing energy utilization as well as increasing sensitivity and selectivity. The purpose of the following project is to explore the mechanisms and characteristics of light activated gas sensing using cuprous oxide (Cu<sub>2</sub>O), such that the findings may contribute to the development of power efficient gas sensors able to distinguish between gases at low concentrations. Several Cu<sub>2</sub>O-sensors with thicknesses of 300, 500 and 700 nm were examined, many of which also were doped with materials such as silver, graphene and titanium. Multiple types of measurements were performed where the sensors were exposed to nitrogen and carbon dioxide gas under illumination from one of three distinct light sources. The results show that conditions such as low light intensities, doping the sensors and air as the operating environment (compared to nitrogen gas) are beneficial for the carbon dioxide response under light activation. However, these findings are only indications and would need confirmation by additional measurements, both in terms of variation and repetition, under improved conditions.</p>

Note added subscripts
----------------------------------------------------------------------
In diva2:549810 - thesis lacks abstract, the text comes from §2.1 "General"
abstract   - correct as is
----------------------------------------------------------------------
In diva2:1373620 
abstract is: 
<p>We consider random symmetric Toeplitz matrices of size n. Assuming that the entries on the diagonals are independent centered random variables with finite γ-th moment (γ&gt;2), a law of large numbers is established for the largest eigenvalue. Following the approach of Sen and Virág (2013), in the limit of large n, the largest rescaled eigenvalue is shown to converge to the limit 0.8288... . The background theory is explained and some symmetry results on the eigenvectors of the Toeplitz matrix and an auxiliary matrix are presented. A numerical investigation illustrates the rate of convergence and the oscillatory nature of the eigenvectors of the Toeplitz matrix. Finally, the possibility of proving a limiting distribution for the largest eigenvalue is discussed, and suggestions for future research are made.</p>

corrected abstract:
<p>We consider random symmetric Toeplitz matrices of size 𝑛. Assuming that entries on every subdiagonal are independent centered random variables with finite γth moment (γ &gt; 2), a law of large numbers is established for the largest eigenvalue. Following the approach of Sen and Virág (2013), in the large 𝑛 limit, the largest eigenvalue rescaled by √<span style="text-decoration: overline;">2𝑛 log 𝑛</span> is shown to converge to the limit 0.8288... . The background theory is explained and some symmetry results on the eigenvectors of the Toeplitz matrix and an auxiliary matrix are presented. A numerical investigation illustrates the rate of convergence and the oscillatory nature of the eigenvectors of the Toeplitz matrix. Finally, the possibility of proving a limiting distribution for the largest eigenvalue is discussed, and suggestions for future research are made.</p>

Note fixed wording and equations
----------------------------------------------------------------------
In diva2:1119957 
abstract is: 
<p>The purpose of this report is to provide a method of reproducing the stiffness of a set of rubber bushings, in three translational and three rotational directions, based on measurements carried out on them, using a single linear FEM element. The model should be simple and useful for use in, for example, multi-body simulation, where you look at the behavior of an entire system. In order to save computation time, simplifications are made; in this case, a linear simplification of a material that in reality shows highly non-linear behavior. Only the stiffness is modeled and the damping is ignored. The report presents the element itself, the form in which the result is given, and the content based on the measurements. It also discusses what constraints the method has because of, among other things, material properties of rubber that can not be reproduced in a linear system. It is apparent from our analysis that the amplitude dependence of rubber will be a critical factor for how well our model works, and that the data we have is insufficient to make reliable extrapolation beyond the limits of the measurement range.</p>

corrected abstract:
<p>The purpose of this report is to provide a method of reproducing the stiffness of a set of rubber bushings, in three translational and three rotational directions, based on measurements carried out on them, using a single linear FEM element. The model should be simple and useful for use in, for example, multi-body simulation, where you look at the behavior of an entire system. In order to save computation time, simplifications are made; in this case, a linear simplification of a material that in reality shows highly non-linear behavior. Only the stiffness is modeled and the damping is ignored.</p><p>The report presents the element itself, the form in which the result is given, and the content based on the measurements. It also discusses what constraints the method has because of, among other things, material properties of rubber that can not be reproduced in a linear system. It is apparent from our analysis that the amplitude dependence of rubber will be a critical factor for how well our model works, and that the data we have is insufficient to make reliable extrapolation beyond the limits of the measurement range.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1800248 
abstract is: 
<p>Link prediction is a crucial task in many downstream applications of graph machine learning. Graph Neural Networks (GNNs) are a prominent approach for transductive link prediction, where the aim is to predict missing links or connections only within the existing nodes of a given graph. However, many real-life applications require inductive link prediction for the newly-coming nodes with no connections to the original graph. Thus, recent approaches have adopted a Multilayer Perceptron (MLP) for inductive link prediction based solely on node features. In this work, we show that incorporating both connectivity structure and features for the new nodes provides better model expressiveness. To bring such expressiveness to inductive link prediction, we propose LEAP, an encoder that features LEArnable toPology augmentation of the original graph and enables message passing with the newly-coming nodes. To the best of our knowledge, this is the first attempt to provide structural contexts for the newly-coming nodes via learnable augmentation under inductive settings. Conducting extensive experiments on four real- world homogeneous graphs demonstrates that LEAP significantly surpasses the state-of-the-art methods in terms of AUC and average precision. The improvements over homogeneous graphs are up to 22% and 17%, respectively. The code and datasets are available on GitHub*.</p>

corrected abstract:
<p>Link prediction is a crucial task in many downstream applications of graph machine learning. Graph Neural Networks (GNNs) are a prominent approach for transductive link prediction, where the aim is to predict missing links or connections only within the existing nodes of a given graph. However, many real-life applications require inductive link prediction for new nodes with no connections to the original graph. Thus, recent approaches have adopted a Multilayer Perceptron (MLP) for inductive link prediction based solely on node features. In this work, we show that incorporating both connectivity structure and features for the new nodes provides better model expressiveness. To bring such expressiveness to inductive link prediction, we propose LEAP, an encoder that features LEArnable toPology augmentation of the original graph and enables message passing with the newly-coming nodes. To the best of our knowledge, this is the first attempt to provide structural contexts for the new nodes via learnable augmentation under inductive settings. Conducting extensive experiments on four real-world, homogeneous graphs demonstrates that LEAP significantly surpasses the state-of-the-art methods in terms of AUC and average precision. The improvements over homogeneous graphs are up to 22% and 17%, respectively. The code and datasets are available on GitHub<sup><a href="#fn1" id="ref1">*</a></sup>.</p>
<div id="footnotes">
    <ul style="list-style: '*'; padding-left: 20px;">
        <li id="fn1"><a href="https://github.com/torileatherman/link_prediction_LEAP">https://github.com/torileatherman/link_prediction_LEAP</a> <a href="#ref1" aria-label="Back to reference">↩</a></li>
    </ul>
</div>

Note corrected the words to match th eoriginal, added missing paragraph break, and added footnote with URL
----------------------------------------------------------------------
In diva2:1216809 
abstract is: 
<p>Increasing environmental laws and regulations regarding emissions and air and water pollution in the transport sector. These laws and regulations combined with the limitation of oil as a natural resource have contributed to research regarding alternative fuels for vehicles. The electric cars have become a strong competitor for replacing the gasoline- driven cars of today. The power for many of these electric vehicles is stored in lithium- ion batteries, who have been questioned regarding their sustainability.</p><p>The purpose of this report is to provide a comprehensive view of the problematics and consequences combined with the implementation of these electric vehicles due to their lithium-ion batteries.</p><p>By collecting, analyzing and discussing research covering electric vehicles and their batteries, a conclusion is drawn with sustainability as a focus. The sustainability is measured regarding economic, environmental, political and social aspects. The research used in this study is collected from KTH Royal Institute of Technology’s library, consisting of peer-reviewed papers and books, supplemented by reports from reputable independent organizations.</p><p>The results of this study show that there are both positive and negative effects of the implementation of the batteries from a sustainability point of view. Changes need to be done in more sectors than the transport sector in order to make the lithium-ion batteries a sustainable solution.</p>

corrected abstract:
<p>Increasing environmental laws and regulations regarding emissions and air and water pollution in the transport sector. These laws and regulations combined with the limitation of oil as a natural resource have contributed to research regarding alternative fuels for vehicles. The electric cars have become a strong competitor for replacing the gasoline-driven cars of today. The power for many of these electric vehicles is stored in lithium-ion batteries, who have been questioned regarding their sustainability.</p><p>The purpose of this report is to provide a comprehensive view of the problematics and consequences combined with the implementation of these electric vehicles due to their lithium-ion batteries.</p><p>By collecting, analyzing and discussing research covering electric vehicles and their batteries, a conclusion is drawn with sustainability as a focus. The sustainability is measured regarding economic, environmental, political and social aspects. The research used in this study is collected from KTH Royal Institute of Technology’s library, consisting of peer-reviewed papers and books, supplemented by reports from reputable independent organizations.</p><p>The results of this study show that there are both positive and negative effects of the implementation of the batteries from a sustainability point of view. Changes need to be done in more sectors than the transport sector in order to make the lithium-ion batteries a sustainable solution.</p>

NOte minor changes with respect to hyphens
----------------------------------------------------------------------
In diva2:1335521 
abstract is: 
<p>Sound analysis in the web can have different meanings. Determining sound quality or recognising sounds for example. For this specific case it means sending data through sound from one device to another. With the help of mathematical methods such as Fast Fourier Transform and the development environment in the web browser. This is done by generating a particular sound identity from a given digit code. The problem at hand is an idea of simplifying user interaction in the web. When a regular user accesses a web page interaction is required to proceed with the task at hand. However, if the task is known for the developer, within a given setting, this task could be automated. Mentimeter is a tool for creating interactive presentations. The presenter prompts the audience to navigate to a specific page to insert a code. The audience can then join the interactivity of the presentation. The task at hand in this case is where the user should enter the code. It is within a given setting and the missing piece for automating this process is the technology for sending data with ease. One way of automating is with the use of sound and different sound frequencies. This report will explore the possibilities for communication through sound within the web. In addition this this, it will also explore the limits of sound analysis in the web.</p>

corrected abstract:
<p>Sound analysis in the web can have different meanings. Determining sound quality or recognising sounds for example. For this specific case it means sending data through sound from one device to another. With the help of mathematical methods such as Fast Fourier Transform and the development environment in the web browser. This is done by generating a particular sound identity from a given digit code.</p><p>The problem at hand is an idea of simplifying user interaction in the web. When a regular user accesses a web page interaction is required to proceed with the task at hand. However, if the task is known for the developer, within a given setting, this task could be automated. Mentimeter is a tool for creating interactive presentations. The presenter prompts the audience to navigate to a specific page to insert a code. The audience can then join the interactivity of the presentation. The task at hand in this case is where the user should enter the code. It is within a given setting and the missing piece for automating this process is the technology for sending data with ease.</p><p>One way of automating is with the use of sound and different sound frequencies. This report will explore the possibilities for communication through sound within the web. In addition this this, it will also explore the limits of sound analysis in the web.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1319711   - correct as is
----------------------------------------------------------------------
In diva2:866721   - correct as is
----------------------------------------------------------------------
In diva2:723400   - correct as is
----------------------------------------------------------------------
In diva2:1359785 
abstract is: 
<p>This project outlines a cost-effective numerical simulation method for the analysis of the longitudinal mechanics of paraglider systems. It is built on static stability methods for the analyses of low subsonic aircraft, non-linear lifting line methods for aerodynamic parameterization, and frequency domain analysis methods derived from system theory. Paragliders possess a glide polar in the range of ≈ 25-60 km.h−1 and display underdamped dynamic responses dominated by a long-period mode. The simulation results for performance and dynamic response are qualitatively valid relative to experimental data and in the same order of magnitude.</p>

corrected abstract:
<p>This project outlines a cost-effective numerical simulation method for the analysis of the longitudinal mechanics of paraglider systems. It is built on static stability methods for the analyses of low subsonic aircraft, non-linear lifting line methods for aerodynamic parameterization, and frequency domain analysis methods derived from system theory. Paragliders possess a glide polar in the range of ≈ 25-60 <em>km.h</em><sup>−1</sup> and display underdamped dynamic responses dominated by a long-period mode. The simulation results for performance and dynamic response are qualitatively valid relative to experimental data and in the same order of magnitude.</p>

Note added superscript and italics
----------------------------------------------------------------------
In diva2:971519   - correct as is
----------------------------------------------------------------------
In diva2:1590465 
abstract is: 
<p>A Kitaev quantum spin liquid is a phase of matter predicted to host excitations that can be used to preform fault-tolerant quantum computation. Though the theoretical prediction of such a state is on firm footing, its realisation in real materials has proven to be elusive. Recent developments have suggested honeycomb materials consisting of 3d transition metal ions as possible candidates. The focus of this thesis is the magnetic properties of one such material, K2Ni2–xCoxTeO6. It is part of a family of layered two dimensional materials consisting of honeycomb structured transition metal layers sandwiched between layers of alkali ions. A characterisation of the magnetic properties of K2Ni2–xCoxTeO6 has been carried out with the techniques of muon spin rotation/relaxation/resonance and bulk magnetisation as a function of the chemical composition. Further investigations of the detailed atomic structure and spin order using neutron scattering was also initiated. The results of such characterisations are presented and discussed in this thesis.</p>
skipping mc='isa'

partal corrected: diva2:1590465: <p>A Kitaev quantum spin liquid is a phase of matter predicted to host excitations that can be used to preform fault-tolerant quantum computation. Though the theoretical prediction of such a state is on firm footing, its realisation in real materials has proven to be elusive. Recent developments have suggested honeycomb materials consisting of 3d transition metal ions as possible candidates. The focus of this thesis is the magnetic properties of one such material, K2Ni2–xCoxTeO6. It is part of a family of layered two dimensional materials consisting of honeycomb structured transition metal layers sandwiched between layers of alkali ions. A characterisation of the magnetic properties of K2Ni2–xCoxTeO6 has been carried out with the techniques of muon spin rotation/relaxation/resonance and bulk magnetisation as a function of the chemical composition. Further investigations of the detailed atomic structure and spin order using neutron scattering was also initiated. The results of such characterisations are presented and discussed in this thesis.</p>

corrected abstract:
<p>A Kitaev quantum spin liquid is a phase of matter predicted to host excitations that can be used to preform fault-tolerant quantum computation. Though the theoretical prediction of such a state is on firm footing, its realisation in real materials has proven to be elusive. Recent developments have suggested honeycomb materials consisting of 3d transition metal ions as possible candidates. The focus of this thesis is the magnetic properties of one such material, K<sub>2</sub>Ni<sub>2–x</sub>Co<sub>x</sub>TeO<sub>6</sub>. It is part of a family of layered two dimensional materials consisting of honeycomb structured transition metal layers sandwiched between layers of alkali ions. A characterisation of the magnetic properties of K<sub>2</sub>Ni<sub>2–x</sub>Co<sub>x</sub>TeO<sub>6</sub> has been carried out with the techniques of muon spin rotation/relaxation/resonance and bulk magnetisation as a function of the chemical composition. Further investigations of the detailed atomic structure and spin order using neutron scattering was also initiated. The results of such characterisations are presented and discussed in this thesis.</p>

Note added subscripts
----------------------------------------------------------------------
In diva2:1141816   - correct as is
----------------------------------------------------------------------
In diva2:1334069 
abstract is: 
<p>The field of natural language processing has received increased attention lately, but less focus is put on comparing models, which differ in complexity. This thesis compares Random Forest to LSTM, for the task of classifying a message as question or non-question. The comparison was done by training and optimizing the models on historic chat data from the Swedish insurance company Hedvig. Different types of word embedding were also tested, such as Word2vec and Bag of Words. The results demonstrated that LSTM achieved slightly higher scores than Random Forest, in terms of F1 and accuracy. The models’ performance were not significantly improved after optimization and it was also dependent on which corpus the models were trained on.</p><p>An investigation of how a chatbot would affect Hedvig’s adoption rate was also conducted, mainly by reviewing previous studies about chatbots’ effects on user experience. The potential effects on the innovation’s five attributes, relative advantage, compatibility, complexity, trialability and observability were analyzed to answer the problem statement. The results showed that the adoption rate of Hedvig could be positively affected, by improving the first two attributes. The effects a chatbot would have on complexity, trialability and observability were however suggested to be negligible, if not negative.</p>

corrected abstract:
<p>The field of natural language processing has received increased attention lately, but less focus is put on comparing models, which differ in complexity. This thesis compares Random Forest to LSTM, for the task of classifying a message as <em>question</em> or <em>non-question</em>. The comparison was done by training and optimizing the models on historic chat data from the Swedish insurance company Hedvig. Different types of word embedding were also tested, such as <em>Word2vec</em> and <em>Bag of Words</em>. The results demonstrated that LSTM achieved slightly higher scores than Random Forest, in terms of F<sub>1</sub> and <em>accuracy</em>. The models’ performance were not significantly improved after optimization and it was also dependent on which corpus the models were trained on.</p><p>An investigation of how a chatbot would affect Hedvig’s <em>adoption rate</em> was also conducted, mainly by reviewing previous studies about chatbots’ effects on user experience. The potential effects on the innovation’s five attributes, <em>relative advantage</em>, <em>compatibility</em>, <em>complexity</em>, <em>trialability</em> and <em>observability</em> were analyzed to answer the problem statement. The results showed that the adoption rate of Hedvig could be positively affected, by improving the first two attributes. The effects a chatbot would have on complexity, trialability and observability were however suggested to be negligible, if not negative.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1673626 
abstract is: 
<p>One idea to stop the increasing threat of global warming is to build a space sunshade, made up of a constellation of satellites to reflect a portion of the sunlight. These satellites need to reach an equilibrium point where they can stay in orbit with minimal adjustments. A few feasibility studies have been made in this area and the location of the equilibrium point, L1' has been found. An area that is still unclear is the best way of getting the satellites to L1'. One idea is to use electric propulsion engines , and utilizing a gravity assist around the Moon on the way to L1'. Gravity assists have mostly been performed with chemical rocket engines, so doing it with electric propulsion causes some unique issues. The goal was to find a trajectory including the gravity assist showing that this could be an effective way of transporting the satellites. To measure how effective this would be, a trajectory without a gravity assist was also created as a means of comparison.</p><p>The trajectory and orbit simulations were done in the program General Missions Analysis Tool (GMAT), and the resulting trajectory took 51 days and used 83.4kg of fuel for a fuel-to-mass-ratio of 21.5%. This was a worse result compared to the trajectory without a gravity assist, which only used 80.4kg of fuel.</p><p>Finally a discussion around a potential trajectory which was shown to have a much greater velocity increase was had, which would indicate that a gravity assist maneuver could provide a trajectory that does save on fuel compared to using no gravity assist.</p>

corrected abstract:
<p>One idea to stop the increasing threat of global warming is to build a space sunshade, made up of a constellation of satellites to reflect a portion of the sunlight. These satellites need to reach an equilibrium point where they can stay in orbit with minimal adjustments. A few feasibility studies have been made in this area and the location of the equilibrium point, L<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>′</sup><sub>1</sub></span></span> has been found. An area that is still unclear is the best way of getting the satellites to L<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>′</sup><sub>1</sub></span></span>. One idea is to use electric propulsion engines , and utilizing a gravity assist around the Moon on the way to L<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>′</sup><sub>1</sub></span></span>. Gravity assists have mostly been performed with chemical rocket engines, so doing it with electric propulsion causes some unique issues. The goal was to find a trajectory including the gravity assist showing that this could be an effective way of transporting the satellites. To measure how effective this would be, a trajectory without a gravity assist was also created as a means of comparison.</p><p>The trajectory and orbit simulations were done in the program General Missions Analysis Tool (GMAT), and the resulting trajectory took 51 days and used 83.4kg of fuel for a fuel-to-mass-ratio of 21.5%. This was a worse result compared to the trajectory without a gravity assist, which only used 80.4kg of fuel.</p><p>Finally a discussion around a potential trajectory which was shown to have a much greater velocity increase was had, which would indicate that a gravity assist maneuver could provide a trajectory that does save on fuel compared to using no gravity assist.</p>

Note fixed stacking of L1 prime
----------------------------------------------------------------------
In diva2:550050   - correct as is
----------------------------------------------------------------------
In diva2:1247391 
abstract is: 
<p>This thesis is focused on the Occupational Pension, an important part of the retiree’s total pension. It is paid by private insurance companies and determined by an annuity divisor. Regression modeling of the annuity divisor is done by using the monthly paid pension as a response and a set of 24 explanatory variables e.g. the expected remaining lifetime and advance interest rate. Two machine learning algorithms, artificial neural networks (ANN) and support vector machines for regression (SVR) are considered in detail. Specifically, different transfer functions for ANN are studied as well as the possibility to improve the SVR model by incorporating a non-linear Gaussian kernel. To compare our result with prior experience of the Swedish Pensions Agency in modeling and predicting the annuity divisor, we also consider the ordinary multiple linear regression (MLR) model. Although ANN, SVR and MLR are of different nature, they demonstrate similar performance accuracy. It turns out that for our data that MLR and SVR with a linear kernel achieve the highest prediction accuracy. When performing feature selection, all methods except SVR with a Gaussian kernel encompass the features corresponding to advance interest rate and expected remaining lifetime, which according to Swedish law {Swedish law: 5 kap. 12 § lagen (1998:674) om inkomstgrundad ålderspension} are main factors that determine the annuity divisor. The results of this study confirm the importance of the two main factors for accurate modeling of the annuity divisor in private insurance. We also conclude that, in addition to the methods used in previous research, methods such as MLR, ANN and SVR may be used to accurately model the annuity divisor.</p>

corrected abstract:
<p>This thesis is focused on the Occupational Pension, an important part of the retiree’s total pension. It is paid by private insurance companies and determined by an annuity divisor. Regression modeling of the annuity divisor is done by using the monthly paid pension as a response and a set of 24 explanatory variables e.g. the expected remaining lifetime and advance interest rate. Two machine learning algorithms, artificial neural networks (ANN) and support vector machines for regression (SVR) are considered in detail. Specifically, different transfer functions for ANN are studied as well as the possibility to improve the SVR model by incorporating a non-linear Gaussian kernel. To compare our result with prior experience of the Swedish Pensions Agency in modeling and predicting the annuity divisor, we also consider the ordinary multiple linear regression (MLR) model. Although ANN, SVR and MLR are of different nature, they demonstrate similar performance accuracy. It turns out that for our data that MLR and SVR with a linear kernel achieve the highest prediction accuracy. When performing feature selection, all methods except SVR with a Gaussian kernel encompass the features corresponding to advance interest rate and expected remaining lifetime, which according to Swedish law<sup><a href="#fn1" id="ref1">1</a></sup> are main factors that determine the annuity divisor. The results of this study confirm the importance of the two main factors for accurate modeling of the annuity divisor in private insurance. We also conclude that, in addition to the methods used in previous research, methods such as MLR, ANN and SVR may be used to accurately model the annuity divisor.</p>
<div id="footnotes">
    <ol style="list-style: '1'; padding-left: 20px;">
        <li id="fn1">Swedish law: 5 kap. 12 § lagen (1998:674) om inkomstgrundad ålderspension <a href="#ref1" aria-label="Back to reference">↩</a></li>
    </ol>
</div>
----------------------------------------------------------------------
In diva2:1437694 
abstract is: 
<p>By using a fully convolutional neural network, accurate predictions are made of turbulent flows in an open channel. The flow fields were predicted at two different heights y+ = 15,50 in the wall-normal direction. The effect of different input parameters of the neural network have been studied by generating two neural network models. The first model takes the shear stress in the span- and streamwise directions aswell as the pressure, while the second model only uses the shear stresses. The statistics of the models were calculated and the pressure was determined to be an important parameter for predictions at the distance y+=50 from the wall. Closer to the wall, at y+ = 15, the pressure had a smaller effect on the error, and in the spanwise direction the prediction made by the two-input model had a lower error.</p>

corrected abstract:
<p>By using a fully convolutional neural network, accurate predictions are made of turbulent flows in an open channel. The flow fields were predicted at two different heights y<sup>+</sup> = 15, 50 in the wall-normal direction. The effect of different input parameters of the neural network have been studied by generating two neural network models. The first model takes the shear stress in the span- and streamwise directions aswell as the pressure, while the second model only uses the shear stresses. The statistics of the models were calculated and the pressure was determined to be an important parameter for predictions at the distance y<sup>+</sup> = 50 from the wall. Closer to the wall, at y<sup>+</sup> = 15, the pressure had a smaller effect on the error, and in the spanwise direction the prediction made by the two-input model had a lower error.</p>

Note fixed the equations and increase spacing between "15," and "50", as this is not "15,50" but rather two different heights
----------------------------------------------------------------------
In diva2:1431654   - correct as is
----------------------------------------------------------------------
In diva2:1386196 
abstract is: 
<p>The desire to model the true gain from targeting an individual in marketing purposes has lead to the common use of uplift modeling. Uplift modeling requires the existence of a treatment group as well as a control group and the objective hence becomes estimating the difference between the success probabilities in the two groups. Efficient methods for estimating the probabilities in uplift models are statistical machine learning methods. In this project the different uplift modeling approaches Subtraction of Two Models, Modeling Uplift Directly and the Class Variable Transformation are investigated. The statistical machine learning methods applied are Random Forests and Neural Networks along with the standard method Logistic Regression. The data is collected from a well established retail company and the purpose of the project is thus to investigate which uplift modeling approach and statistical machine learning method that yields in the best performance given the data used in this project. The variable selection step was shown to be a crucial component in the modeling processes as so was the amount of control data in each data set. For the uplift to be successful, the method of choice should be either the Modeling Uplift Directly using Random Forests, or the Class Variable Transformation using Logistic Regression. Neural network - based approaches are sensitive to uneven class distributions and is hence not able to obtain stable models given the data used in this project. Furthermore, the Subtraction of Two Models did not perform well due to the fact that each model tended to focus too much on modeling the class in both data sets separately instead of modeling the difference between the class probabilities. The conclusion is hence to use an approach that models the uplift directly, and also to use a great amount of control data in each data set.</p>

corrected abstract:
<p>The desire to model the true gain from targeting an individual in marketing purposes has lead to the common use of uplift modeling. Uplift modeling requires the existence of a treatment group as well as a control group and the objective hence becomes estimating the difference between the success probabilities in the two groups. Efficient methods for estimating the probabilities in uplift models are statistical machine learning methods. In this project the different uplift modeling approaches Subtraction of Two Models, Modeling Uplift Directly and the Class Variable Transformation are investigated. The statistical machine learning methods applied are Random Forests and Neural Networks along with the standard method Logistic Regression. The data is collected from a well established retail company and the purpose of the project is thus to investigate which uplift modeling approach and statistical machine learning method that yields in the best performance given the data used in this project. The variable selection step was shown to be a crucial component in the modeling processes as so was the amount of control data in each data set. For the uplift to be successful, the method of choice should be either the Modeling Uplift Directly using Random Forests, or the Class Variable Transformation using Logistic Regression. Neural network - based approaches are sensitive to uneven class distributions and is hence not able to obtain stable models given the data used in this project. Furthermore, the Subtraction of Two Models did not perform well due to the fact that each model tended to focus too much on modeling the class in both data sets separately instead of modeling the difference between the class probabilities. The conclusion is hence to use an approach that models the uplift directly, and also to use a great amount of control data in the data set.</p>

Note "each" whould be "the" to match the original
----------------------------------------------------------------------
In diva2:1585633 
abstract is: 
<p>Constitutive modeling of nonlinear materials is a computationally complex and time-intensive process. To mitigate this, a neural network was used to simulate complex behaviors of non-linear materials. With supervised learning the machine learning model was able to predict the stresses in a material when given the strains. The machine learning algorithm predicted stresses in a linear elastic material with high accuracy, and in a hyperelastic material with lower accuracy. To simulate experimental conditions, artificial Gaussian noise was added to the strain data, and the model was tested with the new input.</p>

corrected abstract:
<p>Constitutive modelling of nonlinear materials is a computationally complex and time-intensive process. To mitigate this, a simple feedforward neural network was used to simulate complex mechanical behaviour of non-linear materials. The machine learning model was trained with supervised learning and was able to predict the stresses in a linear elastic material when given the strains.  The model was unable to predict the stresses for a nonlinear hyperelastic material with high prediction accuracy. As experimental data was unavailable, experimental conditions were simulated by adding randomly generated Gaussian noise to the strain input data, and the model was tested with the new input.  The model predictably delivered results with lower accuracy when the noise was added, but it was still able to simulate the behaviour of the linear material, while the hyperelastic behaviour prediction was still inaccurate.</p>

Note large difference between the wording
----------------------------------------------------------------------
In diva2:1294504 
abstract is: 
<p>Smart Meters are measuring devices collecting labeled time series data of utility consumptions from sub-meters and are capable of automatically transmit-ting this between the customer and utility companies together with other companies that offer services such as monitoring of consumption and cleaning of data. The smart meters are in some cases experiencing communication errors. One such error occurs when the information about what the utility sub-meters are measuring is lost. This information is important for when the producers of the utility are billing the customers for their usage.</p><p>The information has had to be collected manually, something which is inefficient in terms of time and money. In this thesis a method for classifying the meters based on their raw time series data is investigated. The data used in the thesis comes from Metry AB and contains thousands of time series in five different classes. The task is complicated by the fact that the data has a high class imbalance, contains many missing values and that the time series vary substantially in length.</p><p>The proposed method is based on partitioning the time series into slices of equal size and training a Deep Neural Network (DNN) together with a Bayesian Neural Network (BNN) to classify the slices. Prediction on new time series is performed by the prediction of individual slices for that time series followed by a voting procedure. The method is justified through a set of assumptions about the underlying stochastic process generating the time series coupled with an analysis based on the multinomial distribution.</p><p>The results indicate that the models tend to perform worse on the samples coming from the classes ”water” and ”hot water” and that the worst performance is on the ”hot water”-class. On all the classes the models achieve accuracies of around 60%, by excluding the ”hot water” class it is possible to achieve accuracies of at least 70% on the data set. The models perform worse on time series that contain a few number of good quality slices, by considering only time series which has many good quality slices, accuracies of 70% are achieved for all classes and above 80% when excluding ”Hot Water”.</p><p>It is concluded that in order to further improve the classification performance, more data is needed. Drawbacks with the method are the increased number of hyper-parameters involved in the extraction of slices. However, the voting method seems promising enough to investigate further on more highly sparse data sets.</p>

corrected abstract:
<p>Smart Meters are measuring devices collecting labeled time series data of utility consumptions from sub-meters and are capable of automatically transmitting this between the customer and utility companies together with other companies that offer services such as monitoring of consumption and cleaning of data. The smart meters are in some cases experiencing communication errors. One such error occurs when the information about what the utility sub-meters are measuring is lost. This information is important for when the producers of the utility are billing the customers for their usage.</p><p>The information has had to be collected manually, something which is inefficient in terms of time and money. In this thesis a method for classifying the meters based on their raw time series data is investigated. The data used in the thesis comes from Metry AB and contains thousands of time series in five different classes. The task is complicated by the fact that the data has a high class imbalance, contains many missing values and that the time series vary substantially in length.</p><p>The proposed method is based on partitioning the time series into slices of equal size and training a Deep Neural Network (DNN) together with a Bayesian Neural Network (BNN) to classify the slices. Prediction on new time series is performed by the prediction of individual slices for that time series followed by a voting procedure. The method is justified through a set of assumptions about the underlying stochastic process generating the time series coupled with an analysis based on the multinomial distribution.</p><p>The results indicate that the models tend to perform worse on the samples coming from the classes ”water” and ”hot water” and that the worst performance is on the ”hot water”-class. On all the classes the models achieve accuracies of around 60%, by excluding the ”hot water” class it is possible to achieve accuracies of at least 70% on the data set. The models perform worse on time series that contain a few number of good quality slices, by considering only time series which has many good quality slices, accuracies of 70% are achieved for all classes and above 80% when excluding ”Hot Water”.</p><p>It is concluded that in order to further improve the classification performance, more data is needed. Drawbacks with the method are the increased number of hyper-parameters involved in the extraction of slices. However, the voting method seems promising enough to investigate further on more highly sparse data sets.</p>

Note eliminate an unnecessary hyphen
----------------------------------------------------------------------
In diva2:1437702   - correct as is
----------------------------------------------------------------------
In diva2:1450548 
abstract is: 
<p>Small and medium-sized companies constitute a large part of the Swedish economy and are to a great extent exposed to the developments in the macroeconomy. There is a general consensus that it exists a relationship between these two components, but to what dimension is it true? The aim of this thesis is to evaluate if the number of bankruptcies among small and medium-sized companies can be explained by the situation in the macroeconomy. In order to do so, data have been collected and a multiple linear regression analysis has been accomplished.</p><p>The result of the analysis suggests that a model of the six macroeconomic factors months, CPI, retail sales, OMX30, total enterprises and liquidated enterprises, can explain the number of bankruptcies to an extent of 64.49%. When comparing the adequacy of other models used to estimate risks of bankruptcy, it is stated that other models are more accurate. Furthermore, we have concluded that the model is useful to bring insight but only when considered in combinations with other models and tools.</p>

corrected abstract:
<p>Small and medium-sized companies constitute a large part of the Swedish economy and are to a great extent exposed to the developments in the macroeconomy. There is a general consensus that it exists a relationship between these two components, but to what dimension is it true? The aim of this thesis is to evaluate if the number of bankruptcies among small and medium-sized companies can be explained by the situation in the macro economy. In order to do so, data have been collected and a multiple linear regression analysis has been accomplished.</p><p>The result of the analysis suggests that a model of the six macroeconomic factors <em>months</em>, <em>CPI</em>, <em>retail sales</em>, <em>OMX30</em>, <em>total enterprises</em> and <em>liquidated enterprises</em>, can explain the number of bankruptcies to an extent of 64.49%. When comparing the adequacy of other models used to estimate risks of bankruptcy, it is stated that other models are more accurate. Furthermore, we have concluded that the model is useful to bring insight but only when considered in combinations with other models and tools.</p>

Note added space after "macro" and added iatlics
----------------------------------------------------------------------
In diva2:1114471 
abstract is: 
<p>This thesis in Applied Mathematics and Industrial Economics examines which macroeconomic factors, related to the business cycle, that correlate with the performance of Industrial Transportation Companies. The data for the thesis is collected with the help of Nordea and from reports of each variable. The observations stretch from January 2007 to December 2016, a ten-year period. The data is monthly, hence there are 120 observed data points for each variable. A linear regression analysis was performed with the result that there is a linear relationship between the performance of the Industrial Transportation Companies and the variables Price per MWh, Fuel Price, Exchange rate USD-SEK, Exchange rate EUR-SEK, Manpower Employment Outlook Survey, Repo Rate, OECD Index Sweden, and Purchasing Manager’s Index. Further analysis of the Swedish and international economic history in the last decade was conducted which concluded why said variables were significant.</p>

corrected abstract:
<p>This thesis in Applied Mathematics and Industrial Economics examines which macroeconomic factors, related to the business cycle, that correlate with the performance of Industrial Transportation Companies. The data for the thesis is collected with the help of Nordea and from reports of each variable. The observations stretch from January 2007 to December 2016, a ten-year period. The data is monthly, hence there are 120 observed data points for each variable. A linear regression analysis was performed with the result that there is a linear relationship between the performance of the Industrial Transportation Companies and the variables <em>Price per MWh</em>, <em>Fuel Price</em>, <em>Exchange rate USD-SEK</em>, <em>Exchange rate EUR-SEK</em>, <em>Manpower Employment Outlook Survey</em>, <em>Repo Rate</em>, <em>OECD Index Sweden</em>, and <em>Purchasing Manager’s Index</em>. Further analysis of the Swedish and international economic history in the last decade was conducted which concluded why said variables were significant.</p>

Added italics
----------------------------------------------------------------------
In diva2:1441610   - correct as is
----------------------------------------------------------------------
In diva2:756072 
abstract is: 
<p>Recent theoretical and experimental results have revealed the existence of magnetic monopoles, in the form of quasi particles, in both condensed matter known as spin ice, as well as in two-dimensional artificial versions of the same material. In this report a two-dimensional Ising model is first examined, then an artificial square spin ice model using a dipole approximation, only taking into account nearest and next nearest neighbors. The Metropolis algorithm is used to obtain the internal energy, specific heat capacity and entropy as functions of temperature. In the latter model the magnetic monopole concentration and monopole current is also simulated. The two models show similar quantitative behavior in the above mentioned physical quantities, and in comparison to previously published results. In the artificial square spin ice model, under the influence of a magnetic field, a rapidly decreasing monopole current is observed, which decreases faster for higher temperatures. The magnitude of the magnetic field plays a significant role in the generation of the monopole current, and no direct effect of the phase transitionis observed.</p>

corrected abstract:
<p>Recent theoretical and experimental results have revealed the existence of magnetic monopoles, in the form of quasi particles, in both condensed matter known as spin ice, as well as in two-dimensional artificial versions of the same material. In this report a two-dimensional Ising model is first examined, then an artificial square spin ice model using a dipole approximation, only taking into account nearest and next nearest neighbors. The Metropolis algorithm is used to obtain the internal energy, specific heat capacity and entropy as functions of temperature. In the latter model the magnetic monopole concentration and monopole current is also simulated. The two models show similar quantitative behavior in the above mentioned physical quantities, and in comparison to previously published results. In the artificial square spin ice model, under the influence of a magnetic field, a rapidly decreasing monopole current is observed, which decreases faster for higher temperatures. The magnitude of the magnetic field plays a significant role in the generation of the monopole current, and no direct effect of the phase transition is observed.</p>

Note 
w='transitionis' val={'c': 'transition is', 's': 'diva2:756072', 'n': 'correct in original'}
----------------------------------------------------------------------
In diva2:1167376   - correct as is
----------------------------------------------------------------------
In diva2:1213850 
abstract is: 
<p>This is a study on the effect of management and CEO stock ownership on company performance. A regression analysis is performed on panel data consisting of a sample of 30 companies listed on OMX Stockholm Mid Cap. A total of 210 and 2520 observations is considered on a yearly and monthly basis, respectively, for seven years (2010-2016). The Hausman test is applied for determining between the fixed effects and random effects regression models. Results show that management relative stock ownership has a significant positive effect on company net income growth and return on assets. The effect is not significant for CEO stock ownership, which is contrary to what commonly has been shown for large companies in previous research. Moreover, alternative methodology is discussed for the benefit of the future researcher. The authors illustrate how the selection of dummy variables can be vital for final model outcomes, and it is thus an important aspect to consider when performing panel data analysis.</p>

corrected abstract:
<p>This is a study on the effect of management and CEO stock ownership on company performance. A regression analysis is performed on panel data consisting of a sample of 30 companies listed on OMX Stockholm Mid Cap. A total of 210 and 2520 observations is considered on a yearly and monthly basis, respectively, for seven years (2010-2016). The Hausman test is applied for determining between the fixed effects and random effects regression models. Results show that management <em>relative</em> stock ownership has a significant positive effect on company net income growth and return on assets. The effect is not significant for CEO stock ownership, which is contrary to what commonly has been shown for large companies in previous research. Moreover, alternative methodology is discussed for the benefit of the future researcher. The authors illustrate how the selection of dummy variables can be vital for final model outcomes, and it is thus an important aspect to consider when performing panel data analysis.</p>

Notge added italics
----------------------------------------------------------------------
In diva2:820892   - correct as is
----------------------------------------------------------------------
In diva2:1444681 
abstract is: 
<p>This project was done as mandatory executive part of a bachelor thesis performed on another institution during exchange studies in spring term 2020. The student has chosen a project course from the Aerospace Department on own initiative with a content of 3.0 credits (‘6 hp’) and the topic has been chosen due to the interest of enriching more knowledge in the Boeing Manufacturing industry and how commercial aircrafts are assembled through efficiency, sustainability, and cooperation. The Boeing Manufacturing team has consisted of seven members, divided into two sub teams – Boeing Structure and Boeing Automation. This report will mainly focus on the performance of the Structures Team, since the student participated at that team and the focus will lie on model assembling, parts research, limitations, and lastly a presentation of the accomplishments.</p>

corrected abstract:
<p>This project was done as mandatory executive part of a bachelor thesis performed at Iowa State University during exchange studies in spring term 2020. The student has chosen a project course from the Aerospace Department on own initiative with a content of 3.0 credits (‘6 hp’) and the topic has been chosen due to the interest of enriching more knowledge in the Boeing Manufacturing industry and how commercial aircrafts are assembled through efficiency, sustainability, and cooperation. The Boeing Manufacturing team at Iowa State University has consisted of seven members, divided into two sub teams – <em>Boeing Structure</em> and <em>Boeing Automation</em>. This report will mainly focus on the performance of the Structures Team, since the student participated at that team and the focus will lie on model assembling, parts research, limitations, and lastly a presentation of the accomplishments.</p><p>More accurately explained, the mission of the Boeing Manufacturing project was to design and a scaled down robotic system capable of transporting a Boeing 777X wing from any point in the facility to the fuselage, followed by a full wing-to-body connection process. Two milestones have been taken into account in order to keep track on time for both the teams. The COVID-19 pandemic resulted in total restrictions on physical participation on campus, causing delays and therefore incompletion of the full project mission, both from Structures Team and Automation Team. The results can until further notice only be presented in form of finalized CAD models of the prototype, deliverables of all the necessary parts and components for building the prototype, the assembly of the base structure, and the finalized code for the autonomy of the prototype.</p>

Note change wording to match original add missing paragraph and added italics
----------------------------------------------------------------------
In diva2:426250 
abstract is: 
<p>In this work a manufacturing method for UN, ZrN and <strong>(</strong>U,Zr<strong>)</strong>N pellets was established at the nuclear fuel laboratory at KTH Stockholm/Sweden, which consists of the production of nitride powders and their sintering into pellets by spark plasma sintering.</p>
<p>The nitride powders were produced by the hydriding-nitriding route using pure metal as starting material. This synthesis was performed in a stream of the particular reaction gas. A synthesis control and monitoring system was developed, which can follow the reactions in real time by measuring the gas flow difference before and after the reaction chamber. With the help of this system the hydriding and nitriding reactions of uranium and zirconium were studied in detail. Fine nitride powders were obtained; however, the production of zirconium nitride involved one milling step of the brittle zirconium hydride.</p>
<p>Additionally uranium and zirconium alloys with different zirconium contents were produced and synthesized to nitride powders. It was found that also the alloys could be reduced to fine powder, but only by cyclic hydriding-dehydriding.</p>
<p>Pellets were sintered out of uranium nitrides, zirconium nitrides, mixed nitrides and alloy nitrides. These experiments showed that relative densities of more than 90% can easily be achieved for all those powders. Pellets sintered from mechanically mixed nitride powders were found to still consist of two separate nitride phases, while nitride produced from alloy was demonstrated to be a monophasic solid solution both as powder and as sintered pellets.</p>

corrected abstract:
<p>In this work a manufacturing method for <em>UN</em>, <em>ZrN</em> and <span style="font-size: 1.2rem;">(<em>U,Zr</em>)</span><em>N</em> pellets was established at the nuclear fuel laboratory at KTH Stockholm/Sweden, which consists of the production of nitride powders and their sintering into pellets by spark plasma sintering.</p>
<p>The nitride powders were produced by the hydriding-nitriding route using pure metal as starting material. This synthesis was performed in a stream of the particular reaction gas. A synthesis control and monitoring system was developed, which can follow the reactions in real time by measuring the gas flow difference before and after the reaction chamber. With the help of this system the hydriding and nitriding reactions of uranium and zirconium were studied in detail. Fine nitride powders were obtained; however, the production of zirconium nitride involved one milling step of the brittle zirconium hydride.</p>
<p>Additionally uranium and zirconium alloys with different zirconium contents were produced and synthesized to nitride powders. It was found that also the alloys could be reduced to fine powder, but only by cyclic hydriding-dehydriding.</p>
<p>Pellets were sintered out of uranium nitrides, zirconium nitrides, mixed nitrides and alloy nitrides. These experiments showed that relative densities of more than 90% can easily be achieved for all those powders. Pellets sintered from mechanically mixed nitride powders were found to still consist of two separate nitride phases, while nitride produced from alloy was demonstrated to be a monophasic solid solution both as powder and as sintered pellets.</p>

Note tuned up the formula in the first paragraph
----------------------------------------------------------------------
In diva2:408810   - correct as is
----------------------------------------------------------------------
In diva2:1342246   - correct as is
----------------------------------------------------------------------
In diva2:1342270   - correct as is
----------------------------------------------------------------------
In diva2:1812517   - correct as is
----------------------------------------------------------------------
In diva2:458004 
abstract is: 
<p>Rubber is a highly complicated material. Its properties are strongly dependent of temperature, preload, frequency, amplitude and filling materials. In order to have better knowledge about how rubber reacts for these different parameters, a number of different papers that treats these parameters have been reviewed. Main focus is on dynamic properties of carbon-black filled rubber. Mechanical models of rubber isolator with different kinds of possible solution have been suggested. A basic mathematical equation is derived to estimate the static and dynamic stiffness of a rubber isolator depending on the preload for a geometry that is commonly used by Atlas Copco Rock Drills AB. Tests of a rubber isolator with the same geometry is also conducted to validate the model. It is shown that the model corresponds very well to measurement data.</p>

corrected abstract:
<p>Rubber is a highly complicated material. Its properties are strongly dependent of temperature, preload, frequency, amplitude and filling materials. In order to have better knowledge about how rubber reacts for these different parameters, a number of different papers that treats these parameters have been reviewed. Main focus is on dynamic properties of carbon-black filled rubber. Mechanical models of rubber isolator with different kinds of possible solution have been suggested.</p><p>A basic mathematical equation is derived to estimate the static and dynamic stiffness of a rubber isolator depending on the preload for a geometry that is commonly used by Atlas Copco Rock Drills AB. Tests of a rubber isolator with the same geometry is also conducted to validate the model. It is shown that the model corresponds very well to measurement data.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:817226 
abstract is: 
<p>Dual Energy X-ray Absorptiometry is a proven technique used to identify unknown materials, by measuring the transmission of two X-ray energies. This technique is limited to measuring a single chemical quantity and is not able to handle more chemical variation. To overcome this, one approach is to use multiple-energies to resolve more information. The differences in the processes controlling the Xray transmission limits the theoretical resolution capability to three characteristics. Of these three, one is dependent on the sample geometry and density. The remaining two are purely chemical characteristics and are investigated in this thesis. It is found that using X-ray photon energies in the range 20-90 keV, it is possible to measure one chemical characteristic to a high precision. Two chemical characteristics can be measured in limited circumstances and even though the precision is good, the measurement is prone to inaccuracies in machine modeling and stability. A two step method is defined, first finding an approximation of the X-ray spectra and then reconstructing the attenuation coefficient of the sample to a high precision (<em>&lt; </em>0<em>.</em>2%) using a robust low-rank basis for the characteristics.</p>

corrected abstract:
<p>Dual Energy X-ray Absorptiometry is a proven technique used to identify unknown materials, by measuring the transmission of two X-ray energies. This technique is limited to measuring a single chemical quantity and is not able to handle more chemical variation. To overcome this, one approach is to use multiple-energies to resolve more information. The differences in the processes controlling the Xray transmission limits the theoretical resolution capability to three characteristics. Of these three, one is dependent on the sample geometry and density. The remaining two are purely chemical characteristics and are investigated in this thesis. It is found that using X-ray photon energies in the range 20-90 keV, it is possible to measure one chemical characteristic to a high precision. Two chemical characteristics can be measured in limited circumstances and even though the precision is good, the measurement is prone to inaccuracies in machine modeling and stability. A two step method is defined, first finding an approximation of the X-ray spectra and then reconstructing the attenuation coefficient of the sample to a high precision (&lt; 0.2%) using a robust low-rank basis for the characteristics.</p>

Note removed the unnecessary italics
----------------------------------------------------------------------
In diva2:1450073 
abstract is: 
<p>The development of the commercial 5G network with high-frequency mmWave requires tighter base station grid, which increases the demand for smaller and more unobtrusive products. One way to connect materials of different properties and keeping the product size small is to use adhesive joints instead of screw joints. The thesis project is about understanding the material behaviour of adhesive joint and determining the material model.</p><p>Adhesive joints can be described as a highly temperature-dependent material, including both hyperelastic and viscoelastic material behaviours. A relaxation test was carried out to evaluate the joint behaviour and its temperature dependence. The results showed that the Neo Hookean material model and the Generalized Maxwell model can be used to describe the basic joint properties. Additionally, the adhesive joint exhibited a softening behaviour under cyclic loading, in which the softening in the first load cycle was captured by the Mullins effect. A cyclic loading test was carried out to evaluate the joint damage. The results showed that when the joint thickness was equal to the applied displacement, no visible joint damage will occur. However, if the joint thickness is less than the applied displacement, total joint failure can be captured by small reaction force and discontinuities in the force-displacement curve.</p>

corrected abstract:
<p>The development of the commercial 5G network with high-frequency mmWave requires tighter base station grid, which increases the demand for smaller and more unobtrusive products. One way to connect materials of different properties and keeping the product size small is to use adhesive joints instead of screw joints. The thesis project is about understanding the material behaviour of adhesive joint and determining the material model.</p><p>Adhesive joints can be described as a highly temperature-dependent material, including both <em>hyperelastic</em> and <em>viscoelastic</em> material behaviours. A relaxation test was carried out to evaluate the joint behaviour and its temperature dependence. The results showed that the <em>Neo Hookean</em> material model and the <em>Generalized Maxwell</em> model can be used to describe the basic joint properties. Additionally, the adhesive joint exhibited a softening behaviour under cyclic loading, in which the softening in the first load cycle was captured by the <em>Mullins effect</em>. A cyclic loading test was carried out to evaluate the joint damage. The results showed that when the joint thickness was equal to the applied displacement, no visible joint damage will occur. However, if the joint thickness is less than the applied displacement, total joint failure can be captured by small reaction force and discontinuities in the force-displacement curve.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1567690   - correct as is
----------------------------------------------------------------------
In diva2:1851007   - correct as is
----------------------------------------------------------------------
In diva2:1319697   - correct as is
----------------------------------------------------------------------
In diva2:1070420   - correct as is
----------------------------------------------------------------------
In diva2:968663   - correct as is
Note last sentence does not have any temrinal punctuation in the original
----------------------------------------------------------------------
In diva2:1359948   - correct as is
----------------------------------------------------------------------
In diva2:1613480 
abstract is: 
<p>This volume covers the phases from design to manufacturing of a wind tunnel test support structure for a conceptual blended wing-body UAV designed by KTH Green Raven Project students. The innovative aircraft design demonstrates sustainability within aviation by utilizing a hybrid electric-fuel cell propulsion system. The wind tunnel test to be conducted at Bristol University will produce data to evaluate the aerodynamic properties of the model for design verification. The wind tunnel model is a small-scaled 1.5m-span model supported by struts that change the pitch and yaw angles during testing. An external force balance provided by Bristol University measures the loads and moments experienced by the model. The main requirements for the structure are to withstand the aerodynamic loads imposed by the model and to change the model's orientation while maintaining wind speed during the test. The maximum aerodynamic loads were provided in a matrix, the largest of which was used as the load condition for the support equating to a 512N lift at 14 degrees AOA. Trade studies were conducted to determine the mechanisms to satisfy the requirements while staying within budget. The chosen design for the support structure includes a circular base plate constrained by a locking ring with positioning pins to change the yaw angle. The main strut is mounted at the the center of the circular base plate. A hinge bracket at the top of the strut interfaces with another hinge bracket within the model via a clevis pin. An electric linear actuator mounted downstream of the main strut is used to vary the pitch angle, with the center of rotation at the clevis pin. Once the design was finalized, finite element analysis was done to verify the structural stability of the design. The FEA results were compared to Euler-Bernoulli approximations for deflection. Manufacturing of the components was out-sourced while assembly and programming of the actuator was done in-house.</p>

corrected abstract:
<p>This volume covers the phases from design to manufacturing of a wind tunnel test support structure for a conceptual blended wing-body UAV designed by KTH Green Raven Project students. The innovative aircraft design demonstrates sustainability within aviation by utilizing a hybrid electric-fuel cell propulsion system. The wind tunnel test to be conducted at Bristol University will produce data to evaluate the aerodynamic properties of the model for design verification. The wind tunnel model is a small-scaled 1.5m-span model supported by struts that change the pitch and yaw angles during testing. An external force balance provided by Bristol University measures the loads and moments experienced by the model. The main requirements for the structure are to withstand the aerodynamic loads imposed by the model and to change the model's orientation while maintaining wind speed during the test. The maximum aerodynamic loads were provided in a matrix, the largest of which was used as the load condition for the support equating to a 512N lift at 14º AOA. Trade studies were conducted to determine the mechanisms to satisfy the requirements while staying within budget. The chosen design for the support structure includes a circular base plate constrained by a locking ring with positioning pins to change the yaw angle. The main strut is mounted at the the center of the circular base plate. A hinge bracket at the top of the strut interfaces with another hinge bracket within the model via a clevis pin. An electric linear actuator mounted downstream of the main strut is used to vary the pitch angle, with the center of rotation at the clevis pin. Once the design was finalized, finite element analysis was done to verify the structural stability of the design. The FEA results were compared to Euler-Bernoulli approximations for deflection. Manufacturing of the components was out-sourced while assembly and programming of the actuator was done in-house.</p>

Note replaced "degrees" with "º"
----------------------------------------------------------------------
In diva2:1237798   - correct as is
----------------------------------------------------------------------
In diva2:1711817 
abstract is: 
<p>This master thesis project was carried out at a large international company in the forest industry. The company is currently interested in producing sheet formed composite materials with high wood fibre content that can be further processed and formed using conventional compression moulding. The goal of this project was to investigate how the compression moulding temperature and compression moulding hold time influence the mechanical properties of the material, as well as comparing different compositions of the composite material.</p><p>By a series of preparation tests, an appropriate pre-heating method was developed and the compression moulding parameters to be tested were determined. The mechanical properties to evaluate were tensile properties, flexural properties, short bend test properties and impact strength. Testing was based on ISO- standards with minor adjustments.</p><p>The tests showed that the mechanical properties increased with a higher moulding temperature. An increased moulding hold time led to increased mechanical properties as well, but not as significantly as the temperature. The density of the compression moulded composite was the most important factor for the mechanical properties. Comparisons between the tested composite compositions showed that a higher polymer content is favourable. Additionally, using long wood fibres with low lignin content had some advantages compared to short fibres with high lignin content.</p>

corrected abstract:
<p>This master thesis project was carried out at a large international company in the forest industry. The company is currently interested in producing sheet formed composite materials with high wood fibre content that can be further processed and formed using conventional compression moulding. The goal of this project was to investigate how the compression moulding temperature and compression moulding hold time influence the mechanical properties of the material, as well as comparing different compositions of the composite material.</p><p>By a series of preparation tests, an appropriate pre-heating method was developed and the compression moulding parameters to be tested were determined. The mechanical properties to evaluate were tensile properties, flexural properties, short bend test properties and impact strength. Testing was based on ISO-standards with minor adjustments.</p><p>The tests showed that the mechanical properties increased with a higher moulding temperature. An increased moulding hold time led to increased mechanical properties as well, but not as significantly as the temperature. The density of the compression moulded composite was the most important factor for the mechanical properties. Comparisons between the tested composite compositions showed that a higher polymer content is favourable. Additionally, using long wood fibres with low lignin content had some advantages compared to short fibres with high lignin content.</p>

Note removed space after hypen
----------------------------------------------------------------------
In diva2:396162   - correct as is
----------------------------------------------------------------------
In diva2:436555 - full text is only the Examensa
It is not clear where the abstract came from.
----------------------------------------------------------------------
In diva2:1219143   - correct as is
----------------------------------------------------------------------
In diva2:1714275 
abstract is: 
<p>Thanks to advancements in the polymer industry, metal replacement with high performance polymers has become a growing practice in many industries due to the economical, lead time and weight advantages. Getinge AB was interested in introducing metal replacement to the medical technology industry by launching a thesis project to replace the die-cast aluminium in a key structural component of their ventilator products. The ultimate goal of the project was to provide enough information to decide if going to a prototype stage is an investment worthwhile.</p><p>In order to provide the necessary data, the project was conducted in the following manner: an evaluation of the current aluminium design was done using finite element method (FEM) simulations, followed by a thorough material choice and design adjustments in order to enhance strength and manufacturability. Finally another set of FEM simulations were conducted for the new plastic design, that was compared to the performance of the originaldesign, in order to confirm the validity of the updated structure. The results from the project show that metal replacement might be a sound choice for the structure, considering the mechanical performance, potential cost reduction and reduced lead time. However the part strength is a liability and require thorough physical testing.</p>


corrected abstract:
<p>Thanks to advancements in the polymer industry, metal replacement with high performance polymers has become a growing practice in many industries due to the economical, lead time and weight advantages. Getinge AB was interested in introducing metal replacement to the medical technology industry by launching a thesis project to replace the die-cast aluminium in a key structural component of their ventilator products. The ultimate goal of the project was to provide enough information to decide if going to a prototype stage is an investment worthwhile.</p><p>In order to provide the necessary data, the project was conducted in the following manner: an evaluation of the current aluminium design was done using finite element method (FEM) simulations, followed by a thorough material choice and design adjustments in order to enhance strength and manufacturability. Finally another set of FEM simulations were conducted for the new plastic design, that was compared to the performance of the original design, in order to confirm the validity of the updated structure. The results from the project show that metal replacement might be a sound choice for the structure, considering the mechanical performance, potential cost reduction and reduced lead time. However the part strength is a liability and require thorough physical testing.</p>

Note changed 'originaldesign' to 'original design'
----------------------------------------------------------------------
In diva2:1527789 
abstract is: 
<p>The electrification within the automotive industry goes faster than ever, which drives an increased demand for more knowledge about batteries. Vehicle manufacturers should be able to tell how long the batteries will last and have a service program for electrified vehicles, just as there is for traditional, fuel-driven ones. Scania is in the process of developing new service methods for their hybrids and fully electrified vehicles where this thesis has been a part of this development by investigating the possibilities of having a workshop test to measure the capacity of the propulsion batteries. </p><p>During the thesis, essential parameters for cycling the batteries and measure the capacity with high accuracy have been identified and investigated by conducting lab tests. In parallel to defining the properties of a successful capacity measurement, the implementation of such a measurement at a workshop has been studied alongside a brief discussion about scheduling strategies. Conducting a capacity measurement in a workshop environment introduce new challenges, and the critical question arises, how long can the capacity measurement take?</p><p>It is identified that the state of charge window size, the temperature, and the relaxation time are essential parameters to control. From the experimental part of the thesis, it can be concluded that the start temperature should lay in the range of 15-25 °C with a relaxation time of 5-10 minutes providing a satisfying accuracy. A SOC window size of 20-80% seems to be the most optimal balance between time spent and accuracy in the measurement. Furthermore, it is identified that the workshop's equipment is heavily influencing the time it takes to conduct a test. It is concluded that it is necessary to be able to charge and discharge the batteries.</p>

corrected abstract:
<p>The electrification within the automotive industry goes faster than ever, which drives an increased demand for more knowledge about batteries. Vehicle manufacturers should be able to tell how long the batteries will last and have a service program for electrified vehicles, just as there is for traditional, fuel-driven ones. Scania is in the process of developing new service methods for their hybrids and fully electrified vehicles where this thesis has been a part of this development by investigating the possibilities of having a workshop test to measure the capacity of the propulsion batteries.</p><p>During the thesis, essential parameters for cycling the batteries and measure the capacity with high accuracy have been identified and investigated by conducting lab tests. In parallel to defining the properties of a successful capacity measurement, the implementation of such a measurement at a workshop has been studied alongside a brief discussion about scheduling strategies. Conducting a capacity measurement in a workshop environment introduce new challenges, and the critical question arises, how long can the capacity measurement take?</p><p>It is identified that the state of charge window size, the temperature, and the relaxation time are essential parameters to control. From the experimental part of the thesis, it can be concluded that the start temperature should lay in the range of 15-25 °C with a relaxation time of 5-10 minutes providing a satisfying accuracy. A SOC window size of 20-80 % seems to be the most optimal balance between time spent and accuracy in the measurement. Furthermore, it is identified that the workshop's equipment is heavily influencing the time it takes to conduct a test. It is concluded that it is necessary to be able to charge and discharge the batteries.</p>

Note eliminated an unnecessary space at the end of a paragraph and added a space before the "%" to match the original
----------------------------------------------------------------------
In diva2:1877685 
abstract is: 
<p>This paper explores different covariance matrix estimators in application to geometric Brownian motion. Particular interest is given to shrinkage estimation methods. In collaboration with Söderberg &amp; Partners risk management team, the goal is to find an estimation that performs well in low-data scenarios and is robust against erroneous model assumptions, particularly the Gaussian assumption of the stock price distribution. Estimations are compared by two criteria: Frobenius norm distance between the estimate and the true covariance matrix, and the condition number of the estimate. By considering four estimates — the sample covariance matrix, Ledoit-Wolf, Tyler M-estimator, and a novel Tyler-Ledoit-Wolf (TLW) estimator — this paper concludes that the TLW estimator performs best when considering the two criteria.</p>

corrected abstract:
<p>This paper explores different covariance matrix estimators in application to geometric Brownian motion. Particular interest is given to shrinkage estimation methods. In collaboration with Söderberg &amp; Partners risk management team, the goal is to find an estimation that performs well in low data scenarios and is robust against erroneus model assumptions, particularly the Gaussian assumption of the stock price distribution. Estimations are compared by two criteria: Frobenius norm distance between the estimate and the true covariance matrix, and the condition number of the estimate. By considering four estimates — the sample covariance matrix, Ledoit-Wolf, Tyler M-estimator, and a novel Tyler-Ledoit-Wolf (TLW) estimator — this paper concludes that the TLW estimator performs best when considering the two criteria.</p>


Note error:
w='erroneus' val={'c': 'erroneous', 's': 'diva2:1877685', 'n': 'error in original'}
also "low-data" should simply be "low data"
----------------------------------------------------------------------
In diva2:1355716   - correct as is
----------------------------------------------------------------------
In diva2:1583509   - correct as is
----------------------------------------------------------------------
In diva2:1015067   - correct as is
----------------------------------------------------------------------
In diva2:1516110 
abstract is: 
<p>Propulsion systems allow satellites to perform many functionalities in space, such as orbital station keeping, reentry control, attitude control, orbital transferring, rendezvous operation, and even more thrilling, interplanetary travel. Indeed, propulsion systems in satellites have fostered a new favourable era of space exploration and application, therefore, detailed processes to operate propulsion systems need to be developed so that space missions, carrying this valuable system, are completed successfully. The aim of this study is to describe the most relevant operating procedures for the cold gas propulsion system NanoProp 3U, developed by GomSpace, on-board the 3U CubeSat MIST satellite developed by KTH. Procedures, such as power levels, telemetry considerations, propellant mass determination, Fault Detection Isolation and Recovery analysis, and decommissioning plan allow proper operation of NanoProp according to the mission requirements determined for MIST mission. Moreover, this study describes detailed mission experiments to be performed with NanoProp with the objective of assessing the performance delivered by the propulsion system itself, and other on-board subsystems which are required for monitoring and controlling the spacecraft according to the effects generated by the propulsion system. The planning and operation of a propulsion system should be outlined on-ground, during the mission design, so a clear understanding of the characteristics and limitations of the system are highlighted towards the development of a secure and solid space mission.</p>

corrected abstract:
<p>Propulsion systems allow satellites to perform many functionalities in space, such as orbital station keeping, reentry control, attitude control, orbital transferring, rendezvous operation, and even more thrilling, interplanetary travel. Indeed, propulsion systems in satellites have fostered a new favorable era of space exploration and application, therefore, detailed processes to operate propulsion systems need to be developed so that space missions, carrying this valuable system, are completed successfully. The aim of this study is to describe the most relevant operating procedures for the cold gas propulsion system NanoProp 3U, developed by GomSpace, on-board the 3U CubeSat MIST satellite developed by KTH. Procedures, such as power levels, telemetry considerations, propellant mass determination, Fault Detection Isolation and Recovery analysis, and decommissioning plan allow proper operation of NanoProp according to the mission requirements determined for MIST mission. Moreover, this study describes detailed mission experiments to be performed with NanoProp with the objective of assessing the performance delivered by the propulsion system itself, and other on-board subsystems which are required for monitoring and controlling the spacecraft according to the effects generated by the propulsion system. The planning and operation of a propulsion system should be outlined on-ground, during the mission design, so a clear understanding of the characteristics and limitations of the system are highlighted towards the development of a secure and solid space mission.</p>

Note changed 'favourable' to 'favorable' - to match original
----------------------------------------------------------------------
In diva2:1737014 
abstract is: 
<p>A fundamental mathematical object in topological data analysis today is the persistence module. This thesis explores different metrics on multidimensional persistence modules, where spaces are parametrized along multiple dimensions. The focus is especially on metrics constructed by the use of so called noise systems, introduced by Scolamiero et al. in 2015. Furthermore, suggestions for new noise systems are given and bounds for their metrics are presented. An exact computation for the metric induced by the volume noise system is also shown for pairs of modules satisfying certain conditions.</p>

corrected abstract:
<p>A fundamental mathematical object in topological data analysis today is the persistence module. This thesis explores different metrics on multidimensional persistence modules, where spaces are parameterized along multiple dimensions. The focus is especially on metrics constructed by the use of so called noise systems, introduced by Scolamiero et al. in 2015. Furthermore, suggestions for new noise systems are given and bounds for their metrics are presented. An exact computation for the metric induced by the volume noise system is also shown for pairs of modules satisfying certain conditions.</p>


Note 'parametrized' changed to  'parameterized' to match the original
----------------------------------------------------------------------
In diva2:866819   - correct as is
----------------------------------------------------------------------
In diva2:1017317   - correct as is
----------------------------------------------------------------------
In diva2:1817073   - correct as is
----------------------------------------------------------------------
In diva2:1415888 
abstract is: 
<p>Since the 2008 crisis, the hedging instruments have gained popularity with financial institutions. This is the case of the total return swap that is used today by major institutions like Goldman Sachs or J.P. Morgan. Murex is a software provider for financial institutions. The company already had a total return swap product, the RTRS (for Risky Total Return Swap), but with the growing demand Murex decided to develop a new product, the BRS (Bond Return Swap). So now they have two bond total return swaps.</p><p>This master thesis aims to analyze total return swap and highlight the improvement of the BRS. After a theoretical analysis of the total return swap, a test campaign is realized. For different types of bond and different configurations of total return swap, formulas are derived to be compared to the returned values. The results given by the RTRS are good on basic bonds. If the bond is more complex, for instance a bond with credit risk or an amortized bond, the values returned by the RTRS are not reliable if not wrong. On the other hand, the BRS performs well in every situation and positions itself as the best total return swap proposed by Murex.</p>

corrected abstract:
<p>Since the 2008 crisis, the hedging instruments have gained popularity with financial institutions. This is the case of the total return swap that is used today by major institutions like Goldman Sachs or J.P. Morgan. Murex is a software provider for financial institutions. The company already had a total return swap product, the RTRS (for Risky Total Return Swap), but with the growing demand Murex decided to develop a new product, the BRS (Bond Return Swap). So now they have two bond total return swaps.</p><p>This master thesis aims to analyze total return swap and highlight the improvement of the BRS. After a theoretical analysis of the total return swap, a test campaign is realized. For different types of bond and different configurations of total return swap, formulas are derived to be compared to the returned values. The results given by the RTRS are good on basic bonds. If the bond is more complex, for instance a bond with credit risk or an amortized bond, the values returned by the RTRS are not reliable if not wrong. On the other hand, the BRS performs well in every situation and positions itself as the best total return swap proposed by Murex.</p>
----------------------------------------------------------------------
In diva2:1342585   - correct as is
----------------------------------------------------------------------
In diva2:1380157   - correct as is
----------------------------------------------------------------------
In diva2:1081942   - correct as is
----------------------------------------------------------------------
In diva2:1033545 
abstract is: 
<p>The purpose of this bachelor thesis is to build a model for predicting resistance and running attitude of high-speed craft equipped with interceptors. The thesis examines the profits gained in terms of economic, comfort and environmental investment. The current study aims to illustrate the impact of interceptors in planing ships, by simulating a steady state model of the dynamic running position. Thus, data regarding trim and drag will be generated. The model is based on previously made studies by Savitsky, Dawson and Blount, Brown and Steen. Extrapolation of the results is realized with the Swedish Coast Guard boat KBV 315 to validate interceptor benefits with the Coast Guard patrol craft.</p><p>Two different models are presented and the results show a fuel consumption reduction of 8-13%, for the speed range of 15-24 knots, and maximum trim decrease of 1.7˚ for KBV 315. It is concluded that interceptors have a great potential of increasing both comfort (in terms of trim) and efficiency of high-speeds crafts. The provided models can be used as an estimation tool of the outcome before issuing an installing project.</p>

corrected abstract:
<p>The use of high-speed crafts is for many reasons widely spread around the world. The profits that can be made, in terms of increased comfort, reduced costs and emissions, are substantial. The aim to improve the performance on already built high-speed crafts is therefore an important issue for ecological and economical aspects. One way of improving ships ride and trim is the usage of interceptors. This device can easily be described as a vertical plate (~20-75 mm) placed in the aft of a ship.</p>
<p>The purpose of this bachelor thesis is to build a model for predicting resistance and running attitude of high-speed craft equipped with interceptors. The thesis examines the profits gained in terms of economic, comfort and environmental investment. The current study aims to illustrate the impact of interceptors in planing ships, by simulating a steady state model of the dynamic running position. Thus, data regarding trim and drag will be generated. The model is based on previously made studies by Savitsky, Dawson and Blount, Brown and Steen. Extrapolation of the results is realized with the Swedish Coast Guard boat KBV 315 to validate interceptor benefits with the Coast Guard patrol craft.</p><p>Two different models are presented and the results show a fuel consumption reduction of 8-13%, for the speed range of 15-24 knots, and maximum trim decrease of 1.7˚ for KBV 315. It is concluded that interceptors have a great potential of increasing both comfort (in terms of trim) and efficiency of high-speeds crafts. The provided models can be used as an estimation tool of the outcome before issuing an installing project.</p>

Note DiVA abstract is misssing the firsat paragraph
----------------------------------------------------------------------
In diva2:1436820   - correct as is
----------------------------------------------------------------------
In diva2:942703   - correct as is
----------------------------------------------------------------------
In diva2:1450542   - correct as is
----------------------------------------------------------------------
In diva2:1757056   - correct as is
----------------------------------------------------------------------
In diva2:735918   - correct as is
----------------------------------------------------------------------
In diva2:1319672   - correct as is
----------------------------------------------------------------------
In diva2:838543   - correct as is
----------------------------------------------------------------------
In diva2:1046430 
abstract is: 
<p>This paper attempts to study GARCH-type models, with emphasis on fitting GARCH models to exchange rate return series. The symmetric GARCH(1,1) model is compared with the asymmetric EGARCH(1,1) model. Both models are analysed with different conditional distributions, namely Normal, Student's t and skew Student's t for the return innovation. Parameter estimation is performed using a maximum-likelihood approximation. The model performance is assessed by looking at the lowest AIC and BIC. Four exchange rate returns are studied using daily data over the period from 2002 to 2015. Moreover, essential ideas of return time series and stylised facts will be analysed. Our results indicate that the asymmetric GARCH model improves generally estimation with fat-tailed densities in the conditional variance. Furthermore, persistence has found to be reduced with the use of heavy-tailed distributions. Asymmetry presence has been detected in the EGARCH model. Besides, we found that "good news" tend to increase volatility in comparison with "bad news".</p>

corrected abstract:
<p>This paper attempts to study GARCH-type models, with emphasis on fitting GARCH models to exchange rate return series. The symmetric GARCH(1,1) model is compared with the asymmetric EGARCH(1,1) model. Both models are analysed with different conditional distributions, namely Normal, Student's 𝑡 and skew Student's 𝑡 for the return innovation. Parameter estimation is performed using a maximum-likelihood approximation. The model performance is assessed by looking at the lowest AIC and BIC. Four exchange rate returns are studied using daily data over the period from 2002 to 2015. Moreover, essential ideas of return time series and stylised facts will be analysed. Our results indicate that the asymmetric GARCH model improves generally estimation with fat-tailed densities in the conditional variance. Furthermore, persistence has found to be reduced with the use of heavy-tailed distributions. Asymmetry presence has been detected in the EGARCH model. Besides, we found that “good news” tend to increase volatility in comparison with “bad news”.</p>

Note fixed double quotes to match original and change "t" to "𝑡" in "Student's 𝑡"
----------------------------------------------------------------------
In diva2:1057186   - correct as is
----------------------------------------------------------------------
In diva2:1215668   - correct as is
----------------------------------------------------------------------
In diva2:1799813 
abstract is: 
<p>For a long time, being able to model and mitigate financial risk has been a key success factor for institutions. Apart from an internal incentive, legal and regulatory requirements continue to develop which increases the need for extensive internal risk control. Interest rate risk in the banking book ("IRRBB") alludes to the cur- rent or prospective risk to the bank’s earnings and capital emerging from adverse movements in interest rates that influence the bank’s banking book positions. When interest rates change, the value but also the timing of future cash flows are affected. Thus, the underlying value of a bank’s liabilities and assets and other off-balance sheet items change as a consequence, and therefore its economic value. In 2004, the Basel Committee on Banking Supervision published a paper Principles for the Management and Supervision of Interest Rate Risk which later lead the European Banking Authority ("EBA") to publish a renewed framework in 2016. In December 2021, the EBA published a draft of an updated version of this framework. This paper investigates how banks and risk managers should model IRRBB under these new guidelines. This is achieved by constructing an IRRBB model which is then evaluated to see whether the IRRBB framework provided by the EBA is adequate and comprehensive. The IRRBB model by the EBA is fundamentally constructed by creating six different shock scenarios where the yield curve is stressed (parallel- , short rate-, and long rate shifts). Thereafter, one measures risk by investigating how these shifts affect the bank’s or financial institutions’ economic value and net interest income. In this paper, additional stressed scenarios were produced through Principal Component Analysis and Monte Carlo Simulations. This paper found that the framework by the EBA is adequate and formulates good methods. However, the framework is not fully standardized and comprehensive, and some computations and methods are left for the institution to decide. This is most likely due to the uniqueness of each institution and that it is hard to formulate methods that are pertinent for all. A more complete, standardized framework would however be advantageous for, on the one hand, governing agencies which would benefit from decreasing the number of resources needed when supervising institutions’ internal models. On the other, institutions would benefit from decreasing the probability of potentially overlooking some risk. Furthermore, this would help companies de- crease their capital requirement, which is desirable.</p>

corrected abstract:
<p>For a long time, being able to model and mitigate financial risk has been a key success factor for institutions. Apart from an internal incentive, legal and regulatory requirements continue to develop which increases the need for extensive internal risk control. Interest rate risk in the banking book ("IRRBB") alludes to the current or prospective risk to the bank’s earnings and capital emerging from adverse movements in interest rates that influence the bank’s banking book positions. When interest rates change, the value but also the timing of future cash flows are affected. Thus, the underlying value of a bank’s liabilities and assets and other off-balance sheet items change as a consequence, and therefore its economic value. In 2004, the Basel Committee on Banking Supervision published a paper Principles for the Management and Supervision of Interest Rate Risk which later lead the European Banking Authority ("EBA") to publish a renewed framework in 2016. In December 2021, the EBA published a draft of an updated version of this framework. This paper investigates how banks and risk managers should model IRRBB under these new guidelines. This is achieved by constructing an IRRBB model which is then evaluated to see whether the IRRBB framework provided by the EBA is adequate and comprehensive. The IRRBB model by the EBA is fundamentally constructed by creating six different shock scenarios where the yield curve is stressed (parallel- , short rate-, and long rate shifts). Thereafter, one measures risk by investigating how these shifts affect the bank’s or financial institutions’ economic value and net interest income. In this paper, additional stressed scenarios were produced through Principal Component Analysis and Monte Carlo Simulations. This paper found that the framework by the EBA is adequate and formulates good methods. However, the framework is not fully standardized and comprehensive, and some computations and methods are left for the institution to decide. This is most likely due to the uniqueness of each institution and that it is hard to formulate methods that are pertinent for all. A more complete, standardized framework would however be advantageous for, on the one hand, governing agencies which would benefit from decreasing the number of resources needed when supervising institutions’ internal models. On the other, institutions would benefit from decreasing the probability of potentially overlooking some risk. Furthermore, this would help companies decrease their capital requirement, which is desirable.</p>

Note removed unnecessary hyphens
----------------------------------------------------------------------
In diva2:428940 
abstract is: 
<p>Non‐maturing liabilities, such as savings accounts, lack both predetermined maturity and reset dates due to the fact that the depositor is free to withdraw funds at any time and that the depository institution is free to change the rate. These attributes complicate the risk management of such products and no standardized solution exists. The problem is important however since non‐maturing liabilities typically make up a considerable part of the funding of a bank. In this report different modeling approaches to the risk management are described and a method for managing the interest rate risk is implemented. It is a replicating portfolio approach used to approximate the non‐maturing liabilities with a portfolio of fixed income instruments. The search for a replicating portfolio is formulated as an optimization problem based on regression between the deposit rate and market ratesseparated by a fixed margin. In the report two different optimization criteria are compared for the replicating portfolio, minimizing the standard deviation of the margin versus maximizing the risk‐adjusted margin represented by the Sharpe ratio, of which the latter is found to yield superior results. The choice of historical sample interval over which the portfolio is optimized seems to have a rather big impact on the outcome but recalculating the portfolio weights at regular intervals is found to stabilize the results somewhat. All in all, despite the fact that this type of method cannot fully capture the most advanced dynamics of the non‐maturing liabilities, a replicating portfolio still appears to be a feasible approach for the interest risk management.</p>
mc='ratesseparated' c='rates separated'

corrected abstract:
<p>Non‐maturing liabilities, such as savings accounts, lack both predetermined maturity and reset dates due to the fact that the depositor is free to withdraw funds at any time and that the depository institution is free to change the rate. These attributes complicate the risk management of such products and no standardized solution exists. The problem is important however since non‐maturing liabilities typically make up a considerable part of the funding of a bank. In this report different modeling approaches to the risk management are described and a method for managing the interest rate risk is implemented. It is a replicating portfolio approach used to approximate the non‐maturing liabilities with a portfolio of fixed income instruments. The search for a replicating portfolio is formulated as an optimization problem based on regression between the deposit rate and market rates separated by a fixed margin. In the report two different optimization criteria are compared for the replicating portfolio, minimizing the standard deviation of the margin versus maximizing the risk‐adjusted margin represented by the Sharpe ratio, of which the latter is found to yield superior results. The choice of historical sample interval over which the portfolio is optimized seems to have a rather big impact on the outcome but recalculating the portfolio weights at regular intervals is found to stabilize the results somewhat. All in all, despite the fact that this type of method cannot fully capture the most advanced dynamics of the non‐maturing liabilities, a replicating portfolio still appears to be a feasible approach for the interest risk management.</p>
----------------------------------------------------------------------
In diva2:503320   - correct as is
----------------------------------------------------------------------
In diva2:1435849   - correct as is
----------------------------------------------------------------------
In diva2:1720705   - correct as is
----------------------------------------------------------------------
In diva2:1450581  - English abstract is the same as the Swedish abstract and is in Swedish!
The English abstract should be:

<p>This bachelor thesis lies within the field of mathematical statistics. In collaboration with the insurance company Hedvig, this thesis aims to explore a new method of handling Hedvig’s insurance data by building a pricing model for all risk insurances, with Generalized Linear Models. Two Generalized Linear Models were built, where the first predicts the frequency of a claim and the second predicts the severity. The original data was divided into 9 explanatory variables. Both models included five explanatory variables at start and were then reduced. The reduction resulted in four out of five characteristics to be explanatory significant in the frequency model and only one of the five to be explanatory significant in the severity model. Each of the models obtained relative risks of the levels of their explanatory variables. The relative risks resulted in a total risk for each level. Through multiplication of a created base level with a set combination of risk parameters, the premium for a chosen customer can be obtained.</p>
----------------------------------------------------------------------
In diva2:666775 
abstract is: 
<p>The energy in ocean waves is a renewable energy resource not yet fully exploited. Research in converting ocean energy to useful electricity has been ongoing for about 40 years, but no one has so far succeed to do it at sufficiently low cost. <strong>CorPower Ocean </strong>has developed a method, which in theory can achieve this. It uses a light buoy and a control strategy called Phase Control.</p><p>The purpose of this thesis is to develop a mathematical model of this method - using LinearWave Theory to derive the hydrodynamic forces - and from the simulated results analyze the energy output of the method. In the process we create a program that will help realizing and improving the method further.</p><p>The model is implemented and simulated in the simulation program Simulink. On the basis of the simulated results, we can concludes that the <strong>CorPower Ocean </strong>method is promising. The outcome shows that the energy output increases - up to five times- compared to conventional methods.</p>

corrected abstract:
<p>The energy in ocean waves is a renewable energy resource not yet fully exploited. Research in converting ocean energy to useful electricity has been ongoing for about 40 years, but no one has so far succeed to do it at sufficiently low cost. <strong>CorPower Ocean</strong> has developed a method, which in theory can achieve this. It uses a light buoy and a control strategy called Phase Control.</p><p>The purpose of this thesis is to develop a mathematical model of this method&mdash;using Linear Wave Theory to derive the hydrodynamic forces&mdash; and from the simulated results analyze the energy output of the method. In the process we create a program that will help realizing and improving the method further.</p><p>The model is implemented and simulated in the simulation program Simulink. On the basis of the simulated results, we can concludes that the <strong>CorPower Ocean</strong> method is promising. The outcome shows that the energy output increases&mdash;up to five times&mdash;compared to conventional methods.</p>

Note adjusted the italics regions and replaced the long dashes with &mdash;
----------------------------------------------------------------------
In diva2:1591875   - correct as is
----------------------------------------------------------------------
In diva2:1431611   - correct as is
----------------------------------------------------------------------
In diva2:1215619   - correct as is
----------------------------------------------------------------------
In diva2:912818   - correct as is
----------------------------------------------------------------------
In diva2:1894635 
abstract is: 
<p>This study aims to examine what apartment variables influence the contract prices of condominiums in Kungsholmen, Stockholm. Variables such as monthly fee (SEK), number of rooms, living area (square meter), access to elevator, floor level, total number of floors, and construction year are examined in order to determine whether they have a statistically significant impact on the contract price. This is accomplished by constructing multiple linear regression models using a stratification strategy. Furthermore, an analysis of the housing market is conducted to complement the attribute analysis, which aims to reflect customer preferences in real estate purchases. This since there are several additional factors, beyond customer preferences, that influence the housing market. The findings suggest that modeling contract prices with multiple linear regression is most reliable when considering condominiums with 1 or 1,5 rooms and 2 or 2,5 rooms. The results for condominiums with more rooms are more uncertain. Consistently for all stratified models, the variable living area, and the constructed variable relative floor level have a statistically significant impact on the contract price. The analysis of the housing market reveals that current, as well as expectations about future, inflation and interest rate conditions also influence the housing market and thereby contract prices. Additional factors include regulations present in the Swedish mortgage market. In this study, the mortgage ceiling and amortization requirements are considered.</p>

corrected abstract:
<p>This study aims to examine what apartment variables influence the contract prices of condominiums in Kungsholmen, Stockholm. Variables such as <em>monthly fee (SEK)</em>, <em>number of rooms</em>, <em>living area (m<sup>2</sup>)</em>, <em>access to elevator</em>, <em>floor level</em>, <em>total number of floors</em>, and <em>construction year</em> are examined in order to determine whether they have a statistically significant impact on the contract price. This is accomplished by constructing multiple linear regression models using a stratification strategy. Furthermore, an analysis of the housing market is conducted to complement the attribute analysis, which aims to reflect customer preferences in real estate purchases. This since there are several additional factors, beyond customer preferences, that influence the housing market. The findings suggest that modeling contract prices with multiple linear regression is most reliable when considering condominiums with 1 or 1,5 rooms and 2 or 2,5 rooms. The results for condominiums with more rooms are more uncertain. Consistently for all stratified models, the variable <em>living area</em>, and the constructed variable <em>relative floor level</em> have a statistically significant impact on the contract price. The analysis of the housing market reveals that current, as well as expectations about future, inflation and interest rate conditions also influence the housing market and thereby contract prices. Additional factors include regulations present in the Swedish mortgage market. In this study, the mortgage ceiling and amortization requirements are considered.</p>

Note added italics
----------------------------------------------------------------------
In diva2:942662 
abstract is: 
<p>This bachelor’s project thesis in mathematical statistics and industrial engineering and management investigates if reported crime rates had any effect on the price per square meter of housing cooperatives in Stockholm municipality between 2012 and 2014. Small and large housing cooperatives were studied in the municipality’s basområden. The analysis used multiple linear regression and OLS (Ordinary Least Squares). Many reports that model housing prices use linear regression, but mainly study the effect of economic variables. Few reports include crime as variables in their models. In this paper six categories of crime are studied, namely murder, assault, robbery, rape, car theft and burglary. The paper can contribute to improve existing valuation models for housing prices if reported crime prove to be significant.</p><p>The modeling of housing prices was done in collaboration with Valueguard AB.  The Swedish Police Authority in Stockholm provided the crime statistics that were used in the regression.</p><p>The paper consists of two parts. The first part in industrial engineering and management deals with economic indicators that may have an impact on housing prices while the other part in mathematical statistics deals with a regression analysis on the housing prices. The conclusion was that reported crime in five out of six categories had a price effect and some economic variables were also important.</p>

corrected abstract:
<p>This bachelor’s project thesis in mathematical statistics and industrial engineering and management investigates if reported crime rates had any effect on the price per square meter of housing cooperatives in Stockholm municipality between 2012 and 2014. Small and large housing cooperatives were studied in the municipality’s <em lang="sv">basområden</em>. The analysis used multiple linear regression and OLS (<em>Ordinary Least Squares</em>). Many reports that model housing prices use linear regression, but mainly study the effect of economic variables. Few reports include crime as variables in their models. In this paper six categories of crime are studied, namely murder, assault, robbery, rape, car theft and burglary. The paper can contribute to improve existing valuation models for housing prices if reported crime prove to be significant.</p><p>The modeling of housing prices was done in collaboration with Valueguard AB. The Swedish Police Authority in Stockholm provided the crime statistics that were used in the regression.</p><p>The paper consists of two parts. The first part in industrial engineering and management deals with economic indicators that may have an impact on housing prices while the other part in mathematical statistics deals with a regression analysis on the housing prices. The conclusion was that reported crime in five out of six categories had a price effect and some economic variables were also important.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1894636   - correct as is
----------------------------------------------------------------------
In diva2:1341279   - correct as is
----------------------------------------------------------------------
In diva2:1652540   - correct as is
----------------------------------------------------------------------
In diva2:868545   - correct as is
----------------------------------------------------------------------
In diva2:815817 
abstract is: 
<p>Thin cylindrical shell structures may provide an interesting breakthrough for deployable  structures for small satellites. Its bi-stable behaviour allows two different stable configurations: coiled and deployed. Several projects worldwide are using tape springs for satellites and for the SEAM project, at KTH, 1 meter long tape springs will be used for booms. This thesis investigates the energy stored inside the tape spring according to its layup configuration and the different fiber orientations. With a thickness around 0.3 mm and a length of one meter, the booms will deploy sensors with a quite low deployment speed in order to minimize the shock load during the deployment phase. A Matlab code is written to compare the stored strain energy. Another aim is to find an adequate layup all along the tape spring, it means change the fiber orientation to decrease the energy released, but also generating main manufacturing issue.</p>

corrected abstract:
<p>Thin cylindrical shell structures may provide an interesting breakthrough for deployable structures for small satellites. Its bi-stable behaviour allows two different stable configurations: coiled and deployed. Several projects worldwide are using tape springs for satellites and for the SEAM project, at KTH, 1 meter long tape springs will be used for booms.</p><p>This thesis investigates the energy stored inside the tape spring according to its layup configuration and the different fiber orientations. With a thickness around 0.3 mm and a length of one meter, the booms will deploy sensors with a quite low deployment speed in order to minimize the shick load during the deployment phase. A Matlab code is written to compare the stored strain energy. Another aim is to find an adequate layout all along the tape spring, it means change the fiber orientation to decrease the energy released, but also generating main manufacturing issue.</p>

Note - fixed the text to match the original
----------------------------------------------------------------------
In diva2:1450842   - correct as is
----------------------------------------------------------------------
In diva2:1357346   - correct as is
----------------------------------------------------------------------
In diva2:1120027 
abstract is: 
<p>Conservative dynamical systems is modelled with Lagrangian mechanics using Maple TM with the KTH developed plug-in symbolic package Sophia and simulated using Matlab®. Two double pendulum configurations and an object in a Keplerian orbit is studied. Motions and phase portraits are analysed, and numerical verifications of Kepler’s laws are performed. Properties concerning chaos is determined partly by examining sensitivity to initial conditions and it is shown that the 2D pendulum exhibits non-periodic behaviour whilst the 3D pendulum exhibits chaotic behaviour. Kepler’s laws are reproduced under certain assumptions. Finally, the applicability of Lagrangian mechanics when applied to conservative dynamical systems is evaluated.</p>

corrected abstract:
<p>Conservative dynamical systems is modelled with Lagrangian mechanics using Maple™ with the KTH developed plug-in symbolic package Sophia and simulated using Matlab®. Two double pendulum configurations and an object in a Keplerian orbit is studied. Motions and phase portraits are analysed, and numerical verifications of Kepler’s laws are performed. Properties concerning chaos is determined partly by examining sensitivity to initial conditions and it is shown that the 2D pendulum exhibits non-periodic behaviour whilst the 3D pendulum exhibits chaotic behaviour. Kepler’s laws are reproduced under certain assumptions. Finally, the applicability of Lagrangian mechanics when applied to conservative dynamical systems is evaluated.</p>

Note added the trademark symbol
----------------------------------------------------------------------
In diva2:725045 
abstract is: 
<p>This thesis examines factors that are of most statistical significance for the sales prices of apartments in the Stockholm City Centre. Factors examined are address, area, balcony, construction year, elevator, fireplace, floor number, maisonette, monthly fee, penthouse and number of rooms. On the basis of this examination, a model for predicting prices of apartments is constructed. In order to evaluate how the factors influence the price, this thesis analyses sales statistics and the mathematical method used is the multiple linear regression model. In a minor case-study and literature review, included in this thesis, the relationship between proximity to public transport and the prices of apartments in Stockholm are examined.</p><p>The result of this thesis states that it is possible to construct a model, from the factors analysed, which can predict the prices of apartments in Stockholm City Centre with an explanation degree of 91% and a two million SEK confidence interval of 95%. Furthermore, a conclusion can be drawn that the model predicts lower priced apartments more accurately. In the case-study and literature review, the result indicates support for the hypothesis that proximity to public transport is positive for the price of an apartment. However, such a variable should be regarded with caution due to the purpose of the modelling, which differs between an individual application and a social economic application</p>

corrected abstract:
<p>This thesis examines factors that are of most statistical significance for the sales prices of apartments in the Stockholm City Centre. Factors examined are <em>address</em>, <em>area</em>, <em>balcony</em>, <em>construction year</em>, <em>elevator</em>, <em>fireplace</em>, <em>floor number</em>, <em>maisonette</em>, <em>monthly fee</em>, <em>penthouse</em> and <em>number of rooms</em>. On the basis of this examination, a model for predicting prices of apartments is constructed. In order to evaluate how the factors influence the price, this thesis analyses sales statistics and the mathematical method used is the multiple linear regression model. In a minor case-study and literature review, included in this thesis, the relationship between proximity to public transport and the prices of apartments in Stockholm are examined.</p><p>The result of this thesis states that it is possible to construct a model, from the factors analysed, which can predict the prices of apartments in Stockholm City Centre with an explanation degree of 91% and a two million SEK confidence interval of 95%. Furthermore, a conclusion can be drawn that the model predicts lower priced apartments more accurately. In the case-study and literature review, the result indicates support for the hypothesis that proximity to public transport is positive for the price of an apartment. However, such a variable should be regarded with caution due to the purpose of the modelling, which differs between an individual application and a social economic application.</p>

Note added italics and missing terminal punctuaiton of the last sentence.
----------------------------------------------------------------------
In diva2:1431621   - correct as is
----------------------------------------------------------------------
abstract is: 
<p>Fintech companies that offer Buy Now, Pay Later products are heavily dependent on accurate default probability models. This is since the fintech companies bear the risk of customers not fulfilling their obligations. In order to minimize the losses incurred to customers defaulting several machine learning algorithms can be applied but in an era in which machine learning is gaining popularity, there is a vast amount of algorithms to select from. This thesis aims to address this issue by applying three fundamentally different machine learning algorithms in order to find the best algorithm according to a selection of chosen metrics such as ROCAUC and precision-recall AUC. The algorithms that were compared are Logistic Regression, Random Forest and CatBoost. All these algorithms were benchmarked against Klarna's current XGBoost model. The results indicated that the CatBoost model is the optimal one according to the main metric of comparison, the ROCAUC-score. The CatBoost model outperformed the Logistic Regression model by seven percentage points, the Random Forest model by three percentage points and the XGBoost model by one percentage point.</p>

corrected abstract:
<p>Fintech companies that offer <em>Buy Now, Pay Later</em> products are heavily dependent on accurate default probability models. This is since the fintech companies bear the risk of customers not fulfilling their obligations. In order to minimize the losses incurred to customers defaulting several machine learning algorithms can be applied but in an era in which machine learning is gaining popularity, there is a vast amount of algorithms to select from. This thesis aims to address this issue by applying three fundamentally different machine learning algorithms in order to find the best algorithm according to a selection of chosen metrics such as <em>ROCAUC</em> and <em>precision-recall AUC</em>. The algorithms that were compared are Logistic Regression, Random Forest and CatBoost. All these algorithms were benchmarked against Klarna's current XGBoost model. The results indicated that the CatBoost model is the optimal one according to the main metric of comparison, the <em>ROCAUC</em>-score. The CatBoost model outperformed the Logistic Regression model by seven percentage points, the Random Forest model by three percentage points and the XGBoost model by one percentage point.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1114275   - correct as is
----------------------------------------------------------------------
In diva2:1114349 
abstract is: 
<p>This bachelor thesis within mathematical statistics studies the possibility of modelling the renewal probability for commercial non-life insurance policyholders. The project was carried out in collaboration with the non-life insurance company If P&amp;C Insurance Ltd. at their headquarters in Stockholm, Sweden. The paper includes an introduction to underlying concepts within insurance and mathematics and a detailed review of the analytical process followed by a discussion and conclusions.</p><p>The first stages of the project were the initial collection and processing of explanatory insurance data and the development of a logistic regression model for policy renewal. An initial model was built and modern methods of mathematics and statistics were applied in order obtain a final model consisting of 9 significant characteristics. The regression model had a predictive power of 61%. This suggests that it to a certain degree is possible to predict the renewal probability of non-life insurance policyholders based on their characteristics. The results from the final model were ultimately translated into a measure of price sensitivity which can be implemented in both pricing models and CRM systems. We believe that price sensitivity analysis, if done correctly, is a natural step in improving the current pricing models in the insurance industry and this project provides a foundation for further research in this area.</p>

corrected abstract:
<p>This bachelor thesis within mathematical statistics studies the possibility of modelling the renewal probability for commercial non-life insurance policyholders. The project was carried out in collaboration with the non-life insurance company If P&amp;C Insurance Ltd. at their headquarters in Stockholm, Sweden. The paper includes an introduction to underlying concepts within insurance and mathematics and a detailed review of the analytical process followed by a discussion and conclusions.</p><p>The first stages of the project were the initial collection and processing of explanatory insurance data and the development of a logistic regression model for policy renewal. An initial model was built and modern methods of mathematics and statistics were applied in order obtain a final model consisting of 9 significant characteristics. The regression model had a predictive power of 61%. This suggests that it to a certain degree is possible to predict the renewal probability of non-life insurance policyholders based on their characteristics. The results from the final model were ultimately translated into a measure of price sensitivity which can be implemented in both pricing models and CRM systems.</p><p>We believe that price sensitivity analysis, if done correctly, is a natural step in improving the current pricing models in the insurance industry and this project provides a foundation for further research in this area.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1761924 
abstract is: 
<p>This thesis focuses on modelling non-maturing deposits (NMD) and has been written in collaboration with Svenska Handelsbanken. The methodology includes regression analysis and time series analysis, with the Repo rate serving as an exogenous variable in both models. A Vasicek model is employed to generate future Repo rates, which are then used as inputs for forecasting the NMD volume. These simulated rates are then compared to forecasted Repo rates with discrete changes from an external source. The results are utilised to analyze how net interest income can vary in the case of constant volume and in the case of interest rate-dependent volume.</p><p>Effective liquidity management is crucial for banks, and NMDs are an important source of funding. By using regression analysis and time series analysis, combined with the Repo rate as the exogenous variable, this thesis provides insights into the behaviour of NMD volumes, and how it is affected by the Repo rate. The models also enable the forecasting of future trends based on future Repo rates. Additionally, by using different data sets as input for future Repo rates, the behaviour of the model can be evaluated based on how well it coincides with reality. The results obtained from this analysis can also be used to compare the value and interest rate sensitivity of NMD products.</p><p>In conclusion, this thesis provides an approach to modelling the NMD volumes using exogenous factors and demonstrates how this can affect the net interest income from deposit volumes.</p>

corrected abstract:
<p>This thesis focuses on modelling non-maturing deposits (NMD) and has been written in collaboration with Svenska Handelsbanken. The methodology includes regression analysis and time series analysis, with the Repo rate serving as an exogenous variable in both models. A Vasicek model is employed to generate future Repo rates, which are then used as inputs for forecasting the NMD volume. Another data set including forward rates with discrete changes was also used as input data for the forecasts of the deposit volume. The results are utilised to analyse how net interest income can vary in the case of constant volume and in the case of interest rate-dependent volume.</p><p>By using regression analysis and time series analysis, with the Repo rate as the exogenous variable, this thesis provides insights into the behaviour of NMD volumes, and how it is affected by the Repo rate. The models also enable the forecasting of future trends based on future Repo rates. Additionally, by using different data sets as input for future Repo rates, the behaviour of the model can be evaluated based on how well it coincides with reality. The results obtained from this analysis can also be used to compare the value and interest rate sensitivity of NMD products.</p><p>In conclusion, this thesis provides an approach to modelling the NMD volumes using exogenous factors and demonstrates how this can affect the net interest income from deposit volumes.</p>

Note fixed the wording to match the original
----------------------------------------------------------------------
In diva2:1739347   - correct as is
----------------------------------------------------------------------
In diva2:1451464   - correct as is
----------------------------------------------------------------------
In diva2:1380258 
abstract is: 
<p>This Master’s thesis was done in collaboration with Öhlins Racing AB, a Swedish suspension system-manufacturer. For Öhlins, the quality of their products is highly important and they are therefore devoting many resources for the development and testing of their products. Because testing is such a big part of what Öhlins as a company is doing, it is important to continuously strive to improve the testing methods used within the company. Two popular methods for improving testing methods are through automation of the machine control and by simulating the test with mathematical models. Both methods have the potential to reduce the time consumed during testing. This project focuses on these two methods and is therefore split in two parts. The first part focuses on digitizing the motor control of a rolling road test bench called the Cam Drum, which is used to do life cycle tests of suspension assemblies, to allow for automated control. In the second part the rolling road test bench has been modelled as a suspension system to simulate tests prior to production. The goal of the digitization is to enable more advanced tests while simplifying usage of the Cam Drum, thereby reducing the time necessary to operate the machine. The goal of the suspension model is to get validation results that point towards the model being good enough to use as a tool when developing new products. A programmable logic controller was connected to the existing frequency drive that controls motor rotational speed and an HMI screen was used to control the controller. Communication between the controller and frequency drive used the serial protocol Modbus RTU. The hardware with which the new motor control system was built was primarily supplied by Siemens. Controller and HMI programming was carried out in Siemens’ software SIMATIC Step 7 using programming languages LAD and FBD. The digital motor control system was live tested with great results and good feedback from the technicians. The only functionality missing is being able to send webserver data over the buildings industrial network due to IT related security reasons. Future work should focus on solving this problem. A front fork suspension model and a rear swingarm suspension model have been modelled in Matlab Simulink. Both models are designed to simulate motorcycle or mountain bike suspension however the front suspension model has only been validated against mountain bike data and the rear suspension model against motorcycle data. An alternative tire model was developed to handle problems linked to conventional 1-dimensional tire models. The new model estimates the area of compressed air in the side view plane and scales the force output accordingly. New values for tire spring stiffness and damping coefficient for this system was freely estimated during validation. Validation was done using camera recorded position signals and position signals recorded with a position sensor. The front suspension model was tested against two different front fork models, but validation finally focused on several test runs done with one of the forks due to insufficient recorded data with the other fork. The result was a correlation between the behaviour of the real and modelled suspension however further tweaking of the tire parameters should give better results. The result should however be sufficient for making estimations. Validation of the rear suspension was done against a camera recorded position signal but as evidence from the front suspension validation shows this is insufficient. The rear suspension validation still requires more work before being utilized as a development tool.</p>

corrected abstract:
<p>This Master’s thesis was done in collaboration with Öhlins Racing AB, a Swedish suspension system-manufacturer. For Öhlins, the quality of their products is highly important and they are therefore devoting many resources for the development and testing of their products. Because testing is such a big part of what Öhlins as a company is doing, it is important to continuously strive to improve the testing methods used within the company.</p><p>Two popular methods for improving testing methods are through automation of the machine control and by simulating the test with mathematical models. Both methods have the potential to reduce the time consumed during testing. This project focuses on these two methods and is therefore split in two parts. The first part focuses on digitizing the motor control of a rolling road test bench called the Cam Drum, which is used to do life cycle tests of suspension assemblies, to allow for automated control. In the second part the rolling road test bench has been modelled as a suspension system to simulate tests prior to production. The goal of the digitization is to enable more advanced tests while simplifying usage of the Cam Drum, thereby reducing the time necessary to operate the machine. The goal of the suspension model is to get validation results that point towards the model being good enough to use as a tool when developing new products.</p><p>A programmable logic controller was connected to the existing frequency drive that controls motor rotational speed and an HMI screen was used to control the controller. Communication between the controller and frequency drive used the serial protocol Modbus RTU. The hardware with which the new motor control system was built was primarily supplied by Siemens. Controller and HMI programming was carried out in Siemens’ software SIMATIC Step 7 using programming languages LAD and FBD. The digital motor control system was live tested with great results and good feedback from the technicians. The only functionality missing is being able to send webserver data over the buildings industrial network due to IT related security reasons. Future work should focus on solving this problem.</p><p>A front fork suspension model and a rear swingarm suspension model have been modelled in Matlab Simulink. Both models are designed to simulate motorcycle or mountain bike suspension however the front suspension model has only been validated against mountain bike data and the rear suspension model against motorcycle data.</p><p>An alternative tire model was developed to handle problems linked to conventional 1-dimensional tire models. The new model estimates the area of compressed air in the side view plane and scales the force output accordingly. New values for tire spring stiffness and damping coefficient for this system was freely estimated during validation.</p><p>Validation was done using camera recorded position signals and position signals recorded with a position sensor. The front suspension model was tested against two different front fork models, but validation finally focused on several test runs done with one of the forks due to insufficient recorded data with the other fork. The result was a correlation between the behaviour of the real and modelled suspension however further tweaking of the tire parameters should give better results. The result should however be sufficient for making estimations.</p><p>Validation of the rear suspension was done against a camera recorded position signal but as evidence from the front suspension validation shows this is insufficient. The rear suspension validation still requires more work before being utilized as a development tool.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1686234 
abstract is: 
<p>We investigate a method for reconstructing the anterior corneal surface using wearable eye tracking devices. The presented method assumes that the four reflection points from two light sources viewed by two cameras lie on a locally spherical surface of a cornea. We then can iteratively estimate the points on the cornea surface by optimizing the local spheres’ center points and radii. Reconstructed surface heights and curvatures are thereafter obtained by polynomial regression. Synthetic data consisting of recordings of images capturing the eyes and corneal reflections from different gaze angles and different corneal shapes are generated for reconstruction and evaluation. The estimated polynomial parameters are compared to the ground truth values of the synthetic data. A spherical cornea, the simplest case used for method verification, can be reconstructed with an order of accuracy of 10−3mm for the cornea radius. We also investigate the impact of different distances between eye and camera setup on the cornea surface reconstruction. For shorter distances, fewer surface points can be computed, and the distance between points of reflection on the surface increases, making the assumption of a local spherical surface less accurate. Therefore, only a lower order of polynomials can be accurately estimated. Contrarily, for longer distances, more surface points can be computed and the assumption of a local spherical surface is more accurate. More accurate and stable results can then be obtained with surface height errors of the order 10−1mm. Given the simple setup using two cameras and ten light sources per eye, the presented method showed great potential for capturing the anterior corneal height and curvature over time. The results should not be generalized to real data without further investigation.</p>

corrected abstract:
<p>We investigate a method for reconstructing the anterior corneal surface using wearable eye tracking devices. The presented method assumes that the four reflection points from two light sources viewed by two cameras lie on a locally spherical surface of a cornea. We then can iteratively estimate the points on the cornea surface by optimizing the local spheres’ center points and radii. Reconstructed surface heights and curvatures are thereafter obtained by polynomial regression. Synthetic data consisting of recordings of images capturing the eyes and corneal reflections from different gaze angles and different corneal shapes are generated for reconstruction and evaluation. The estimated polynomial parameters are compared to the ground truth values of the synthetic data.</p><p>A spherical cornea, the simplest case used for method verification, can be reconstructed with an order of accuracy of 10<sup>−3</sup>mm for the cornea radius. We also investigate the impact of different distances between eye and camera setup on the cornea surface reconstruction. For shorter distances, fewer surface points can be computed, and the distance between points of reflection on the surface increases, making the assumption of a local spherical surface less accurate. Therefore, only a lower order of polynomials can be accurately estimated. Contrarily, for longer distances, more surface points can be computed and the assumption of a local spherical surface is more accurate. More accurate and stable results can then be obtained with cornea radius errors of the order 10<sup>−2</sup>mm.</p><p>Given the simple setup using two cameras and ten light sources per eye, the presented method showed great potential for capturing the anterior corneal height and curvature over time. The results should not be generalized to real data without further investigation.</p>

Note added missing paragraph breaks and added superscripts
----------------------------------------------------------------------
In diva2:612291   - correct as is
----------------------------------------------------------------------
In diva2:865217 
abstract is: 
<p>The present work describes the construction of a semi-analytical model for prediction of buckling loads in simply supported corrugated paperboard panels. The model accounts for transverse shear, due to the weakness of the core in such plates compared to the facings. This was done utilizing energy relations and rst order transverse shear. The panel was homogenised using laminate theory. A detailed model using FEM was derived in order to validate the predictive capabilities of the analytical model. Experimental testing was performed to estimate the accuracy of both theoretical models, and assess the limitation of the analytical model. All modes of analysis showed good agreement for cubic boxes. Further investigation into expanding the scope of the analytical model was carried out and commented on.</p>

corrected abstract:
<p>The present work describes the construction of a semi-analytical model for prediction of buckling loads in simply supported corrugated paperboard panels. The model accounts for transverse shear, due to the weakness of the core in such plates compared to the facings. This was done utilizing energy relations and first order transverse shear. The panel was homogenised using laminate theory. A detailed model using FEM was derived in order to validate the predictive capabilities of the analytical model. Experimental testing was performed to estimate the accuracy of both theoretical models, and assess the limitation of the analytical model. All modes of analysis showed good agreement for cubic boxes. Further investigation into expanding the scope of the analytical model was carried out and commented on.</p>

Note added "fi" to "rst"  to get "first" - missing ligrature
----------------------------------------------------------------------
In diva2:1752043   - correct as is
----------------------------------------------------------------------
In diva2:974083   - correct as is
----------------------------------------------------------------------
In diva2:1450562 -- full text is for the thesis diva2:1450297 !!!
abstract is: 
<p>This thesis was based on the subjects of mathematical statistics and industrial economics and management in order to analyze the grades of pupils in the final year of elementary school. The purpose was to find out what variables had a statistically significant impact on pupils’ final grades so that municipalities and schools could better understand what variables are important when trying to improve the average school results. A multiple regression model was used on data, obtained from the database of Skolverket, in order to examine what variables were statistically important. The final regression model acquired through a model reduction procedure showed that mostly structural covariates such as the academic background of pupils, percentage of female pupils and the percentage with Swedish background had a statistically significant impact on the academic performances of the students. R2 adjusted of the final model was 0.5289. The multiple regression model was discussed by referencing to previous research. In addition, the strategic management performance framework known as Balanced Scorecard which was introduced by Robert S. Kaplan and David P. Norton was used to discuss relevant key performance indicators to achieve the strategic objectives of schools.</p>

corrected abstract:
<p>This thesis was based on the subjects of mathematical statistics and industrial economics and management in order to analyze the grades of pupils in the final year of elementary school. The purpose was to find out what variables had a statistically significant impact on pupils’ final grades so that municipalities and schools could better understand what variables are important when trying to improve the average school results. A multiple regression model was used on data, obtained from the database of Skolverket, in order to examine what variables were statistically important. The final regression model acquired through a model reduction procedure showed that mostly structural covariates such as the academic background of pupils, percentage of female pupils and the percentage with Swedish background had a statistically significant impact on the academic performances of the students. R<sup>2</sup> adjusted of the final model was 0.5289. The multiple regression model was discussed by referencing to previous research. In addition, the strategic management performance framework known as Balanced Scorecard which was introduced by Robert S. Kaplan and David P. Norton was used to discuss relevant key performance indicators to achieve the strategic objectives of schools.</p>

Note at a minimum it needs the superscipt
----------------------------------------------------------------------
In diva2:1663240   - correct as is
----------------------------------------------------------------------
In diva2:1776888   - correct as is
----------------------------------------------------------------------
In diva2:1800531   - correct as is
----------------------------------------------------------------------
In diva2:1105332   - correct as is
----------------------------------------------------------------------
In diva2:938934 
abstract is: 
<p>In this thesis, we mainly study the correlation between stocks. The correlation between stocks has been receiving increasing attention. Usually the correlation is considered to be a constant, although it is observed to be varying over time. In this thesis, we study the properties of correlations between Wiener processes and introduce a stochastic correlation model. Following the calibration methods by Zetocha, we implement the calibration for a new set of market data.</p>

corrected abstract:
<p>In this thesis, we mainly study the the correlation between stocks. The correlation between stocks has been receiving increasing attention. Usually the correlation is considered to be a constant, although it is observed to be varying over time. In this thesis, we study the properties of correlations between Wiener processes and introduce a stochastic correlation model. Following the calibration methods by Zetocha, we implement the calibration for a new set of market data.</p>

Note orginal thesis has "the the"
----------------------------------------------------------------------
In diva2:1720907   - correct as is
----------------------------------------------------------------------
In diva2:1572122 
abstract is: 
<p>In this thesis we present some relevant theory, and then we rigorously investigate the existence intervals and the asymptotic behaviors of three cosmological models. The first model we investigate is based on the Friedmann-Lemaître-Robertson-Walker (FLRW) metric, which is consistent with the cosmological principle. This is a common assumption which asserts that the universe is spatially homogeneous and isotropic. The second and third models are of Bianchi type I and Bianchi type II respectively, which are both anisotropic, but spatially homogeneous models. For all models we find that the existence interval is (0,∞), meaning that they all predict an origin of the universe for some past time, while guaranteeing the existence of the universe for all future times. Furthermore we prove that in all models the universe expands exponentially for times far in the future and that the non-isotropic solutions tend towards isotropic solutions forward in time. Differences were found in the asymptotic behavior backward in time, as the FLRW-model was shown to behave like the square root for times close to t=0, while the anisotropy in the Bianchi type I and Bianchi type II models became unbounded close to t=0. It was found that there were no differences in the asymptotic behavior between the two anisotropic models. Finally we investigated some interesting aspects specific for each model. For instance the behaviour of light-like curves were analysed in the FLRW-solutions and vacuum solutions were investigated in the Bianchi type I and Bianchi type II models.</p>

corrected abstract:
<p>In this thesis we present some relevant theory, and then we rigorously investigate the existence intervals and the asymptotic behaviors of three cosmological models. The first model we investigate is based on the Friedmann-Lemaître-Robertson-Walker (FLRW) metric, which is consistent with the cosmological principle. This is a common assumption which asserts that the universe is spatially homogeneous and isotropic. The second and third models are of Bianchi type I and Bianchi type II respectively, which are both anisotropic, but spatially homogeneous models. For all models we find that the existence interval is (0, ∞), meaning that they all predict an origin of the universe for some past time, while guaranteeing the existence of the universe for all future times. Furthermore we prove that in all models the universe expands exponentially for times far in the future and that the non-isotropic solutions tend towards isotropic solutions forward in time. Differences were found in the asymptotic behavior backward in time, as the FLRW-model was shown to behave like the square root for times close to 𝑡 = 0, while the anisotropy in the Bianchi type I and Bianchi type II models became unbounded close to 𝑡 = 0. It was found that there were no differences in the asymptotic behavior between the two anisotropic models. Finally we investigated some interesting aspects specific for each model. For instance the behaviour of light-like curves were analysed in the FLRW-solutions and vacuum solutions were investigated in the Bianchi type I and Bianchi type II models.</p>

Note change the spacing of the equations and replace "t" by "𝑡"
----------------------------------------------------------------------
In diva2:1794800 
abstract is: 
<p>This thesis presents a method of solving the Helmholtz equation coupled with the Navier-Cauchy equation over a cross section of a Towed Array Sonar (TAS) modeled as a simplified cylindrical shell. The purpose of solving these equations is to estimate the transfer function of the pressure between the outside boundary and a position inside the TAS. Being able to find the transfer function in a sonar system is important since pressure that is not damped through the system is recorded as noise by acoustic sensors. This thesis focuses on the pressure generated at the surface of the TAS and how it propagates into the system. Other sources of acoustic pressure are neglected. The fluid pressure and solid material displacement over the TAS cross section are solved by utilizing the finite element method. Necessary equations are formulated on weak form and solved over a mesh depicting the cross section of interest. Pressure results inside the TAS are computed for three different frequencies of interest over several wavenumbers associated with the dimension along the TAS axis. The results show that the pressure inside the TAS is amplified mainly close to wave numbers associated with breathing waves that propagate along the TAS axis. Smaller peaks of amplification appear also for lower wave numbers, but as the wave number is increased the pressure inside the TAS is damped.</p>

corrected abstract:
<p>This thesis presents a method of solving the Helmholtz equation coupled with the Navier-Cauchy equation over a cross section of a Towed Array Sonar (TAS) modeled as a simplified cylindrical shell. The purpose of solving these equations is to estimate the transfer function of the pressure between the outside boundary and a position inside the TAS. Being able to find the transfer function in a sonar system is important since pressure that is not damped through the system is recorded as noise by acoustic sensors. This thesis focuses on the pressure generated at the surface of the TAS and how it propagates into the system. Other sources of acoustic pressure are neglected. The fluid pressure and solid material displacement over the TAS cross section are solved by utilizing the finite element method. Necessary equations are formulated on weak form and solved over a mesh depicting the cross section of interest.</p><p>Pressure results inside the TAS are computed for three different frequencies of interest over several wavenumbers associated with the dimension along the TAS axis. The results show that the pressure inside the TAS is amplified mainly close to wave numbers associated with breathing waves that propagate along the TAS axis. Smaller peaks of amplification appear also for lower wave numbers, but as the wave number is increased the pressure inside the TAS is damped.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1880374   - correct as is
----------------------------------------------------------------------
In diva2:1120414 
abstract is: 
<p>At any given moment, self-driving vehicles need to be able to perform a safe stop maneuver in order to come to a complete stop in case of an emergency. It is important to find the maximum allowed entry speed in order to follow the given emergency path and stop in time without losing grip or rolling over. A new method is developed for this purpose and compared to an existing method developed by Joseph Funke and Christian Gerdes.It is found that the new method is more general but does not always converge to a solution for bad guesses and extreme paths. Also, the existing method cannot determine the initial speed for all paths. The increased generality lies in optimizing the emergency path given by a safe zone and considering different friction coefficients in lateral and longitudinal directions. Plots are presented visualizing the maximum speed´s dependence of various parameters for a specific path. The software CarMaker by IPG Automotive is used to validate the results for the developed method. The simulations done show that the method works well for paths in two dimensions but limits the initial speed more than necessary in three dimensions. Both methods find the accelerations needed at every point which may be translated into control signals as an additional use. Methods of knowing the friction coefficients in advance are also discussed.</p>

corrected abstract:
<p>At any given moment, self-driving vehicles need to be able to perform a safe stop maneuver in order to come to a complete stop in case of an emergency. It is important to find the maximum allowed entry speed in order to follow the given emergency path and stop in time without losing grip or rolling over. A new method is developed for this purpose and compared to an existing method developed by Joseph Funke and Christian Gerdes.</p><p>It is found that the new method is more general but does not always converge to a solution for bad guesses and extreme paths. Also, the existing method cannot determine the initial speed for all paths. The increased generality lies in optimizing the emergency path given by a safe zone and considering different friction coefficients in lateral and longitudinal directions. Plots are presented visualizing the maximum speed´s dependence of various parameters for a specific path. The software CarMaker by IPG Automotive is used to validate the results for the developed method. The simulations done show that the method works well for paths in two dimensions but limits the initial speed more than necessary in three dimensions. Both methods find the accelerations needed at every point which may be translated into control signals as an additional use. Methods of knowing the friction coefficients in advance are also discussed.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:654876 
abstract is: 
<p>A new aerodynamic device, based on flow interference effects, is studied in order to significantly improve the cornering performance of racing motorcycles in MotoGP. After a brief overview on why standard downforce devices cannot be used on motorcycles, the new idea is introduced and a simplified mechanic analysis is provided to prove its effectiveness. The concept is based on the use of anhedral wings placed on the front fairing, with the rider acting as an interference device, aiming to reduce the lift generation of one wing. Numerical calculations, based on Reynolds-averaged Navier-Stokes equations, are performed on simplified static 2D and 3D cases, as a proof of concept of the idea and as a preparation for further analysis which may involve experimental wind-tunnel testing. The obtained results show that the flow interference has indeed a significant impact on the lift on a single wing. For some cases the lift can be reduced by 70% to over 90% - which strengthens the possibility of a realistic implementation.</p>

corrected abstract:
<p>A new aerodynamic device, based on flow interference effects, is studied in order to significantly improve the cornering performance of racing motorcycles in MotoGP.</p><p>After a brief overview on why standard downforce devices cannot be used on motorcycles, the new idea is introduced and a simplified mechanic analysis is provided to prove its effectiveness. The concept is based on the use of anhedral wings placed on the front fairing, with the rider acting as an interference device, aiming to reduce the lift generation of one wing. Numerical calculations, based on Reynolds-averaged Navier-Stokes equations, are performed on simplified static 2D and 3D cases, as a proof of concept of the idea and as a preparation for further analysis which may involve experimental wind-tunnel testing. The obtained results show that the flow interference has indeed a significant impact on the lift on a single wing. For some cases the lift can be reduced by 70% to over 90% - which strengthens the possibility of a realistic implementation.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:812894   - correct as is
----------------------------------------------------------------------
In diva2:1431632 
abstract is: 
<p>Today defence systems are becoming more complex as technology advances and it is of great importance to explore new ways of solving problems and keep national defence current. In particular, Artificial Intelligence (AI) is used in an increasing number of industries such as logistic solutions, inventory management and defence. This thesis will evaluate the possibility to use Reinforcement Learning (RL) in an Air Defence Coordination(ADC) scenario at Saab AB. To evaluate RL, a simplified ADC-scenario is considered and solved using two different methods, Q-learning and Deep Q-learning (DQL).</p><p>The results of the two methods are discussed as well as the limitations in scope and complexity for Q-learning. Deep Q-learning, on the other hand shows to be relatively easy to apply to more complicated scenarios. Finally, one last experiment with a far more complex scenario is constructed in order to show the scalability of DQL and create a foundation for future work in this field.</p>

corrected abstract:
<p>Today defence systems are becoming more complex as technology advances and it is of great importance to explore new ways of solving problems and keep national defence current. In particular, Artificial Intelligence (AI) is used in an increasing number of industries such as logistic solutions, inventory management and defence. This thesis will evaluate the possibility to use Reinforcement Learning (RL) in an Air Defence Coordination (ADC) scenario at Saab AB. To evaluate RL, a simplified ADC-scenario is considered and solved using two different methods, Q-learning and Deep Q-learning (DQL). The results of the two methods are discussed as well as the limitations in scope and complexity for Q-learning. Deep Q-learning, on the other hand shows to be relatively easy to apply to more complicated scenarios. Finally, one last experiment with a far more complex scenario is constructed in order to show the scalability of DQL and create a foundation for future work in this field.</p>

Note eliminates an unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1794320   - correct as is
----------------------------------------------------------------------
In diva2:1319876 
abstract is: 
<p>Textual data is one of the most widespread forms of data and the amount of such data available in the world increases at a rapid rate. Text can be understood as either a sequence of characters or words, where the latter approach is the most common. With the breakthroughs within the area of applied artificial intelligence in recent years, more and more tasks are aided by automatic processing of text in various applications. The models introduced in the following sections rely on deep-learning sequence-processing in order to process and text to produce a regression algorithm for classification of what the text input refers to. We investigate and compare the performance of several model architectures along with different hyperparameters. The data set was provided by e-Avrop, a Swedish company which hosts a web platform for posting and bidding of public procurements. It consists of titles and descriptions of Swedish public procurements posted on the website of e-Avrop, along with the respective category/categories of each text. When the texts are described by several categories (multi label case) we suggest a deep learning sequence-processing regression algorithm, where a set of deep learning classifiers are used. Each model uses one of the several labels in the multi label case, along with the text input to produce a set of text - label observation pairs. The goal becomes to investigate whether these classifiers can carry out different levels of intent, an intent which should theoretically be imposed by the different training data sets used by each of the individual deep learning classifiers.</p>

corrected abstract:
<p>Textual data is one of the most widespread forms of data and the amount of such data available in the world increases at a rapid rate. Text can be understood as either a sequence of characters or words, where the latter approach is the most common. With the breakthroughs within the area of applied artificial intelligence in recent years, more and more tasks are aided by automatic processing of text in various applications. The models introduced in the following sections rely on deep-learning sequence-processing in order to process and text to produce a regression algorithm for classification of what the text input refers to. We investigate and compare the performance of several model architectures along with different hyperparameters.</p><p>The data set was provided by e-Avrop, a Swedish company which hosts a web platform for posting and bidding of public procurements. It consists of titles and descriptions of Swedish public procurements posted on the website of e-Avrop, along with the respective category/categories of each text.</p><p>When the texts are described by several categories (multi label case) we suggest a deep learning sequence-processing regression algorithm, where a set of deep learning classifiers are used. Each model uses one of the several labels in the multi label case, along with the text input to produce a set of text - label observation pairs. The goal becomes to investigate whether these classifiers can carry out different levels of intent, an intent which should theoretically be imposed by the different training data sets used by each of the individual deep learning classifiers.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1577386   - correct as is
----------------------------------------------------------------------
In diva2:1211759 
abstract is: 
<p>An apartment owner who is not closely monitoring the rental market may have no idea what to set as a price if he or she ever were to sublet their apartment. In addition to having a rent-controlled market, Stockholm has also experienced a price hike in apartment prices the last few years. In this study, a linear model for estimating the rental price of an apartment in Stockholm has been developed by using a dataset of signed rental contracts in Stockholm. For anyone wishing to rent out their apartment this study can offer a price interval supported by a mathematical model. In this study, it is shown that a cross-validated linear model could explain more than 2/3s of the variation of the rental price of an apartment in Stockholm by four simple parameters, m2, room count, distance from central Stockholm and rental contract length. This study lays the ground work for anyone wishing to apply the same concept in other geographical areas or complete the dataset for even more precision.</p>

corrected abstract:
<p>An apartment owner who is not closely monitoring the rental market may have no idea what to set as a price if he or she ever were to sublet their apartment. In addition to having a rent-controlled market, Stockholm has also experienced a price hike in apartment prices the last few years. In this study, a linear model for estimating the rental price of an apartment in Stockholm has been developed by using a dataset of signed rental contracts in Stockholm. For anyone wishing to rent out their apartment this study can offer a price interval supported by a mathematical model.</p><p>In this study, it is shown that a cross-validated linear model could explain more than 2/3s of the variation of the rental price of an apartment in Stockholm by four simple parameters, <em>m<sup>2</sup></em>, <em>room count</em>, <em>distance from central Stockholm</em> and <em>rental contract length</em>. This study lays the ground work for anyone wishing to apply the same concept in other geographical areas or complete the dataset for even more precision.</p>

Note added superscript, italics, and added missing paragraph breaks
......................................................................
In diva2:1514600   - correct as is
----------------------------------------------------------------------
In diva2:735913 
abstract is: 
<p>The focus of this thesis is to find efficient ways of solving certain types of <em>ODEs </em>and <em>PDEs</em>. We have implemented a time upscaling method called Multiscale timestepping technique for this problems. In this method discretization of <em>PDEs </em>are transformed into wavelet basis, which divides the solution and the discretized differential operator into coarse scales and fine scales. Larger time steps are then used for solving the fine scale elements. In numerical experiments we show that the accuracy of the solution is maintained but the computational cost is significantly reduced compared to standard methods.</p>

corrected abstract:
<p>The focus of this thesis is to find efficient ways of solving certain types of <em>ODEs</em> and <em>PDEs</em>. We have implemented a time upscaling method called Multiscale timestepping technique for this problems. In this method discretization of <em>PDEs</em> are transformed into wavelet basis, which divides the solution and the discretized differential operator into coarse scales and fine scales. Larger time steps are then used for solving the fine scale elements. In numerical experiments we show that the accuracy of the solution is maintained but the computational cost is significantly reduced compared to standard methods.</p>

Note - slight change to the ragion that is in italics
----------------------------------------------------------------------
In diva2:1900967   - correct as is
----------------------------------------------------------------------
In diva2:865397   - correct as is
----------------------------------------------------------------------
In diva2:510132 
abstract is: 
<p>The Swedish government have a goal that wind turbines shall produce 30 TWh by the year 2020, compared to about 3,5 TWh produced in Sweden during 2010. To minimize the disturbance that wind turbines create the Swedish Environmental Protection Agency has stated a guideline value that noise from wind turbines at nearby residents shouldn’t exceed 40 dBA, at 8 m/s wind speed. With advanced calculation models, like Nord 2000, the emitted sound from future wind turbines can be calculated at an early stage to optimize the power production without exceeding the 40 dBA. When the turbines have been built there is a need to verify that they really are within the guideline. To measure the sound pressure level at nearby residents, a sound immission measurement, there is a Swedish method, Elforsk 98:24 (1) with the translated titled “measuring of noise immission from wind turbines”. There is also an international standard IEC 61400-11 (2) focused on how to measure the emitted sound power level, also called sound emission measurement. To verify the sound level at a resident building a sound immission measurement is the preferred method. However this method has some practical limitations, for example the background sound level is often higher than the guideline value of 40 dBA, which makes it hard to estimate the equivalent sound pressure level of the wind turbine. A further development of this method was outlined in a draft translated “Measuring of sound from wind turbines –draft 2005” (3). The method in this draft is based on a sound emission measurement, compatible with the IEC 61400-11, in combination with a simple formula to calculate the sound immission level. The draft also contains a method for measuring the sound attenuation from the wind turbine to the resident, which can be used together with the measured sound power level to calculate the sound immission level. The intention of this thesis is to further develop today’s method for sound immission measurement of wind turbines. To achieve this, theoretical studies have been undertaken to study the elements relevant to the measurement, like sound generation, attenuation and relevant measurement equipment. Swedish and international measurement methods and guideline values have been studied. Measurements based on the draft (3) have been performed and shows that the method of measuring the sound attenuation, outlined in the draft, can provide results where the present method would fail. But the sound attenuation measuring method is still hampered by high background sound levels and has some practical limitations compared to the present method Elforsk 98:24. Conclusions from this thesis is that calculations of sound immission levels, based on measured sound power level, is a method that is and will continue to be a part of the assessment of sound levels from wind turbines. To ensure high quality of these assessments the procedure should be standardized and the use of advanced calculation models should be included. In cases where the sound attenuation path crosses hilly terrain, water or where the guideline values are exceeded, calculations with more advanced method should be advocated.</p>

corrected abstract:
<p>The Swedish government have a goal that wind turbines shall produce 30 TWh by the year 2020, compared to about 3,5 TWh produced in Sweden during 2010. To minimize the disturbance that wind turbines create the Swedish Environmental Protection Agency has stated a guideline value that noise from wind turbines at nearby residents shouldn’t exceed 40 dBA, at 8 m/s wind speed. With advanced calculation models, like Nord 2000, the emitted sound from future wind turbines can be calculated at an early stage to optimize the power production without exceeding the 40 dBA.</p><p>When the turbines have been built there is a need to verify that they really are within the guideline. To measure the sound pressure level at nearby residents, a sound immission measurement, there is a Swedish method, Elforsk 98:24 (1) with the translated titled “measuring of noise immission from wind turbines”. There is also an international standard IEC 61400-11 (2) focused on how to measure the emitted sound power level, also called sound emission measurement. To verify the sound level at a resident building a sound immission measurement is the preferred method. However this method has some practical limitations, for example the background sound level is often higher than the guideline value of 40 dBA, which makes it hard to estimate the equivalent sound pressure level of the wind turbine.</p><p>A further development of this method was outlined in a draft translated “Measuring of sound from wind turbine s– draft 2005” (3). The method in this draft is based on a sound emission measurement, compatible with the IEC 61400-11, in combination with a simple formula to calculate the sound immission level. The draft also contains a method for measuring the sound attenuation from the wind turbine to the resident, which can be used together with the measured sound power level to calculate the sound immission level.</p><p>The intention of this thesis is to further develop today’s method for sound immission measurement of wind turbines. To achieve this, theoretical studies have been undertaken to study the elements relevant to the measurement, like sound generation, attenuation and relevant measurement equipment. Swedish and international measurement methods and guideline values have been studied.</p><p>Measurements based on the draft (3) have been performed and shows that the method of measuring the sound attenuation, outlined in the draft, can provide results where the present method would fail. But the sound attenuation measuring method is still hampered by high background sound levels and has some practical limitations compared to the present method Elforsk 98:24.</p><p>Conclusions from this thesis is that calculations of sound immission levels, based on measured sound power level, is a method that is and will continue to be a part of the assessment of sound levels from wind turbines. To ensure high quality of these assessments the procedure should be standardized and the use of advanced calculation models should be included. In cases where the sound attenuation path crosses hilly terrain, water or where the guideline values are exceeded, calculations with more advanced method should be advocated.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1701297   - correct as is
----------------------------------------------------------------------
In diva2:1791350 
abstract is: 
<p>This report consists of two parts. Firstly, an introduction to neutrino oscillations is presented. This includes introducing neutrino mass terms into the standard model, the leptonic mixing matrix, neutrino oscillations in vacuum and neutrino oscillations in matter. Furthermore, the concept of scalar non-standard interactions is introduced and how neutrino oscillations are altered by them. Secondly, scalar non-standard interactions are investigated in the context of the ESSnuSB experiment. The investigation is done using the GLoBES software package. With it we investigate the effects of scalar non-standard interaction on the probability curves, we check the sensitivity of the scalar non-standard interaction parameters at ESSnuSB, with which we can determine the expected three sigma range of ESSnuSB for the real diagonal SNSI parameters. Finally we look at the main question this report is investigating, which is how the SNSI parameters affect the ability for ESSnuSB to claim charge-parity violation. We find that SNSI can have a significant effect on the ability for ESSnuSB to claim charge-parity violation in the lepton sector. For certain values of the SNSI parameters, delta CP is suppressed in the oscillation probability, resulting in a drastic worsening of this ability.</p>

corrected abstract:
<p>This report consists of two parts. Firstly, an introduction to neutrino oscillations is presented. This includes introducing neutrino mass terms into the standard model, the leptonic mixing matrix, neutrino oscillations in vacuum and neutrino oscillations in matter. Furthermore, the concept of scalar non-standard interactions is introduced and how neutrino oscillations are altered by them. Secondly, scalar non-standard interactions are investigated in the context of the ESSnuSB experiment. The investigation is done using the GLoBES software package. With it we investigate the effects of scalar non-standard interaction on the probability curves, we check the sensitivity of the scalar non-standard interaction parameters at ESSnuSB, with which we can determine the expected 3σ range of ESSnuSB for the real diagonal SNSI parameters. Finally we look at the main question this report is investigating, which is how the SNSI parameters affect the ability for ESSnuSB to claim charge-parity violation. We find that SNSI can have a significant effect on the ability for ESSnuSB to claim charge-parity violation in the lepton sector. For certain values of the SNSI parameters, δ<sub>CP</sub> is suppressed in the oscillation probability, resulting in a drastic worsening of this ability.</p>


Note error in original "ESnuSB" should be "ESSnuSB", added subscript and replace the string "three sigma" and "delta" with the symbols - as per the original
----------------------------------------------------------------------
In diva2:1897932   - correct as is
----------------------------------------------------------------------
In diva2:1739362 - missing space in title:
"NewSpace mission analysis:A case study of how to reduce the information gap between mission owner and supplier"==>
"NewSpace mission analysis: A case study of how to reduce the information gap between mission owner and supplier"
abstract   - correct as is
----------------------------------------------------------------------
In diva2:1303915 
abstract is: 
<p>Particles with energies higher than 3. 10.19 eV are generally referred to as Ultra- High Energy Cosmic Rays (UHECRs) and their sources are currently unknown. The observations of these particles are performed during the night from ground observatories or telescopes mounted on stratospheric balloons or, in the near future, from the International Space Station (ISS). In this report, different orbit types are analysed and compared with the ISS in terms of the duration of the sub-point in night-time (umbra), the observed area on the ground in umbra and estimations of the observable number of events.</p>

corrected abstract:
<p>Particles with energies higher than 3 · 10<sup>19</sup> eV are generally referred to as <em>Ultra-High Energy Cosmic Rays</em> (UHECRs) and their sources are currently unknown. The observations of these particles are performed during the night from ground-observatories or telescopes mounted on stratospheric balloons or, in the near future, from the International Space Station (ISS). In this report, different orbit types are analysed and compared with the ISS in terms of the duration of the sub-point in night-time (umbra), the observed area on the ground in umbra and estimations of the observable number of events.</p><p>The most promising results were for a slightly elliptic orbit with a maximum altitude of 1000 km and minimum altitude of 200 km (half of the altitude of the ISS), with 7 % increased night-time compared to the ISS. Even though the observation time itself is not significantly prolonged, the observed area in nadir mode on the Earth’s surface during the course of a year is predicted to be 250 % larger as a consequence of the increased altitude. The tilted mode (tilt angle 25º) would have an increase of 540 % compared to the ISS in nadir mode and 300 % increase compared to the ISS in tilted mode with the same angle. Whilst a sunsynchronous orbit in tilted mode (tilt angle 30º) at an altitude of 800 km would increase the observed area by 1 260 % compared to the nadir mode and 750 % compared to the tilted mode of ISS. Both orbit types increase the observational area on the surface, however, this is not enough to estimate the possible number of annually observed UV-light events relating to UHECR. For this estimation the altitude and the telescope’s efficiency have to be evaluated together with the observational area on the surface. In conclusion it is estimated that it is possible to increase the number of annually observed events by 260 % for the elliptic orbit in nadir mode compared to the ISS in the range of 3·10<sup>19</sup> − 10<sup>21</sup> eV.  Further investigation into adjusting the ascending node to constantly place the perigee at zenith is of interest. Moreover, to simulate a tilted sun-synchronuous orbit with a tilt angle between 30º < ξ < 40º at an altitude of 800 km or lower and analysing the number of observable events in comparison to the ISS.</p>

Missing text, fixed superscripts and centered dots, and fixed degree symbols ("Y")
----------------------------------------------------------------------
In diva2:1102665   - correct as is
----------------------------------------------------------------------
In diva2:1320100 
abstract is: 
<p>This report compares the effectiveness of three statistical methods for predicting defecting viewers in SVT's video on demand (VOD) services: logistic regression, random forests, and long short-term memory recurrent neural networks (LSTMs). In particular, the report investigates whether or not sequential data consisting of users' weekly watch histories can be used with LSTMs to achieve better predictive performance than the two other methods. The study found that the best LSTM models did outperform the other methods in terms of precision, recall, F-measure and AUC – but not accuracy. Logistic regression and random forests offered comparable performance results. The models are however subject to several notable limitations, so further research is advised.</p>

corrected abstract:
<p>This report compares the effectiveness of three statistical methods for predicting defecting viewers in SVT's video on demand (VOD) services: logistic regression, random forests, and long short-term memory recurrent neural networks (LSTMs). In particular, the report investigates whether or not sequential data consisting of users' weekly watch histories can be used with LSTMs to achieve better predictive performance than the two other methods. The study found that the best LSTM models did outperform the other methods in terms of precision, recall, 𝐹-measure and AUC – but not accuracy. Logistic regression and random forests offered comparable performance results. The models are however subject to several notable limitations, so further research is advised.</p>

Note replaced "F" by "𝐹"
----------------------------------------------------------------------
In diva2:1686223 
abstract is: 
<p>Non-intrusive load monitoring (NILM) refers to a set of statistical methods for inferring information about a household from its electricity load curve, without adding any additional sensor. The aim of this master thesis is to adapt NILM techniques for the assessment of the efficiency of retrofitting work to provide a first version of a retrofitting assessment tool. Two models are developed: a model corresponding to a constrained optimization problem, and a hierarchical Bayesian mixture model. These models are tested on a set of houses that have electric heating (which are the main target of retrofitting work). These models offer a satisfactory accuracy retrofitting assessment for about half of the houses.</p>

corrected abstract:
<p><em>Non-intrusive load monitoring</em> (NILM) refers to a set of statistical methods for inferring information about a household from its electricity load curve, without adding any additional sensor. The aim of this master thesis is to adapt NILM techniques for the assessment of the efficiency of retrofitting work to provide a first version of a retrofitting assessment tool. Two models are developed: a model corresponding to a constrained optimization problem, and a hierarchical Bayesian mixture model. These models are tested on a set of houses that have electric heating (which are the main target of retrofitting work). These models offer a satisfactory accuracy retrofitting assessment for about half of the houses.</p>

Added italics
----------------------------------------------------------------------
In diva2:818695   - correct as is
----------------------------------------------------------------------
In diva2:1062255   - correct as is
----------------------------------------------------------------------
In diva2:1380114 
abstract is: 
<p>For rattle event prediction accurate models capable of estimating system response at impact are required. In this thesis, a finite element model is built to imitate a physical setup designed for testing rattle. First, a modal analysis is performed in ABAQUS/Standard to validate the CAE model. Second, rattle is simulated in ABAQUS/Explicit and a correlation between the simulated and the experimental results is sought. Third, a sensitivity analysis is performed to investigate the effects of selected contact parameters on rattle-sensitive criteria. The important parameters are then identified, and preliminary guidelines are given for future CAE modelling of rattle.  </p>

corrected abstract:
<p>For rattle event prediction accurate models capable of estimating system response at impact are required. In this thesis, a finite element model is built to imitate a physical setup designed for testing rattle. First, a modal analysis is performed in ABAQUS/Standard to validate the CAE model. Second, rattle is simulated in ABAQUS/Explicit and a correlation between the simulated and the experimental results is sought. Third, a sensitivity analysis is performed to investigate the effects of selected contact parameters on rattle-sensitive criteria. The important parameters are then identified, and preliminary guidelines are given for future CAE modelling of rattle.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1130070   - correct as is
----------------------------------------------------------------------
In diva2:766806 
abstract is: 
<p>There is large theoretical, experimental and numerical interest in studying boundary layers, which develop around any body moving through a fluid. The simplest of these boundary layers lead to the theoretical abstraction of a so-called Blasius boundary layer, which can be derived under the assumption of a flat plate and zero external pressure gradient. The Blasius solution is characterised by a slow growth of the boundary layer in the streamwise direction. For practical purposes, in particular related to studying transition scenarios, non-linear finite-amplitude states (exact coherent states, edge states), but also for turbulence, a major simplification of the problem could be attained by removing this slow streamwise growth, and instead consider a parallel boundary layer. Parallel boundary layers are found in reality, e.g. when applying suction (asymptotic suction boundary layer) or rotation (Ekman boundary layer), but not in the Blasius case. As this is only a model which is not an exact solution to the Navier-Stokes (or boundary-layer) equations, some modifications have to be introduced into the governing equations in order for such an approach to be feasible. Spalart and Yang introduced a modification term to the governing Navier-Stokes equations in 1987. In this thesis work, we adapted the amplitude of the modification term introduced by Spalart and Yang to identify the nonlinear states in the parallel Blasius boundary layer. A final application of this modification was in determining the so-called edge states for boundary layers, previously found in the asymptotic suction boundary layer</p>

corrected abstract:
<p>There is large theoretical, experimental and numerical interest in studying boundary layers, which develop around any body moving through a fluid. The simplest of these boundary layers lead to the theoretical abstraction of a so-called Blasius boundary layer, which can be derived under the assumption of a flat plate and zero external pressure gradient. The Blasius solution is characterised by a slow growth of the boundary layer in the streamwise direction. For practical purposes, in particular related to studying transition scenarios, non-linear finite-amplitude states (exact coherent states, edge states), but also for turbulence, a major simplification of the problem could be attained by removing this slow streamwise growth, and instead consider a parallel boundary layer. Parallel boundary layers are found in reality, e.g. when applying suction (asymptotic suction boundary layer) or rotation (Ekman boundary layer), but not in the Blasius case. As this is only a model which is not an exact solution to the Navier-Stokes (or boundary-layer) equations, some modifications have to be introduced into the governing equations in order for such an approach to be feasible. Spalart and Yang introduced a modification term to the governing Navier-Stokes equations in 1987. In this thesis work, we adapted the amplitude of the modification term introduced by Spalart and Yang to identify the nonlinear states in the parallel Blasius boundary layer. A final application of this modification was in determining the so-called edge states for boundary layers, previously found in the asymptotic suction boundary layer.</p>

Note added terminal period ot last sentence (as per original),
----------------------------------------------------------------------
In diva2:1105331 - missing space intitle:
"Non-parametricbacktesting of expected shortfall"
==>
"Non-parametric backtesting of expected shortfall"

abstract is: 
<p>Since the Basel Committee on Banking Supervision first suggested a transition to Expected Shortfall as the primary risk measure for financial institutions, the question on how to backtest it has been widely discussed. Still, there is a lack of studies that compare the different proposed backtesting methods. This thesis uses simulations and empirical data to evaluate the performance of non-parametric backtests under different circumstances. An important takeaway from the thesis is that the different backtests all use some kind of trade-off between measuring the number of Value at Risk exceedances and their magnitudes. The main finding of this thesis is a list, ranking the non-parametric backtests. This list can be used to choose backtesting method by cross-referencing to what is possible to implement given the estimation method that the financial institution uses. </p>

corrected abstract:
<p>Since the Basel Committee on Banking Supervision first suggested a transition to Expected Shortfall as the primary risk measure for financial institutions, the question on how to backtest it has been widely discussed. Still, there is a lack of studies that compare the different proposed backtesting methods. This thesis uses simulations and empirical data to evaluate the performance of non-parametric backtests under different circumstances. An important takeaway from the thesis is that the different backtests all use some kind of trade-off between measuring the number of Value at Risk exceedances and their magnitudes. The main finding of this thesis is a list, ranking the non-parametric backtests. This list can be used to choose backtesting method by cross-referencing to what is possible to implement given the estimation method that the financial institution uses.</p>

Note only change was to eliminate an unnecessary space at the end of last paragraph
----------------------------------------------------------------------
In diva2:1040708   - correct as is
----------------------------------------------------------------------
In diva2:612017 - missing spaces in title:
"Numerical and experimental investigation of the effectof geometry modification on the aerodynamic characteristics of a NACA 64(2)-415wing"
==>
"Numerical and experimental investigation of the effect of geometry modification on the aerodynamic characteristics of a NACA 64(2)-415 wing"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1839587 
abstract is: 
<p>The experimentally obtained value of the critical exponent ν is presently in significant disagreement with current theoretical predictions for the λ-universality class. We suggest two novel approaches of determining the exponents ν and η by utilizing the effects of finite size scaling. The numerical computations are performed using Monte Carlo simulations of a 3D XY model, realized on a bc-lattice. Different sizes of systems are then either compared in pairs (pairwise fit) or all together (joint fit), in order to fit the correct value of the critical exponents to our sampled data. We find for the pairwise fitting procedure that ν = 0.6731(36) and η = 0.0351(39). Likewise, the joint fitting procedure yields ν = 0.6727(58) and η = 0.0349(49). The predictions for ν are very consistent with existing works, while the values for η are somewhat lower than expected from existing literature results.</p>

corrected abstract:
<p>The experimentally obtained value of the critical exponent <em>ν</em> is presently in significant disagreement with current theoretical predictions for the <em>λ</em>-universality class. We suggest two novel approaches of determining the exponents <em>ν</em> and <em>η</em> by utilizing the effects of finite size scaling. The numerical computations are performed using Monte Carlo simulations of a 3D XY model, realized on a 𝑏𝑐-lattice. Different sizes of systems are then either compared in pairs (<em>pairwise fit</em>) or all together (<em>joint fit</em>), in order to fit the correct value of the critical exponents to our sampled data. We find for the pairwise fitting procedure that <em>ν</em> = 0.6731(36) and <em>η</em> = 0.0351(39). Likewise, the joint fitting procedure yields <em>ν</em> = 0.6727(58) and <em>η</em> = 0.0349(49). The predictions for ν are very consistent with existing works, while the values for η are somewhat lower than expected from existing literature results.</p>

Note replace "bc" with "𝑏𝑐", added italics to the Greek symbols, and added italics
----------------------------------------------------------------------
In diva2:1678945 
abstract is: 
<p>This study is an investigation of analytical and numerical predictions of the relativistic perihelion shift of the planets in the Solar System. These shifts are a consequence of the general theory of relativity and the theoretical expression for the perihelion shift can be derived using different approximations. Two of these approximations are presented in this report. Due to these approximations, it is of interest to investigate when the analytical expression provides accurate values of perihelion shifts. This is performed by calculating the perihelion shifts for the planets in the Solar System numerically and comparing the results. To summarize the results, the numerical method slightly outperforms the analyt- ical method for almost all planets in the Solar System but the results are overall similar. Furthermore, a parameter study is conducted to investigate how the numerical and an- alytical perihelion shift predictions are dependent on these parameters. The parameter study shows that the methods diverge for some cases and based on this predictions are made for when numerical methods could be of use when predicting perihelion shifts.</p>

corrected abstract:
<p>This study is an investigation of analytical and numerical predictions of the relativistic perihelion shift of the planets in the Solar System. These shifts are a consequence of the general theory of relativity and the theoretical expression for the perihelion shift can be derived using different approximations. Two of these approximations are presented in this report. Due to these approximations, it is of interest to investigate when the analytical expression provides accurate values of perihelion shifts. This is performed by calculating the perihelion shifts for the planets in the Solar System numerically and comparing the results. To summarize the results, the numerical method slightly outperforms the analytical method for almost all planets in the Solar System but the results are overall similar. Furthermore, a parameter study is conducted to investigate how the numerical and analytical perihelion shift predictions are dependent on these parameters. The parameter study shows that the methods diverge for some cases and based on this predictions are made for when numerical methods could be of use when predicting perihelion shifts.</p>

Note removed unnecessary hyphens
----------------------------------------------------------------------
In diva2:1040712 
abstract is: 
<p>This thesis includes a numerical comparison of different turbulence models and particle models in terms of convergence time and physical accuracy. A cyclone is used as the computational domain. Cyclones are common devices for separating two or more substances. The work is divided into an experimental part and a numerical part. In the experiments, characteristics of the cyclone were measured. This data is then used to evaluate different numerical modeling approaches. The numerical part consists of two parts, namely single phase flow and multiphase flow, where different modeling aspects are examined and presented. Furthermore, important parameters that characterize a cyclone, such as pressure drop and separation efficiency, are calculated. The separation efficiency, i.e. how much dust that actually goes to the dust bin, is calculated for two different types of dust. The software used for the numerical simulations has been Star-CCM+.</p>

corrected abstract:
<p>This thesis includes a numerical comparison of different turbulence models and particle models in terms of convergence time and physical accuracy. A cyclone is used as the computational domain. Cyclones are common devices for separating two or more substances. The work is divided into an experimental part and a numerical part. In the experiments, characteristics of the cyclone were measured. This data is then used to evaluate different numerical modeling approaches.</p><p>The numerical part consists of two parts, namely single phase flow and multiphase flow, where different modeling aspects are examined and presented. Furthermore, important parameters that characterize a cyclone, such as pressure drop and separation efficiency, are calculated. The separation efficiency, i.e. how much dust that actually goes to the dust bin, is calculated for two different types of dust. The software used for the numerical simulations has been Star-CCM+.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:784017   - correct as is
----------------------------------------------------------------------
In diva2:483399 
abstract is: 
<p>The thesis work is carried out in Innventia AB and Royal Institute of Technology (KTH). The objective of the study was to numerically simulate the flow inside the disc refiner and to determine the factors influencing the development of flow in the rotor and the stator of the disc refiner. Simplified single groove model is used to analyze the flow in the rotor and stator of the refiner. Fluid is assumed to be Newtonian and single phase with dynamic viscosity 100 times higher than water. Model and mesh used for the rotor and stator are identical with different wall boundary conditions and fluid zone conditions. Simulations were performed with different pressure gradients and angular speed of the refiner. The study shows that the flow in the rotor depends on the pressure difference and the speed of the refiner. Flow in the stator depends on the pressure difference; speed has little effect on the flow in the stator. In the rotor fluid flows towards the periphery while in the stator direction of flow is towards origin. Rotational motion is observed both in the rotor and the stator groove. This rotational motion carries the fluid into the gap, fibres present in the fluid form flocs and got stapled at the bar edge. They get treated every time a rotating bar crosses a stationary bar. To study the intensity of refining, strain rate and shear force is determined. Shear force is due to the rotation of the rotor and due to the motion of the fluid down into the groove along its wall i.e. fluid that is carried along with the rotating plate.</p>

corrected abstract:
<p>The thesis work is carried out in Innventia AB and Royal Institute of Technology (KTH). The objective of the study was to numerically simulate the flow inside the disc refiner and to determine the factors influencing the development of flow in the rotor and the stator of the disc refiner.</p><p>Simplified single groove model is used to analyze the flow in the rotor and stator of the refiner. Fluid is assumed to be Newtonian and single phase with dynamic viscosity 100 times higher than water. Model and mesh used for the rotor and stator are identical with different wall boundary conditions and fluid zone conditions. Simulations were performed with different pressure gradients and angular speed of the refiner.</p><p>The study shows that the flow in the rotor depends on the pressure difference and the speed of the refiner. Flow in the stator depends on the pressure difference; speed has little effect on the flow in the stator. In the rotor fluid flows towards the periphery while in the stator direction of flow is towards origin. Rotational motion is observed both in the rotor and the stator groove. This rotational motion carries the fluid into the gap, fibres present in the fluid form flocs and got stapled at the bar edge. They get treated every time a rotating bar crosses a stationary bar. To study the intensity of refining, strain rate and shear force is determined. Shear force is due to the rotation of the rotor and due to the motion of the fluid down into the groove along its wall i.e. fluid that is carried along with the rotating plate.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1207066 - missing space in title:
"Numerical simulations of Carbon Fiber ReinforcedPolymers under dynamic loading"
==
"Numerical simulations of Carbon Fiber Reinforced Polymers under dynamic loading"


abstract   - correct as is
----------------------------------------------------------------------
In diva2:891965 
abstract is: 
<p>Automotive parts may be the cause of very annoying friction-induced noise and the source of many customer complaints. Indeed, when a wiper operates on a windshield, vibratory phenomena may appear due to flutter instabilities and may generate squeal noise. As squeal noise generated by wiper system is a random and complex phenomenon, there are only few studies dealing with the wiper noise. The complexity of this phenomenon is due to the cinematic of the movement and to the various environmental parameters which have an influence on the appearance of the noise. This master thesis is a research and development project and presents a numerical simulation methodology used in the aim to reduce and eradicate squeal noise of wiper systems.  In the first part, the finite element model representing a wiper system and the numerical simulation methodology will be presented in detail. In the second part, stability analysis will be carried out in nominal studies and in designs of experiments. Parametric studies will also be achieved to understand the behavior and the influence of each considered input parameters. Two wiper blades, with the same geometry but with different material, will be considered for the different studies. These two wiper blades will be examined to figure out when squeal noises appear.</p>

corrected abstract:
<p>Automotive parts may be the cause of very annoying friction-induced noise and the source of many customer complaints. Indeed, when a wiper operates on a windshield, vibratory phenomena may appear due to flutter instabilities and may generate squeal noise. As squeal noise generated by wiper system is a random and complex phenomenon, there are only few studies dealing with the wiper noise. The complexity of this phenomenon is due to the cinematic of the movement and to the various environmental parameters which have an influence on the appearance of the noise. This master thesis is a research and development project and presents a numerical simulation methodology used in the aim to reduce and eradicate squeal noise of wiper systems.</p><p>In the first part, the finite element model representing a wiper system and the numerical simulation methodology will be presented in detail.</p><p>In the second part, stability analysis will be carried out in nominal studies and in designs of experiments. Parametric studies will also be achieved to understand the behavior and the influence of each considered input parameters. Two wiper blades, with the same geometry but with different material, will be considered for the different studies. These two wiper blades will be examined to figure out when squeal noises appear.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1778843   - correct as is
----------------------------------------------------------------------
In diva2:1306988 - missing spaces in title:
"Numerical simulations of thedecomposition of a greenpropellant"
==>
"Numerical simulations of the decomposition of a green propellant"

abstract is: 
<p>Concerns about the use of certain chemical species within the aerospace field are growing in recent years. A European regulation, REACh, now makes the use of hydrazine uncertain in – among others- attitude control thrusters. Green monopropellants, which are alternatives for this species already exist, but they all require a catalyst to react. Catalysts constitute the limiting factor for the lifespan of satellites because of the number of thermal cycles they endure. A joint project between ONERA, the French aerospace research center and CNES, the French space agency, was born to develop a high-performance green monopropellant thruster operating without any catalyst. Sizing the thruster and particularly its combustion chamber is not an easy task because of the explosive properties and the lack of knowledge regarding the monopropellant reaction process. The thesis aims at simulating the flow in a combustion chamber using CNES05, a new promising green monopropellant. This monopropellant has a very low vapor pressure and is an energetic liquid. As such, its reaction above a certain temperature -which is called decompositionis not well understood and must be observed closely. For this matter, a test bench was created, and it paved the way for the development of a specific model of decomposition. Indeed, even if the CNES05 decomposition cannot be modeled with the classical theory of isolated droplets, the setup showed us the order of magnitude of the reaction kinetics and the presence of a break up phenomenon. Using this model, the simulations of the flow inside the combustion chamber give us the heat flux profile through its walls, a sizing parameter for the thruster. Large recirculation zones are observed and the influence of the angle of injection seems to be the major injection parameter of influence. The sensitivity of the parameters used in the model is also studied.</p>

corrected abstract:
<p>Concerns about the use of certain chemical species within the aerospace field are growing in recent years. A European regulation, REACh, now makes the use of hydrazine uncertain in – among others- attitude control thrusters. Green monopropellants, which are alternatives for this species already exist, but they all require a catalyst to react. Catalysts constitute the limiting factor for the lifespan of satellites because of the number of thermal cycles they endure.</p><p>A joint project between ONERA, the French aerospace research center and CNES, the French space agency, was born to develop a high-performance green monopropellant thruster operating without any catalyst. Sizing the thruster and particularly its combustion chamber is not an easy task because of the explosive properties and the lack of knowledge regarding the monopropellant reaction process.</p><p>The thesis aims at simulating the flow in a combustion chamber using CNES05, a new promising green monopropellant. This monopropellant has a very low vapor pressure and is an energetic liquid. As such, its reaction above a certain temperature - which is called decomposition - is not well understood and must be observed closely. For this matter, a test bench was created, and it paved the way for the development of a specific model of decomposition. Indeed, even if the CNES05 decomposition cannot be modeled with the classical theory of isolated droplets, the setup showed us the order of magnitude of the reaction kinetics and the presence of a break up phenomenon.</p><p>Using this model, the simulations of the flow inside the combustion chamber give us the heat flux profile through its walls, a sizing parameter for the thruster. Large recirculation zones are observed and the influence of the angle of injection seems to be the major injection parameter of influence. The sensitivity of the parameters used in the model is also studied.</p>

Note added missing paragraph breaks and adjusted space for the gash separed parentheticaal
----------------------------------------------------------------------
In diva2:1438331 - the title should have "𝑝" and not "p"
"Numerical Solvers of the p-Stokes Equations with Applications in Ice-Sheet Dynamics"
==>
"Numerical Solvers of the 𝑝-Stokes Equations with Applications in Ice-Sheet Dynamics"

abstract is: 
<p>In glacier dynamics – i.e. the field of research concerning the movement of ice – intricate mathematical models are used to describe its motion. Ice can be considered as a highly viscous, non-Newtonian fluid, obeying a set of equations closely related to the well-known Navier-Stokes equations describing classical fluid mechanics. These equations are called the p-Stokes equations and is to date the most precise mathematical description of the flow of ice. Using the finite element method, the non-linear p-Stokes equations are most efficiently solved numerically by a Newton solver in combination with preconditioned iterative solvers. This thesis investigate the use of such solvers when applied to glacier dynamics. To avoid singularities, the non-linear shear dependent viscosity that arise in the p-Stokes equations is modeled with a regularization term to avoid singularities. Inorder to facilitate fast convergence for glacier simulations we implement and discuss the use of an optimal expression of this regularization. The regularization term is tested fora simplified flow configuration, for well-known glaciological benchmark experiments and lastly, an Antarctic glacial geometry. We come to the conclusion that the regularization parameter implemented increase the efficiency of the numerical methods used, generally without the introduction of significant errors to the model.</p>

corrected abstract:
<p>In glacier dynamics – i.e. the field of research concerning the movement of ice – intricate mathematical models are used to describe its motion. Ice can be considered as a highly viscous, non-Newtonian fluid, obeying a set of equations closely related to the well-known Navier-Stokes equations describing classical fluid mechanics. These equations are called the 𝑝-Stokes equations and is to date the most precise mathematical description of the flow of ice. Using the finite element method, the non-linear 𝑝-Stokes equations are most efficiently solved numerically by a Newton solver in combination with preconditioned iterative solvers. This thesis investigate the use of such solvers when applied to glacier dynamics. To avoid singularities, the non-linear shear dependent viscosity that arise in the 𝑝-Stokes equations is modeled with a regularization term to avoid singularities. In order to facilitate fast convergence for glacier simulations, we implement and discuss the use of an optimal expression of this regularization. The regularization term is tested for a simplified flow configuration, for well-known glaciological benchmark experiments and lastly, an Antarctic glacial geometry. We come to the conclusion that the regularization parameter implemented increase the efficiency of the numerical methods used, generally without the introduction of significant errors to the model.</p>

Note added a comma and some spaces (where words had been merged), converted "p" to "𝑝"
----------------------------------------------------------------------
In diva2:618585   - correct as is
----------------------------------------------------------------------
In diva2:1335372 
abstract is: 
<p>The anomalous precession of the perihelion of Mercury troubled the brilliant scientists of the time when it was first discovered. With ideas ranging from the possibilities of a new planet to interstellar dust, it was a mystery until Einstein correctly described it using his theory of general relativity in the early 20th century. This project revisits the idea of a finite propagation speed of gravity, using time-delayed Newtonian gravity as a means to investigate if the anomalous precession can be explained in a simpler way. The delay-differential equations that arise are solved using numerical analysis, with a modified Runge-Kutta-Fehlberg integrator. When solving delay-differential equations, either constant or state dependent time delays typically must be considered. For the time-delayed gravity it is shown that the time delays must instead be implicit and history-dependent, and an efficient method of solving these are proposed. Finally, it is shown that time-delayed gravity is not sufficient to describe the anomalous precession and that the systems lose energy over time, dependent on the speed of gravity. This verifies the work done analytically in the 18th and 19th century.</p>

corrected abstract:
<p>The anomalous precession of the perihelion of Mercury troubled the brilliant scientists of the time when it was first discovered. With ideas ranging from the possibilities of a new planet to interstellar dust, it was a mystery until Einstein correctly described it using his theory of general relativity in the early 20th century.</p><p>This project revisits the idea of a finite propagation speed of gravity, using time-delayed Newtonian gravity as a means to investigate if the anomalous precession can be explained in a simpler way. The delay-differential equations that arise are solved using numerical analysis, with a modified Runge-Kutta-Fehlberg integrator.</p><p>When solving delay-differential equations, either constant or state dependent time delays typically must be considered. For the time-delayed gravity it is shown that the time delays must instead be implicit and history-dependent, and an efficient method of solving these are proposed.</p><p>Finally, it is shown that time-delayed gravity is not sufficient to describe the anomalous precession and that the systems lose energy over time, dependent on the speed of gravity. This verifies the work done analytically in the 18th and 19th century.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1348449 
abstract is: 
<p>In this report, the calculation tool ANSYS was used together with the support of experiments on prototypes to develop a new swinging concept for BabyBjörns babysitter. The challenge was to get a progressive stiffness behavior from the construction and thereby reducing the swinging frequencies for larger children while maintaining the swing rates for smaller children. The analysis took place primarily in two stages. First, the existing construction was analysed, weaknesses and strengths were identified, as well as development opportunities. After this, a new solution was developed. The concept used in the already existing construction does not succeed in obtaining a progressive stiffness behavior within reasonable modifications. Although, the proposed solution can with the help of hyperelastic materials achieve the desired stiffness.</p>

corrected abstract:
<p>In this report, the calculation tool ANSYS was used together with the support of experiments on prototypes to develop a new swinging concept for BabyBjörns babysitter.</p><p>The challenge was to get a progressive stiffness behavior from the construction and thereby reducing the swinging frequencies for larger children while maintaining the swing rates for smaller children.</p><p>The analysis took place primarily in two stages. First, the existing construction was analysed, weaknesses and strengths were identified, as well as development opportunities. After this, a new solution was developed. The concept used in the already existing construction does not succeed in obtaining a progressive stiffness behavior within reasonable modifications. Although, the proposed solution can with the help of hyperelastic materials achieve the desired stiffness.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1320122   - correct as is
----------------------------------------------------------------------
In diva2:1695219 
abstract is: 
<p>In telecom networks adjusting the tilt of antennas in an optimal manner, the so called remote electrical tilt (RET) optimization, is a method to ensure quality of service (QoS) for network users. Tilt adjustments made during operations in real-world networks are usually executed through a suboptimal policy, and a significant amount of data is collected during the execution of such policy. The policy collecting the data is known as the behavior policy and can be used to learn improved tilt update policies in an offline manner. In this thesis the RET optimization problem is formulated in a offline Reinforcement Learning (RL) setting, where the objective is to learn an optimal policy from batches of data collected by the logging policy. Offline RL is a challenging problem where traditional RL algorithms can fail to learn policies that will perform well when evaluated online.In this thesis Conservative Q-learning (CQL) is applied to tackle the challenges of offline RL, with the purpose of learning improved policies for tilt adjustment from data in a simulated environment. Experiments are made with different types of function approximators to model the Q-function. Specifically, an Artificial Neural Network (ANN) and a linear model are employed in the experiments. With linear function approximation, two novel algorithms which combine the properties of CQL and the classic Least Squares Policy Iteration (LSPI) algorithm are proposed. They are also used for learning RET adjustment policies. In online evaluation in the simulator one of the proposed algorithms with simple linear function approximation achieves similar results to CQL with the more complex artificial neural network function approximator. These versions of CQL outperform both the behavior policy and the naive Deep Q-Networks (DQN) method.</p>

corrected abstract:
<p>In telecom networks adjusting the tilt of antennas in an optimal manner, the so called remote electrical tilt (RET) optimization, is a method to ensure quality of service (QoS) for network users. Tilt adjustments made during operations in real-world networks are usually executed through a suboptimal policy, and a significant amount of data is collected during the execution of such policy. The policy collecting the data is known as the behavior policy and can be used to learn improved tilt update policies in an offline manner. In this thesis the RET optimization problem is formulated in a offline Reinforcement Learning (RL) setting, where the objective is to learn an optimal policy from batches of data collected by the logging policy. Offline RL is a challenging problem where traditional RL algorithms can fail to learn policies that will perform well when evaluated online. In this thesis Conservative Q-learning (CQL) is applied to tackle the challenges of offline RL, with the purpose of learning improved policies for tilt adjustment from data in a simulated environment. Experiments are made with different types of function approximators to model the Q-function. Specifically, an Artificial Neural Network (ANN) and a linear model are employed in the experiments. With linear function approximation, two novel algorithms which combine the properties of CQL and the classic Least Squares Policy Iteration (LSPI) algorithm are proposed. They are also used for learning RET adjustment policies. In online evaluation in the simulator one of the proposed algorithms with simple linear function approximation achieves similar results to CQL with the more complex artificial neural network function approximator. These versions of CQL outperform both the behavior policy and the naive Deep Q-Networks (DQN) method.</p>

Note inserted one space:
mc='online.In' c='online. In'
----------------------------------------------------------------------
In diva2:1686714 
abstract is: 
<p>In the present work, we introduce a never before studied soliton equation called the intermediate mixed Manakov (IMM) equation. Through a pole ansatz, we prove that the equation has N-soliton solutions with pole parameters governed by the hyperbolic Calogero-Moser system. We also show that there are spatially periodic N-soliton solutions with poles obeying elliptic Calogero-Moser dynamics. A Lax pair is given in the form of a Riemann-Hilbert problem on a cylinder. A similar Lax pair is shown to imply a novel spin generalization of the intermediate nonlinear Schrödinger equation. Some conservation laws for the IMM are proven. We demonstrate that the IMM can be written as a Hamiltonian system, with one of these conserved quantities as the Hamiltonian. Finally, a physical interpretation is given by showing that the IMM can be rewritten to describe a system of two nonlocally coupled fluids, with nonlinear self-interactions.</p>

corrected abstract:
<p>In the present work, we introduce a never before studied soliton equation called the <em>intermediate mixed Manakov</em> (IMM) equation. Through a pole ansatz, we prove that the equation has 𝑁-soliton solutions with pole parameters governed by the hyperbolic Calogero-Moser system. We also show that there are spatially periodic 𝑁-soliton solutions with poles obeying elliptic Calogero-Moser dynamics. A Lax pair is given in the form of a Riemann-Hilbert problem on a cylinder. A similar Lax pair is shown to imply a novel spin generalization of the intermediate nonlinear Schrödinger equation. Some conservation laws for the IMM are proven. We demonstrate that the IMM can be written as a Hamiltonian system, with one of these conserved quantities as the Hamiltonian. Finally, a physical interpretation is given by showing that the IMM can be rewritten to describe a system of two nonlocally coupled fluids, with nonlinear self-interactions.</p>

Note added italics and replaced "N" with "𝑁"
----------------------------------------------------------------------
In diva2:894183 - note tha in the subtitle the dashes are just to indicate this is a subtitle:
"On Aircraft Systems’ Acquisition Cost Estimation: – A Parametric Approach –"
==>
"On Aircraft Systems’ Acquisition Cost Estimation: A Parametric Approach"

abstract is: 
<p><strong>The aim of this master thesis is to increase the precision of the aircraft systems’ acquisition cost estimations required as input to the Saab in-house software employed in concept studies at the company. This is achieved by developing a systematic methodology based on parametric techniques for data collection, normalization and validation, resulting in a mathematical Cost Estimating Relationship (CER) between a technical parameter and the acquisition cost. The accuracy of the CER is evaluated through residual and confidence band-analysis. </strong></p><p>This paper is a summarized, de-classified version of the company confidential 82-page project report written at Saab. Confidential information such as numeric values, data sources and references are omitted and replaced by generic substitutes where necessary.</p>

corrected abstract:
<p>The aim of this master thesis is to increase the precision of the aircraft systems’ acquisition cost estimations required as input to the Saab in-house software employed in concept studies at the company. This is achieved by developing a systematic methodology based on parametric techniques for data collection, normalization and validation, resulting in a mathematical Cost Estimating Relationship (CER) between a technical parameter and the acquisition cost. The accuracy of the CER is evaluated through residual and confidence band-analysis.</p><p>This paper is a summarized, de-classified version of the company confidential 82-page project report written at Saab. Confidential information such as numeric values, data sources and references are omitted and replaced by generic substitutes where necessary.</p>

Note eliminated an unnecessary space at the end of a paragraph, removed unnecessary >strong>
----------------------------------------------------------------------
In diva2:794012   - correct as is
----------------------------------------------------------------------
In diva2:1070418   - correct as is
----------------------------------------------------------------------
In diva2:1821333   - correct as is
----------------------------------------------------------------------
In diva2:1780147 
abstract is: 
<p>An overview of some concepts regarding Lie groups, Lie algebra, culminating in the construction of an exceptional Lie group</p>

corrected abstract:
<p>In this thesis we are going to explore the concepts surrounding Lie groups and Lie algebra, and in the conclusion the construction of what is called exceptional Lie groups.</p>

Note original abstract is different, corrected to match original
----------------------------------------------------------------------
In diva2:1780187 
abstract is: 
<p>In 1952 Alan Turing published his paper "The Chemical Basis of Morphogenesis", which described a model for how naturally occurring patterns, such as the stripes of a zebra and the spots of a leopard, can arise from a spatially homogeneous steady state through diffusion. Turing suggested that the concentration of the substances producing the patterns is determined by the reaction kinetics, how the substances interact, and diffusion. </p><p>In this project Turing's model with linear reactions kinetics was studied. The model was first solved using two different numerical methods; the finite difference method (FDM) and the finite element method (FEM) with different boundary conditions. A parameter study was then conducted, investigating the effect on the patterns of changing the parameters of the model. Lastly the controllability of the model and the least energy control was considered.</p><p>The simulations were found to produce patterns provided the right parameters, as expected. From the investigation of the parameters it could be concluded that the size/tightness of the pattern and similarity of the substance concentration distributions depended on the choice of parameters. As for the controllability, a desired final state could be produced thorough simulations using control of the boundary and the energy cost of producing the pattern increased when decreasing the number of controls.</p>

corrected abstract:
<p>In 1952 Alan Turing published his paper ”The Chemical Basis of Morphogenesis”, which described a model for how naturally occurring patterns, such as the stripes of a zebra and the spots of a leopard, can arise from a spatially homogeneous steady state through diffusion. Turing suggested that the concentration of the substances producing the patterns is determined by the reaction kinetics, how the substances interact, and diffusion.</p><p>In this project Turing's model with linear reactions kinetics was studied. The model was first solved using two different numerical methods; the finite difference method (FDM) and the finite element method (FEM) with different boundary conditions. A parameter study was then conducted, investigating the effect on the patterns of changing the parameters of the model. Lastly the controllability of the model and the least energy control was considered.</p><p>The simulations were found to produce patterns provided the right parameters, as expected. From the investigation of the parameters it could be concluded that the size/tightness of the pattern and similarity of the substance concentration distributions depended on the choice of parameters. As for the controllability, a desired final state could be produced thorough simulations using control of the boundary and the energy cost of producing the pattern increased when decreasing the number of controls.</p>

Note fixed double quotes to match the original and eliminated an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
