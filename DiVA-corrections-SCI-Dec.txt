This file contains corrected abstracts from DiVA for SCI added on 2024-12-05
----------------------------------------------------------------------
In diva2:1888871   - correct as is
----------------------------------------------------------------------
In diva2:1881342 
abstract is: 
<p>This thesis investigates the impedance of acoustic liners, to attenuate noise originating from jet engines and enable compliance with international standards and regulations regarding noise from airplane jet engines. Experimental tests of two supplied liners were conducted in an impedance tube; one liner with known and predictable properties, and one liner with unknown properties.</p><p>The tests included tonal excitations in the formats of stepped sine and random noise with frequencies within set boundaries. After post-processing of the captured data, the desired impedance could be analysed in terms of excitated frequencies and sound pressure levels.</p><p>The conclusions from this project are that both of the liners deviated from their expected behavior, which was that liner 1 should have been unaffected by the alternated sound pressure levels, and liner 3 should have shown bigger affection due to the changed sound pressure level. Since the results were different than expected, there might have been minor sources of error during the measurements. It could be investigated if there is leakage from the mounting of the liners, or if the 3D printing resolution is sufficient.</p><p>Because of limitations in time, there is more left in this project to investigate. Therefore, conducting similar studies where more frequencies, sound pressure levels, and multi-tonal measurements can be included, is suggested as future work.</p><p> </p>

corrected abstract:
<p>This thesis investigates the impedance of acoustic liners, to attenuate noise originating from jet engines and enable compliance with international standards and regulations regarding noise from airplane jet engines. Experimental tests of two supplied liners were conducted in an impedance tube; one liner with known and predictable properties, and one liner with unknown properties.</p><p>The tests included tonal excitations in the formats of stepped sine and random noise with frequencies within set boundaries. After post-processing of the captured data, the desired impedance could be analysed in terms of excitated frequencies and sound pressure levels.</p><p>The conclusions from this project are that both of the liners deviated from their expected behavior, which was that liner 1 should have been unaffected by the alternated sound pressure levels, and liner 3 should have shown bigger affection due to the changed sound pressure level. Since the results were different than expected, there might have been minor sources of error during the measurements. It could be investigated if there is leakage from the mounting of the liners, or if the 3D printing resolution is sufficient.</p><p>Because of limitations in time, there is more left in this project to investigate. Therefore, conducting similar studies where more frequencies, sound pressure levels, and multi-tonal measurements can be included, is suggested as future work.</p>

Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1881022 
abstract is: 
<p>This study compares transportation preferences among students and staff from two different universities: KTH in Stockholm and ITBA in Buenos Aires. The aim was to identify the factors influencing transportation decisions and their impact on perceptions regarding its usage. A survey was conducted at both universities to gain insights from participants and analyze the collected information, aiming to propose solutions that enhance the quality and sustainability of transportation. This work presents a novel analysis by comparing two cities with seemingly different characteristics in terms of transportation and urban development, with an emphasis on sustainability. The results obtained are positive as potential improvements in both systems could be observed. Moreover, the results have generated ideas to improve the adoption of green and sustainable transportation, without compromising the quality of existing transportation for members of these educational institutions. The importance of environmental awareness and its positive impact on quality of life and urban mobility is highlighted.</p><p> </p>

corrected abstract:
<p>This study compares transportation preferences among students and staff from two different universities: KTH in Stockholm and ITBA in Buenos Aires. The aim was to identify the factors influencing transportation decisions and their impact on perceptions regarding its usage. A survey was conducted at both universities to gain insights from participants and analyze the collected information, aiming to propose solutions that enhance the quality and sustainability of transportation. This work presents a novel analysis by comparing two cities with seemingly different characteristics in terms of transportation and urban development, with an emphasis on sustainability. The results obtained are positive as potential improvements in both systems could be observed. Moreover, the results have generated ideas to improve the adoption of green and sustainable transportation, without compromising the quality of existing transportation for members of these educational institutions. The importance of environmental awareness and its positive impact on quality of life and urban mobility is highlighted.</p>

Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1880964 
abstract is: 
<p>Urban Air mobility (UAM) promises reduced congestion on roads, reduced travel times and stronger overall efficiency in densely populated areas. However several challenges arise when wanting to implement UAM such as safety and social acceptance. The aim of this paper is to gain valuable insights how to implement safe and socially accepted UAM into society. Current regulations are discussed as well as X, Y and Z volumes in U-space, flight separations with ellipsoidal safety buffers, high speed corridors, landing separation at vertiports and airspace partition with voronoi diagrams are proposed and discussed. Social acceptance is addressed with some of the most prominent concerns e.g. safety, privacy and noise. Examples are set in Stockholm, Sweden, resulting in a maximum airspace occupation of 1 % which means 210 UAS (Unmanned Aircraft Systems) on each flight level. Sensitive areas and people with privacy concerns should have the option to opt-out of having their properties under the flight paths of UAM-vehicles. Concerns with UAM from the public has to be taken into great consideration for a successful implementation of UAM.</p>

corrected abstract:
<p>Urban Air mobility (UAM) promises reduced congestion on roads, reduced travel times and stronger overall efficiency in densely populated areas. However several challenges arise when wanting to implement UAM such as safety and social acceptance. The aim of this paper is to gain valuable insights how to implement safe and socially accepted UAM into society. Current regulations are discussed as well as X, Y and Z volumes in U-space, flight separations with ellipsoidal safety buffers, high speed corridors, landing separation at vertiports and airspace partition with voronoi diagrams are proposed and discussed. Social acceptance is addressed with some of the most prominent concerns e.g. safety, privacy and noise. Examples are set in Stockholm, Sweden resulting in a maximum airspace occupation of 1 % which means 210 UAS (Unmanned Aircraft Systems) on each flight level. Sensitive areas and people with privacy concerns should have the option to opt-out of having their properties under the flight paths of UAM-vehicles. Concerns with UAM from the public has to be taken into great consideration for a successful implementation of UAM.</p>

Note - only change was to remove the comma after "Sweden" as it is not in the original, although grammatically it might be correct. It might also be noted that commas should before and after the "e.g." - but are not in the original.
----------------------------------------------------------------------
In diva2:1880436   - correct as is
----------------------------------------------------------------------
In diva2:1880384   - correct as is
----------------------------------------------------------------------
In diva2:1880323 
abstract is: 
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p><p> </p>

corrected abstract:
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p>


Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:877595 
abstract is: 
<p> </p><p>A recent trend in the world is that more and more countries and therefore their mil-itaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to ad-opt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense com-pany, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an e˙ective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare di˙erent solutions. The delta model includes reference values for relevant costs that can be a˙ected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>
w='di˙erent' val={'c': 'different', 's': ['diva2:877595', 'diva2:1380198', 'diva2:891912', 'diva2:919302', 'diva2:1328904', 'diva2:891537', 'diva2:1440147', 'diva2:1087823'], 'n': 'missing ligature'}
w='ad-opt' val={'c': 'adopt', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='com-pany' val={'c': 'company', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='mil-itaries' val={'c': 'militaries', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='a˙ected' val={'c': 'affected', 's': 'diva2:877595', 'n': 'missing ligature'}
w='e˙ective' val={'c': 'effective', 's': 'diva2:877595', 'n': 'error due to ligature'}

corrected abstract:
<p>A recent trend in the world is that more and more countries and therefore their militaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to adopt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense company, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an effective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare different solutions. The delta model includes reference values for relevant costs that can be affected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>

Note - also remove the empty paragraph at the start.
----------------------------------------------------------------------
In diva2:711193 
abstract is: 
<p>Consensus problem with multi-agent systems has interested researchers in various areas. Its difficulties tend to appear when available information of each agent is limited for achieving consensus. Besides, it is not always the case that agents can catch the whole states of the others; an output is often the only possible measurement for each agent in applications. The idea of graph Laplacian is then of help to address such a troublesome situation. While every single agent obviously makes decision to achieve an individual goal of minimizing its own cost functional, all agents as a team can obtain even more improvement by cooperation in some cases, which leads to cooperative game theoretic approach. The main goal of this master thesis is to accomplish a combination of optimal control theory and cooperative game theory in order to solve the output consensus problem with limited network connectivity. Along with this combination, bargaining problems are considered out of necessity.</p><p> </p>

corrected abstract:
<p>Consensus problem with multi-agent systems has interested researchers in various areas. Its difficulties tend to appear when available information of each agent is limited for achieving consensus. Besides, it is not always the case that agents can catch the whole states of the others; an output is often the only possible measurement for each agent in applications. The idea of graph Laplacian is then of help to address such a troublesome situation.</p><p>While every single agent obviously makes decision to achieve an individual goal of minimizing its own cost functional, all agents as a team can obtain even more improvement by cooperation in some cases, which leads to cooperative game theoretic approach. The main goal of this master thesis is to accomplish a combination of optimal control theory and cooperative game theory in order to solve the output consensus problem with limited network connectivity. Along with this combination, bargaining problems are considered out of necessity.</p>

Note added missing paragraph break and removed empty paragraph
----------------------------------------------------------------------
In diva2:1211524 
abstract is: 
<p>This thesis in applied statistics and industrial economics examines the correlation between a number of market conditions on the Swedish and Global market and the yield difference between the Swedish stock market and the Global stock market. The report is based on data from the index MSCI Sweden Net Return, MSCI World Net Return and the Volatility index S&amp;P 500. The market conditions that have been examined are Bull markets, Bear markets, periods of high volatility. We also examined how the appreciation of the SEK in comparison to the USD and the yield of the Swedish stock market correlated with the yield difference between the Swedish Stock Market and the Global stock market. The correlation was examined using multiple linear regression. The results indicated a positive correlation between the yield difference between the Swedish stock market and the Global stock market and the yield of the Swedish stock market, the appreciation of the SEK compared to the USD and Bull markets. We found a negative correlation with Bear markets and no correlation at all with the volatility.</p><p> </p><p>The results are in line with what could be expected and give a stronger statistical ground for the idea that the Swedish stock market has larger fluctuations than the Global stock market during large-scale market fluctuations.</p>

corrected abstract:
<p>This thesis in applied statistics and industrial economics examines the correlation between a number of market conditions on the Swedish and Global market and the yield difference between the Swedish stock market and the Global stock market. The report is based on data from the index MSCI Sweden Net Return, MSCI World Net Return and the Volatility index S&amp;P 500. The market conditions that have been examined are Bull markets, Bear markets, periods of high volatility. We also examined how the appreciation of the SEK in comparison to the USD and the yield of the Swedish stock market correlated with the yield difference between the Swedish Stock Market and the Global stock market. The correlation was examined using multiple linear regression. The results indicated a positive correlation between the yield difference between the Swedish stock market and the Global stock market and the yield of the Swedish stock market, the appreciation of the SEK compared to the USD and Bull markets. We found a negative correlation with Bear markets and no correlation at all with the volatility.</p><p>The results are in line with what could be expected and give a stronger statistical ground for the idea that the Swedish stock market has larger fluctuations than the Global stock market during large-scale market fluctuations.</p>

Note - only change - removed empty paragraph in the middle
----------------------------------------------------------------------
In diva2:1674019 
abstract is: 
<p>Low Reynolds number airfoil analysis has become increasingly significant as urban air mobility vehicles and unmanned aerial vehicles surge in popularity. The Green Raven project at KTH Aero aims to use reflex airfoils where little data is available beyond classical analysis. Viscous formulations of the panel method and computational fluid dynamics (CFD) have been used to simulate lift, drag and moments for the MH61 and MH104 airfoils at different angles of attack (AOAs). XFOIL and CFD turbulence models such as Spalart-Allmaras (SA), k-w Shear Stress Transport (SST) with and without damping coefficients were used. The strengths and limitations of each model were used to justify results. Due to clear computational advantages, XFOIL produced adequate results and is tailored toward use in initial design stages where repeated measurements are crucial. The SA turbulence stood out as the model produced accurate results in a reasonable time. The abundance of published CFD material comparing different turbulence models increased the credibility of the results. The two airfoils had similar lift and drag characteristics at AOAs of 0-6 deg while the MH104 was superior near stall. However, due to the lack of experimental data of the airfoils no particular model could be commended or verified.</p><p> </p>

corrected abstract:
<p>Low Reynolds number airfoil analysis has become increasingly significant as urban air mobility vehicles and unmanned aerial vehicles surge in popularity. The Green Raven project at KTH Aero aims to use reflex airfoils where little data is available beyond classical analysis. Viscous formulations of the panel method and computational fluid dynamics (CFD) have been used to simulate lift, drag and moments for the MH61 and MH104 airfoils at different angles of attack (AOAs). XFOIL and CFD turbulence models such as Spalart-Allmaras (SA), 𝑘-ω Shear Stress Transport (SST) with and without damping coefficients were used. The strengths and limitations of each model were used to justify results. Due to clear computational advantages, XFOIL produced adequate results and is tailored toward use in initial design stages where repeated measurements are crucial. The SA turbulence stood out as the model produced accurate results in a reasonable time. The abundance of published CFD material comparing different turbulence models increased the credibility of the results. The two airfoils had similar lift and drag characteristics at AOAs of 0-6º while the MH104 was superior near stall. However, due to the lack of experimental data of the airfoils no particular model could be commended or verified.</p>
----------------------------------------------------------------------
In diva2:1778751 
abstract is: 
<p>Formula Student is a global engineering competition where university students collaborate to design, construct, and race formula-style cars. Aerodynamics is one aspect in the vehicle design that can improve on-track performance by increasing cornering and straight-line speed.</p><p>To improve the aerodynamics of KTH Formula Student's DeV18 vehicle, the side structure is being redesigned. The current model, DeV17, features an underperforming tunnel-based side structure. To address this issue, this had the goal to investigate a new multi-element wing design that utilizes ground effect.</p><p>The design study of the DeV18 vehicle is conducted using Siemens NX 2212 for 3D modelling and Simcenter Star-CCM+ 17.06.008-R8 for airflow simulations. To quickly investigate certain design parameters effect on the results, Design Manager Project inside Simcenter Star-CCM+ is used.</p><p>The resulting side structure produces a total of 26 N of downforce and 6 N of drag at 40 kph, more than twice that of DeV17’s side structure while also producing less drag. Although this significant improvement compared to DeV17, it is believed that further increases in performance are necessary to compete with top teams. By using a more sophisticated method to optimize the multi-element wing, such as adjoint optimization, the concept could be improved. However, the overall potential of the concept is still considered too limited to achieve the desired performance goals, which is why it will no longer be investigated further.</p><p> </p>

corrected abstract:
<p>Formula Student is a global engineering competition where university students collaborate to design, construct, and race formula-style cars. Aerodynamics is one aspect in the vehicle design that can improve on-track performance by increasing cornering and straight-line speed.</p><p>To improve the aerodynamics of KTH Formula Student's DeV18 vehicle, the side structure is being redesigned. The current model, DeV17, features an underperforming tunnel-based side structure. To address this issue, this had the goal to investigate a new multi-element wing design that utilizes ground effect.</p><p>The design study of the DeV18 vehicle is conducted using Siemens NX 2212 for 3D modelling and Simcenter Star-CCM+ 17.06.008-R8 for airflow simulations. To quickly investigate certain design parameters effect on the results, Design Manager Project inside Simcenter Star-CCM+ is used.</p><p>The resulting side structure produces a total of 26 N of downforce and 6 N of drag at 40 kph, more than twice that of DeV17’s side structure while also producing less drag. Although this significant improvement compared to DeV17, it is believed that further increases in performance are necessary to compete with top teams. By using a more sophisticated method to optimize the multi-element wing, such as adjoint optimization, the concept could be improved. However, the overall potential of the concept is still considered too limited to achieve the desired performance goals, which is why it will no longer be investigated further.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1776777 
abstract is: 
<p>In this Bachelor thesis, a novel algorithm for sampling from bandlimited circular probability distributions is presented. The algorithm leverages results from Fourier analysis concerning the Fejér kernel to simulate data with some desired probability distribution, realized as a sum of data sampled from a discrete distribution and a small continuous perturbation sampled from the Fejér kernel distribution. Relevant theory is presented before formally proving exact simulation using the algorithm. Experimental results confirm the validity of the theoretical results, and the efficiency of the algorithm is then compared with that of other sampling methods such as rejection sampling with a uniform envelope function.</p><p> </p>

corrected abstract:
<p>In this Bachelor thesis, a novel algorithm for sampling from bandlimited circular probability distributions is presented. The algorithm leverages results from Fourier analysis concerning the Fejér kernel to simulate data with some desired probability distribution, realized as a sum of data sampled from a discrete distribution and a small continuous perturbation sampled from the Fejér kernel distribution. Relevant theory is presented before formally proving exact simulation using the algorithm. Experimental results confirm the validity of the theoretical results, and the efficiency of the algorithm is then compared with that of other sampling methods such as rejection sampling with a uniform envelope function.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:525720 
abstract is: 
<p>Abstract</p><p> </p><p>In this report analysis of foreign exchange rates time series are performed. First, triangular arbitrage is detected and eliminated from data series using linear algebra tools. Then Vector Autoregressive processes are calibrated and used to replicate dynamics of exchange rates as well as to forecast time series. Finally, optimal portfolio of currencies with minimal Expected Shortfall is formed using one time period ahead forecasts</p>

corrected abstract:
<p>In this report analysis of foreign exchange rates time series are performed. First, triangular arbitrage is detected and eliminated from data series using linear algebra tools. Then Vector Autoregressive processes are calibrated and used to replicate dynamics of exchange rates as well as to forecast time series. Finally, optimal portfolio of currencies with minimal Expected Shortfall is formed using one time period ahead forecasts.</p>

Note - removed unnecessary "<p>Abstract</p><p> </p>" and added final missing period.
----------------------------------------------------------------------
In diva2:1566509 
abstract is: 
<p>This report is about a novel approach to attenuation of fan noise in aerial vehicles, by way of implementing a ducted fan in the chassis of a four meter blended wing body plane. Three different one meter PVC pipes were used and their performances as silencers were tested by measuring the sound power level and calculating the insulation loss compared to a fan by itself. The ducts were either empty or lined with acoustic absorbents and micro perforated panels. Experiments were carried out in the reverberation room at KTH Marcus Wallenberg laboratory for sound and vibration research using the guidelines in ISO 3741 (2010). The results showed that the empty duct lead to a 15.3 dB(A) insulation loss with no decrease in thrust from the fan. The absorbent and micro perforated panel, however, lead to a 22.7 dB(A) insulation loss while giving a major decrease in thrust of more than one order of magnitude. The results show the failure of implementation of the latter two silencers due to choking, but also the success of the empty duct. This shows that there is room for improvement and perhaps even a future possibility of a successful implementation in a real vehicle.</p><p> </p>

corrected abstract:
<p>This report is about a novel approach to attenuation of fan noise in aerial vehicles, by way of implementing a ducted fan in the chassis of a four meter blended wing body plane. Three different one meter PVC pipes were used and their performances as silencers were tested by measuring the sound power level and calculating the insulation loss compared to a fan by itself. The ducts were either empty or lined with acoustic absorbents and micro perforated panels. Experiments were carried out in the reverberation room at KTH Marcus Wallenberg laboratory for sound and vibration research using the guidelines in ISO 3741 (2010). The results showed that the empty duct lead to a 15.3 dB(A) insulation loss with no decrease in thrust from the fan. The absorbent and micro perforated panel, however, lead to a 22.7 dB(A) insulation loss while giving a major decrease in thrust of more than one order of magnitude. The results show the failure of implementation of the latter two silencers due to choking, but also the success of the empty duct. This shows that there is room for improvement and perhaps even a future possibility of a successful implementation in a real vehicle.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1673538 
abstract is: 
<p>The purpose of the study is to evaluate the possibility of using gridded ion thrusters as a means of attitude control for a solar sail as a part of the sunshade project, which aims to place 10^8 solar sail sunshade spacecraft, each with an area of about 10 000 m^2, at the Sun-Earth Lagrangian point L1 in order to reduce Earth's global temperature. Two types of solar sail sunshade spacecraft were studied. The first type, referred to as the sunshade demonstrator, had an area of 100 m^2 and a mass of 10 kg, and the second type, referred to as the full-sized sunshade, had an area of 10 000 m^2 and a mass of 90 kg. To determine the significance of using ion thrusters for the attitude control system, the mass of the required fuel, as well as the total mass that had to be added to the spacecraft to implement the attitude control system, was calculated. Two types of journeys were studied for each spacecraft type: starting from Low Earth Orbit (LEO) to L1 and from Geostationary Orbit (GEO) to L1, respectively. The results showed that the duration of the journey of the full-sized spacecraft was about 570 days from LEO to L1 and 370 from GEO to L1, respectively. The required amounts of fuel for the respective journeys were 580 g and 15 g, respectively, and resulted in a total additional mass of 7.8 kg and 7.2 kg, respectively.</p><p> </p>

corrected abstract:
<p>The purpose of the study is to evaluate the possibility of using gridded ion thrusters as a means of attitude control for a solar sail as a part of the sunshade project, which aims to place 10<sup>8</sup> solar sail sunshade spacecraft, each with an area of about 10 000 m<sup>2</sup>, at the Sun-Earth Lagrangian point L<sub>1</sub> in order to reduce Earth's global temperature. Two types of solar sail sunshade spacecraft were studied. The first type, referred to as the sunshade demonstrator, had an area of 100 m<sup>2</sup> and a mass of 10 kg, and the second type, referred to as the full-sized sunshade, had an area of 10 000 m<sup>2</sup> and a mass of 90 kg. To determine the significance of using ion thrusters for the attitude control system, the mass of the required fuel, as well as the total mass that had to be added to the spacecraft to implement the attitude control system, was calculated. Two types of journeys were studied for each spacecraft type: starting from Low Earth Orbit (LEO) to L<sub>1</sub> and from Geostationary Orbit (GEO) to L<sub>1</sub>, respectively. The results showed that the duration of the journey of the full-sized spacecraft was about 570 days from LEO to L<sub>1</sub> and 370 from GEO to L<sub>1</sub>, respectively. The required amounts of fuel for the respective journeys were 580 g and 15 g, respectively, and resulted in a total additional mass of 7.8 kg and 7.2 kg, respectively.</p>
----------------------------------------------------------------------
In diva2:1780538 
abstract is: 
<p>The displacement of the flow by a passing freight train can often result in dangerous conditions for railway equipment and people standing in the vicinity of the train. In this work, Computational Fluid Dynamics (CFD) simulations are performed to study the flow development around a moving freight train comprised of a Class 66 locomotive and four container wagons. The results will give a better insight into the effects that each flow structure can have in the flow within the train's slipstream. Both two- and three-dimensional simulations are carried out around the freight train using three different RANS turbulence models: the Spalart-Allmaras, the SST k-ω and the W&amp;J EARSM. Two cases of 10o and 30o crosswinds are also considered and compared to the no-crosswind case, as side-winds characterize the majority of real-life situations and are known to amplify the slipstream effects. The results are validated against available experimental and numerical data and they are thoroughly presented and discussed. The 30o crosswind case is also computed using a DDES simulation. A meshing strategy which involves the assembly of different mesh blocks with a non-matching interface boundary condition to create the complete domain is used and assessed, as an alternative meshing approach that can simplify and accelerate the set-up of different case-studies. Additionally, the two-dimensional study is used to assess the influence of different parameters on the solution, such as the grid resolution and the moving-ground boundary condition.</p><p> </p>

corrected abstract:
<p>The displacement of the flow by a passing freight train can often result in dangerous conditions for railway equipment and people standing in the vicinity of the train. In this work, Computational Fluid Dynamics (CFD) simulations are performed to study the flow development around a moving freight train comprised of a Class 66 locomotive and four container wagons. The results will give a better insight into the effects that each flow structure can have in the flow within the train's slipstream. Both two- and three-dimensional simulations are carried out around the freight train using three different RANS turbulence models: the Spalart-Allmaras, the SST 𝑘-ω and the W&amp;J EARSM. Two cases of 10º and 30º crosswinds are also considered and compared to the no-crosswind case, as side-winds characterize the majority of real-life situations and are known to amplify the slipstream effects. The results are validated against available experimental and numerical data and they are thoroughly presented and discussed. The 30º crosswind case is also computed using a DDES simulation. A meshing strategy which involves the assembly of different mesh blocks with a non-matching interface boundary condition to create the complete domain is used and assessed, as an alternative meshing approach that can simplify and accelerate the set-up of different case-studies. Additionally, the two-dimensional study is used to assess the influence of different parameters on the solution, such as the grid resolution and the moving-ground boundary condition.</p>
----------------------------------------------------------------------
In diva2:1780172 
abstract is: 
<p>In this report, we analyze general relativistic effects on celestial bodies, including gravitational strength in different metrics, gravitational radiation, and frame-dragging. We present simulation methods for classical and general relativistic motion, through the use of systems of equations that may be numerically integrated. The amount of energy leaving the system as gravitational radiation is approximated using the quadrupole formula, and by using a binary pair of planetary bodies as an approximation for orbital motion. Here we demonstrate that classical approximations may be suitable in low-mass high-distance scenarios. The eccentricity of an orbit also affects the gravitational radiation and would have to be much less than one for reliable results. It is concluded that frame-dragging effects are negligible for slowly rotating objects only, which is a well-known result.</p><p> </p>

corrected abstract:
<p>In this report, we analyze general relativistic effects on celestial bodies, including gravitational strength in different metrics, gravitational radiation, and frame-dragging. We present simulation methods for classical and general relativistic motion, through the use of systems of equations that may be numerically integrated. The amount of energy leaving the system as gravitational radiation is approximated using the quadrupole formula, and by using a binary pair of planetary bodies as an approximation for orbital motion. Here we demonstrate that classical approximations may be suitable in low-mass high-distance scenarios. The eccentricity of an orbit also affects the gravitational radiation and would have to be much less than one for reliable results. It is concluded that frame-dragging effects are negligible for slowly rotating objects only, which is a well-known result.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1218168 - missing space in title:
"Classification of social gestures: Recognizing waving using supervised machinelearning"
==>
"Classification of social gestures: Recognizing waving using supervised machine learning"


abstract is: 
<p>This paper presents an approach to gesture recognition including the use of a tool in order to extract certain key-points of the human body in each frame, and then processing this data and extracting features from this. The gestures recognized were two-handed waving and clapping. The features used were the maximum co-variance from a sine-fit to time-series of arm angles, as well as the max and min of this fitted sinus function. A support vector machine was used for the learning. The result was a promising accuracy of 93% <em>± </em>4% using 5-fold cross-validation. The limitations of the methods used are then discussed, which includes lack of support for more than one gesture in the data as well as some lack of generality in means of the features used. Finally some suggestions are made as to what improvements and further explorations could be made.</p><p> </p>

corrected abstract:
<p>This paper presents an approach to gesture recognition including the use of a tool in order to extract certain key-points of the human body in each frame, and then processing this data and extracting features from this. The gestures recognized were two-handed waving and clapping. The features used were the maximum co-variance from a sine-fit to time-series of arm angles, as well as the max and min of this fitted sinus function. A support vector machine was used for the learning. The result was a promising accuracy of 93%±4% using 5-fold cross-validation. The limitations of the methods used are then discussed, which includes lack of support for more than one gesture in the data as well as some lack of generality in means of the features used. Finally some suggestions are made as to what improvements and further explorations could be made.</p>

Note - removed the empty paragraph and removed the italization of "±" to match the original
----------------------------------------------------------------------
In diva2:802173 
abstract is: 
<p>A bank borrowing some money has to give some securities to the lender, which is called collateral. Different kinds of collateral can be posted, like cash in different currencies or a stock portfolio depending on the terms of the contract, which is called a Credit Support Annex (CSA). Those contracts specify eligible collateral, interest rate, frequency of collateral posting, minimum transfer amounts, etc. This guarantee reduces the counterparty risk associated with this type of transaction.</p><p>If a CSA allows for posting cash in different currencies as collateral, then the party posting collateral can, now and at each future point in time, choose which currency to post. This choice leads to optionality that needs to be accounted for when valuing even the most basic of derivatives such as forwards or swaps.</p><p>In this thesis, we deal with the valuation of embedded optionality in collateral contracts. We consider the case when collateral can be posted in two different currencies, which seems sufficient since collateral contracts are soon going to be simplified.</p><p>This study is based on the conditional independence approach proposed by Piterbarg [8]. This method is compared to both Monte-Carlo simulation and finite- difference method.</p><p>A practical application is finally presented with the example of a contract between Natixis and Barclays.</p><p> </p>

corrected abstract:
<p>A bank borrowing some money has to give some securities to the lender, which is called collateral. Different kinds of collateral can be posted, like cash in different currencies or a stock portfolio depending on the terms of the contract, which is called a Credit Support Annex (CSA). Those contracts specify eligible collateral, interest rate, frequency of collateral posting, minimum transfer amounts, etc. This guarantee reduces the counterparty risk associated with this type of transaction.</p><p>If a CSA allows for posting cash in different currencies as collateral, then the party posting collateral can, now and at each future point in time, choose which currency to post. This choice leads to optionality that needs to be accounted for when valuing even the most basic of derivatives such as forwards or swaps.</p><p>In this thesis, we deal with the valuation of embedded optionality in collateral contracts. We consider the case when collateral can be posted in two different currencies, which seems sufficient since collateral contracts are soon going to be simplified.</p><p>This study is based on the conditional independence approach proposed by Piterbarg [8]. This method is compared to both Monte-Carlo simulation and finite-difference method.</p><p>A practical application is finally presented with the example of a contract between Natixis and Barclays.</p>
----------------------------------------------------------------------
In diva2:1440982 
abstract is: 
<p>This report presents the design of the wing structure for a UAV called Skywalker X8. A model of the UAV was given and analyzed to design a wing box structure that is twice the size of the current model, with "greener" technology and lightweight materials. The loads that act upon the UAV were simulated and thereafter analyzed with the help of the CFD program called Star CCM+. Modifications on the CAD model and the FEM simulations were performed in Siemens NX. Eight different combinations were tested from the following five materials: CFRP (carbon fiber reinforced polymer), LDPE (low density polyethylene), polyethylene, polypropylene, and balsa wood. The results that best fit the requirements given was the combination of polypropylene as the wing skin and balsa as the honeycomb structure. This design weighed 3.576 kg and had the following stresses: 0.671 MPa, 0.340 MPa, 1 MPa, and 4 MPa for the angle of attacks at 1,2,3, and 6 degrees respectively. A modification of the trailing edge, which was the implementation of a Gurney flap, was made to see if it improved the lift-to-drag ratio, but unfortunately it did not so it was not developed further.</p><p> </p>


corrected abstract:
<p>This report presents the design of the wing structure for a UAV called Skywalker X8. A model of the UAV was given and analyzed to design a wing box structure that is twice the size of the current model, with “greener” technology and lightweight materials. The loads that act upon the UAV were simulated and thereafter analyzed with the help of the CFD program called Star CCM+. Modifications on the CAD model and the FEM simulations were performed in Siemens NX. Eight different combinations were tested from the following five materials: CFRP (carbon fiber reinforced polymer), LDPE (low density polyethylene), polyethylene, polypropylene, and balsa wood. The results that best fit the requirements given was the combination of polypropylene as the wing skin and balsa as the honeycomb structure. This design weighed 3.576 kg and had the following stresses: 0.671 MPa, 0.340 MPa, 1 MPa, and 4 MPa for the angle of attacks at 1,2,3, and 6 degrees respectively. A modification of the trailing edge, which was the implementation of a Gurney flap, was made to see if it improved the lift-to-drag ratio, but unfortunately it did not so it was not developed further.</p>

Note error in original
mc='1,2,3' c='1, 2, 3'
Also removed unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:1220102 
abstract is: 
<p>Chatbots, also called conversational agents, with speech interfaces are being used to a greater and greater extent, but there are still many areas that are not completely explored. The idea of this project was born out of the belief that there is a need for an assistant in the kitchen that is able to search for recipes, answer questions regarding them and guide and assist the user throughout the cooking process, all through conversation since the hands are busy. This paper begins with an introduction in the subject of conversational agents and the related technology, then similar, already existing studies and methods are presented with their pros and cons. After follows an in-depth explanation on how the program was constructed into a working kitchen assistant. Lastly, the users’ experiences of the performance and usability of the program was evaluated through tests and discussed. It turns out that conversational agents definitely can be integrated in the kitchen, and according to several sources, in a few years they will be implemented in all possible areas and change the technology of our time.</p><p> </p>

corrected abstract:
<p>Chatbots, also called conversational agents, with speech interfaces are being used to a greater and greater extent, but there are still many areas that are not completely explored. The idea of this project was born out of the belief that there is a need for an assistant in the kitchen that is able to search for recipes, answer questions regarding them and guide and assist the user throughout the cooking process, all through conversation since the hands are busy. This paper begins with an introduction in the subject of conversational agents and the related technology, then similar, already existing studies and methods are presented with their pros and cons. After follows an in-depth explanation on how the program was constructed into a working kitchen assistant. Lastly, the users’ experiences of the performance and usability of the program was evaluated through tests and discussed. It turns out that conversational agents definitely can be integrated in the kitchen, and according to several sources, in a few years they will be implemented in all possible areas and change the technology of our time.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1880482 
abstract is: 
<p>A time-effective coverage path can be decisive in catastrophic and war scenarios for saving countless lives where UAVs are used to scan an area looking for an objective. Given an area shaped as a polygon, a quadratic decomposition method is used to discretize the area into nodes. A model of the optimization problem constraint is created and solved using mixed-integer linear programming, taking into consideration simple dynamics and coverage path planning definitions. Simulations in different scenarios are presented, showing that the presence of no-fly zones can negatively affect the coverage time. The relationship between coverage time and the number of UAVs employed is nonlinear and converges to a constant value. The result has a direct impact on the evaluation of benefits and the cost of adding UAVs to a search mission.</p><p> </p>

corrected abstract:
<p>A time-effective coverage path can be decisive in catastrophic and war scenarios for saving countless lives where UAVs are used to scan an area looking for an objective. Given an area shaped as a convex polygon, a grid-based decomposition method is used to discretize the area into squares, represented by nodes. An optimization problem, considering simple dynamics and coverage path planning definitions, is developed using mixed-integer linear programming framework. Simulations in different scenarios are presented, showing that the presence of no-fly zones can negatively affect the coverage time. The relationship between coverage time and the number of UAVs employed is nonlinear and converges to a constant value. The result has a direct impact on the evaluation of benefits and the cost of adding UAVs to search missions.</p>

Note the change in wording (based on the original) and remove of the unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:414817 
abstract is: 
<p>The purpose with this thesis work is to simulate the deflection due to creep of Kanthal(R) APMT furnace tubes using the finite element method (FEM). Kanthal APMT is a material which shows a larger primary creep compared to other metals. Therefore the creep deformation must be described with a material model which takes both primary and secondary creep into consideration. In this thesis work a material model called time hardening has been used.</p>
<p>*C2 is stress dependent. By modifying C2 so that the results from the simulations better corresponds with test data an equation for how C2 depends on the stress could be obtained.</p>
<p>The value for C2 is then calculated for each tube dimension giving results which are close to the data from sagging tests. The results may be seen as an overestimation of the actual deflection. A sensitivity analysis showed that the model is very sensitive to changes in the material parameters. A few percent change in C2 for example will change the deflection by more than 100 percent. </p>
<p>
<p>* For equation see full text</p>
<p>
<p> </p>
<p> </p>
</p>
</p>

corrected abstract:
<p>The purpose with this thesis work is to simulate the deflection due to creep of Kanthal® APMT furnace tubes using the finite element method (FEM). Kanthal APMT is a material which shows a larger primary creep compared to other metals. Therefore the creep deformation must be described with a material model which takes both primary and secondary creep into consideration. In this thesis work a material model called time hardening has been used.</p>
<p>C2 is stress dependent<sup><a href="#fn1" id="ref1">*</a></sup>. By modifying C2 so that the results from the simulations better corresponds with test data an equation for how C2 depends on the stress could be obtained.</p>
<p>The value for C2 is then calculated for each tube dimension giving results which are close to the data from sagging tests. The results may be seen as an overestimation of the actual deflection. A sensitivity analysis showed that the model is very sensitive to changes in the material parameters. A few percent change in C2 for example will change the deflection by more than 100 percent.</p>
<div id="footnotes">
    <ul style="list-style: '*'; padding-left: 20px;">
        <li id="fn1">For equation see full text <a href="#ref1" aria-label="Back to reference">↩</a></li>
    </ul>
</div>

----------------------------------------------------------------------
In diva2:894096 
abstract is: 
<p>E-sports is growing and the price pools in e-sports tournaments are increasing, Valves video game DotA 2 is one of the bigger e-sports. As professional gamers train to increase their skill, new tools to help the training might become very important. Eye tracking can give an extra training dimension for the gamer. The aim of this master thesis is to develop a Visual Attention Index for DotA 2, that is, a number that reflects the player’s visual attention during a game. Interviews with gamers combined with data collection from gamers with eye trackers and statistical methods were used to find relevant metrics to use in the work. The results show that linear regression did not work very well on the data set, however, since there were a low number of test persons, further data collection and testing needs to be done before any statistically significant conclusions can be drawn. Support Vector Machines (SVM) was also used and turned out to be an effective way of separating better players from less good players. A new SVM method, based on linear programming, was also tested and found to be efficient and easy to apply on the given data set.</p><p> </p>

corrected abstract:
<p>E-sports is growing and the price pools in e-sports tournaments are increasing, Valves video game DotA <span style="font-size: 0.8em;">&#x1D7E4;</span> is one of the bigger e-sports. As professional gamers train to increase their skill, new tools to help the training might become very important. Eye tracking can give an extra training dimension for the gamer. The aim of this master thesis is to develop a Visual Attention Index for DotA <span style="font-size: 0.8em;">&#x1D7E4;</span>, that is, a number that reflects the player’s visual attention during a game. Interviews with gamers combined with data collection from gamers with eye trackers and statistical methods were used to find relevant metrics to use in the work. The results show that linear regression did not work very well on the data set, however, since there were a low number of test persons, further data collection and testing needs to be done before any statistically significant conclusions can be drawn. Support Vector Machines (SVM) was also used and turned out to be an effective way of separating better players from less good players. A new SVM method, based on linear programming, was also tested and found to be efficient and easy to apply on the given data set.</p>

Note removed the unnecessary empty paragraph and change "2" to "&#x1D7E4;" - a Mathematical Sans-Serif Digit Two;  <span style="font-size: 0.8em;"> has been used to reduce the relative font size to match the look of the original
----------------------------------------------------------------------
In diva2:1319924 
abstract is: 
<p>Credit scoring using machine learning has been gaining attention within the research field in recent decades and it is widely used in the financial sector today. Studies covering binary credit scoring of securitized non-performing loans are however very scarce. This paper is using random forest and artificial neural networks to predict debt recovery for such portfolios. As a performance benchmark, logistic regression is used. Due to the nature of high imbalance between the classes, the performance is evaluated mainly on the area under both the receiver operating characteristic curve and the precision-recall curve. This paper shows that random forest, artificial neural networks and logistic regression have similar performance. They all indicate an overall satisfactory ability to predict debt recovery and hold potential to be implemented in day-to-day business related to non-performing loans.</p><p> </p>

corrected abstract:
<p>Credit scoring using machine learning has been gaining attention within the research field in recent decades and it is widely used in the financial sector today. Studies covering binary credit scoring of securitized non-performing loans are however very scarce. This paper is using random forest and artificial neural networks to predict debt recovery for such portfolios. As a performance benchmark, logistic regression is used. Due to the nature of high imbalance between the classes, the performance is evaluated mainly on the area under both the receiver operating characteristic curve and the precision-recall curve. This paper shows that random forest, artificial neural networks and logistic regression have similar performance. They all indicate an overall satisfactory ability to predict debt recovery and hold potential to be implemented in day-to-day business related to non-performing loans.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1568336 
abstract is: 
<p>X-ray computed tomography (CT) has since its introduction in the early 1970s become one of the most important tools used for medical imaging. In CT, a large number of x-ray attenuation measurements are combined and reconstructed to form a three-dimensional image of the targeted area. In the recent years, a new type of detector called photon counting detector (PCD) has attracted considerable interest. This new type of detector acquires spectral information is associated with several benefits and has shown to be very valuable. </p><p>Furthermore, the use of deep learning to reconstruct images produced by CT has attracted significant attention in the last couple of years. However, the best way of incorporating deep learning into the reconstruction chain into the reconstruction chain is still incompletely understood. Additionally, the use of deep learning has mainly been investigated for the case of conventional CT and not for CT performed with PCDs. It these two points that this work aims to address. </p><p>Multiple deep learning architectures were implemented and evaluated on material images acquired by simulating a PCD. The deep-learning part of the reconstruction took the form of image-domain denoising after the material images had been obtained from the material sinograms through filtered back projection. Then, a comparison between the different deep learning architectures was made to find out which architecture is the most suited for denoising images produced by PCDs in the image domain.</p><p> </p>

corrected abstract:
<p>X-ray computed tomography (CT) has since its introduction in the early 1970s become one of the most important tools used for medical imaging. In CT, a large number of x-ray attenuation measurements are combined and reconstructed to form a three-dimensional image of the targeted area. In the recent years, a new type of detector called photon counting detector (PCD) has attracted considerable interest. This new type of detector acquires spectral information is associated with several benefits and has shown to be very valuable.</p><p>Furthermore, the use of deep learning to reconstruct images produced by CT has attracted significant attention in the last couple of years. However, the best way of incorporating deep learning into the reconstruction chain into the reconstruction chain is still incompletely understood. Additionally, the use of deep learning has mainly been investigated for the case of conventional CT and not for CT performed with PCDs. It these two points that this work aims to address.</p><p>Multiple deep learning architectures were implemented and evaluated on material images acquired by simulating a PCD. The deep-learning part of the reconstruction took the form of image-domain denoising after the material images had been obtained from the material sinograms through filtered back projection. Then, a comparison between the different deep learning architectures was made to find out which architecture is the most suited for denoising images produced by PCDs in the image domain.</p>

Note - only change to remove the empty paragraph and removing unnecessary spaces at end of paragraphs.
----------------------------------------------------------------------
In diva2:1334765 
abstract is: 
<p>A global statement about a compact surface with constant Gaussian curvature is derived by elementary differential geometry methods. Surfaces and curves embedded in three-dimensional Euclidian space are introduced, as well as several key properties such as the tangent plane, the first and second fundamental form, and the Weingarten map. Furthermore, intrinsic and extrinsic properties of surfaces are analyzed, and the Gaussian curvature, originally derived as an extrinsic property, is proven to be an intrinsic property in Gauss Theorema Egregium. Lastly, through the aid of umbilical points on a surface, the statement that a compact, connected surface with constant Gaussian curvature is a sphere is proven.</p><p> </p>

corrected abstract:
<p>A global statement about a compact surface with constant Gaussian curvature is derived by elementary differential geometry methods. Surfaces and curves embedded in three-dimensional Euclidian space are introduced, as well as several key properties such as the tangent plane, the first and second fundamental form, and the Weingarten map. Furthermore, intrinsic and extrinsic properties of surfaces are analyzed, and the Gaussian curvature, originally derived as an extrinsic property, is proven to be an intrinsic property in Gauss Theorema Egregium. Lastly, through the aid of umbilical points on a surface, the statement that a compact, connected surface with constant Gaussian curvature is a sphere is proven.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1682470 
abstract is: 
<p>We present and prove some important theorems regarding determinantal point processes. In particular we focus on existance and uniqueness theorems. Furthermore, we present an algorithm for generating determinantal point processes with a finite-dimensional projection kernel. Also, we go through the mathematical preliminaries required to understand the theory.</p><p> </p>

corrected abstract:
<p>We present and prove some important theorems regarding determinantal point processes. In particular we focus on existance and uniqueness theorems. Furthermore, we present an algorithm for generating determinantal point processes with a finite-dimensional projection kernel. Also, we go through the mathematical preliminaries required to understand the theory.</p>

Note spelling error:
----------------------------------------------------------------------
In diva2:1380177 
abstract is: 
<p>The objective of this project is the development of a mission analysis tool for the nanosatellite company GomSpace Sweden. Although there are many existing software, they can be quite complicated and time consuming to use. The goal of this work is to build a simple app to be used at the earliest stages of space missions in order to obtain key figures of merit quickly and easily. By comparing results, assessing the feasibility of customer needs, analysing how various parameters affect each other, it enables immediate deeper understanding of the implications of the main design decisions that are taken at the very beginning of a mission. The tool shall aid the system engineering process of determining orbit manoeuvre capability specifically for CubeSat electric propulsion systems taking into account the most relevant factors for perturbation in Low Earth Orbit (LEO), i.e. atmospheric drag and Earth’s oblateness effects. The manoeuvres investigated are: orbit raising from an insert orbit to an operating orbit, orbit maintenance, deorbiting within the space debris mitigation guidelines and collision avoidance within the 12 to 24 hours that the system has to react. The manoeuvres cost is assessed in terms of Delta v requirements, propellant mass and transfer times. The tool was developed with MATLAB and packaged as a standalone Linux application.</p><p> </p>

corrected abstract:
<p>The objective of this project is the development of a mission analysis tool for the nanosatellite company GomSpace Sweden. Although there are many existing software, they can be quite complicated and time consuming to use. The goal of this work is to build a simple app to be used at the earliest stages of space missions in order to obtain key figures of merit quickly and easily. By comparing results, assessing the feasibility of customer needs, analysing how various parameters affect each other, it enables immediate deeper understanding of the implications of the main design decisions that are taken at the very beginning of a mission. The tool shall aid the system engineering process of determining orbit manoeuvre capability specifically for CubeSat electric propulsion systems taking into account the most relevant factors for perturbation in Low Earth Orbit (LEO), i.e. atmospheric drag and Earth’s oblateness effects. The manoeuvres investigated are: orbit raising from an insert orbit to an operating orbit, orbit maintenance, deorbiting within the space debris mitigation guidelines and collision avoidance within the 12 to 24 hours that the system has to react. The manoeuvres cost is assessed in terms of ∆v requirements, propellant mass and transfer times. The tool was developed with MATLAB and packaged as a standalone Linux application.</p>

Note - only change to remove the empty paragraph and replacing "Delta" with "∆".
----------------------------------------------------------------------
In diva2:1524971 
abstract is: 
<p>In this master thesis an input-model of a Nordic BWR power plant has been developed in APROS. The plant model contains key systems and major thermohydraulic components of the steam cycle, including I&amp;C systems (i.e. power, pressure, level and flow controls). The plant model is primarily designed for balance of plant studies at discrete power levels.</p><p>The input-model of the power plant focuses especially on the steam cycle which is crucial for analysing water and steam behaviour and its influence on the reactor power. At the current stage, the model primarily handles steady-state conditions of full-power operation, which has been the design point. It has also been shown that reduced-power operation can be simulated with a reasonable trendline of pressure and temperature progression over facility components.</p><p> </p>

corrected abstract:
<p>Nuclear power plants have proved to produce reliable and economic electricity but at the same time provoked debates mainly because of the risks involved during operation. To prevent unwanted events, it is important to identify and estimate their occurrence, and introduce measures to counteract them. Modelling and simulations are powerful tools that can be used to gain this type of knowledge and obtain information about the birthplace of incidents. However, the area of use is not limited to the safety perspective, as computer simulations can also introduce true advances in performance and reliability of power plants.</p><p>This master thesis was conducted at Westinghouse Electric Sweden AB, with the purpose to design and implement an input-model of a Nordic Boiling Water Reactor in APROS. The inputmodel of the power plant focused especially on the steam cycle which is crucial for analysing water and steam behaviour in the power plant and its influence on the reactor power. The inputmodel has been limited to representsteady-state conditions at full-power operation, and to some extent reduced-power operation. Thereby, plant model is primarily designed for Balance of Plant studies at discrete power levels.</p><p>The first section of the report contains an introduction to the concepts of nuclear energy and fundamentals of boiling water reactors. It is supposed to provide the reader with a basis for a fair understanding of nuclear power plant operation. Theoretical concepts of thermodynamics and fluid mechanics, which have been crucial for a proper approach in the process of creating the input-model, can be found in the theory section. The report does also contain a brief description of the plant systems upon which the design has been based on.</p><p>The report consists of further sections, where the model components and their implementation are presented followed by a model validation. The model validation is performed by a comparison approach, where simulation data is presented in relationship to reference data. The validation is done for full-power and reduce-power operation, at steady-state conditions, at which the model has shown to have decent compliance with the available reference data.</p><p>At the final stage of the project, the created input-model was used to evaluate an induced perturbation of feedwater temperature. The behaviour of the reactor, dependent of the feedwater temperature, is discussed for two simulation cases; with and without forced power control. The simulation enabled to perform a first step analysis of the effectiveness of the power control system.</p>

Note the abstract in the thesis is completely different from that in DiVA!
----------------------------------------------------------------------
In diva2:699782 
abstract is: 
<p>The dose distribution in the Gamma Knife (developed and produced by Elekta) is optimized over the weights (or Beam-on time) using different models other than the radiosurgical one used in Leksell Gamma Plan . These are based on DVH, EUD, TCP and NTCP. Also adding hypoxic regions are tested in the Gamma Knife to see whether or not the dose can be guided to these areas. This is done in two ways. For the DVH and EUD model the hypoxic area is regarded as a organ by itself and higher constraints is defined on it. In the TCP case blood vessels are outlined and the α and β parameters are perturbed to describe a hypoxic area. The models are tested in two cases. The first one is one tumour close to the brainstem and the second case is two tumours located far away from each other. Finally the results are compared to the dose distribution computed by the Gamma Knife.</p><p> </p>

corrected abstract:
<p>The dose distribution in the Gamma Knife is optimized over the weights (or Beam-on time) using different models other than the radiosurgical one used in Leksell Gamma Plan®. These are based on DVH, EUD, TCP and NTCP. Also adding hypoxic regions are tested in the Gamma Knife to see whether or not the dose can be guided to these areas. This is done in two ways. For the DVH and EUD model the hypoxic area is regarded as a organ by itself and higher constraints is defined on it. In the TCP case blood vessels are outlined and the α and β parameters are perturbed to describe a hypoxic area. The models are tested in two cases. The first one is one tumour close to the brainstem and the second case is two tumours located far away from each other. Finally the results are compared to the dose distribution computed by the Gamma Knife.</p>

Nore removed text that was not in the original, added the registered trademark symbol, and eliminated the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1519571 
abstract is: 
<p>For the purpose of americium recycling, the effect of americium content on the nuclear fuel behaviour needs to be investigated. Atomic scale simulations and classical molecular dynamic simulations provide a tool of choice for the study of thermophysical properties of the nuclear fuel.</p><p>In this work, we fitted a new interatomic empirical potential for (U,Am)O2 based on the CRG formalism. Our work enabled us to propose at the same time a new potential for the study of the Am-O system. The proposed potentials show good agreement with lattice parameters and enthalpy increments. We finally computed the heat capacity of (U,Am)O2 from 350 K to 3200 K for 0, 10, 20, 30, 40 and 50% americium contents using the potential obtained. The heat capacities calculated reveal a Bredig transition, as seen in UO2 and (U,Pu)O2. This transition shifts toward lower temperatures and its peak decreases in intensity when the Am content increases.</p><p> </p>

corrected abstract:
<p>For the purpose of americium recycling, the effect of americium content on the nuclear fuel behaviour needs to be investigated. Atomic scale simulations and classical molecular dynamic simulations provide a tool of choice for the study of thermophysical properties of the nuclear fuel.</p><p>In this work, we fitted a new interatomic empirical potential for (U,Am)O<sub>2</sub> based on the CRG formalism. Our work enabled us to propose at the same time a new potential for the study of the Am-O system. The proposed potentials show good agreement with lattice parameters and enthalpy increments. We finally computed the heat capacity of (U,Am)O<sub>2</sub> from 350 K to 3200 K for 0, 10, 20, 30, 40 and 50% americium contents using the potential obtained. The heat capacities calculated reveal a Bredig transition, as seen in UO<sub>2</sub> and (U,Pu)O<sub>2</sub>. This transition shifts toward lower temperatures and its peak decreases in intensity when the Am content increases.</p>

Note - only change to remove the empty paragraph and adding the subscripts
----------------------------------------------------------------------
In diva2:1779375 
abstract is: 
<p>The purpose of this project is to simulate the detection of γ-ray spectra emitted by radon isotopes and their daughters. This is done as a contribution to the development of radiation detectors to be used in a research project investigating the possibility of using increased amounts of the radioactive gas radon as an earthquake precursor. Before the onset of an earthquake, microcracks are formed in the surrounding stone structures due to stress, releasing greater than usual amounts of radon gas contained within the rock pores. A way of predicting an upcoming earthquake would then be to place radiation detectors in areas with high seismicity in order to measure possible changes. This could be done in soil, groundwater (via springs, wells, and boreholes), or air. In this project, we aim to understand how measurements in groundwater would differ from ones in air, and how to best make use of the spectra as seen in water. This was done by simulating a scenario in which a scintillator detector, made of cesium iodide, is placed in each media and then assessing the resulting γ-ray spectra.</p><p> </p>

corrected abstract:
<p>The purpose of this project is to simulate the detection of γ-ray spectra emitted by radon isotopes and their daughters. This is done as a contribution to the development of radiation detectors to be used in a research project investigating the possibility of using increased amounts of the radioactive gas radon as an earthquake precursor. Before the onset of an earthquake, microcracks are formed in the surrounding stone structures due to stress, releasing greater than usual amounts of radon gas contained within the rock pores. A way of predicting an upcoming earthquake would then be to place radiation detectors in areas with high seismicity in order to measure possible changes. This could be done in soil, groundwater (via springs, wells, and boreholes), or air. In this project, we aim to understand how measurements in groundwater would differ from ones in air, and how to best make use of the spectra as seen in water. This was done by simulating a scenario in which a scintillator detector, made of cesium iodide, is placed in each media and then assessing the resulting γ-ray spectra.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:373694
Note: no full text in DiVA

abstract is: 
<p>The department of Medical Technology, where I have done Master thesis project, develops and researches new method and technique within areas where ultrasound can be used to obtain the image of anatomical structure, functional capabilities and to suggest required treatment.</p>
<p>Nowadays cardio-vascular diseases, such as infarct, atherosclerosis and ischemic syndrome, are one of the most widespread diseases in the world that’s why timely detection, identification and treatment are so important.</p>
<p>The Master of Science qualification report consists 3 major parts: Medico-biological part, Design and Research parts.</p>
<p><strong>In Medico-biological part </strong>has been analyzed anatomical and physiological structure of the heart, current status of echocardiography with comparing with other techniques, summary of ultrasound methods with list of parameters that can be achieved is presented.</p>
<p><strong>In Design part </strong>has been developed new graphical modality based on Delta-V pump model using vector based statistical analysis for identification patients with ischemia. Software algorithm for automatically determine characteristic points for state diagram written in MatLab has been developed and implemented.</p>
<p><strong>In Research part </strong>in the first task using commercially available software based on Principal Component Analysis collected data from the hospital patients has been studied, results proved hypothesis concerning time variables importance; in the second task graphical module has been examined using collected data from the hospital patients both normal and with different cardio-vascular disease, and the results show good detection power of the algorithm.</p>
<p>At the end of the project presentation has been done and report has been published.</p>
<p>This project has been done in collaboration with the biggest medical institute in Sweden – Karolinska Institute - and results will be used in medical practice in Karolinska University Hospital in Huddinge and for future scientific needs.</p>
<p> </p>

corrected abstract:
<p>The department of Medical Technology, where I have done Master thesis project, develops and researches new method and technique within areas where ultrasound can be used to obtain the image of anatomical structure, functional capabilities and to suggest required treatment.</p>
<p>Nowadays cardio-vascular diseases, such as infarct, atherosclerosis and ischemic syndrome, are one of the most widespread diseases in the world that’s why timely detection, identification and treatment are so important.</p>
<p>The Master of Science qualification report consists 3 major parts: Medico-biological part, Design and Research parts.</p>
<p><strong>In Medico-biological part </strong>has been analyzed anatomical and physiological structure of the heart, current status of echocardiography with comparing with other techniques, summary of ultrasound methods with list of parameters that can be achieved is presented.</p>
<p><strong>In Design part </strong>has been developed new graphical modality based on Delta-V pump model using vector based statistical analysis for identification patients with ischemia. Software algorithm for automatically determine characteristic points for state diagram written in MatLab has been developed and implemented.</p>
<p><strong>In Research part </strong>in the first task using commercially available software based on Principal Component Analysis collected data from the hospital patients has been studied, results proved hypothesis concerning time variables importance; in the second task graphical module has been examined using collected data from the hospital patients both normal and with different cardio-vascular disease, and the results show good detection power of the algorithm.</p>
<p>At the end of the project presentation has been done and report has been published.</p>
<p>This project has been done in collaboration with the biggest medical institute in Sweden – Karolinska Institute - and results will be used in medical practice in Karolinska University Hospital in Huddinge and for future scientific needs.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1673952 
abstract is: 
<p>The conventional tube-and-wing aircraft has been around since the 1950s, with little to no innovative progress being made towards redesigning the conventional aircraft. The blended wing body (BWB) shape fuses the wing of the aircraft with the fuselage increasing structural strength while also increasing potential surface area to create lift, making it more efficient than conventional wing shapes. Today aviation has a 2 % CO2 contribution to global emissions. Aircraft manufacturers are predicting a steady rise for the aviation industry. The contribution of green-house gases is set to increase exponentially. Hydrogen fuel cells could deem a good fit between traditional combustion engine aircraft and electrical aircraft having a high efficiency but also being fuel-based. This report investigates the possibility of a prototype model of the Project ''Green Raven'' from KTH of creating a hybrid fuel cell BWB UAV with a 4 m wingspan. The analytical data is from literature and available benchmark data. First, an electrically driven subscale prototype is made and tested, and then the full-scale model is made. The prototype is pro-posed to be driven by a single two-bladed propeller with 10 x 4.7-inch dimensions running at 10000-13000 rpm with a takeoff weight of 4 kg, where 0.75 kg of the weight was from 5 Li-Po batteries. Performance parameters were calculated by given data with a given cruise speed of 30 m/s and a cruise endurance of 1 hour. The prototype will fly for close to maximum load at climb with an angle of 6°. With the Li-Po batteries with a total of 11 Ah, the aircraft has more than 10 % to spare for safety reasons.</p><p> </p>

corrected abstract:
<p>The conventional tube-and-wing aircraft has been around since the 1950s, with little to no innovative progress being made towards redesigning the conventional aircraft. The blended wing body (BWB) shape fuses the wing of the aircraft with the fuselage increasing structural strength while also increasing potential surface area to create lift, making it more efficient than conventional wing shapes. Today aviation has a 2 % CO<sub>2</sub> contribution to global emissions. Aircraft manufacturers are predicting a steady rise for the aviation industry. The contribution of greenhouse gases is set to increase exponentially. Hydrogen fuel cells could deem a good fit between traditional combustion engine aircraft and electrical aircraft having a high efficiency but also being fuel-based. This report investigates the possibility of a prototype model of the Project ”Green Raven” from KTH of creating a hybrid fuel cell BWB UAV with a 4 m wingspan. The analytical data is from literature and available benchmark data. First, an electrically driven subscale prototype is made and tested, and then the full-scale model is made. The prototype is proposed to be driven by a single two-bladed propeller with 10 x 4.7-inch dimensions running at 10000-13000 rpm with a takeoff weight of 4 kg, where 0.75 kg of the weight was from 5 Li-Po batteries. Performance parameters were calculated by given data with a given cruise speed of 30 m/s and a cruise endurance of 1 hour. The prototype will fly for close to maximum load at climb with an angle of 6°. With the Li-Po batteries with a total of 11 Ah, the aircraft has more than 10 % to spare for safety reasons.</p>

Note - only changes to remove the empty paragraph, added subscript, and removed the unnecessary hpyehn in "greenhouse".
----------------------------------------------------------------------
In diva2:1342226 
abstract is: 
<p>Functional testing is a vital process when building a satellite. However, often using flight-ready hardware for testing is not feasible. The work in this project has been to construct a flight representative model of the antenna deployment system for the KTH student-built MIST satellite. Specifically, the focus has been on creating a physical simulator for the antenna system. The purpose of the simulator created is to achieve the correct behavior, but without the need to use the real flight hardware. The challenges mainly concern establishing communication between the on-board computer of the satellite and the microcontroller on the created antenna deployment system, via the I$^2$C bus, and ensuring that physical responses occur in a useful manner. Further, the simulator needed to implement software with the same functionality as the real system. The microcontroller used in this project was an Arduino Due that represented the antenna deployment system's microcontroller. All the functions, e.g. temperature sensor and LEDs, were put together on a custom-made add-on circuit for the Arduino. Moreover, a 3D-printed model has been made for the deployment mechanism of the antenna elements. A simulation of the antenna system has been produced, determining whether a custom-built simulator can be used for functional testing of the antenna deployment system. The simulator can later be used for functional testing of the MIST satellite and also be the base for testing the deployment of the solar panels.</p><p> </p>

corrected abstract:
<p>Functional testing is a vital process when building a satellite. However, often using flight-ready hardware for testing is not feasible. The work in this project has been to construct a flight representative model of the antenna deployment system for the KTH student-built MIST satellite. Specifically, the focus has been on creating a physical simulator for the antenna system. The purpose of the simulator created is to achieve the correct behavior, but without the need to use the real flight hardware.</p><p>The challenges mainly concern establishing communication between the on-board computer of the satellite and the microcontroller on the created antenna deployment system, via the I<sup>2</sub>C bus, and ensuring that physical responses occur in a useful manner. Further, the simulator needed to implement software with the same functionality as the real system. The microcontroller used in this project was an Arduino Due that represented the antenna deployment system's microcontroller.</p><p>All the functions, e.g. temperature sensor and LEDs, were put together on a custom-made add-on circuit for the Arduino. Moreover, a 3D-printed model has been made for the deployment mechanism of the antenna elements. A simulation of the antenna system has been produced, determining whether a custom-built simulator can be used for functional testing of the antenna deployment system. The simulator can later be used for functional testing of the MIST satellite and also be the base for testing the deployment of the solar panels.</p>
----------------------------------------------------------------------
In diva2:1334807 
abstract is: 
<p>In this report, we study an abstract representation of reflection groups called Coxeter groups. Firstly, we introduce some important aspects of group theory. Next, we describe a concept called the word problem. Then, a way of defining groups given a set of generators and relations is presented. This theory is used to define the Coxeter groups, followed by a complete classification of the finite Coxeter groups as presented by H.S.M. Coxeter in 1935. Finally, we present a solution to the word problem for Coxeter groups and discuss some applications.</p><p> </p>

corrected abstract:
<p>In this report, we study an abstract representation of reflection groups called Coxeter groups. Firstly, we introduce some important aspects of group theory. Next, we describe a concept called the word problem. Then, a way of defining groups given a set of generators and relations is presented. This theory is used to define the Coxeter groups, followed by a complete classification of the finite Coxeter groups as presented by H.S.M. Coxeter in 1935. Finally, we present a solution to the word problem for Coxeter groups and discuss some applications.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:839498 
abstract is: 
<p>Our immune system uses antibodies to neutralize pathogens such as bacteria and viruses. Antibodies bind to parts of foreign proteins with high efficiency and specificity. We call such parts epitopes. The identification of epitopes, namely epitope mapping, may contribute to various immunological applications such as vaccine design, antibody production and immunological diagnosis.</p><p>Therefore, a fast and reliable method that can predict epitopes from the whole proteome is highly desirable.</p><p> </p><p>In this work we have developed a computational method that predicts epitopes based on sequence information. We focus on using local alignment to extract features from peptides and classifying them using Support Vector Machine. We also propose two approaches to optimize the features. Results show that our method can reliably predict epitopes and significantly outperforms some most commonly used tools.</p><p> </p>

corrected abstract:
<p>Our immune system uses antibodies to neutralize pathogens such as bacteria and viruses. Antibodies bind to parts of foreign proteins with high efficiency and specificity. We call such parts epitopes. The identification of epitopes, namely epitope mapping, may contribute to various immunological applications such as vaccine design, antibody production and immunological diagnosis. Therefore, a fast and reliable method that can predict epitopes from the whole proteome is highly desirable.</p><p>In this work we have developed a computational method that predicts epitopes based on sequence information. We focus on using local alignment to extract features from peptides and classifying them using Support Vector Machine. We also propose two approaches to optimize the features. The results show that our method can reliably predict epitopes and significantly outperforms some most commonly used tools.</p>
----------------------------------------------------------------------
In diva2:1879496 
abstract is: 
<p>Life insurance companies rely on mortality rate models to set appropriate premiums for their services. Over the past century, average life expectancy has increased and continues to do so, necessitating more accurate models. Two commonly used models are the Gompertz-Makeham law of mortality and the Lee-Carter model. The Gompertz-Makeham model depends solely on an age variable, while the Lee-Carter model incorporates a time-varying aspect which accounts for the increase in life expectancy over time. This paper constructs both models using training data acquired from Skandia Mutual Life Insurance Company and compares them to validation data from the same set. The study suggests that the Lee-Carter model may be able to offer some improvements compared to the Gompertz-Makeham law of mortality in terms of predicting future mortality rates. However, due to a lack of qualitative data, creating a competitive Lee-Carter model through Singular Value Decomposition, SVD, proved to be problematic. Switching from the current Gompertz-Makeham model to the Lee-Carter model should, therefore, be explored further when more high quality data becomes available.</p><p> </p>

corrected abstract:
<p>Life insurance companies rely on mortality rate models to set appropriate premiums for their services. Over the past century, average life expectancy has increased and continues to do so, necessitating more accurate models. Two commonly used models are the Gompertz-Makeham law of mortality and the Lee-Carter model. The Gompertz-Makeham model depends solely on an age variable, while the Lee-Carter model incorporates a time-varying aspect which accounts for the increase in life expectancy over time. This paper constructs both models using training data acquired from Skandia Mutual Life Insurance Company and compares them to validation data from the same set. The study suggests that the Lee-Carter model may be able to offer some improvements compared to the Gompertz-Makeham law of mortality in terms of predicting future mortality rates. However, due to a lack of qualitative data, creating a competitive Lee-Carter model through Singular Value Decomposition, SVD, proved to be problematic. Switching from the current Gompertz-Makeham model to the Lee-Carter model should, therefore, be explored further when more high quality data becomes available.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1800189 
abstract is: 
<p>In this master thesis, the accuracy of the crack depth meter RMG 4015 was evaluated for different types of cracks with various damage mechanisms. In total, 61 crack depth measurements were conducted with the crack depth meter on 56 cracks which were located in the 23 different test pieces supplied by Kiwa. The measured crack depths were then compared to the true crack depths, which were determined by cutting the test pieces and measuring directly on the cross-sections of the cracks using a light optical microscope. The results of the comparison showed that the RMG 4015, which uses potential drop techniques, was very accurate at measuring both strain induced and alkaline stress corrosion cracks. However, the results also showed that the crack depth meter underestimates chloride induced stress corrosion cracks, corrosion fatigue cracks and stress corrosion cracks/hydrogen embrittlement cracks at varying degrees. Therefore, the main recommendation for Kiwa is to switch the RMG 4015 to a crack depth meter that uses ultrasonic techniques instead.</p><p>The master thesis also explored the possibilities to improve an FE model produced by Kiwa in a previous project which involved an analysis of a cracked component. The present crack depth measure program included a test piece from this component. The stress distribution in the original model did not represent the cracks found in the real structure and it was suspected to be the result of some boundary conditions not corresponding to those acting in the actual pipe system. Some adjustments to the boundary conditions and contact regions were made and a new improved model with a better representing stress distribution was found.</p><p> </p>

corrected abstract:
<p>In this master thesis, the accuracy of the crack depth meter RMG 4015 was evaluated for different types of cracks with various damage mechanisms. In total, 61 crack depth measurements were conducted with the crack depth meter on 56 cracks which were located in the 23 different test pieces supplied by Kiwa. The measured crack depths were then compared to the true crack depths, which were determined by cutting the test pieces and measuring directly on the cross-sections of the cracks using a light optical microscope. The results of the comparison showed that the RMG 4015, which uses potential drop techniques, was very accurate at measuring both strain induced and alkaline stress corrosion cracks. However, the results also showed that the crack depth meter underestimates chloride induced stress corrosion cracks, corrosion fatigue cracks and stress corrosion cracks/hydrogen embrittlement cracks at varying degrees. Therefore, the main recommendation for Kiwa is to switch the RMG 4015 to a crack depth meter that uses ultrasonic techniques instead.</p><p>The master thesis also explored the possibilities to improve an FE model produced by Kiwa in a previous project which involved an analysis of a cracked component. The present crack depth measure program included a test piece from this component. The stress distribution in the original model did not represent the cracks found in the real structure and it was suspected to be the result of some boundary conditions not corresponding to those acting in the actual pipe system. Some adjustments to the boundary conditions and contact regions were made and a new improved model with a better representing stress distribution was found.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1111160 
abstract is: 
<p>Gaussian process methods are flexible non-parametric Bayesian methods used for regression and classification. They allow for explicit handling of uncertainty and are able to learn complex structures in the data. Their main limitation is their scaling characteristics: for n training points the complexity is <em>O</em>(n³) for training and <em>O</em>(n²) for prediction per test data point.</p><p>This makes full Gaussian process methods prohibitive to use on training sets larger than a few thousand data points. There has been recent research on approximation methods to make Gaussian processes scalable without severely affecting the performance. Some of these new approximation techniques are still not fully investigated and in a practical situation it is hard to know which method to choose. This thesis examines and evaluates scalable GP methods, especially focusing on the framework Massively Scalable Gaussian Processes introduced by Wilson et al. in 2016, which reduces the training complexity to nearly <em>O</em>(<em>n</em>) and the prediction complexity to <em>O</em>(1). The framework involves inducing point methods, local covariance function interpolation, exploitations of structured matrices and projections to low-dimensional spaces. The properties of the different approximations are studied and the possibilities of making improvements are discussed.</p><p> </p>

corrected abstract:
<p>Gaussian process methods are flexible non-parametric Bayesian methods used for regression and classification. They allow for explicit handling of uncertainty and are able to learn complex structures in the data. Their main limitation is their scaling characteristics: for 𝑛 training points the complexity is &Oscr;(𝑛<sup>3</sup>) for training and &Oscr;(𝑛<sup>2</sup>) for prediction per test data point. This makes full Gaussian process methods prohibitive to use on training sets larger than a few thousand data points.</p><p>There has been recent research on approximation methods to make Gaussian processes scalable without severely affecting the performance. Some of these new approximation techniques are still not fully investigated and in a practical situation it is hard to know which method to choose. This thesis examines and evaluates scalable GP methods, especially focusing on the framework Massively Scalable Gaussian Processes introduced by Wilson et al. in 2016, which reduces the training complexity to nearly &Oscr;(𝑛) and the prediction complexity to &Oscr;(1). The framework involves inducing point methods, local covariance function interpolation, exploitations of structured matrices and projections to low-dimensional spaces. The properties of the different approximations are studied and the possibilities of making improvements are discussed.</p>
----------------------------------------------------------------------
In diva2:1440824
Note: no full text in DiVA

abstract is: 
<p>This thesis focuses on the design and qualities of a gamma type Stirling engine from a thermodynamic point of view. The purpose is to calculate the efficiency of the gamma type Stirling engine, as well as the work output. An ideal thermodynamic Stirling cycle consists of an isothermal expansion, isochoric heat removal, isothermal compression, and lastly isochoric heat addition. An advantage with the Stirling engine is that it is able to work using any form of working gas and is described to work in a closed regenerative thermodynamic state. The gathered data from measuring the pressure and volume when energy is applied is used to calculate different values such as efficiency and net work from the engine. The data was collected by using different tools. A heat sensor was taped on the bottom plate to measure the temperature at a specific time. Secondly, a pressure sensor was connected to one of the six tubes on the top plate. Where the pressure tube was connected, varied in order to analyze if there was a difference in pressure at different distances from the center of the top plate. A photosensor was used to indicate when a full revolution had occurred so that the right data to represent a full cycle could be collected. The result was PV and Ts-diagrams for each pipe at different temperatures. The results indicated that there is a pressure difference of 800 Pa. By integrating these diagrams, the net work could be calculated. The highest measured net work was $0.32mJ$ through pipe 2 when the bottom plate has a temperature of 80°C. In conclusion, changing the placement of the pipe showed no remarkable differences, however, the theoretical efficiency increased with the temperature. The engine has more parts that can be analyzed such as the materialistic parameters and the relation in volume and temperature difference but are not taken into account in this thesis.</p><p> </p>

corrected abstract:
<p>This thesis focuses on the design and qualities of a gamma type Stirling engine from a thermodynamic point of view. The purpose is to calculate the efficiency of the gamma type Stirling engine, as well as the work output. An ideal thermodynamic Stirling cycle consists of an isothermal expansion, isochoric heat removal, isothermal compression, and lastly isochoric heat addition. An advantage with the Stirling engine is that it is able to work using any form of working gas and is described to work in a closed regenerative thermodynamic state. The gathered data from measuring the pressure and volume when energy is applied is used to calculate different values such as efficiency and net work from the engine. The data was collected by using different tools. A heat sensor was taped on the bottom plate to measure the temperature at a specific time. Secondly, a pressure sensor was connected to one of the six tubes on the top plate. Where the pressure tube was connected, varied in order to analyze if there was a difference in pressure at different distances from the center of the top plate. A photosensor was used to indicate when a full revolution had occurred so that the right data to represent a full cycle could be collected. The result was PV and Ts-diagrams for each pipe at different temperatures. The results indicated that there is a pressure difference of 800 Pa. By integrating these diagrams, the net work could be calculated. The highest measured net work was <em>0.32mJ</em> through pipe 2 when the bottom plate has a temperature of 80°C. In conclusion, changing the placement of the pipe showed no remarkable differences, however, the theoretical efficiency increased with the temperature. The engine has more parts that can be analyzed such as the materialistic parameters and the relation in volume and temperature difference but are not taken into account in this thesis.</p>

Note - only change to remove the empty paragraph and to set "0.32mJ" in italics (as it was an inline equation)
----------------------------------------------------------------------
In diva2:1342442 
abstract is: 
<p>In this report we demonstrate the usefulness of hidden Markov model estimation as a method to construct models of mouse behavior. We used a neural network to retrieve positional data of different body parts from overhead video recordings of lone mice in an enclosure. We then extracted features such as velocity and elongation from the positional data and used an implementation of the Baum-Welch algorithm to fit hidden Markov models to the feature data. We could identify recurring behaviors such as "running next to wall" and "investigating wall" among the estimated states in several different mice, which was consistent with what we could see in the actual videos. We thereby demonstrate that hidden Markov model estimation by the Baum-Welch algorithm can be utilized to automatically find models of mouse behavior.</p><p> </p>

corrected abstract:
<p>In this report we demonstrate the usefulness of hidden Markov model estimation as a method to construct models of mouse behavior. We used a neural network to retrieve positional data of different body parts from overhead video recordings of lone mice in an enclosure. We then extracted features such as velocity and elongation from the positional data and used an implementation of the Baum-Welch algorithm to fit hidden Markov models to the feature data. We could identify recurring behaviors such as ”running next to wall” and ”investigating wall” among the estimated states in several different mice, which was consistent with what we could see in the actual videos. We thereby demonstrate that hidden Markov model estimation by the Baum-Welch algorithm can be utilized to automatically find models of mouse behavior.</p>

Note - only change to remove the empty paragraph and fixed double quotes to match the original
----------------------------------------------------------------------
In diva2:1781495 
abstract is: 
<p>Capacitive Deionization (CDI) is an energy-efficient desalination technology that utilizes an electric field to extract ions from water. Flow-through CDI systems show potential for superior desalination performance compared to traditional flow-by CDI; however, they face the challenge of increased occurrence of Faradaic reactions, leading to undesired by-products and reduced energy efficiency. In this study, we constructed a flow-through CDI cell and investigated the desalination performance of the two possible cell configurations: upstream anode mode and downstream anode mode. A series of experiments were conducted, measuring conductivity and pH of the effluent solution during charging and discharging phases. The results were analyzed in terms of salt adsorption capacity and charge efficiency. We used pH fluctuations in the effluent solution as indicators of Faradaic reactions. It was found that upstream anode mode yielded superior desalination, with a salt adsorption capacity of 6.79 mg/g and charge efficiency of 64.3%, compared to downstream anode mode, which displayed a salt adsorption capacity of 5.19 mg/g and charge efficiency of 50.8%. However, upstream anode mode also produced more pronounced pH oscillations, suggesting a higher occurrence of Faradaic reactions. Reconciling these conflicting results and shedding light on the complex processes within the CDI cell calls for further investigation.</p><p> </p>

corrected abstract:
<p>Capacitive Deionization (CDI) is an energy-efficient desalination technology that utilizes an electric field to extract ions from water. Flow-through CDI systems show potential for superior desalination performance compared to traditional flow-by CDI; however, they face the challenge of increased occurrence of Faradaic reactions, leading to undesired by-products and reduced energy efficiency. In this study, we constructed a flow-through CDI cell and investigated the desalination performance of the two possible cell configurations: upstream anode mode and downstream anode mode. A series of experiments were conducted, measuring conductivity and pH of the effluent solution during charging and discharging phases. The results were analyzed in terms of salt adsorption capacity and charge efficiency. We used pH fluctuations in the effluent solution as indicators of Faradaic reactions. It was found that upstream anode mode yielded superior desalination, with a salt adsorption capacity of 6.79 mg g<sup>-1</sup> and charge efficiency of 64.3%, compared to downstream anode mode, which displayed a salt adsorption capacity of 5.19 mg g<sup>-1</sup> and charge efficiency of 50.8%. However, upstream anode mode also produced more pronounced pH oscillations, suggesting a higher occurrence of Faradaic reactions. Reconciling these conflicting results and shedding light on the complex processes within the CDI cell calls for further investigation.</p>

Note - only change to remove the empty paragraph and change the "/g" into "g<sup>-1</sup>" as in the original
----------------------------------------------------------------------
In diva2:1354140 
Note: no full text in DiVA

abstract is: 
<p>In this project, the fatigue behaviour of aluminium (AL 5083 H111) gusset and flange joined with a fillet weld, is investigated through experiments and numerical methods. The work aims at improved knowledge on fatigue in an aluminium welded joint subjected to constant amplitude varying load.</p><p> </p><p>The results of the experiments are investigated with the Basquin equation. The mean curve is estimated by the maximum likelihood estimation (MLE) by using both the failed specimen data and the run-out data. From the mean curve, a component specific design curve is estimated. Comparison of the component specific design curve with the recommendation’s design curve highlighted the inherent conservatism of the recommendation’s design curve.</p><p> </p><p>Nominal, hot-spot and equivalent notch methods were evaluated, and a comparative study was performed. The numerical investigation showed that the predicted fatigue life increased with model complexity. Comparison to the experimentally derived component specific design curve highlighted non-conservatism of the numerically predicted fatigue life for large stress ranges. The degree of conservatism of the numerical methods is however strongly affected by the slope of the considered design curve.</p>

corrected abstract:
<p>In this project, the fatigue behaviour of aluminium (AL 5083 H111) gusset and flange joined with a fillet weld, is investigated through experiments and numerical methods. The work aims at improved knowledge on fatigue in an aluminium welded joint subjected to constant amplitude varying load.</p><p>The results of the experiments are investigated with the Basquin equation. The mean curve is estimated by the maximum likelihood estimation (MLE) by using both the failed specimen data and the run-out data. From the mean curve, a component specific design curve is estimated. Comparison of the component specific design curve with the recommendation’s design curve highlighted the inherent conservatism of the recommendation’s design curve.</p><p>Nominal, hot-spot and equivalent notch methods were evaluated, and a comparative study was performed. The numerical investigation showed that the predicted fatigue life increased with model complexity. Comparison to the experimentally derived component specific design curve highlighted non-conservatism of the numerically predicted fatigue life for large stress ranges. The degree of conservatism of the numerical methods is however strongly affected by the slope of the considered design curve.</p>


Note - only change to remove the empty paragraphs
----------------------------------------------------------------------
In diva2:1436832 
abstract is: 
<p>In this report, we first briefly summarize Hermitian quantum mechanics before moving on to the non-Hermitian case. We then review PT-symmetric quantum mechanics with a focus on finite-dimensional systems, and include a novel generalization of a perturbative calculation of the C-operator. After briefly covering the basics of neutrino oscillations, we perturbatively examine a PT-symmetric addition to the neutrino oscillation Hamiltonian. We examine the effects of the addition with two different definitions of transition probabilities. However, probability is not conserved to first order with either definition. Further, we note that the effect of the chosen perturbation is to shift the transition probabilities by some phase, and to change the amplitudes of the transition probabilities.</p><p> </p>

corrected abstract:
<p>In this report, we first briefly summarize Hermitian quantum mechanics before moving on to the non-Hermitian case. We then review &Pscr;&Tscr;-symmetric quantum mechanics with a focus on finite-dimensional systems, and include a novel generalization of a perturbative calculation of the &Cscr;-operator. After briefly covering the basics of neutrino oscillations, we perturbatively examine a &Pscr;&Tscr;-symmetric addition to the neutrino oscillation Hamiltonian. We examine the effects of the addition with two different definitions of transition probabilities. However, probability is not conserved to first order with either definition. Further, we note that the effect of the chosen perturbation is to shift the transition probabilities by some phase, and to change the amplitudes of the transition probabilities.</p>

Note - removed the empty paragraph and corrected the script characters
----------------------------------------------------------------------
In diva2:1878884 
abstract is: 
<p>Reinforcement learning (RL) algorithms aim to identify optimal action sequences for an agent in a given environment, traditionally maximizing the expected rewards received from the environment by taking each action and transitioning between states. This thesis explores approaching RL distributionally, replacing the expected reward function by the full distribution over the possible rewards received, known as the value distribution. We focus on the quantile regression distributional RL (QR-DQN) algorithm introduced by Dabney et al. (2017), which models the value distribution by representing its quantiles. With such information of the value distribution, we modify the QR-DQN algorithm to enhance the agent's risk sensitivity. Our risk-averse algorithm is evaluated against the original QR-DQN in the Atari 2600 and in the Gymnasium environment, specifically in the games Breakout, Pong, Lunar Lander and Cartpole. Results indicate that the risk-averse variant performs comparably in terms of rewards while exhibiting increased robustness and risk aversion. Potential refinements of the risk-averse algorithm are presented.</p><p> </p>

corrected abstract:
<p>Reinforcement learning (RL) algorithms aim to identify optimal action sequences for an agent in a given environment, traditionally maximizing the expected rewards received from the environment by taking each action and transitioning between states. This thesis explores approaching RL distributionally, replacing the expected reward function by the full distribution over the possible rewards received, known as the value distribution. We focus on the quantile regression distributional RL (QR-DQN) algorithm introduced by Dabney et al. (2017), which models the value distribution by representing its quantiles. With such information of the value distribution, we modify the QR-DQN algorithm to enhance the agent's risk sensitivity. Our risk-averse algorithm is evaluated against the original QR-DQN in the Atari 2600 and in the Gymnasium environment, specifically in the games Breakout, Pong, Lunar Lander and Cartpole. Results indicate that the risk-averse variant performs comparably in terms of rewards while exhibiting increased robustness and risk aversion. Potential refinements of the risk-averse algorithm are presented.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:735921 
abstract is: 
<p>This paper presents PDEs that describes sedimentation by a system of diffusion and transportation equations. These PDEs are implemented with a semi-implicit scheme and solved on a Graphics Processing Unit (GPU). The equations are solved with the iterative solvers (conjugate gradient and biconjugate gradient stabilized method) provided by the software ViennaCL. The timings from these operations are compared with a CPU implementation.</p><p>Before using the iterative solvers, a sparse matrix and a right hand side vector is set. The sparse matrix and the right hand side vector are efficiently updated on the GPU. The implicit terms of the PDEs are stored in the sparse matrix and the explicit terms in the right hand side vector. The sparse matrix is stored in the compressed sparse row (CSR) format. Algorithms to update the sparse matrix for the PDEs, which have Neumann or a mix of Neumann and Dirichlet boundary conditions, are presented. As the values in the sparse matrix depend on values from the previous results, the sparse matrix has to be updated frequently. Considerable time is saved by updating the sparse matrix on the GPU instead of on the CPU (slow data transfers between CPU and GPU are reduced).</p><p>The speedup for the GPU implementation was found to be 8-10 and 12-18 for the GPUs GTX 590 and K20m respectively, depending on grid size. The high speedup is due to the CPU model of the CPUs used for timings being an older model. If a newer CPU model were used, the speedup would be lower. Due to limited access to newer hardware, a more accurate value for speedup comparison has not been acquired. Indications still prove that the GPU implementation is faster than the sequential CPU implementation.</p><p> </p>

corrected abstract:
<p>This paper presents PDEs that describes sedimentation by a system of diffusion and transportation equations. These PDEs are implemented with a semi-implicit scheme and solved on a Graphics Processing Unit (GPU). The equations are solved with the iterative solvers (conjugate gradient and biconjugate gradient stabilized method) provided by the software ViennaCL. The timings from these operations are compared with a CPU implementation.</p><p>Before using the iterative solvers, a sparse matrix and a right hand side vector is set. The sparse matrix and the right hand side vector are efficiently updated on the GPU. The implicit terms of the PDEs are stored in the sparse matrix and the explicit terms in the right hand side vector. The sparse matrix is stored in the compressed sparse row (CSR) format. Algorithms to update the sparse matrix for the PDEs, which have Neumann or a mix of Neumann and Dirichlet boundary conditions, are presented. As the values in the sparse matrix depend on values from the previous results, the sparse matrix has to be updated frequently. Considerable time is saved by updating the sparse matrix on the GPU instead of on the CPU (slow data transfers between CPU and GPU are reduced).</p><p>The speedup for the GPU implementation was found to be 8-10 and 12-18 for the GPUs GTX 590 and K20m respectively, depending on grid size. The high speedup is due to the CPU model of the CPUs used for timings being an older model. If a newer CPU model were used, the speedup would be lower. Due to limited access to newer hardware, a more accurate value for speedup comparison has not been acquired. Indications still prove that the GPU implementation is faster than the sequential CPU implementation.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1350076 
abstract is: 
<p>In this report we investigate the exotic hadrons known as pentaquarks. A brief overview of relevant concepts and theory is initially presented in order to aid the reader. Thereafter, the history of this field with regards to theory and experiments is discussed. In particular, a group theoretic classification of these states is studied. A simple mass formula for pentaquark states is examined and predictions are subsequently made about the composition and mass of possible pentaquark states. Furthermore, this mass formula is modified to examine and predict additional pentaquark states. A number of numerical fits concerning the masses of pentaquarks are performed and studied. Future research is explored with regards to the information presented in this thesis.</p><p> </p>

corrected abstract:
<p>In this report we investigate the exotic hadrons known as pentaquarks. A brief overview of relevant concepts and theory is initially presented in order to aid the reader. Thereafter, the history of this field with regards to theory and experiments is discussed. In particular, a group theoretic classification of these states is studied. A simple mass formula for pentaquark states is examined and predictions are subsequently made about the composition and mass of possible pentaquark states. Furthermore, this mass formula is modified to examine and predict additional pentaquark states. A number of numerical fits concerning the masses of pentaquarks are performd and studied. Future research is explored with regards to the information presented in this thesis.</p>

Note - only change to remove the empty paragraph and replacing "performed" with "performd" - error in the original
----------------------------------------------------------------------
In diva2:1873671 
abstract is: 
<p>In this report, we present a novel Bayesian inference framework to reconstruct the three-dimensional initial conditions of cosmic structure formation from data. To achieve this goal, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. According to the cosmological paradigm, all observable structures were formed from tiny primordial quantum fluctuations generated during the early stages of the Universe. As time passed, these seed fluctuations grew via gravitational aggregation to form the presently observed cosmic web traced by galaxies. For this reason, the specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we develop a novel machine learning approach that leverages the hierarchical nature of structure formation. We demonstrate our method in a mock analysis and find that we can recover the initial conditions with high accuracy, showing the potential of our model.</p><p> </p>

corrected abstract:
<p>In this report, we present a novel Bayesian inference framework to reconstruct the three-dimensional initial conditions of cosmic structure formation from data. To achieve this goal, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. According to the cosmological paradigm, all observable structures were formed from tiny primordial quantum fluctuations generated during the early stages of the Universe. As time passed, these seed fluctuations grew via gravitational aggregation to form the presently observed cosmic web traced by galaxies. For this reason, the specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we develop a novel machine learning approach that leverages the hierarchical nature of structure formation. We demonstrate our method in a mock analysis and find that we can recover the initial conditions with high accuracy, showing the potential of our model.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1342319 
abstract is: 
<p>To balance and control different aircraft, it is often necessary to use some type of control system, the less stable the craft is without any control system, the more advanced the system is required to be. This project is an experiment which purpose is to attempt to control a very unstable craft using steerable rudders. A design of the craft is modeled in CAD after a rough estimation to determine the required capacity of the components. Then a simulation of the craft is modeled in Matlab’s Simulink environment, which is used to test the control system’s capabilities and determine its optimal settings. Finally a physical model is built to see if the control system is sufficiently designed to stabilize the vessel under real conditions, which when flown did not successfully balance due to insufficient roll capability. Different solutions to this problem and other potential improvements is then discussed.</p><p> </p>

corrected abstract:
<p>To balance and control different aircraft it is often necessary to use some type of control system, the less stable the craft is without any control system, the more advanced the system is required to be. This project is an experiment which goal is to attempt to control a very unstable craft using steerable rudders. A design of the craft is modeled in CAD after a rough estimation to determine the required capacity of the components. Then a simulation of the craft is modeled in Matlab’s Simulink environment which is used to test the control system’s capabilities and determine its optimal settings, the simulation was considered successful because it showed that the craft could balance. Finally a physical model is built to test if the control system is sufficient to stabilize the vessel under real world conditions. When the real aircraft was flown it did not successfully balance due to insufficient roll capability caused by the motor torque being larger than expected. Different solutions to this problem and other potential improvements is then discussed.</p>

Note many changes in the wording betweeen the DiVA and original text, also removed the unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:1450318 
abstract is: 
<p>Sheet-swept connection solutions are a method for joining structural pipes in aircraft constructions. It is a simple approach that avoids the extensive demands placed on, among other things, welded connections. Previously, calculation data were not available, which this study aims to meet in the form of specifications and comparisons. Graham Lee's drawings and design specifications for the replica variant of the Nieuport 12 aircraft have been followed in the analysis of the connection solution. The method is based on three parts, calculations, experimental testing and FEM analysis. These form a specification of the joint's four most central load cases and their strength: tension (1130 N), plane deflection (4.5 Nm), in plane deflection (17.5 Nm) and torsion (4.5 Nm). The results are compared with calculations of loads in the pendulum rudder to determine how well the connection solution is suitable for this purpose. Identified load cases in the rudder are: plane deflection (3.2 Nm) and torsion (≤3.2 Nm). The results indicate that this type of connection is weak in the proposed purpose. The analysis highlights clear weaknesses in the connection solution and recommendations for improvements are given, these are primarily aimed at the thickness of the gusset and the introduction of an additional rivet. Furthermore, the sheet-swept joint solution is compared with welded joints which prove to be more suitable for this application.</p><p> </p>

corrected abstract:
<p>Sheet-swept connection solutions are a method for joining structural pipes in aircraft constructions. It is a simple approach that avoids the extensive demands placed on, among other things, welded connections. Previously, calculation data were not available, which this study aims to meet in the form of specifications and comparisons. Graham Lee's drawings and design specifications for the replica variant of the Nieuport 12 aircraft have been followed in the analysis of the connection solution. The method is based on three parts, calculations, experimental testing and FEM analysis. These form a specification of the joint's four most central load cases and their strength: tension (1130 N), plane deflection (4.5 Nm), in plane deflection (17.5 Nm) and torsion (4.5 Nm). The results are compared with calculations of loads in the pendulum rudder to determine how well the connection solution is suitable for this purpose. Identified load cases in the rudder are: plane deflection (3.2 Nm) and torsion (≤3.2 Nm). The results indicate that this type of connection is weak in the proposed purpose. The analysis highlights clear weaknesses in the connection solution and recommendations for improvements are given, these are primarily aimed at the thickness of the gusset and the introduction of an additional rivet. Furthermore, the sheet-swept joint solution is compared with welded joints which prove to be more suitable for this application.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1441692 
abstract is: 
<p>This report is part of a bachelor’s degree project in solid mechanics at KTH, Stockholm. It is performed by two students on behalf of the social enterprise Better Shelter, which provides shelters for people displaced by war and natural disasters. The scope of the project is to expand Better Shelters product specifications by providing improvements of the shelter earth anchors. This would allow use of the shelter in areas affected by higher wind speeds and thereby help more people in need of temporary housing and shelters. The earth anchors prevent the shelter from uplifting and tilting by taking uplift forces when horizontal wind loads acts on the structure. Two wind models with wind speeds up to 36 m/s were created to find the reaction forces on the anchors resisting the wind load. The wind models were compared with each other to validate the results and find the largest reaction forces on the anchors. Simulations of the anchors were made to analyse occurring stresses due to wind loads. Redesigns of the current earth anchor were made to find improvements of the anchor shape and reduce the stresses on the anchor. Experiments were then performed to analyse the redesigned anchor shapes in practise. The redesign, calculations and computational analyses of the anchors were done using the programs SolidEdge, ANSYS and Matlab. Results showed that redesigning the anchor contact area with the anchor wire reduced the stresses on the anchors drastically. Increasing the wing size of the anchors proved to be successful for preventing anchors from being pulled out of the soil. This allows better use of the current anchor material volume. Experiments also proved that burying the anchor deeper into the soil is an effective way of increasing the resistance from being pulled out of the ground. By reducing the stresses on the anchor, more materials are available for use. This could be explored further and is a suggested as a continuation of this project. The current anchor material is aluminium, and most aluminium alloys can be used with the redesigned ball joint connection to the anchor wire even when wind forces are large.</p><p> </p>

corrected abstract:
<p>This report is part of a bachelor’s degree project in solid mechanics at KTH, Stockholm. It is performed by two students on behalf of the social enterprise Better Shelter, which provides shelters for people displaced by war and natural disasters. The scope of the project is to expand Better Shelters product specifications by providing improvements of the shelter earth anchors. This would allow use of the shelter in areas affected by higher wind speeds and thereby help more people in need of temporary housing and shelters.</p><p>The earth anchors prevent the shelter from uplifting and tilting by taking uplift forces when horizontal wind loads acts on the structure. Two wind models with wind speeds up to 36 𝑚/𝑠 were created to find the reaction forces on the anchors resisting the wind load. The wind models were compared with each other to validate the results and find the largest reaction forces on the anchors. Simulations of the anchors were made to analyse occurring stresses due to wind loads.</p><p>Redesigns of the current earth anchor were made to find improvements of the anchor shape and reduce the stresses on the anchor. Experiments were then performed to analyse the redesigned anchor shapes in practise. The redesign, calculations and computational analyses of the anchors were done using the programs SolidEdge, ANSYS and Matlab.</p><p>Results showed that redesigning the anchor contact area with the anchor wire reduced the stresses on the anchors drastically. Increasing the wing size of the anchors proved to be successful for preventing anchors from being pulled out of the soil. This allows better use of the current anchor material volume. Experiments also proved that burying the anchor deeper into the soil is an effective way of increasing the resistance from being pulled out of the ground.</p><p>By reducing the stresses on the anchor, more materials are available for use. This could be explored further and is a suggested as a continuation of this project. The current anchor material is aluminium, and most aluminium alloys can be used with the redesigned ball joint connection to the anchor wire even when wind forces are large.</p>
----------------------------------------------------------------------
In diva2:1876745 
abstract is: 
<p>The Thermo-Calc software is a key tool in the research process for many material engineers. However, integrating multiple modules in Thermo-Calc requires the user to write code in a Python-based language, which can be challenging for novice programmers. This project aims to enable the generation of such code from user prompts by using existing generative AI models. In particular, we use a retrieval-augmented generation architecture applied to LLaMA and Mistral models. We use Code LLaMA-Instruct models with 7, 13, and 34 billion parameters, and a Mistral-Instruct model with 7 billion parameters. These models are all based on LLaMA 2. We also use a LLaMA 3-Instruct model with 8 billion parameters. All these models are instruction-tuned, which suggests that they have the capability to interpret natural language and identify appropriate options for a command-line program such as Python. In our testing, the LLaMA 3-Instruct model performed best, achieving 53% on the industry benchmark HumanEval and 49% on our internal adequacy assessment at pass@1, which is the expected probability of getting a correct solution when generating a response. This indicates that the model generates approximately every other answer correct. Due to GPU memory limitations, we had to apply quantisation to process the 13 and 34 billion parameter models. Our results revealed a mismatch between model size and optimal levels of quantisation, indicating that reduced precision adversely affects the performance of these models. Our findings suggest that a properly customised large language model can greatly reduce the coding effort of novice programmers, thereby improving productivity in material research.</p><p> </p>

corrected abstract:
<p>The Thermo-Calc software is a key tool in the research process for many material engineers. However, integrating multiple modules in Thermo-Calc requires the user to write code in a Python-based language, which can be challenging for novice programmers. This project aims to enable the generation of such code from user prompts by using existing generative AI models. In particular, we use a retrieval-augmented generation architecture applied to LLaMA and Mistral models. We use Code LLaMA-Instruct models with 7, 13, and 34 billion parameters, and a Mistral-Instruct model with 7 billion parameters. These models are all based on LLaMA 2. We also use a LLaMA 3-Instruct model with 8 billion parameters. All these models are instruction-tuned, which suggests that they have the capability to interpret natural language and identify appropriate options for a command-line program such as Python. In our testing, the LLaMA 3-Instruct model performed best, achieving 53% on the industry benchmark HumanEval and 49% on our internal adequacy assessment at pass@1, which is the expected probability of getting a correct solution when generating a response. This indicates that the model generates approximately every other answer correct. Due to GPU memory limitations, we had to apply quantisation to process the 13 and 34 billion parameter models. Our results revealed a mismatch between model size and optimal levels of quantisation, indicating that reduced precision adversely affects the performance of these models. Our findings suggest that a properly customised large language model can greatly reduce the coding effort of novice programmers, thereby improving productivity in material research.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1285800 
abstract is: 
<p> As the challenge grows in the vehicle industry, tightening the margins on financial and environmental costs of the vehicle development, computer aided engineering becomes more and more attractive. Extensive work is being invested in creating detailed models that can replicate vehicle behaviour accurately and efficiently. The work in this thesis starts with studying objective and subjective evaluations of vehicles as well as their counterparts in vehicle models and a simulator environment. Then, it continues to locate the weaknesses in the models, and investigate the possible improvements. The first part of the thesis focused on performing a literature study concerning the objective metrics and their use in the vehicle industry, as well as the use of simulators. This served as a foundation for the use of objective metrics in the validation of the CarRealTime models. The tools used in the thesis were also introduced. The work continued with the study of previously collected data concerning vehicle evaluation through subjective assessment and objective metrics, with different anti-roll bar configurations, to build trust in the ability of the drivers in evaluating these criteria. Similar data from the CarRealTime models and the simulator were also studied. The aim was to evaluate the simulator driving experience accuracy through the subjective assessment. The weaknesses of the model were identified, and an improved steering model was introduced, replacing the old lookup tables with a Pfeffer model from CarRealTime combined with the steering assist unit in Simulink. An extensive parameter study was performed to understand the effect of selected parameters on the driving experience. Using the same model, the simulator delays were studied in terms of replicating yaw and lateral movements, and how this can affect the driver’s perception of the driving experience. Finally, the results from the parameter study were used to assign the weight parameters in the optimization objective function where the goal was to study the possibility of improving the accuracy of the driving experience as well as counteracting the effects of simulator delays. The Matlab Optimization Toolkit was used in the process. As a conclusion, it was shown that the subjective assessment together with the objective metrics played a crucial role in identifying model and simulator weaknesses. The parameter study showed promising opportunities in solving the aforementioned issues, with the optimization tool and boundaries needing more elaborate work to reach conclusive results.</p><p> </p>

corrected abstract:
<p> As the challenge grows in the vehicle industry, tightening the margins on financial and environmental costs of the vehicle development, computer aided engineering becomes more and more attractive. Extensive work is being invested in creating detailed models that can replicate vehicle behaviour accurately and efficiently. The work in this thesis starts with studying objective and subjective evaluations of vehicles as well as their counterparts in vehicle models and a simulator environment. Then, it continues to locate the weaknesses in the models, and investigate the possible improvements. The first part of the thesis focused on performing a literature study concerning the objective metrics and their use in the vehicle industry, as well as the use of simulators. This served as a foundation for the use of objective metrics in the validation of the CarRealTime models. The tools used in the thesis were also introduced. The work continued with the study of previously collected data concerning vehicle evaluation through subjective assessment and objective metrics, with different anti-roll bar configurations, to build trust in the ability of the drivers in evaluating these criteria. Similar data from the CarRealTime models and the simulator were also studied. The aim was to evaluate the simulator driving experience accuracy through the subjective assessment. The weaknesses of the model were identified, and an improved steering model was introduced, replacing the old lookup tables with a Pfeffer model from CarRealTime combined with the steering assist unit in Simulink. An extensive parameter study was performed to understand the effect of selected parameters on the driving experience. Using the same model, the simulator delays were studied in terms of replicating yaw and lateral movements, and how this can affect the driver’s perception of the driving experience. Finally, the results from the parameter study were used to assign the weight parameters in the optimization objective function where the goal was to study the possibility of improving the accuracy of the driving experience as well as counteracting the effects of simulator delays. The Matlab Optimization Toolkit was used in the process. As a conclusion, it was shown that the subjective assessment together with the objective metrics played a crucial role in identifying model and simulator weaknesses. The parameter study showed promising opportunities in solving the aforementioned issues, with the optimization tool and boundaries needing more elaborate work to reach conclusive results.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:741548 
abstract is: 
<p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p><p> </p>
skipping mc='isa'

partal corrected: diva2:741548: <p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p><p> </p>

corrected abstract:
<p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1781509 
Note: no full text in DiVA

abstract is: 
<p>The objective of this report is to explore and assess the potential of two modified designs derived from a perfectly circular nested tubes NANF configuration. By altering the curvature profile and adopting a rounded triangular shape, various parameters are systematically varied to evaluate their impact on the confinement loss properties within the operating range around 850 nm. By leveraging geometric optics, an attempt is made to increase the angle of incidence of light on the tubes in order to maximize reflection. Through extensive simulations, the behavior of the modified designs is analyzed and compared against the literature-based NANF-B. The simulation results indicate that the simulated NANF-B exhibits a skewed wavelength range towards lower wavelengths compared to the literature version probably because of slight differences in how the model was designed. However, the proposed alterations demonstrate significantly reduced bandwidth and only show agreement with NANF-B within the lower wavelength range. Furthermore, the simulations reveal that the points where the tubes connect to the outer cladding play a critical role in the overall loss characteristics at longer wavelengths. This finding suggests that designs incorporating inward-curving tubes towards the outer cladding offer improved performance throughout the anti-resonant window where propagation is feasible.</p><p> </p>

corrected abstract:
<p>The objective of this report is to explore and assess the potential of two modified designs derived from a perfectly circular nested tubes NANF configuration. By altering the curvature profile and adopting a rounded triangular shape, various parameters are systematically varied to evaluate their impact on the confinement loss properties within the operating range around 850 nm. By leveraging geometric optics, an attempt is made to increase the angle of incidence of light on the tubes in order to maximize reflection. Through extensive simulations, the behavior of the modified designs is analyzed and compared against the literature-based NANF-B. The simulation results indicate that the simulated NANF-B exhibits a skewed wavelength range towards lower wavelengths compared to the literature version probably because of slight differences in how the model was designed. However, the proposed alterations demonstrate significantly reduced bandwidth and only show agreement with NANF-B within the lower wavelength range. Furthermore, the simulations reveal that the points where the tubes connect to the outer cladding play a critical role in the overall loss characteristics at longer wavelengths. This finding suggests that designs incorporating inward-curving tubes towards the outer cladding offer improved performance throughout the anti-resonant window where propagation is feasible.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1865115 
abstract is: 
<p>This thesis deals with Large Eddy Simulations (LES) of the Atmospheric Boundary Layer (ABL), focusing on studying the resolution dependence of turbulent passive scalar transport within the layer. The ABL is the lowest part of the atmosphere, where humans live and conduct most of their daily activities. Here, a scalar was injected at four different heights in a mixed shear- and convective-driven ABL, which was simulated using the Spectral Element Method (SEM) code Nek5000. The statistics of the four scalars were analysed and their resolution dependence was studied and compared to that of non-scalar quantities. No significant resolution dependence was found with regards to non-scalar quantities, while scalar quantities show a rather strong dependence on resolution especially in the first quarter of the simulation. Negative concentration values are found within the layer and some approaches to solve the problem are proposed. Statistics alone provide an accurate description of the general ABL behaviour, but are found to be insufficient to capture the dynamics of the scalar injection, which ought to be analysed with more advanced methods (e.g. modal decomposition). The structures arising within the layer are also analysed, and further work regarding the study of scalar fronts is suggested.</p><p> </p>

corrected abstract:
<p>This thesis deals with Large Eddy Simulations (LES) of the Atmospheric Boundary Layer (ABL), focusing on studying the resolution dependence of turbulent passive scalar transport within the layer. The ABL is the lowest part of the atmosphere, where humans live and conduct most of their daily activities. Here, a scalar was injected at four different heights in a mixed shear- and convective-driven ABL, which was simulated using the Spectral Element Method (SEM) code Nek5000. The statistics of the four scalars were analysed and their resolution dependence was studied and compared to that of non-scalar quantities. No significant resolution dependence was found with regards to non-scalar quantities, while scalar quantities show a rather strong dependence on resolution especially in the first quarter of the simulation. Negative concentration values are found within the layer and some approaches to solve the problem are proposed. Statistics alone provide an accurate description of the general ABL behaviour, but are found to be insufficient to capture the dynamics of the scalar injection, which ought to be analysed with more advanced methods (e.g. modal decomposition). The structures arising within the layer are also analysed, and further work regarding the study of scalar fronts is suggested.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1197616 
abstract is: 
<p>The use of turbocharged Diesel engines is nowadays a widespread practice in the automotive sector: heavy-duty vehicles like trucks or buses, in particular, are often equipped with turbocharged engines. An accurate study of the flow field developing inside both the main components of a turbocharger, i.e. compressor and turbine, is therefore necessary: the synergistic use of CFD simulations and experimental tests allows to fulfill this requirement.</p><p>The aim of this thesis is to investigate the performance and the flow field that develops inside a centrifugal compressor for automotive turbochargers. The study is carried out by means of numerical simulations, both steady-state and transient, based on RANS models (Reynolds Averaged Navier-Stokes equations). The code utilized for the numerical simulations is Ansys CFX.</p><p> </p><p>The first part of the work is an engineering attempt to develop a CFD method for predicting the performance of a centrifugal compressor which is based solely on steady-state RANS models. The results obtained are then compared with experimental observations. The study continues with an analysis of the sensitivity of the developed CFD method to different parameters: influence of both position and model used for the rotor-stator interfaces and the axial tip-clearance on the global performances is studied and quantified.</p><p> </p><p>In the second part, a design optimization study based on the Design of Experiments (DoE) approach is performed. In detail, transient RANS simulations are used to identify which geometry of the recirculation cavity hollowed inside the compressor shroud (ported shroud design) allows to mitigate the backflow that appears at low mass-flow rates. Backflow can be observed when the operational point of the compressor is suddenly moved from design to surge conditions. On actual heavy-duty vehicles, these conditions may arise when a rapid gear shift is performed.</p>

corrected abstract:
<p>The use of turbocharged Diesel engines is nowadays a widespread practice in the automotive sector: heavy-duty vehicles like trucks or buses, in particular, are often equipped with turbocharged engines. An accurate study of the flow field developing inside both the main components of a turbocharger, i.e. compressor and turbine, is therefore necessary: the synergistic use of CFD simulations and experimental tests allows to fulfill this requirement.</p><p>The aim of this thesis is to investigate the performance and the flow field that develops inside a centrifugal compressor for automotive turbochargers. The study is carried out by means of numerical simulations, both steady-state and transient, based on RANS models (Reynolds Averaged Navier-Stokes equations). The code utilized for the numerical simulations is Ansys CFX.</p><p>The first part of the work is an engineering attempt to develop a CFD method for predicting the performance of a centrifugal compressor which is based solely on steady-state RANS models. The results obtained are then compared with experimental observations. The study continues with an analysis of the sensitivity of the developed CFD method to different parameters: influence of both position and model used for the rotor-stator interfaces and the axial tip-clearance on the global performances is studied and quantified.</p><p>In the second part, a design optimization study based on the Design of Experiments (DoE) approach is performed. In detail, transient RANS simulations are used to identify which geometry of the recirculation cavity hollowed inside the compressor shroud (ported shroud design) allows to mitigate the backflow that appears at low mass-flow rates. Backflow can be observed when the operational point of the compressor is suddenly moved from design to surge conditions. On actual heavy-duty vehicles, these conditions may arise when a rapid gear shift is performed.</p>


Note - only change to remove the empty paragraphs
----------------------------------------------------------------------
In diva2:1341272 
abstract is: 
<p>In the autumn 2018 political elections were held in Sweden and consequently it is interesting to investigate what can affect how people vote. The purpose with this report is investigating if there are correspondences between the characteristics of a municipality and how the people in that municipality voted in the general election. Clustering on data sets with municipality characteristics and municipality general election statistics from 2018 is the basis of this study. K-means clustering and hierarchical clustering are the clustering methods that are used. In the report results of the clustering and the construction of a method for comparing clusterings are presented. The results show that there are some correspondences but that clustering is not the optimal method for analysing this data set.</p><p> </p>

corrected abstract:
<p>In the autumn 2018 political elections were held in Sweden and consequently it is interesting to investigate what can affect how people vote. The purpose with this report is investigating if there are correspondences between the characteristics of a municipality and how the people in that municipality voted in the general election. Clustering on data sets with municipality characteristics and municipality general election statistics from 2018 is the basis of this study. 𝐾-means clustering and hierarchical clustering are the clustering methods that are used. In the report results of the clustering and the construction of a method for comparing clusterings are presented. The results show that there are some correspondences but that clustering is not the optimal method for analysing this data set.</p>

Note - only change to remove the empty paragraph and replace "K" by "𝐾".
----------------------------------------------------------------------
In diva2:1342347 
abstract is: 
<p>The current trend in the automotive industry towards more fuel efficient vehicles requires all components to be as light as possible while still meeting other demands such as stiffness and feasible cost. The purpose of this study was to investigate the possibility to replace a partial chassis structure in a Scania low-entry city bus. The partial chassis to be replaced consists of a steel structure and an inner flooring, with the purpose to support loads that the bus is subject to. This was to be done with a composite sandwich structure, with the primary goal to reduce weight by at least 40% and number of components by 50%. The replacement structure needed to meet the stiffness and strength requirements that the current structure fulfils. This was achieved by designing two concepts, concept 1 and concept 2, through an iterative FE-analysis in ANSYS. Two prototypes where built and tested for real world load applications. The result from this study showed that it was possible to meet both the weight and component reduction goal. Concept 1 and concept 2 achieved a weight reduction of 62% and 68% respectively and the number of components was reduced significantly. Further work would be to investigate the interface between the new structure and the rest of the bus, modal- and fatigue analyses, production implementation and economical aspects to name a few.</p><p> </p>

corrected abstract:
<p>The current trend in the automotive industry towards more fuel efficient vehicles requires all components to be as light as possible while still meeting other demands such as stiffness and feasible cost.</p><p>The purpose of this study was to investigate the possibility to replace a partial chassis structure in a Scania low-entry city bus. The partial chassis to be replaced consists of a steel structure and an inner flooring, with the purpose to support loads that the bus is subject to. This was to be done with a composite sandwich structure, with the primary goal to reduce weight by at least 40% and number of components by 50%. The replacement structure needed to meet the stiffness and strength requirements that the current structure fulfils. This was achieved by designing two concepts, <em>concept 1</em> and <em>concept 2</em>, through an iterative FE-analysis in ANSYS. Two prototypes where built and tested for real world load applications.</p><p>The result from this study showed that it was possible to meet both the weight and component reduction goal. Concept 1 and concept 2 achieved a weight reduction of 62% and 68% respectively and the number of components was reduced significantly. Further work would be to investigate the interface between the new structure and the rest of the bus, modal- and fatigue analyses, production implementation and economical aspects to name a few.</p>

Note - only changes to remove the empty paragraph,  add paragraph breaks, and add italics
----------------------------------------------------------------------
In diva2:1335215 
abstract is: 
<p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for functioning quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si3N4 has been used as a strip waveguide, placed on top of a silicon oxide SiO2 wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p><p> </p>
mc='functionin' c='function in'

partal corrected: diva2:1335215: <p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for function ing quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si3N4 has been used as a strip waveguide, placed on top of a silicon oxide SiO2 wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p><p> </p>

corrected abstract:
<p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for functioning quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si<sub>3</sub>N<sub>4</sub> has been used as a strip waveguide, placed on top of a silicon oxide SiO<sub>2</sub> wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p>

Note - only change to remove the empty paragraph and added subscripts
----------------------------------------------------------------------
In diva2:1334842 
abstract is: 
<p>Mathematics contains many hard problems. In this paper we discuss how some of these hard problems can be solved with techniques from other math fields than the problems own discipline. First we solve some combinatorial problems using the knowledge that a maximum amount of vectors in a linearly independent set over a subset of a vector space F^n over a field F is n. Then we discuss and explain Z.Dvir's famous proof regarding Kakeya sets over finite fields. He is able to establish a lower bound of the size of Kakeya sets using polynomials.</p><p> </p>

corrected abstract:
<p>Mathematics contains many hard problems. In this paper we discuss how some of these hard problems can be solved with techniques from other math fields than the problems own discipline. First we solve some combinatorial problems using the knowledge that a maximum amount of vectors in a linearly independent set over a subset of a vector space 𝔽<sup>𝑛</sup> over a field 𝔽 is 𝑛. Then we discuss and explain Z.Dvir's famous proof regarding Kakeya sets over finite fields. He is able to establish a lower bound of the size of Kakeya sets using polynomials.</p>

Note spelling error in original:
mc='Z.Dvir' c='Z. Dvir'
----------------------------------------------------------------------
In diva2:704889 
abstract is: 
<p>In this thesis a corporate bond valuation model based on Dick-Nielsen, Feldhütter, and Lando (2011) and Chen, Lesmond, and Wei (2007) is examined. The aim is for the model to price corporate bond spreads and in particular capture the price effects of liquidity as well as credit risk. The valuation model is based on linear regression and is conducted on the Swedish market with data provided by Handelsbanken. Two measures of liquidity are analyzed: the bid-ask spread and the zero-trading days. The investigation shows that the bid-ask spread outperforms the zero-trading days in both significance and robustness. The valuation model with the bid-ask spread explains 59% of the cross-sectional variation and has a standard error of 56 bps in its pricing predictions of corporate spreads. A reduced version of the valuation model is also developed to address simplicity and target a larger group of users. The reduced model is shown to maintain a large proportion of the explanation power while including fewer and simpler variables.</p><p> </p>

corrected abstract:
<p>In this thesis a corporate bond valuation model based on Dick-Nielsen, Feldhütter, and Lando (2011) and Chen, Lesmond, and Wei (2007) is examined. The aim is for the model to price corporate bond spreads and in particular capture the price effects of liquidity as well as credit risk. The valuation model is based on linear regression and is conducted on the Swedish market with data provided by <em lang="sv">Handelsbanken</em>. Two measures of liquidity are analyzed: the bid-ask spread and the zero-trading days. The investigation shows that the bid-ask spread outperforms the zero-trading days in both significance and robustness. The valuation model with the bid-ask spread explains 59% of the cross-sectional variation and has a standard error of 56 bps in its pricing predictions of corporate spreads. A reduced version of the valuation model is also developed to address simplicity and target a larger group of users. The reduced model is shown to maintain a large proportion of the explanation power while including fewer and simpler variables.</p>

Note - only change to remove the empty paragraph and added italics
----------------------------------------------------------------------
In diva2:1442642 
abstract is: 
<p>In recent decades, sound reproduction research has shown great progress. Audio reproduction changed from a simple two-channel stereo system to a surround sound system and three dimensional sound system. The use of height and angle related speakers was introduced with the aim of improving sound reproduction. With the listener’s experience in focus, this developed reproductive systems were examined. This bachelor’s thesis in sound and vibration compiles three articles on sound quality with regard to different sound reproduction media, with the aim of getting a better understanding of the various psychoacoustical and technical parameters that influence the sound experience. Various subjective evaluation methods are presented, including the PREQUEL method (PerceptualReproduction Quality Evaluation for Loudspeakers), MUSHRA method (MUlti Stimulus test withHidden Reference and Anchor) and OLE method (Overall Listening Experience). The results show how a surround sound system and a three-dimensional sound system provide a more appreciated sound experience and how this experience can be enhanced with a height-related sound system .</p><p> </p>
mc='withHidden' c='with Hidden'
mc='PerceptualReproduction' c='Perceptual Reproduction'

corrected abstract:
<p>In recent decades, sound reproduction research has shown great progress. Audio reproduction changed from a simple two-channel stereo system to a surround sound system and three dimensional sound system. The use of height and angle related speakers was introduced with the aim of improving sound reproduction. With the listener’s experience in focus, this developed reproductive systems were examined.</p><p>This bachelor’s thesis in sound and vibration compiles three articles on sound quality with regard to different sound reproduction media, with the aim of getting a better understanding of the various psychoacoustical and technical parameters that influence the sound experience. Various subjective evaluation methods are presented, including the PREQUEL method (perceptual reproduction quality evaluation for loudspeakers), MUSHRA method (MUlti Stimulus test with Hidden Reference and Anchor) and OLE method (overall listening experience)). The results show how a surround sound system and a three-dimensional sound system provide a more appreciated sound experience and how this experience can be enhanced with a height-related sound system .</p>

Note that there is a space before the final period, several of the spelledout versions of acronyms are in lower case. Also removed the empty paragraph.
----------------------------------------------------------------------
In diva2:415054 
abstract is: 
<p> </p>
<p>Abstract</p>
<p>Thermally-controlled exchange coupling between two strong ferromagnetic (FM) layers separated by a weak FM spacer has promising application in current-driven spintronic devices. In this thesis magnetic property of Ni xFe1-x thin films in the Invar composition range are studied. The focus is on investigating the magnetic transition from ferromagnetic to paramagnetic state of the material and finding the composition with lowest Curie point.</p>
<p>The fabrication process of NixFe1-x  thin films having variable Ni concentration using multi magnetron sputtering is described in the first part of the thesis. In the second part of the thesis the magnetic characterization technique and measurement results are discussed. The Invar effect is observed at approximately 30% Ni content, where the films exhibit a pronounced minimum in the Curie temperature versus Ni concentration</p>

corrected abstract:
<p>Thermally-controlled exchange coupling between two strong ferromagnetic (FM) layers separated by a weak FM spacer has promising application in current-driven spintronic devices. In this thesis magnetic property of Ni<sub>x</sub>Fe<sub>1-x</sub> thin films in the Invar composition range are studied. The focus is on investigating the magnetic transition from ferromagnetic to paramagnetic state of the material and finding the composition with lowest Curie point. The fabrication process of Ni<sub>x</sub>Fe<sub>1-x</sub> thin films having variable Ni concentration using multi magnetron sputtering is described in the first part of the thesis. In the second part of the thesis the magnetic characterization technique and measurement results are discussed. The Invar effect is observed at approximately 30% Ni content, where the films exhibit a pronounced minimum in the Curie temperature versus Ni concentration.</p>
----------------------------------------------------------------------
In diva2:1335210 
abstract is: 
<p>G protein-coupled receptors are one of the biggest targets for pharmaceutical drugs today. The aim with this project was to use different machine learning algorithms to classify the protein into different functional states and compare the results obtained by different algorithms. Both the supervised and unsupervised methods implemented in this project identified similar regions of the protein as important for classification of their functional state. More specifically, the supervised methods Random Forest and Multilayer Perceptron were able to make predictions of the functional state of a protein with great accuracy. The methods investigated will be useful for designing and analyzing the molecular dynamics simulations of GPCRs. Ultimately this will further our understanding of the drug binding mechanics, a critical step for the rational development of new drugs to treat various diseases.</p><p> </p>

corrected abstract:
<p>G protein-coupled receptors are one of the biggest targets for pharmaceutical drugs today. The aim with this project was to use different machine learning algorithms to classify the protein into different functional states and compare the results obtained by different algorithms. Both the supervised and unsupervised methods implemented in this project identified similar regions of the protein as important for classification of their functional state. More specifically, the supervised methods Random Forest and Multilayer Perceptron were able to make predictions of the functional state of a protein with great accuracy. The methods investigated will be useful for designing and analyzing the molecular dynamics simulations of GPCRs. Ultimately this will further our understanding of the drug binding mechanics, a critical step for the rational development of new drugs to treat various diseases.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1879658 
abstract is: 
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>

corrected abstract:
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>
In diva2:1879658 
abstract is: 
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>

corrected abstract:
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1363231 
abstract is: 
<p>In the truck industry, it is crustal to test components against fatigue to make sure that the trucks stand up to the high demands on durability. Today’s testing methods have some disadvantages; it is quite a time-consuming process, but more important, similar tested components cannot easily be compared due to the load spread the components are subjected to. It is therefore desirable to test the components in a standardized way. One way to do this is to use a synthetic signal which is a large number of unique truck measurements combined. The synthetic signal only contains information of the frame’s vibration and not any components. The purpose of this project was to create a model that uses the synthetic signal to describe the motion of components.</p><p> </p><p>Two approaches were used, the first was to base the model on previous measurements, the second one was to base the model on analytical equations. These models were experimentally tested in a 4 channel shake rig, and a silencer was the component chosen to be tested. For the model based on measurements, the load was shown to have a large spread which was hard to control due to the spread in the measurements. The second model was easier to control where the damping factor can be chosen and varied. A promising model was the analytical model using 10% damping applied to the synthetic signal, it covers most measurements without overestimate the load of the component. However, the model was only developed for the silencer acceleration in the z-direction, and it is recommended to develop it for the x-direction as well. The method used in this project could also be used to develop models for other components.</p>

corrected abstract:
<p>In the truck industry, it is crustal to test components against fatigue to make sure that the trucks stand up to the high demands on durability. Today’s testing methods have some disadvantages; it is quite a time-consuming process, but more important, similar tested components cannot easily be compared due to the load spread the components are subjected to. It is therefore desirable to test the components in a standardized way. One way to do this is to use a synthetic signal which is a large number of unique truck measurements combined. The synthetic signal only contains information of the frame’s vibration and not any components. The purpose of this project was to create a model that uses the synthetic signal to describe the motion of components.</p><p>Two approaches were used, the first was to base the model on previous measurements, the second one was to base the model on analytical equations. These models were experimentally tested in a 4 channel shake rig, and a silencer was the component chosen to be tested. For the model based on measurements, the load was shown to have a large spread which was hard to control due to the spread in the measurements. The second model was easier to control where the damping factor can be chosen and varied. A promising model was the analytical model using 10% damping applied to the synthetic signal, it covers most measurements without overestimate the load of the component. However, the model was only developed for the silencer acceleration in the z-direction, and it is recommended to develop it for the x-direction as well. The method used in this project could also be used to develop models for other components.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1137873 
abstract is: 
<p>This report was carried out at Gleechi, a Swedish start-up company working with implementing hand use in Virtual Reality. The thesis presents hand models used to generate natural looking grasping motions. One model were made for each of the thirty-three different grasp types in Feix’s The GRASP Taxonomy.</p><p>Each model is based on functional principal components analysis which was performed on data containing recorded joint angles of grasping motions from real subjects. Prior to functional principal components analysis, dynamic time warping was performed on the recorded joint angles in order to put them on the same length and make them suitable for statistical analysis. The last step of the analysis was to project the data onto the functional principal components and train Gaussian mixture models on the weights obtained. New grasping motions could be generated by sampling weights from the Gaussian mixture models and attaching them to the functional principal components.</p><p>The generated grasps were in general satisfying, but all of the thirty-three grasps were not distinguishable from each other. This was most likely caused by the fact that each degree of freedom was modelled in isolation from each other, so that no correlation between them was included in the model.</p><p> </p>

corrected abstract:
<p>This report was carried out at Gleechi, a Swedish start-up company working with implementing hand use in Virtual Reality. The thesis presents hand models used to generate natural looking grasping motions. One model were made for each of the thirty-three different grasp types in Feix’s <em>The GRASP Taxonomy</em>.</p><p>Each model is based on functional principal components analysis which was performed on data containing recorded joint angles of grasping motions from real subjects. Prior to functional principal components analysis, dynamic time warping was performed on the recorded joint angles in order to put them on the same length and make them suitable for statistical analysis. The last step of the analysis was to project the data onto the functional principal components and train Gaussian mixture models on the weights obtained. New grasping motions could be generated by sampling weights from the Gaussian mixture models and attaching them to the functional principal components.</p><p>The generated grasps were in general satisfying, but all of the thirty-three grasps were not distinguishable from each other. This was most likely caused by the fact that each degree of freedom was modelled in isolation from each other, so that no correlation between them was included in the model.</p>

Note - only change to remove the empty paragraph and adding italics
----------------------------------------------------------------------
In diva2:738788 - - unnessary period at end of title:
"Modellering av turboladdarens surge-beteende för fordonsindustrin."
==>
"Modellering av turboladdarens surge-beteende för fordonsindustrin"

abstract is: 
<p>The turbocharger was originally designed and used to boost engine power of vehicles. Nowadays, when the demand for low carbon vehicles is increasing rapidly, a new application for the turbocharger has been found by using it to downsizing the engine. Using experimental as well as theoretical simulation models we can estimate the outcome and behavior of the compressor in an extended work range. An aspect that has a substantial effect on the turbocharger and engine is the surge line. Surge is a problematic stage where the compressor creates unwanted behavior which could damage the turbocharger as well as the engine. The surge line is a line where the transition to surge occurs. By changing the surge line, through simulations and calculations surge can be avoided, you can optimize and improve the turbocharger. This report mainly discusses and investigates the possibility to use fast one-dimensional simulation software instead of full scaled laboratories in the automotive industry, and estimate the work range of the given compressor.</p><p> </p>

corrected abstract:
<p>The turbocharger was originally designed and used to boost engine power of vehicles. Nowadays, when the demand for low carbon vehicles is increasing rapidly, a new application for the turbocharger has been found by using it to downsizing the engine. Using experimental as well as theoretical simulation models we can estimate the outcome and behavior of the compressor in an extended work range. An aspect that has a substantial effect on the turbocharger and engine is the surge line. Surge is a problematic stage where the compressor creates unwanted behavior which could damage the turbocharger as well as the engine. The surge line is a line where the transition to surge occurs. By changing the surge line, through simulations and calculations surge can be avoided, you can optimize and improve the turbocharger. This report mainly discusses and investigates the possibility to use fast one-dimensional simulation software instead of full scaled laboratories in the automotive industry, and estimate the work range of the given compressor.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:716518 
abstract is: 
<p>Cancer is a common cause of death worldwide and radiotherapy is one of the treatments used. Since treatment planning is a time consuming matter for the radiation therapist, a way to decrease the time spent finding the plan would be an improvement. This can be achieved by precalculating a number of optimal plans and then choosing among these in real-time.</p><p>In this thesis a dual algorithm for approximation of the Pareto optimal plans suggested by Bokrantz and Forsgren, was adapted to the parameters of the Leksell Gamma Knife®. A Graphical User Interface was also created, based on the navigation tool described by Monz et al to enable choosing among the pre-calculated dose plans.</p><p>The computational time of the algorithm was investigated and the dimensionality of the solutions and Pareto optimal points were looked at to see if it might be possible to reduce the number of dimensions to speed up computations.</p><p>Although no certain conclusions can be drawn about dimensionality reduction, I found no reason to rule that possibility out. It was also confirmed that there is reason to keep the number of objectives low to get a better approximation.</p><p> </p>

corrected abstract:
<p>Cancer is a common cause of death worldwide and radiotherapy is one of the treatments used. Since treatment planning is a time consuming matter for the radiation therapist, a way to decrease the time spent finding the plan would be an improvement. This can be achieved by precalculating a number of optimal plans and then choosing among these in real-time.</p><p>In this thesis a dual algorithm for approximation of the Pareto optimal plans suggested by Bokrantz and Forsgren, was adapted to the parameters of the Leksell Gamma Knife®. A Graphical User Interface was also created, based on the navigation tool described by Monz et al to enable choosing among the pre-calculated dose plans.</p><p>The computational time of the algorithm was investigated and the dimensionality of the solutions and Pareto optimal points were looked at to see if it might be possible to reduce the number of dimensions to speed up computations.</p><p>Although no certain conclusions can be drawn about dimensionality reduction, I found no reason to rule that possibility out. It was also confirmed that there is reason to keep the number of objectives low to get a better approximation.</p>

Note - only change to remove the empty paragraph
Note spelling error:
"et al" should be "et al."
----------------------------------------------------------------------
In diva2:1848437 
abstract is: 
<p>Turbulent mixing of single or multi-phase flows is common in diverse research fields, and direct numerical simulation is useful for understanding such phenomenon. To study the scaler transport in turbulence, the computational grids must resolve the Batchelor scale, which is smaller than the Kolmogorov scale by a factor of √Sc. This would commonly lead to the over-resolving of the Navier-Stokes equation, making DNS even more expensive. To overcome this issue, this thesis presents a method to reduce the computational cost in scalar turbulent flows, by using a coarse grid for the velocity and a fine grid for the scalar. A divergence-free Hermite interpolation is implemented for the velocity field to ensure the continuity equation is fulfilled on the fine grid. The interpolation scheme is validated by cases of Arnold–Beltrami–Childress flow and Taylor–Green vortex. For the active scaler, the integration schemes are included for fine-to-coarse integration. The multi-resolution method is parallelised by MPI and integrated into the open-source multiphase flow solver FluTAS. For the diffuse interface modelling in FluTAS, the indicator function is updated on the fine grid, and the surface tension force is then calculated and extrapolated back to the coarse grid for momentum equation update. The method is evaluated against the single-resolution method at different refinement factors.</p><p> </p>

corrected abstract:
<p>Turbulent mixing in multi-phase flows is prevalent across various research domains, and direct numerical simulation (DNS) is often used for understanding such phenomena. However, DNS of turbulent scalar transport entails resolving the Batchelor scale, which is typically smaller than the Kolmogorov scale by a factor of 𝑆𝑐<sup>−1/2</sup>. This can lead to increased number of grid points and the over-resolve of NaiverStokes equations, making the DNS even more expensive. To overcome this issue, this thesis presents a method to reduce the computational cost, by using a coarse grid for solving the velocity and pressure, and a fine grid for solving the scalar. A divergencefree interpolation scheme is implemented for velocity to ensure the continuity equation is fulfilled on the fine grid. The interpolation scheme is validated by Arnold–Beltrami– Childress flow and Taylor–Green vortex. For active scalars, a scheme of weighted averaging facilitates the fine-to-coarse integration. The multi-resolution method is parallelised by MPI and integrated into the open-source code FluTAS for multiphase flow simulation. For the diffuse-interface modelling in FluTAS, the phase-field variable and surface tension force are computed on the fine grid and integrated to the coarse grid to couple with the momentum equation. The presented method is evaluated against the single-resolution method using the rising bubble benchmark.</p>

Note - major differences between DiVA and original abstract and removed empty paragraph
----------------------------------------------------------------------
In diva2:1776757 
abstract is: 
<p>This paper aims at presenting the necessary tools to prove that a scheme of finite type over Z exhibits the same singularities as those which occur on a Grassmann variety. First, basic theory regarding the combinatorial objects matroids is presented. Some important examples for the remainder of the paper are given, which also serve to aid the reader in intuition and understanding of matroids. Basic algebraic geometry is presented, and the building blocks affine varieties, projective varieties and general varieties are introduced. These object are generalised in the following subsection as affine schemes and schemes, which are the central object of study in modern algebraic geometry. Important results from the theory of algebraic groups are shown in order to better understand the formulation and proof of the Gelfand–MacPherson theorem, which in turn is utilised, together with Mnëv’s universality theorem, to prove the main result of the paper.</p><p> </p>

corrected abstract:
<p>This paper aims at presenting the necessary tools to prove that a scheme of finite type over ℤ exhibits the same singularities as those which occur on a Grassmann variety. First, basic theory regarding the combinatorial objects matroids is presented. Some important examples for the remainder of the paper are given, which also serve to aid the reader in intuition and understanding of matroids. Basic algebraic geometry is presented, and the building blocks affine varieties, projective varieties and general varieties are introduced. These object are generalised in the following subsection as affine schemes and schemes, which are the central object of study in modern algebraic geometry. Important results from the theory of algebraic groups are shown in order to better understand the formulation and proof of the Gelfand–MacPherson theorem, which in turn is utilised, together with Mnëv’s universality theorem, to prove the main result of the paper.</p>

Note - only change to remove the empty paragraph and replacement of "Z" by "ℤ"
----------------------------------------------------------------------
In diva2:1436960 
abstract is: 
<p>The Aviation industry is important to the European economy and development, therefore a study of the sensitivity of the European flight network is interesting. If clusters exist within the network, that could indicate possible vulnerabilities or bottlenecks, since that would represent a group of airports poorly connected to other parts of the network. In this paper a cluster analysis using spectral clustering is performed with flight data from 34 different European countries. The report also looks at how to implement the spectral clustering algorithm for large data sets. After performing the spectral clustering it appears as if the European flight network is not clustered, and thus does not appear to be sensitive.</p><p> </p>

corrected abstract:
<p>The Aviation industry is important to the European economy and development, therefore a study of the sensitivity of the European flight network is interesting. If clusters exist within the network, that could indicate possible vulnerabilities or bottlenecks, since that would represent a group of airports poorly connected to other parts of the network. In this paper a cluster analysis using spectral clustering is performed with flight data from 34 different European countries. The report also looks at how to implement the spectral clustering algorithm for large data sets. After performing the spectral clustering it appears as if the European flight network is not clustered, and thus does not appear to be sensitive.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:892288 
abstract is: 
<p>Particle suspensions occur in many situations in nature and industry. In this master’s thesis, the motion of a single rigid spheroidal particle immersed in Stokes flow is studied numerically using a boundary integral method and a new specialized quadrature method known as quadrature by expansion (QBX). This method allows the spheroid to be massless or inertial, and placed in any kind of underlying Stokesian flow.</p><p> </p><p>A parameter study of the QBX method is presented, together with validation cases for spheroids in linear shear flow and quadratic flow. The QBX method is able to compute the force and torque on the spheroid as well as the resulting rigid body motion with small errors in a short time, typically less than one second per time step on a regular desktop computer. Novel results are presented for the motion of an inertial spheroid in quadratic flow, where in contrast to linear shear flow the shear rate is not constant. It is found that particle inertia induces a translational drift towards regions in the fluid with higher shear rate.</p>

corrected abstract:
<p>Particle suspensions occur in many situations in nature and industry. In this master’s thesis, the motion of a single rigid spheroidal particle immersed in Stokes flow is studied numerically using a boundary integral method and a new specialized quadrature method known as quadrature by expansion (QBX). This method allows the spheroid to be massless or inertial, and placed in any kind of underlying Stokesian flow.</p><p>A parameter study of the QBX method is presented, together with validation cases for spheroids in linear shear flow and quadratic flow. The QBX method is able to compute the force and torque on the spheroid as well as the resulting rigid body motion with small errors in a short time, typically less than one second per time step on a regular desktop computer. Novel results are presented for the motion of an inertial spheroid in quadratic flow, where in contrast to linear shear flow the shear rate is not constant. It is found that particle inertia induces a translational drift towards regions in the fluid with higher shear rate.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1341297 
Note: no full text in DiVA

abstract is: 
<p>Stochastic gradient descent is an algorithm used in optimization. Learning rate plays a central role in the stochastic gradient descent algorithm. If the selected learning rate lies within the appropriate interval, it affects the algorithm's convergence rate towards local minimum as well as its accuracy. In this work the stochastic gradient descent algorithm was used to treat two simple optimization problems. Firstly, a series of numerical experiments for a plethora of learning rate were performed where the behavior of the algorithm was studied. Secondly, the training of a neural network using the stochastic gradient descent was experimentally studied. The effect of learning rate values was tested as well as the neural network’s performance by varying parameters such as the number of nodes, the activation function and combinations of the above.</p><p> </p>

corrected abstract:
<p>Stochastic gradient descent is an algorithm used in optimization. Learning rate plays a central role in the stochastic gradient descent algorithm. If the selected learning rate lies within the appropriate interval, it affects the algorithm's convergence rate towards local minimum as well as its accuracy. In this work the stochastic gradient descent algorithm was used to treat two simple optimization problems. Firstly, a series of numerical experiments for a plethora of learning rate were performed where the behavior of the algorithm was studied. Secondly, the training of a neural network using the stochastic gradient descent was experimentally studied. The effect of learning rate values was tested as well as the neural network’s performance by varying parameters such as the number of nodes, the activation function and combinations of the above.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1877780 
abstract is: 
<p>This bachelor thesis aims to give an introduction to various Hopf algebras that arise in combinatorics, with a view towards symmetric functions. We begin by covering the algebraic background needed to define Hopf algebras, including a discussion of the algebra-coalgebra duality. Takeuchi's formula for the antipode is stated and proved. It is then generalised to incidence Hopf algebras. This is followed by a discussion of the Hopf algebra of symmetric functions. It is shown that the Hopf algebra of symmetric functions is self-dual. We also show that the graded dual of the Hopf algebra of quasisymmetric functions is the Hopf algebra of non-commutative symmetric functions. Relations to the Hopf algebra of symmetric functions in non-commuting variables are emphasised. Finally, we state and prove the Aguiar-Bergeron-Sottile universality theorem.</p><p> </p>

corrected abstract:
<p>This bachelor thesis aims to give an introduction to various Hopf algebras that arise in combinatorics, with a view towards symmetric functions. We begin by covering the algebraic background needed to define Hopf algebras, including a discussion of the algebra-coalgebra duality. Takeuchi's formula for the antipode is stated and proved. It is then generalised to incidence Hopf algebras. This is followed by a discussion of the Hopf algebra of symmetric functions. It is shown that the Hopf algebra of symmetric functions is self-dual. We also show that the graded dual of the Hopf algebra of quasisymmetric functions is the Hopf algebra of non-commutative symmetric functions. Relations to the Hopf algebra of symmetric functions in non-commuting variables are emphasised. Finally, we state and prove the Aguiar-Bergeron-Sottile universality theorem.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:740516 - unnessary period at end of title:
"On the 1932 Discovery of the Positron."
==>
"On the 1932 Discovery of the Positron."

abstract is: 
<p>An experiment on Cosmic rays performed by Carl D Anderson led to the accidental discovery of the positron in 1932. The discovery was a turning point in particle physics which led to numerous other theories and has been discussed by scientists all over the world. Anderson had photographed a 63 MeV, upward moving electron. The possible origin of such a positron has never before been discussed and is what this report will aim to explain. The report will include some evidence that the particle is in fact a positron as well as a discussion of the four main theories whose possibility and probability will be discussed; pion decay, muon decay, magnetic field bending and pair production. The report will also cover a historical background for Anderson’s experiment, as well as a theoretical background needed for the theories of the origin. The probability of discovering a positron with any of the theorized origins is extremely low and for some theories, even impossible.</p><p> </p>

corrected abstract:
<p>An experiment on Cosmic rays performed by Carl D Anderson led to the accidental discovery of the positron in 1932. The discovery was a turning point in particle physics which led to numerous other theories and has been discussed by scientists all over the world. Anderson had photographed a 63 MeV, upward moving electron. The possible origin of such a positron has never before been discussed and is what this report will aim to explain. The report will include some evidence that the particle is in fact a positron as well as a discussion of the four main theories whose possibility and probability will be discussed; pion decay, muon decay, magnetic field bending and pair production. The report will also cover a historical background for Anderson’s experiment, as well as a theoretical background needed for the theories of the origin. The probability of discovering a positron with any of the theorized origins is extremely low and for some theories, even impossible.</p>

Note - only change to remove the empty paragraph
Note spelling error: there should be a period after "D." as it is an initial - this is correct in the Swedish abstract
----------------------------------------------------------------------
In diva2:1779459 
abstract is: 
<p>The focus of this thesis is to find, visualize and analyze the optimal flow of autonomous electric vehicles with charge constraints in urban traffic with respect to energy consumption. The traffic has been formulated as a static multi-commodity network flow problem, for which two different models have been implemented to handle the charge constraints. The first model uses a recursive algorithm to find the optimal solution fulfilling the charge constraints, while the second model discretizes the commodities’ battery to predetermined battery levels. An implementation of both methods is provided through simulations on scenarios of three different sizes. The results show that both methods are capable of representing the traffic flow with charge constraints, with limitations given by the size of the problem. In particular, the recursive model has the advantage of considering the charge as a continuous quantity. On the other hand the discretization of battery levels allows to handle charge constraint setups with higher complexity, that is when longer detours are needed to fulfill the charge constraints.</p><p> </p>

corrected abstract:
<p>The focus of this thesis is to find, visualize and analyze the optimal flow of autonomous electric vehicles with charge constraints in urban traffic with respect to energy consumption. The traffic has been formulated as a static multi-commodity network flow problem, for which two different models have been implemented to handle the charge constraints. The first model uses a recursive algorithm to find the optimal solution fulfilling the charge constraints, while the second model discretizes the commodities’ battery to predetermined battery levels. An implementation of both methods is provided through simulations on scenarios of three different sizes. The results show that both methods are capable of representing the traffic flow with charge constraints, with limitations given by the size of the problem. In particular, the recursive model has the advantage of considering the charge as a continuous quantity. On the other hand the discretization of battery levels allows to handle charge constraint setups with higher complexity, that is when longer detours are needed to fulfill the charge constraints.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1352113 
abstract is: 
<p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p> </p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome beforehand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>
mc='forehand' c='fore hand'

partal corrected: diva2:1352113: <p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p> </p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome before hand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>

corrected abstract:
<p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome beforehand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:904606 
abstract is: 
<p>Today’s society keeps trying to improve efficiency in production and general reliability in systems. This is true for all types of companies that are driven to use optimization techniques in their work. One major field impacted by these studies is transport. Railway companies set the example and make optimization a priority in order to handle train flows the best way possible. That is why the French National Railway Company imagined an informatic tool to decide how trains are to be parked optimally. The thesis exposed in this report has been proposed for treating the modeling and programming parts of the project.</p><p>Modeling the station has been done in association with railway experts who bring their knowledge about the station infrastructure. They are also a reference for determining the constraints a train must obey when arriving at the station, parking and leaving. Only useful aspects of the station must be taken into account. That is why a compromise has been chosen in the level of detail modeled.</p><p>Then, a mathematical formulation of the problem has been proposed. The decision variables are for each train its arrival path, parking track and departure path. There are lots of possibilities and the choice can be difficult. The constraints are quite numerous as well. Thus, it has been decided to divide the constraints in two parts: hard and soft constraints. The general idea is to find a solution that obeys all hard constraints and minimizes the number of soft constraints not respected. </p><p>The project consisted also in choosing a language to code the model and a solver. The mathematical model has been written with the modeling language OPL that calls the solver Cplex. This software is one of the best to solve large scale problems that may arise for complex stations. Since the problem was a Mixed Integer Programming; the solver uses the Branch &amp; Cut algorithm that is a combination between different techniques. It consists in applying the Branch &amp; Bound algorithm while cutting the feasibility region. Heuristics techniques are also used by the algorithm in order to provide quickly a feasible solution. After running the program, we get a solution which is optimal according to the approach discussed earlier on. More precisely, it would determine for each train an arrival path, a parking track and a departure path.</p><p> </p><p>The program developed during this thesis is proposed to the stations. It would help them in deciding how trains are parked on one day. This decision is made in a conception phase happening several months before the actual day treated. Several major stations have tested the optimization tool. Thanks to a regularity coefficient, they are able to judge the quality of the solution. Since this score has been increased, the solution is judged as rather good. Furthermore, they will use it for the year 2016. A significant advantage is also the time for making the decision. When the modeling phase is done, a few minutes are enough to produce a solution for a given train set, whereas it can take several weeks of work by hand.</p>

corrected abstract:
<p>Today’s society keeps trying to improve efficiency in production and general reliability in systems. This is true for all types of companies that are driven to use optimization techniques in their work. One major field impacted by these studies is transport. Railway companies set the example and make optimization a priority in order to handle train flows the best way possible. That is why the French National Railway Company imagined an informatic tool to decide how trains are to be parked optimally. The thesis exposed in this report has been proposed for treating the modeling and programming parts of the project.</p><p>Modeling the station has been done in association with railway experts who bring their knowledge about the station infrastructure. They are also a reference for determining the constraints a train must obey when arriving at the station, parking and leaving. Only useful aspects of the station must be taken into account. That is why a compromise has been chosen in the level of detail modeled.</p><p>Then, a mathematical formulation of the problem has been proposed. The decision variables are for each train its arrival path, parking track and departure path. There are lots of possibilities and the choice can be difficult. The constraints are quite numerous as well. Thus, it has been decided to divide the constraints in two parts: hard and soft constraints. The general idea is to find a solution that obeys all hard constraints and minimizes the number of soft constraints not respected.</p><p>The project consisted also in choosing a language to code the model and a solver. The mathematical model has been written with the modeling language OPL that calls the solver Cplex. This software is one of the best to solve large scale problems that may arise for complex stations. Since the problem was a Mixed Integer Programming; the solver uses the Branch &amp; Cut algorithm that is a combination between different techniques. It consists in applying the Branch &amp; Bound algorithm while cutting the feasibility region. Heuristics techniques are also used by the algorithm in order to provide quickly a feasible solution. After running the program, we get a solution which is optimal according to the approach discussed earlier on. More precisely, it would determine for each train an arrival path, a parking track and a departure path.</p><p>The program developed during this thesis is proposed to the stations. It would help them in deciding how trains are parked on one day. This decision is made in a conception phase happening several months before the actual day treated. Several major stations have tested the optimization tool. Thanks to a regularity coefficient, they are able to judge the quality of the solution. Since this score has been increased, the solution is judged as rather good. Furthermore, they will use it for the year 2016. A significant advantage is also the time for making the decision. When the modeling phase is done, a few minutes are enough to produce a solution for a given train set, whereas it can take several weeks of work by hand.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1360595
Note: no full text in DiVA

abstract is: 
<p>Investigations on the Circular Restricted 3- Body Problem (CR3BP) and the motion about the Lagrangian points are not recent. Several past (upcoming) missions have used (plan to use) its dynamics. The existence of specific periodic orbits and their associated invariant manifolds is one property of the CR3BP that raises the interest. These periodic orbits are interesting for their great observation properties, eclipse avoidance, communication’s continuity with the Earth, etc. However, to reach them, optimized transfer trajectories have to be found. A numerical tool is developed to construct firstly these orbits, before using them as input parameters for the invariant manifolds. Indeed, when a spacecraft is inserted into one of these manifolds, it shall naturally reach the orbit without any additional cost. This numerical computation provides manifolds’s insertion points, which are used in return by a Nonlinear Programming (NLP) tool to eventually find the optimized trajectory. Families of periodic orbits and manifolds, together with optimized transfer trajectories, have been successfully computed, with a focus on the Halo Orbits of the Earth-Moon system. Some members of this family, the Near-Rectilinear Halo Orbits (NRHOs), are of a great interest both for their geometry characteristics (close approach of the secondary body) and stability properties. However, in the Earth-Moon system, the associated manifolds do not have points relatively close from the Earth. The thesis work hence does not ensure that using manifolds as transfer arcs is beneficial, compared to a direct transfer. Besides, the Time-Of-Flight (TOF) is significantly larger. Transfer strategies making use of the CR3BP dynamics still are interesting, radically different from the usual trajectories and offering a larger number of opportunities. They may be less expansive, and could particularly be used for uncrewed space missions.</p><p> </p>

corrected abstract:
<p>Investigations on the Circular Restricted 3- Body Problem (CR3BP) and the motion about the Lagrangian points are not recent. Several past (upcoming) missions have used (plan to use) its dynamics. The existence of specific periodic orbits and their associated invariant manifolds is one property of the CR3BP that raises the interest. These periodic orbits are interesting for their great observation properties, eclipse avoidance, communication’s continuity with the Earth, etc. However, to reach them, optimized transfer trajectories have to be found. A numerical tool is developed to construct firstly these orbits, before using them as input parameters for the invariant manifolds. Indeed, when a spacecraft is inserted into one of these manifolds, it shall naturally reach the orbit without any additional cost. This numerical computation provides manifolds’s insertion points, which are used in return by a Nonlinear Programming (NLP) tool to eventually find the optimized trajectory. Families of periodic orbits and manifolds, together with optimized transfer trajectories, have been successfully computed, with a focus on the Halo Orbits of the Earth-Moon system. Some members of this family, the Near-Rectilinear Halo Orbits (NRHOs), are of a great interest both for their geometry characteristics (close approach of the secondary body) and stability properties. However, in the Earth-Moon system, the associated manifolds do not have points relatively close from the Earth. The thesis work hence does not ensure that using manifolds as transfer arcs is beneficial, compared to a direct transfer. Besides, the Time-Of-Flight (TOF) is significantly larger. Transfer strategies making use of the CR3BP dynamics still are interesting, radically different from the usual trajectories and offering a larger number of opportunities. They may be less expansive, and could particularly be used for uncrewed space missions.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1198117 
abstract is: 
<p>Climate change is an evermore urging existential treat to the human enterprise. Mean temperature and greenhouse gas emissions have in-creased exponentially since the industrial revolution. But solutions are also mushrooming with exponential pace. Renewable energy technologies, such as wind and solar power, are deployed like never before and their costs have decreased signiﬁcantly. In order to allow for further transformation of the energy system these technologies must be reﬁned and optimised. In wind energy one important ﬁeld with high potential of reﬁnement is aerodynamics. The aerodynamics of wind turbines constitutes one challenging research frontier in aerodynamics today.</p><p> </p><p>In this study, a novel approach for calculating wind turbine ﬂow is developed. The approach is based on the partially parabolic Navier-Stokes equations, which can be solved computationally with higher eﬃciency as compared to the fully elliptic version. The modelling of wind turbine thrust is done using actuator-disk theory and the torque is modelled by application of the Joukowsky rotor. A validation of the developed model and force implementation is conducted using four diﬀerent validation cases.</p><p> </p><p>In order to provide value for industrial wind energy projects, the model must be extended to account for turbulence (and terrain in case of onshore projects). Possible candidates for turbulence modelling are parabolic k-ε and explicit Reynolds stress turbulence models. The terrain could possibly be incorporated consistently with the used projection method by altering the ﬁnite diﬀerence grid layout.</p>

corrected abstract:
<p>Climate change is an evermore urging existential treat to the human enterprise. Mean temperature and greenhouse gas emissions have increased exponentially since the industrial revolution. But solutions are also mushrooming with exponential pace. Renewable energy technologies, such as wind and solar power, are deployed like never before and their costs have decreased significantly. In order to allow for further transformation of the energy system these technologies must be refined and optimised. In wind energy one important field with high potential of refinement is aerodynamics. The aerodynamics of wind turbines constitutes one challenging research frontier in aerodynamics today.</p><p>In this study, a novel approach for calculating wind turbine flow is developed. The approach is based on the partially parabolic Navier-Stokes equations, which can be solved computationally with higher efficiency as compared to the fully elliptic version. The modelling of wind turbine thrust is done using actuator-disk theory and the torque is modelled by application of the Joukowsky rotor. A validation of the developed model and force implementation is conducted using four different validation cases.</p><p>In order to provide value for industrial wind energy projects, the model must be extended to account for turbulence (and terrain in case of onshore projects). Possible candidates for turbulence modelling are parabolic 𝑘-<em>ε</em> and explicit Reynolds stress turbulence models. The terrain could possibly be incorporated consistently with the used projection method by altering the finite difference grid layout.</p>

Note - only changes to remove the empty paragraphs, removed an unnecessary hyphen, and replaced "k" with "𝑘" and added italics for the omega.
----------------------------------------------------------------------
In diva2:560786 
abstract is: 
<p>Several models for predicting future customer profitability early into customer life-cycles in the property and casualty business are constructed and studied. The objective is to model risk at a customer level with input data available early into a private consumer’s lifespan. Two retained models, one using Generalized Linear Model another using a multilayer perceptron, a special form of Artificial Neural Network are evaluated using actual data. Numerical results show that differentiation on estimated future risk is most effective for customers with highest claim frequencies.</p><p> </p>

corrected abstract:
<p>Several models for predicting future customer profitability early into customer life-cycles in the property and casualty business are constructed and studied. The objective is to model risk at a customer level with input data available early into a private consumer’s lifespan. Two retained models, one using Generalized Linear Model another using a multilayer perceptron, a special form of Artificial Neural Network are evaluated using actual data. Numerical results show that differentiation on estimated future risk is most effective for customers with highest claim frequencies.</p>
----------------------------------------------------------------------
In diva2:1878726 
abstract is: 
<p>This project applied statistical inference methods to historical data of mixed martial arts (MMA) matches from the Ultimate Fighting Championship (UFC). The goal of the project was to create a model to predict the outcome of Ultimate Fighting Championship matches with the best possible accuracy. The main methods used in the project were logistic regression and Bayesian regression. The data used for said model was taken from matches between early April 2000 and mid April 2024. The predictions made by these models were compared with the predictions of various betting sites as well as with the true outcomes of the matches. The logistic regression model and the Bayesian model predicted the true outcome of the matches 60% and 70% of the time respectively, with both having comparable predictions to those of the betting sites.</p><p> </p>

corrected abstract:
<p>This project applied statistical inference methods to historical data of mixed martial arts (MMA) matches from the Ultimate Fighting Championship (UFC). The goal of the project was to create a model to predict the outcome of Ultimate Fighting Championship matches with the best possible accuracy. The main methods used in the project were logistic regression and Bayesian regression. The data used for said model was taken from matches between early April 2000 and mid April 2024. The predictions made by these models were compared with the predictions of various betting sites as well as with the true outcomes of the matches. The logistic regression model and the Bayesian model predicted the true outcome of the matches 60% and 70% of the time respectively, with both having comparable predictions to those of the betting sites.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:690036 - unnessary period at end of title:
"Prediktion av villapris och dess faktorers inverkan."
==>
"Prediktion av villapris och dess faktorers inverkan"

abstract is: 
<p> </p><p><em>A villas price </em>depends on several important factors. By statistical data, a mathematical <em>multiple regression model </em>was modeled. The model has important explanatory variables such as living space, renovation year and standard points has been taken into consideration, in order to assess their impact on the <em>final price </em>for private homes.</p><p>By using a statistical program,</p><p><em>Minitab 16</em>, the final model was selected with <em>eight explanatory variables</em>. The regression for this model explains up to <em>67.3 % </em>of the variation on the final price.</p><p>The results showed percentage wise that the standard points had the greatest impact on the price, there after renovation year and then living space.</p>

corrected abstract:
<p>A villas <em>price</em> depends on several important factors. By statistical data, a mathematical <em>multiple regression model</em> was modeled. The model has important explanatory variables such as living space, renovation year and standard points has been taken into consideration, in order to assess their impact on the <em>final price</em> for private homes.</p><p>By using a statistical program, <em>Minitab 16</em>, the final model was selected with <em>eight explanatory variables</em>. The regression for this model explains up to <em>67.3 %</em> of the variation on the final price.</p><p>The results showed percentage wise that the standard points had the greatest impact on the price, there after renovation year and then living space.</p>

Note - removed the empty paragraph and adjusted the italics
----------------------------------------------------------------------
In diva2:1341561 
abstract is: 
<p>This report describes the design and production of a 3-axis Helmholtz coil assembly and its control unit. The purpose of the system is to simulate the magnetic environment that the CubeSat MIST will need to measure in order to determine and control its attitude. To achieve this, the system consists of three Helmholtz coils with diameters of roughly 1 metre, supplied by a circuit that filters, transforms, and amplifies signals of 0-5 V (e.g. Arduino signals) to -50 to 50 V. The size of the coils allow for a near-homogeneous magnetic field large enough to cover the whole satellite. By adjusting the input, two necessary tests can be done on the satellite's attitude determination and control system. The first consists of verifying the magnetometer's correct measurement of the direction of the ambient magnetic field, and the other of testing the detumbling capability of the system when the satellite is in a rotating field. The equipment produced has been tested to verify its operation meets set requirements for testing.</p><p> </p>

corrected abstract:
<p>This report describes the design and production of a 3-axis Helmholtz coil assembly and its control unit. The purpose of the system is to simulate the magnetic environment that the CubeSat MIST will need to measure in order to determine and control its attitude. To achieve this, the system consists of three Helmholtz coils with diameters of roughly 1 metre, supplied by a circuit that filters, transforms, and amplifies signals of 0-5 V (e.g. Arduino signals) to -50 to 50 V. The size of the coils allow for a near-homogeneous magnetic field large enough to cover the whole satellite. By adjusting the input, two necessary tests can be done on the satellite's attitude determination and control system. The first consists of verifying the magnetometer's correct measurement of the direction of the ambient magnetic field, and the other of testing the detumbling capability of the system when the satellite is in a rotating field. The equipment produced has been tested to verify its operation meets set requirements for testing.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1798454 
abstract is: 
<p>GaN-HEMTs (Gallium Nitride-based High Electron Mobility Transistors) have, thanks to the large band gap of GaN, electrical properties that are suitable for applications of high electrical voltages, high currents, and fast switching. The large band gap also gives GaN-HEMTs a high resistance to radiation. In this degree project, the effects of 2 MeV proton irradiation of GaN-HEMTs constructed on both silicon carbide and silicon substrates are investigated. 20 transistors per substrate were irradiated in the particle accelerator 5 MV NEC Pelletron in the Ångström laboratory at Uppsala University. These transistors were exposed to radiation doses in the range of 10^11 to 10^15 protons/cm^2. The analysis shows that both transistors on silicon, as well as silicon carbide, are unaffected by proton irradiation up to a dose of 10^14 protons/cm^2. GaN-on-Si transistors show less influence of radiation than GaN-on-SiC transistors. The capacitances between gate and drain as well as drain and source for both GaN-on-SiC and GaN-on-Si HEMTs show hysteresis as a function of forward and backward gate voltage sweeps for the radiation dose of 10^15 protons/cm^2.</p><p> </p>

corrected abstract:
<p>GaN-HEMTs (Gallium Nitride-based High Electron Mobility Transistors) have, thanks to the large band gap of GaN, electrical properties that are suitable for applications of high electrical voltages, high currents, and fast switching. The large band gap also gives GaN-HEMTs a high resistance to radiation. In this degree project, the effects of 2 MeV proton irradiation of GaN-HEMTs constructed on both silicon carbide and silicon substrates are investigated. 20 transistors per substrate were irradiated in the particle accelerator 5 MV NEC Pelletron in the Ångström laboratory at Uppsala University. These transistors were exposed to radiation doses in the range of 10<sup>11</sup> to 10<sup>15</sup> protons/cm<sup>2</sup>. The analysis shows that both transistors on silicon, as well as silicon carbide are unaffected by proton irradiation up to a dose of 10<sup>14</sup> protons/cm<sup>2</sup>. GaN-on-Si transistors show less influence of radiation than GaN-on-SiC transistors. The capacitances between gate and drain as well as drain and source for both GaN-on-SiC and GaN-on-Si HEMTs show hysteresis as a function of forward and backward gate voltage sweeps for the radiation dose of 10<sup>15</sup> protons/cm<sup>2</sup>.</p>

Note - removed the empty paragraph and fixed the superscripts
----------------------------------------------------------------------
In diva2:858615 
abstract is: 
<p>This thesis describes a Coq formalization of realizability interpretations of arithmetic. The realizability interpretations are based on partial combinatory algebras—to each partial combinatory algebra there is an associated realizability interpretation. I construct two partial combinatory algebras. One of these gives a realizability interpretation equivalent to Kleene’s original one, without involving the usual recursion-theoretic machinery.</p><p> </p>

corrected abstract:
<p>This thesis describes a Coq formalization of realizability interpretations of arithmetic. The realizability interpretations are based on partial combinatory algebras&mdash;to each partial combinatory algebra there is an associated realizability interpretation. I construct two partial combinatory algebras. One of these gives a realizability interpretation equivalent to Kleene’s original one, without involving the usual recursion-theoretic machinery.</p>

Note - removed the empty paragraph and change on ". " to an &mdash;"
----------------------------------------------------------------------
In diva2:1570223 
abstract is: 
<p>Spotify, which is one of the worlds biggest music services, posted a data set and an open-ended challenge for music recommendation research. This study's goal is to recommend songs to playlists with the given data set from Spotify using Spectral clustering. While the given data set had 1 000 000 playlists, Spectral clustering was performed on a subset with 16 000 playlists due to the lack of computational resources. With four different weighting methods describing the connection between playlists, the study shows results of reasonable clusters where similar category of playlists were clustered together although most of the results also had a very large clusters where a lot of different sorts of playlists were clustered together. The conclusion of the results were that the data was overly connected as an effect of our weighting methods. While the results show the possibility of recommending songs to a limited number of playlists, hierarchical clustering would possibly be helpful to be able to recommend song to a larger amount of playlists, but that is left to future research to conclude.</p><p> </p>

corrected abstract:
<p>Spotify, which is one of the worlds biggest music services, posted a data set and an open-ended challenge for music recommendation research. This study's goal is to recommend songs to playlists with the given data set from Spotify using Spectral clustering. While the given data set had 1 000 000 playlists, Spectral clustering was performed on a subset with 16 000 playlists due to the lack of computational resources. With four different weighting methods describing the connection between playlists, the study shows results of reasonable clusters where similar category of playlists were clustered together although most of the results also had a very large clusters where a lot of different sorts of playlists were clustered together. The conclusion of the results were that the data was overly connected as an effect of our weighting methods. While the results show the possibility of recommending songs to a limited number of playlists, hierarchical clustering would possibly be helpful to be able to recommend song to a larger amount of playlists, but that is left to future research to conclude.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:555815 
abstract is: 
<p>Abstract</p><p> </p><p>As accurately as possible, creditors wish to determine if a potential debtor will repay the borrowed sum. To achieve this mathematical models known as credit scorecards quantifying the risk of default are used. In this study it is investigated whether the scorecard can be improved by using reject inference and thereby include the characteristics of the rejected population when refining the scorecard. The reject inference method used is parcelling. Logistic regression is used to estimate probability of default based on applicant characteristics. Two models, one with and one without reject inference, are compared using Gini coefficient and estimated profitability. The results yield that, when comparing the two models, the model with reject inference both has a slightly higher Gini coefficient as well a showing an increase in profitability. Thus, this study suggests that reject inference does improve the predictive power of the scorecard, but in order to verify the results additional testing on a larger calibration set is needed</p>

corrected abstract:
<p>As accurately as possible, creditors wish to determine if a potential debtor will repay the borrowed sum. To achieve this mathematical models known as credit scorecards quantifying the risk of default are used. In this study it is investigated whether the scorecard can be improved by using reject inference and thereby include the characteristics of the rejected population when refining the scorecard. The reject inference method used is parcelling. Logistic regression is used to estimate probability of default based on applicant characteristics. Two models, one with and one without reject inference, are compared using Gini coefficient and estimated profitability. The results yield that, when comparing the two models, the model with reject inference both has a slightly higher Gini coefficient as well as showing an increase in profitability. Thus, this study suggests that reject inference does improve the predictive power of the scorecard, but in order to verify the results additional testing on a larger calibration set is needed.</p>

Note - removed "<p>Abstract</p><p> </p>" and corrected "a showing" to "as showing"
----------------------------------------------------------------------
In diva2:1334832 
abstract is: 
<p>This paper will present a concrete study of representations of the special orthogonal group, SO(3), a group of great importance in physics. Specifically, we will study a natural representation of SO(3) in the space of polynomials in three variables with complex coefficients, C[x, y, z]. We will find that this special case provides all irreducible representations of SO(3), and also present some corollaries about spherical harmonics. Some preparatory theory regarding abstract algebra, linear algebra, topology and measure theory will also be presented.</p><p> </p>

corrected abstract:
<p>This paper will present a concrete study of representations of the special orthogonal group, <em>SO(3)</em>, a group of great importance in physics. Specifically, we will study a natural representation of <em>SO(3)</em> in the space of polynomials in three variables with complex coefficients, ℂ[𝑥, 𝑦, 𝑧]. We will find that this special case provides all irreducible representations of <em>SO(3)</em>, and also present some corollaries about spherical harmonics. Some preparatory theory regarding abstract algebra, linear algebra, topology and measure theory will also be presented.</p>

Note changes in mathematical elements and removed the empty paragraph
----------------------------------------------------------------------
In diva2:1462397 
abstract is: 
<p>The space industry and the technological developments regarding space exploration hasn’t been this popular since the first moon landing. The privatization of space exploration and the vertical landing rockets made rocket science mainstream again. While being able to reuse rockets is efficient both in terms of profitability and popularity, these developments are still in their early stages. Vertical landing has challenges that, if neglected, can cause disastrous consequences. The existing studies on the matter usually don’t account for aerodynamics forces and corresponding controls, which results in higher fuel consumption thus lessening the economical benefits of vertical landing. Similar problems have been tackled in studies not regarding booster landings but regarding planetary landings. And while multiple solutions have been proposed for these problems regarding planetary landings, the fact that the reinforcement learning concepts work well and provide robustness made them a valid candidate for applying to booster landings. In this study, we focus on developing a vertical booster descent guidance and control law that’s robust by applying reinforcement learning concept. Since reinforcement learning method that is chosen requires solving Optimal Control Problems (OCP), we also designed and developed an OCP solver software. The robustness of resulting hybrid guidance and control policy will be examined against various different uncertainties including but not limited to wind, delay and aerodynamic uncertainty.</p><p> </p>

corrected abstract:
<p>The space industry and the technological developments regarding space exploration hasn’t been this popular since the first moon landing. The privatization of space exploration and the vertical landing rockets made rocket science mainstream again. While being able to re-use rockets is efficient both in terms of profitability and popularity, these developments are still in their early stages. Vertical landing has challenges that, if neglected, can cause disastrous consequences. The existing studies on the matter usually don’t account for aerodynamics forces and corresponding controls, which results in higher fuel consumption thus lessening the economical benefits of vertical landing.</p><p>Similar problems have been tackled in studies not regarding booster landings but regarding planetary landings. And while multiple solutions have been proposed for these problems regarding planetary landings, the fact that the reinforcement learning concepts work well and provide robustness made them a valid candidate for applying to booster landings. In this study, we focus on developing a vertical booster descent guidance and control law that’s robust by applying reinforcement learning concept. Since reinforcement learning method that is chosen requires solving Optimal Control Problems (OCP), we also designed and developed an OCP solver software. The robustness of resulting hybrid guidance and control policy will be examined against various different uncertainties including but not limited to wind, delay and aerodynamic uncertainty.</p>

Note - removed the empty paragraph, inserted missing paragraph break, and inserted missing hyphen
----------------------------------------------------------------------
In diva2:1776588 
abstract is: 
<p>The objective of robust portfolio optimization is to find a way to allocate capital to some financial assets such that portfolio return is maximized in the worst-case scenario, which is desirable for investors with a low tolerance for risk. This study aims to apply the robust approach to asset allocation based on 30 of the biggest stocks on the Stockholm Stock Exchange. Three models with different constraints on portfolio return and variance are obtained and solved using the Gurobi Optimizer. The result of any one of the models could be proposed as a low-risk portfolio. The choice between the models is a trade-off between higher expected return and lower variance, and it depends on the individual preferences of the investor.</p><p> </p>

corrected abstract:
<p>The objective of robust portfolio optimization is to find a way to allocate capital to some financial assets such that portfolio return is maximized in the worst-case scenario, which is desirable for investors with a low tolerance for risk. This study aims to apply the robust approach to asset allocation based on 30 of the biggest stocks on the Stockholm Stock Exchange. Three models with different constraints on portfolio return and variance are obtained and solved using the Gurobi Optimizer. The result of any one of the models could be proposed as a low-risk portfolio. The choice between the models is a trade-off between higher expected return and lower variance, and it depends on the individual preferences of the investor.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:902529 
abstract is: 
<p>The topic of this thesis is the implementation of rapid branching to find an integer solution for the train timetabling problem. The techniques that rapid branching are based on are presented. The important aspect of rapid branching are discussed and then the algorithm is applied to some artificial problems. It is shown that rapid branching can be both faster and slower than a standard integer solver depending on the problem instance. For the most realistic set of the examined instances, rapid branching turned out to be faster than the standard integer solver and produce satisficingly high quality solutions.</p><p> </p>

corrected abstract:
<p>The topic of this thesis is the implementation of rapid branching to find an integer solution for the train timetabling problem. The techniques that rapid branching are based on are presented. The important aspects of rapid branching are discussed and then the algorithm is applied to some artificial problems. It is shown that rapid branching can be both faster and slower than a standard integer solver depending on the problem instance. For the most realistic set of the examined instances, rapid branching turned out to be faster than the standard integer solver and produce satisficingly high quality solutions.</p>

Note - removed the empty paragraph and added the "s" to "aspects"
----------------------------------------------------------------------
In diva2:1219083 
abstract is: 
<p>The purpose of this project was to implement an inverse diffusion algorithm to locate the sources of an emitted substance. This algorithm has yielded successful results when applied to biological cell detection, and it has been suggested that the run time could be greatly reduced if adaptions for a computation graph framework are made. This would automate calculations of gradients and allow for faster execution on a graphics processing unit.</p><p>The algorithm implementation was realized in TensorFlow, which is primarily a machine learning oriented programming library. Computer-generated biological test images were then used to evaluate the performance using regular image analysis software and accuracy metrics.</p><p>Comparisons reveal that the TensorFlow implementation of the algorithm can match the accuracy metrics of traditional implementations of the same algorithm. Viewed in a broader scope this serves as an example to highlight the possibility of using computation graph frameworks to solve large scale optimization problems, and more specifically inverse problems.</p><p> </p>

corrected abstract:
<p>The purpose of this project was to implement an inverse diffusion algorithm to locate the sources of an emitted substance. This algorithm has yielded successful results when applied to biological cell detection, and it has been suggested that the run time could be greatly reduced if adaptions for a computation graph framework are made. This would automate calculations of gradients and allow for faster execution on a graphics processing unit.</p><p>The algorithm implementation was realized in TensorFlow, which is primarily a machine learning oriented programming library. Computer-generated biological test images were then used to evaluate the performance using regular image analysis software and accuracy metrics.</p><p>Comparisons reveal that the TensorFlow implementation of the algorithm can match the accuracy metrics of traditional implementations of the same algorithm. Viewed in a broader scope this serves as an example to highlight the possibility of using computation graph frameworks to solve large scale optimization problems, and more specifically inverse problems.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1570353 
abstract is: 
<p>Climate is a tremendously complex topic, affecting many aspects of human activity and constantly changing. Defining some structures and rules for how it works is thereof of the utmost importance even though it might only cover a small part of the complexity. Cluster analysis is a tool developed in data analysis that is able to categorize data into groups of similar type. In this paper data from the Swedish Meteorological and Hydrological Institute (SMHI) is clustered to find a partitioning. The cluster analysis used is called Spectral clustering which is a family of methods making use of the spectral properties of graphs. Concrete results over different groupings of climate over Sweden were found.</p><p> </p>

corrected abstract:
<p>Climate is a tremendously complex topic, affecting many aspects of human activity and constantly changing. Defining some structures and rules for how it works is thereof of the utmost importance even though it might only cover a small part of the complexity. Cluster analysis is a tool developed in data analysis that is able to categorize data into groups of similar type. In this paper data from the Swedish Meteorological and Hydrological Institute (SMHI) is clustered to find a partitioning. The cluster analysis used is called Spectral clustering which is a family of methods making use of the spectral properties of graphs. Concrete results over different groupings of climate over Sweden were found.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1742865 
abstract is: 
<p>As cyber-physical systems become increasingly complex, the management and verification of requirements during design is essential. A new language called CRML (Common Requirement Modelling Language) has been created during the European EMBrACE project to formalize realistic dynamical requirements, but a method for representing these requirements and a framework for using them as a design aid must be defined to ease appropriation by engineers. This report therefore presents a draft of a new graphical method for representing system requirements and evaluating different architectures, extending classical Systems Engineering (SE) approaches to the complexity of dynamic physical systems. Once the method completed, the verification and validation process can then be carried out through the simulation of solution models with CRML models. Solution models state how the system will behave while the CRML models state whether this behaviour is compliant with the requirements. The new graphical approach has been applied to two energy systems found in the nuclear industry. This report presents the first promising results as well as some perspectives to consolidate it.</p><p> </p>

corrected abstract:
<p>As cyber-physical systems become increasingly complex, the management and verification of requirements during design is essential. A new language called CRML (Common Requirement Modelling Language) has been created during the European EMBrACE project to formalize realistic dynamical requirements, but a method for representing these requirements and a framework for using them as a design aid must be defined to ease appropriation by engineers. This report therefore presents a draft of a new graphical method for representing system requirements and evaluating different architectures, extending classical Systems Engineering (SE) approaches to the complexity of dynamic physical systems. Once the method completed, the verification and validation process can then be carried out through the simulation of solution models with CRML models. Solution models state how the system will behave while the CRML models state whether this behaviour is compliant with the requirements. The new graphical approach has been applied to two energy systems found in the nuclear industry. This report presents the first promising results as well as some perspectives to consolidate it.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:558590 
abstract is: 
<p>It is of great importance to find an analytical copula that will represent the empirical lower tail dependence. In this study, the pairwise empirical copula are estimated using data of the S&amp;P 500 stocks during the period 2007-2010.Different optimization methods and measures of dependence have been used to fit Gaussian, t and Clayton copula to the empirical copulas, in order to represent the empirical lower tail dependence. These different measures of dependence and optimization methods with their restrictions, point at different analytical copulas being optimal. In this study the t copula with 5 degrees of freedom is giving the most fulfilling result, when it comes to representing lower tail dependence. The t copula with 5 degrees of freedom gives the best representation of empirical lower tail dependence, whether one uses the 'Empirical maximum likelihood estimator', or 'Equal Ƭ' as an approach.</p><p> </p>

corrected abstract:
<p>It is of great importance to find an analytical copula that will represent the empirical lower tail dependence. In this study, the pairwise empirical copula are estimated using data of the S&amp;P 500 stocks during the period 2007-2010. Different optimization methods and measures of dependence have been used to fit Gaussian, 𝑡 and Clayton copula to the empirical copulas, in order to represent the empirical lower tail dependence. These different measures of dependence and optimization methods with their restrictions, point at different analytical copulas being optimal. In this study the 𝑡 copula with 5 degrees of freedom is giving the most fulfilling result, when it comes to representing lower tail dependence. The 𝑡 copula with 5 degrees of freedom gives the best representation of empirical lower tail dependence, whether one uses the ’Empirical maximum likelihood estimator’, or ’Equal &#x1D70F;’ as an approach.</p>

Note - removed the empty paragraph, fixed quote marks to match original, and fixed the math symbols
----------------------------------------------------------------------
In diva2:813868 
abstract is: 
<p>Feasibly computable analytic solutions for systems of many particles in fluid dynamics and electrostatics are few and far-between. Simulations and numerical approximations are essential to studying these systems. This is commonly done without directly calculating the interacting field between particles. In this report a method utilizing the spectral accuracy of the Fourier transform is studied to calculate particle velocities via the surrounding fluid velocity field. The method is applied to a periodic cube of a suspension of small, spherical particles sedimenting in a fluid affected by gravity, in an attempt to mimic the behaviour of a similar infinite system. Results for a few particles qualitatively relate the shape of the solution to the choice of interpolation between particles to grid and quantitatively maps some convergence properties of a certain class of interpolating functions, cardinal B-splines. The properties of the method on the periodic system are also examined and compared to a similar study of the infinite system for many, ~1000, particles.</p><p> </p>

corrected abstract:
<p>Feasibly computable analytic solutions for systems of many particles in fluid dynamics and electrostatics are few and far-between. Simulations and numerical approximations are essential to studying these systems. This is commonly done without directly calculating the interacting field between particles. In this report a method utilizing the spectral accuracy of the Fourier transform is studied to calculate particle velocities via the surrounding fluid velocity field. The method is applied to a periodic cube of a suspension of small, spherical particles sedimenting in a fluid affected by gravity, in an attempt to mimic the behaviour of a similar infinite system. Results for a few particles qualitatively relate the shape of the solution to the choice of interpolation between particles to grid and quantitatively maps some convergence properties of a certain class of interpolating functions, cardinal B-splines. The properties of the method on the periodic system are also examined and compared to a similar study of the infinite system for many, ∼ 1000, particles.</p>
----------------------------------------------------------------------
In diva2:1572684 
abstract is: 
<p>Aortic diseases are a common and fatal health issue, regarding various conditions targeting the aorta. The arterial wall consists of two proteins; collagen and elastin (fibers), and they will contribute to the elasticity and strength of the wall. In recent times there has been a growing interest in studying the microstructure of the aortic tissues because it is believed that changes in the amount and/or the construction of the fibers will result in mechanical as well as functional changes that are associated with these heart conditions. Therefore, a better understanding of collagen and its load-carrying properties in vascular tissue is needed for others to be able to develop new designs of cardiovascular medical devices that may help the medical field to find other therapies and treatments for patients with aortic diseases. </p><p>This bachelor's degree thesis is based on a literature study, experimental testing, finite element method (FEM) analysis, and a microscopy study. To get a broader understanding of the arterial wall, the collagen, and its mechanical properties a literature study was conducted. The experimental testing was made with tensile testing equipment called CellScales BioTester 5000 uniaxial bio-testings and the test specimen used was porcine aorta from a local abattoir. The data obtained from the testing was put in MATLAB to produce graphs visualizing different data, such as; stresses, strain, forces and stiffness. Then a FEM-simulation was made by Christopher Miller and the data and images obtained from the analysis were used to compare with the results from MATLAB. Furthermore, the ruptured test specimen after the tensile testing was sent to Karolinska Institutionen (KI) for a microscope study.</p><p>The results from MATLAB were used to receive information regarding the material properties, to calculate the stiffness as well as the strain at the rupture. There were two samples made, sample 7 and sample 10, and the data from the MATLAB graphs were used to determine where the rupture occurred. For sample 7 this occurs when the force is 7.29 [N], elongation is 22.93 x 10^(-3) [m] and stress is 1210 [kPa], for sample 10 the rupture of sample 10 occurred when the force is 6.65 [N], elongation is 17.4 x 10^(-3) [m] and stress is 606 [kPa]. The FEM-simulation showed where the maximum deformation takes place, which was in the middle of our tissues. From the microscope images from KI, the accuracy of the FEM-model could be seen.</p><p> </p>

corrected abstract:
<p>A common and fatal health issue in the world is aortic diseases, which are various conditions targeting the aorta. To get a better understanding of these types of diseases there has been a growing interest in studying the microstructure and mechanical behavior of the aortic tissue, because it is believed that changes in the amount and/or the construction of the fibers (collagen and elastin) will result in mechanical as well as functional changes that can be associated with these types of different vascular conditions.  Therefore, a better understanding of collagen and its load-carrying properties in vascular tissues is needed for others to be able to develop new designs of cardiovascular medical devices that will proceed to help the medical field to find other therapies and treatments for patients with aortic diseases.</p><p>This bachelor’s degree thesis explores the role of collagen in the vessel wall by a literature study, experimental testing, Finite Element Method (FEM) as well as a microscopy study. A literature study was conducted to get a broader understanding of the arterial wall, collagen, and its mechanical properties.  The experimental testing was performed with tensile testing equipment called CellScale BioTester 5000 and the test specimen used was a porcine aorta bought from a local abattoir. The data obtained from the tensile testing was put into MATLAB for post-processing to receive graphs that visualized different data, such as; stresses, strain, forces, and stiffness. Then a FEM-simulation was executed and the data and images obtained from the analysis were used to compare with the results from the post-processing. Furthermore, the ruptured test specimen from the tensile testing was sent to Karolinska Institutionen (KI) for a microscopy study.</p><p>The results from the post-processing were used to receive information regarding the material properties, to calculate the stiffness as well as the strain at the rupture. The data from the post-processing graphs for Sample 10 was used to determine where the rupture occurred which was when the force was 6.65 [N], the elongation 17.4 [mm] and stress 606 [kPa]. The stiffness in the tissue sample showed a decrease with the ramps, cycles, and stretch. This could be explained through that the tissue’s resistance towards elastic deformation decreases.</p><p>The FEM-simulation (like the tensile test and the microscopic images) show that the biggest and considerable deformation takes place at the smallest diameter. The FEM-graphs like the physical tests exhibit a relaxation even without an increased displacement. The graphs also exhibit that the FEManalysis relates to the physical tests and with the aid of the FEM-analysis it is possible to see where the prominent stresses are located in the tissue and where the accuracy of the FEM-model is confirmed by the microscopic pictures. From the microscope images from KI, the rupture of collagen could be visualized.</p><p>The post-processing results of the stiffness and stress-strain curve, the FEManalysis and the microscope observations were compared and show a clear qualitative agreement, which indicates that the method is suitable for future development.</p>

Note major differences in wording between DiVA and original - the above is based on the original
also removed the empty paragraph
----------------------------------------------------------------------
In diva2:1165816 
abstract is: 
<p>In 1983 Grothendieck wrote a letter to Faltings, [Gro83], outlining what is today known as the anabelian conjectures. These conjectures concern the possibility to reconstruct curves and schemes from their étale fundamental group. Although Faltings never replied to the letter, his student Mochizuki began working on it. A major achievement by Mochizuki and Tamagawa was to prove several important versions of these conjectures.</p><p>In this thesis we will first introduce Grothendieck’s Galois theory with the aim to define the étale fundamental group and formulate Mochizuki’s result. After recalling some necessary homotopy theory, we will introduce the étale homotopy type, which is an extension of the étale fundamental group developed by Artin, Mazur and Friedlander. This is done in order to describe some recentwork by Schmidt and Stix that improves on the results of Mochizuki and Tamagawa by extending them from étale fundamental groups to étale homotopy types of certain (possibly higher-dimensional) schemes.</p><p> </p>

corrected abstract:
<p>In 1983 Grothendieck wrote a letter to Faltings, [Gro83], outlining what is today known as the anabelian conjectures. These conjectures concern the possibility to reconstruct curves and schemes from their étale fundamental group. Although Faltings never replied to the letter, his student Mochizuki began working on it. A major achievement by Mochizuki and Tamagawa was to prove several important versions of these conjectures.</p><p>In this thesis we will first introduce Grothendieck’s Galois theory with the aim to define the étale fundamental group and formulate Mochizuki’s result. After recalling some necessary homotopy theory, we will introduce the étale homotopy type, which is an extension of the étale fundamental group developed by Artin, Mazur and Friedlander. This is done in order to describe some recent work by Schmidt and Stix that improves on the results of Mochizuki and Tamagawa by extending them from étale fundamental groups to étale homotopy types of certain (possibly higher-dimensional) schemes.</p>
----------------------------------------------------------------------
In diva2:1880323 
abstract is: 
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p><p> </p>

corrected abstract:
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1571248 
abstract is: 
<p>This thesis aims to investigate whether a fiber Bragg grating array (FBGA) can be designed to reduce the bandwidth of light from a 1.5 µm fiber laser while the distances between the fiber Bragg gratings are in the 10-20 cm range. The results from simulations show that for FBGA with multiple cavities, small changes in cavity length impact transmission drastically. For FBGA consisting of only two fiber Bragg gratings, the quality of the laser cavity's spectrum is improved, as the peak width is reduced and stability is increased. The conclusions of the experiments and the simulations are that FBGA with a single cavity gives best performance. For future studies, if higher computational power and production capabilities are available, an in depth analysis of a multi cavity FBGA could be conducted.</p><p> </p>

corrected abstract:
<p>This thesis aims to investigate whether a fiber Bragg grating array (FBGA) can be designed to reduce the bandwidth of light from a 1.5 µm fiber laser while the distances between the fiber Bragg gratings are in the 10-20 cm range. The results from simulations show that for FBGA with multiple cavities, small changes in cavity length impact transmission drastically. For FBGA consisting of only two fiber Bragg gratings, the quality of the laser cavity's spectrum is improved, as the peak width is reduced and stability is increased. The conclusions of the experiments and the simulations are that FBGA with a single cavity gives best performance. For future studies, if higher computational power and production capabilities are available, an in depth analysis of a multi cavity FBGA could be conducted.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1779454 
abstract is: 
<p>The dopamine receptor is one of the main therapeutic targets for neurological disorders, such as Parkinson’s disease and schizophrenia. Although dopamine and adrenaline are structurally similar and both bind to the D2 dopamine receptor, they activate the receptor differently. This study aims to investigate the mechanism by which dopamine and adrenaline trigger conformational changes resulting in the receptor activation. To achieve this, two data sets derived from molecular dynamic (MD) simulations were analyzed, in which one integrates D2 ensembles with and without ligand bound and the other includes D2 ensembles in the presence of the dopamine and adrenaline bound. Both supervised and unsupervised machine learning methods were applied to interpret the high dimensional data provided by the MD simulations. The results obtained from these methods provided the important residues in the D2 dopamine receptor, suggesting they constitute different conformations and interactions with surrounding residues under various conditions. The NPxxY motif in the bottom of transmembrane helix 7 was identified as the most important to distinguish different states induced by distinct ligands binding, according to the supervised methods. Meanwhile, the unsupervised methods showed a general trend of high importance around certain sections, such as the intracellular portion of the transmembrane helix number 6. Taken together, our findings lay the groundwork for the understanding of molecular activation mechanism of D2 dopamine receptor modulated by different ligands.</p><p> </p>

corrected abstract:
<p>The dopamine receptor is one of the main therapeutic targets for neurological disorders, such as Parkinson’s disease and schizophrenia. Although dopamine and adrenaline are structurally similar and both bind to the D<sub>2</sub> dopamine receptor, they activate the receptor differently. This study aims to investigate the mechanism by which dopamine and adrenaline trigger conformational changes resulting in the receptor activation. To achieve this, two data sets derived from molecular dynamic (MD) simulations were analyzed, in which one integrates D<sub>2</sub> ensembles with and without ligand bound and the other includes D<sub>2</sub> ensembles in the presence of the dopamine and adrenaline bound. Both supervised and unsupervised machine learning methods were applied to interpret the high dimensional data provided by the MD simulations. The results obtained from these methods provided the important residues in the D<sub>2</sub> dopamine receptor, suggesting they constitute different conformations and interactions with surrounding residues under various conditions. The NPxxY motif in the bottom of transmembrane helix 7 was identified as the most important to distinguish different states induced by distinct ligands binding, according to the supervised methods. Meanwhile, the unsupervised methods showed a general trend of high importance around certain sections, such as the intracellular portion of the transmembrane helix number 6. Taken together, our findings lay the groundwork for the understanding of molecular activation mechanism of D<sub>2</sub> dopamine receptor modulated by different ligands.</p>

Note - removed the empty paragraph and fixed subscripts
----------------------------------------------------------------------
In diva2:1880199 
abstract is: 
<p>This work explores the unsolvability of the general quintic equation through the lens of Galois theory. We begin by providing a historical perspective on the problem. This starts with the solution of the general cubic equation derived by Italian mathematicians. We then move on to Lagrange's insights on the importance of studying the permutations of roots. Finally, we discuss the critical contributions of Évariste Galois, who connected the solvability of polynomials to the properties of permutation groups. Central to our thesis is the introduction and motivation of key concepts such as fields, solvable groups, Galois groups, Galois extensions, and radical extensions.</p><p>We rigorously develop the theory that connects the solvability of a polynomial to the solvability of its Galois group. After developing this theoretical framework, we go on to show that there exist quintic polynomials with Galois groups that are isomorphic to the symmetric group S5. Given that S5 is not a solvable group, we establish that the general quintic polynomial is not solvable by radicals. Our work aims to provide a comprehensive and intuitive understanding of the deep connections between polynomial equations and abstract algebra.</p><p> </p>

corrected abstract:
<p>This work explores the unsolvability of the general quintic equation through the lens of Galois theory. We begin by providing a historical perspective on the problem. This starts with the solution of the general cubic equation derived by Italian mathematicians. We then move on to Lagrange's insights on the importance of studying the permutations of roots. Finally, we discuss the critical contributions of Évariste Galois, who connected the solvability of polynomials to the properties of permutation groups. Central to our thesis is the introduction and motivation of key concepts such as fields, solvable groups, Galois groups, Galois extensions, and radical extensions.</p><p>We rigorously develop the theory that connects the solvability of a polynomial to the solvability of its Galois group. After developing this theoretical framework, we go on to show that there exist quintic polynomials with Galois groups that are isomorphic to the symmetric group 𝑆<sub>5</sub>. Given that 𝑆<sub>5</sub> is not a solvable group, we establish that the general quintic polynomial is not solvable by radicals. Our work aims to provide a comprehensive and intuitive understanding of the deep connections between polynomial equations and abstract algebra.</p>

Note - removed the empty paragraph, fixed the subscripts, and replaced "S" with "𝑆"
----------------------------------------------------------------------
In diva2:1592993 
abstract is: 
<p>Digitization of the energy industry, introduction of smart grids and increasing regulation of electricity consumption metering have resulted in vast amounts of electricity data. This data presents a unique opportunity to understand the electricity usage and to make it more efficient, reducing electricity consumption and carbon emissions. An important initial step in analyzing the data is to identify anomalies.</p><p>In this thesis the problem of anomaly detection in electricity consumption series is addressed using four machine learning methods: density based spatial clustering for applications with noise (DBSCAN), local outlier factor (LOF), isolation forest (iForest) and one-class support vector machine (OC-SVM). In order to evaluate the methods synthetic anomalies were introduced to the electricity consumption series and the methods were then evaluated for the two anomaly types point anomaly and collective anomaly. In addition to electricity consumption data, features describing the prior consumption, outdoor temperature and date-time properties were included in the models. Results indicate that the addition of the temperature feature and the lag features generally impaired anomaly detection performance, while the inclusion of date-time features improved it. Of the four methods, OC-SVM was found to perform the best at detecting point anomalies, while LOF performed the best at detecting collective anomalies. In an attempt to improve the models' detection power the electricity consumption series were de-trended and de-seasonalized and the same experiments were carried out. The models did not perform better on the decomposed series than on the non-decomposed.</p><p> </p>

corrected abstract:
<p>Digitization of the energy industry, introduction of smart grids and increasing regulation of electricity consumption metering have resulted in vast amounts of electricity data. This data presents a unique opportunity to understand the electricity usage and to make it more efficient, reducing electricity consumption and carbon emissions. An important initial step in analyzing the data is to identify anomalies.</p><p>In this thesis the problem of anomaly detection in electricity consumption series is addressed using four machine learning methods: density based spatial clustering for applications with noise (DBSCAN), local outlier factor (LOF), isolation forest (iForest) and one-class support vector machine (OC-SVM). In order to evaluate the methods synthetic anomalies were introduced to the electricity consumption series and the methods were then evaluated for the two anomaly types <em>point anomaly</em> and <em>collective anomaly</em>. In addition to electricity consumption data, features describing the prior consumption, outdoor temperature and date-time properties were included in the models. Results indicate that the addition of the temperature feature and the lag features generally impaired anomaly detection performance, while the inclusion of date-time features improved it. Of the four methods, OC-SVM was found to perform the best at detecting point anomalies, while LOF performed the best at detecting collective anomalies. In an attempt to improve the models' detection power the electricity consumption series were de-trended and de-seasonalized and the same experiments were carried out. The models did not perform better on the decomposed series than on the non-decomposed.</p>

Note - removed the empty paragraph and added italics as per the original
---------------------------------------------------------------------
In diva2:692474 
abstract is: 
<p> </p><p></p><p>With the increases in fuel costs due to the depletion of the world oil reserves and the increase of greenhouse gasses as a consequence to using oil as a fuel many companies are looking to new and innovative ways to power their aircraft. One of these new ways to power an aircraft is using fuel cells powered using hydrogen and oxygen, thus producing nothing but water vapour and small amounts of nitrogen dioxide as well as trace amounts of other emissions. Both Boeing (1) and Politecnico di Torino (2) have shown that it is possible to build an all-electric aircraft powered by fuel cells. Both flights used small, two-seater aircraft and a constant between them was the loss of the co-pilot seat due to weight and lack of space. As this paper will deal with a commercial aircraft a primary concern is the cargo and passenger capacity and whatever impact switching propulsion system has on these. The aircraft used to test the feasibility of these fuel-cells is the SAAB 340 passenger aircraft/airliner chosen for its twin-engine turboprop configuration and generally conventional design. Its engines and fuel tanks were removed and electrical motors, fuel cells and hydrogen tanks were installed, all the while taking care not to move its centre of gravity too much. Based upon the calculations performed the new aircraft appears to be airworthy though it has a very low rate of climb and because of this an extremely short range. The vehicles passenger and cargo carrying capacity has been severely diminished due to the weight and size of the new components. Other parameters have also decreased, such as speeds and power outputs from the motors. Despite performance reductions the aircraft seem to be able to fulfil the demands placed upon it though carrying capacity appears to be severely diminished.</p>

corrected abstract:
<p>With the increases in fuel costs due to the depletion of the world oil reserves and the increase of greenhouse gasses as a consequence to using oil as a fuel many companies are looking to new and innovative ways to power their aircraft. One of these new ways to power an aircraft is using fuel cells powered using hydrogen and oxygen, thus producing nothing but water vapour and small amounts of nitrogen dioxide as well as trace amounts of other emissions. Both Boeing (1) and Politecnico di Torino (2) have shown that it is possible to build an all-electric aircraft powered by fuel cells. Both flights used small, two-seater aircraft and a constant between them was the loss of the co-pilot seat due to weight and lack of space. As this paper will deal with a commercial aircraft a primary concern is the cargo and passenger capacity and whatever impact switching propulsion system has on these. The aircraft used to test the feasibility of these fuel-cells is the SAAB 340 passenger aircraft/airliner chosen for its twin-engine turboprop configuration and generally conventional design. Its engines and fuel tanks were removed and electrical motors, fuel cells and hydrogen tanks were installed, all the while taking care not to move its centre of gravity too much. Based upon the calculations performed the new aircraft appears to be airworthy though it has a very low rate of climb and because of this an extremely short range. The vehicles passenger and cargo carrying capacity has been severely diminished due to the weight and size of the new components. Other parameters have also decreased, such as speeds and power outputs from the motors. Despite performance reductions the aircraft seem to be able to fulfil the demands placed upon it though carrying capacity appears to be severely diminished.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1431653 
abstract is: 
<p>This thesis tackles the problem of predicting the collision risk for vehicles driving in complex traffic scenes for a few seconds into the future. The method is based on previous research using dynamic Bayesian networks to represent the state of the system.</p><p>Common risk prediction methods are often categorized into three different groups depending on their abstraction level. The most complex of these are interaction-aware models which take driver interactions into account. These models often suffer from high computational complexity which is a key limitation in practical use. The model studied in this work takes interactions between drivers into account by considering driver intentions and the traffic rules in the scene.</p><p>The state of the traffic scene used in the model contains the physical state of vehicles, the intentions of drivers and the expected behaviour of drivers according to the traffic rules. To allow for real-time risk assessment, an approximate inference of the state given the noisy sensor measurements is done using sequential importance resampling. Two different measures of risk are studied. The first is based on driver intentions not matching the expected maneuver, which in turn could lead to a dangerous situation. The second measure is based on a trajectory prediction step and uses the two measures time to collision (TTC) and time to critical collision probability (TTCCP).</p><p>The implemented model can be applied in complex traffic scenarios with numerous participants. In this work, we focus on intersection and roundabout scenarios. The model is tested on simulated and real data from these scenarios. %Simulations of these scenarios is used to test the model. In these qualitative tests, the model was able to correctly identify collisions a few seconds before they occur and is also able to avoid false positives by detecting the vehicles that will give way.</p><p> </p>

corrected abstract:
<p>This thesis tackles the problem of predicting the collision risk for vehicles driving in complex traffic scenes for a few seconds into the future. The method is based on previous research using dynamic Bayesian networks to represent the state of the system.</p><p>Common risk prediction methods are often categorized into three different groups depending on their abstraction level. The most complex of these are interaction-aware models which take driver interactions into account. These models often suffer from high computational complexity which is a key limitation in practical use. The model studied in this work takes interactions between drivers into account by considering driver intentions and the traffic rules in the scene.</p><p>The state of the traffic scene used in the model contains the physical state of vehicles, the intentions of drivers and the expected behaviour of drivers according to the traffic rules. To allow for real-time risk assessment, an approximate inference of the state given the noisy sensor measurements is done using sequential importance resampling. Two different measures of risk are studied. The first is based on driver intentions not matching the expected maneuver, which in turn could lead to a dangerous situation. The second measure is based on a trajectory prediction step and uses the two measures <em>time to collision</em> (TTC) and <em>time to critical collision probability</em> (TTCCP).</p><p>The implemented model can be applied in complex traffic scenarios with numerous participants. In this work, we focus on intersection and roundabout scenarios. The model is tested on simulated and real data from these scenarios. In these qualitative tests, the model was able to correctly identify collisions a few seconds before they occur and is also able to avoid false positives by detecting the vehicles that will give way.</p>

Note - removed the empty paragraph, added itliacs as per the original, and removed "%Simulations of these scenarios is used to test the model." - a sentence that was commented out in the LaTeX.
----------------------------------------------------------------------
In diva2:1218276 
abstract is: 
<p> </p><p>In this thesis, image classification of difficult data through the use of machine learning algorithms is evaluated using a Kernel Machine. When trying to classify objects in im- ages in real world situations the object in question might be behind some form of transparent obstruction or barrier. By implementing a simple machine learning algorithm this thesis aims to provide an approximate lower bound for the performance of machine learning algorithms in such circumstances.</p><p>Results show that machine learning is a viable option even though performance decrease with barrier complexity and thickness. In the best case performance dropped less than one percentage point when using a simple barrier compared to using no barrier and allowing the algorithm to train on images with objects behind said barrier. Performance is much worse when not allowing the algorithm to train on images with barriers. Furthermore, performance seems to be largely independent on image size despite the loss of information associated with introducing barriers.</p>

corrected abstract:
<p>In this thesis, image classification of difficult data through the use of machine learning algorithms is evaluated using a Kernel Machine. When trying to classify objects in images in real world situations the object in question might be behind some form of transparent obstruction or barrier. By implementing a simple machine learning algorithm this thesis aims to provide an approximate lower bound for the performance of machine learning algorithms in such circumstances.</p><p>Results show that machine learning is a viable option even though performance decrease with barrier complexity and thickness. In the best case performance dropped less than one percentage point when using a simple barrier compared to using no barrier and allowing the algorithm to train on images with objects behind said barrier. Performance is much worse when not allowing the algorithm to train on images with barriers. Furthermore, performance seems to be largely independent on image size despite the loss of information associated with introducing barriers.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1450330 
abstract is: 
<p>The purpose of this report was to investigate whether the weight of Marsblade's new steel runner could be reduced without making the blade too weak and still retain its properties. Marsblade introduces a unique concept with a moving steel runner in the holder which creates many challenges for the geometry as it has to cope with stresses and at the same time fulfill it's unique function. However, the unique geometry contributes to increased weight and a weight reduction are therefore desirable, in order to make the blade more competitive with other brands on the market. Marsblade's steel runner has thus been compared to a conventional steel runner which is considered to meet the requirements. When analyzing the steel runners, FEA analyses have been performed to investigate their strength properties. A carefully made trade off between strength and reduced weight resulted in the new weight optimized steel runner weighs 13 grams less than the original one, this weight loss means that the weight has decreased by 8.8%. The new geometry is optimized in such a way that the occurrence of stress concentrations at dimensional transitions has decreased.</p><p> </p>

corrected abstract:
<p>The purpose of this report was to investigate whether the weight of Marsblade's new steel runner could be reduced without making the blade too weak and still retain its properties. Marsblade introduces a unique concept with a moving steel runner in the holder which creates many challenges for the geometry as it has to cope with stresses and at the same time fulfill it's unique function. However, the unique geometry contributes to increased weight and a weight reduction are therefore desirable, in order to make the blade more competitive with other brands on the market. Marsblade's steel runner has thus been compared to a conventional steel runner which is considered to meet the requirements. When analyzing the steel runners, FEA analyses have been performed to investigate their strength properties. A carefully made trade off between strength and reduced weight resulted in the new weight optimized steel runner weighs 13 grams less than the original one, this weight loss means that the weight has decreased by 8.8%. The new geometry is optimized in such a way that the occurrence of stress concentrations at dimensional transitions has decreased.</p>
----------------------------------------------------------------------
In diva2:1832663 
abstract is: 
<p>This study aims to investigate the relationship between multiple air pollution and different vehicle variables, such as vehicle year, fuel type and vehicle type, on Hornsgatan in Stockholm. The study intends to answer which factors have the greatest impact on air quality. The implementation is based on the two machine learning algorithms Random Forest and Support Vector Regression, which are compared based on R^2 and RMSE. The models created with Random Forest outperform Support Vector Regression for the various air pollutants. The best performing model was the carbon monoxide model which had an R^2-value of 99.7%. The model that gave predictions with the lowest R^2-value, 68.4%, was the model for nitrogen dioxide. Overall, the results were good in relation to previous studies. With regards to these models, the impact of variables and different measures that can be introduced in the City of Stockholm and on Hornsgatan to improve air quality are discussed.</p>

corrected abstract:
<p>This study aims to investigate the relationship between multiple air pollution and different vehicle variables, such as vehicle year, fuel type and vehicle type, on Hornsgatan in Stockholm. The study intends to answer which factors have the greatest impact on air quality. The implementation is based on the two machine learning algorithms Random Forest and Support Vector Regression, which are compared based on R<sup>2</sup> and RMSE. The models created with Random Forest outperform Support Vector Regression for the various air pollutants. The best performing model was the carbon monoxide model which had an R<sup>2</sup>-value of 99.7%. The model that gave predictions with the lowest R<sup>2</sup>-value, 68.4%, was the model for nitrogen dioxide. Overall, the results were good in relation to previous studies. With regards to these models, the impact of variables and different measures that can be introduced in the City of Stockholm and on Hornsgatan to improve air quality are discussed.</p>

Note - removed the empty paragraph and fixed superscripts
----------------------------------------------------------------------
In diva2:1788396 
abstract is: 
<p>It is important to understand the aerodynamic properties of tensioned cables (e.g. used in suspension bridges and yacht riggings), both for drag reduction and vibrational suppression purposes. In this study, the cross-sectional shape and surface structure of solid cables were investigated in order to improve the performance of sailing racing yachts. The apparent wind angle range 15-60° was identified as the most important for drag reduction. Thereafter, the aerodynamic properties of different shapes and surfaces were investigated in the Reynolds number range 5 x 10^3 ≤ Re ≤ 4 x 10^4, by performing computational fluid dynamics simulations and wind tunnel tests (the aerodynamic forces were measured using load cells). No significant effect of changing the surface roughness could be found for the investigated Reynolds number range. The results were compared to literature values for validation.</p><p>Elliptical shapes with a fineness ratio between 1:1-3:1, together with three complex shapes, were tested. It could be shown that the largest performance gain was obtained for cables with more sail-like aerodynamic properties (for apparent wind angles below 90° a large lift/drag ratio is sought). This study was performed in collaboration with Carbo-Link AG, as an outlook, the manufacturability of carbon fiber reinforced polymer cables in the most aerodynamically efficient shape was explored.</p>

corrected abstract:
<p>It is important to understand the aerodynamic properties of tensioned cables (e.g. used in suspension bridges and yacht riggings), both for drag reduction and vibrational suppression purposes. In this study, the cross-sectional shape and surface structure of solid cables were investigated in order to improve the performance of sailing racing yachts. The apparent wind angle range 15-60° was identified as the most important for drag reduction. Thereafter, the aerodynamic properties of different shapes and surfaces were investigated in the Reynolds number range 5 x 10<sup>3</sup> ≤ 𝑅𝑒 ≤ 4 x 10<<sup>4</sup>, by performing computational fluid dynamics simulations and wind tunnel tests (the aerodynamic forces were measured using load cells). No significant effect of changing the surface roughness could be found for the investigated Reynolds number range. The results were compared to literature values for validation.</p><p>Elliptical shapes with a fineness ratio between 1:1-3:1, together with three complex shapes, were tested. It could be shown that the largest performance gain was obtained for cables with more sail-like aerodynamic properties (for apparent wind angles below 90° a large lift/drag ratio is sought). This study was performed in collaboration with Carbo-Link AG, as an outlook, the manufacturability of carbon fiber reinforced polymer cables in the most aerodynamically efficient shape was explored.</p>
----------------------------------------------------------------------
In diva2:1282825 
abstract is: 
<p>In this thesis, we study the non-symmetric Macdonald polynomials E_λ (x;q,t) at t=0 from a combinatorial point of view, using the combinatorial formula found by J. Haglund, M. Haiman, and N. Loehr. Our primary focus is when λ is a partition. We summarize the known theory about this specialization and prove some new results related to this combinatorial formula. We also define the cyclic sieving phenomenon (CSP). For rectangular λ, we present an instance of cyclic sieving with E_λ (1,q,q^2,...,q^(k-1);1,0) as CSP-polynomial. We also conjecture another instance of CSP with E_λ (1,1,...,1;q,0) as CSP-polynomial. This conjecture generalizes a previously known CSP-triple. Furthermore, we prove this conjecture in the case when is λ an m×2 diagram.</p>

corrected abstract:
<p>In this thesis, we study the non-symmetric Macdonald polynomials E<sub>λ</sub>(𝑥; 𝑞, 𝑡) at 𝑡=0 from a combinatorial point of view, using the combinatorial formula found by J. Haglund, M. Haiman, and N. Loehr. Our primary focus is when λ is a partition. We summarize the known theory about this specialization and prove some new results related to this combinatorial formula. We also define the cyclic sieving phenomenon (CSP). For rectangular λ, we present an instance of cyclic sieving with E<sub>λ</sub>(1,𝑞,𝑞<sup>2</sup>,...,𝑞<sup>𝑘-1</sup>; 1,0) as CSP-polynomial. We also conjecture another instance of CSP with E<sub>λ</sub> (1,1,...,1; 𝑞, 0) as CSP-polynomial. This conjecture generalizes a previously known CSP-triple. Furthermore, we prove this conjecture in the case when is λ an 𝑚 × 2 diagram.</p>
----------------------------------------------------------------------
In diva2:1738124 
abstract is: 
<p>The treatment of spatial infinity is one of the remaining major open problems in the theory of isolated self-gravitating systems. Especially when one wants to model scattering of gravitational radiation in spacetime. In this thesis the conformal theory is used to study simple electromagnetic fields, close to spatial infinity. In particular, the trajectory of the moving Coulomb field is studied in compactified Minkowski space. In the formalism, introduced by Penrose, Minkowski metric is rescaled g = Ω^2η to Einstein’s universe, R × S^3. A dual particle, passing through spatial infinity in Einstein’s Universe, emerges from the conformally extended Coulomb field. The particle pair moves antipodally with respect to the retarded and advanced directions. Furthermore, a more recent treatment of spatial infinity, proposed by Friedrich, is studied in conjunction with the electromagnetic field. In this treatment, spatial infinity is blown-up to a cylinder that is a total characteristic of the spacetime. The Newman-Penrose formalism is central to the theory and is used here to rewrite Maxwell’s equations. The blow-up is linked to the sigma-process, a process used to treat singularities in the theory of differential equations. Boosted space-like curves are linked to points on the cylinder via a bijective function. The Newman-Penrose scalars are studied on the cylinder. Finally, a global treatment of spacetime, using global coordinates for adS2 ×S^2, is proposed for further study of spatial infinity in e.g. numerical codes and Newman-Penrose formalism.</p>

corrected abstract:
<p>The treatment of spatial infinity is one of the remaining major open problems in the theory of isolated self-gravitating systems. Especially when one wants to model scattering of gravitational radiation in spacetime. In this thesis the conformal theory is used to study simple electromagnetic fields, close to spatial infinity. In particular, the trajectory of the moving Coulomb field is studied in compactified Minkowski space. In the formalism, introduced by Penrose, Minkowski metric is rescaled 𝑔 = Ω<sup>2</sup>η to Einstein’s Universe, ℝ × 𝑆<sup>3</sup>. A dual particle, passing through spatial infinity in Einstein’s Universe, emerges from the conformally extended Coulomb field. The particle pair moves antipodally with respect to the retarded and advanced directions. Furthermore, a more recent treatment of spatial infinity, proposed by Friedrich, is studied in conjunction with the electromagnetic field. In this treatment, spatial infinity is blown-up to a cylinder that is a total characteristic of the spacetime. The Newman-Penrose formalism is central to the theory and is used here to rewrite Maxwell’s equations. The blow-up is linked to the sigma-process, a process used to treat singularities in the theory of differential equations. Boosted space-like curves are linked to points on the cylinder via a bijective function. The Newman-Penrose scalars are studied on the cylinder. Finally, a global treatment of spacetime, using global coordinates for adS<sub>2</sub> ×𝑆<sup>2</sup>, is proposed for further study of spatial infinity in e.g. numerical codes and Newman-Penrose formalism.</p>
----------------------------------------------------------------------
In diva2:1341293 
abstract is: 
<p>This report of a conceptual design of an electric driven aircraft was driven with the goal of making the flying sector more environmentally viable. The designated mission was chosen freely for the airplane. The result became a short distance plane with a range of 500 km, seating for eight passengers, primarily aimed towards companies. It was decided that the plane would have its cruising altitude at 4500 m, with a cruising speed of 280 km/h and have a short takeoff and landing distance. The airplane would be able to climb with a vertical speed of 6.67 m/s and have a stall speed of 150 km/h. From the specifications and the assumptions regarding different variables, the fuel weight and total weight was decided to be 2654 kg and 8294 kg respectively. The range of a corresponding aircraft driven with fossil fuels were calculated to be 2070 km. A constraint diagram was then constructed based on five chosen requirements. From this diagram the least power to weight and the highest possible wingloading was determined. A point slightly higher than the least required power to weight was chosen, leading to the engines needing to produce 1900 hp at takeoff. The wing area could be calculated from the decided wingloading and it ended up at 44 m^2 with a wingspan of 23.8 m because of a previously chosen aspect ratio. The length of the fuselage was calculated to be 16.2 m and its effective diameter 2.03 m. Finally an initial layout could be developed where a relationship between the wing and horizontal stabilizer was calculated and the center of gravity for the airplane was placed. The final airplane has a longer wingspan, is heavier and has a shorter range than what similar aircraft that are not driven with electricity have. It can be seen that the cruise speed can be increased above the 280 km/h in the constraint diagram, but the specified requirements were met, which was the main priority.</p>

corrected abstract:
<p>This report of a conceptual design of an electric driven aircraft was driven with the goal of making the flying sector more environmentally viable. The designated mission was chosen freely for the airplane. The result became a short distance plane with a range of 500 km, seating for eight passengers, primarily aimed towards companies. It was decided that the plane would have its cruising altitude at 4500 m, with a cruising speed of 280 km/h and have a short takeoff and landing distance. The airplane would be able to climb with a vertical speed of 6.67 m/s and have a stall speed of 150 km/h.</p><p>From the specifications and the assumptions regarding different variables, the fuel weight and total weight was decided to be 2654 kg and 8294 kg respectively. The range of a corresponding aircraft driven with fossil fuels were calculated to be 2070 km. A constraint diagram was then constructed based on five chosen requirements. From this diagram the least power to weight and the highest possible wingloading was determined. A point slightly higher than the least required power to weight was chosen, leading to the engines needing to produce 1900 hp at takeoff.</p><p>The wing area could be calculated from the decided wingloading and it ended up at 44 m<sup>2</sup> with a wingspan of 23.8 m because of a previously chosen aspect ratio. The length of the fuselage was calculated to be 16.2 m and its effective diameter 2.03 m. Finally an initial layout could be developed where a relationship between the wing and horizontal stabilizer was calculated and the center of gravity for the airplane was placed.</p><p>The final airplane has a longer wingspan, is heavier and has a shorter range than what similar aircraft that are not driven with electricity have. It can be seen that the cruise speed can be increased above the 280 km/h in the constraint diagram, but the specified requirements were met, which was the main priority.</p>
----------------------------------------------------------------------
In diva2:1578887 
abstract is: 
<p>Dryout and Departure from Nucleate boiling (DNB) are utmost thermal-hydraulic concerns for the safety of LWRs. The behavior of two-phase flows at these conditions is still not fully understood. There is at least a need for a good local velocity and void fraction database at these conditions. This database can be exploited by CFD codes, thereby leading to understanding and predicting DNB and boiling crisis. Since these conditions occur in LWR at pressures greater than 70 bar and temperatures above 285 $^oC$, most instrumentations fail at these conditions. So there is a need for developing or optimizing new instruments for this specific objective. This study will look into the application of Hot Wire Anemometry (HWA) for this application. Previous experiments at near saturation conditions were studied, the hurdles of application of HWA in the HWAT loop at KTH were also investigated. Finally, the deposition of thin film on the HWA sensors for protection was studied.</p>

corrected abstract:
<p>Dryout and Departure from Nucleate boiling (DNB) are utmost thermal-hydraulic concerns for the safety of LWRs. The behavior of two-phase flows at these conditions is still not fully understood. There is at least a need for a good local velocity and void fraction database at these conditions. This database can be exploited by CFD codes, thereby leading to understanding and predicting DNB and boiling crisis. Since these conditions occur in LWR at pressures greater than 70 bar and temperatures above 285ºC, most instrumentations fail at these conditions. So there is a need for developing or optimizing new instruments for this specific objective. This study will look into the application of Hot Wire Anemometry (HWA) for this application. Previous experiments at near saturation conditions were studied, the hurdles of application of HWA in the HWAT loop at KTH were also investigated. Finally, the deposition of thin film on the HWA sensors for protection was studied.</p>
----------------------------------------------------------------------
In diva2:1780561 
abstract is: 
<p>In this report, a Fourier spectral approximation of the solution to the linear convection--diffusion equation for initial conditions of different smoothness, and for Burger's equation for the initial condition f(x) = sin(x), was constructed, and implemented. Three different filters (Cesàro, Lanczos, and 4--th order exponential cutoff) were either applied to the initial condition or to the numerical approximation after the last integration step has been performed. The local error was then calculated in order to compare the performance of the three filters. Filtering was found to improve the local accuracy of the numerical approximation for the linear convection--diffusion equation for a diffusivity constant of 0 and for initial conditions of low smoothness, i.e. discontinuous functions or functions in C^0 or C^1. For initial conditions of infinite smoothness and a larger diffusivity constant, filtering did not improve the local accuracy but rather made it worse. No significant difference was found between filtering the initial condition and filtering after the final integration step. The 4--th order exponential cutoff performed best overall for most initial conditions. For Burger's equation, filtering only improved the local accuracy when applied after the final integration step and only if a discontinuity had started to form. For a discontinuity to form, the diffusivity constant furthermore needed to be sufficiently small. In conclusion, filtering is applicable when solving the linear convection--diffusion equation for a low diffusivity constant and initial conditions of low smoothness. For Burger's equation, filtering is applicable after a discontinuity starts to form. These results were in line with the theory presented in the report.</p>

corrected abstract:
<p>In this report, a Fourier spectral approximation of the solution to the linear convection-diffusion equation for initial conditions of different smoothness, and for Burger's equation for the initial condition 𝑓(𝑥) = sin(𝑥), was constructed, and implemented. Three different filters (Cesàro, Lanczos, and 4-th order exponential cutoff) were either applied to the initial condition or to the numerical approximation after the last integration step has been performed. The local error was then calculated in order to compare the performance of the three filters. Filtering was found to improve the local accuracy of the numerical approximation for the linear convection-diffusion equation for a diffusivity constant of 0 and for initial conditions of low smoothness, i.e. discontinuous functions or functions ∈ 𝐶<sup>0</sup> or 𝐶<sup>1</sup>. For initial conditions of infinite smoothness and a larger diffusivity constant, filtering did not improve the local accuracy but rather made it worse. No significant difference was found between filtering the initial condition and filtering after the final integration step. The 4-th order exponential cutoff performed best overall for most initial conditions. For Burger's equation, filtering only improved the local accuracy when applied after the final integration step and only if a discontinuity had started to form. For a discontinuity to form, the diffusivity constant furthermore needed to be sufficiently small. In conclusion, filtering is applicable when solving the linear convection-diffusion equation for a low diffusivity constant and initial conditions of low smoothness. For Burger's equation, filtering is applicable after a discontinuity starts to form. These results were in line with the theory presented in the report.</p>
----------------------------------------------------------------------
In diva2:1579559 
abstract is: 
<p>While there are existing methods of gamma ray-track reconstruction in specialized detectors such as AGATA, including backtracking and clustering, it is naturally of interest to diversify the portfolio of available tools to provide us viable alternatives. In this study some possibilities found in the field of machine learning were investigated, more specifically within the field of graph neural networks.</p><p>In this project there was attempt to reconstruct gamma tracks in a germanium solid using data simulated in Geant4. The data consists of photon energies below the pair production limit and so we are limited to the processes of photoelectric absorption and Compton scattering. The author turned to the field of graph networks to utilize its edge and node structure for data of such variable input size as found in this task. A graph neural network (GNN) was implemented and trained on a variety of gamma multiplicities and energies and was subsequently tested in terms of various accuracy parameters and generated energy spectra.</p><p>In the end the best result involved an edge classifier trained on a large dataset containing a 10^6 tracks bundled together into separate events to be resolved. The network was capable of recalling up to 95 percent of the connective edges for the selected threshold in the infinite resolution case with a peak-to-total ratio of 68 percent for a set of packed data with a model trained on simulated data including realistic uncertainties in both position and energy.</p>

corrected abstract:
<p>While there are existing methods of gamma ray-track reconstruction in specialized detectors such as AGATA, including backtracking and clustering, it is naturally of interest to diversify the portfolio of available tools to provide us viable alternatives. In this study some possibilities found in the field of machine learning were investigated, more specifically within the field of graph neural networks.</p><p>In this project there was attempt to reconstruct gamma tracks in a germanium solid using data simulated in Geant4. The data consists of photon energies below the pair production limit and so we are limited to the processes of photoelectric absorption and Compton scattering. The author turned to the field of graph networks to utilize its edge and node structure for data of such variable input size as found in this task. A graph neural network (GNN) was implemented and trained on a variety of gamma multiplicities and energies and was subsequently tested in terms of various accuracy parameters and generated energy spectra.</p><p>In the end the best result involved an edge classifier trained on a large dataset containing a 10<sup>6</sup> tracks bundled together into separate events to be resolved. The network was capable of recalling up to 95 percent of the connective edges for the selected threshold in the infinite resolution case with a peak-to-total ratio of 68 percent for a set of packed data with a model trained on simulated data including realistic uncertainties in both position and energy.</p>
----------------------------------------------------------------------
In diva2:1568137 
abstract is: 
<p>Supernovae (SNe) are explosions following the death of massive stars. Core-collapse supernovae (CCSNe) occur when the heavy iron core of these stars collapse in on themselves. The resulting remnant of the core of a CCSN is a compact object: either a black hole or a neutron star. During the collapse and following explosion, massive amounts of energy and material are expelled. The compact objects emit high-energy radiation. With X-ray astronomy, we can observe it and study the processes behind these events. In this thesis, we determine a limit on the X-ray luminosity of SN 2002ap, and constrain the parameters for the magnetic field of the central object, potentially a neutron star. We model the absorption of the radiation by the material in the surrounding area, the so-called SN ejecta, as well as the absorption by the interstellar medium (ISM). We construct the model using the spectral fitting program XSPEC. Assumptions about the abundance of X-ray absorbing elements in the ejecta and ISM are based on earlier models and the explosion energy is taken from previous estimations. The mass of the ejecta is assumed to be 2.5-5 solar masses and the distance 9.34 Mpc. We compare the absorption model to the data taken by the Chandra telescope in 2018. From this comparison, we determine the maximum luminosity to be L &lt; 2*10^40 erg/s and constrain the magnetic field to a minimum of B &gt; 3*10^13 G.</p>

corrected abstract:
<p>Supernovae (SNe) are explosions following the death of massive stars. Core-collapse supernovae (CCSNe) occur when the heavy iron core of these stars collapse in on themselves. The resulting remnant of the core of a CCSN is a compact object: either a black hole or a neutron star. During the collapse and following explosion, massive amounts of energy and material are expelled. The compact objects emit high-energy radiation. With X-ray astronomy, we can observe it and study the processes behind these events. In this thesis, we determine a limit on the X-ray luminosity of SN 2002ap, and constrain the parameters for the magnetic field of the central object, potentially a neutron star. We model the absorption of the radiation by the material in the surrounding area, the so-called SN ejecta, as well as the absorption by the interstellar medium (ISM). We construct the model using the spectral fitting program XSPEC. Assumptions about the abundance of X-ray absorbing elements in the ejecta and ISM are based on earlier models and the explosion energy is taken from previous estimations. The mass of the ejecta is assumed to be 2.5-5 M<sub>☉</sub> and the distance 9.34 Mpc. We compare the absorption model to the data taken by the Chandra telescope in 2018. From this comparison, we determine the maximum luminosity to be 𝐿 ≲ 2 × 10<sup>40</sup> erg s<sup>-1</sup> and constrain the magnetic field to a minimum of 𝐵 ≳ 3 × 10<sup>13</sup> G.</p>
----------------------------------------------------------------------
In diva2:1334784 
abstract is: 
<p>Given an integer n, this text explores different ways of finding factors of n, with focus on Shanks’ Class Group Method as described by Henri Cohen in A Course in Algebraic Number Theory, although brief summaries of Pollard’s p - 1 and rho algorithms for factoring an integer are given as well. The class group is introduced as a set of equivalence classes of fractional ideals in imaginary quadratic fields before an isomorphism with a set of equivalence classes of binary quadratic forms of a given discriminant, D, is given. This isomorphism gives rise to an abstract group operation under which elements of order 2, so called ambiguous forms, constitute a factorisation of n. The Class Group Method describes how to find ambiguous elements and factorise n in O(|D|^(1/4)) time, where the main problem lies in finding the exponent of the class group, which is done using Shanks’ Baby-Steps-Giant-Steps method for finite groups. Implementations in Python 3 are found in the appendix.</p>

corrected abstract:
<p>Given an integer 𝑛, this text explores different ways of finding factors of 𝑛, with focus on Shanks’ Class Group Method as described by Henri Cohen in <em>A Course in Algebraic Number Theory</em>, although brief summaries of Pollard’s 𝑝 - 1 and ρ algorithms for factoring an integer are given as well. The class group is introduced as a set of equivalence classes of fractional ideals in imaginary quadratic fields before an isomorphism with a set of equivalence classes of binary quadratic forms of a given discriminant, 𝐷, is given. This isomorphism gives rise to an abstract group operation under which elements of order 2, so called ambiguous forms, constitute a factorisation of 𝑛. The Class Group Method describes how to find ambiguous elements and factorise 𝑛 in &Oscr;(|𝐷|<sup>1/4</sup>) time, where the main problem lies in finding the exponent of the class group, which is done using Shanks’ Baby-Steps-Giant-Steps method for finite groups. Implementations in Python 3 are found in the appendix.</p>
----------------------------------------------------------------------
In diva2:1341556 
abstract is: 
<p>The purpose of this bachelor thesis is to design an electric powered commercial short-range aircraft that is set to take off in 2030 with reasonable technical advancement assumptions made. The aircraft is designed with the ATR 42-500 as inspiration and has therefore similar requirements. The aircraft has a payload of 5070 kg and cruises at 7600 m above sea level. It has a max speed of Mach 0.5 and a stall speed of 41 m/s. Climb rate is 560 m/min, takeoff distance is 1165 m and landing distance is 960 m. The conceptually designed aircraft has a range of 400 km that is approximately the distance London-Amsterdam and is able to carry up to 48 passengers in a two by two seat configuration. Batteries are expected to improve with 30 % during the next ten years resulting in a maximum takeoff weight of 19900 kg, where 3220 kg is battery weight. Fuel powered it has a maximum takeoff weight of 19200 kg and a fuel weight of 2900 kg. The power needed for propulsion was found to be 4.18 MW which would be equally divided over the engines that drive the two propellers. These are positioned one on each wing. The 26 m long aircraft is equipped with an unswept high mounted wing with a wingspan of 29 m and a wing reference area of 75 m^2. The horizontal stabilizer is 12 m^2 and the vertical stabilizer is 11 m^2.</p>

corrected abstract:
<p>The purpose of this bachelor thesis is to design an electric powered commercial short range aircraft that is set to take-off in 2030 with reasonable technical advancement assumptions made.</p><p>The aircraft is designed with the ATR 42-500 as inspiration and has therefore similar requirements. The aircraft has a payload of 5070 kg and cruises at 7600 m above sea level. It has a max speed of Mach 0.5 and a stall speed of 41 m/s. Climb rate is 560 m/min, take-off distance is 1165 m and landing distance is 960 m.</p><p>The conceptually designed aircraft has a range of 400 km that is approximately the distance London-Amsterdam and is able to carry up to 48 passengers in a two by two seat configuration. Batteries are expected to improve with 30 % during the next ten years resulting in a maximum take-off weight of 19900 kg, where 3220 kg is battery weight. Fuel powered it has a maximum take-off weight of 19200 kg and a fuel weight of 2900 kg.</p><p>The power needed for propulsion was found to be 4.18 MW which would be equally divided over the engines that drive the two propellers. These are positioned one on each wing.</p><p>The 26 m long aircraft is equipped with an unswept high mounted wing with a wingspan of 29 m and a wing reference area of 75 m<sup>2</sup>. The horizontal stabilizer is 13 m<sup>2</sup> and the vertical stabilizer is 11 m<sup>2</sup>.</p>
----------------------------------------------------------------------
In diva2:1827769 
abstract is: 
<p>This study investigated the relationship between Sweden’s CO2e (Carbon Dioxide Equivalent) emissions and key macroeconomic factors, for the period 2008Q1- 2022Q3. The aim was to enhance the understanding of the link between macroeconomic factors and greenhouse gas emissions in a post-industrial economy, using multiple regression analysis. The study identified several significant macroeconomic factors affecting CO2e emissions and examined the extent to which these variables explain the fluctuations in Sweden’s emissions. Additionally, the study assessed the validity of the Environmental Kuznets Curve and Porter Hypothesis within Sweden’s environmental context. In the study, two multiple regression models were developed. Model 1 had an R^2 of 0.90, using the macroeconomic variables Industry Fuel Consumption, Population, Net Export, and Oil Prices. However, since the first model displayed moderate autocorrelation, a second model was also built by introducing a lagged dependent variable which yielded an R^2 of 0.92.</p>

corrected abstract:
<p>This study investigated the relationship between Sweden’s CO<sub>2</sub>e (Carbon Dioxide Equivalent) emissions and key macroeconomic factors, for the period 2008Q1- 2022Q3. The aim was to enhance the understanding of the link between macroeconomic factors and greenhouse gas emissions in a post-industrial economy, using multiple regression analysis. The study identified several significant macroeconomic factors affecting CO<sub>2</sub>e emissions and examined the extent to which these variables explain the fluctuations in Sweden’s emissions. Additionally, the study assessed the validity of the Environmental Kuznets Curve and Porter Hypothesis within Sweden’s environmental context. In the study, two multiple regression models were developed. Model 1 had an 𝑅<sup>2</sup> of 0.90, using the macroeconomic variables Industry Fuel Consumption, Population, Net Export, and Oil Prices. However, since the first model displayed moderate autocorrelation, a second model was also built by introducing a lagged dependent variable which yielded an 𝑅<sup>2</sup> of 0.92.</p>
----------------------------------------------------------------------
In diva2:1827787 
abstract is: 
<p>This study aims to identify whether a relationship between ESG performance and financial performance exists for Nordic publicly-listed companies, by conducting a multiple linear regression analysis. Also, it will be observed which (if any) ESG variables are of relevance.</p><p>The regression analysis conducted in this study arrives at the conclusion that there is a relationship between ESG performance and financial performance. However, the models have low explanatory power, with Adjusted R^2 values of 0.36 for the accounting-based financial measure Return on Assets (ROA), and 0.30 for the market-based financial measure Tobin´s Q.In both the ROA and Tobin's Q model, social variables are the most significant. Supplier evaluation disclosure is the only variable that is highly significant and positively correlated to both ROA and Tobin's Q. Consistent with previous literature, our results show that female board participation is positively correlated with ROA. The results also show that ROA correlates negatively with compensation of board members and senior executives being linked to environmental and social factors. In conclusion, some variables were identified that are significant for financial performance. However, the overall explanatory power of the model is low. It is suggested that future studies adopt a materiality approach.</p>

corrected abstract:
<p>This study aims to identify whether a relationship between ESG performance and financial performance exists for Nordic publicly-listed companies, by conducting a multiple linear regression analysis. Also, it will be observed which (if any) ESG variables are of relevance.</p><p>The regression analysis conducted in this study arrives at the conclusion that there is a relationship between ESG performance and financial performance. However, the models have low explanatory power, with Adjusted 𝑅<sup>2</sup> values of 0.36 for the accounting-based financial measure Return on Assets (ROA), and 0.30 for the market-based financial measure Tobin´s Q. In both the ROA and Tobin's Q model, social variables are the most significant. Supplier evaluation disclosure is the only variable that is highly significant and positively correlated to both ROA and Tobin's Q. Consistent with previous literature, our results show that female board participation is positively correlated with ROA. The results also show that ROA correlates negatively with compensation of board members and senior executives being linked to environmental and social factors. In conclusion, some variables were identified that are significant for financial performance. However, the overall explanatory power of the model is low. It is suggested that future studies adopt a materiality approach.</p>
----------------------------------------------------------------------
In diva2:690751 
abstract is: 
<p>A one phase Hele-Shaw flow, described by a domain D(t) (t represents time) in the plane is the flow of a liquid injected at a constant rate in the separation between two narrowly separated parallel planes. This thesis deals with the formulation and proof of existence for a multiple phase Hele-Shaw flow in arbitrary dimension R^n exhibiting separation of the phases. A smooth version of the problem, depending on a small parameter epsilon, has been considered. Solutions to this smooth problem approximate the multiple-phase Hele-Shaw flow. We show that the smooth problem has a solution using a variational technique with functions u=u(t;eps) in the Sobolev space H_0^1 describing the Hele-Shaw flow with D(t)=support(u(t;eps)). As we let the parameter epsilon tend to zero we get that the solutions u(t;eps) converges weakly to a family of functions u(t) in the same Sobolev space which describe the desired Hele-Shaw flow. Furthermore the phases represented by the components of u(t) are separated in the sense that the overlap of any two distinct phases has vanishing n-dimensional Lebesgue measure. </p><p>We also touch upon a formulation of the multiple phase Hele-Shaw flow which would, beyond separation of the phases, provide freezing of the intersecting boundary of two phases. This formulation of the problem tries to incorporate memory in to the system via means of an integration over previous states. </p>

corrected abstract:
<p>A one phase Hele-Shaw flow, described by a domain</p> <p>𝐷<sub>𝑡</sub> = {𝑥 : 𝑢<sub>𝑡</sub>(𝑥) > 0} ∪ 𝐷<sub>0</sub></p> for some functions u<sub>𝑡</sub> : ℝ<sup>n</sup> ⊃ Ω → ℝ and 𝑡 ≥ 0, is the flow of a liquid injected at a constant rate in the separation between two narrowly separated parallel planes.  This thesis deals with the formulation and proof of existence for a multiple phase Hele-Shaw flow exhibiting separation of the phases. A smooth version of the problem, depending on the parameter ε > 0, has been considered giving rise to the equations in 𝐻<sup>−1</sup>(Ω) for the phases 𝑢<sup>𝑖</sup></p>
<p>(0.1)&nbsp;&nbsp;&nbsp;&nbsp; −∆𝑢<sup>𝑖</sup> + (1 − χ<sub>𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span></sub>)β<sub>ε</sub>(𝑢<sup>𝑖</sup>) = 𝑡µ<sup>𝑖</sup> − (1/ε)∑<sub>𝑗&ne;𝑖</sub> B<sub>ε</sub> (𝑢<sup>𝑗</sup>)β<sub>ε</sub>(𝑢<sup>𝑖</sup>) for 𝑖 = 1, . . . , m</p>
<p>where 𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span> ⊂ Ω ⊂ ℝ<sup>n</sup>, µ<sup> </sup> ∈ H<sup>−1</sup>(Ω), β<sub>ε</sub> is a mollification of the Heaviside step function, 𝐵(𝑠) = &int;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑠</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span> β<sub>ε</sub>(𝑠&prime;)𝑑𝑠&prime;, 𝑢 = (𝑢<sup>𝑖</sup>)<sub>𝑖 = 1, . . . , m</sub> a vector with components 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω) and 𝐻<sup>−1</sup>(Ω) is the dual of the Sobolev space 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω). We show that the smooth problem has a solution [0, ∞) &in; 𝑡 ↦ 𝑢<sub>𝑡</sub>;ε ∈ 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω; ℝ<sup>m</sup>) depending on ε using a variational technique. Upon letting ε → 0<sup>+</sup>, for fixed 𝑡, the solution 𝑢<sub>𝑡;ε</sub> converges weakly to some 𝑢<sub>𝑡</sub> ∈ 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω; ℝ<sup>m</sup>) solving</p>
<p>(0.2)&nbsp;&nbsp;&nbsp;&nbsp; −∆𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> + (1 − χ<sub>𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span></sub>)χ<sub>{𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.6rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> &gt; 0}</sub> = 𝑡µ<sup>𝑖</sup> - κ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span>  in H<sup>−1</sup>(Ω),</p><p>for some non-negative elements κ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> ∈ H<sup>−1</sup>(Ω) having support on ∂𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span>. Furthermore the phases represented by the components of 𝑢<sub>𝑡</sub> are separated in the sense that the overlap of any two distinct phases has vanishing n-dimensional Lebesgue measure i.e.</p>
<p>(0.3) &nbsp;&nbsp;&nbsp;&nbsp; ∣supp 𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> &cap; supp 𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑗</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span>∣ = 0 for 𝑖 &ne; 𝑗.</p>
<p>We also touch upon a formulation of the multiple phase Hele-Shaw flow which would, beyond separation of the phases, provide freezing of the intersecting boundary &Gamma;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖𝑗</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> = ∂𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> &cap; ∂𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑗</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> of two phases. This formulation of the problem tries to incorporate memory in to the system via means of an integration over previous states.</p>

----------------------------------------------------------------------
In diva2:1780566 
abstract is: 
<p>This study evaluates the feasibility of reusing lightweight solar sails in order to transport 1.69 * 10^6 sunshades, made out of occulting membranes with free-standing SiO2 nanotube films, to the adjusted sun-Earth Lagrange point, L1'. The purpose of the study was therefore to evaluate if this method is sufficient enough to lower Earth's average surface temperature by 1 degree C within a reasonable time frame, due to the rapid climate change, and compare the total launch mass to previously proposed methods. Two mission times of 10 years and 15 years were used, and three different starting altitudes, the GEO, MEO and LEO orbits, were investigated. The results showed that the method in this study was feasible for all combinations of starting altitudes and mission times. The solution where the mission time was set to 15 years and where the starting altitude was set to the GEO orbit, resulted in a launch mass of 11\% of the mass of the previously proposed solution. Furthermore, the investigation showed that high altitude starting orbits and long mission times resulted in a lower launch mass. However, in order to fulfill the goal of reducing the average temperature by 1 degree C in a reasonable time frame, the mission time cannot be too long. Finally, the results and calculations in this study are partially based on assumptions and simplifications, and therefore the results should be considered as approximations and not exact analytical solutions.</p>

corrected abstract:
<p>This study evaluates the feasibility of reusing lightweight solar sails in order to transport 1.69 · 10<sup>6</sup> sunshades, made out of occulting membranes with free-standing <em>SiO<sub>2</sub></em> nanotube films, to the adjusted sun-Earth Lagrange point, 𝐿1&prime;. The purpose of the study was therefore to evaluate if this method is sufficient enough to lower Earth's average surface temperature by 1ºC within a reasonable time frame, due to the rapid climate change, and compare the total launch mass to previously proposed methods. Two mission times of 10 years and 15 years were used, and three different starting altitudes, the GEO, MEO and LEO orbits, were investigated. The results showed that the method in this study was feasible for all combinations of starting altitudes and mission times. The solution where the mission time was set to 15 years and where the starting altitude was set to the GEO orbit, resulted in a launch mass of 11% of the mass of the previously proposed solution. Furthermore, the investigation showed that high altitude starting orbits and long mission times resulted in a lower launch mass. However, in order to fulfill the goal of reducing the average temperature by 1ºC in a reasonable time frame, the mission time cannot be too long. Finally, the results and calculations in this study are partially based on assumptions and simplifications, and therefore the results should be considered as approximations and not exact analytical solutions.</p>
----------------------------------------------------------------------
In diva2:1851005 
abstract is: 
<p>Renormalization is a powerful tool showing up in different contexts of mathematics and physics. In the context of circle diffeomorphisms, the renormalization operator acts like a microscope and allows to study the dynamics of a circle diffeomorphism on a small scale. The convergence of renormalization leads to a proof of the so-called rigidity theorem, which classifies the dynamics of circle diffeomorphisms geometrically: the conjugacy between $C^3$ circle diffeomorphism with Diophantine rotation number and the corresponding rotation is $C^1$.</p><p>In this thesis, we define the renormalization of circle diffeomorphisms and study its dynamics. In particular, we prove that the renormalization of orientation preserving $C^3$ circle diffeomorphisms with irrational rotation number of bounded type converges to rotations at exponential speed. We also introduce the necessary relevant concepts such as rotation number, distortion and non-linearity and discuss some of their properties.</p><p>This thesis is a summary and supplement to the book One-Dimensional Dynamics: from Poincaré to Renormalization.</p>

corrected abstract:
<p>Renormalization is a powerful tool showing up in different contexts of mathematics and physics. In the context of circle diffeomorphisms, the renormalization operator acts like a microscope and allows to study the dynamics of a circle diffeomorphism on a small scale. The convergence of renormalization leads to a proof of the so-called rigidity theorem, which classifies the dynamics of circle diffeomorphisms geometrically: the conjugacy between 𝐶<sup>3</sup> circle diffeomorphism with Diophantine rotation number and the corresponding rotation is 𝐶<sup>1</sup>.</p><p>In this thesis, we define the renormalization of circle diffeomorphisms and study its dynamics. In particular, we prove that the renormalization of orientation preserving 𝐶<sup>3</sup> circle diffeomorphisms with irrational rotation number of bounded type converges to rotations at exponential speed. We also introduce the necessary relevant concepts such as rotation number, distortion and non-linearity and discuss some of their properties.</p><p>This thesis is a summary and supplement to the book <em>One-Dimensional Dynamics: From Poincaré to Renormalization</em>.</p>
----------------------------------------------------------------------
In diva2:558519 
abstract is: 
<p>How should n points be distributed in a given region F in R^d such that they are separated as much as possible?</p><p>This general problem is studied in this paper, for some combinations of F, d, n, and the ways one can state the problem mathematically. Some numerical optimization methods are suggested and tested, both on the point separation problem and the closely related circle packing problem. The results are compared with some known analytical results. The main conclusion is that the suggested numerical methods are useful general tools to obtain optimal solutions to the considered problems.</p>

corrected abstract:
<p>How should 𝑛 points be distributed in a given region <img style="display: inline;" src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?\mathcal{F}" /> in ℝ<sup>𝑑</sup> such that they are separated as much as possible? This general problem is studied in this paper, for some combinations of <img style="display: inline;" src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?\mathcal{F}" />, 𝑑, 𝑛, and the ways one can state this problem mathematically. Some numerical optimization methods are suggested and tested, both on the point separation problem and the closely related circle packing problem. The results are compared with some known analytical results. The main conclusion is that the suggested numerical methods are useful general tools to obtain optimal solutions to the considered problems.</p>
----------------------------------------------------------------------
In diva2:1806023 
abstract is: 
<p>A Linear Program is a problem where the goal is to maximize a linear function subject to a set of linear inequalities. Geometrically, this can be rephrased as finding the highest point on a polyhedron. The Simplex method is a commonly used algorithm to solve Linear Programs. It traverses the vertices of the polyhedron, and in each step, it selects one adjacent better vertex and moves there. There can be multiple vertices to choose from, and therefore the Simplex method has different variants deciding how the next vertex is selected. One of the most natural variants is Random Edge, which in each step of the Simplex method uniformly at random selects one of the better adjacent vertices.</p><p>It is interesting and non-trivial to study the complexity of variants of the Simplex method in the number of variables, d, and inequalities, N. In 2011, Friedmann, Hansen, and Zwick found a class of Linear Programs for which the Random Edge algorithm is subexponential with complexity 2^Ω(N^(1/4)), where d=Θ(N). Previously all known lower bounds were polynomial. We give an improved lower bound of 2^Ω(N^(1/2)), for Random Edge on Linear Programs where d=Θ(N).</p><p>Another well studied variant of the Simplex method is Random Facet. It is upper bounded by 2^O(N^(1/2)) when d=Θ(N). Thus we prove that Random Edge is not faster than Random Facet on Linear Programs where d=Θ(N).</p><p>Our construction is very similar to the previous construction of Friedmann, Hansen and Zwick. We construct a Markov Decision Process which behaves like a binary counter with linearly many levels and linearly many nodes on each level. The new idea is a new type of delay gadget which can switch quickly from 0 to 1 in some circumstances, leading to fewer nodes needed on each level of the construction. The key idea is that it is worth taking a large risk of getting a small negative reward if the potential positive reward is large enough in comparison.</p>

corrected abstract:
<p>A Linear Program is a problem where the goal is to maximize a linear function subject to a set of linear inequalities. Geometrically, this can be rephrased as finding the highest point on a polyhedron. The Simplex method is a commonly used algorithm to solve Linear Programs. It traverses the vertices of the polyhedron, and in each step, it selects one adjacent better vertex and moves there. There can be multiple vertices to choose from, and therefore the Simplex method has different variants deciding how the next vertex is selected. One of the most natural variants is Random Edge, which in each step of the Simplex method uniformly at random selects one of the better adjacent vertices.</p><p>It is interesting and non-trivial to study the complexity of variants of the Simplex method in the number of variables, 𝑑, and inequalities, 𝑁. In 2011, Friedmann, Hansen, and Zwick found a class of Linear Programs for which the Random Edge algorithm is subexponential with complexity 2<sup>Ω(𝑁<sup>1/4</sup>)</sup>, where 𝑑=Θ(𝑁). Previously all known lower bounds were polynomial. We give an improved lower bound of 2<sup>Ω(√<span style="text-decoration: overline;">𝑁</span>)</sup>, for Random Edge on Linear Programs where 𝑑=Θ(𝑁).</p><p>Another well studied variant of the Simplex method is Random Facet. It is upper bounded by 2<sup>O(√<span style="text-decoration: overline;">𝑁</span>)</sup> when 𝑑=Θ(𝑁). Thus we prove that Random Edge is not faster than Random Facet on Linear Programs where 𝑑=Θ(𝑁).</p><p>Our construction is very similar to the previous construction of Friedmann, Hansen and Zwick. We construct a Markov Decision Process which behaves like a binary counter with linearly many levels and linearly many nodes on each level. The new idea is a new type of delay gadget which can switch quickly from 0 to 1 in some circumstances, leading to fewer nodes needed on each level of the construction. The key idea is that it is worth taking a large risk of getting a small negative reward if the potential positive reward is large enough in comparison.</p>
----------------------------------------------------------------------
In diva2:1801622 
abstract is: 
<p>It is established that paper properties depend on the loading rate. The rule of thumb is that the in-plane strength and stiffness increases about 10\%, when the strain rate increases by a factor of 10. Converting of paperboard into packages requires creasing of the paperboard followed by folding to make 3 dimensional packages. Crease response is controlled by in-plane properties, which contribute to the loading and the spring back of the crease which gives the paperboard its final geometry. </p><p>This work aims to characterize the rate and time dependent properties of paper, done by tensile testing at high strain rates of up-to 100 000 mm/min using an electro-mechanical testing machine. Also investigated in this work are the rate dependence and characterization of the plies for a deeper understanding of the contributing factors to this rate dependence. At the end of this work the aim is to retrieve the rate dependent behavior of the materials and compare them with the existing rule of thumb.</p><p>In this work it was concluded that the rule of thumb is accurate for the ultimate strength of the material in the strain rate range of 10^4 to 10^1 strains/second. It was also observed that stiffness of the material increases, but at a rate lower than the stated rule of thumb.</p>

corrected abstract:
<p>It is established that paper properties depend on the loading rate. The rule of thumb is that the in-plane strength and stiffness increases about 10%, when the strain rate increases by a factor of 10. Converting of paperboard into packages requires creasing of the paperboard followed by folding to make 3 dimensional packages. Crease response is controlled by in-plane properties, which contribute to the loading and the spring back of the crease which gives the paperboard its final geometry.</p><p>This work aims to characterise the rate and time dependent properties of paper, done by tensile testing at high strain rates of up-to 100 000 mm/min using an electro-mechanical testing machine. Also investigated in this work are the rate dependence and characterization of the plies for a deeper understanding of the contributing factors to this rate dependence. At the end of this work the aim is to retrieve the rate dependent behaviour of the materials and compare them with the existing rule of thumb.</p><p>In this work it was concluded that the rule of thumb is accurate for the ultimate strength of the material in the strain rate range of 10<sup>4</sup> to 10<sup>1</sup> strains/second. It was also observed that stiffness of the material increases but at a rate lower than the stated rule of thumb.</p>
----------------------------------------------------------------------
In diva2:1626655 
abstract is: 
<p>The failure of the Standard Model of particle physics to predict neutrino masses invites us to amend it. We have no reason to believe that the interactions currently described by the gauge group are the whole picture, nor should we expect that the number of fermions hitherto observed is correct. In this thesis, we explore two amendments to the Standard Model, and examine whether either of these is consistent with measured data. Firstly, we consider the sterile neutrino, which has no interactions described by the Standard Model. We examine the effect of this new fermion on the neutrino oscillation probabilities and present what could be a detectable signal in the TeV energy range. Secondly, we consider interactions beyond the Standard Model, possibly stemming from a higher-order theory. We show how the parameters of these Non-Standard Interactions (NSI) can modify the oscillation probabilities and within which energy range we expect to discern this signal. We use data from and simulate event counts in two Cherenkov detectors: IceCube and DeepCore. Moreover, we generate data and simulate a proposed upgrade of the DeepCore detector: PINGU. Using IceCube track events, we obtain best-fit values ∆m^2_41 = 0.01eV^2 and θ_24 = 0.67 for our sterile neutrino hypothesis at a p-value of 20%, which is not statistically significant. Hence, we found no evidence of a sterile neutrino in IceCube data. Moreover, we were unable to distinguish a signal from θ_34 in our IceCube simulation. We obtain stringent bounds on the NSI parameters and compare those to previous results in literature. We show that PINGU is expected to narrow the bound further on ε_μτ , especially by considering a joint analysis with IceCube and DeepCore. Finally, we see that an anti-correlation between ε_eμ and ε_eτ at probability level was propagated down to event level, which we expect to be observable by PINGU.</p>

corrected abstract:
<p>The failure of the Standard Model of particle physics to predict neutrino masses invites us to amend it. We have no reason to believe that the interactions currently described by the gauge group are the whole picture, nor should we expect that the number of fermions hitherto observed is correct. In this thesis, we explore two amendments to the Standard Model, and examine whether either of these are consistent with measured data.</p><p>Firstly, we consider the sterile neutrino, which has no interactions described by the Standard Model. We examine the effect of this new fermion on the neutrino oscillation probabilities, and present what could be a detectable signal in the TeV energy range. Secondly, we consider interactions beyond the Standard Model, possibly stemming from a higher-order theory. We show how the parameters of these Non-Standard Interactions (NSI) can modify the oscillation probabilities and within which energy range we expect to discern this signal. We use data from and simulate event counts in two Cherenkov detectors: IceCube and DeepCore. Moreover, we generate data and simulate a proposed upgrade of the DeepCore detector: PINGU.</p><p>Using IceCube track events, we obtain best-fit values ∆m<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>2</sup><sub>41</sub></span></span> = 0.01eV<sup>2</sup> and θ<sub>24</sub> = 0.67 for our sterile neutrino hypothesis at a p-value of 20%, which is not statistically significant. Hence, we found no evidence of a sterile neutrino in IceCube data. Moreover, we were unable to distinguish a signal from θ<sub>34</sub> in our IceCube simulation. We obtain stringent bounds on the NSI parameters and compare those to previous results in literature. We show that PINGU is expected to narrow the bound further on ε<sub>μτ</sub>, especially by considering a joint analysis with IceCube and DeepCore. Finally, we see that an anti-correlation between ε<sub>eμ</sub> and ε<sub>eτ</sub> at probability level was propagated down to event level, which we expect to be observable by PINGU.</p>
----------------------------------------------------------------------
In diva2:1652347 
abstract is: 
<p>CUBES is an X-ray detector that will be placed aboard the KTH 3U CubeSat mission, MIST. Its purpose is to detect high energy X-rays as well as to test various components in a space environment. Two CUBES will be placed on the satellite. Each CUBES consists of a printed circuit board (PCB) with three multi pixel photon counters (MPPCs). On top of these, three Germanium Aluminium Gadolinium Garnet (GAGG) scintillators are glued. These GAGG scintillators are of the dimension 1X1X1 cm^3 and are covered with PTFE tape and an opaque potting compound to prevent photons from leaving the scintillator. The MPPCs consists of a large amount of semi conductors operated in Geiger mode. The data is processed by an application specific integrated circuit (ASIC). In order to prepare the CUBES instrument for satellite flight, energy and thermal characterisation have been performed. The energy range was determined to be 40-1200 keV. The detector system shows linear behaviour and operates stably in a temperature range of -20 °C to +30 °C. The preparation of the boards and test results are presented in this thesis.</p>

corrected abstract:
<p>CUBES is an X-ray detector that will be placed aboard the KTH 3U CubeSat mission, MIST. Its purpose is to detect high energy X-rays as well as to test various components in a space environment. Two CUBES will be placed on the satellite. Each CUBES consists of a printed circuit board (PCB) with three multi pixel photon counters (MPPCs). On top of these, three Gd<sub>3</sub>Al<sub>2</sub>Ga<sub>3</sub>O<sub>12</sub> (GAGG) scintillators are glued. These GAGG scintillators are of the dimension 1 × 1 × 1 cm<sup>3</sup> and are covered with PTFE tape and an opaque potting compound to prevent photons from leaving the detector. The MPPCs consists of a large amount of semi conductors operated in Geiger mode. The data is processed by an application specific integrated circuit (ASIC). In order to prepare the CUBES instrument for satellite flight, energy and thermal characterisation have been performed. The energy range was determined to be 40-1200 keV. The detector system shows linear behaviour and operates stably in a temperature range of -20 °C to +30 °C. The preparation of the boards and test results are presented in this thesis.</p>
----------------------------------------------------------------------
In diva2:1445991 
abstract is: 
<p>During 2018, the Public Transport Administration (Trafikförvaltningen) in the Stockholm region spent approximately 2.2 billion SEK on new infrastructure investments related to the public transport system, many of which were based on their public transport models. The previously used method for validating these models has lacked scientific rigour, efficiency and a systematic approach, which has led to uncertainty in decision making. Furthermore, few scientific studies have been conducted to develop validation methodologies for large-scale models, such as public transport models. For these reasons, a scientific validation methodology for public transport models has been developed in this thesis. This validation methodology has been applied on the 2014 route assignment model used by Trafikförvaltningen, for the transport modes bus, commuter train and local tram.</p><p>In the developed validation methodology, the selected validation metrics called MAPE, %RMSE and R^2 are used to compare link loads from a route assignment model with observed link loads from an Automatic Passenger Counting (APC) system. To obtain an overview of the performance of the route assignment model, eight different scenarios are set, based on whether the validation metrics meet acceptable thresholds or not.</p><p>In the application of the developed validation methodology, the average link loads for the morning rush have been validated. To adjust the developed validation methodology to system-specific factors and to set acceptable metric thresholds, discussions with model practitioners have taken place. The validation has been performed on both lines and links, and for bus entire line number series have been validated as well. The validation results show that commuter train meets the set threshold values in a higher proportion than bus and local tram do. However, Trafikförvaltningen is recommended to further calibrate the route assignment model in order to achieve a better model performance.</p><p>The developed validation methodology can be used for validation of public transport models, and can in combination with model calibration be used in an iterative process to fine-tune model parameters for optimising validation results. Finally, a number of recommendations are proposed for Trafikförvaltningen to increase the efficiency and quality of the validation process, such as synchronising model data with the observed data.</p>

corrected abstract:
<p>During 2018, the Public Transport Administration (Trafikförvaltningen) in the Stockholm region spent approximately 2.2 billion SEK on new infrastructure investments related to the public transport system, many of which were based on their public transport models. The previously used method for validating these models has lacked scientific rigour, efficiency and a systematic approach, which has led to uncertainty in decision making. Furthermore, few scientific studies have been conducted to develop validation methodologies for large-scale models, such as public transport models. For these reasons, a scientific validation methodology for public transport models has been developed in this thesis. This validation methodology has been applied on the 2014 route assignment model used by Trafikförvaltningen, for the transport modes bus, commuter train and local tram.</p><p>In the developed validation methodology, the selected validation metrics called MAPE, %RMSE and R<sup>2</sup> are used to compare link loads from a route assignment model with observed link loads from an Automatic Passenger Counting (APC) system. To obtain an overview of the performance of the route assignment model, eight different scenarios are set, based on whether the validation metrics meet acceptable thresholds or not.</p><p>In the application of the developed validation methodology, the average link loads for the morning rush have been validated. To adjust the developed validation methodology to system-specific factors and to set acceptable metric thresholds, discussions with model practitioners have taken place. The validation has been performed on both lines and links, and for bus entire line number series have been validated as well. The validation results show that commuter train meets the set threshold values in a higher proportion than bus and local tram do. However, Trafikförvaltningen is recommended to further calibrate the route assignment model in order to achieve a better model performance.</p><p>The developed validation methodology can be used for validation of public transport models, and can in combination with model calibration be used in an iterative process to fine-tune model parameters for optimising validation results. Finally, a number of recommendations are proposed for Trafikförvaltningen to increase the efficiency and quality of the validation process, such as synchronising model data with the observed data.</p>
----------------------------------------------------------------------
In diva2:1360711 
abstract is: 
<p>The Giraffe 1X is a mobile short range 3D radar from Saab used for example to detect threats and create protection in a ground based air defence system. It can also be used on naval platforms for air and surface surveillance. During the development of the radar, the system needs to be tested for both sea and mobile land applications. The most convenient place for testing is on the roof of Saab’s facility in Gothenburg. There elevators can raise the radar to the roof giving an excellent view of for example Landvetter airport and the sea. To aid future verification experiments of the radar system, this project was started in order to develop and construct a motion platform used to simulate sea- and vehicle motions. During a six month period at Saab, the work of the project was started with a thorough research of motions platforms to conduct preliminary concept studies. Furthermore the concepts were drawn as 3D-CAD models in Creo Parametric in order to visualise the different solutions and present them for suppliers. The report also covers the assembly of the produced parts, together with the development of a user interface to control the motion platform.</p><p>Lastly, the result of product development is a two-degree of freedom (DOF) motion platform influenced by the gyroscopic gimbal concept. The G1X radar is mounted on a gimbal platform which is made out of two aluminium frames, whereas the outer frame rotates around an horizontal axis while the inner frame rotates around a transversely mounted horizontal axis mounted on the outer frame. Each aluminium frame is attached to a link arm which is mounted on a motor that is used to tilt the frame. The platform can be tilted _ 22 o in pitch and _ 22 o in roll. The gimbal is supported by a steel structure to allow ground clearance and to raise the radar to a comfortable working height.</p>

corrected abstract:
<p>The Giraffe 1X is a mobile short range 3D radar from Saab used for example to detect threats and create protection in a ground based air defence system. It can also be used on naval platforms for air and surface surveillance. During the development of the radar, the system needs to be tested for both sea and mobile land applications. The most convenient place for testing is on the roof of Saab’s facility in Gothenburg. There elevators can raise the radar to the roof giving an excellent view of for example Landvetter airport and the sea. To aid future verification experiments of the radar system, this project was started in order to develop and construct a motion platform used to simulate sea- and vehicle motions. During a six month period at Saab, the work of the project was started with a thorough research of motions platforms to conduct preliminary concept studies. Furthermore the concepts were drawn as 3D-CAD models in Creo Parametric in order to visualise the different solutions and present them for suppliers. The report also covers the assembly of the produced parts, together with the development of a user interface to control the motion platform.</p><p>Lastly, the result of product development is a two-degree of freedom (DOF) motion platform influenced by the gyroscopic gimbal concept. The G1X radar is mounted on a gimbal platform which is made out of two aluminium frames, whereas the outer frame rotates around an horizontal axis while the inner frame rotates around a transversely mounted horizontal axis mounted on the outer frame. Each aluminium frame is attached to a link arm which is mounted on a motor that is used to tilt the frame. The platform can be tilted ± 22 º in pitch and ± 22 º in roll. The gimbal is supported by a steel structure to allow ground clearance and to raise the radar to a comfortable working height.</p>
----------------------------------------------------------------------
In diva2:1698174 
abstract is: 
<p>This study has investigated the conceptual feasibility of a rocket propelled kinetic energy penetrator (KEP), designed for the handheld recoilless rifle Carl-Gustaf® 84 mm calibre system, from an exterior ballistics perspective. The methodology is based upon evaluating the aerodynamic properties of different conceptual design proposals through CFD-simulations and then performing trajectory analysis to assess their exterior ballistic performance. In particular, the main focus has been to optimize the stability, velocity and spin rate of the KEP. The results of the study indicates that the final chosen KEP design retains, from an aerodynamic perspective, longitudinal stability for Mach numbers up to 4.5, regardless if the rocket motor is ignited or not. Furthermore, if using NK1384 propellant, the final chosen design in the study is, according to the calculations, able to achieve a maximum velocity of 0.7⋅v_ref and retain a minimum velocity of 0.628⋅v_ref in the horizontal range of [0.318⋅x_ref,0.648⋅x_ref] measured from the shooter. In addition, the angular spin velocity achieves a maximum value of 15.5 Hz, satisfying the performance limitation of the rocket motor which only functions properly for frequencies up to 30 Hz, while simultaneously providing a sufficiently considered spin rate in order to/ average possible thrust and mass deviations of the KEP. The results also show that if using ammonium dinitramide (ADN) propellant, the KEP is able to achieve a maximum velocity of 0.786⋅v_ref, retain a minimum velocity of 0.628⋅v_ref in the horizontal range of [0.28⋅x_ref,0.98⋅x_ref] and achieve a maximum spin rate of 17.5 Hz.</p>

corrected abstract:
<p>This study has investigated the conceptual feasibility of a rocket propelled kinetic energy penetrator (KEP), designed for the handheld recoilless rifle Carl-Gustaf® 84 𝑚𝑚 calibre system, from an exterior ballistics perspective. The methodology is based upon evaluating the aerodynamic properties of different conceptual design proposals through CFD-simulations and then performing trajectory analysis to assess their exterior ballistic performance. In particular, the main focus has been to optimize the stability, velocity and spin rate of the KEP. The results of the study indicates that the final chosen KEP design retains, from an aerodynamic perspective, longitudinal stability for Mach numbers up to 4.5, regardless if the rocket motor is ignited or not. Furthermore, if using NK1384 propellant, the final chosen design in the study is, according to the calculations, able to achieve a maximum velocity of 0.7 ⋅ 𝑣<sub>ref</sub> and retain a minimum velocity of 0.628⋅𝑣<sub>ref</sub> in the horizontal range of <span style="font-size: 1.25rem;">[</span>0.318⋅𝑥<sub>ref</sub>,0.648⋅𝑥<sub>ref</sub><span style="font-size: 1.25rem;">]</span> measured from the shooter. In addition, the angular spin velocity achieves a maximum value of 15.5 𝐻𝑧, satisfying the performance limitation of the rocket motor which only functions properly for frequencies up to 30 𝐻𝑧, while simultaneously providing a sufficiently considered spin rate in order to average possible thrust and mass deviations of the KEP. The results also show that if using ammonium dinitramide (ADN) propellant, the KEP is able to achieve a maximum velocity of 0.786⋅𝑣<sub>ref</sub>, retain a minimum velocity of 0.628⋅𝑣<sub>ref</sub> in the horizontal range of <span style="font-size: 1.25rem;">[</span>0.28⋅𝑥<sub>ref</sub>,0.98⋅𝑥<sub>ref</sub><span style="font-size: 1.25rem;">]</span> and achieve a maximum spin rate of 17.5 𝐻𝑧.</p>
----------------------------------------------------------------------
In diva2:612264 
abstract is: 
<p>Switched systems form a special class of hybrid dynamical systems, i.e. systems with both continuous and discrete dynamics. A switched system contains a family of continuous subsystems and a discrete variable that governs the switching between them. The problem of finding necessary and sufficient conditions for asymptotic stability under arbitrary switching has recently been solved for the two-dimensional focused switched system <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cdot%7Bx%7D=uAx+(1-u)Bx" /> . We consider a generalization <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cdot%7Bx%7D=uA(x-x_c)+(1-u)Bx" />  where the equilibrium points of the two subsystems have been separated, a <em>defocused</em> system. Using geometrical arguments we show that whenever the focused system is asymptotically stable, a corresponding defocused system will contain a ‘smallest’ invariant set <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" />. In the case when both the subsystems have non-real eigenvalues we are able to completely characterize <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" /> and prove that all trajectories converge to <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" />. We investigate topological properties of <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" /> and classify the possible irregularities of <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cpartial%5COmega" />. We also build time-optimal syntheses inside <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" /> which exhibit the behavior of minimal-time trajectories. Finally we briefly look at additional phenomena that occur when real eigenvalues are present in the subsystems.</p>

corrected abstract:
<p>Switched systems form a special class of hybrid dynamical systems, i.e. systems with both continuous and discrete dynamics. A switched system contains a family of continuous subsystems and a discrete variable that governs the switching between them. The problem of finding necessary and sufficient conditions for asymptotic stability under arbitrary switching has recently been solved for the two-dimensional <em>focused</em> switched system <em>ẋ</em> = 𝑢𝐴𝑥 + (1 - 𝑢)𝐵𝑥. We consider a generalization <em>ẋ</em> = 𝑢𝐴(𝑥 - 𝑥<sub>𝑐</sub>) + (1 - 𝑢)𝐵𝑥 where the equilibrium points of the two subsystems have been separated, a <em>defocused</em> system. Using geometrical arguments we show that whenever the focused system is asymptotically stable, a corresponding defocused system will contain a ‘smallest’ invariant set Ω. In the case when both the subsystems have non-real eigenvalues we are able to completely characterize Ω and prove that all trajectories converge to Ω. We investigate topological properties of Ω and classify the possible irregularities of ∂Ω. We also build time-optimal syntheses inside Ω which exhibit the behavior of minimal-time trajectories. Finally we briefly look at additional phenomena that occur when real eigenvalues are present in the subsystems.</p>

Note the equations were simple enough that they could simply be done in unicode and HTML
----------------------------------------------------------------------
In diva2:1441946 
abstract is: 
<p>The purpose of this project is to investigate the angled nutrunner, which is a hand held torque tool often used in the industry to tighten bolted joints. The goal is to estimate parameter values for an existing model that can describe the reaction force and the angular displacement of the tool as a function of the torque transferred to the joint. The model is based on a damped mass-spring system with one degree of freedom. The short torque pulse in the tool will induce an oscillating motion of the system. A large amount of data is used that has been collected at an assembly factory. Different tightening conditions tested include tightening strategy (Quick Step and Turbo Tight), joint stiffness (hard and soft joint), tightening pace (5 and 8 tightenings per minute) and target torque. Joint torque, angular acceleration for the tool and, for the Quick Step tightenings, the reaction force is measured. The grey-box model is used in order to estimate the parameters of the model through curve fitting of the data describing the angular displacement. The reaction force on the user is also examined with different methods. The resulting mean values of the model parameters are the mass m = 2.31 kg, the dampening constant m = 103.45 kg/s and the spring stiffness k = 2314.84 N/m. The mean value of the natural frequency of the hand-arm system is f_n = 5.46 Hz. A statistical significance is found for all conditions except the tightening pace.</p>

corrected abstract:
<p>The purpose of this project is to investigate the angled nutrunner, which is a hand held torque tool often used in the industry to tighten bolted joints. The goal is to estimate parameter values for an existing model that can describe the reaction force and the angular displacement of the tool as a function of the torque transferred to the joint.</p><p>The model is based on a damped mass-spring system with one degree of freedom. The short torque pulse in the tool will induce an oscillating motion of the system.</p><p>A large amount of data is used that has been collected at an assembly factory. Different tightening conditions tested include tightening strategy (Quick Step and Turbo Tight), joint stiffness (hard and soft joint), tightening pace (5 and 8 tightenings per minute) and target torque. Joint torque, angular acceleration for the tool and, for the Quick Step tightenings, the reaction force is measured. The <em>grey-box model</em> is used in order to estimate the parameters of the model through curve fitting of the data describing the angular displacement. The reaction force on the user is also examined with different methods.</p><p>The resulting mean values of the model parameters are the mass 𝑚 = 2.31 kg, the dampening constant 𝑐 = 103.45 kg/s and the spring stiffness 𝑘 = 2314.84 N/m. The mean value of the natural frequency of the hand-arm system is 𝑓<sub>𝑛</sub> = 5.46 Hz. A statistical significance is found for all conditions except the tightening pace.</p>
----------------------------------------------------------------------
In diva2:1231299 
Note: no full text in DiVA

abstract is: 
<p>An important step in manufacturing humanoid robots is being able to imitate human movement. In the case of this project, optimal movement patterns retrieved from solving an optimal control problem serve as a substitute for human movement. A supervised learning algorithm learns to model the control signal of a mobile manipulator striking a projectile to hit a specific target. Data fed to the algorithm contains trajectories generated by solving an optimal control problem. The supervised learning algorithm applied to the problem was written in Python using the TensorFlow software library. By dividing the data in two sets, one for training and one for testing, progress is measured and overfitting estimated by calculating the relative percentage error between values predicted by the model and the corresponding values in the two data sets. A mean training accuracy of 90.7 percent and a mean validation accuracy of -226 percent. The source code can be found on https://github.com/JeremiGrosz/Optimal_control_supervised_learning</p>

corrected abstract:
<p>An important step in manufacturing humanoid robots is being able to imitate human movement. In the case of this project, optimal movement patterns retrieved from solving an optimal control problem serve as a substitute for human movement. A supervised learning algorithm learns to model the control signal of a mobile manipulator striking a projectile to hit a specific target. Data fed to the algorithm contains trajectories generated by solving an optimal control problem. The supervised learning algorithm applied to the problem was written in Python using the TensorFlow software library. By dividing the data in two sets, one for training and one for testing, progress is measured and overfitting estimated by calculating the relative percentage error between values predicted by the model and the corresponding values in the two data sets. A mean training accuracy of 90.7 percent and a mean validation accuracy of -226 percent. The source code can be found on <a href="https://github.com/JeremiGrosz/Optimal_control_supervised_learning">https://github.com/JeremiGrosz/Optimal_control_supervised_learning</a></p>
----------------------------------------------------------------------
In diva2:643302 
abstract is: 
<p>There are various technologies used for reducing fuel consumption of automobiles. Hybrid electric vehicles is one approach that has been used, which can reduce fuel consumption by 10-30% compared to conventional vehicles.</p><p>In this master thesis the minimization of fuel consumption of a power-split type HEV along a given route is considered, where the vehicle speed has been assumed to be known <em>a priori</em>. This minimization was made by first deriving a model of the HEV powertrain, followed by creating a Dynamical programming based program for finding the optimal distribution of torques.</p><p>The performance was evaluated through the commercial software GT-Suite. The resulting control from the Dynamic program could follow the reference speed in many situations. However the battery state-of-charge calculated in the Dynamic program did not update properly, resulting in a depleted battery in some cases.</p><p>The model derived could follow the dynamics of the vehicle, but there are some parts which could be improved. One of them is the dynamical model of the rotational speed for the engine <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Comega_%7Be%7D" />.</p><p> The Dynamic program works for finding the controller, and can be modified to work with improved state-equations.</p>

corrected abstract:
<p>There are various technologies used for reducing fuel consumption of automobiles. Hybrid electric vehicles is one approach that has been used, which can reduce fuel consumption by 10-30% compared to conventional vehicles.</p><p>In this master thesis the minimization of fuel consumption of a power-split type HEV along a given route is considered, where the vehicle speed has been assumed to be known <em>a priori</em>. This minimization was made by first deriving a model of the HEV powertrain, followed by creating a Dynamical programming based program for finding the optimal distribution of torques.</p><p>The performance was evaluated through the commercial software GT-Suite. The resulting control from the Dynamic program could follow the reference speed in many situations. However the battery state-of-charge calculated in the Dynamic program did not update properly, resulting in a depleted battery in some cases.</p><p>The model derived could follow the dynamics of the vehicle, but there are some parts which could be improved. One of them is the dynamical model of the rotational speed for the engine <em>ω</em><sub>𝑒</sub>.</p><p> The Dynamic program works for finding the controller, and can be modified to work with improved state-equations.</p>
----------------------------------------------------------------------
In diva2:1658938 
abstract is: 
<p>In this thesis, logistic regression, random forest and statistical analysis are used to both predict and explain insurance purchases. The models are tested together with the oversampling method known as SMOTE. The result was that the random forest model together with SMOTE best predicted insurance purchases with an $F_{1}$-score of 93.2\% and ROC-AUC of 96\%. Another important discovery comes from the explanatory part where it turns out that the price of the order and the length between the order and departure date greatly influences insurance purchases. Increased values for these features increase the insurance purchase rate. For prices of the order belonging to the 90th percentile, the insurance purchase rate is approximately 2 times higher than the average and for prices of the order belonging to the 99th percentile, the insurance purchase rate is approximately 3 times higher than the average. The purchase rate is approximately 2.5 times higher for lengths longer than 8 months between order and departure compared to the length being less than one month. These are useful insights from a business perspective that can improve the travel agency’s pricing. Other more general findings include that SMOTE worked well to handle the class imbalance in this data set and that the F1-score seems to be superior to ROC-AUC as evaluation metric when it comes to the sort of problems with unbalanced data where the positive class is a minority and most important.</p>

corrected abstract:
<p>In this thesis, logistic regression, random forest and statistical analysis are used to both predict and explain insurance purchases. The models are tested together with the oversampling method known as SMOTE. The result was that the random forest model together with SMOTE best predicted insurance purchases with an 𝐹<sub>1</sub>-score of 93.2% and ROC-AUC of 96%. Another important discovery comes from the explanatory part where it turns out that the price of the order and the length between the order and departure date greatly influences insurance purchases. Increased values for these features increase the insurance purchase rate. For prices of the order belonging to the 90th percentile, the insurance purchase rate is approximately 2 times higher than the average and for prices of the order belonging to the 99th percentile, the insurance purchase rate is approximately 3 times higher than the average. The purchase rate is approximately 2.5 times higher for lengths longer than 8 months between order and departure compared to the length being less than one month. These are useful insights from a business perspective that can improve the travel agency’s pricing. Other more general findings include that SMOTE worked well to handle the class imbalance in this data set and that the F1-score seems to be superior to ROC-AUC as evaluation metric when it comes to the sort of problems with unbalanced data where the positive class is a minority and most important.</p>
----------------------------------------------------------------------
