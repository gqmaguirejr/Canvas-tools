This file contains corrected abstracts from DiVA for SCI added on 2024-12-05
----------------------------------------------------------------------
In diva2:1888871   - correct as is
----------------------------------------------------------------------
In diva2:1881342 
abstract is: 
<p>This thesis investigates the impedance of acoustic liners, to attenuate noise originating from jet engines and enable compliance with international standards and regulations regarding noise from airplane jet engines. Experimental tests of two supplied liners were conducted in an impedance tube; one liner with known and predictable properties, and one liner with unknown properties.</p><p>The tests included tonal excitations in the formats of stepped sine and random noise with frequencies within set boundaries. After post-processing of the captured data, the desired impedance could be analysed in terms of excitated frequencies and sound pressure levels.</p><p>The conclusions from this project are that both of the liners deviated from their expected behavior, which was that liner 1 should have been unaffected by the alternated sound pressure levels, and liner 3 should have shown bigger affection due to the changed sound pressure level. Since the results were different than expected, there might have been minor sources of error during the measurements. It could be investigated if there is leakage from the mounting of the liners, or if the 3D printing resolution is sufficient.</p><p>Because of limitations in time, there is more left in this project to investigate. Therefore, conducting similar studies where more frequencies, sound pressure levels, and multi-tonal measurements can be included, is suggested as future work.</p><p> </p>

corrected abstract:
<p>This thesis investigates the impedance of acoustic liners, to attenuate noise originating from jet engines and enable compliance with international standards and regulations regarding noise from airplane jet engines. Experimental tests of two supplied liners were conducted in an impedance tube; one liner with known and predictable properties, and one liner with unknown properties.</p><p>The tests included tonal excitations in the formats of stepped sine and random noise with frequencies within set boundaries. After post-processing of the captured data, the desired impedance could be analysed in terms of excitated frequencies and sound pressure levels.</p><p>The conclusions from this project are that both of the liners deviated from their expected behavior, which was that liner 1 should have been unaffected by the alternated sound pressure levels, and liner 3 should have shown bigger affection due to the changed sound pressure level. Since the results were different than expected, there might have been minor sources of error during the measurements. It could be investigated if there is leakage from the mounting of the liners, or if the 3D printing resolution is sufficient.</p><p>Because of limitations in time, there is more left in this project to investigate. Therefore, conducting similar studies where more frequencies, sound pressure levels, and multi-tonal measurements can be included, is suggested as future work.</p>

Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1881022 
abstract is: 
<p>This study compares transportation preferences among students and staff from two different universities: KTH in Stockholm and ITBA in Buenos Aires. The aim was to identify the factors influencing transportation decisions and their impact on perceptions regarding its usage. A survey was conducted at both universities to gain insights from participants and analyze the collected information, aiming to propose solutions that enhance the quality and sustainability of transportation. This work presents a novel analysis by comparing two cities with seemingly different characteristics in terms of transportation and urban development, with an emphasis on sustainability. The results obtained are positive as potential improvements in both systems could be observed. Moreover, the results have generated ideas to improve the adoption of green and sustainable transportation, without compromising the quality of existing transportation for members of these educational institutions. The importance of environmental awareness and its positive impact on quality of life and urban mobility is highlighted.</p><p> </p>

corrected abstract:
<p>This study compares transportation preferences among students and staff from two different universities: KTH in Stockholm and ITBA in Buenos Aires. The aim was to identify the factors influencing transportation decisions and their impact on perceptions regarding its usage. A survey was conducted at both universities to gain insights from participants and analyze the collected information, aiming to propose solutions that enhance the quality and sustainability of transportation. This work presents a novel analysis by comparing two cities with seemingly different characteristics in terms of transportation and urban development, with an emphasis on sustainability. The results obtained are positive as potential improvements in both systems could be observed. Moreover, the results have generated ideas to improve the adoption of green and sustainable transportation, without compromising the quality of existing transportation for members of these educational institutions. The importance of environmental awareness and its positive impact on quality of life and urban mobility is highlighted.</p>

Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1880964 
abstract is: 
<p>Urban Air mobility (UAM) promises reduced congestion on roads, reduced travel times and stronger overall efficiency in densely populated areas. However several challenges arise when wanting to implement UAM such as safety and social acceptance. The aim of this paper is to gain valuable insights how to implement safe and socially accepted UAM into society. Current regulations are discussed as well as X, Y and Z volumes in U-space, flight separations with ellipsoidal safety buffers, high speed corridors, landing separation at vertiports and airspace partition with voronoi diagrams are proposed and discussed. Social acceptance is addressed with some of the most prominent concerns e.g. safety, privacy and noise. Examples are set in Stockholm, Sweden, resulting in a maximum airspace occupation of 1 % which means 210 UAS (Unmanned Aircraft Systems) on each flight level. Sensitive areas and people with privacy concerns should have the option to opt-out of having their properties under the flight paths of UAM-vehicles. Concerns with UAM from the public has to be taken into great consideration for a successful implementation of UAM.</p>

corrected abstract:
<p>Urban Air mobility (UAM) promises reduced congestion on roads, reduced travel times and stronger overall efficiency in densely populated areas. However several challenges arise when wanting to implement UAM such as safety and social acceptance. The aim of this paper is to gain valuable insights how to implement safe and socially accepted UAM into society. Current regulations are discussed as well as X, Y and Z volumes in U-space, flight separations with ellipsoidal safety buffers, high speed corridors, landing separation at vertiports and airspace partition with voronoi diagrams are proposed and discussed. Social acceptance is addressed with some of the most prominent concerns e.g. safety, privacy and noise. Examples are set in Stockholm, Sweden resulting in a maximum airspace occupation of 1 % which means 210 UAS (Unmanned Aircraft Systems) on each flight level. Sensitive areas and people with privacy concerns should have the option to opt-out of having their properties under the flight paths of UAM-vehicles. Concerns with UAM from the public has to be taken into great consideration for a successful implementation of UAM.</p>

Note - only change was to remove the comma after "Sweden" as it is not in the original, although grammatically it might be correct. It might also be noted that commas should before and after the "e.g." - but are not in the original.
----------------------------------------------------------------------
In diva2:1880436   - correct as is
----------------------------------------------------------------------
In diva2:1880384   - correct as is
----------------------------------------------------------------------
In diva2:1880323 
abstract is: 
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p><p> </p>

corrected abstract:
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p>


Note - only change to elimnate the empty paragraph at the end
----------------------------------------------------------------------
In diva2:877595 
abstract is: 
<p> </p><p>A recent trend in the world is that more and more countries and therefore their mil-itaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to ad-opt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense com-pany, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an e˙ective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare di˙erent solutions. The delta model includes reference values for relevant costs that can be a˙ected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>
w='di˙erent' val={'c': 'different', 's': ['diva2:877595', 'diva2:1380198', 'diva2:891912', 'diva2:919302', 'diva2:1328904', 'diva2:891537', 'diva2:1440147', 'diva2:1087823'], 'n': 'missing ligature'}
w='ad-opt' val={'c': 'adopt', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='com-pany' val={'c': 'company', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='mil-itaries' val={'c': 'militaries', 's': 'diva2:877595', 'n': 'unnecessary hyphen'}
w='a˙ected' val={'c': 'affected', 's': 'diva2:877595', 'n': 'missing ligature'}
w='e˙ective' val={'c': 'effective', 's': 'diva2:877595', 'n': 'error due to ligature'}

corrected abstract:
<p>A recent trend in the world is that more and more countries and therefore their militaries have made their spending more streamlined by considering the true cost of a system, also called its Life Cycle Cost. This has forced the defense industry to adopt the same way of thinking when developing their systems in order to stay competitive.</p><p>Electronic Defense Systems (EDS) is a business area within Saab, a Swedish defense company, that has experienced this. Within EDS and its business unit Electronic Warfare (EW), the ILS-department (Integrated Logistics Support) is tasked with implementing this line of thinking within EDS. The ILS-department has seen the need for a greater leverage in the decision making process, both during product development and in the after sales market. In order to achieve this increased leverage, they saw the need for an evaluation tool to decrease Life Support Costs (LSC).</p><p>This thesis aims to create a tool to meet the demands of the ILS department and enhance their way of thinking by calculating the relevant costs and presenting them in a clear and comprehensive way, so that the finished LSC evaluation framework can be an effective aid in the decision making process.</p><p>The main result of this thesis is a LSC evaluation framework that can show the impact of both small and large changes to the technical and/or support system on LSC. In order to do this, the LSC evaluation framework utilizes the OPUS suite software; OPUS10, Simlox and Catloc together with supporting documents. The end result is a delta model in order to compare different solutions. The delta model includes reference values for relevant costs that can be affected by such changes.</p><p>Included is also two cases in which the model is used. The data shown during these cases have been altered to comply with confidentiality requirements.</p>

Note - also remove the empty paragraph at the start.
----------------------------------------------------------------------
In diva2:711193 
abstract is: 
<p>Consensus problem with multi-agent systems has interested researchers in various areas. Its difficulties tend to appear when available information of each agent is limited for achieving consensus. Besides, it is not always the case that agents can catch the whole states of the others; an output is often the only possible measurement for each agent in applications. The idea of graph Laplacian is then of help to address such a troublesome situation. While every single agent obviously makes decision to achieve an individual goal of minimizing its own cost functional, all agents as a team can obtain even more improvement by cooperation in some cases, which leads to cooperative game theoretic approach. The main goal of this master thesis is to accomplish a combination of optimal control theory and cooperative game theory in order to solve the output consensus problem with limited network connectivity. Along with this combination, bargaining problems are considered out of necessity.</p><p> </p>

corrected abstract:
<p>Consensus problem with multi-agent systems has interested researchers in various areas. Its difficulties tend to appear when available information of each agent is limited for achieving consensus. Besides, it is not always the case that agents can catch the whole states of the others; an output is often the only possible measurement for each agent in applications. The idea of graph Laplacian is then of help to address such a troublesome situation.</p><p>While every single agent obviously makes decision to achieve an individual goal of minimizing its own cost functional, all agents as a team can obtain even more improvement by cooperation in some cases, which leads to cooperative game theoretic approach. The main goal of this master thesis is to accomplish a combination of optimal control theory and cooperative game theory in order to solve the output consensus problem with limited network connectivity. Along with this combination, bargaining problems are considered out of necessity.</p>

Note added missing paragraph break and removed empty paragraph
----------------------------------------------------------------------
In diva2:1211524 
abstract is: 
<p>This thesis in applied statistics and industrial economics examines the correlation between a number of market conditions on the Swedish and Global market and the yield difference between the Swedish stock market and the Global stock market. The report is based on data from the index MSCI Sweden Net Return, MSCI World Net Return and the Volatility index S&amp;P 500. The market conditions that have been examined are Bull markets, Bear markets, periods of high volatility. We also examined how the appreciation of the SEK in comparison to the USD and the yield of the Swedish stock market correlated with the yield difference between the Swedish Stock Market and the Global stock market. The correlation was examined using multiple linear regression. The results indicated a positive correlation between the yield difference between the Swedish stock market and the Global stock market and the yield of the Swedish stock market, the appreciation of the SEK compared to the USD and Bull markets. We found a negative correlation with Bear markets and no correlation at all with the volatility.</p><p> </p><p>The results are in line with what could be expected and give a stronger statistical ground for the idea that the Swedish stock market has larger fluctuations than the Global stock market during large-scale market fluctuations.</p>

corrected abstract:
<p>This thesis in applied statistics and industrial economics examines the correlation between a number of market conditions on the Swedish and Global market and the yield difference between the Swedish stock market and the Global stock market. The report is based on data from the index MSCI Sweden Net Return, MSCI World Net Return and the Volatility index S&amp;P 500. The market conditions that have been examined are Bull markets, Bear markets, periods of high volatility. We also examined how the appreciation of the SEK in comparison to the USD and the yield of the Swedish stock market correlated with the yield difference between the Swedish Stock Market and the Global stock market. The correlation was examined using multiple linear regression. The results indicated a positive correlation between the yield difference between the Swedish stock market and the Global stock market and the yield of the Swedish stock market, the appreciation of the SEK compared to the USD and Bull markets. We found a negative correlation with Bear markets and no correlation at all with the volatility.</p><p>The results are in line with what could be expected and give a stronger statistical ground for the idea that the Swedish stock market has larger fluctuations than the Global stock market during large-scale market fluctuations.</p>

Note - only change - removed empty paragraph in the middle
----------------------------------------------------------------------
In diva2:1674019 
abstract is: 
<p>Low Reynolds number airfoil analysis has become increasingly significant as urban air mobility vehicles and unmanned aerial vehicles surge in popularity. The Green Raven project at KTH Aero aims to use reflex airfoils where little data is available beyond classical analysis. Viscous formulations of the panel method and computational fluid dynamics (CFD) have been used to simulate lift, drag and moments for the MH61 and MH104 airfoils at different angles of attack (AOAs). XFOIL and CFD turbulence models such as Spalart-Allmaras (SA), k-w Shear Stress Transport (SST) with and without damping coefficients were used. The strengths and limitations of each model were used to justify results. Due to clear computational advantages, XFOIL produced adequate results and is tailored toward use in initial design stages where repeated measurements are crucial. The SA turbulence stood out as the model produced accurate results in a reasonable time. The abundance of published CFD material comparing different turbulence models increased the credibility of the results. The two airfoils had similar lift and drag characteristics at AOAs of 0-6 deg while the MH104 was superior near stall. However, due to the lack of experimental data of the airfoils no particular model could be commended or verified.</p><p> </p>

corrected abstract:
<p>Low Reynolds number airfoil analysis has become increasingly significant as urban air mobility vehicles and unmanned aerial vehicles surge in popularity. The Green Raven project at KTH Aero aims to use reflex airfoils where little data is available beyond classical analysis. Viscous formulations of the panel method and computational fluid dynamics (CFD) have been used to simulate lift, drag and moments for the MH61 and MH104 airfoils at different angles of attack (AOAs). XFOIL and CFD turbulence models such as Spalart-Allmaras (SA), 𝑘-ω Shear Stress Transport (SST) with and without damping coefficients were used. The strengths and limitations of each model were used to justify results. Due to clear computational advantages, XFOIL produced adequate results and is tailored toward use in initial design stages where repeated measurements are crucial. The SA turbulence stood out as the model produced accurate results in a reasonable time. The abundance of published CFD material comparing different turbulence models increased the credibility of the results. The two airfoils had similar lift and drag characteristics at AOAs of 0-6º while the MH104 was superior near stall. However, due to the lack of experimental data of the airfoils no particular model could be commended or verified.</p>
----------------------------------------------------------------------
In diva2:1778751 
abstract is: 
<p>Formula Student is a global engineering competition where university students collaborate to design, construct, and race formula-style cars. Aerodynamics is one aspect in the vehicle design that can improve on-track performance by increasing cornering and straight-line speed.</p><p>To improve the aerodynamics of KTH Formula Student's DeV18 vehicle, the side structure is being redesigned. The current model, DeV17, features an underperforming tunnel-based side structure. To address this issue, this had the goal to investigate a new multi-element wing design that utilizes ground effect.</p><p>The design study of the DeV18 vehicle is conducted using Siemens NX 2212 for 3D modelling and Simcenter Star-CCM+ 17.06.008-R8 for airflow simulations. To quickly investigate certain design parameters effect on the results, Design Manager Project inside Simcenter Star-CCM+ is used.</p><p>The resulting side structure produces a total of 26 N of downforce and 6 N of drag at 40 kph, more than twice that of DeV17’s side structure while also producing less drag. Although this significant improvement compared to DeV17, it is believed that further increases in performance are necessary to compete with top teams. By using a more sophisticated method to optimize the multi-element wing, such as adjoint optimization, the concept could be improved. However, the overall potential of the concept is still considered too limited to achieve the desired performance goals, which is why it will no longer be investigated further.</p><p> </p>

corrected abstract:
<p>Formula Student is a global engineering competition where university students collaborate to design, construct, and race formula-style cars. Aerodynamics is one aspect in the vehicle design that can improve on-track performance by increasing cornering and straight-line speed.</p><p>To improve the aerodynamics of KTH Formula Student's DeV18 vehicle, the side structure is being redesigned. The current model, DeV17, features an underperforming tunnel-based side structure. To address this issue, this had the goal to investigate a new multi-element wing design that utilizes ground effect.</p><p>The design study of the DeV18 vehicle is conducted using Siemens NX 2212 for 3D modelling and Simcenter Star-CCM+ 17.06.008-R8 for airflow simulations. To quickly investigate certain design parameters effect on the results, Design Manager Project inside Simcenter Star-CCM+ is used.</p><p>The resulting side structure produces a total of 26 N of downforce and 6 N of drag at 40 kph, more than twice that of DeV17’s side structure while also producing less drag. Although this significant improvement compared to DeV17, it is believed that further increases in performance are necessary to compete with top teams. By using a more sophisticated method to optimize the multi-element wing, such as adjoint optimization, the concept could be improved. However, the overall potential of the concept is still considered too limited to achieve the desired performance goals, which is why it will no longer be investigated further.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1776777 
abstract is: 
<p>In this Bachelor thesis, a novel algorithm for sampling from bandlimited circular probability distributions is presented. The algorithm leverages results from Fourier analysis concerning the Fejér kernel to simulate data with some desired probability distribution, realized as a sum of data sampled from a discrete distribution and a small continuous perturbation sampled from the Fejér kernel distribution. Relevant theory is presented before formally proving exact simulation using the algorithm. Experimental results confirm the validity of the theoretical results, and the efficiency of the algorithm is then compared with that of other sampling methods such as rejection sampling with a uniform envelope function.</p><p> </p>

corrected abstract:
<p>In this Bachelor thesis, a novel algorithm for sampling from bandlimited circular probability distributions is presented. The algorithm leverages results from Fourier analysis concerning the Fejér kernel to simulate data with some desired probability distribution, realized as a sum of data sampled from a discrete distribution and a small continuous perturbation sampled from the Fejér kernel distribution. Relevant theory is presented before formally proving exact simulation using the algorithm. Experimental results confirm the validity of the theoretical results, and the efficiency of the algorithm is then compared with that of other sampling methods such as rejection sampling with a uniform envelope function.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:525720 
abstract is: 
<p>Abstract</p><p> </p><p>In this report analysis of foreign exchange rates time series are performed. First, triangular arbitrage is detected and eliminated from data series using linear algebra tools. Then Vector Autoregressive processes are calibrated and used to replicate dynamics of exchange rates as well as to forecast time series. Finally, optimal portfolio of currencies with minimal Expected Shortfall is formed using one time period ahead forecasts</p>

corrected abstract:
<p>In this report analysis of foreign exchange rates time series are performed. First, triangular arbitrage is detected and eliminated from data series using linear algebra tools. Then Vector Autoregressive processes are calibrated and used to replicate dynamics of exchange rates as well as to forecast time series. Finally, optimal portfolio of currencies with minimal Expected Shortfall is formed using one time period ahead forecasts.</p>

Note - removed unnecessary "<p>Abstract</p><p> </p>" and added final missing period.
----------------------------------------------------------------------
In diva2:1566509 
abstract is: 
<p>This report is about a novel approach to attenuation of fan noise in aerial vehicles, by way of implementing a ducted fan in the chassis of a four meter blended wing body plane. Three different one meter PVC pipes were used and their performances as silencers were tested by measuring the sound power level and calculating the insulation loss compared to a fan by itself. The ducts were either empty or lined with acoustic absorbents and micro perforated panels. Experiments were carried out in the reverberation room at KTH Marcus Wallenberg laboratory for sound and vibration research using the guidelines in ISO 3741 (2010). The results showed that the empty duct lead to a 15.3 dB(A) insulation loss with no decrease in thrust from the fan. The absorbent and micro perforated panel, however, lead to a 22.7 dB(A) insulation loss while giving a major decrease in thrust of more than one order of magnitude. The results show the failure of implementation of the latter two silencers due to choking, but also the success of the empty duct. This shows that there is room for improvement and perhaps even a future possibility of a successful implementation in a real vehicle.</p><p> </p>

corrected abstract:
<p>This report is about a novel approach to attenuation of fan noise in aerial vehicles, by way of implementing a ducted fan in the chassis of a four meter blended wing body plane. Three different one meter PVC pipes were used and their performances as silencers were tested by measuring the sound power level and calculating the insulation loss compared to a fan by itself. The ducts were either empty or lined with acoustic absorbents and micro perforated panels. Experiments were carried out in the reverberation room at KTH Marcus Wallenberg laboratory for sound and vibration research using the guidelines in ISO 3741 (2010). The results showed that the empty duct lead to a 15.3 dB(A) insulation loss with no decrease in thrust from the fan. The absorbent and micro perforated panel, however, lead to a 22.7 dB(A) insulation loss while giving a major decrease in thrust of more than one order of magnitude. The results show the failure of implementation of the latter two silencers due to choking, but also the success of the empty duct. This shows that there is room for improvement and perhaps even a future possibility of a successful implementation in a real vehicle.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1673538 
abstract is: 
<p>The purpose of the study is to evaluate the possibility of using gridded ion thrusters as a means of attitude control for a solar sail as a part of the sunshade project, which aims to place 10^8 solar sail sunshade spacecraft, each with an area of about 10 000 m^2, at the Sun-Earth Lagrangian point L1 in order to reduce Earth's global temperature. Two types of solar sail sunshade spacecraft were studied. The first type, referred to as the sunshade demonstrator, had an area of 100 m^2 and a mass of 10 kg, and the second type, referred to as the full-sized sunshade, had an area of 10 000 m^2 and a mass of 90 kg. To determine the significance of using ion thrusters for the attitude control system, the mass of the required fuel, as well as the total mass that had to be added to the spacecraft to implement the attitude control system, was calculated. Two types of journeys were studied for each spacecraft type: starting from Low Earth Orbit (LEO) to L1 and from Geostationary Orbit (GEO) to L1, respectively. The results showed that the duration of the journey of the full-sized spacecraft was about 570 days from LEO to L1 and 370 from GEO to L1, respectively. The required amounts of fuel for the respective journeys were 580 g and 15 g, respectively, and resulted in a total additional mass of 7.8 kg and 7.2 kg, respectively.</p><p> </p>

corrected abstract:
<p>The purpose of the study is to evaluate the possibility of using gridded ion thrusters as a means of attitude control for a solar sail as a part of the sunshade project, which aims to place 10<sup>8</sup> solar sail sunshade spacecraft, each with an area of about 10 000 m<sup>2</sup>, at the Sun-Earth Lagrangian point L<sub>1</sub> in order to reduce Earth's global temperature. Two types of solar sail sunshade spacecraft were studied. The first type, referred to as the sunshade demonstrator, had an area of 100 m<sup>2</sup> and a mass of 10 kg, and the second type, referred to as the full-sized sunshade, had an area of 10 000 m<sup>2</sup> and a mass of 90 kg. To determine the significance of using ion thrusters for the attitude control system, the mass of the required fuel, as well as the total mass that had to be added to the spacecraft to implement the attitude control system, was calculated. Two types of journeys were studied for each spacecraft type: starting from Low Earth Orbit (LEO) to L<sub>1</sub> and from Geostationary Orbit (GEO) to L<sub>1</sub>, respectively. The results showed that the duration of the journey of the full-sized spacecraft was about 570 days from LEO to L<sub>1</sub> and 370 from GEO to L<sub>1</sub>, respectively. The required amounts of fuel for the respective journeys were 580 g and 15 g, respectively, and resulted in a total additional mass of 7.8 kg and 7.2 kg, respectively.</p>
----------------------------------------------------------------------
In diva2:1780538 
abstract is: 
<p>The displacement of the flow by a passing freight train can often result in dangerous conditions for railway equipment and people standing in the vicinity of the train. In this work, Computational Fluid Dynamics (CFD) simulations are performed to study the flow development around a moving freight train comprised of a Class 66 locomotive and four container wagons. The results will give a better insight into the effects that each flow structure can have in the flow within the train's slipstream. Both two- and three-dimensional simulations are carried out around the freight train using three different RANS turbulence models: the Spalart-Allmaras, the SST k-ω and the W&amp;J EARSM. Two cases of 10o and 30o crosswinds are also considered and compared to the no-crosswind case, as side-winds characterize the majority of real-life situations and are known to amplify the slipstream effects. The results are validated against available experimental and numerical data and they are thoroughly presented and discussed. The 30o crosswind case is also computed using a DDES simulation. A meshing strategy which involves the assembly of different mesh blocks with a non-matching interface boundary condition to create the complete domain is used and assessed, as an alternative meshing approach that can simplify and accelerate the set-up of different case-studies. Additionally, the two-dimensional study is used to assess the influence of different parameters on the solution, such as the grid resolution and the moving-ground boundary condition.</p><p> </p>

corrected abstract:
<p>The displacement of the flow by a passing freight train can often result in dangerous conditions for railway equipment and people standing in the vicinity of the train. In this work, Computational Fluid Dynamics (CFD) simulations are performed to study the flow development around a moving freight train comprised of a Class 66 locomotive and four container wagons. The results will give a better insight into the effects that each flow structure can have in the flow within the train's slipstream. Both two- and three-dimensional simulations are carried out around the freight train using three different RANS turbulence models: the Spalart-Allmaras, the SST 𝑘-ω and the W&amp;J EARSM. Two cases of 10º and 30º crosswinds are also considered and compared to the no-crosswind case, as side-winds characterize the majority of real-life situations and are known to amplify the slipstream effects. The results are validated against available experimental and numerical data and they are thoroughly presented and discussed. The 30º crosswind case is also computed using a DDES simulation. A meshing strategy which involves the assembly of different mesh blocks with a non-matching interface boundary condition to create the complete domain is used and assessed, as an alternative meshing approach that can simplify and accelerate the set-up of different case-studies. Additionally, the two-dimensional study is used to assess the influence of different parameters on the solution, such as the grid resolution and the moving-ground boundary condition.</p>
----------------------------------------------------------------------
In diva2:1780172 
abstract is: 
<p>In this report, we analyze general relativistic effects on celestial bodies, including gravitational strength in different metrics, gravitational radiation, and frame-dragging. We present simulation methods for classical and general relativistic motion, through the use of systems of equations that may be numerically integrated. The amount of energy leaving the system as gravitational radiation is approximated using the quadrupole formula, and by using a binary pair of planetary bodies as an approximation for orbital motion. Here we demonstrate that classical approximations may be suitable in low-mass high-distance scenarios. The eccentricity of an orbit also affects the gravitational radiation and would have to be much less than one for reliable results. It is concluded that frame-dragging effects are negligible for slowly rotating objects only, which is a well-known result.</p><p> </p>

corrected abstract:
<p>In this report, we analyze general relativistic effects on celestial bodies, including gravitational strength in different metrics, gravitational radiation, and frame-dragging. We present simulation methods for classical and general relativistic motion, through the use of systems of equations that may be numerically integrated. The amount of energy leaving the system as gravitational radiation is approximated using the quadrupole formula, and by using a binary pair of planetary bodies as an approximation for orbital motion. Here we demonstrate that classical approximations may be suitable in low-mass high-distance scenarios. The eccentricity of an orbit also affects the gravitational radiation and would have to be much less than one for reliable results. It is concluded that frame-dragging effects are negligible for slowly rotating objects only, which is a well-known result.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1218168 - missing space in title:
"Classification of social gestures: Recognizing waving using supervised machinelearning"
==>
"Classification of social gestures: Recognizing waving using supervised machine learning"


abstract is: 
<p>This paper presents an approach to gesture recognition including the use of a tool in order to extract certain key-points of the human body in each frame, and then processing this data and extracting features from this. The gestures recognized were two-handed waving and clapping. The features used were the maximum co-variance from a sine-fit to time-series of arm angles, as well as the max and min of this fitted sinus function. A support vector machine was used for the learning. The result was a promising accuracy of 93% <em>± </em>4% using 5-fold cross-validation. The limitations of the methods used are then discussed, which includes lack of support for more than one gesture in the data as well as some lack of generality in means of the features used. Finally some suggestions are made as to what improvements and further explorations could be made.</p><p> </p>

corrected abstract:
<p>This paper presents an approach to gesture recognition including the use of a tool in order to extract certain key-points of the human body in each frame, and then processing this data and extracting features from this. The gestures recognized were two-handed waving and clapping. The features used were the maximum co-variance from a sine-fit to time-series of arm angles, as well as the max and min of this fitted sinus function. A support vector machine was used for the learning. The result was a promising accuracy of 93%±4% using 5-fold cross-validation. The limitations of the methods used are then discussed, which includes lack of support for more than one gesture in the data as well as some lack of generality in means of the features used. Finally some suggestions are made as to what improvements and further explorations could be made.</p>

Note - removed the empty paragraph and removed the italization of "±" to match the original
----------------------------------------------------------------------
In diva2:802173 
abstract is: 
<p>A bank borrowing some money has to give some securities to the lender, which is called collateral. Different kinds of collateral can be posted, like cash in different currencies or a stock portfolio depending on the terms of the contract, which is called a Credit Support Annex (CSA). Those contracts specify eligible collateral, interest rate, frequency of collateral posting, minimum transfer amounts, etc. This guarantee reduces the counterparty risk associated with this type of transaction.</p><p>If a CSA allows for posting cash in different currencies as collateral, then the party posting collateral can, now and at each future point in time, choose which currency to post. This choice leads to optionality that needs to be accounted for when valuing even the most basic of derivatives such as forwards or swaps.</p><p>In this thesis, we deal with the valuation of embedded optionality in collateral contracts. We consider the case when collateral can be posted in two different currencies, which seems sufficient since collateral contracts are soon going to be simplified.</p><p>This study is based on the conditional independence approach proposed by Piterbarg [8]. This method is compared to both Monte-Carlo simulation and finite- difference method.</p><p>A practical application is finally presented with the example of a contract between Natixis and Barclays.</p><p> </p>

corrected abstract:
<p>A bank borrowing some money has to give some securities to the lender, which is called collateral. Different kinds of collateral can be posted, like cash in different currencies or a stock portfolio depending on the terms of the contract, which is called a Credit Support Annex (CSA). Those contracts specify eligible collateral, interest rate, frequency of collateral posting, minimum transfer amounts, etc. This guarantee reduces the counterparty risk associated with this type of transaction.</p><p>If a CSA allows for posting cash in different currencies as collateral, then the party posting collateral can, now and at each future point in time, choose which currency to post. This choice leads to optionality that needs to be accounted for when valuing even the most basic of derivatives such as forwards or swaps.</p><p>In this thesis, we deal with the valuation of embedded optionality in collateral contracts. We consider the case when collateral can be posted in two different currencies, which seems sufficient since collateral contracts are soon going to be simplified.</p><p>This study is based on the conditional independence approach proposed by Piterbarg [8]. This method is compared to both Monte-Carlo simulation and finite-difference method.</p><p>A practical application is finally presented with the example of a contract between Natixis and Barclays.</p>
----------------------------------------------------------------------
In diva2:1440982 
abstract is: 
<p>This report presents the design of the wing structure for a UAV called Skywalker X8. A model of the UAV was given and analyzed to design a wing box structure that is twice the size of the current model, with "greener" technology and lightweight materials. The loads that act upon the UAV were simulated and thereafter analyzed with the help of the CFD program called Star CCM+. Modifications on the CAD model and the FEM simulations were performed in Siemens NX. Eight different combinations were tested from the following five materials: CFRP (carbon fiber reinforced polymer), LDPE (low density polyethylene), polyethylene, polypropylene, and balsa wood. The results that best fit the requirements given was the combination of polypropylene as the wing skin and balsa as the honeycomb structure. This design weighed 3.576 kg and had the following stresses: 0.671 MPa, 0.340 MPa, 1 MPa, and 4 MPa for the angle of attacks at 1,2,3, and 6 degrees respectively. A modification of the trailing edge, which was the implementation of a Gurney flap, was made to see if it improved the lift-to-drag ratio, but unfortunately it did not so it was not developed further.</p><p> </p>


corrected abstract:
<p>This report presents the design of the wing structure for a UAV called Skywalker X8. A model of the UAV was given and analyzed to design a wing box structure that is twice the size of the current model, with “greener” technology and lightweight materials. The loads that act upon the UAV were simulated and thereafter analyzed with the help of the CFD program called Star CCM+. Modifications on the CAD model and the FEM simulations were performed in Siemens NX. Eight different combinations were tested from the following five materials: CFRP (carbon fiber reinforced polymer), LDPE (low density polyethylene), polyethylene, polypropylene, and balsa wood. The results that best fit the requirements given was the combination of polypropylene as the wing skin and balsa as the honeycomb structure. This design weighed 3.576 kg and had the following stresses: 0.671 MPa, 0.340 MPa, 1 MPa, and 4 MPa for the angle of attacks at 1,2,3, and 6 degrees respectively. A modification of the trailing edge, which was the implementation of a Gurney flap, was made to see if it improved the lift-to-drag ratio, but unfortunately it did not so it was not developed further.</p>

Note error in original
mc='1,2,3' c='1, 2, 3'
Also removed unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:1220102 
abstract is: 
<p>Chatbots, also called conversational agents, with speech interfaces are being used to a greater and greater extent, but there are still many areas that are not completely explored. The idea of this project was born out of the belief that there is a need for an assistant in the kitchen that is able to search for recipes, answer questions regarding them and guide and assist the user throughout the cooking process, all through conversation since the hands are busy. This paper begins with an introduction in the subject of conversational agents and the related technology, then similar, already existing studies and methods are presented with their pros and cons. After follows an in-depth explanation on how the program was constructed into a working kitchen assistant. Lastly, the users’ experiences of the performance and usability of the program was evaluated through tests and discussed. It turns out that conversational agents definitely can be integrated in the kitchen, and according to several sources, in a few years they will be implemented in all possible areas and change the technology of our time.</p><p> </p>

corrected abstract:
<p>Chatbots, also called conversational agents, with speech interfaces are being used to a greater and greater extent, but there are still many areas that are not completely explored. The idea of this project was born out of the belief that there is a need for an assistant in the kitchen that is able to search for recipes, answer questions regarding them and guide and assist the user throughout the cooking process, all through conversation since the hands are busy. This paper begins with an introduction in the subject of conversational agents and the related technology, then similar, already existing studies and methods are presented with their pros and cons. After follows an in-depth explanation on how the program was constructed into a working kitchen assistant. Lastly, the users’ experiences of the performance and usability of the program was evaluated through tests and discussed. It turns out that conversational agents definitely can be integrated in the kitchen, and according to several sources, in a few years they will be implemented in all possible areas and change the technology of our time.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1880482 
abstract is: 
<p>A time-effective coverage path can be decisive in catastrophic and war scenarios for saving countless lives where UAVs are used to scan an area looking for an objective. Given an area shaped as a polygon, a quadratic decomposition method is used to discretize the area into nodes. A model of the optimization problem constraint is created and solved using mixed-integer linear programming, taking into consideration simple dynamics and coverage path planning definitions. Simulations in different scenarios are presented, showing that the presence of no-fly zones can negatively affect the coverage time. The relationship between coverage time and the number of UAVs employed is nonlinear and converges to a constant value. The result has a direct impact on the evaluation of benefits and the cost of adding UAVs to a search mission.</p><p> </p>

corrected abstract:
<p>A time-effective coverage path can be decisive in catastrophic and war scenarios for saving countless lives where UAVs are used to scan an area looking for an objective. Given an area shaped as a convex polygon, a grid-based decomposition method is used to discretize the area into squares, represented by nodes. An optimization problem, considering simple dynamics and coverage path planning definitions, is developed using mixed-integer linear programming framework. Simulations in different scenarios are presented, showing that the presence of no-fly zones can negatively affect the coverage time. The relationship between coverage time and the number of UAVs employed is nonlinear and converges to a constant value. The result has a direct impact on the evaluation of benefits and the cost of adding UAVs to search missions.</p>

Note the change in wording (based on the original) and remove of the unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:414817 
abstract is: 
<p>The purpose with this thesis work is to simulate the deflection due to creep of Kanthal(R) APMT furnace tubes using the finite element method (FEM). Kanthal APMT is a material which shows a larger primary creep compared to other metals. Therefore the creep deformation must be described with a material model which takes both primary and secondary creep into consideration. In this thesis work a material model called time hardening has been used.</p>
<p>*C2 is stress dependent. By modifying C2 so that the results from the simulations better corresponds with test data an equation for how C2 depends on the stress could be obtained.</p>
<p>The value for C2 is then calculated for each tube dimension giving results which are close to the data from sagging tests. The results may be seen as an overestimation of the actual deflection. A sensitivity analysis showed that the model is very sensitive to changes in the material parameters. A few percent change in C2 for example will change the deflection by more than 100 percent. </p>
<p>
<p>* For equation see full text</p>
<p>
<p> </p>
<p> </p>
</p>
</p>

corrected abstract:
<p>The purpose with this thesis work is to simulate the deflection due to creep of Kanthal® APMT furnace tubes using the finite element method (FEM). Kanthal APMT is a material which shows a larger primary creep compared to other metals. Therefore the creep deformation must be described with a material model which takes both primary and secondary creep into consideration. In this thesis work a material model called time hardening has been used.</p>
<p>C2 is stress dependent<sup><a href="#fn1" id="ref1">*</a></sup>. By modifying C2 so that the results from the simulations better corresponds with test data an equation for how C2 depends on the stress could be obtained.</p>
<p>The value for C2 is then calculated for each tube dimension giving results which are close to the data from sagging tests. The results may be seen as an overestimation of the actual deflection. A sensitivity analysis showed that the model is very sensitive to changes in the material parameters. A few percent change in C2 for example will change the deflection by more than 100 percent.</p>
<div id="footnotes">
    <ul style="list-style: '*'; padding-left: 20px;">
        <li id="fn1">For equation see full text <a href="#ref1" aria-label="Back to reference">↩</a></li>
    </ul>
</div>

----------------------------------------------------------------------
In diva2:894096 
abstract is: 
<p>E-sports is growing and the price pools in e-sports tournaments are increasing, Valves video game DotA 2 is one of the bigger e-sports. As professional gamers train to increase their skill, new tools to help the training might become very important. Eye tracking can give an extra training dimension for the gamer. The aim of this master thesis is to develop a Visual Attention Index for DotA 2, that is, a number that reflects the player’s visual attention during a game. Interviews with gamers combined with data collection from gamers with eye trackers and statistical methods were used to find relevant metrics to use in the work. The results show that linear regression did not work very well on the data set, however, since there were a low number of test persons, further data collection and testing needs to be done before any statistically significant conclusions can be drawn. Support Vector Machines (SVM) was also used and turned out to be an effective way of separating better players from less good players. A new SVM method, based on linear programming, was also tested and found to be efficient and easy to apply on the given data set.</p><p> </p>

corrected abstract:
<p>E-sports is growing and the price pools in e-sports tournaments are increasing, Valves video game DotA <span style="font-size: 0.8em;">&#x1D7E4;</span> is one of the bigger e-sports. As professional gamers train to increase their skill, new tools to help the training might become very important. Eye tracking can give an extra training dimension for the gamer. The aim of this master thesis is to develop a Visual Attention Index for DotA <span style="font-size: 0.8em;">&#x1D7E4;</span>, that is, a number that reflects the player’s visual attention during a game. Interviews with gamers combined with data collection from gamers with eye trackers and statistical methods were used to find relevant metrics to use in the work. The results show that linear regression did not work very well on the data set, however, since there were a low number of test persons, further data collection and testing needs to be done before any statistically significant conclusions can be drawn. Support Vector Machines (SVM) was also used and turned out to be an effective way of separating better players from less good players. A new SVM method, based on linear programming, was also tested and found to be efficient and easy to apply on the given data set.</p>

Note removed the unnecessary empty paragraph and change "2" to "&#x1D7E4;" - a Mathematical Sans-Serif Digit Two;  <span style="font-size: 0.8em;"> has been used to reduce the relative font size to match the look of the original
----------------------------------------------------------------------
In diva2:1319924 
abstract is: 
<p>Credit scoring using machine learning has been gaining attention within the research field in recent decades and it is widely used in the financial sector today. Studies covering binary credit scoring of securitized non-performing loans are however very scarce. This paper is using random forest and artificial neural networks to predict debt recovery for such portfolios. As a performance benchmark, logistic regression is used. Due to the nature of high imbalance between the classes, the performance is evaluated mainly on the area under both the receiver operating characteristic curve and the precision-recall curve. This paper shows that random forest, artificial neural networks and logistic regression have similar performance. They all indicate an overall satisfactory ability to predict debt recovery and hold potential to be implemented in day-to-day business related to non-performing loans.</p><p> </p>

corrected abstract:
<p>Credit scoring using machine learning has been gaining attention within the research field in recent decades and it is widely used in the financial sector today. Studies covering binary credit scoring of securitized non-performing loans are however very scarce. This paper is using random forest and artificial neural networks to predict debt recovery for such portfolios. As a performance benchmark, logistic regression is used. Due to the nature of high imbalance between the classes, the performance is evaluated mainly on the area under both the receiver operating characteristic curve and the precision-recall curve. This paper shows that random forest, artificial neural networks and logistic regression have similar performance. They all indicate an overall satisfactory ability to predict debt recovery and hold potential to be implemented in day-to-day business related to non-performing loans.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1568336 
abstract is: 
<p>X-ray computed tomography (CT) has since its introduction in the early 1970s become one of the most important tools used for medical imaging. In CT, a large number of x-ray attenuation measurements are combined and reconstructed to form a three-dimensional image of the targeted area. In the recent years, a new type of detector called photon counting detector (PCD) has attracted considerable interest. This new type of detector acquires spectral information is associated with several benefits and has shown to be very valuable. </p><p>Furthermore, the use of deep learning to reconstruct images produced by CT has attracted significant attention in the last couple of years. However, the best way of incorporating deep learning into the reconstruction chain into the reconstruction chain is still incompletely understood. Additionally, the use of deep learning has mainly been investigated for the case of conventional CT and not for CT performed with PCDs. It these two points that this work aims to address. </p><p>Multiple deep learning architectures were implemented and evaluated on material images acquired by simulating a PCD. The deep-learning part of the reconstruction took the form of image-domain denoising after the material images had been obtained from the material sinograms through filtered back projection. Then, a comparison between the different deep learning architectures was made to find out which architecture is the most suited for denoising images produced by PCDs in the image domain.</p><p> </p>

corrected abstract:
<p>X-ray computed tomography (CT) has since its introduction in the early 1970s become one of the most important tools used for medical imaging. In CT, a large number of x-ray attenuation measurements are combined and reconstructed to form a three-dimensional image of the targeted area. In the recent years, a new type of detector called photon counting detector (PCD) has attracted considerable interest. This new type of detector acquires spectral information is associated with several benefits and has shown to be very valuable.</p><p>Furthermore, the use of deep learning to reconstruct images produced by CT has attracted significant attention in the last couple of years. However, the best way of incorporating deep learning into the reconstruction chain into the reconstruction chain is still incompletely understood. Additionally, the use of deep learning has mainly been investigated for the case of conventional CT and not for CT performed with PCDs. It these two points that this work aims to address.</p><p>Multiple deep learning architectures were implemented and evaluated on material images acquired by simulating a PCD. The deep-learning part of the reconstruction took the form of image-domain denoising after the material images had been obtained from the material sinograms through filtered back projection. Then, a comparison between the different deep learning architectures was made to find out which architecture is the most suited for denoising images produced by PCDs in the image domain.</p>

Note - only change to remove the empty paragraph and removing unnecessary spaces at end of paragraphs.
----------------------------------------------------------------------
In diva2:1334765 
abstract is: 
<p>A global statement about a compact surface with constant Gaussian curvature is derived by elementary differential geometry methods. Surfaces and curves embedded in three-dimensional Euclidian space are introduced, as well as several key properties such as the tangent plane, the first and second fundamental form, and the Weingarten map. Furthermore, intrinsic and extrinsic properties of surfaces are analyzed, and the Gaussian curvature, originally derived as an extrinsic property, is proven to be an intrinsic property in Gauss Theorema Egregium. Lastly, through the aid of umbilical points on a surface, the statement that a compact, connected surface with constant Gaussian curvature is a sphere is proven.</p><p> </p>

corrected abstract:
<p>A global statement about a compact surface with constant Gaussian curvature is derived by elementary differential geometry methods. Surfaces and curves embedded in three-dimensional Euclidian space are introduced, as well as several key properties such as the tangent plane, the first and second fundamental form, and the Weingarten map. Furthermore, intrinsic and extrinsic properties of surfaces are analyzed, and the Gaussian curvature, originally derived as an extrinsic property, is proven to be an intrinsic property in Gauss Theorema Egregium. Lastly, through the aid of umbilical points on a surface, the statement that a compact, connected surface with constant Gaussian curvature is a sphere is proven.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1682470 
abstract is: 
<p>We present and prove some important theorems regarding determinantal point processes. In particular we focus on existance and uniqueness theorems. Furthermore, we present an algorithm for generating determinantal point processes with a finite-dimensional projection kernel. Also, we go through the mathematical preliminaries required to understand the theory.</p><p> </p>

corrected abstract:
<p>We present and prove some important theorems regarding determinantal point processes. In particular we focus on existance and uniqueness theorems. Furthermore, we present an algorithm for generating determinantal point processes with a finite-dimensional projection kernel. Also, we go through the mathematical preliminaries required to understand the theory.</p>

Note spelling error:
----------------------------------------------------------------------
In diva2:1380177 
abstract is: 
<p>The objective of this project is the development of a mission analysis tool for the nanosatellite company GomSpace Sweden. Although there are many existing software, they can be quite complicated and time consuming to use. The goal of this work is to build a simple app to be used at the earliest stages of space missions in order to obtain key figures of merit quickly and easily. By comparing results, assessing the feasibility of customer needs, analysing how various parameters affect each other, it enables immediate deeper understanding of the implications of the main design decisions that are taken at the very beginning of a mission. The tool shall aid the system engineering process of determining orbit manoeuvre capability specifically for CubeSat electric propulsion systems taking into account the most relevant factors for perturbation in Low Earth Orbit (LEO), i.e. atmospheric drag and Earth’s oblateness effects. The manoeuvres investigated are: orbit raising from an insert orbit to an operating orbit, orbit maintenance, deorbiting within the space debris mitigation guidelines and collision avoidance within the 12 to 24 hours that the system has to react. The manoeuvres cost is assessed in terms of Delta v requirements, propellant mass and transfer times. The tool was developed with MATLAB and packaged as a standalone Linux application.</p><p> </p>

corrected abstract:
<p>The objective of this project is the development of a mission analysis tool for the nanosatellite company GomSpace Sweden. Although there are many existing software, they can be quite complicated and time consuming to use. The goal of this work is to build a simple app to be used at the earliest stages of space missions in order to obtain key figures of merit quickly and easily. By comparing results, assessing the feasibility of customer needs, analysing how various parameters affect each other, it enables immediate deeper understanding of the implications of the main design decisions that are taken at the very beginning of a mission. The tool shall aid the system engineering process of determining orbit manoeuvre capability specifically for CubeSat electric propulsion systems taking into account the most relevant factors for perturbation in Low Earth Orbit (LEO), i.e. atmospheric drag and Earth’s oblateness effects. The manoeuvres investigated are: orbit raising from an insert orbit to an operating orbit, orbit maintenance, deorbiting within the space debris mitigation guidelines and collision avoidance within the 12 to 24 hours that the system has to react. The manoeuvres cost is assessed in terms of ∆v requirements, propellant mass and transfer times. The tool was developed with MATLAB and packaged as a standalone Linux application.</p>

Note - only change to remove the empty paragraph and replacing "Delta" with "∆".
----------------------------------------------------------------------
In diva2:1524971 
abstract is: 
<p>In this master thesis an input-model of a Nordic BWR power plant has been developed in APROS. The plant model contains key systems and major thermohydraulic components of the steam cycle, including I&amp;C systems (i.e. power, pressure, level and flow controls). The plant model is primarily designed for balance of plant studies at discrete power levels.</p><p>The input-model of the power plant focuses especially on the steam cycle which is crucial for analysing water and steam behaviour and its influence on the reactor power. At the current stage, the model primarily handles steady-state conditions of full-power operation, which has been the design point. It has also been shown that reduced-power operation can be simulated with a reasonable trendline of pressure and temperature progression over facility components.</p><p> </p>

corrected abstract:
<p>Nuclear power plants have proved to produce reliable and economic electricity but at the same time provoked debates mainly because of the risks involved during operation. To prevent unwanted events, it is important to identify and estimate their occurrence, and introduce measures to counteract them. Modelling and simulations are powerful tools that can be used to gain this type of knowledge and obtain information about the birthplace of incidents. However, the area of use is not limited to the safety perspective, as computer simulations can also introduce true advances in performance and reliability of power plants.</p><p>This master thesis was conducted at Westinghouse Electric Sweden AB, with the purpose to design and implement an input-model of a Nordic Boiling Water Reactor in APROS. The inputmodel of the power plant focused especially on the steam cycle which is crucial for analysing water and steam behaviour in the power plant and its influence on the reactor power. The inputmodel has been limited to representsteady-state conditions at full-power operation, and to some extent reduced-power operation. Thereby, plant model is primarily designed for Balance of Plant studies at discrete power levels.</p><p>The first section of the report contains an introduction to the concepts of nuclear energy and fundamentals of boiling water reactors. It is supposed to provide the reader with a basis for a fair understanding of nuclear power plant operation. Theoretical concepts of thermodynamics and fluid mechanics, which have been crucial for a proper approach in the process of creating the input-model, can be found in the theory section. The report does also contain a brief description of the plant systems upon which the design has been based on.</p><p>The report consists of further sections, where the model components and their implementation are presented followed by a model validation. The model validation is performed by a comparison approach, where simulation data is presented in relationship to reference data. The validation is done for full-power and reduce-power operation, at steady-state conditions, at which the model has shown to have decent compliance with the available reference data.</p><p>At the final stage of the project, the created input-model was used to evaluate an induced perturbation of feedwater temperature. The behaviour of the reactor, dependent of the feedwater temperature, is discussed for two simulation cases; with and without forced power control. The simulation enabled to perform a first step analysis of the effectiveness of the power control system.</p>

Note the abstract in the thesis is completely different from that in DiVA!
----------------------------------------------------------------------
In diva2:699782 
abstract is: 
<p>The dose distribution in the Gamma Knife (developed and produced by Elekta) is optimized over the weights (or Beam-on time) using different models other than the radiosurgical one used in Leksell Gamma Plan . These are based on DVH, EUD, TCP and NTCP. Also adding hypoxic regions are tested in the Gamma Knife to see whether or not the dose can be guided to these areas. This is done in two ways. For the DVH and EUD model the hypoxic area is regarded as a organ by itself and higher constraints is defined on it. In the TCP case blood vessels are outlined and the α and β parameters are perturbed to describe a hypoxic area. The models are tested in two cases. The first one is one tumour close to the brainstem and the second case is two tumours located far away from each other. Finally the results are compared to the dose distribution computed by the Gamma Knife.</p><p> </p>

corrected abstract:
<p>The dose distribution in the Gamma Knife is optimized over the weights (or Beam-on time) using different models other than the radiosurgical one used in Leksell Gamma Plan®. These are based on DVH, EUD, TCP and NTCP. Also adding hypoxic regions are tested in the Gamma Knife to see whether or not the dose can be guided to these areas. This is done in two ways. For the DVH and EUD model the hypoxic area is regarded as a organ by itself and higher constraints is defined on it. In the TCP case blood vessels are outlined and the α and β parameters are perturbed to describe a hypoxic area. The models are tested in two cases. The first one is one tumour close to the brainstem and the second case is two tumours located far away from each other. Finally the results are compared to the dose distribution computed by the Gamma Knife.</p>

Nore removed text that was not in the original, added the registered trademark symbol, and eliminated the empty paragraph at the end
----------------------------------------------------------------------
In diva2:1519571 
abstract is: 
<p>For the purpose of americium recycling, the effect of americium content on the nuclear fuel behaviour needs to be investigated. Atomic scale simulations and classical molecular dynamic simulations provide a tool of choice for the study of thermophysical properties of the nuclear fuel.</p><p>In this work, we fitted a new interatomic empirical potential for (U,Am)O2 based on the CRG formalism. Our work enabled us to propose at the same time a new potential for the study of the Am-O system. The proposed potentials show good agreement with lattice parameters and enthalpy increments. We finally computed the heat capacity of (U,Am)O2 from 350 K to 3200 K for 0, 10, 20, 30, 40 and 50% americium contents using the potential obtained. The heat capacities calculated reveal a Bredig transition, as seen in UO2 and (U,Pu)O2. This transition shifts toward lower temperatures and its peak decreases in intensity when the Am content increases.</p><p> </p>

corrected abstract:
<p>For the purpose of americium recycling, the effect of americium content on the nuclear fuel behaviour needs to be investigated. Atomic scale simulations and classical molecular dynamic simulations provide a tool of choice for the study of thermophysical properties of the nuclear fuel.</p><p>In this work, we fitted a new interatomic empirical potential for (U,Am)O<sub>2</sub> based on the CRG formalism. Our work enabled us to propose at the same time a new potential for the study of the Am-O system. The proposed potentials show good agreement with lattice parameters and enthalpy increments. We finally computed the heat capacity of (U,Am)O<sub>2</sub> from 350 K to 3200 K for 0, 10, 20, 30, 40 and 50% americium contents using the potential obtained. The heat capacities calculated reveal a Bredig transition, as seen in UO<sub>2</sub> and (U,Pu)O<sub>2</sub>. This transition shifts toward lower temperatures and its peak decreases in intensity when the Am content increases.</p>

Note - only change to remove the empty paragraph and adding the subscripts
----------------------------------------------------------------------
In diva2:1779375 
abstract is: 
<p>The purpose of this project is to simulate the detection of γ-ray spectra emitted by radon isotopes and their daughters. This is done as a contribution to the development of radiation detectors to be used in a research project investigating the possibility of using increased amounts of the radioactive gas radon as an earthquake precursor. Before the onset of an earthquake, microcracks are formed in the surrounding stone structures due to stress, releasing greater than usual amounts of radon gas contained within the rock pores. A way of predicting an upcoming earthquake would then be to place radiation detectors in areas with high seismicity in order to measure possible changes. This could be done in soil, groundwater (via springs, wells, and boreholes), or air. In this project, we aim to understand how measurements in groundwater would differ from ones in air, and how to best make use of the spectra as seen in water. This was done by simulating a scenario in which a scintillator detector, made of cesium iodide, is placed in each media and then assessing the resulting γ-ray spectra.</p><p> </p>

corrected abstract:
<p>The purpose of this project is to simulate the detection of γ-ray spectra emitted by radon isotopes and their daughters. This is done as a contribution to the development of radiation detectors to be used in a research project investigating the possibility of using increased amounts of the radioactive gas radon as an earthquake precursor. Before the onset of an earthquake, microcracks are formed in the surrounding stone structures due to stress, releasing greater than usual amounts of radon gas contained within the rock pores. A way of predicting an upcoming earthquake would then be to place radiation detectors in areas with high seismicity in order to measure possible changes. This could be done in soil, groundwater (via springs, wells, and boreholes), or air. In this project, we aim to understand how measurements in groundwater would differ from ones in air, and how to best make use of the spectra as seen in water. This was done by simulating a scenario in which a scintillator detector, made of cesium iodide, is placed in each media and then assessing the resulting γ-ray spectra.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:373694
Note: no full text in DiVA

abstract is: 
<p>The department of Medical Technology, where I have done Master thesis project, develops and researches new method and technique within areas where ultrasound can be used to obtain the image of anatomical structure, functional capabilities and to suggest required treatment.</p>
<p>Nowadays cardio-vascular diseases, such as infarct, atherosclerosis and ischemic syndrome, are one of the most widespread diseases in the world that’s why timely detection, identification and treatment are so important.</p>
<p>The Master of Science qualification report consists 3 major parts: Medico-biological part, Design and Research parts.</p>
<p><strong>In Medico-biological part </strong>has been analyzed anatomical and physiological structure of the heart, current status of echocardiography with comparing with other techniques, summary of ultrasound methods with list of parameters that can be achieved is presented.</p>
<p><strong>In Design part </strong>has been developed new graphical modality based on Delta-V pump model using vector based statistical analysis for identification patients with ischemia. Software algorithm for automatically determine characteristic points for state diagram written in MatLab has been developed and implemented.</p>
<p><strong>In Research part </strong>in the first task using commercially available software based on Principal Component Analysis collected data from the hospital patients has been studied, results proved hypothesis concerning time variables importance; in the second task graphical module has been examined using collected data from the hospital patients both normal and with different cardio-vascular disease, and the results show good detection power of the algorithm.</p>
<p>At the end of the project presentation has been done and report has been published.</p>
<p>This project has been done in collaboration with the biggest medical institute in Sweden – Karolinska Institute - and results will be used in medical practice in Karolinska University Hospital in Huddinge and for future scientific needs.</p>
<p> </p>

corrected abstract:
<p>The department of Medical Technology, where I have done Master thesis project, develops and researches new method and technique within areas where ultrasound can be used to obtain the image of anatomical structure, functional capabilities and to suggest required treatment.</p>
<p>Nowadays cardio-vascular diseases, such as infarct, atherosclerosis and ischemic syndrome, are one of the most widespread diseases in the world that’s why timely detection, identification and treatment are so important.</p>
<p>The Master of Science qualification report consists 3 major parts: Medico-biological part, Design and Research parts.</p>
<p><strong>In Medico-biological part </strong>has been analyzed anatomical and physiological structure of the heart, current status of echocardiography with comparing with other techniques, summary of ultrasound methods with list of parameters that can be achieved is presented.</p>
<p><strong>In Design part </strong>has been developed new graphical modality based on Delta-V pump model using vector based statistical analysis for identification patients with ischemia. Software algorithm for automatically determine characteristic points for state diagram written in MatLab has been developed and implemented.</p>
<p><strong>In Research part </strong>in the first task using commercially available software based on Principal Component Analysis collected data from the hospital patients has been studied, results proved hypothesis concerning time variables importance; in the second task graphical module has been examined using collected data from the hospital patients both normal and with different cardio-vascular disease, and the results show good detection power of the algorithm.</p>
<p>At the end of the project presentation has been done and report has been published.</p>
<p>This project has been done in collaboration with the biggest medical institute in Sweden – Karolinska Institute - and results will be used in medical practice in Karolinska University Hospital in Huddinge and for future scientific needs.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1673952 
abstract is: 
<p>The conventional tube-and-wing aircraft has been around since the 1950s, with little to no innovative progress being made towards redesigning the conventional aircraft. The blended wing body (BWB) shape fuses the wing of the aircraft with the fuselage increasing structural strength while also increasing potential surface area to create lift, making it more efficient than conventional wing shapes. Today aviation has a 2 % CO2 contribution to global emissions. Aircraft manufacturers are predicting a steady rise for the aviation industry. The contribution of green-house gases is set to increase exponentially. Hydrogen fuel cells could deem a good fit between traditional combustion engine aircraft and electrical aircraft having a high efficiency but also being fuel-based. This report investigates the possibility of a prototype model of the Project ''Green Raven'' from KTH of creating a hybrid fuel cell BWB UAV with a 4 m wingspan. The analytical data is from literature and available benchmark data. First, an electrically driven subscale prototype is made and tested, and then the full-scale model is made. The prototype is pro-posed to be driven by a single two-bladed propeller with 10 x 4.7-inch dimensions running at 10000-13000 rpm with a takeoff weight of 4 kg, where 0.75 kg of the weight was from 5 Li-Po batteries. Performance parameters were calculated by given data with a given cruise speed of 30 m/s and a cruise endurance of 1 hour. The prototype will fly for close to maximum load at climb with an angle of 6°. With the Li-Po batteries with a total of 11 Ah, the aircraft has more than 10 % to spare for safety reasons.</p><p> </p>

corrected abstract:
<p>The conventional tube-and-wing aircraft has been around since the 1950s, with little to no innovative progress being made towards redesigning the conventional aircraft. The blended wing body (BWB) shape fuses the wing of the aircraft with the fuselage increasing structural strength while also increasing potential surface area to create lift, making it more efficient than conventional wing shapes. Today aviation has a 2 % CO<sub>2</sub> contribution to global emissions. Aircraft manufacturers are predicting a steady rise for the aviation industry. The contribution of greenhouse gases is set to increase exponentially. Hydrogen fuel cells could deem a good fit between traditional combustion engine aircraft and electrical aircraft having a high efficiency but also being fuel-based. This report investigates the possibility of a prototype model of the Project ”Green Raven” from KTH of creating a hybrid fuel cell BWB UAV with a 4 m wingspan. The analytical data is from literature and available benchmark data. First, an electrically driven subscale prototype is made and tested, and then the full-scale model is made. The prototype is proposed to be driven by a single two-bladed propeller with 10 x 4.7-inch dimensions running at 10000-13000 rpm with a takeoff weight of 4 kg, where 0.75 kg of the weight was from 5 Li-Po batteries. Performance parameters were calculated by given data with a given cruise speed of 30 m/s and a cruise endurance of 1 hour. The prototype will fly for close to maximum load at climb with an angle of 6°. With the Li-Po batteries with a total of 11 Ah, the aircraft has more than 10 % to spare for safety reasons.</p>

Note - only changes to remove the empty paragraph, added subscript, and removed the unnecessary hpyehn in "greenhouse".
----------------------------------------------------------------------
In diva2:1342226 
abstract is: 
<p>Functional testing is a vital process when building a satellite. However, often using flight-ready hardware for testing is not feasible. The work in this project has been to construct a flight representative model of the antenna deployment system for the KTH student-built MIST satellite. Specifically, the focus has been on creating a physical simulator for the antenna system. The purpose of the simulator created is to achieve the correct behavior, but without the need to use the real flight hardware. The challenges mainly concern establishing communication between the on-board computer of the satellite and the microcontroller on the created antenna deployment system, via the I$^2$C bus, and ensuring that physical responses occur in a useful manner. Further, the simulator needed to implement software with the same functionality as the real system. The microcontroller used in this project was an Arduino Due that represented the antenna deployment system's microcontroller. All the functions, e.g. temperature sensor and LEDs, were put together on a custom-made add-on circuit for the Arduino. Moreover, a 3D-printed model has been made for the deployment mechanism of the antenna elements. A simulation of the antenna system has been produced, determining whether a custom-built simulator can be used for functional testing of the antenna deployment system. The simulator can later be used for functional testing of the MIST satellite and also be the base for testing the deployment of the solar panels.</p><p> </p>

corrected abstract:
<p>Functional testing is a vital process when building a satellite. However, often using flight-ready hardware for testing is not feasible. The work in this project has been to construct a flight representative model of the antenna deployment system for the KTH student-built MIST satellite. Specifically, the focus has been on creating a physical simulator for the antenna system. The purpose of the simulator created is to achieve the correct behavior, but without the need to use the real flight hardware.</p><p>The challenges mainly concern establishing communication between the on-board computer of the satellite and the microcontroller on the created antenna deployment system, via the I<sup>2</sub>C bus, and ensuring that physical responses occur in a useful manner. Further, the simulator needed to implement software with the same functionality as the real system. The microcontroller used in this project was an Arduino Due that represented the antenna deployment system's microcontroller.</p><p>All the functions, e.g. temperature sensor and LEDs, were put together on a custom-made add-on circuit for the Arduino. Moreover, a 3D-printed model has been made for the deployment mechanism of the antenna elements. A simulation of the antenna system has been produced, determining whether a custom-built simulator can be used for functional testing of the antenna deployment system. The simulator can later be used for functional testing of the MIST satellite and also be the base for testing the deployment of the solar panels.</p>
----------------------------------------------------------------------
In diva2:1334807 
abstract is: 
<p>In this report, we study an abstract representation of reflection groups called Coxeter groups. Firstly, we introduce some important aspects of group theory. Next, we describe a concept called the word problem. Then, a way of defining groups given a set of generators and relations is presented. This theory is used to define the Coxeter groups, followed by a complete classification of the finite Coxeter groups as presented by H.S.M. Coxeter in 1935. Finally, we present a solution to the word problem for Coxeter groups and discuss some applications.</p><p> </p>

corrected abstract:
<p>In this report, we study an abstract representation of reflection groups called Coxeter groups. Firstly, we introduce some important aspects of group theory. Next, we describe a concept called the word problem. Then, a way of defining groups given a set of generators and relations is presented. This theory is used to define the Coxeter groups, followed by a complete classification of the finite Coxeter groups as presented by H.S.M. Coxeter in 1935. Finally, we present a solution to the word problem for Coxeter groups and discuss some applications.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:839498 
abstract is: 
<p>Our immune system uses antibodies to neutralize pathogens such as bacteria and viruses. Antibodies bind to parts of foreign proteins with high efficiency and specificity. We call such parts epitopes. The identification of epitopes, namely epitope mapping, may contribute to various immunological applications such as vaccine design, antibody production and immunological diagnosis.</p><p>Therefore, a fast and reliable method that can predict epitopes from the whole proteome is highly desirable.</p><p> </p><p>In this work we have developed a computational method that predicts epitopes based on sequence information. We focus on using local alignment to extract features from peptides and classifying them using Support Vector Machine. We also propose two approaches to optimize the features. Results show that our method can reliably predict epitopes and significantly outperforms some most commonly used tools.</p><p> </p>

corrected abstract:
<p>Our immune system uses antibodies to neutralize pathogens such as bacteria and viruses. Antibodies bind to parts of foreign proteins with high efficiency and specificity. We call such parts epitopes. The identification of epitopes, namely epitope mapping, may contribute to various immunological applications such as vaccine design, antibody production and immunological diagnosis. Therefore, a fast and reliable method that can predict epitopes from the whole proteome is highly desirable.</p><p>In this work we have developed a computational method that predicts epitopes based on sequence information. We focus on using local alignment to extract features from peptides and classifying them using Support Vector Machine. We also propose two approaches to optimize the features. The results show that our method can reliably predict epitopes and significantly outperforms some most commonly used tools.</p>
----------------------------------------------------------------------
In diva2:1879496 
abstract is: 
<p>Life insurance companies rely on mortality rate models to set appropriate premiums for their services. Over the past century, average life expectancy has increased and continues to do so, necessitating more accurate models. Two commonly used models are the Gompertz-Makeham law of mortality and the Lee-Carter model. The Gompertz-Makeham model depends solely on an age variable, while the Lee-Carter model incorporates a time-varying aspect which accounts for the increase in life expectancy over time. This paper constructs both models using training data acquired from Skandia Mutual Life Insurance Company and compares them to validation data from the same set. The study suggests that the Lee-Carter model may be able to offer some improvements compared to the Gompertz-Makeham law of mortality in terms of predicting future mortality rates. However, due to a lack of qualitative data, creating a competitive Lee-Carter model through Singular Value Decomposition, SVD, proved to be problematic. Switching from the current Gompertz-Makeham model to the Lee-Carter model should, therefore, be explored further when more high quality data becomes available.</p><p> </p>

corrected abstract:
<p>Life insurance companies rely on mortality rate models to set appropriate premiums for their services. Over the past century, average life expectancy has increased and continues to do so, necessitating more accurate models. Two commonly used models are the Gompertz-Makeham law of mortality and the Lee-Carter model. The Gompertz-Makeham model depends solely on an age variable, while the Lee-Carter model incorporates a time-varying aspect which accounts for the increase in life expectancy over time. This paper constructs both models using training data acquired from Skandia Mutual Life Insurance Company and compares them to validation data from the same set. The study suggests that the Lee-Carter model may be able to offer some improvements compared to the Gompertz-Makeham law of mortality in terms of predicting future mortality rates. However, due to a lack of qualitative data, creating a competitive Lee-Carter model through Singular Value Decomposition, SVD, proved to be problematic. Switching from the current Gompertz-Makeham model to the Lee-Carter model should, therefore, be explored further when more high quality data becomes available.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1800189 
abstract is: 
<p>In this master thesis, the accuracy of the crack depth meter RMG 4015 was evaluated for different types of cracks with various damage mechanisms. In total, 61 crack depth measurements were conducted with the crack depth meter on 56 cracks which were located in the 23 different test pieces supplied by Kiwa. The measured crack depths were then compared to the true crack depths, which were determined by cutting the test pieces and measuring directly on the cross-sections of the cracks using a light optical microscope. The results of the comparison showed that the RMG 4015, which uses potential drop techniques, was very accurate at measuring both strain induced and alkaline stress corrosion cracks. However, the results also showed that the crack depth meter underestimates chloride induced stress corrosion cracks, corrosion fatigue cracks and stress corrosion cracks/hydrogen embrittlement cracks at varying degrees. Therefore, the main recommendation for Kiwa is to switch the RMG 4015 to a crack depth meter that uses ultrasonic techniques instead.</p><p>The master thesis also explored the possibilities to improve an FE model produced by Kiwa in a previous project which involved an analysis of a cracked component. The present crack depth measure program included a test piece from this component. The stress distribution in the original model did not represent the cracks found in the real structure and it was suspected to be the result of some boundary conditions not corresponding to those acting in the actual pipe system. Some adjustments to the boundary conditions and contact regions were made and a new improved model with a better representing stress distribution was found.</p><p> </p>

corrected abstract:
<p>In this master thesis, the accuracy of the crack depth meter RMG 4015 was evaluated for different types of cracks with various damage mechanisms. In total, 61 crack depth measurements were conducted with the crack depth meter on 56 cracks which were located in the 23 different test pieces supplied by Kiwa. The measured crack depths were then compared to the true crack depths, which were determined by cutting the test pieces and measuring directly on the cross-sections of the cracks using a light optical microscope. The results of the comparison showed that the RMG 4015, which uses potential drop techniques, was very accurate at measuring both strain induced and alkaline stress corrosion cracks. However, the results also showed that the crack depth meter underestimates chloride induced stress corrosion cracks, corrosion fatigue cracks and stress corrosion cracks/hydrogen embrittlement cracks at varying degrees. Therefore, the main recommendation for Kiwa is to switch the RMG 4015 to a crack depth meter that uses ultrasonic techniques instead.</p><p>The master thesis also explored the possibilities to improve an FE model produced by Kiwa in a previous project which involved an analysis of a cracked component. The present crack depth measure program included a test piece from this component. The stress distribution in the original model did not represent the cracks found in the real structure and it was suspected to be the result of some boundary conditions not corresponding to those acting in the actual pipe system. Some adjustments to the boundary conditions and contact regions were made and a new improved model with a better representing stress distribution was found.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1111160 
abstract is: 
<p>Gaussian process methods are flexible non-parametric Bayesian methods used for regression and classification. They allow for explicit handling of uncertainty and are able to learn complex structures in the data. Their main limitation is their scaling characteristics: for n training points the complexity is <em>O</em>(n³) for training and <em>O</em>(n²) for prediction per test data point.</p><p>This makes full Gaussian process methods prohibitive to use on training sets larger than a few thousand data points. There has been recent research on approximation methods to make Gaussian processes scalable without severely affecting the performance. Some of these new approximation techniques are still not fully investigated and in a practical situation it is hard to know which method to choose. This thesis examines and evaluates scalable GP methods, especially focusing on the framework Massively Scalable Gaussian Processes introduced by Wilson et al. in 2016, which reduces the training complexity to nearly <em>O</em>(<em>n</em>) and the prediction complexity to <em>O</em>(1). The framework involves inducing point methods, local covariance function interpolation, exploitations of structured matrices and projections to low-dimensional spaces. The properties of the different approximations are studied and the possibilities of making improvements are discussed.</p><p> </p>

corrected abstract:
<p>Gaussian process methods are flexible non-parametric Bayesian methods used for regression and classification. They allow for explicit handling of uncertainty and are able to learn complex structures in the data. Their main limitation is their scaling characteristics: for 𝑛 training points the complexity is &Oscr;(𝑛<sup>3</sup>) for training and &Oscr;(𝑛<sup>2</sup>) for prediction per test data point. This makes full Gaussian process methods prohibitive to use on training sets larger than a few thousand data points.</p><p>There has been recent research on approximation methods to make Gaussian processes scalable without severely affecting the performance. Some of these new approximation techniques are still not fully investigated and in a practical situation it is hard to know which method to choose. This thesis examines and evaluates scalable GP methods, especially focusing on the framework Massively Scalable Gaussian Processes introduced by Wilson et al. in 2016, which reduces the training complexity to nearly &Oscr;(𝑛) and the prediction complexity to &Oscr;(1). The framework involves inducing point methods, local covariance function interpolation, exploitations of structured matrices and projections to low-dimensional spaces. The properties of the different approximations are studied and the possibilities of making improvements are discussed.</p>
----------------------------------------------------------------------
In diva2:1440824
Note: no full text in DiVA

abstract is: 
<p>This thesis focuses on the design and qualities of a gamma type Stirling engine from a thermodynamic point of view. The purpose is to calculate the efficiency of the gamma type Stirling engine, as well as the work output. An ideal thermodynamic Stirling cycle consists of an isothermal expansion, isochoric heat removal, isothermal compression, and lastly isochoric heat addition. An advantage with the Stirling engine is that it is able to work using any form of working gas and is described to work in a closed regenerative thermodynamic state. The gathered data from measuring the pressure and volume when energy is applied is used to calculate different values such as efficiency and net work from the engine. The data was collected by using different tools. A heat sensor was taped on the bottom plate to measure the temperature at a specific time. Secondly, a pressure sensor was connected to one of the six tubes on the top plate. Where the pressure tube was connected, varied in order to analyze if there was a difference in pressure at different distances from the center of the top plate. A photosensor was used to indicate when a full revolution had occurred so that the right data to represent a full cycle could be collected. The result was PV and Ts-diagrams for each pipe at different temperatures. The results indicated that there is a pressure difference of 800 Pa. By integrating these diagrams, the net work could be calculated. The highest measured net work was $0.32mJ$ through pipe 2 when the bottom plate has a temperature of 80°C. In conclusion, changing the placement of the pipe showed no remarkable differences, however, the theoretical efficiency increased with the temperature. The engine has more parts that can be analyzed such as the materialistic parameters and the relation in volume and temperature difference but are not taken into account in this thesis.</p><p> </p>

corrected abstract:
<p>This thesis focuses on the design and qualities of a gamma type Stirling engine from a thermodynamic point of view. The purpose is to calculate the efficiency of the gamma type Stirling engine, as well as the work output. An ideal thermodynamic Stirling cycle consists of an isothermal expansion, isochoric heat removal, isothermal compression, and lastly isochoric heat addition. An advantage with the Stirling engine is that it is able to work using any form of working gas and is described to work in a closed regenerative thermodynamic state. The gathered data from measuring the pressure and volume when energy is applied is used to calculate different values such as efficiency and net work from the engine. The data was collected by using different tools. A heat sensor was taped on the bottom plate to measure the temperature at a specific time. Secondly, a pressure sensor was connected to one of the six tubes on the top plate. Where the pressure tube was connected, varied in order to analyze if there was a difference in pressure at different distances from the center of the top plate. A photosensor was used to indicate when a full revolution had occurred so that the right data to represent a full cycle could be collected. The result was PV and Ts-diagrams for each pipe at different temperatures. The results indicated that there is a pressure difference of 800 Pa. By integrating these diagrams, the net work could be calculated. The highest measured net work was <em>0.32mJ</em> through pipe 2 when the bottom plate has a temperature of 80°C. In conclusion, changing the placement of the pipe showed no remarkable differences, however, the theoretical efficiency increased with the temperature. The engine has more parts that can be analyzed such as the materialistic parameters and the relation in volume and temperature difference but are not taken into account in this thesis.</p>

Note - only change to remove the empty paragraph and to set "0.32mJ" in italics (as it was an inline equation)
----------------------------------------------------------------------
In diva2:1342442 
abstract is: 
<p>In this report we demonstrate the usefulness of hidden Markov model estimation as a method to construct models of mouse behavior. We used a neural network to retrieve positional data of different body parts from overhead video recordings of lone mice in an enclosure. We then extracted features such as velocity and elongation from the positional data and used an implementation of the Baum-Welch algorithm to fit hidden Markov models to the feature data. We could identify recurring behaviors such as "running next to wall" and "investigating wall" among the estimated states in several different mice, which was consistent with what we could see in the actual videos. We thereby demonstrate that hidden Markov model estimation by the Baum-Welch algorithm can be utilized to automatically find models of mouse behavior.</p><p> </p>

corrected abstract:
<p>In this report we demonstrate the usefulness of hidden Markov model estimation as a method to construct models of mouse behavior. We used a neural network to retrieve positional data of different body parts from overhead video recordings of lone mice in an enclosure. We then extracted features such as velocity and elongation from the positional data and used an implementation of the Baum-Welch algorithm to fit hidden Markov models to the feature data. We could identify recurring behaviors such as ”running next to wall” and ”investigating wall” among the estimated states in several different mice, which was consistent with what we could see in the actual videos. We thereby demonstrate that hidden Markov model estimation by the Baum-Welch algorithm can be utilized to automatically find models of mouse behavior.</p>

Note - only change to remove the empty paragraph and fixed double quotes to match the original
----------------------------------------------------------------------
In diva2:1781495 
abstract is: 
<p>Capacitive Deionization (CDI) is an energy-efficient desalination technology that utilizes an electric field to extract ions from water. Flow-through CDI systems show potential for superior desalination performance compared to traditional flow-by CDI; however, they face the challenge of increased occurrence of Faradaic reactions, leading to undesired by-products and reduced energy efficiency. In this study, we constructed a flow-through CDI cell and investigated the desalination performance of the two possible cell configurations: upstream anode mode and downstream anode mode. A series of experiments were conducted, measuring conductivity and pH of the effluent solution during charging and discharging phases. The results were analyzed in terms of salt adsorption capacity and charge efficiency. We used pH fluctuations in the effluent solution as indicators of Faradaic reactions. It was found that upstream anode mode yielded superior desalination, with a salt adsorption capacity of 6.79 mg/g and charge efficiency of 64.3%, compared to downstream anode mode, which displayed a salt adsorption capacity of 5.19 mg/g and charge efficiency of 50.8%. However, upstream anode mode also produced more pronounced pH oscillations, suggesting a higher occurrence of Faradaic reactions. Reconciling these conflicting results and shedding light on the complex processes within the CDI cell calls for further investigation.</p><p> </p>

corrected abstract:
<p>Capacitive Deionization (CDI) is an energy-efficient desalination technology that utilizes an electric field to extract ions from water. Flow-through CDI systems show potential for superior desalination performance compared to traditional flow-by CDI; however, they face the challenge of increased occurrence of Faradaic reactions, leading to undesired by-products and reduced energy efficiency. In this study, we constructed a flow-through CDI cell and investigated the desalination performance of the two possible cell configurations: upstream anode mode and downstream anode mode. A series of experiments were conducted, measuring conductivity and pH of the effluent solution during charging and discharging phases. The results were analyzed in terms of salt adsorption capacity and charge efficiency. We used pH fluctuations in the effluent solution as indicators of Faradaic reactions. It was found that upstream anode mode yielded superior desalination, with a salt adsorption capacity of 6.79 mg g<sup>-1</sup> and charge efficiency of 64.3%, compared to downstream anode mode, which displayed a salt adsorption capacity of 5.19 mg g<sup>-1</sup> and charge efficiency of 50.8%. However, upstream anode mode also produced more pronounced pH oscillations, suggesting a higher occurrence of Faradaic reactions. Reconciling these conflicting results and shedding light on the complex processes within the CDI cell calls for further investigation.</p>

Note - only change to remove the empty paragraph and change the "/g" into "g<sup>-1</sup>" as in the original
----------------------------------------------------------------------
In diva2:1354140 
Note: no full text in DiVA

abstract is: 
<p>In this project, the fatigue behaviour of aluminium (AL 5083 H111) gusset and flange joined with a fillet weld, is investigated through experiments and numerical methods. The work aims at improved knowledge on fatigue in an aluminium welded joint subjected to constant amplitude varying load.</p><p> </p><p>The results of the experiments are investigated with the Basquin equation. The mean curve is estimated by the maximum likelihood estimation (MLE) by using both the failed specimen data and the run-out data. From the mean curve, a component specific design curve is estimated. Comparison of the component specific design curve with the recommendation’s design curve highlighted the inherent conservatism of the recommendation’s design curve.</p><p> </p><p>Nominal, hot-spot and equivalent notch methods were evaluated, and a comparative study was performed. The numerical investigation showed that the predicted fatigue life increased with model complexity. Comparison to the experimentally derived component specific design curve highlighted non-conservatism of the numerically predicted fatigue life for large stress ranges. The degree of conservatism of the numerical methods is however strongly affected by the slope of the considered design curve.</p>

corrected abstract:
<p>In this project, the fatigue behaviour of aluminium (AL 5083 H111) gusset and flange joined with a fillet weld, is investigated through experiments and numerical methods. The work aims at improved knowledge on fatigue in an aluminium welded joint subjected to constant amplitude varying load.</p><p>The results of the experiments are investigated with the Basquin equation. The mean curve is estimated by the maximum likelihood estimation (MLE) by using both the failed specimen data and the run-out data. From the mean curve, a component specific design curve is estimated. Comparison of the component specific design curve with the recommendation’s design curve highlighted the inherent conservatism of the recommendation’s design curve.</p><p>Nominal, hot-spot and equivalent notch methods were evaluated, and a comparative study was performed. The numerical investigation showed that the predicted fatigue life increased with model complexity. Comparison to the experimentally derived component specific design curve highlighted non-conservatism of the numerically predicted fatigue life for large stress ranges. The degree of conservatism of the numerical methods is however strongly affected by the slope of the considered design curve.</p>


Note - only change to remove the empty paragraphs
----------------------------------------------------------------------
In diva2:1436832 
abstract is: 
<p>In this report, we first briefly summarize Hermitian quantum mechanics before moving on to the non-Hermitian case. We then review PT-symmetric quantum mechanics with a focus on finite-dimensional systems, and include a novel generalization of a perturbative calculation of the C-operator. After briefly covering the basics of neutrino oscillations, we perturbatively examine a PT-symmetric addition to the neutrino oscillation Hamiltonian. We examine the effects of the addition with two different definitions of transition probabilities. However, probability is not conserved to first order with either definition. Further, we note that the effect of the chosen perturbation is to shift the transition probabilities by some phase, and to change the amplitudes of the transition probabilities.</p><p> </p>

corrected abstract:
<p>In this report, we first briefly summarize Hermitian quantum mechanics before moving on to the non-Hermitian case. We then review &Pscr;&Tscr;-symmetric quantum mechanics with a focus on finite-dimensional systems, and include a novel generalization of a perturbative calculation of the &Cscr;-operator. After briefly covering the basics of neutrino oscillations, we perturbatively examine a &Pscr;&Tscr;-symmetric addition to the neutrino oscillation Hamiltonian. We examine the effects of the addition with two different definitions of transition probabilities. However, probability is not conserved to first order with either definition. Further, we note that the effect of the chosen perturbation is to shift the transition probabilities by some phase, and to change the amplitudes of the transition probabilities.</p>

Note - removed the empty paragraph and corrected the script characters
----------------------------------------------------------------------
In diva2:1878884 
abstract is: 
<p>Reinforcement learning (RL) algorithms aim to identify optimal action sequences for an agent in a given environment, traditionally maximizing the expected rewards received from the environment by taking each action and transitioning between states. This thesis explores approaching RL distributionally, replacing the expected reward function by the full distribution over the possible rewards received, known as the value distribution. We focus on the quantile regression distributional RL (QR-DQN) algorithm introduced by Dabney et al. (2017), which models the value distribution by representing its quantiles. With such information of the value distribution, we modify the QR-DQN algorithm to enhance the agent's risk sensitivity. Our risk-averse algorithm is evaluated against the original QR-DQN in the Atari 2600 and in the Gymnasium environment, specifically in the games Breakout, Pong, Lunar Lander and Cartpole. Results indicate that the risk-averse variant performs comparably in terms of rewards while exhibiting increased robustness and risk aversion. Potential refinements of the risk-averse algorithm are presented.</p><p> </p>

corrected abstract:
<p>Reinforcement learning (RL) algorithms aim to identify optimal action sequences for an agent in a given environment, traditionally maximizing the expected rewards received from the environment by taking each action and transitioning between states. This thesis explores approaching RL distributionally, replacing the expected reward function by the full distribution over the possible rewards received, known as the value distribution. We focus on the quantile regression distributional RL (QR-DQN) algorithm introduced by Dabney et al. (2017), which models the value distribution by representing its quantiles. With such information of the value distribution, we modify the QR-DQN algorithm to enhance the agent's risk sensitivity. Our risk-averse algorithm is evaluated against the original QR-DQN in the Atari 2600 and in the Gymnasium environment, specifically in the games Breakout, Pong, Lunar Lander and Cartpole. Results indicate that the risk-averse variant performs comparably in terms of rewards while exhibiting increased robustness and risk aversion. Potential refinements of the risk-averse algorithm are presented.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:735921 
abstract is: 
<p>This paper presents PDEs that describes sedimentation by a system of diffusion and transportation equations. These PDEs are implemented with a semi-implicit scheme and solved on a Graphics Processing Unit (GPU). The equations are solved with the iterative solvers (conjugate gradient and biconjugate gradient stabilized method) provided by the software ViennaCL. The timings from these operations are compared with a CPU implementation.</p><p>Before using the iterative solvers, a sparse matrix and a right hand side vector is set. The sparse matrix and the right hand side vector are efficiently updated on the GPU. The implicit terms of the PDEs are stored in the sparse matrix and the explicit terms in the right hand side vector. The sparse matrix is stored in the compressed sparse row (CSR) format. Algorithms to update the sparse matrix for the PDEs, which have Neumann or a mix of Neumann and Dirichlet boundary conditions, are presented. As the values in the sparse matrix depend on values from the previous results, the sparse matrix has to be updated frequently. Considerable time is saved by updating the sparse matrix on the GPU instead of on the CPU (slow data transfers between CPU and GPU are reduced).</p><p>The speedup for the GPU implementation was found to be 8-10 and 12-18 for the GPUs GTX 590 and K20m respectively, depending on grid size. The high speedup is due to the CPU model of the CPUs used for timings being an older model. If a newer CPU model were used, the speedup would be lower. Due to limited access to newer hardware, a more accurate value for speedup comparison has not been acquired. Indications still prove that the GPU implementation is faster than the sequential CPU implementation.</p><p> </p>

corrected abstract:
<p>This paper presents PDEs that describes sedimentation by a system of diffusion and transportation equations. These PDEs are implemented with a semi-implicit scheme and solved on a Graphics Processing Unit (GPU). The equations are solved with the iterative solvers (conjugate gradient and biconjugate gradient stabilized method) provided by the software ViennaCL. The timings from these operations are compared with a CPU implementation.</p><p>Before using the iterative solvers, a sparse matrix and a right hand side vector is set. The sparse matrix and the right hand side vector are efficiently updated on the GPU. The implicit terms of the PDEs are stored in the sparse matrix and the explicit terms in the right hand side vector. The sparse matrix is stored in the compressed sparse row (CSR) format. Algorithms to update the sparse matrix for the PDEs, which have Neumann or a mix of Neumann and Dirichlet boundary conditions, are presented. As the values in the sparse matrix depend on values from the previous results, the sparse matrix has to be updated frequently. Considerable time is saved by updating the sparse matrix on the GPU instead of on the CPU (slow data transfers between CPU and GPU are reduced).</p><p>The speedup for the GPU implementation was found to be 8-10 and 12-18 for the GPUs GTX 590 and K20m respectively, depending on grid size. The high speedup is due to the CPU model of the CPUs used for timings being an older model. If a newer CPU model were used, the speedup would be lower. Due to limited access to newer hardware, a more accurate value for speedup comparison has not been acquired. Indications still prove that the GPU implementation is faster than the sequential CPU implementation.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1350076 
abstract is: 
<p>In this report we investigate the exotic hadrons known as pentaquarks. A brief overview of relevant concepts and theory is initially presented in order to aid the reader. Thereafter, the history of this field with regards to theory and experiments is discussed. In particular, a group theoretic classification of these states is studied. A simple mass formula for pentaquark states is examined and predictions are subsequently made about the composition and mass of possible pentaquark states. Furthermore, this mass formula is modified to examine and predict additional pentaquark states. A number of numerical fits concerning the masses of pentaquarks are performed and studied. Future research is explored with regards to the information presented in this thesis.</p><p> </p>

corrected abstract:
<p>In this report we investigate the exotic hadrons known as pentaquarks. A brief overview of relevant concepts and theory is initially presented in order to aid the reader. Thereafter, the history of this field with regards to theory and experiments is discussed. In particular, a group theoretic classification of these states is studied. A simple mass formula for pentaquark states is examined and predictions are subsequently made about the composition and mass of possible pentaquark states. Furthermore, this mass formula is modified to examine and predict additional pentaquark states. A number of numerical fits concerning the masses of pentaquarks are performd and studied. Future research is explored with regards to the information presented in this thesis.</p>

Note - only change to remove the empty paragraph and replacing "performed" with "performd" - error in the original
----------------------------------------------------------------------
In diva2:1873671 
abstract is: 
<p>In this report, we present a novel Bayesian inference framework to reconstruct the three-dimensional initial conditions of cosmic structure formation from data. To achieve this goal, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. According to the cosmological paradigm, all observable structures were formed from tiny primordial quantum fluctuations generated during the early stages of the Universe. As time passed, these seed fluctuations grew via gravitational aggregation to form the presently observed cosmic web traced by galaxies. For this reason, the specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we develop a novel machine learning approach that leverages the hierarchical nature of structure formation. We demonstrate our method in a mock analysis and find that we can recover the initial conditions with high accuracy, showing the potential of our model.</p><p> </p>

corrected abstract:
<p>In this report, we present a novel Bayesian inference framework to reconstruct the three-dimensional initial conditions of cosmic structure formation from data. To achieve this goal, we leverage deep learning technologies to create a generative model of cosmic initial conditions paired with a fast machine learning surrogate model emulating the complex gravitational structure formation. According to the cosmological paradigm, all observable structures were formed from tiny primordial quantum fluctuations generated during the early stages of the Universe. As time passed, these seed fluctuations grew via gravitational aggregation to form the presently observed cosmic web traced by galaxies. For this reason, the specific shape of a configuration of the observed galaxy distribution retains a memory of its initial conditions and the physical processes that shaped it. To recover this information, we develop a novel machine learning approach that leverages the hierarchical nature of structure formation. We demonstrate our method in a mock analysis and find that we can recover the initial conditions with high accuracy, showing the potential of our model.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1342319 
abstract is: 
<p>To balance and control different aircraft, it is often necessary to use some type of control system, the less stable the craft is without any control system, the more advanced the system is required to be. This project is an experiment which purpose is to attempt to control a very unstable craft using steerable rudders. A design of the craft is modeled in CAD after a rough estimation to determine the required capacity of the components. Then a simulation of the craft is modeled in Matlab’s Simulink environment, which is used to test the control system’s capabilities and determine its optimal settings. Finally a physical model is built to see if the control system is sufficiently designed to stabilize the vessel under real conditions, which when flown did not successfully balance due to insufficient roll capability. Different solutions to this problem and other potential improvements is then discussed.</p><p> </p>

corrected abstract:
<p>To balance and control different aircraft it is often necessary to use some type of control system, the less stable the craft is without any control system, the more advanced the system is required to be. This project is an experiment which goal is to attempt to control a very unstable craft using steerable rudders. A design of the craft is modeled in CAD after a rough estimation to determine the required capacity of the components. Then a simulation of the craft is modeled in Matlab’s Simulink environment which is used to test the control system’s capabilities and determine its optimal settings, the simulation was considered successful because it showed that the craft could balance. Finally a physical model is built to test if the control system is sufficient to stabilize the vessel under real world conditions. When the real aircraft was flown it did not successfully balance due to insufficient roll capability caused by the motor torque being larger than expected. Different solutions to this problem and other potential improvements is then discussed.</p>

Note many changes in the wording betweeen the DiVA and original text, also removed the unnecessary empty paragraph
----------------------------------------------------------------------
In diva2:1450318 
abstract is: 
<p>Sheet-swept connection solutions are a method for joining structural pipes in aircraft constructions. It is a simple approach that avoids the extensive demands placed on, among other things, welded connections. Previously, calculation data were not available, which this study aims to meet in the form of specifications and comparisons. Graham Lee's drawings and design specifications for the replica variant of the Nieuport 12 aircraft have been followed in the analysis of the connection solution. The method is based on three parts, calculations, experimental testing and FEM analysis. These form a specification of the joint's four most central load cases and their strength: tension (1130 N), plane deflection (4.5 Nm), in plane deflection (17.5 Nm) and torsion (4.5 Nm). The results are compared with calculations of loads in the pendulum rudder to determine how well the connection solution is suitable for this purpose. Identified load cases in the rudder are: plane deflection (3.2 Nm) and torsion (≤3.2 Nm). The results indicate that this type of connection is weak in the proposed purpose. The analysis highlights clear weaknesses in the connection solution and recommendations for improvements are given, these are primarily aimed at the thickness of the gusset and the introduction of an additional rivet. Furthermore, the sheet-swept joint solution is compared with welded joints which prove to be more suitable for this application.</p><p> </p>

corrected abstract:
<p>Sheet-swept connection solutions are a method for joining structural pipes in aircraft constructions. It is a simple approach that avoids the extensive demands placed on, among other things, welded connections. Previously, calculation data were not available, which this study aims to meet in the form of specifications and comparisons. Graham Lee's drawings and design specifications for the replica variant of the Nieuport 12 aircraft have been followed in the analysis of the connection solution. The method is based on three parts, calculations, experimental testing and FEM analysis. These form a specification of the joint's four most central load cases and their strength: tension (1130 N), plane deflection (4.5 Nm), in plane deflection (17.5 Nm) and torsion (4.5 Nm). The results are compared with calculations of loads in the pendulum rudder to determine how well the connection solution is suitable for this purpose. Identified load cases in the rudder are: plane deflection (3.2 Nm) and torsion (≤3.2 Nm). The results indicate that this type of connection is weak in the proposed purpose. The analysis highlights clear weaknesses in the connection solution and recommendations for improvements are given, these are primarily aimed at the thickness of the gusset and the introduction of an additional rivet. Furthermore, the sheet-swept joint solution is compared with welded joints which prove to be more suitable for this application.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1441692 
abstract is: 
<p>This report is part of a bachelor’s degree project in solid mechanics at KTH, Stockholm. It is performed by two students on behalf of the social enterprise Better Shelter, which provides shelters for people displaced by war and natural disasters. The scope of the project is to expand Better Shelters product specifications by providing improvements of the shelter earth anchors. This would allow use of the shelter in areas affected by higher wind speeds and thereby help more people in need of temporary housing and shelters. The earth anchors prevent the shelter from uplifting and tilting by taking uplift forces when horizontal wind loads acts on the structure. Two wind models with wind speeds up to 36 m/s were created to find the reaction forces on the anchors resisting the wind load. The wind models were compared with each other to validate the results and find the largest reaction forces on the anchors. Simulations of the anchors were made to analyse occurring stresses due to wind loads. Redesigns of the current earth anchor were made to find improvements of the anchor shape and reduce the stresses on the anchor. Experiments were then performed to analyse the redesigned anchor shapes in practise. The redesign, calculations and computational analyses of the anchors were done using the programs SolidEdge, ANSYS and Matlab. Results showed that redesigning the anchor contact area with the anchor wire reduced the stresses on the anchors drastically. Increasing the wing size of the anchors proved to be successful for preventing anchors from being pulled out of the soil. This allows better use of the current anchor material volume. Experiments also proved that burying the anchor deeper into the soil is an effective way of increasing the resistance from being pulled out of the ground. By reducing the stresses on the anchor, more materials are available for use. This could be explored further and is a suggested as a continuation of this project. The current anchor material is aluminium, and most aluminium alloys can be used with the redesigned ball joint connection to the anchor wire even when wind forces are large.</p><p> </p>

corrected abstract:
<p>This report is part of a bachelor’s degree project in solid mechanics at KTH, Stockholm. It is performed by two students on behalf of the social enterprise Better Shelter, which provides shelters for people displaced by war and natural disasters. The scope of the project is to expand Better Shelters product specifications by providing improvements of the shelter earth anchors. This would allow use of the shelter in areas affected by higher wind speeds and thereby help more people in need of temporary housing and shelters.</p><p>The earth anchors prevent the shelter from uplifting and tilting by taking uplift forces when horizontal wind loads acts on the structure. Two wind models with wind speeds up to 36 𝑚/𝑠 were created to find the reaction forces on the anchors resisting the wind load. The wind models were compared with each other to validate the results and find the largest reaction forces on the anchors. Simulations of the anchors were made to analyse occurring stresses due to wind loads.</p><p>Redesigns of the current earth anchor were made to find improvements of the anchor shape and reduce the stresses on the anchor. Experiments were then performed to analyse the redesigned anchor shapes in practise. The redesign, calculations and computational analyses of the anchors were done using the programs SolidEdge, ANSYS and Matlab.</p><p>Results showed that redesigning the anchor contact area with the anchor wire reduced the stresses on the anchors drastically. Increasing the wing size of the anchors proved to be successful for preventing anchors from being pulled out of the soil. This allows better use of the current anchor material volume. Experiments also proved that burying the anchor deeper into the soil is an effective way of increasing the resistance from being pulled out of the ground.</p><p>By reducing the stresses on the anchor, more materials are available for use. This could be explored further and is a suggested as a continuation of this project. The current anchor material is aluminium, and most aluminium alloys can be used with the redesigned ball joint connection to the anchor wire even when wind forces are large.</p>
----------------------------------------------------------------------
In diva2:1876745 
abstract is: 
<p>The Thermo-Calc software is a key tool in the research process for many material engineers. However, integrating multiple modules in Thermo-Calc requires the user to write code in a Python-based language, which can be challenging for novice programmers. This project aims to enable the generation of such code from user prompts by using existing generative AI models. In particular, we use a retrieval-augmented generation architecture applied to LLaMA and Mistral models. We use Code LLaMA-Instruct models with 7, 13, and 34 billion parameters, and a Mistral-Instruct model with 7 billion parameters. These models are all based on LLaMA 2. We also use a LLaMA 3-Instruct model with 8 billion parameters. All these models are instruction-tuned, which suggests that they have the capability to interpret natural language and identify appropriate options for a command-line program such as Python. In our testing, the LLaMA 3-Instruct model performed best, achieving 53% on the industry benchmark HumanEval and 49% on our internal adequacy assessment at pass@1, which is the expected probability of getting a correct solution when generating a response. This indicates that the model generates approximately every other answer correct. Due to GPU memory limitations, we had to apply quantisation to process the 13 and 34 billion parameter models. Our results revealed a mismatch between model size and optimal levels of quantisation, indicating that reduced precision adversely affects the performance of these models. Our findings suggest that a properly customised large language model can greatly reduce the coding effort of novice programmers, thereby improving productivity in material research.</p><p> </p>

corrected abstract:
<p>The Thermo-Calc software is a key tool in the research process for many material engineers. However, integrating multiple modules in Thermo-Calc requires the user to write code in a Python-based language, which can be challenging for novice programmers. This project aims to enable the generation of such code from user prompts by using existing generative AI models. In particular, we use a retrieval-augmented generation architecture applied to LLaMA and Mistral models. We use Code LLaMA-Instruct models with 7, 13, and 34 billion parameters, and a Mistral-Instruct model with 7 billion parameters. These models are all based on LLaMA 2. We also use a LLaMA 3-Instruct model with 8 billion parameters. All these models are instruction-tuned, which suggests that they have the capability to interpret natural language and identify appropriate options for a command-line program such as Python. In our testing, the LLaMA 3-Instruct model performed best, achieving 53% on the industry benchmark HumanEval and 49% on our internal adequacy assessment at pass@1, which is the expected probability of getting a correct solution when generating a response. This indicates that the model generates approximately every other answer correct. Due to GPU memory limitations, we had to apply quantisation to process the 13 and 34 billion parameter models. Our results revealed a mismatch between model size and optimal levels of quantisation, indicating that reduced precision adversely affects the performance of these models. Our findings suggest that a properly customised large language model can greatly reduce the coding effort of novice programmers, thereby improving productivity in material research.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1285800 
abstract is: 
<p> As the challenge grows in the vehicle industry, tightening the margins on financial and environmental costs of the vehicle development, computer aided engineering becomes more and more attractive. Extensive work is being invested in creating detailed models that can replicate vehicle behaviour accurately and efficiently. The work in this thesis starts with studying objective and subjective evaluations of vehicles as well as their counterparts in vehicle models and a simulator environment. Then, it continues to locate the weaknesses in the models, and investigate the possible improvements. The first part of the thesis focused on performing a literature study concerning the objective metrics and their use in the vehicle industry, as well as the use of simulators. This served as a foundation for the use of objective metrics in the validation of the CarRealTime models. The tools used in the thesis were also introduced. The work continued with the study of previously collected data concerning vehicle evaluation through subjective assessment and objective metrics, with different anti-roll bar configurations, to build trust in the ability of the drivers in evaluating these criteria. Similar data from the CarRealTime models and the simulator were also studied. The aim was to evaluate the simulator driving experience accuracy through the subjective assessment. The weaknesses of the model were identified, and an improved steering model was introduced, replacing the old lookup tables with a Pfeffer model from CarRealTime combined with the steering assist unit in Simulink. An extensive parameter study was performed to understand the effect of selected parameters on the driving experience. Using the same model, the simulator delays were studied in terms of replicating yaw and lateral movements, and how this can affect the driver’s perception of the driving experience. Finally, the results from the parameter study were used to assign the weight parameters in the optimization objective function where the goal was to study the possibility of improving the accuracy of the driving experience as well as counteracting the effects of simulator delays. The Matlab Optimization Toolkit was used in the process. As a conclusion, it was shown that the subjective assessment together with the objective metrics played a crucial role in identifying model and simulator weaknesses. The parameter study showed promising opportunities in solving the aforementioned issues, with the optimization tool and boundaries needing more elaborate work to reach conclusive results.</p><p> </p>

corrected abstract:
<p> As the challenge grows in the vehicle industry, tightening the margins on financial and environmental costs of the vehicle development, computer aided engineering becomes more and more attractive. Extensive work is being invested in creating detailed models that can replicate vehicle behaviour accurately and efficiently. The work in this thesis starts with studying objective and subjective evaluations of vehicles as well as their counterparts in vehicle models and a simulator environment. Then, it continues to locate the weaknesses in the models, and investigate the possible improvements. The first part of the thesis focused on performing a literature study concerning the objective metrics and their use in the vehicle industry, as well as the use of simulators. This served as a foundation for the use of objective metrics in the validation of the CarRealTime models. The tools used in the thesis were also introduced. The work continued with the study of previously collected data concerning vehicle evaluation through subjective assessment and objective metrics, with different anti-roll bar configurations, to build trust in the ability of the drivers in evaluating these criteria. Similar data from the CarRealTime models and the simulator were also studied. The aim was to evaluate the simulator driving experience accuracy through the subjective assessment. The weaknesses of the model were identified, and an improved steering model was introduced, replacing the old lookup tables with a Pfeffer model from CarRealTime combined with the steering assist unit in Simulink. An extensive parameter study was performed to understand the effect of selected parameters on the driving experience. Using the same model, the simulator delays were studied in terms of replicating yaw and lateral movements, and how this can affect the driver’s perception of the driving experience. Finally, the results from the parameter study were used to assign the weight parameters in the optimization objective function where the goal was to study the possibility of improving the accuracy of the driving experience as well as counteracting the effects of simulator delays. The Matlab Optimization Toolkit was used in the process. As a conclusion, it was shown that the subjective assessment together with the objective metrics played a crucial role in identifying model and simulator weaknesses. The parameter study showed promising opportunities in solving the aforementioned issues, with the optimization tool and boundaries needing more elaborate work to reach conclusive results.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:741548 
abstract is: 
<p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p><p> </p>
skipping mc='isa'

partal corrected: diva2:741548: <p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p><p> </p>

corrected abstract:
<p>The aim of this study is to describe how Sweden can design a sustainable energy supply in the future. By listing the advantages and disadvantages of the various energy sources and by studying Germany's conversion to renewable energy sources, I propose how Sweden should replace the lost power that disappears when three of the Swedish nuclear reactors will be phased out. I have also studied how Sweden can reduce dependence on fossil fuels, particularly in the transport sector where most emissions from fossil fuels occurs.</p><p>Sweden needs inexpensive and reliable electricity production to be able to continue with a competitive basic industry. However, renewable energy sources such as solar and wind energy are dependent on the weather and their electricity production therefore varies which cause huge problems in the electricity production. Germany's transition towards renewables and decommissioning of nuclear power has forced the Germans to pay expensive electricity prices due to the certificates, and they have also been expanding coal and gas power plants. I believe that Sweden should aim for a fossil free society instead of going the same way as Germany has done to get a nuclear-free society. I also believe that Sweden should replace the lost power with new nuclear power. To reach a fossil free society Sweden needs to replace the fossil fuels in the transport sector, with biofuels and electric motors.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1781509 
Note: no full text in DiVA

abstract is: 
<p>The objective of this report is to explore and assess the potential of two modified designs derived from a perfectly circular nested tubes NANF configuration. By altering the curvature profile and adopting a rounded triangular shape, various parameters are systematically varied to evaluate their impact on the confinement loss properties within the operating range around 850 nm. By leveraging geometric optics, an attempt is made to increase the angle of incidence of light on the tubes in order to maximize reflection. Through extensive simulations, the behavior of the modified designs is analyzed and compared against the literature-based NANF-B. The simulation results indicate that the simulated NANF-B exhibits a skewed wavelength range towards lower wavelengths compared to the literature version probably because of slight differences in how the model was designed. However, the proposed alterations demonstrate significantly reduced bandwidth and only show agreement with NANF-B within the lower wavelength range. Furthermore, the simulations reveal that the points where the tubes connect to the outer cladding play a critical role in the overall loss characteristics at longer wavelengths. This finding suggests that designs incorporating inward-curving tubes towards the outer cladding offer improved performance throughout the anti-resonant window where propagation is feasible.</p><p> </p>

corrected abstract:
<p>The objective of this report is to explore and assess the potential of two modified designs derived from a perfectly circular nested tubes NANF configuration. By altering the curvature profile and adopting a rounded triangular shape, various parameters are systematically varied to evaluate their impact on the confinement loss properties within the operating range around 850 nm. By leveraging geometric optics, an attempt is made to increase the angle of incidence of light on the tubes in order to maximize reflection. Through extensive simulations, the behavior of the modified designs is analyzed and compared against the literature-based NANF-B. The simulation results indicate that the simulated NANF-B exhibits a skewed wavelength range towards lower wavelengths compared to the literature version probably because of slight differences in how the model was designed. However, the proposed alterations demonstrate significantly reduced bandwidth and only show agreement with NANF-B within the lower wavelength range. Furthermore, the simulations reveal that the points where the tubes connect to the outer cladding play a critical role in the overall loss characteristics at longer wavelengths. This finding suggests that designs incorporating inward-curving tubes towards the outer cladding offer improved performance throughout the anti-resonant window where propagation is feasible.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1865115 
abstract is: 
<p>This thesis deals with Large Eddy Simulations (LES) of the Atmospheric Boundary Layer (ABL), focusing on studying the resolution dependence of turbulent passive scalar transport within the layer. The ABL is the lowest part of the atmosphere, where humans live and conduct most of their daily activities. Here, a scalar was injected at four different heights in a mixed shear- and convective-driven ABL, which was simulated using the Spectral Element Method (SEM) code Nek5000. The statistics of the four scalars were analysed and their resolution dependence was studied and compared to that of non-scalar quantities. No significant resolution dependence was found with regards to non-scalar quantities, while scalar quantities show a rather strong dependence on resolution especially in the first quarter of the simulation. Negative concentration values are found within the layer and some approaches to solve the problem are proposed. Statistics alone provide an accurate description of the general ABL behaviour, but are found to be insufficient to capture the dynamics of the scalar injection, which ought to be analysed with more advanced methods (e.g. modal decomposition). The structures arising within the layer are also analysed, and further work regarding the study of scalar fronts is suggested.</p><p> </p>

corrected abstract:
<p>This thesis deals with Large Eddy Simulations (LES) of the Atmospheric Boundary Layer (ABL), focusing on studying the resolution dependence of turbulent passive scalar transport within the layer. The ABL is the lowest part of the atmosphere, where humans live and conduct most of their daily activities. Here, a scalar was injected at four different heights in a mixed shear- and convective-driven ABL, which was simulated using the Spectral Element Method (SEM) code Nek5000. The statistics of the four scalars were analysed and their resolution dependence was studied and compared to that of non-scalar quantities. No significant resolution dependence was found with regards to non-scalar quantities, while scalar quantities show a rather strong dependence on resolution especially in the first quarter of the simulation. Negative concentration values are found within the layer and some approaches to solve the problem are proposed. Statistics alone provide an accurate description of the general ABL behaviour, but are found to be insufficient to capture the dynamics of the scalar injection, which ought to be analysed with more advanced methods (e.g. modal decomposition). The structures arising within the layer are also analysed, and further work regarding the study of scalar fronts is suggested.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1197616 
abstract is: 
<p>The use of turbocharged Diesel engines is nowadays a widespread practice in the automotive sector: heavy-duty vehicles like trucks or buses, in particular, are often equipped with turbocharged engines. An accurate study of the flow field developing inside both the main components of a turbocharger, i.e. compressor and turbine, is therefore necessary: the synergistic use of CFD simulations and experimental tests allows to fulfill this requirement.</p><p>The aim of this thesis is to investigate the performance and the flow field that develops inside a centrifugal compressor for automotive turbochargers. The study is carried out by means of numerical simulations, both steady-state and transient, based on RANS models (Reynolds Averaged Navier-Stokes equations). The code utilized for the numerical simulations is Ansys CFX.</p><p> </p><p>The first part of the work is an engineering attempt to develop a CFD method for predicting the performance of a centrifugal compressor which is based solely on steady-state RANS models. The results obtained are then compared with experimental observations. The study continues with an analysis of the sensitivity of the developed CFD method to different parameters: influence of both position and model used for the rotor-stator interfaces and the axial tip-clearance on the global performances is studied and quantified.</p><p> </p><p>In the second part, a design optimization study based on the Design of Experiments (DoE) approach is performed. In detail, transient RANS simulations are used to identify which geometry of the recirculation cavity hollowed inside the compressor shroud (ported shroud design) allows to mitigate the backflow that appears at low mass-flow rates. Backflow can be observed when the operational point of the compressor is suddenly moved from design to surge conditions. On actual heavy-duty vehicles, these conditions may arise when a rapid gear shift is performed.</p>

corrected abstract:
<p>The use of turbocharged Diesel engines is nowadays a widespread practice in the automotive sector: heavy-duty vehicles like trucks or buses, in particular, are often equipped with turbocharged engines. An accurate study of the flow field developing inside both the main components of a turbocharger, i.e. compressor and turbine, is therefore necessary: the synergistic use of CFD simulations and experimental tests allows to fulfill this requirement.</p><p>The aim of this thesis is to investigate the performance and the flow field that develops inside a centrifugal compressor for automotive turbochargers. The study is carried out by means of numerical simulations, both steady-state and transient, based on RANS models (Reynolds Averaged Navier-Stokes equations). The code utilized for the numerical simulations is Ansys CFX.</p><p>The first part of the work is an engineering attempt to develop a CFD method for predicting the performance of a centrifugal compressor which is based solely on steady-state RANS models. The results obtained are then compared with experimental observations. The study continues with an analysis of the sensitivity of the developed CFD method to different parameters: influence of both position and model used for the rotor-stator interfaces and the axial tip-clearance on the global performances is studied and quantified.</p><p>In the second part, a design optimization study based on the Design of Experiments (DoE) approach is performed. In detail, transient RANS simulations are used to identify which geometry of the recirculation cavity hollowed inside the compressor shroud (ported shroud design) allows to mitigate the backflow that appears at low mass-flow rates. Backflow can be observed when the operational point of the compressor is suddenly moved from design to surge conditions. On actual heavy-duty vehicles, these conditions may arise when a rapid gear shift is performed.</p>


Note - only change to remove the empty paragraphs
----------------------------------------------------------------------
In diva2:1341272 
abstract is: 
<p>In the autumn 2018 political elections were held in Sweden and consequently it is interesting to investigate what can affect how people vote. The purpose with this report is investigating if there are correspondences between the characteristics of a municipality and how the people in that municipality voted in the general election. Clustering on data sets with municipality characteristics and municipality general election statistics from 2018 is the basis of this study. K-means clustering and hierarchical clustering are the clustering methods that are used. In the report results of the clustering and the construction of a method for comparing clusterings are presented. The results show that there are some correspondences but that clustering is not the optimal method for analysing this data set.</p><p> </p>

corrected abstract:
<p>In the autumn 2018 political elections were held in Sweden and consequently it is interesting to investigate what can affect how people vote. The purpose with this report is investigating if there are correspondences between the characteristics of a municipality and how the people in that municipality voted in the general election. Clustering on data sets with municipality characteristics and municipality general election statistics from 2018 is the basis of this study. 𝐾-means clustering and hierarchical clustering are the clustering methods that are used. In the report results of the clustering and the construction of a method for comparing clusterings are presented. The results show that there are some correspondences but that clustering is not the optimal method for analysing this data set.</p>

Note - only change to remove the empty paragraph and replace "K" by "𝐾".
----------------------------------------------------------------------
In diva2:1342347 
abstract is: 
<p>The current trend in the automotive industry towards more fuel efficient vehicles requires all components to be as light as possible while still meeting other demands such as stiffness and feasible cost. The purpose of this study was to investigate the possibility to replace a partial chassis structure in a Scania low-entry city bus. The partial chassis to be replaced consists of a steel structure and an inner flooring, with the purpose to support loads that the bus is subject to. This was to be done with a composite sandwich structure, with the primary goal to reduce weight by at least 40% and number of components by 50%. The replacement structure needed to meet the stiffness and strength requirements that the current structure fulfils. This was achieved by designing two concepts, concept 1 and concept 2, through an iterative FE-analysis in ANSYS. Two prototypes where built and tested for real world load applications. The result from this study showed that it was possible to meet both the weight and component reduction goal. Concept 1 and concept 2 achieved a weight reduction of 62% and 68% respectively and the number of components was reduced significantly. Further work would be to investigate the interface between the new structure and the rest of the bus, modal- and fatigue analyses, production implementation and economical aspects to name a few.</p><p> </p>

corrected abstract:
<p>The current trend in the automotive industry towards more fuel efficient vehicles requires all components to be as light as possible while still meeting other demands such as stiffness and feasible cost.</p><p>The purpose of this study was to investigate the possibility to replace a partial chassis structure in a Scania low-entry city bus. The partial chassis to be replaced consists of a steel structure and an inner flooring, with the purpose to support loads that the bus is subject to. This was to be done with a composite sandwich structure, with the primary goal to reduce weight by at least 40% and number of components by 50%. The replacement structure needed to meet the stiffness and strength requirements that the current structure fulfils. This was achieved by designing two concepts, <em>concept 1</em> and <em>concept 2</em>, through an iterative FE-analysis in ANSYS. Two prototypes where built and tested for real world load applications.</p><p>The result from this study showed that it was possible to meet both the weight and component reduction goal. Concept 1 and concept 2 achieved a weight reduction of 62% and 68% respectively and the number of components was reduced significantly. Further work would be to investigate the interface between the new structure and the rest of the bus, modal- and fatigue analyses, production implementation and economical aspects to name a few.</p>

Note - only changes to remove the empty paragraph,  add paragraph breaks, and add italics
----------------------------------------------------------------------
In diva2:1335215 
abstract is: 
<p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for functioning quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si3N4 has been used as a strip waveguide, placed on top of a silicon oxide SiO2 wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p><p> </p>
mc='functionin' c='function in'

partal corrected: diva2:1335215: <p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for function ing quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si3N4 has been used as a strip waveguide, placed on top of a silicon oxide SiO2 wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p><p> </p>

corrected abstract:
<p>Recent technological advances have made it possible to miniaturize and integrate optical components in quantum circuits. The connection between different components is enabled by waveguides, which support the propagation of the information carrier, a single-photon. A prerequisite for functioning quantum photonic chips is the efficient coupling of non-classical light into the circuit. In this work, this coupling efficiency from an on-chip single-photon source, approximated by a dipole, into a waveguide has been simulated. The high refractive index material silicon nitride Si<sub>3</sub>N<sub>4</sub> has been used as a strip waveguide, placed on top of a silicon oxide SiO<sub>2</sub> wafer with surrounding air. To solve Maxwell’s equations in the structures, the finite difference time-domain (FDTD) method has been used through software by Lumerical. It is shown that for the light spectrum with wavelengths 750 to 800 nm a waveguide with cross section dimensions 600x250 nm supports the fundamental transversal electric (TE) and transversal magnetic (TM) modes. The coupling efficiency is shown to reach 7 % in each direction when the dipole is placed on top of the waveguide. Having the dipole on in front of the waveguide, however, results in over 50 % coupling in the forward direction. Additionally, it is shown that in-plane 2D-material single-photon emitters, approximated by in-plane dipoles, give better results than out-of-plane dipoles for most of the tested configurations. In conclusion, these results present evidence for a substantially higher coupling efficiency from 2D-material quantum dots than have been achieved in experiments.</p>

Note - only change to remove the empty paragraph and added subscripts
----------------------------------------------------------------------
In diva2:1334842 
abstract is: 
<p>Mathematics contains many hard problems. In this paper we discuss how some of these hard problems can be solved with techniques from other math fields than the problems own discipline. First we solve some combinatorial problems using the knowledge that a maximum amount of vectors in a linearly independent set over a subset of a vector space F^n over a field F is n. Then we discuss and explain Z.Dvir's famous proof regarding Kakeya sets over finite fields. He is able to establish a lower bound of the size of Kakeya sets using polynomials.</p><p> </p>

corrected abstract:
<p>Mathematics contains many hard problems. In this paper we discuss how some of these hard problems can be solved with techniques from other math fields than the problems own discipline. First we solve some combinatorial problems using the knowledge that a maximum amount of vectors in a linearly independent set over a subset of a vector space 𝔽<sup>𝑛</sup> over a field 𝔽 is 𝑛. Then we discuss and explain Z.Dvir's famous proof regarding Kakeya sets over finite fields. He is able to establish a lower bound of the size of Kakeya sets using polynomials.</p>

Note spelling error in original:
mc='Z.Dvir' c='Z. Dvir'
----------------------------------------------------------------------
In diva2:704889 
abstract is: 
<p>In this thesis a corporate bond valuation model based on Dick-Nielsen, Feldhütter, and Lando (2011) and Chen, Lesmond, and Wei (2007) is examined. The aim is for the model to price corporate bond spreads and in particular capture the price effects of liquidity as well as credit risk. The valuation model is based on linear regression and is conducted on the Swedish market with data provided by Handelsbanken. Two measures of liquidity are analyzed: the bid-ask spread and the zero-trading days. The investigation shows that the bid-ask spread outperforms the zero-trading days in both significance and robustness. The valuation model with the bid-ask spread explains 59% of the cross-sectional variation and has a standard error of 56 bps in its pricing predictions of corporate spreads. A reduced version of the valuation model is also developed to address simplicity and target a larger group of users. The reduced model is shown to maintain a large proportion of the explanation power while including fewer and simpler variables.</p><p> </p>

corrected abstract:
<p>In this thesis a corporate bond valuation model based on Dick-Nielsen, Feldhütter, and Lando (2011) and Chen, Lesmond, and Wei (2007) is examined. The aim is for the model to price corporate bond spreads and in particular capture the price effects of liquidity as well as credit risk. The valuation model is based on linear regression and is conducted on the Swedish market with data provided by <em lang="sv">Handelsbanken</em>. Two measures of liquidity are analyzed: the bid-ask spread and the zero-trading days. The investigation shows that the bid-ask spread outperforms the zero-trading days in both significance and robustness. The valuation model with the bid-ask spread explains 59% of the cross-sectional variation and has a standard error of 56 bps in its pricing predictions of corporate spreads. A reduced version of the valuation model is also developed to address simplicity and target a larger group of users. The reduced model is shown to maintain a large proportion of the explanation power while including fewer and simpler variables.</p>

Note - only change to remove the empty paragraph and added italics
----------------------------------------------------------------------
In diva2:1442642 
abstract is: 
<p>In recent decades, sound reproduction research has shown great progress. Audio reproduction changed from a simple two-channel stereo system to a surround sound system and three dimensional sound system. The use of height and angle related speakers was introduced with the aim of improving sound reproduction. With the listener’s experience in focus, this developed reproductive systems were examined. This bachelor’s thesis in sound and vibration compiles three articles on sound quality with regard to different sound reproduction media, with the aim of getting a better understanding of the various psychoacoustical and technical parameters that influence the sound experience. Various subjective evaluation methods are presented, including the PREQUEL method (PerceptualReproduction Quality Evaluation for Loudspeakers), MUSHRA method (MUlti Stimulus test withHidden Reference and Anchor) and OLE method (Overall Listening Experience). The results show how a surround sound system and a three-dimensional sound system provide a more appreciated sound experience and how this experience can be enhanced with a height-related sound system .</p><p> </p>
mc='withHidden' c='with Hidden'
mc='PerceptualReproduction' c='Perceptual Reproduction'

corrected abstract:
<p>In recent decades, sound reproduction research has shown great progress. Audio reproduction changed from a simple two-channel stereo system to a surround sound system and three dimensional sound system. The use of height and angle related speakers was introduced with the aim of improving sound reproduction. With the listener’s experience in focus, this developed reproductive systems were examined.</p><p>This bachelor’s thesis in sound and vibration compiles three articles on sound quality with regard to different sound reproduction media, with the aim of getting a better understanding of the various psychoacoustical and technical parameters that influence the sound experience. Various subjective evaluation methods are presented, including the PREQUEL method (perceptual reproduction quality evaluation for loudspeakers), MUSHRA method (MUlti Stimulus test with Hidden Reference and Anchor) and OLE method (overall listening experience)). The results show how a surround sound system and a three-dimensional sound system provide a more appreciated sound experience and how this experience can be enhanced with a height-related sound system .</p>

Note that there is a space before the final period, several of the spelledout versions of acronyms are in lower case. Also removed the empty paragraph.
----------------------------------------------------------------------
In diva2:415054 
abstract is: 
<p> </p>
<p>Abstract</p>
<p>Thermally-controlled exchange coupling between two strong ferromagnetic (FM) layers separated by a weak FM spacer has promising application in current-driven spintronic devices. In this thesis magnetic property of Ni xFe1-x thin films in the Invar composition range are studied. The focus is on investigating the magnetic transition from ferromagnetic to paramagnetic state of the material and finding the composition with lowest Curie point.</p>
<p>The fabrication process of NixFe1-x  thin films having variable Ni concentration using multi magnetron sputtering is described in the first part of the thesis. In the second part of the thesis the magnetic characterization technique and measurement results are discussed. The Invar effect is observed at approximately 30% Ni content, where the films exhibit a pronounced minimum in the Curie temperature versus Ni concentration</p>

corrected abstract:
<p>Thermally-controlled exchange coupling between two strong ferromagnetic (FM) layers separated by a weak FM spacer has promising application in current-driven spintronic devices. In this thesis magnetic property of Ni<sub>x</sub>Fe<sub>1-x</sub> thin films in the Invar composition range are studied. The focus is on investigating the magnetic transition from ferromagnetic to paramagnetic state of the material and finding the composition with lowest Curie point. The fabrication process of Ni<sub>x</sub>Fe<sub>1-x</sub> thin films having variable Ni concentration using multi magnetron sputtering is described in the first part of the thesis. In the second part of the thesis the magnetic characterization technique and measurement results are discussed. The Invar effect is observed at approximately 30% Ni content, where the films exhibit a pronounced minimum in the Curie temperature versus Ni concentration.</p>
----------------------------------------------------------------------
In diva2:1335210 
abstract is: 
<p>G protein-coupled receptors are one of the biggest targets for pharmaceutical drugs today. The aim with this project was to use different machine learning algorithms to classify the protein into different functional states and compare the results obtained by different algorithms. Both the supervised and unsupervised methods implemented in this project identified similar regions of the protein as important for classification of their functional state. More specifically, the supervised methods Random Forest and Multilayer Perceptron were able to make predictions of the functional state of a protein with great accuracy. The methods investigated will be useful for designing and analyzing the molecular dynamics simulations of GPCRs. Ultimately this will further our understanding of the drug binding mechanics, a critical step for the rational development of new drugs to treat various diseases.</p><p> </p>

corrected abstract:
<p>G protein-coupled receptors are one of the biggest targets for pharmaceutical drugs today. The aim with this project was to use different machine learning algorithms to classify the protein into different functional states and compare the results obtained by different algorithms. Both the supervised and unsupervised methods implemented in this project identified similar regions of the protein as important for classification of their functional state. More specifically, the supervised methods Random Forest and Multilayer Perceptron were able to make predictions of the functional state of a protein with great accuracy. The methods investigated will be useful for designing and analyzing the molecular dynamics simulations of GPCRs. Ultimately this will further our understanding of the drug binding mechanics, a critical step for the rational development of new drugs to treat various diseases.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1879658 
abstract is: 
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>

corrected abstract:
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>
In diva2:1879658 
abstract is: 
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p><p> </p>

corrected abstract:
<p>This paper explores strategies in portfolio optimization, focusing on integrating mean-variance optimization (MVO) frameworks with cardinality constraints to enhance investment decision-making. Using a combination of quadratic programming and mixed-integer linear programming, the Gurobi optimizer handles complex constraints and achieves computational solutions. The study compares two mathematical formulations of the cardinality constraint: the Complementary Model and the Big M Model. As cardinality increased, risk decreased exponentially, converging at higher cardinalities. This behavior aligns with the theory of risk reduction through diversification. Additionally, despite initial expectations, both models performed similarly in terms of root relaxation risk and execution time due to Gurobi's presolve transformation of the Complementary Model into the Big M Model. Root relaxation risks were identical while execution times varied slightly without a consistent trend, underscoring the Big M Model's versatility and highlighting the limitations of the Complementary Model.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1363231 
abstract is: 
<p>In the truck industry, it is crustal to test components against fatigue to make sure that the trucks stand up to the high demands on durability. Today’s testing methods have some disadvantages; it is quite a time-consuming process, but more important, similar tested components cannot easily be compared due to the load spread the components are subjected to. It is therefore desirable to test the components in a standardized way. One way to do this is to use a synthetic signal which is a large number of unique truck measurements combined. The synthetic signal only contains information of the frame’s vibration and not any components. The purpose of this project was to create a model that uses the synthetic signal to describe the motion of components.</p><p> </p><p>Two approaches were used, the first was to base the model on previous measurements, the second one was to base the model on analytical equations. These models were experimentally tested in a 4 channel shake rig, and a silencer was the component chosen to be tested. For the model based on measurements, the load was shown to have a large spread which was hard to control due to the spread in the measurements. The second model was easier to control where the damping factor can be chosen and varied. A promising model was the analytical model using 10% damping applied to the synthetic signal, it covers most measurements without overestimate the load of the component. However, the model was only developed for the silencer acceleration in the z-direction, and it is recommended to develop it for the x-direction as well. The method used in this project could also be used to develop models for other components.</p>

corrected abstract:
<p>In the truck industry, it is crustal to test components against fatigue to make sure that the trucks stand up to the high demands on durability. Today’s testing methods have some disadvantages; it is quite a time-consuming process, but more important, similar tested components cannot easily be compared due to the load spread the components are subjected to. It is therefore desirable to test the components in a standardized way. One way to do this is to use a synthetic signal which is a large number of unique truck measurements combined. The synthetic signal only contains information of the frame’s vibration and not any components. The purpose of this project was to create a model that uses the synthetic signal to describe the motion of components.</p><p>Two approaches were used, the first was to base the model on previous measurements, the second one was to base the model on analytical equations. These models were experimentally tested in a 4 channel shake rig, and a silencer was the component chosen to be tested. For the model based on measurements, the load was shown to have a large spread which was hard to control due to the spread in the measurements. The second model was easier to control where the damping factor can be chosen and varied. A promising model was the analytical model using 10% damping applied to the synthetic signal, it covers most measurements without overestimate the load of the component. However, the model was only developed for the silencer acceleration in the z-direction, and it is recommended to develop it for the x-direction as well. The method used in this project could also be used to develop models for other components.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1137873 
abstract is: 
<p>This report was carried out at Gleechi, a Swedish start-up company working with implementing hand use in Virtual Reality. The thesis presents hand models used to generate natural looking grasping motions. One model were made for each of the thirty-three different grasp types in Feix’s The GRASP Taxonomy.</p><p>Each model is based on functional principal components analysis which was performed on data containing recorded joint angles of grasping motions from real subjects. Prior to functional principal components analysis, dynamic time warping was performed on the recorded joint angles in order to put them on the same length and make them suitable for statistical analysis. The last step of the analysis was to project the data onto the functional principal components and train Gaussian mixture models on the weights obtained. New grasping motions could be generated by sampling weights from the Gaussian mixture models and attaching them to the functional principal components.</p><p>The generated grasps were in general satisfying, but all of the thirty-three grasps were not distinguishable from each other. This was most likely caused by the fact that each degree of freedom was modelled in isolation from each other, so that no correlation between them was included in the model.</p><p> </p>

corrected abstract:
<p>This report was carried out at Gleechi, a Swedish start-up company working with implementing hand use in Virtual Reality. The thesis presents hand models used to generate natural looking grasping motions. One model were made for each of the thirty-three different grasp types in Feix’s <em>The GRASP Taxonomy</em>.</p><p>Each model is based on functional principal components analysis which was performed on data containing recorded joint angles of grasping motions from real subjects. Prior to functional principal components analysis, dynamic time warping was performed on the recorded joint angles in order to put them on the same length and make them suitable for statistical analysis. The last step of the analysis was to project the data onto the functional principal components and train Gaussian mixture models on the weights obtained. New grasping motions could be generated by sampling weights from the Gaussian mixture models and attaching them to the functional principal components.</p><p>The generated grasps were in general satisfying, but all of the thirty-three grasps were not distinguishable from each other. This was most likely caused by the fact that each degree of freedom was modelled in isolation from each other, so that no correlation between them was included in the model.</p>

Note - only change to remove the empty paragraph and adding italics
----------------------------------------------------------------------
In diva2:738788 - - unnessary period at end of title:
"Modellering av turboladdarens surge-beteende för fordonsindustrin."
==>
"Modellering av turboladdarens surge-beteende för fordonsindustrin"

abstract is: 
<p>The turbocharger was originally designed and used to boost engine power of vehicles. Nowadays, when the demand for low carbon vehicles is increasing rapidly, a new application for the turbocharger has been found by using it to downsizing the engine. Using experimental as well as theoretical simulation models we can estimate the outcome and behavior of the compressor in an extended work range. An aspect that has a substantial effect on the turbocharger and engine is the surge line. Surge is a problematic stage where the compressor creates unwanted behavior which could damage the turbocharger as well as the engine. The surge line is a line where the transition to surge occurs. By changing the surge line, through simulations and calculations surge can be avoided, you can optimize and improve the turbocharger. This report mainly discusses and investigates the possibility to use fast one-dimensional simulation software instead of full scaled laboratories in the automotive industry, and estimate the work range of the given compressor.</p><p> </p>

corrected abstract:
<p>The turbocharger was originally designed and used to boost engine power of vehicles. Nowadays, when the demand for low carbon vehicles is increasing rapidly, a new application for the turbocharger has been found by using it to downsizing the engine. Using experimental as well as theoretical simulation models we can estimate the outcome and behavior of the compressor in an extended work range. An aspect that has a substantial effect on the turbocharger and engine is the surge line. Surge is a problematic stage where the compressor creates unwanted behavior which could damage the turbocharger as well as the engine. The surge line is a line where the transition to surge occurs. By changing the surge line, through simulations and calculations surge can be avoided, you can optimize and improve the turbocharger. This report mainly discusses and investigates the possibility to use fast one-dimensional simulation software instead of full scaled laboratories in the automotive industry, and estimate the work range of the given compressor.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:716518 
abstract is: 
<p>Cancer is a common cause of death worldwide and radiotherapy is one of the treatments used. Since treatment planning is a time consuming matter for the radiation therapist, a way to decrease the time spent finding the plan would be an improvement. This can be achieved by precalculating a number of optimal plans and then choosing among these in real-time.</p><p>In this thesis a dual algorithm for approximation of the Pareto optimal plans suggested by Bokrantz and Forsgren, was adapted to the parameters of the Leksell Gamma Knife®. A Graphical User Interface was also created, based on the navigation tool described by Monz et al to enable choosing among the pre-calculated dose plans.</p><p>The computational time of the algorithm was investigated and the dimensionality of the solutions and Pareto optimal points were looked at to see if it might be possible to reduce the number of dimensions to speed up computations.</p><p>Although no certain conclusions can be drawn about dimensionality reduction, I found no reason to rule that possibility out. It was also confirmed that there is reason to keep the number of objectives low to get a better approximation.</p><p> </p>

corrected abstract:
<p>Cancer is a common cause of death worldwide and radiotherapy is one of the treatments used. Since treatment planning is a time consuming matter for the radiation therapist, a way to decrease the time spent finding the plan would be an improvement. This can be achieved by precalculating a number of optimal plans and then choosing among these in real-time.</p><p>In this thesis a dual algorithm for approximation of the Pareto optimal plans suggested by Bokrantz and Forsgren, was adapted to the parameters of the Leksell Gamma Knife®. A Graphical User Interface was also created, based on the navigation tool described by Monz et al to enable choosing among the pre-calculated dose plans.</p><p>The computational time of the algorithm was investigated and the dimensionality of the solutions and Pareto optimal points were looked at to see if it might be possible to reduce the number of dimensions to speed up computations.</p><p>Although no certain conclusions can be drawn about dimensionality reduction, I found no reason to rule that possibility out. It was also confirmed that there is reason to keep the number of objectives low to get a better approximation.</p>

Note - only change to remove the empty paragraph
Note spelling error:
"et al" should be "et al."
----------------------------------------------------------------------
In diva2:1848437 
abstract is: 
<p>Turbulent mixing of single or multi-phase flows is common in diverse research fields, and direct numerical simulation is useful for understanding such phenomenon. To study the scaler transport in turbulence, the computational grids must resolve the Batchelor scale, which is smaller than the Kolmogorov scale by a factor of √Sc. This would commonly lead to the over-resolving of the Navier-Stokes equation, making DNS even more expensive. To overcome this issue, this thesis presents a method to reduce the computational cost in scalar turbulent flows, by using a coarse grid for the velocity and a fine grid for the scalar. A divergence-free Hermite interpolation is implemented for the velocity field to ensure the continuity equation is fulfilled on the fine grid. The interpolation scheme is validated by cases of Arnold–Beltrami–Childress flow and Taylor–Green vortex. For the active scaler, the integration schemes are included for fine-to-coarse integration. The multi-resolution method is parallelised by MPI and integrated into the open-source multiphase flow solver FluTAS. For the diffuse interface modelling in FluTAS, the indicator function is updated on the fine grid, and the surface tension force is then calculated and extrapolated back to the coarse grid for momentum equation update. The method is evaluated against the single-resolution method at different refinement factors.</p><p> </p>

corrected abstract:
<p>Turbulent mixing in multi-phase flows is prevalent across various research domains, and direct numerical simulation (DNS) is often used for understanding such phenomena. However, DNS of turbulent scalar transport entails resolving the Batchelor scale, which is typically smaller than the Kolmogorov scale by a factor of 𝑆𝑐<sup>−1/2</sup>. This can lead to increased number of grid points and the over-resolve of NaiverStokes equations, making the DNS even more expensive. To overcome this issue, this thesis presents a method to reduce the computational cost, by using a coarse grid for solving the velocity and pressure, and a fine grid for solving the scalar. A divergencefree interpolation scheme is implemented for velocity to ensure the continuity equation is fulfilled on the fine grid. The interpolation scheme is validated by Arnold–Beltrami– Childress flow and Taylor–Green vortex. For active scalars, a scheme of weighted averaging facilitates the fine-to-coarse integration. The multi-resolution method is parallelised by MPI and integrated into the open-source code FluTAS for multiphase flow simulation. For the diffuse-interface modelling in FluTAS, the phase-field variable and surface tension force are computed on the fine grid and integrated to the coarse grid to couple with the momentum equation. The presented method is evaluated against the single-resolution method using the rising bubble benchmark.</p>

Note - major differences between DiVA and original abstract and removed empty paragraph
----------------------------------------------------------------------
In diva2:1776757 
abstract is: 
<p>This paper aims at presenting the necessary tools to prove that a scheme of finite type over Z exhibits the same singularities as those which occur on a Grassmann variety. First, basic theory regarding the combinatorial objects matroids is presented. Some important examples for the remainder of the paper are given, which also serve to aid the reader in intuition and understanding of matroids. Basic algebraic geometry is presented, and the building blocks affine varieties, projective varieties and general varieties are introduced. These object are generalised in the following subsection as affine schemes and schemes, which are the central object of study in modern algebraic geometry. Important results from the theory of algebraic groups are shown in order to better understand the formulation and proof of the Gelfand–MacPherson theorem, which in turn is utilised, together with Mnëv’s universality theorem, to prove the main result of the paper.</p><p> </p>

corrected abstract:
<p>This paper aims at presenting the necessary tools to prove that a scheme of finite type over ℤ exhibits the same singularities as those which occur on a Grassmann variety. First, basic theory regarding the combinatorial objects matroids is presented. Some important examples for the remainder of the paper are given, which also serve to aid the reader in intuition and understanding of matroids. Basic algebraic geometry is presented, and the building blocks affine varieties, projective varieties and general varieties are introduced. These object are generalised in the following subsection as affine schemes and schemes, which are the central object of study in modern algebraic geometry. Important results from the theory of algebraic groups are shown in order to better understand the formulation and proof of the Gelfand–MacPherson theorem, which in turn is utilised, together with Mnëv’s universality theorem, to prove the main result of the paper.</p>

Note - only change to remove the empty paragraph and replacement of "Z" by "ℤ"
----------------------------------------------------------------------
In diva2:1436960 
abstract is: 
<p>The Aviation industry is important to the European economy and development, therefore a study of the sensitivity of the European flight network is interesting. If clusters exist within the network, that could indicate possible vulnerabilities or bottlenecks, since that would represent a group of airports poorly connected to other parts of the network. In this paper a cluster analysis using spectral clustering is performed with flight data from 34 different European countries. The report also looks at how to implement the spectral clustering algorithm for large data sets. After performing the spectral clustering it appears as if the European flight network is not clustered, and thus does not appear to be sensitive.</p><p> </p>

corrected abstract:
<p>The Aviation industry is important to the European economy and development, therefore a study of the sensitivity of the European flight network is interesting. If clusters exist within the network, that could indicate possible vulnerabilities or bottlenecks, since that would represent a group of airports poorly connected to other parts of the network. In this paper a cluster analysis using spectral clustering is performed with flight data from 34 different European countries. The report also looks at how to implement the spectral clustering algorithm for large data sets. After performing the spectral clustering it appears as if the European flight network is not clustered, and thus does not appear to be sensitive.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:892288 
abstract is: 
<p>Particle suspensions occur in many situations in nature and industry. In this master’s thesis, the motion of a single rigid spheroidal particle immersed in Stokes flow is studied numerically using a boundary integral method and a new specialized quadrature method known as quadrature by expansion (QBX). This method allows the spheroid to be massless or inertial, and placed in any kind of underlying Stokesian flow.</p><p> </p><p>A parameter study of the QBX method is presented, together with validation cases for spheroids in linear shear flow and quadratic flow. The QBX method is able to compute the force and torque on the spheroid as well as the resulting rigid body motion with small errors in a short time, typically less than one second per time step on a regular desktop computer. Novel results are presented for the motion of an inertial spheroid in quadratic flow, where in contrast to linear shear flow the shear rate is not constant. It is found that particle inertia induces a translational drift towards regions in the fluid with higher shear rate.</p>

corrected abstract:
<p>Particle suspensions occur in many situations in nature and industry. In this master’s thesis, the motion of a single rigid spheroidal particle immersed in Stokes flow is studied numerically using a boundary integral method and a new specialized quadrature method known as quadrature by expansion (QBX). This method allows the spheroid to be massless or inertial, and placed in any kind of underlying Stokesian flow.</p><p>A parameter study of the QBX method is presented, together with validation cases for spheroids in linear shear flow and quadratic flow. The QBX method is able to compute the force and torque on the spheroid as well as the resulting rigid body motion with small errors in a short time, typically less than one second per time step on a regular desktop computer. Novel results are presented for the motion of an inertial spheroid in quadratic flow, where in contrast to linear shear flow the shear rate is not constant. It is found that particle inertia induces a translational drift towards regions in the fluid with higher shear rate.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1341297 
Note: no full text in DiVA

abstract is: 
<p>Stochastic gradient descent is an algorithm used in optimization. Learning rate plays a central role in the stochastic gradient descent algorithm. If the selected learning rate lies within the appropriate interval, it affects the algorithm's convergence rate towards local minimum as well as its accuracy. In this work the stochastic gradient descent algorithm was used to treat two simple optimization problems. Firstly, a series of numerical experiments for a plethora of learning rate were performed where the behavior of the algorithm was studied. Secondly, the training of a neural network using the stochastic gradient descent was experimentally studied. The effect of learning rate values was tested as well as the neural network’s performance by varying parameters such as the number of nodes, the activation function and combinations of the above.</p><p> </p>

corrected abstract:
<p>Stochastic gradient descent is an algorithm used in optimization. Learning rate plays a central role in the stochastic gradient descent algorithm. If the selected learning rate lies within the appropriate interval, it affects the algorithm's convergence rate towards local minimum as well as its accuracy. In this work the stochastic gradient descent algorithm was used to treat two simple optimization problems. Firstly, a series of numerical experiments for a plethora of learning rate were performed where the behavior of the algorithm was studied. Secondly, the training of a neural network using the stochastic gradient descent was experimentally studied. The effect of learning rate values was tested as well as the neural network’s performance by varying parameters such as the number of nodes, the activation function and combinations of the above.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1877780 
abstract is: 
<p>This bachelor thesis aims to give an introduction to various Hopf algebras that arise in combinatorics, with a view towards symmetric functions. We begin by covering the algebraic background needed to define Hopf algebras, including a discussion of the algebra-coalgebra duality. Takeuchi's formula for the antipode is stated and proved. It is then generalised to incidence Hopf algebras. This is followed by a discussion of the Hopf algebra of symmetric functions. It is shown that the Hopf algebra of symmetric functions is self-dual. We also show that the graded dual of the Hopf algebra of quasisymmetric functions is the Hopf algebra of non-commutative symmetric functions. Relations to the Hopf algebra of symmetric functions in non-commuting variables are emphasised. Finally, we state and prove the Aguiar-Bergeron-Sottile universality theorem.</p><p> </p>

corrected abstract:
<p>This bachelor thesis aims to give an introduction to various Hopf algebras that arise in combinatorics, with a view towards symmetric functions. We begin by covering the algebraic background needed to define Hopf algebras, including a discussion of the algebra-coalgebra duality. Takeuchi's formula for the antipode is stated and proved. It is then generalised to incidence Hopf algebras. This is followed by a discussion of the Hopf algebra of symmetric functions. It is shown that the Hopf algebra of symmetric functions is self-dual. We also show that the graded dual of the Hopf algebra of quasisymmetric functions is the Hopf algebra of non-commutative symmetric functions. Relations to the Hopf algebra of symmetric functions in non-commuting variables are emphasised. Finally, we state and prove the Aguiar-Bergeron-Sottile universality theorem.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:740516 - unnessary period at end of title:
"On the 1932 Discovery of the Positron."
==>
"On the 1932 Discovery of the Positron."

abstract is: 
<p>An experiment on Cosmic rays performed by Carl D Anderson led to the accidental discovery of the positron in 1932. The discovery was a turning point in particle physics which led to numerous other theories and has been discussed by scientists all over the world. Anderson had photographed a 63 MeV, upward moving electron. The possible origin of such a positron has never before been discussed and is what this report will aim to explain. The report will include some evidence that the particle is in fact a positron as well as a discussion of the four main theories whose possibility and probability will be discussed; pion decay, muon decay, magnetic field bending and pair production. The report will also cover a historical background for Anderson’s experiment, as well as a theoretical background needed for the theories of the origin. The probability of discovering a positron with any of the theorized origins is extremely low and for some theories, even impossible.</p><p> </p>

corrected abstract:
<p>An experiment on Cosmic rays performed by Carl D Anderson led to the accidental discovery of the positron in 1932. The discovery was a turning point in particle physics which led to numerous other theories and has been discussed by scientists all over the world. Anderson had photographed a 63 MeV, upward moving electron. The possible origin of such a positron has never before been discussed and is what this report will aim to explain. The report will include some evidence that the particle is in fact a positron as well as a discussion of the four main theories whose possibility and probability will be discussed; pion decay, muon decay, magnetic field bending and pair production. The report will also cover a historical background for Anderson’s experiment, as well as a theoretical background needed for the theories of the origin. The probability of discovering a positron with any of the theorized origins is extremely low and for some theories, even impossible.</p>

Note - only change to remove the empty paragraph
Note spelling error: there should be a period after "D." as it is an initial - this is correct in the Swedish abstract
----------------------------------------------------------------------
In diva2:1779459 
abstract is: 
<p>The focus of this thesis is to find, visualize and analyze the optimal flow of autonomous electric vehicles with charge constraints in urban traffic with respect to energy consumption. The traffic has been formulated as a static multi-commodity network flow problem, for which two different models have been implemented to handle the charge constraints. The first model uses a recursive algorithm to find the optimal solution fulfilling the charge constraints, while the second model discretizes the commodities’ battery to predetermined battery levels. An implementation of both methods is provided through simulations on scenarios of three different sizes. The results show that both methods are capable of representing the traffic flow with charge constraints, with limitations given by the size of the problem. In particular, the recursive model has the advantage of considering the charge as a continuous quantity. On the other hand the discretization of battery levels allows to handle charge constraint setups with higher complexity, that is when longer detours are needed to fulfill the charge constraints.</p><p> </p>

corrected abstract:
<p>The focus of this thesis is to find, visualize and analyze the optimal flow of autonomous electric vehicles with charge constraints in urban traffic with respect to energy consumption. The traffic has been formulated as a static multi-commodity network flow problem, for which two different models have been implemented to handle the charge constraints. The first model uses a recursive algorithm to find the optimal solution fulfilling the charge constraints, while the second model discretizes the commodities’ battery to predetermined battery levels. An implementation of both methods is provided through simulations on scenarios of three different sizes. The results show that both methods are capable of representing the traffic flow with charge constraints, with limitations given by the size of the problem. In particular, the recursive model has the advantage of considering the charge as a continuous quantity. On the other hand the discretization of battery levels allows to handle charge constraint setups with higher complexity, that is when longer detours are needed to fulfill the charge constraints.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1352113 
abstract is: 
<p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p> </p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome beforehand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>
mc='forehand' c='fore hand'

partal corrected: diva2:1352113: <p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p> </p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome before hand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>

corrected abstract:
<p>The optimization of case hardening depth for small gears was investigated with the use of Abaqus and the subroutine DANTE to simulate the formation of the microstructural phases, resulting in residual stresses and increased hardness. This was done with a step wise increment of the carburizing time, resulting in a theoretical maximum for compressive residual stresses at the surface. The heat treatment parameters were then used for case hardening two gears with different carburizing times. The heat treated gears were then tested for tooth root bending fatigue. The fatigue testing resulted in a fatigue limit increase, where the gear with largest simulated compressive stress showed the highest fatigue limit.</p><p>Both the heat treated gears were hardness tested and compared with the conducted simulations resulting in an underestimated hardness. An investigation to see whenever the simulations could predict the fatigue outcome beforehand with a probabilistic model was put into place. This resulted in an underestimated fatigue limit in relation to the raw fatigue data.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:904606 
abstract is: 
<p>Today’s society keeps trying to improve efficiency in production and general reliability in systems. This is true for all types of companies that are driven to use optimization techniques in their work. One major field impacted by these studies is transport. Railway companies set the example and make optimization a priority in order to handle train flows the best way possible. That is why the French National Railway Company imagined an informatic tool to decide how trains are to be parked optimally. The thesis exposed in this report has been proposed for treating the modeling and programming parts of the project.</p><p>Modeling the station has been done in association with railway experts who bring their knowledge about the station infrastructure. They are also a reference for determining the constraints a train must obey when arriving at the station, parking and leaving. Only useful aspects of the station must be taken into account. That is why a compromise has been chosen in the level of detail modeled.</p><p>Then, a mathematical formulation of the problem has been proposed. The decision variables are for each train its arrival path, parking track and departure path. There are lots of possibilities and the choice can be difficult. The constraints are quite numerous as well. Thus, it has been decided to divide the constraints in two parts: hard and soft constraints. The general idea is to find a solution that obeys all hard constraints and minimizes the number of soft constraints not respected. </p><p>The project consisted also in choosing a language to code the model and a solver. The mathematical model has been written with the modeling language OPL that calls the solver Cplex. This software is one of the best to solve large scale problems that may arise for complex stations. Since the problem was a Mixed Integer Programming; the solver uses the Branch &amp; Cut algorithm that is a combination between different techniques. It consists in applying the Branch &amp; Bound algorithm while cutting the feasibility region. Heuristics techniques are also used by the algorithm in order to provide quickly a feasible solution. After running the program, we get a solution which is optimal according to the approach discussed earlier on. More precisely, it would determine for each train an arrival path, a parking track and a departure path.</p><p> </p><p>The program developed during this thesis is proposed to the stations. It would help them in deciding how trains are parked on one day. This decision is made in a conception phase happening several months before the actual day treated. Several major stations have tested the optimization tool. Thanks to a regularity coefficient, they are able to judge the quality of the solution. Since this score has been increased, the solution is judged as rather good. Furthermore, they will use it for the year 2016. A significant advantage is also the time for making the decision. When the modeling phase is done, a few minutes are enough to produce a solution for a given train set, whereas it can take several weeks of work by hand.</p>

corrected abstract:
<p>Today’s society keeps trying to improve efficiency in production and general reliability in systems. This is true for all types of companies that are driven to use optimization techniques in their work. One major field impacted by these studies is transport. Railway companies set the example and make optimization a priority in order to handle train flows the best way possible. That is why the French National Railway Company imagined an informatic tool to decide how trains are to be parked optimally. The thesis exposed in this report has been proposed for treating the modeling and programming parts of the project.</p><p>Modeling the station has been done in association with railway experts who bring their knowledge about the station infrastructure. They are also a reference for determining the constraints a train must obey when arriving at the station, parking and leaving. Only useful aspects of the station must be taken into account. That is why a compromise has been chosen in the level of detail modeled.</p><p>Then, a mathematical formulation of the problem has been proposed. The decision variables are for each train its arrival path, parking track and departure path. There are lots of possibilities and the choice can be difficult. The constraints are quite numerous as well. Thus, it has been decided to divide the constraints in two parts: hard and soft constraints. The general idea is to find a solution that obeys all hard constraints and minimizes the number of soft constraints not respected.</p><p>The project consisted also in choosing a language to code the model and a solver. The mathematical model has been written with the modeling language OPL that calls the solver Cplex. This software is one of the best to solve large scale problems that may arise for complex stations. Since the problem was a Mixed Integer Programming; the solver uses the Branch &amp; Cut algorithm that is a combination between different techniques. It consists in applying the Branch &amp; Bound algorithm while cutting the feasibility region. Heuristics techniques are also used by the algorithm in order to provide quickly a feasible solution. After running the program, we get a solution which is optimal according to the approach discussed earlier on. More precisely, it would determine for each train an arrival path, a parking track and a departure path.</p><p>The program developed during this thesis is proposed to the stations. It would help them in deciding how trains are parked on one day. This decision is made in a conception phase happening several months before the actual day treated. Several major stations have tested the optimization tool. Thanks to a regularity coefficient, they are able to judge the quality of the solution. Since this score has been increased, the solution is judged as rather good. Furthermore, they will use it for the year 2016. A significant advantage is also the time for making the decision. When the modeling phase is done, a few minutes are enough to produce a solution for a given train set, whereas it can take several weeks of work by hand.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1360595
Note: no full text in DiVA

abstract is: 
<p>Investigations on the Circular Restricted 3- Body Problem (CR3BP) and the motion about the Lagrangian points are not recent. Several past (upcoming) missions have used (plan to use) its dynamics. The existence of specific periodic orbits and their associated invariant manifolds is one property of the CR3BP that raises the interest. These periodic orbits are interesting for their great observation properties, eclipse avoidance, communication’s continuity with the Earth, etc. However, to reach them, optimized transfer trajectories have to be found. A numerical tool is developed to construct firstly these orbits, before using them as input parameters for the invariant manifolds. Indeed, when a spacecraft is inserted into one of these manifolds, it shall naturally reach the orbit without any additional cost. This numerical computation provides manifolds’s insertion points, which are used in return by a Nonlinear Programming (NLP) tool to eventually find the optimized trajectory. Families of periodic orbits and manifolds, together with optimized transfer trajectories, have been successfully computed, with a focus on the Halo Orbits of the Earth-Moon system. Some members of this family, the Near-Rectilinear Halo Orbits (NRHOs), are of a great interest both for their geometry characteristics (close approach of the secondary body) and stability properties. However, in the Earth-Moon system, the associated manifolds do not have points relatively close from the Earth. The thesis work hence does not ensure that using manifolds as transfer arcs is beneficial, compared to a direct transfer. Besides, the Time-Of-Flight (TOF) is significantly larger. Transfer strategies making use of the CR3BP dynamics still are interesting, radically different from the usual trajectories and offering a larger number of opportunities. They may be less expansive, and could particularly be used for uncrewed space missions.</p><p> </p>

corrected abstract:
<p>Investigations on the Circular Restricted 3- Body Problem (CR3BP) and the motion about the Lagrangian points are not recent. Several past (upcoming) missions have used (plan to use) its dynamics. The existence of specific periodic orbits and their associated invariant manifolds is one property of the CR3BP that raises the interest. These periodic orbits are interesting for their great observation properties, eclipse avoidance, communication’s continuity with the Earth, etc. However, to reach them, optimized transfer trajectories have to be found. A numerical tool is developed to construct firstly these orbits, before using them as input parameters for the invariant manifolds. Indeed, when a spacecraft is inserted into one of these manifolds, it shall naturally reach the orbit without any additional cost. This numerical computation provides manifolds’s insertion points, which are used in return by a Nonlinear Programming (NLP) tool to eventually find the optimized trajectory. Families of periodic orbits and manifolds, together with optimized transfer trajectories, have been successfully computed, with a focus on the Halo Orbits of the Earth-Moon system. Some members of this family, the Near-Rectilinear Halo Orbits (NRHOs), are of a great interest both for their geometry characteristics (close approach of the secondary body) and stability properties. However, in the Earth-Moon system, the associated manifolds do not have points relatively close from the Earth. The thesis work hence does not ensure that using manifolds as transfer arcs is beneficial, compared to a direct transfer. Besides, the Time-Of-Flight (TOF) is significantly larger. Transfer strategies making use of the CR3BP dynamics still are interesting, radically different from the usual trajectories and offering a larger number of opportunities. They may be less expansive, and could particularly be used for uncrewed space missions.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1198117 
abstract is: 
<p>Climate change is an evermore urging existential treat to the human enterprise. Mean temperature and greenhouse gas emissions have in-creased exponentially since the industrial revolution. But solutions are also mushrooming with exponential pace. Renewable energy technologies, such as wind and solar power, are deployed like never before and their costs have decreased signiﬁcantly. In order to allow for further transformation of the energy system these technologies must be reﬁned and optimised. In wind energy one important ﬁeld with high potential of reﬁnement is aerodynamics. The aerodynamics of wind turbines constitutes one challenging research frontier in aerodynamics today.</p><p> </p><p>In this study, a novel approach for calculating wind turbine ﬂow is developed. The approach is based on the partially parabolic Navier-Stokes equations, which can be solved computationally with higher eﬃciency as compared to the fully elliptic version. The modelling of wind turbine thrust is done using actuator-disk theory and the torque is modelled by application of the Joukowsky rotor. A validation of the developed model and force implementation is conducted using four diﬀerent validation cases.</p><p> </p><p>In order to provide value for industrial wind energy projects, the model must be extended to account for turbulence (and terrain in case of onshore projects). Possible candidates for turbulence modelling are parabolic k-ε and explicit Reynolds stress turbulence models. The terrain could possibly be incorporated consistently with the used projection method by altering the ﬁnite diﬀerence grid layout.</p>

corrected abstract:
<p>Climate change is an evermore urging existential treat to the human enterprise. Mean temperature and greenhouse gas emissions have increased exponentially since the industrial revolution. But solutions are also mushrooming with exponential pace. Renewable energy technologies, such as wind and solar power, are deployed like never before and their costs have decreased significantly. In order to allow for further transformation of the energy system these technologies must be refined and optimised. In wind energy one important field with high potential of refinement is aerodynamics. The aerodynamics of wind turbines constitutes one challenging research frontier in aerodynamics today.</p><p>In this study, a novel approach for calculating wind turbine flow is developed. The approach is based on the partially parabolic Navier-Stokes equations, which can be solved computationally with higher efficiency as compared to the fully elliptic version. The modelling of wind turbine thrust is done using actuator-disk theory and the torque is modelled by application of the Joukowsky rotor. A validation of the developed model and force implementation is conducted using four different validation cases.</p><p>In order to provide value for industrial wind energy projects, the model must be extended to account for turbulence (and terrain in case of onshore projects). Possible candidates for turbulence modelling are parabolic 𝑘-<em>ε</em> and explicit Reynolds stress turbulence models. The terrain could possibly be incorporated consistently with the used projection method by altering the finite difference grid layout.</p>

Note - only changes to remove the empty paragraphs, removed an unnecessary hyphen, and replaced "k" with "𝑘" and added italics for the omega.
----------------------------------------------------------------------
In diva2:560786 
abstract is: 
<p>Several models for predicting future customer profitability early into customer life-cycles in the property and casualty business are constructed and studied. The objective is to model risk at a customer level with input data available early into a private consumer’s lifespan. Two retained models, one using Generalized Linear Model another using a multilayer perceptron, a special form of Artificial Neural Network are evaluated using actual data. Numerical results show that differentiation on estimated future risk is most effective for customers with highest claim frequencies.</p><p> </p>

corrected abstract:
<p>Several models for predicting future customer profitability early into customer life-cycles in the property and casualty business are constructed and studied. The objective is to model risk at a customer level with input data available early into a private consumer’s lifespan. Two retained models, one using Generalized Linear Model another using a multilayer perceptron, a special form of Artificial Neural Network are evaluated using actual data. Numerical results show that differentiation on estimated future risk is most effective for customers with highest claim frequencies.</p>
----------------------------------------------------------------------
In diva2:1878726 
abstract is: 
<p>This project applied statistical inference methods to historical data of mixed martial arts (MMA) matches from the Ultimate Fighting Championship (UFC). The goal of the project was to create a model to predict the outcome of Ultimate Fighting Championship matches with the best possible accuracy. The main methods used in the project were logistic regression and Bayesian regression. The data used for said model was taken from matches between early April 2000 and mid April 2024. The predictions made by these models were compared with the predictions of various betting sites as well as with the true outcomes of the matches. The logistic regression model and the Bayesian model predicted the true outcome of the matches 60% and 70% of the time respectively, with both having comparable predictions to those of the betting sites.</p><p> </p>

corrected abstract:
<p>This project applied statistical inference methods to historical data of mixed martial arts (MMA) matches from the Ultimate Fighting Championship (UFC). The goal of the project was to create a model to predict the outcome of Ultimate Fighting Championship matches with the best possible accuracy. The main methods used in the project were logistic regression and Bayesian regression. The data used for said model was taken from matches between early April 2000 and mid April 2024. The predictions made by these models were compared with the predictions of various betting sites as well as with the true outcomes of the matches. The logistic regression model and the Bayesian model predicted the true outcome of the matches 60% and 70% of the time respectively, with both having comparable predictions to those of the betting sites.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:690036 - unnessary period at end of title:
"Prediktion av villapris och dess faktorers inverkan."
==>
"Prediktion av villapris och dess faktorers inverkan"

abstract is: 
<p> </p><p><em>A villas price </em>depends on several important factors. By statistical data, a mathematical <em>multiple regression model </em>was modeled. The model has important explanatory variables such as living space, renovation year and standard points has been taken into consideration, in order to assess their impact on the <em>final price </em>for private homes.</p><p>By using a statistical program,</p><p><em>Minitab 16</em>, the final model was selected with <em>eight explanatory variables</em>. The regression for this model explains up to <em>67.3 % </em>of the variation on the final price.</p><p>The results showed percentage wise that the standard points had the greatest impact on the price, there after renovation year and then living space.</p>

corrected abstract:
<p>A villas <em>price</em> depends on several important factors. By statistical data, a mathematical <em>multiple regression model</em> was modeled. The model has important explanatory variables such as living space, renovation year and standard points has been taken into consideration, in order to assess their impact on the <em>final price</em> for private homes.</p><p>By using a statistical program, <em>Minitab 16</em>, the final model was selected with <em>eight explanatory variables</em>. The regression for this model explains up to <em>67.3 %</em> of the variation on the final price.</p><p>The results showed percentage wise that the standard points had the greatest impact on the price, there after renovation year and then living space.</p>

Note - removed the empty paragraph and adjusted the italics
----------------------------------------------------------------------
In diva2:1341561 
abstract is: 
<p>This report describes the design and production of a 3-axis Helmholtz coil assembly and its control unit. The purpose of the system is to simulate the magnetic environment that the CubeSat MIST will need to measure in order to determine and control its attitude. To achieve this, the system consists of three Helmholtz coils with diameters of roughly 1 metre, supplied by a circuit that filters, transforms, and amplifies signals of 0-5 V (e.g. Arduino signals) to -50 to 50 V. The size of the coils allow for a near-homogeneous magnetic field large enough to cover the whole satellite. By adjusting the input, two necessary tests can be done on the satellite's attitude determination and control system. The first consists of verifying the magnetometer's correct measurement of the direction of the ambient magnetic field, and the other of testing the detumbling capability of the system when the satellite is in a rotating field. The equipment produced has been tested to verify its operation meets set requirements for testing.</p><p> </p>

corrected abstract:
<p>This report describes the design and production of a 3-axis Helmholtz coil assembly and its control unit. The purpose of the system is to simulate the magnetic environment that the CubeSat MIST will need to measure in order to determine and control its attitude. To achieve this, the system consists of three Helmholtz coils with diameters of roughly 1 metre, supplied by a circuit that filters, transforms, and amplifies signals of 0-5 V (e.g. Arduino signals) to -50 to 50 V. The size of the coils allow for a near-homogeneous magnetic field large enough to cover the whole satellite. By adjusting the input, two necessary tests can be done on the satellite's attitude determination and control system. The first consists of verifying the magnetometer's correct measurement of the direction of the ambient magnetic field, and the other of testing the detumbling capability of the system when the satellite is in a rotating field. The equipment produced has been tested to verify its operation meets set requirements for testing.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1798454 
abstract is: 
<p>GaN-HEMTs (Gallium Nitride-based High Electron Mobility Transistors) have, thanks to the large band gap of GaN, electrical properties that are suitable for applications of high electrical voltages, high currents, and fast switching. The large band gap also gives GaN-HEMTs a high resistance to radiation. In this degree project, the effects of 2 MeV proton irradiation of GaN-HEMTs constructed on both silicon carbide and silicon substrates are investigated. 20 transistors per substrate were irradiated in the particle accelerator 5 MV NEC Pelletron in the Ångström laboratory at Uppsala University. These transistors were exposed to radiation doses in the range of 10^11 to 10^15 protons/cm^2. The analysis shows that both transistors on silicon, as well as silicon carbide, are unaffected by proton irradiation up to a dose of 10^14 protons/cm^2. GaN-on-Si transistors show less influence of radiation than GaN-on-SiC transistors. The capacitances between gate and drain as well as drain and source for both GaN-on-SiC and GaN-on-Si HEMTs show hysteresis as a function of forward and backward gate voltage sweeps for the radiation dose of 10^15 protons/cm^2.</p><p> </p>

corrected abstract:
<p>GaN-HEMTs (Gallium Nitride-based High Electron Mobility Transistors) have, thanks to the large band gap of GaN, electrical properties that are suitable for applications of high electrical voltages, high currents, and fast switching. The large band gap also gives GaN-HEMTs a high resistance to radiation. In this degree project, the effects of 2 MeV proton irradiation of GaN-HEMTs constructed on both silicon carbide and silicon substrates are investigated. 20 transistors per substrate were irradiated in the particle accelerator 5 MV NEC Pelletron in the Ångström laboratory at Uppsala University. These transistors were exposed to radiation doses in the range of 10<sup>11</sup> to 10<sup>15</sup> protons/cm<sup>2</sup>. The analysis shows that both transistors on silicon, as well as silicon carbide are unaffected by proton irradiation up to a dose of 10<sup>14</sup> protons/cm<sup>2</sup>. GaN-on-Si transistors show less influence of radiation than GaN-on-SiC transistors. The capacitances between gate and drain as well as drain and source for both GaN-on-SiC and GaN-on-Si HEMTs show hysteresis as a function of forward and backward gate voltage sweeps for the radiation dose of 10<sup>15</sup> protons/cm<sup>2</sup>.</p>

Note - removed the empty paragraph and fixed the superscripts
----------------------------------------------------------------------
In diva2:858615 
abstract is: 
<p>This thesis describes a Coq formalization of realizability interpretations of arithmetic. The realizability interpretations are based on partial combinatory algebras—to each partial combinatory algebra there is an associated realizability interpretation. I construct two partial combinatory algebras. One of these gives a realizability interpretation equivalent to Kleene’s original one, without involving the usual recursion-theoretic machinery.</p><p> </p>

corrected abstract:
<p>This thesis describes a Coq formalization of realizability interpretations of arithmetic. The realizability interpretations are based on partial combinatory algebras&mdash;to each partial combinatory algebra there is an associated realizability interpretation. I construct two partial combinatory algebras. One of these gives a realizability interpretation equivalent to Kleene’s original one, without involving the usual recursion-theoretic machinery.</p>

Note - removed the empty paragraph and change on ". " to an &mdash;"
----------------------------------------------------------------------
In diva2:1570223 
abstract is: 
<p>Spotify, which is one of the worlds biggest music services, posted a data set and an open-ended challenge for music recommendation research. This study's goal is to recommend songs to playlists with the given data set from Spotify using Spectral clustering. While the given data set had 1 000 000 playlists, Spectral clustering was performed on a subset with 16 000 playlists due to the lack of computational resources. With four different weighting methods describing the connection between playlists, the study shows results of reasonable clusters where similar category of playlists were clustered together although most of the results also had a very large clusters where a lot of different sorts of playlists were clustered together. The conclusion of the results were that the data was overly connected as an effect of our weighting methods. While the results show the possibility of recommending songs to a limited number of playlists, hierarchical clustering would possibly be helpful to be able to recommend song to a larger amount of playlists, but that is left to future research to conclude.</p><p> </p>

corrected abstract:
<p>Spotify, which is one of the worlds biggest music services, posted a data set and an open-ended challenge for music recommendation research. This study's goal is to recommend songs to playlists with the given data set from Spotify using Spectral clustering. While the given data set had 1 000 000 playlists, Spectral clustering was performed on a subset with 16 000 playlists due to the lack of computational resources. With four different weighting methods describing the connection between playlists, the study shows results of reasonable clusters where similar category of playlists were clustered together although most of the results also had a very large clusters where a lot of different sorts of playlists were clustered together. The conclusion of the results were that the data was overly connected as an effect of our weighting methods. While the results show the possibility of recommending songs to a limited number of playlists, hierarchical clustering would possibly be helpful to be able to recommend song to a larger amount of playlists, but that is left to future research to conclude.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:555815 
abstract is: 
<p>Abstract</p><p> </p><p>As accurately as possible, creditors wish to determine if a potential debtor will repay the borrowed sum. To achieve this mathematical models known as credit scorecards quantifying the risk of default are used. In this study it is investigated whether the scorecard can be improved by using reject inference and thereby include the characteristics of the rejected population when refining the scorecard. The reject inference method used is parcelling. Logistic regression is used to estimate probability of default based on applicant characteristics. Two models, one with and one without reject inference, are compared using Gini coefficient and estimated profitability. The results yield that, when comparing the two models, the model with reject inference both has a slightly higher Gini coefficient as well a showing an increase in profitability. Thus, this study suggests that reject inference does improve the predictive power of the scorecard, but in order to verify the results additional testing on a larger calibration set is needed</p>

corrected abstract:
<p>As accurately as possible, creditors wish to determine if a potential debtor will repay the borrowed sum. To achieve this mathematical models known as credit scorecards quantifying the risk of default are used. In this study it is investigated whether the scorecard can be improved by using reject inference and thereby include the characteristics of the rejected population when refining the scorecard. The reject inference method used is parcelling. Logistic regression is used to estimate probability of default based on applicant characteristics. Two models, one with and one without reject inference, are compared using Gini coefficient and estimated profitability. The results yield that, when comparing the two models, the model with reject inference both has a slightly higher Gini coefficient as well as showing an increase in profitability. Thus, this study suggests that reject inference does improve the predictive power of the scorecard, but in order to verify the results additional testing on a larger calibration set is needed.</p>

Note - removed "<p>Abstract</p><p> </p>" and corrected "a showing" to "as showing"
----------------------------------------------------------------------
In diva2:1334832 
abstract is: 
<p>This paper will present a concrete study of representations of the special orthogonal group, SO(3), a group of great importance in physics. Specifically, we will study a natural representation of SO(3) in the space of polynomials in three variables with complex coefficients, C[x, y, z]. We will find that this special case provides all irreducible representations of SO(3), and also present some corollaries about spherical harmonics. Some preparatory theory regarding abstract algebra, linear algebra, topology and measure theory will also be presented.</p><p> </p>

corrected abstract:
<p>This paper will present a concrete study of representations of the special orthogonal group, <em>SO(3)</em>, a group of great importance in physics. Specifically, we will study a natural representation of <em>SO(3)</em> in the space of polynomials in three variables with complex coefficients, ℂ[𝑥, 𝑦, 𝑧]. We will find that this special case provides all irreducible representations of <em>SO(3)</em>, and also present some corollaries about spherical harmonics. Some preparatory theory regarding abstract algebra, linear algebra, topology and measure theory will also be presented.</p>

Note changes in mathematical elements and removed the empty paragraph
----------------------------------------------------------------------
In diva2:1462397 
abstract is: 
<p>The space industry and the technological developments regarding space exploration hasn’t been this popular since the first moon landing. The privatization of space exploration and the vertical landing rockets made rocket science mainstream again. While being able to reuse rockets is efficient both in terms of profitability and popularity, these developments are still in their early stages. Vertical landing has challenges that, if neglected, can cause disastrous consequences. The existing studies on the matter usually don’t account for aerodynamics forces and corresponding controls, which results in higher fuel consumption thus lessening the economical benefits of vertical landing. Similar problems have been tackled in studies not regarding booster landings but regarding planetary landings. And while multiple solutions have been proposed for these problems regarding planetary landings, the fact that the reinforcement learning concepts work well and provide robustness made them a valid candidate for applying to booster landings. In this study, we focus on developing a vertical booster descent guidance and control law that’s robust by applying reinforcement learning concept. Since reinforcement learning method that is chosen requires solving Optimal Control Problems (OCP), we also designed and developed an OCP solver software. The robustness of resulting hybrid guidance and control policy will be examined against various different uncertainties including but not limited to wind, delay and aerodynamic uncertainty.</p><p> </p>

corrected abstract:
<p>The space industry and the technological developments regarding space exploration hasn’t been this popular since the first moon landing. The privatization of space exploration and the vertical landing rockets made rocket science mainstream again. While being able to re-use rockets is efficient both in terms of profitability and popularity, these developments are still in their early stages. Vertical landing has challenges that, if neglected, can cause disastrous consequences. The existing studies on the matter usually don’t account for aerodynamics forces and corresponding controls, which results in higher fuel consumption thus lessening the economical benefits of vertical landing.</p><p>Similar problems have been tackled in studies not regarding booster landings but regarding planetary landings. And while multiple solutions have been proposed for these problems regarding planetary landings, the fact that the reinforcement learning concepts work well and provide robustness made them a valid candidate for applying to booster landings. In this study, we focus on developing a vertical booster descent guidance and control law that’s robust by applying reinforcement learning concept. Since reinforcement learning method that is chosen requires solving Optimal Control Problems (OCP), we also designed and developed an OCP solver software. The robustness of resulting hybrid guidance and control policy will be examined against various different uncertainties including but not limited to wind, delay and aerodynamic uncertainty.</p>

Note - removed the empty paragraph, inserted missing paragraph break, and inserted missing hyphen
----------------------------------------------------------------------
In diva2:1776588 
abstract is: 
<p>The objective of robust portfolio optimization is to find a way to allocate capital to some financial assets such that portfolio return is maximized in the worst-case scenario, which is desirable for investors with a low tolerance for risk. This study aims to apply the robust approach to asset allocation based on 30 of the biggest stocks on the Stockholm Stock Exchange. Three models with different constraints on portfolio return and variance are obtained and solved using the Gurobi Optimizer. The result of any one of the models could be proposed as a low-risk portfolio. The choice between the models is a trade-off between higher expected return and lower variance, and it depends on the individual preferences of the investor.</p><p> </p>

corrected abstract:
<p>The objective of robust portfolio optimization is to find a way to allocate capital to some financial assets such that portfolio return is maximized in the worst-case scenario, which is desirable for investors with a low tolerance for risk. This study aims to apply the robust approach to asset allocation based on 30 of the biggest stocks on the Stockholm Stock Exchange. Three models with different constraints on portfolio return and variance are obtained and solved using the Gurobi Optimizer. The result of any one of the models could be proposed as a low-risk portfolio. The choice between the models is a trade-off between higher expected return and lower variance, and it depends on the individual preferences of the investor.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:902529 
abstract is: 
<p>The topic of this thesis is the implementation of rapid branching to find an integer solution for the train timetabling problem. The techniques that rapid branching are based on are presented. The important aspect of rapid branching are discussed and then the algorithm is applied to some artificial problems. It is shown that rapid branching can be both faster and slower than a standard integer solver depending on the problem instance. For the most realistic set of the examined instances, rapid branching turned out to be faster than the standard integer solver and produce satisficingly high quality solutions.</p><p> </p>

corrected abstract:
<p>The topic of this thesis is the implementation of rapid branching to find an integer solution for the train timetabling problem. The techniques that rapid branching are based on are presented. The important aspects of rapid branching are discussed and then the algorithm is applied to some artificial problems. It is shown that rapid branching can be both faster and slower than a standard integer solver depending on the problem instance. For the most realistic set of the examined instances, rapid branching turned out to be faster than the standard integer solver and produce satisficingly high quality solutions.</p>

Note - removed the empty paragraph and added the "s" to "aspects"
----------------------------------------------------------------------
In diva2:1219083 
abstract is: 
<p>The purpose of this project was to implement an inverse diffusion algorithm to locate the sources of an emitted substance. This algorithm has yielded successful results when applied to biological cell detection, and it has been suggested that the run time could be greatly reduced if adaptions for a computation graph framework are made. This would automate calculations of gradients and allow for faster execution on a graphics processing unit.</p><p>The algorithm implementation was realized in TensorFlow, which is primarily a machine learning oriented programming library. Computer-generated biological test images were then used to evaluate the performance using regular image analysis software and accuracy metrics.</p><p>Comparisons reveal that the TensorFlow implementation of the algorithm can match the accuracy metrics of traditional implementations of the same algorithm. Viewed in a broader scope this serves as an example to highlight the possibility of using computation graph frameworks to solve large scale optimization problems, and more specifically inverse problems.</p><p> </p>

corrected abstract:
<p>The purpose of this project was to implement an inverse diffusion algorithm to locate the sources of an emitted substance. This algorithm has yielded successful results when applied to biological cell detection, and it has been suggested that the run time could be greatly reduced if adaptions for a computation graph framework are made. This would automate calculations of gradients and allow for faster execution on a graphics processing unit.</p><p>The algorithm implementation was realized in TensorFlow, which is primarily a machine learning oriented programming library. Computer-generated biological test images were then used to evaluate the performance using regular image analysis software and accuracy metrics.</p><p>Comparisons reveal that the TensorFlow implementation of the algorithm can match the accuracy metrics of traditional implementations of the same algorithm. Viewed in a broader scope this serves as an example to highlight the possibility of using computation graph frameworks to solve large scale optimization problems, and more specifically inverse problems.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1570353 
abstract is: 
<p>Climate is a tremendously complex topic, affecting many aspects of human activity and constantly changing. Defining some structures and rules for how it works is thereof of the utmost importance even though it might only cover a small part of the complexity. Cluster analysis is a tool developed in data analysis that is able to categorize data into groups of similar type. In this paper data from the Swedish Meteorological and Hydrological Institute (SMHI) is clustered to find a partitioning. The cluster analysis used is called Spectral clustering which is a family of methods making use of the spectral properties of graphs. Concrete results over different groupings of climate over Sweden were found.</p><p> </p>

corrected abstract:
<p>Climate is a tremendously complex topic, affecting many aspects of human activity and constantly changing. Defining some structures and rules for how it works is thereof of the utmost importance even though it might only cover a small part of the complexity. Cluster analysis is a tool developed in data analysis that is able to categorize data into groups of similar type. In this paper data from the Swedish Meteorological and Hydrological Institute (SMHI) is clustered to find a partitioning. The cluster analysis used is called Spectral clustering which is a family of methods making use of the spectral properties of graphs. Concrete results over different groupings of climate over Sweden were found.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1742865 
abstract is: 
<p>As cyber-physical systems become increasingly complex, the management and verification of requirements during design is essential. A new language called CRML (Common Requirement Modelling Language) has been created during the European EMBrACE project to formalize realistic dynamical requirements, but a method for representing these requirements and a framework for using them as a design aid must be defined to ease appropriation by engineers. This report therefore presents a draft of a new graphical method for representing system requirements and evaluating different architectures, extending classical Systems Engineering (SE) approaches to the complexity of dynamic physical systems. Once the method completed, the verification and validation process can then be carried out through the simulation of solution models with CRML models. Solution models state how the system will behave while the CRML models state whether this behaviour is compliant with the requirements. The new graphical approach has been applied to two energy systems found in the nuclear industry. This report presents the first promising results as well as some perspectives to consolidate it.</p><p> </p>

corrected abstract:
<p>As cyber-physical systems become increasingly complex, the management and verification of requirements during design is essential. A new language called CRML (Common Requirement Modelling Language) has been created during the European EMBrACE project to formalize realistic dynamical requirements, but a method for representing these requirements and a framework for using them as a design aid must be defined to ease appropriation by engineers. This report therefore presents a draft of a new graphical method for representing system requirements and evaluating different architectures, extending classical Systems Engineering (SE) approaches to the complexity of dynamic physical systems. Once the method completed, the verification and validation process can then be carried out through the simulation of solution models with CRML models. Solution models state how the system will behave while the CRML models state whether this behaviour is compliant with the requirements. The new graphical approach has been applied to two energy systems found in the nuclear industry. This report presents the first promising results as well as some perspectives to consolidate it.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:558590 
abstract is: 
<p>It is of great importance to find an analytical copula that will represent the empirical lower tail dependence. In this study, the pairwise empirical copula are estimated using data of the S&amp;P 500 stocks during the period 2007-2010.Different optimization methods and measures of dependence have been used to fit Gaussian, t and Clayton copula to the empirical copulas, in order to represent the empirical lower tail dependence. These different measures of dependence and optimization methods with their restrictions, point at different analytical copulas being optimal. In this study the t copula with 5 degrees of freedom is giving the most fulfilling result, when it comes to representing lower tail dependence. The t copula with 5 degrees of freedom gives the best representation of empirical lower tail dependence, whether one uses the 'Empirical maximum likelihood estimator', or 'Equal Ƭ' as an approach.</p><p> </p>

corrected abstract:
<p>It is of great importance to find an analytical copula that will represent the empirical lower tail dependence. In this study, the pairwise empirical copula are estimated using data of the S&amp;P 500 stocks during the period 2007-2010. Different optimization methods and measures of dependence have been used to fit Gaussian, 𝑡 and Clayton copula to the empirical copulas, in order to represent the empirical lower tail dependence. These different measures of dependence and optimization methods with their restrictions, point at different analytical copulas being optimal. In this study the 𝑡 copula with 5 degrees of freedom is giving the most fulfilling result, when it comes to representing lower tail dependence. The 𝑡 copula with 5 degrees of freedom gives the best representation of empirical lower tail dependence, whether one uses the ’Empirical maximum likelihood estimator’, or ’Equal &#x1D70F;’ as an approach.</p>

Note - removed the empty paragraph, fixed quote marks to match original, and fixed the math symbols
----------------------------------------------------------------------
In diva2:813868 
abstract is: 
<p>Feasibly computable analytic solutions for systems of many particles in fluid dynamics and electrostatics are few and far-between. Simulations and numerical approximations are essential to studying these systems. This is commonly done without directly calculating the interacting field between particles. In this report a method utilizing the spectral accuracy of the Fourier transform is studied to calculate particle velocities via the surrounding fluid velocity field. The method is applied to a periodic cube of a suspension of small, spherical particles sedimenting in a fluid affected by gravity, in an attempt to mimic the behaviour of a similar infinite system. Results for a few particles qualitatively relate the shape of the solution to the choice of interpolation between particles to grid and quantitatively maps some convergence properties of a certain class of interpolating functions, cardinal B-splines. The properties of the method on the periodic system are also examined and compared to a similar study of the infinite system for many, ~1000, particles.</p><p> </p>

corrected abstract:
<p>Feasibly computable analytic solutions for systems of many particles in fluid dynamics and electrostatics are few and far-between. Simulations and numerical approximations are essential to studying these systems. This is commonly done without directly calculating the interacting field between particles. In this report a method utilizing the spectral accuracy of the Fourier transform is studied to calculate particle velocities via the surrounding fluid velocity field. The method is applied to a periodic cube of a suspension of small, spherical particles sedimenting in a fluid affected by gravity, in an attempt to mimic the behaviour of a similar infinite system. Results for a few particles qualitatively relate the shape of the solution to the choice of interpolation between particles to grid and quantitatively maps some convergence properties of a certain class of interpolating functions, cardinal B-splines. The properties of the method on the periodic system are also examined and compared to a similar study of the infinite system for many, ∼ 1000, particles.</p>
----------------------------------------------------------------------
In diva2:1572684 
abstract is: 
<p>Aortic diseases are a common and fatal health issue, regarding various conditions targeting the aorta. The arterial wall consists of two proteins; collagen and elastin (fibers), and they will contribute to the elasticity and strength of the wall. In recent times there has been a growing interest in studying the microstructure of the aortic tissues because it is believed that changes in the amount and/or the construction of the fibers will result in mechanical as well as functional changes that are associated with these heart conditions. Therefore, a better understanding of collagen and its load-carrying properties in vascular tissue is needed for others to be able to develop new designs of cardiovascular medical devices that may help the medical field to find other therapies and treatments for patients with aortic diseases. </p><p>This bachelor's degree thesis is based on a literature study, experimental testing, finite element method (FEM) analysis, and a microscopy study. To get a broader understanding of the arterial wall, the collagen, and its mechanical properties a literature study was conducted. The experimental testing was made with tensile testing equipment called CellScales BioTester 5000 uniaxial bio-testings and the test specimen used was porcine aorta from a local abattoir. The data obtained from the testing was put in MATLAB to produce graphs visualizing different data, such as; stresses, strain, forces and stiffness. Then a FEM-simulation was made by Christopher Miller and the data and images obtained from the analysis were used to compare with the results from MATLAB. Furthermore, the ruptured test specimen after the tensile testing was sent to Karolinska Institutionen (KI) for a microscope study.</p><p>The results from MATLAB were used to receive information regarding the material properties, to calculate the stiffness as well as the strain at the rupture. There were two samples made, sample 7 and sample 10, and the data from the MATLAB graphs were used to determine where the rupture occurred. For sample 7 this occurs when the force is 7.29 [N], elongation is 22.93 x 10^(-3) [m] and stress is 1210 [kPa], for sample 10 the rupture of sample 10 occurred when the force is 6.65 [N], elongation is 17.4 x 10^(-3) [m] and stress is 606 [kPa]. The FEM-simulation showed where the maximum deformation takes place, which was in the middle of our tissues. From the microscope images from KI, the accuracy of the FEM-model could be seen.</p><p> </p>

corrected abstract:
<p>A common and fatal health issue in the world is aortic diseases, which are various conditions targeting the aorta. To get a better understanding of these types of diseases there has been a growing interest in studying the microstructure and mechanical behavior of the aortic tissue, because it is believed that changes in the amount and/or the construction of the fibers (collagen and elastin) will result in mechanical as well as functional changes that can be associated with these types of different vascular conditions.  Therefore, a better understanding of collagen and its load-carrying properties in vascular tissues is needed for others to be able to develop new designs of cardiovascular medical devices that will proceed to help the medical field to find other therapies and treatments for patients with aortic diseases.</p><p>This bachelor’s degree thesis explores the role of collagen in the vessel wall by a literature study, experimental testing, Finite Element Method (FEM) as well as a microscopy study. A literature study was conducted to get a broader understanding of the arterial wall, collagen, and its mechanical properties.  The experimental testing was performed with tensile testing equipment called CellScale BioTester 5000 and the test specimen used was a porcine aorta bought from a local abattoir. The data obtained from the tensile testing was put into MATLAB for post-processing to receive graphs that visualized different data, such as; stresses, strain, forces, and stiffness. Then a FEM-simulation was executed and the data and images obtained from the analysis were used to compare with the results from the post-processing. Furthermore, the ruptured test specimen from the tensile testing was sent to Karolinska Institutionen (KI) for a microscopy study.</p><p>The results from the post-processing were used to receive information regarding the material properties, to calculate the stiffness as well as the strain at the rupture. The data from the post-processing graphs for Sample 10 was used to determine where the rupture occurred which was when the force was 6.65 [N], the elongation 17.4 [mm] and stress 606 [kPa]. The stiffness in the tissue sample showed a decrease with the ramps, cycles, and stretch. This could be explained through that the tissue’s resistance towards elastic deformation decreases.</p><p>The FEM-simulation (like the tensile test and the microscopic images) show that the biggest and considerable deformation takes place at the smallest diameter. The FEM-graphs like the physical tests exhibit a relaxation even without an increased displacement. The graphs also exhibit that the FEManalysis relates to the physical tests and with the aid of the FEM-analysis it is possible to see where the prominent stresses are located in the tissue and where the accuracy of the FEM-model is confirmed by the microscopic pictures. From the microscope images from KI, the rupture of collagen could be visualized.</p><p>The post-processing results of the stiffness and stress-strain curve, the FEManalysis and the microscope observations were compared and show a clear qualitative agreement, which indicates that the method is suitable for future development.</p>

Note major differences in wording between DiVA and original - the above is based on the original
also removed the empty paragraph
----------------------------------------------------------------------
In diva2:1165816 
abstract is: 
<p>In 1983 Grothendieck wrote a letter to Faltings, [Gro83], outlining what is today known as the anabelian conjectures. These conjectures concern the possibility to reconstruct curves and schemes from their étale fundamental group. Although Faltings never replied to the letter, his student Mochizuki began working on it. A major achievement by Mochizuki and Tamagawa was to prove several important versions of these conjectures.</p><p>In this thesis we will first introduce Grothendieck’s Galois theory with the aim to define the étale fundamental group and formulate Mochizuki’s result. After recalling some necessary homotopy theory, we will introduce the étale homotopy type, which is an extension of the étale fundamental group developed by Artin, Mazur and Friedlander. This is done in order to describe some recentwork by Schmidt and Stix that improves on the results of Mochizuki and Tamagawa by extending them from étale fundamental groups to étale homotopy types of certain (possibly higher-dimensional) schemes.</p><p> </p>

corrected abstract:
<p>In 1983 Grothendieck wrote a letter to Faltings, [Gro83], outlining what is today known as the anabelian conjectures. These conjectures concern the possibility to reconstruct curves and schemes from their étale fundamental group. Although Faltings never replied to the letter, his student Mochizuki began working on it. A major achievement by Mochizuki and Tamagawa was to prove several important versions of these conjectures.</p><p>In this thesis we will first introduce Grothendieck’s Galois theory with the aim to define the étale fundamental group and formulate Mochizuki’s result. After recalling some necessary homotopy theory, we will introduce the étale homotopy type, which is an extension of the étale fundamental group developed by Artin, Mazur and Friedlander. This is done in order to describe some recent work by Schmidt and Stix that improves on the results of Mochizuki and Tamagawa by extending them from étale fundamental groups to étale homotopy types of certain (possibly higher-dimensional) schemes.</p>
----------------------------------------------------------------------
In diva2:1880323 
abstract is: 
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p><p> </p>

corrected abstract:
<p>This thesis introduces the emerging field of quantum computing, emphasizing its capability to surpass traditional computing by solving complex problems that are beyond the reach of classical computers. Unlike classical systems that operate with bits and logic gates, quantum computing utilizes qubits and quantum gates, exploiting the vast computational space offered by quantum mechanics. A focal point of this study is topological quantum computing, a novel approach designed to overcome the inherent vulnerability of quantum systems to errors, such as decoherence and operational inaccuracies. At the heart of this method lies the use of non-Abelian anyons, with a particular focus on Fibonacci anyons, whose unique topological characteristics and braiding operations present a viable path to fault-tolerant quantum computation. This thesis aims to elucidate how the braiding of Fibonacci anyons can be employed to construct the necessary quantum gates for topological quantum computing. By offering a foundational exploration of quantum computing principles, especially topological quantum computing, and detailing the process for creating quantum gates through braiding of Fibonacci anyons, the work sets the stage for further research and development in this transformative computing paradigm.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1571248 
abstract is: 
<p>This thesis aims to investigate whether a fiber Bragg grating array (FBGA) can be designed to reduce the bandwidth of light from a 1.5 µm fiber laser while the distances between the fiber Bragg gratings are in the 10-20 cm range. The results from simulations show that for FBGA with multiple cavities, small changes in cavity length impact transmission drastically. For FBGA consisting of only two fiber Bragg gratings, the quality of the laser cavity's spectrum is improved, as the peak width is reduced and stability is increased. The conclusions of the experiments and the simulations are that FBGA with a single cavity gives best performance. For future studies, if higher computational power and production capabilities are available, an in depth analysis of a multi cavity FBGA could be conducted.</p><p> </p>

corrected abstract:
<p>This thesis aims to investigate whether a fiber Bragg grating array (FBGA) can be designed to reduce the bandwidth of light from a 1.5 µm fiber laser while the distances between the fiber Bragg gratings are in the 10-20 cm range. The results from simulations show that for FBGA with multiple cavities, small changes in cavity length impact transmission drastically. For FBGA consisting of only two fiber Bragg gratings, the quality of the laser cavity's spectrum is improved, as the peak width is reduced and stability is increased. The conclusions of the experiments and the simulations are that FBGA with a single cavity gives best performance. For future studies, if higher computational power and production capabilities are available, an in depth analysis of a multi cavity FBGA could be conducted.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1779454 
abstract is: 
<p>The dopamine receptor is one of the main therapeutic targets for neurological disorders, such as Parkinson’s disease and schizophrenia. Although dopamine and adrenaline are structurally similar and both bind to the D2 dopamine receptor, they activate the receptor differently. This study aims to investigate the mechanism by which dopamine and adrenaline trigger conformational changes resulting in the receptor activation. To achieve this, two data sets derived from molecular dynamic (MD) simulations were analyzed, in which one integrates D2 ensembles with and without ligand bound and the other includes D2 ensembles in the presence of the dopamine and adrenaline bound. Both supervised and unsupervised machine learning methods were applied to interpret the high dimensional data provided by the MD simulations. The results obtained from these methods provided the important residues in the D2 dopamine receptor, suggesting they constitute different conformations and interactions with surrounding residues under various conditions. The NPxxY motif in the bottom of transmembrane helix 7 was identified as the most important to distinguish different states induced by distinct ligands binding, according to the supervised methods. Meanwhile, the unsupervised methods showed a general trend of high importance around certain sections, such as the intracellular portion of the transmembrane helix number 6. Taken together, our findings lay the groundwork for the understanding of molecular activation mechanism of D2 dopamine receptor modulated by different ligands.</p><p> </p>

corrected abstract:
<p>The dopamine receptor is one of the main therapeutic targets for neurological disorders, such as Parkinson’s disease and schizophrenia. Although dopamine and adrenaline are structurally similar and both bind to the D<sub>2</sub> dopamine receptor, they activate the receptor differently. This study aims to investigate the mechanism by which dopamine and adrenaline trigger conformational changes resulting in the receptor activation. To achieve this, two data sets derived from molecular dynamic (MD) simulations were analyzed, in which one integrates D<sub>2</sub> ensembles with and without ligand bound and the other includes D<sub>2</sub> ensembles in the presence of the dopamine and adrenaline bound. Both supervised and unsupervised machine learning methods were applied to interpret the high dimensional data provided by the MD simulations. The results obtained from these methods provided the important residues in the D<sub>2</sub> dopamine receptor, suggesting they constitute different conformations and interactions with surrounding residues under various conditions. The NPxxY motif in the bottom of transmembrane helix 7 was identified as the most important to distinguish different states induced by distinct ligands binding, according to the supervised methods. Meanwhile, the unsupervised methods showed a general trend of high importance around certain sections, such as the intracellular portion of the transmembrane helix number 6. Taken together, our findings lay the groundwork for the understanding of molecular activation mechanism of D<sub>2</sub> dopamine receptor modulated by different ligands.</p>

Note - removed the empty paragraph and fixed subscripts
----------------------------------------------------------------------
In diva2:1880199 
abstract is: 
<p>This work explores the unsolvability of the general quintic equation through the lens of Galois theory. We begin by providing a historical perspective on the problem. This starts with the solution of the general cubic equation derived by Italian mathematicians. We then move on to Lagrange's insights on the importance of studying the permutations of roots. Finally, we discuss the critical contributions of Évariste Galois, who connected the solvability of polynomials to the properties of permutation groups. Central to our thesis is the introduction and motivation of key concepts such as fields, solvable groups, Galois groups, Galois extensions, and radical extensions.</p><p>We rigorously develop the theory that connects the solvability of a polynomial to the solvability of its Galois group. After developing this theoretical framework, we go on to show that there exist quintic polynomials with Galois groups that are isomorphic to the symmetric group S5. Given that S5 is not a solvable group, we establish that the general quintic polynomial is not solvable by radicals. Our work aims to provide a comprehensive and intuitive understanding of the deep connections between polynomial equations and abstract algebra.</p><p> </p>

corrected abstract:
<p>This work explores the unsolvability of the general quintic equation through the lens of Galois theory. We begin by providing a historical perspective on the problem. This starts with the solution of the general cubic equation derived by Italian mathematicians. We then move on to Lagrange's insights on the importance of studying the permutations of roots. Finally, we discuss the critical contributions of Évariste Galois, who connected the solvability of polynomials to the properties of permutation groups. Central to our thesis is the introduction and motivation of key concepts such as fields, solvable groups, Galois groups, Galois extensions, and radical extensions.</p><p>We rigorously develop the theory that connects the solvability of a polynomial to the solvability of its Galois group. After developing this theoretical framework, we go on to show that there exist quintic polynomials with Galois groups that are isomorphic to the symmetric group 𝑆<sub>5</sub>. Given that 𝑆<sub>5</sub> is not a solvable group, we establish that the general quintic polynomial is not solvable by radicals. Our work aims to provide a comprehensive and intuitive understanding of the deep connections between polynomial equations and abstract algebra.</p>

Note - removed the empty paragraph, fixed the subscripts, and replaced "S" with "𝑆"
----------------------------------------------------------------------
In diva2:1592993 
abstract is: 
<p>Digitization of the energy industry, introduction of smart grids and increasing regulation of electricity consumption metering have resulted in vast amounts of electricity data. This data presents a unique opportunity to understand the electricity usage and to make it more efficient, reducing electricity consumption and carbon emissions. An important initial step in analyzing the data is to identify anomalies.</p><p>In this thesis the problem of anomaly detection in electricity consumption series is addressed using four machine learning methods: density based spatial clustering for applications with noise (DBSCAN), local outlier factor (LOF), isolation forest (iForest) and one-class support vector machine (OC-SVM). In order to evaluate the methods synthetic anomalies were introduced to the electricity consumption series and the methods were then evaluated for the two anomaly types point anomaly and collective anomaly. In addition to electricity consumption data, features describing the prior consumption, outdoor temperature and date-time properties were included in the models. Results indicate that the addition of the temperature feature and the lag features generally impaired anomaly detection performance, while the inclusion of date-time features improved it. Of the four methods, OC-SVM was found to perform the best at detecting point anomalies, while LOF performed the best at detecting collective anomalies. In an attempt to improve the models' detection power the electricity consumption series were de-trended and de-seasonalized and the same experiments were carried out. The models did not perform better on the decomposed series than on the non-decomposed.</p><p> </p>

corrected abstract:
<p>Digitization of the energy industry, introduction of smart grids and increasing regulation of electricity consumption metering have resulted in vast amounts of electricity data. This data presents a unique opportunity to understand the electricity usage and to make it more efficient, reducing electricity consumption and carbon emissions. An important initial step in analyzing the data is to identify anomalies.</p><p>In this thesis the problem of anomaly detection in electricity consumption series is addressed using four machine learning methods: density based spatial clustering for applications with noise (DBSCAN), local outlier factor (LOF), isolation forest (iForest) and one-class support vector machine (OC-SVM). In order to evaluate the methods synthetic anomalies were introduced to the electricity consumption series and the methods were then evaluated for the two anomaly types <em>point anomaly</em> and <em>collective anomaly</em>. In addition to electricity consumption data, features describing the prior consumption, outdoor temperature and date-time properties were included in the models. Results indicate that the addition of the temperature feature and the lag features generally impaired anomaly detection performance, while the inclusion of date-time features improved it. Of the four methods, OC-SVM was found to perform the best at detecting point anomalies, while LOF performed the best at detecting collective anomalies. In an attempt to improve the models' detection power the electricity consumption series were de-trended and de-seasonalized and the same experiments were carried out. The models did not perform better on the decomposed series than on the non-decomposed.</p>

Note - removed the empty paragraph and added italics as per the original
---------------------------------------------------------------------
In diva2:692474 
abstract is: 
<p> </p><p></p><p>With the increases in fuel costs due to the depletion of the world oil reserves and the increase of greenhouse gasses as a consequence to using oil as a fuel many companies are looking to new and innovative ways to power their aircraft. One of these new ways to power an aircraft is using fuel cells powered using hydrogen and oxygen, thus producing nothing but water vapour and small amounts of nitrogen dioxide as well as trace amounts of other emissions. Both Boeing (1) and Politecnico di Torino (2) have shown that it is possible to build an all-electric aircraft powered by fuel cells. Both flights used small, two-seater aircraft and a constant between them was the loss of the co-pilot seat due to weight and lack of space. As this paper will deal with a commercial aircraft a primary concern is the cargo and passenger capacity and whatever impact switching propulsion system has on these. The aircraft used to test the feasibility of these fuel-cells is the SAAB 340 passenger aircraft/airliner chosen for its twin-engine turboprop configuration and generally conventional design. Its engines and fuel tanks were removed and electrical motors, fuel cells and hydrogen tanks were installed, all the while taking care not to move its centre of gravity too much. Based upon the calculations performed the new aircraft appears to be airworthy though it has a very low rate of climb and because of this an extremely short range. The vehicles passenger and cargo carrying capacity has been severely diminished due to the weight and size of the new components. Other parameters have also decreased, such as speeds and power outputs from the motors. Despite performance reductions the aircraft seem to be able to fulfil the demands placed upon it though carrying capacity appears to be severely diminished.</p>

corrected abstract:
<p>With the increases in fuel costs due to the depletion of the world oil reserves and the increase of greenhouse gasses as a consequence to using oil as a fuel many companies are looking to new and innovative ways to power their aircraft. One of these new ways to power an aircraft is using fuel cells powered using hydrogen and oxygen, thus producing nothing but water vapour and small amounts of nitrogen dioxide as well as trace amounts of other emissions. Both Boeing (1) and Politecnico di Torino (2) have shown that it is possible to build an all-electric aircraft powered by fuel cells. Both flights used small, two-seater aircraft and a constant between them was the loss of the co-pilot seat due to weight and lack of space. As this paper will deal with a commercial aircraft a primary concern is the cargo and passenger capacity and whatever impact switching propulsion system has on these. The aircraft used to test the feasibility of these fuel-cells is the SAAB 340 passenger aircraft/airliner chosen for its twin-engine turboprop configuration and generally conventional design. Its engines and fuel tanks were removed and electrical motors, fuel cells and hydrogen tanks were installed, all the while taking care not to move its centre of gravity too much. Based upon the calculations performed the new aircraft appears to be airworthy though it has a very low rate of climb and because of this an extremely short range. The vehicles passenger and cargo carrying capacity has been severely diminished due to the weight and size of the new components. Other parameters have also decreased, such as speeds and power outputs from the motors. Despite performance reductions the aircraft seem to be able to fulfil the demands placed upon it though carrying capacity appears to be severely diminished.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1431653 
abstract is: 
<p>This thesis tackles the problem of predicting the collision risk for vehicles driving in complex traffic scenes for a few seconds into the future. The method is based on previous research using dynamic Bayesian networks to represent the state of the system.</p><p>Common risk prediction methods are often categorized into three different groups depending on their abstraction level. The most complex of these are interaction-aware models which take driver interactions into account. These models often suffer from high computational complexity which is a key limitation in practical use. The model studied in this work takes interactions between drivers into account by considering driver intentions and the traffic rules in the scene.</p><p>The state of the traffic scene used in the model contains the physical state of vehicles, the intentions of drivers and the expected behaviour of drivers according to the traffic rules. To allow for real-time risk assessment, an approximate inference of the state given the noisy sensor measurements is done using sequential importance resampling. Two different measures of risk are studied. The first is based on driver intentions not matching the expected maneuver, which in turn could lead to a dangerous situation. The second measure is based on a trajectory prediction step and uses the two measures time to collision (TTC) and time to critical collision probability (TTCCP).</p><p>The implemented model can be applied in complex traffic scenarios with numerous participants. In this work, we focus on intersection and roundabout scenarios. The model is tested on simulated and real data from these scenarios. %Simulations of these scenarios is used to test the model. In these qualitative tests, the model was able to correctly identify collisions a few seconds before they occur and is also able to avoid false positives by detecting the vehicles that will give way.</p><p> </p>

corrected abstract:
<p>This thesis tackles the problem of predicting the collision risk for vehicles driving in complex traffic scenes for a few seconds into the future. The method is based on previous research using dynamic Bayesian networks to represent the state of the system.</p><p>Common risk prediction methods are often categorized into three different groups depending on their abstraction level. The most complex of these are interaction-aware models which take driver interactions into account. These models often suffer from high computational complexity which is a key limitation in practical use. The model studied in this work takes interactions between drivers into account by considering driver intentions and the traffic rules in the scene.</p><p>The state of the traffic scene used in the model contains the physical state of vehicles, the intentions of drivers and the expected behaviour of drivers according to the traffic rules. To allow for real-time risk assessment, an approximate inference of the state given the noisy sensor measurements is done using sequential importance resampling. Two different measures of risk are studied. The first is based on driver intentions not matching the expected maneuver, which in turn could lead to a dangerous situation. The second measure is based on a trajectory prediction step and uses the two measures <em>time to collision</em> (TTC) and <em>time to critical collision probability</em> (TTCCP).</p><p>The implemented model can be applied in complex traffic scenarios with numerous participants. In this work, we focus on intersection and roundabout scenarios. The model is tested on simulated and real data from these scenarios. In these qualitative tests, the model was able to correctly identify collisions a few seconds before they occur and is also able to avoid false positives by detecting the vehicles that will give way.</p>

Note - removed the empty paragraph, added itliacs as per the original, and removed "%Simulations of these scenarios is used to test the model." - a sentence that was commented out in the LaTeX.
----------------------------------------------------------------------
In diva2:1218276 
abstract is: 
<p> </p><p>In this thesis, image classification of difficult data through the use of machine learning algorithms is evaluated using a Kernel Machine. When trying to classify objects in im- ages in real world situations the object in question might be behind some form of transparent obstruction or barrier. By implementing a simple machine learning algorithm this thesis aims to provide an approximate lower bound for the performance of machine learning algorithms in such circumstances.</p><p>Results show that machine learning is a viable option even though performance decrease with barrier complexity and thickness. In the best case performance dropped less than one percentage point when using a simple barrier compared to using no barrier and allowing the algorithm to train on images with objects behind said barrier. Performance is much worse when not allowing the algorithm to train on images with barriers. Furthermore, performance seems to be largely independent on image size despite the loss of information associated with introducing barriers.</p>

corrected abstract:
<p>In this thesis, image classification of difficult data through the use of machine learning algorithms is evaluated using a Kernel Machine. When trying to classify objects in images in real world situations the object in question might be behind some form of transparent obstruction or barrier. By implementing a simple machine learning algorithm this thesis aims to provide an approximate lower bound for the performance of machine learning algorithms in such circumstances.</p><p>Results show that machine learning is a viable option even though performance decrease with barrier complexity and thickness. In the best case performance dropped less than one percentage point when using a simple barrier compared to using no barrier and allowing the algorithm to train on images with objects behind said barrier. Performance is much worse when not allowing the algorithm to train on images with barriers. Furthermore, performance seems to be largely independent on image size despite the loss of information associated with introducing barriers.</p>

Note - only change to remove the empty paragraph
----------------------------------------------------------------------
In diva2:1450330 
abstract is: 
<p>The purpose of this report was to investigate whether the weight of Marsblade's new steel runner could be reduced without making the blade too weak and still retain its properties. Marsblade introduces a unique concept with a moving steel runner in the holder which creates many challenges for the geometry as it has to cope with stresses and at the same time fulfill it's unique function. However, the unique geometry contributes to increased weight and a weight reduction are therefore desirable, in order to make the blade more competitive with other brands on the market. Marsblade's steel runner has thus been compared to a conventional steel runner which is considered to meet the requirements. When analyzing the steel runners, FEA analyses have been performed to investigate their strength properties. A carefully made trade off between strength and reduced weight resulted in the new weight optimized steel runner weighs 13 grams less than the original one, this weight loss means that the weight has decreased by 8.8%. The new geometry is optimized in such a way that the occurrence of stress concentrations at dimensional transitions has decreased.</p><p> </p>

corrected abstract:
<p>The purpose of this report was to investigate whether the weight of Marsblade's new steel runner could be reduced without making the blade too weak and still retain its properties. Marsblade introduces a unique concept with a moving steel runner in the holder which creates many challenges for the geometry as it has to cope with stresses and at the same time fulfill it's unique function. However, the unique geometry contributes to increased weight and a weight reduction are therefore desirable, in order to make the blade more competitive with other brands on the market. Marsblade's steel runner has thus been compared to a conventional steel runner which is considered to meet the requirements. When analyzing the steel runners, FEA analyses have been performed to investigate their strength properties. A carefully made trade off between strength and reduced weight resulted in the new weight optimized steel runner weighs 13 grams less than the original one, this weight loss means that the weight has decreased by 8.8%. The new geometry is optimized in such a way that the occurrence of stress concentrations at dimensional transitions has decreased.</p>
----------------------------------------------------------------------
In diva2:1832663 
abstract is: 
<p>This study aims to investigate the relationship between multiple air pollution and different vehicle variables, such as vehicle year, fuel type and vehicle type, on Hornsgatan in Stockholm. The study intends to answer which factors have the greatest impact on air quality. The implementation is based on the two machine learning algorithms Random Forest and Support Vector Regression, which are compared based on R^2 and RMSE. The models created with Random Forest outperform Support Vector Regression for the various air pollutants. The best performing model was the carbon monoxide model which had an R^2-value of 99.7%. The model that gave predictions with the lowest R^2-value, 68.4%, was the model for nitrogen dioxide. Overall, the results were good in relation to previous studies. With regards to these models, the impact of variables and different measures that can be introduced in the City of Stockholm and on Hornsgatan to improve air quality are discussed.</p>

corrected abstract:
<p>This study aims to investigate the relationship between multiple air pollution and different vehicle variables, such as vehicle year, fuel type and vehicle type, on Hornsgatan in Stockholm. The study intends to answer which factors have the greatest impact on air quality. The implementation is based on the two machine learning algorithms Random Forest and Support Vector Regression, which are compared based on R<sup>2</sup> and RMSE. The models created with Random Forest outperform Support Vector Regression for the various air pollutants. The best performing model was the carbon monoxide model which had an R<sup>2</sup>-value of 99.7%. The model that gave predictions with the lowest R<sup>2</sup>-value, 68.4%, was the model for nitrogen dioxide. Overall, the results were good in relation to previous studies. With regards to these models, the impact of variables and different measures that can be introduced in the City of Stockholm and on Hornsgatan to improve air quality are discussed.</p>

Note - removed the empty paragraph and fixed superscripts
----------------------------------------------------------------------
In diva2:1788396 
abstract is: 
<p>It is important to understand the aerodynamic properties of tensioned cables (e.g. used in suspension bridges and yacht riggings), both for drag reduction and vibrational suppression purposes. In this study, the cross-sectional shape and surface structure of solid cables were investigated in order to improve the performance of sailing racing yachts. The apparent wind angle range 15-60° was identified as the most important for drag reduction. Thereafter, the aerodynamic properties of different shapes and surfaces were investigated in the Reynolds number range 5 x 10^3 ≤ Re ≤ 4 x 10^4, by performing computational fluid dynamics simulations and wind tunnel tests (the aerodynamic forces were measured using load cells). No significant effect of changing the surface roughness could be found for the investigated Reynolds number range. The results were compared to literature values for validation.</p><p>Elliptical shapes with a fineness ratio between 1:1-3:1, together with three complex shapes, were tested. It could be shown that the largest performance gain was obtained for cables with more sail-like aerodynamic properties (for apparent wind angles below 90° a large lift/drag ratio is sought). This study was performed in collaboration with Carbo-Link AG, as an outlook, the manufacturability of carbon fiber reinforced polymer cables in the most aerodynamically efficient shape was explored.</p>

corrected abstract:
<p>It is important to understand the aerodynamic properties of tensioned cables (e.g. used in suspension bridges and yacht riggings), both for drag reduction and vibrational suppression purposes. In this study, the cross-sectional shape and surface structure of solid cables were investigated in order to improve the performance of sailing racing yachts. The apparent wind angle range 15-60° was identified as the most important for drag reduction. Thereafter, the aerodynamic properties of different shapes and surfaces were investigated in the Reynolds number range 5 x 10<sup>3</sup> ≤ 𝑅𝑒 ≤ 4 x 10<<sup>4</sup>, by performing computational fluid dynamics simulations and wind tunnel tests (the aerodynamic forces were measured using load cells). No significant effect of changing the surface roughness could be found for the investigated Reynolds number range. The results were compared to literature values for validation.</p><p>Elliptical shapes with a fineness ratio between 1:1-3:1, together with three complex shapes, were tested. It could be shown that the largest performance gain was obtained for cables with more sail-like aerodynamic properties (for apparent wind angles below 90° a large lift/drag ratio is sought). This study was performed in collaboration with Carbo-Link AG, as an outlook, the manufacturability of carbon fiber reinforced polymer cables in the most aerodynamically efficient shape was explored.</p>
----------------------------------------------------------------------
In diva2:1282825 
abstract is: 
<p>In this thesis, we study the non-symmetric Macdonald polynomials E_λ (x;q,t) at t=0 from a combinatorial point of view, using the combinatorial formula found by J. Haglund, M. Haiman, and N. Loehr. Our primary focus is when λ is a partition. We summarize the known theory about this specialization and prove some new results related to this combinatorial formula. We also define the cyclic sieving phenomenon (CSP). For rectangular λ, we present an instance of cyclic sieving with E_λ (1,q,q^2,...,q^(k-1);1,0) as CSP-polynomial. We also conjecture another instance of CSP with E_λ (1,1,...,1;q,0) as CSP-polynomial. This conjecture generalizes a previously known CSP-triple. Furthermore, we prove this conjecture in the case when is λ an m×2 diagram.</p>

corrected abstract:
<p>In this thesis, we study the non-symmetric Macdonald polynomials E<sub>λ</sub>(𝑥; 𝑞, 𝑡) at 𝑡=0 from a combinatorial point of view, using the combinatorial formula found by J. Haglund, M. Haiman, and N. Loehr. Our primary focus is when λ is a partition. We summarize the known theory about this specialization and prove some new results related to this combinatorial formula. We also define the cyclic sieving phenomenon (CSP). For rectangular λ, we present an instance of cyclic sieving with E<sub>λ</sub>(1,𝑞,𝑞<sup>2</sup>,...,𝑞<sup>𝑘-1</sup>; 1,0) as CSP-polynomial. We also conjecture another instance of CSP with E<sub>λ</sub> (1,1,...,1; 𝑞, 0) as CSP-polynomial. This conjecture generalizes a previously known CSP-triple. Furthermore, we prove this conjecture in the case when is λ an 𝑚 × 2 diagram.</p>
----------------------------------------------------------------------
In diva2:1738124 
abstract is: 
<p>The treatment of spatial infinity is one of the remaining major open problems in the theory of isolated self-gravitating systems. Especially when one wants to model scattering of gravitational radiation in spacetime. In this thesis the conformal theory is used to study simple electromagnetic fields, close to spatial infinity. In particular, the trajectory of the moving Coulomb field is studied in compactified Minkowski space. In the formalism, introduced by Penrose, Minkowski metric is rescaled g = Ω^2η to Einstein’s universe, R × S^3. A dual particle, passing through spatial infinity in Einstein’s Universe, emerges from the conformally extended Coulomb field. The particle pair moves antipodally with respect to the retarded and advanced directions. Furthermore, a more recent treatment of spatial infinity, proposed by Friedrich, is studied in conjunction with the electromagnetic field. In this treatment, spatial infinity is blown-up to a cylinder that is a total characteristic of the spacetime. The Newman-Penrose formalism is central to the theory and is used here to rewrite Maxwell’s equations. The blow-up is linked to the sigma-process, a process used to treat singularities in the theory of differential equations. Boosted space-like curves are linked to points on the cylinder via a bijective function. The Newman-Penrose scalars are studied on the cylinder. Finally, a global treatment of spacetime, using global coordinates for adS2 ×S^2, is proposed for further study of spatial infinity in e.g. numerical codes and Newman-Penrose formalism.</p>

corrected abstract:
<p>The treatment of spatial infinity is one of the remaining major open problems in the theory of isolated self-gravitating systems. Especially when one wants to model scattering of gravitational radiation in spacetime. In this thesis the conformal theory is used to study simple electromagnetic fields, close to spatial infinity. In particular, the trajectory of the moving Coulomb field is studied in compactified Minkowski space. In the formalism, introduced by Penrose, Minkowski metric is rescaled 𝑔 = Ω<sup>2</sup>η to Einstein’s Universe, ℝ × 𝑆<sup>3</sup>. A dual particle, passing through spatial infinity in Einstein’s Universe, emerges from the conformally extended Coulomb field. The particle pair moves antipodally with respect to the retarded and advanced directions. Furthermore, a more recent treatment of spatial infinity, proposed by Friedrich, is studied in conjunction with the electromagnetic field. In this treatment, spatial infinity is blown-up to a cylinder that is a total characteristic of the spacetime. The Newman-Penrose formalism is central to the theory and is used here to rewrite Maxwell’s equations. The blow-up is linked to the sigma-process, a process used to treat singularities in the theory of differential equations. Boosted space-like curves are linked to points on the cylinder via a bijective function. The Newman-Penrose scalars are studied on the cylinder. Finally, a global treatment of spacetime, using global coordinates for adS<sub>2</sub> ×𝑆<sup>2</sup>, is proposed for further study of spatial infinity in e.g. numerical codes and Newman-Penrose formalism.</p>
----------------------------------------------------------------------
In diva2:1341293 
abstract is: 
<p>This report of a conceptual design of an electric driven aircraft was driven with the goal of making the flying sector more environmentally viable. The designated mission was chosen freely for the airplane. The result became a short distance plane with a range of 500 km, seating for eight passengers, primarily aimed towards companies. It was decided that the plane would have its cruising altitude at 4500 m, with a cruising speed of 280 km/h and have a short takeoff and landing distance. The airplane would be able to climb with a vertical speed of 6.67 m/s and have a stall speed of 150 km/h. From the specifications and the assumptions regarding different variables, the fuel weight and total weight was decided to be 2654 kg and 8294 kg respectively. The range of a corresponding aircraft driven with fossil fuels were calculated to be 2070 km. A constraint diagram was then constructed based on five chosen requirements. From this diagram the least power to weight and the highest possible wingloading was determined. A point slightly higher than the least required power to weight was chosen, leading to the engines needing to produce 1900 hp at takeoff. The wing area could be calculated from the decided wingloading and it ended up at 44 m^2 with a wingspan of 23.8 m because of a previously chosen aspect ratio. The length of the fuselage was calculated to be 16.2 m and its effective diameter 2.03 m. Finally an initial layout could be developed where a relationship between the wing and horizontal stabilizer was calculated and the center of gravity for the airplane was placed. The final airplane has a longer wingspan, is heavier and has a shorter range than what similar aircraft that are not driven with electricity have. It can be seen that the cruise speed can be increased above the 280 km/h in the constraint diagram, but the specified requirements were met, which was the main priority.</p>

corrected abstract:
<p>This report of a conceptual design of an electric driven aircraft was driven with the goal of making the flying sector more environmentally viable. The designated mission was chosen freely for the airplane. The result became a short distance plane with a range of 500 km, seating for eight passengers, primarily aimed towards companies. It was decided that the plane would have its cruising altitude at 4500 m, with a cruising speed of 280 km/h and have a short takeoff and landing distance. The airplane would be able to climb with a vertical speed of 6.67 m/s and have a stall speed of 150 km/h.</p><p>From the specifications and the assumptions regarding different variables, the fuel weight and total weight was decided to be 2654 kg and 8294 kg respectively. The range of a corresponding aircraft driven with fossil fuels were calculated to be 2070 km. A constraint diagram was then constructed based on five chosen requirements. From this diagram the least power to weight and the highest possible wingloading was determined. A point slightly higher than the least required power to weight was chosen, leading to the engines needing to produce 1900 hp at takeoff.</p><p>The wing area could be calculated from the decided wingloading and it ended up at 44 m<sup>2</sup> with a wingspan of 23.8 m because of a previously chosen aspect ratio. The length of the fuselage was calculated to be 16.2 m and its effective diameter 2.03 m. Finally an initial layout could be developed where a relationship between the wing and horizontal stabilizer was calculated and the center of gravity for the airplane was placed.</p><p>The final airplane has a longer wingspan, is heavier and has a shorter range than what similar aircraft that are not driven with electricity have. It can be seen that the cruise speed can be increased above the 280 km/h in the constraint diagram, but the specified requirements were met, which was the main priority.</p>
----------------------------------------------------------------------
In diva2:1578887 
abstract is: 
<p>Dryout and Departure from Nucleate boiling (DNB) are utmost thermal-hydraulic concerns for the safety of LWRs. The behavior of two-phase flows at these conditions is still not fully understood. There is at least a need for a good local velocity and void fraction database at these conditions. This database can be exploited by CFD codes, thereby leading to understanding and predicting DNB and boiling crisis. Since these conditions occur in LWR at pressures greater than 70 bar and temperatures above 285 $^oC$, most instrumentations fail at these conditions. So there is a need for developing or optimizing new instruments for this specific objective. This study will look into the application of Hot Wire Anemometry (HWA) for this application. Previous experiments at near saturation conditions were studied, the hurdles of application of HWA in the HWAT loop at KTH were also investigated. Finally, the deposition of thin film on the HWA sensors for protection was studied.</p>

corrected abstract:
<p>Dryout and Departure from Nucleate boiling (DNB) are utmost thermal-hydraulic concerns for the safety of LWRs. The behavior of two-phase flows at these conditions is still not fully understood. There is at least a need for a good local velocity and void fraction database at these conditions. This database can be exploited by CFD codes, thereby leading to understanding and predicting DNB and boiling crisis. Since these conditions occur in LWR at pressures greater than 70 bar and temperatures above 285ºC, most instrumentations fail at these conditions. So there is a need for developing or optimizing new instruments for this specific objective. This study will look into the application of Hot Wire Anemometry (HWA) for this application. Previous experiments at near saturation conditions were studied, the hurdles of application of HWA in the HWAT loop at KTH were also investigated. Finally, the deposition of thin film on the HWA sensors for protection was studied.</p>
----------------------------------------------------------------------
In diva2:1780561 
abstract is: 
<p>In this report, a Fourier spectral approximation of the solution to the linear convection--diffusion equation for initial conditions of different smoothness, and for Burger's equation for the initial condition f(x) = sin(x), was constructed, and implemented. Three different filters (Cesàro, Lanczos, and 4--th order exponential cutoff) were either applied to the initial condition or to the numerical approximation after the last integration step has been performed. The local error was then calculated in order to compare the performance of the three filters. Filtering was found to improve the local accuracy of the numerical approximation for the linear convection--diffusion equation for a diffusivity constant of 0 and for initial conditions of low smoothness, i.e. discontinuous functions or functions in C^0 or C^1. For initial conditions of infinite smoothness and a larger diffusivity constant, filtering did not improve the local accuracy but rather made it worse. No significant difference was found between filtering the initial condition and filtering after the final integration step. The 4--th order exponential cutoff performed best overall for most initial conditions. For Burger's equation, filtering only improved the local accuracy when applied after the final integration step and only if a discontinuity had started to form. For a discontinuity to form, the diffusivity constant furthermore needed to be sufficiently small. In conclusion, filtering is applicable when solving the linear convection--diffusion equation for a low diffusivity constant and initial conditions of low smoothness. For Burger's equation, filtering is applicable after a discontinuity starts to form. These results were in line with the theory presented in the report.</p>

corrected abstract:
<p>In this report, a Fourier spectral approximation of the solution to the linear convection-diffusion equation for initial conditions of different smoothness, and for Burger's equation for the initial condition 𝑓(𝑥) = sin(𝑥), was constructed, and implemented. Three different filters (Cesàro, Lanczos, and 4-th order exponential cutoff) were either applied to the initial condition or to the numerical approximation after the last integration step has been performed. The local error was then calculated in order to compare the performance of the three filters. Filtering was found to improve the local accuracy of the numerical approximation for the linear convection-diffusion equation for a diffusivity constant of 0 and for initial conditions of low smoothness, i.e. discontinuous functions or functions ∈ 𝐶<sup>0</sup> or 𝐶<sup>1</sup>. For initial conditions of infinite smoothness and a larger diffusivity constant, filtering did not improve the local accuracy but rather made it worse. No significant difference was found between filtering the initial condition and filtering after the final integration step. The 4-th order exponential cutoff performed best overall for most initial conditions. For Burger's equation, filtering only improved the local accuracy when applied after the final integration step and only if a discontinuity had started to form. For a discontinuity to form, the diffusivity constant furthermore needed to be sufficiently small. In conclusion, filtering is applicable when solving the linear convection-diffusion equation for a low diffusivity constant and initial conditions of low smoothness. For Burger's equation, filtering is applicable after a discontinuity starts to form. These results were in line with the theory presented in the report.</p>
----------------------------------------------------------------------
In diva2:1579559 
abstract is: 
<p>While there are existing methods of gamma ray-track reconstruction in specialized detectors such as AGATA, including backtracking and clustering, it is naturally of interest to diversify the portfolio of available tools to provide us viable alternatives. In this study some possibilities found in the field of machine learning were investigated, more specifically within the field of graph neural networks.</p><p>In this project there was attempt to reconstruct gamma tracks in a germanium solid using data simulated in Geant4. The data consists of photon energies below the pair production limit and so we are limited to the processes of photoelectric absorption and Compton scattering. The author turned to the field of graph networks to utilize its edge and node structure for data of such variable input size as found in this task. A graph neural network (GNN) was implemented and trained on a variety of gamma multiplicities and energies and was subsequently tested in terms of various accuracy parameters and generated energy spectra.</p><p>In the end the best result involved an edge classifier trained on a large dataset containing a 10^6 tracks bundled together into separate events to be resolved. The network was capable of recalling up to 95 percent of the connective edges for the selected threshold in the infinite resolution case with a peak-to-total ratio of 68 percent for a set of packed data with a model trained on simulated data including realistic uncertainties in both position and energy.</p>

corrected abstract:
<p>While there are existing methods of gamma ray-track reconstruction in specialized detectors such as AGATA, including backtracking and clustering, it is naturally of interest to diversify the portfolio of available tools to provide us viable alternatives. In this study some possibilities found in the field of machine learning were investigated, more specifically within the field of graph neural networks.</p><p>In this project there was attempt to reconstruct gamma tracks in a germanium solid using data simulated in Geant4. The data consists of photon energies below the pair production limit and so we are limited to the processes of photoelectric absorption and Compton scattering. The author turned to the field of graph networks to utilize its edge and node structure for data of such variable input size as found in this task. A graph neural network (GNN) was implemented and trained on a variety of gamma multiplicities and energies and was subsequently tested in terms of various accuracy parameters and generated energy spectra.</p><p>In the end the best result involved an edge classifier trained on a large dataset containing a 10<sup>6</sup> tracks bundled together into separate events to be resolved. The network was capable of recalling up to 95 percent of the connective edges for the selected threshold in the infinite resolution case with a peak-to-total ratio of 68 percent for a set of packed data with a model trained on simulated data including realistic uncertainties in both position and energy.</p>
----------------------------------------------------------------------
In diva2:1568137 
abstract is: 
<p>Supernovae (SNe) are explosions following the death of massive stars. Core-collapse supernovae (CCSNe) occur when the heavy iron core of these stars collapse in on themselves. The resulting remnant of the core of a CCSN is a compact object: either a black hole or a neutron star. During the collapse and following explosion, massive amounts of energy and material are expelled. The compact objects emit high-energy radiation. With X-ray astronomy, we can observe it and study the processes behind these events. In this thesis, we determine a limit on the X-ray luminosity of SN 2002ap, and constrain the parameters for the magnetic field of the central object, potentially a neutron star. We model the absorption of the radiation by the material in the surrounding area, the so-called SN ejecta, as well as the absorption by the interstellar medium (ISM). We construct the model using the spectral fitting program XSPEC. Assumptions about the abundance of X-ray absorbing elements in the ejecta and ISM are based on earlier models and the explosion energy is taken from previous estimations. The mass of the ejecta is assumed to be 2.5-5 solar masses and the distance 9.34 Mpc. We compare the absorption model to the data taken by the Chandra telescope in 2018. From this comparison, we determine the maximum luminosity to be L &lt; 2*10^40 erg/s and constrain the magnetic field to a minimum of B &gt; 3*10^13 G.</p>

corrected abstract:
<p>Supernovae (SNe) are explosions following the death of massive stars. Core-collapse supernovae (CCSNe) occur when the heavy iron core of these stars collapse in on themselves. The resulting remnant of the core of a CCSN is a compact object: either a black hole or a neutron star. During the collapse and following explosion, massive amounts of energy and material are expelled. The compact objects emit high-energy radiation. With X-ray astronomy, we can observe it and study the processes behind these events. In this thesis, we determine a limit on the X-ray luminosity of SN 2002ap, and constrain the parameters for the magnetic field of the central object, potentially a neutron star. We model the absorption of the radiation by the material in the surrounding area, the so-called SN ejecta, as well as the absorption by the interstellar medium (ISM). We construct the model using the spectral fitting program XSPEC. Assumptions about the abundance of X-ray absorbing elements in the ejecta and ISM are based on earlier models and the explosion energy is taken from previous estimations. The mass of the ejecta is assumed to be 2.5-5 M<sub>☉</sub> and the distance 9.34 Mpc. We compare the absorption model to the data taken by the Chandra telescope in 2018. From this comparison, we determine the maximum luminosity to be 𝐿 ≲ 2 × 10<sup>40</sup> erg s<sup>-1</sup> and constrain the magnetic field to a minimum of 𝐵 ≳ 3 × 10<sup>13</sup> G.</p>
----------------------------------------------------------------------
In diva2:1334784 
abstract is: 
<p>Given an integer n, this text explores different ways of finding factors of n, with focus on Shanks’ Class Group Method as described by Henri Cohen in A Course in Algebraic Number Theory, although brief summaries of Pollard’s p - 1 and rho algorithms for factoring an integer are given as well. The class group is introduced as a set of equivalence classes of fractional ideals in imaginary quadratic fields before an isomorphism with a set of equivalence classes of binary quadratic forms of a given discriminant, D, is given. This isomorphism gives rise to an abstract group operation under which elements of order 2, so called ambiguous forms, constitute a factorisation of n. The Class Group Method describes how to find ambiguous elements and factorise n in O(|D|^(1/4)) time, where the main problem lies in finding the exponent of the class group, which is done using Shanks’ Baby-Steps-Giant-Steps method for finite groups. Implementations in Python 3 are found in the appendix.</p>

corrected abstract:
<p>Given an integer 𝑛, this text explores different ways of finding factors of 𝑛, with focus on Shanks’ Class Group Method as described by Henri Cohen in <em>A Course in Algebraic Number Theory</em>, although brief summaries of Pollard’s 𝑝 - 1 and ρ algorithms for factoring an integer are given as well. The class group is introduced as a set of equivalence classes of fractional ideals in imaginary quadratic fields before an isomorphism with a set of equivalence classes of binary quadratic forms of a given discriminant, 𝐷, is given. This isomorphism gives rise to an abstract group operation under which elements of order 2, so called ambiguous forms, constitute a factorisation of 𝑛. The Class Group Method describes how to find ambiguous elements and factorise 𝑛 in &Oscr;(|𝐷|<sup>1/4</sup>) time, where the main problem lies in finding the exponent of the class group, which is done using Shanks’ Baby-Steps-Giant-Steps method for finite groups. Implementations in Python 3 are found in the appendix.</p>
----------------------------------------------------------------------
In diva2:1341556 
abstract is: 
<p>The purpose of this bachelor thesis is to design an electric powered commercial short-range aircraft that is set to take off in 2030 with reasonable technical advancement assumptions made. The aircraft is designed with the ATR 42-500 as inspiration and has therefore similar requirements. The aircraft has a payload of 5070 kg and cruises at 7600 m above sea level. It has a max speed of Mach 0.5 and a stall speed of 41 m/s. Climb rate is 560 m/min, takeoff distance is 1165 m and landing distance is 960 m. The conceptually designed aircraft has a range of 400 km that is approximately the distance London-Amsterdam and is able to carry up to 48 passengers in a two by two seat configuration. Batteries are expected to improve with 30 % during the next ten years resulting in a maximum takeoff weight of 19900 kg, where 3220 kg is battery weight. Fuel powered it has a maximum takeoff weight of 19200 kg and a fuel weight of 2900 kg. The power needed for propulsion was found to be 4.18 MW which would be equally divided over the engines that drive the two propellers. These are positioned one on each wing. The 26 m long aircraft is equipped with an unswept high mounted wing with a wingspan of 29 m and a wing reference area of 75 m^2. The horizontal stabilizer is 12 m^2 and the vertical stabilizer is 11 m^2.</p>

corrected abstract:
<p>The purpose of this bachelor thesis is to design an electric powered commercial short range aircraft that is set to take-off in 2030 with reasonable technical advancement assumptions made.</p><p>The aircraft is designed with the ATR 42-500 as inspiration and has therefore similar requirements. The aircraft has a payload of 5070 kg and cruises at 7600 m above sea level. It has a max speed of Mach 0.5 and a stall speed of 41 m/s. Climb rate is 560 m/min, take-off distance is 1165 m and landing distance is 960 m.</p><p>The conceptually designed aircraft has a range of 400 km that is approximately the distance London-Amsterdam and is able to carry up to 48 passengers in a two by two seat configuration. Batteries are expected to improve with 30 % during the next ten years resulting in a maximum take-off weight of 19900 kg, where 3220 kg is battery weight. Fuel powered it has a maximum take-off weight of 19200 kg and a fuel weight of 2900 kg.</p><p>The power needed for propulsion was found to be 4.18 MW which would be equally divided over the engines that drive the two propellers. These are positioned one on each wing.</p><p>The 26 m long aircraft is equipped with an unswept high mounted wing with a wingspan of 29 m and a wing reference area of 75 m<sup>2</sup>. The horizontal stabilizer is 13 m<sup>2</sup> and the vertical stabilizer is 11 m<sup>2</sup>.</p>
----------------------------------------------------------------------
In diva2:1827769 
abstract is: 
<p>This study investigated the relationship between Sweden’s CO2e (Carbon Dioxide Equivalent) emissions and key macroeconomic factors, for the period 2008Q1- 2022Q3. The aim was to enhance the understanding of the link between macroeconomic factors and greenhouse gas emissions in a post-industrial economy, using multiple regression analysis. The study identified several significant macroeconomic factors affecting CO2e emissions and examined the extent to which these variables explain the fluctuations in Sweden’s emissions. Additionally, the study assessed the validity of the Environmental Kuznets Curve and Porter Hypothesis within Sweden’s environmental context. In the study, two multiple regression models were developed. Model 1 had an R^2 of 0.90, using the macroeconomic variables Industry Fuel Consumption, Population, Net Export, and Oil Prices. However, since the first model displayed moderate autocorrelation, a second model was also built by introducing a lagged dependent variable which yielded an R^2 of 0.92.</p>

corrected abstract:
<p>This study investigated the relationship between Sweden’s CO<sub>2</sub>e (Carbon Dioxide Equivalent) emissions and key macroeconomic factors, for the period 2008Q1- 2022Q3. The aim was to enhance the understanding of the link between macroeconomic factors and greenhouse gas emissions in a post-industrial economy, using multiple regression analysis. The study identified several significant macroeconomic factors affecting CO<sub>2</sub>e emissions and examined the extent to which these variables explain the fluctuations in Sweden’s emissions. Additionally, the study assessed the validity of the Environmental Kuznets Curve and Porter Hypothesis within Sweden’s environmental context. In the study, two multiple regression models were developed. Model 1 had an 𝑅<sup>2</sup> of 0.90, using the macroeconomic variables Industry Fuel Consumption, Population, Net Export, and Oil Prices. However, since the first model displayed moderate autocorrelation, a second model was also built by introducing a lagged dependent variable which yielded an 𝑅<sup>2</sup> of 0.92.</p>
----------------------------------------------------------------------
In diva2:1827787 
abstract is: 
<p>This study aims to identify whether a relationship between ESG performance and financial performance exists for Nordic publicly-listed companies, by conducting a multiple linear regression analysis. Also, it will be observed which (if any) ESG variables are of relevance.</p><p>The regression analysis conducted in this study arrives at the conclusion that there is a relationship between ESG performance and financial performance. However, the models have low explanatory power, with Adjusted R^2 values of 0.36 for the accounting-based financial measure Return on Assets (ROA), and 0.30 for the market-based financial measure Tobin´s Q.In both the ROA and Tobin's Q model, social variables are the most significant. Supplier evaluation disclosure is the only variable that is highly significant and positively correlated to both ROA and Tobin's Q. Consistent with previous literature, our results show that female board participation is positively correlated with ROA. The results also show that ROA correlates negatively with compensation of board members and senior executives being linked to environmental and social factors. In conclusion, some variables were identified that are significant for financial performance. However, the overall explanatory power of the model is low. It is suggested that future studies adopt a materiality approach.</p>

corrected abstract:
<p>This study aims to identify whether a relationship between ESG performance and financial performance exists for Nordic publicly-listed companies, by conducting a multiple linear regression analysis. Also, it will be observed which (if any) ESG variables are of relevance.</p><p>The regression analysis conducted in this study arrives at the conclusion that there is a relationship between ESG performance and financial performance. However, the models have low explanatory power, with Adjusted 𝑅<sup>2</sup> values of 0.36 for the accounting-based financial measure Return on Assets (ROA), and 0.30 for the market-based financial measure Tobin´s Q. In both the ROA and Tobin's Q model, social variables are the most significant. Supplier evaluation disclosure is the only variable that is highly significant and positively correlated to both ROA and Tobin's Q. Consistent with previous literature, our results show that female board participation is positively correlated with ROA. The results also show that ROA correlates negatively with compensation of board members and senior executives being linked to environmental and social factors. In conclusion, some variables were identified that are significant for financial performance. However, the overall explanatory power of the model is low. It is suggested that future studies adopt a materiality approach.</p>
----------------------------------------------------------------------
In diva2:690751 
abstract is: 
<p>A one phase Hele-Shaw flow, described by a domain D(t) (t represents time) in the plane is the flow of a liquid injected at a constant rate in the separation between two narrowly separated parallel planes. This thesis deals with the formulation and proof of existence for a multiple phase Hele-Shaw flow in arbitrary dimension R^n exhibiting separation of the phases. A smooth version of the problem, depending on a small parameter epsilon, has been considered. Solutions to this smooth problem approximate the multiple-phase Hele-Shaw flow. We show that the smooth problem has a solution using a variational technique with functions u=u(t;eps) in the Sobolev space H_0^1 describing the Hele-Shaw flow with D(t)=support(u(t;eps)). As we let the parameter epsilon tend to zero we get that the solutions u(t;eps) converges weakly to a family of functions u(t) in the same Sobolev space which describe the desired Hele-Shaw flow. Furthermore the phases represented by the components of u(t) are separated in the sense that the overlap of any two distinct phases has vanishing n-dimensional Lebesgue measure. </p><p>We also touch upon a formulation of the multiple phase Hele-Shaw flow which would, beyond separation of the phases, provide freezing of the intersecting boundary of two phases. This formulation of the problem tries to incorporate memory in to the system via means of an integration over previous states. </p>

corrected abstract:
<p>A one phase Hele-Shaw flow, described by a domain</p> <p>𝐷<sub>𝑡</sub> = {𝑥 : 𝑢<sub>𝑡</sub>(𝑥) > 0} ∪ 𝐷<sub>0</sub></p> for some functions u<sub>𝑡</sub> : ℝ<sup>n</sup> ⊃ Ω → ℝ and 𝑡 ≥ 0, is the flow of a liquid injected at a constant rate in the separation between two narrowly separated parallel planes.  This thesis deals with the formulation and proof of existence for a multiple phase Hele-Shaw flow exhibiting separation of the phases. A smooth version of the problem, depending on the parameter ε > 0, has been considered giving rise to the equations in 𝐻<sup>−1</sup>(Ω) for the phases 𝑢<sup>𝑖</sup></p>
<p>(0.1)&nbsp;&nbsp;&nbsp;&nbsp; −∆𝑢<sup>𝑖</sup> + (1 − χ<sub>𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span></sub>)β<sub>ε</sub>(𝑢<sup>𝑖</sup>) = 𝑡µ<sup>𝑖</sup> − (1/ε)∑<sub>𝑗&ne;𝑖</sub> B<sub>ε</sub> (𝑢<sup>𝑗</sup>)β<sub>ε</sub>(𝑢<sup>𝑖</sup>) for 𝑖 = 1, . . . , m</p>
<p>where 𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span> ⊂ Ω ⊂ ℝ<sup>n</sup>, µ<sup> </sup> ∈ H<sup>−1</sup>(Ω), β<sub>ε</sub> is a mollification of the Heaviside step function, 𝐵(𝑠) = &int;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑠</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span> β<sub>ε</sub>(𝑠&prime;)𝑑𝑠&prime;, 𝑢 = (𝑢<sup>𝑖</sup>)<sub>𝑖 = 1, . . . , m</sub> a vector with components 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω) and 𝐻<sup>−1</sup>(Ω) is the dual of the Sobolev space 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω). We show that the smooth problem has a solution [0, ∞) &in; 𝑡 ↦ 𝑢<sub>𝑡</sub>;ε ∈ 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω; ℝ<sup>m</sup>) depending on ε using a variational technique. Upon letting ε → 0<sup>+</sup>, for fixed 𝑡, the solution 𝑢<sub>𝑡;ε</sub> converges weakly to some 𝑢<sub>𝑡</sub> ∈ 𝐻<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>1</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span>(Ω; ℝ<sup>m</sup>) solving</p>
<p>(0.2)&nbsp;&nbsp;&nbsp;&nbsp; −∆𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> + (1 − χ<sub>𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>0</sub></span></span></sub>)χ<sub>{𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.6rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> &gt; 0}</sub> = 𝑡µ<sup>𝑖</sup> - κ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span>  in H<sup>−1</sup>(Ω),</p><p>for some non-negative elements κ<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> ∈ H<sup>−1</sup>(Ω) having support on ∂𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span>. Furthermore the phases represented by the components of 𝑢<sub>𝑡</sub> are separated in the sense that the overlap of any two distinct phases has vanishing n-dimensional Lebesgue measure i.e.</p>
<p>(0.3) &nbsp;&nbsp;&nbsp;&nbsp; ∣supp 𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> &cap; supp 𝑢<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑗</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span>∣ = 0 for 𝑖 &ne; 𝑗.</p>
<p>We also touch upon a formulation of the multiple phase Hele-Shaw flow which would, beyond separation of the phases, provide freezing of the intersecting boundary &Gamma;<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖𝑗</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> = ∂𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑖</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> &cap; ∂𝐷<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 1.0rem;"><sup>𝑗</sup><span style="font-size: 0.2rem;">&nbsp;</span><sub>𝑡</sub></span></span> of two phases. This formulation of the problem tries to incorporate memory in to the system via means of an integration over previous states.</p>

----------------------------------------------------------------------
In diva2:1780566 
abstract is: 
<p>This study evaluates the feasibility of reusing lightweight solar sails in order to transport 1.69 * 10^6 sunshades, made out of occulting membranes with free-standing SiO2 nanotube films, to the adjusted sun-Earth Lagrange point, L1'. The purpose of the study was therefore to evaluate if this method is sufficient enough to lower Earth's average surface temperature by 1 degree C within a reasonable time frame, due to the rapid climate change, and compare the total launch mass to previously proposed methods. Two mission times of 10 years and 15 years were used, and three different starting altitudes, the GEO, MEO and LEO orbits, were investigated. The results showed that the method in this study was feasible for all combinations of starting altitudes and mission times. The solution where the mission time was set to 15 years and where the starting altitude was set to the GEO orbit, resulted in a launch mass of 11\% of the mass of the previously proposed solution. Furthermore, the investigation showed that high altitude starting orbits and long mission times resulted in a lower launch mass. However, in order to fulfill the goal of reducing the average temperature by 1 degree C in a reasonable time frame, the mission time cannot be too long. Finally, the results and calculations in this study are partially based on assumptions and simplifications, and therefore the results should be considered as approximations and not exact analytical solutions.</p>

corrected abstract:
<p>This study evaluates the feasibility of reusing lightweight solar sails in order to transport 1.69 · 10<sup>6</sup> sunshades, made out of occulting membranes with free-standing <em>SiO<sub>2</sub></em> nanotube films, to the adjusted sun-Earth Lagrange point, 𝐿1&prime;. The purpose of the study was therefore to evaluate if this method is sufficient enough to lower Earth's average surface temperature by 1ºC within a reasonable time frame, due to the rapid climate change, and compare the total launch mass to previously proposed methods. Two mission times of 10 years and 15 years were used, and three different starting altitudes, the GEO, MEO and LEO orbits, were investigated. The results showed that the method in this study was feasible for all combinations of starting altitudes and mission times. The solution where the mission time was set to 15 years and where the starting altitude was set to the GEO orbit, resulted in a launch mass of 11% of the mass of the previously proposed solution. Furthermore, the investigation showed that high altitude starting orbits and long mission times resulted in a lower launch mass. However, in order to fulfill the goal of reducing the average temperature by 1ºC in a reasonable time frame, the mission time cannot be too long. Finally, the results and calculations in this study are partially based on assumptions and simplifications, and therefore the results should be considered as approximations and not exact analytical solutions.</p>
----------------------------------------------------------------------
In diva2:1851005 
abstract is: 
<p>Renormalization is a powerful tool showing up in different contexts of mathematics and physics. In the context of circle diffeomorphisms, the renormalization operator acts like a microscope and allows to study the dynamics of a circle diffeomorphism on a small scale. The convergence of renormalization leads to a proof of the so-called rigidity theorem, which classifies the dynamics of circle diffeomorphisms geometrically: the conjugacy between $C^3$ circle diffeomorphism with Diophantine rotation number and the corresponding rotation is $C^1$.</p><p>In this thesis, we define the renormalization of circle diffeomorphisms and study its dynamics. In particular, we prove that the renormalization of orientation preserving $C^3$ circle diffeomorphisms with irrational rotation number of bounded type converges to rotations at exponential speed. We also introduce the necessary relevant concepts such as rotation number, distortion and non-linearity and discuss some of their properties.</p><p>This thesis is a summary and supplement to the book One-Dimensional Dynamics: from Poincaré to Renormalization.</p>

corrected abstract:
<p>Renormalization is a powerful tool showing up in different contexts of mathematics and physics. In the context of circle diffeomorphisms, the renormalization operator acts like a microscope and allows to study the dynamics of a circle diffeomorphism on a small scale. The convergence of renormalization leads to a proof of the so-called rigidity theorem, which classifies the dynamics of circle diffeomorphisms geometrically: the conjugacy between 𝐶<sup>3</sup> circle diffeomorphism with Diophantine rotation number and the corresponding rotation is 𝐶<sup>1</sup>.</p><p>In this thesis, we define the renormalization of circle diffeomorphisms and study its dynamics. In particular, we prove that the renormalization of orientation preserving 𝐶<sup>3</sup> circle diffeomorphisms with irrational rotation number of bounded type converges to rotations at exponential speed. We also introduce the necessary relevant concepts such as rotation number, distortion and non-linearity and discuss some of their properties.</p><p>This thesis is a summary and supplement to the book <em>One-Dimensional Dynamics: From Poincaré to Renormalization</em>.</p>
----------------------------------------------------------------------
In diva2:558519 
abstract is: 
<p>How should n points be distributed in a given region F in R^d such that they are separated as much as possible?</p><p>This general problem is studied in this paper, for some combinations of F, d, n, and the ways one can state the problem mathematically. Some numerical optimization methods are suggested and tested, both on the point separation problem and the closely related circle packing problem. The results are compared with some known analytical results. The main conclusion is that the suggested numerical methods are useful general tools to obtain optimal solutions to the considered problems.</p>

corrected abstract:
<p>How should 𝑛 points be distributed in a given region <img style="display: inline;" src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?\mathcal{F}" /> in ℝ<sup>𝑑</sup> such that they are separated as much as possible? This general problem is studied in this paper, for some combinations of <img style="display: inline;" src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?\mathcal{F}" />, 𝑑, 𝑛, and the ways one can state this problem mathematically. Some numerical optimization methods are suggested and tested, both on the point separation problem and the closely related circle packing problem. The results are compared with some known analytical results. The main conclusion is that the suggested numerical methods are useful general tools to obtain optimal solutions to the considered problems.</p>
----------------------------------------------------------------------
In diva2:1806023 
abstract is: 
<p>A Linear Program is a problem where the goal is to maximize a linear function subject to a set of linear inequalities. Geometrically, this can be rephrased as finding the highest point on a polyhedron. The Simplex method is a commonly used algorithm to solve Linear Programs. It traverses the vertices of the polyhedron, and in each step, it selects one adjacent better vertex and moves there. There can be multiple vertices to choose from, and therefore the Simplex method has different variants deciding how the next vertex is selected. One of the most natural variants is Random Edge, which in each step of the Simplex method uniformly at random selects one of the better adjacent vertices.</p><p>It is interesting and non-trivial to study the complexity of variants of the Simplex method in the number of variables, d, and inequalities, N. In 2011, Friedmann, Hansen, and Zwick found a class of Linear Programs for which the Random Edge algorithm is subexponential with complexity 2^Ω(N^(1/4)), where d=Θ(N). Previously all known lower bounds were polynomial. We give an improved lower bound of 2^Ω(N^(1/2)), for Random Edge on Linear Programs where d=Θ(N).</p><p>Another well studied variant of the Simplex method is Random Facet. It is upper bounded by 2^O(N^(1/2)) when d=Θ(N). Thus we prove that Random Edge is not faster than Random Facet on Linear Programs where d=Θ(N).</p><p>Our construction is very similar to the previous construction of Friedmann, Hansen and Zwick. We construct a Markov Decision Process which behaves like a binary counter with linearly many levels and linearly many nodes on each level. The new idea is a new type of delay gadget which can switch quickly from 0 to 1 in some circumstances, leading to fewer nodes needed on each level of the construction. The key idea is that it is worth taking a large risk of getting a small negative reward if the potential positive reward is large enough in comparison.</p>

corrected abstract:
<p>A Linear Program is a problem where the goal is to maximize a linear function subject to a set of linear inequalities. Geometrically, this can be rephrased as finding the highest point on a polyhedron. The Simplex method is a commonly used algorithm to solve Linear Programs. It traverses the vertices of the polyhedron, and in each step, it selects one adjacent better vertex and moves there. There can be multiple vertices to choose from, and therefore the Simplex method has different variants deciding how the next vertex is selected. One of the most natural variants is Random Edge, which in each step of the Simplex method uniformly at random selects one of the better adjacent vertices.</p><p>It is interesting and non-trivial to study the complexity of variants of the Simplex method in the number of variables, 𝑑, and inequalities, 𝑁. In 2011, Friedmann, Hansen, and Zwick found a class of Linear Programs for which the Random Edge algorithm is subexponential with complexity 2<sup>Ω(𝑁<sup>1/4</sup>)</sup>, where 𝑑=Θ(𝑁). Previously all known lower bounds were polynomial. We give an improved lower bound of 2<sup>Ω(√<span style="text-decoration: overline;">𝑁</span>)</sup>, for Random Edge on Linear Programs where 𝑑=Θ(𝑁).</p><p>Another well studied variant of the Simplex method is Random Facet. It is upper bounded by 2<sup>O(√<span style="text-decoration: overline;">𝑁</span>)</sup> when 𝑑=Θ(𝑁). Thus we prove that Random Edge is not faster than Random Facet on Linear Programs where 𝑑=Θ(𝑁).</p><p>Our construction is very similar to the previous construction of Friedmann, Hansen and Zwick. We construct a Markov Decision Process which behaves like a binary counter with linearly many levels and linearly many nodes on each level. The new idea is a new type of delay gadget which can switch quickly from 0 to 1 in some circumstances, leading to fewer nodes needed on each level of the construction. The key idea is that it is worth taking a large risk of getting a small negative reward if the potential positive reward is large enough in comparison.</p>
----------------------------------------------------------------------
In diva2:1801622 
abstract is: 
<p>It is established that paper properties depend on the loading rate. The rule of thumb is that the in-plane strength and stiffness increases about 10\%, when the strain rate increases by a factor of 10. Converting of paperboard into packages requires creasing of the paperboard followed by folding to make 3 dimensional packages. Crease response is controlled by in-plane properties, which contribute to the loading and the spring back of the crease which gives the paperboard its final geometry. </p><p>This work aims to characterize the rate and time dependent properties of paper, done by tensile testing at high strain rates of up-to 100 000 mm/min using an electro-mechanical testing machine. Also investigated in this work are the rate dependence and characterization of the plies for a deeper understanding of the contributing factors to this rate dependence. At the end of this work the aim is to retrieve the rate dependent behavior of the materials and compare them with the existing rule of thumb.</p><p>In this work it was concluded that the rule of thumb is accurate for the ultimate strength of the material in the strain rate range of 10^4 to 10^1 strains/second. It was also observed that stiffness of the material increases, but at a rate lower than the stated rule of thumb.</p>

corrected abstract:
<p>It is established that paper properties depend on the loading rate. The rule of thumb is that the in-plane strength and stiffness increases about 10%, when the strain rate increases by a factor of 10. Converting of paperboard into packages requires creasing of the paperboard followed by folding to make 3 dimensional packages. Crease response is controlled by in-plane properties, which contribute to the loading and the spring back of the crease which gives the paperboard its final geometry.</p><p>This work aims to characterise the rate and time dependent properties of paper, done by tensile testing at high strain rates of up-to 100 000 mm/min using an electro-mechanical testing machine. Also investigated in this work are the rate dependence and characterization of the plies for a deeper understanding of the contributing factors to this rate dependence. At the end of this work the aim is to retrieve the rate dependent behaviour of the materials and compare them with the existing rule of thumb.</p><p>In this work it was concluded that the rule of thumb is accurate for the ultimate strength of the material in the strain rate range of 10<sup>4</sup> to 10<sup>1</sup> strains/second. It was also observed that stiffness of the material increases but at a rate lower than the stated rule of thumb.</p>
----------------------------------------------------------------------
In diva2:1626655 
abstract is: 
<p>The failure of the Standard Model of particle physics to predict neutrino masses invites us to amend it. We have no reason to believe that the interactions currently described by the gauge group are the whole picture, nor should we expect that the number of fermions hitherto observed is correct. In this thesis, we explore two amendments to the Standard Model, and examine whether either of these is consistent with measured data. Firstly, we consider the sterile neutrino, which has no interactions described by the Standard Model. We examine the effect of this new fermion on the neutrino oscillation probabilities and present what could be a detectable signal in the TeV energy range. Secondly, we consider interactions beyond the Standard Model, possibly stemming from a higher-order theory. We show how the parameters of these Non-Standard Interactions (NSI) can modify the oscillation probabilities and within which energy range we expect to discern this signal. We use data from and simulate event counts in two Cherenkov detectors: IceCube and DeepCore. Moreover, we generate data and simulate a proposed upgrade of the DeepCore detector: PINGU. Using IceCube track events, we obtain best-fit values ∆m^2_41 = 0.01eV^2 and θ_24 = 0.67 for our sterile neutrino hypothesis at a p-value of 20%, which is not statistically significant. Hence, we found no evidence of a sterile neutrino in IceCube data. Moreover, we were unable to distinguish a signal from θ_34 in our IceCube simulation. We obtain stringent bounds on the NSI parameters and compare those to previous results in literature. We show that PINGU is expected to narrow the bound further on ε_μτ , especially by considering a joint analysis with IceCube and DeepCore. Finally, we see that an anti-correlation between ε_eμ and ε_eτ at probability level was propagated down to event level, which we expect to be observable by PINGU.</p>

corrected abstract:
<p>The failure of the Standard Model of particle physics to predict neutrino masses invites us to amend it. We have no reason to believe that the interactions currently described by the gauge group are the whole picture, nor should we expect that the number of fermions hitherto observed is correct. In this thesis, we explore two amendments to the Standard Model, and examine whether either of these are consistent with measured data.</p><p>Firstly, we consider the sterile neutrino, which has no interactions described by the Standard Model. We examine the effect of this new fermion on the neutrino oscillation probabilities, and present what could be a detectable signal in the TeV energy range. Secondly, we consider interactions beyond the Standard Model, possibly stemming from a higher-order theory. We show how the parameters of these Non-Standard Interactions (NSI) can modify the oscillation probabilities and within which energy range we expect to discern this signal. We use data from and simulate event counts in two Cherenkov detectors: IceCube and DeepCore. Moreover, we generate data and simulate a proposed upgrade of the DeepCore detector: PINGU.</p><p>Using IceCube track events, we obtain best-fit values ∆m<span style="display: inline-flex;"><span style="display: flex; flex-direction: column; font-size: 0.8rem;"><sup>2</sup><sub>41</sub></span></span> = 0.01eV<sup>2</sup> and θ<sub>24</sub> = 0.67 for our sterile neutrino hypothesis at a p-value of 20%, which is not statistically significant. Hence, we found no evidence of a sterile neutrino in IceCube data. Moreover, we were unable to distinguish a signal from θ<sub>34</sub> in our IceCube simulation. We obtain stringent bounds on the NSI parameters and compare those to previous results in literature. We show that PINGU is expected to narrow the bound further on ε<sub>μτ</sub>, especially by considering a joint analysis with IceCube and DeepCore. Finally, we see that an anti-correlation between ε<sub>eμ</sub> and ε<sub>eτ</sub> at probability level was propagated down to event level, which we expect to be observable by PINGU.</p>
----------------------------------------------------------------------
In diva2:1652347 
abstract is: 
<p>CUBES is an X-ray detector that will be placed aboard the KTH 3U CubeSat mission, MIST. Its purpose is to detect high energy X-rays as well as to test various components in a space environment. Two CUBES will be placed on the satellite. Each CUBES consists of a printed circuit board (PCB) with three multi pixel photon counters (MPPCs). On top of these, three Germanium Aluminium Gadolinium Garnet (GAGG) scintillators are glued. These GAGG scintillators are of the dimension 1X1X1 cm^3 and are covered with PTFE tape and an opaque potting compound to prevent photons from leaving the scintillator. The MPPCs consists of a large amount of semi conductors operated in Geiger mode. The data is processed by an application specific integrated circuit (ASIC). In order to prepare the CUBES instrument for satellite flight, energy and thermal characterisation have been performed. The energy range was determined to be 40-1200 keV. The detector system shows linear behaviour and operates stably in a temperature range of -20 °C to +30 °C. The preparation of the boards and test results are presented in this thesis.</p>

corrected abstract:
<p>CUBES is an X-ray detector that will be placed aboard the KTH 3U CubeSat mission, MIST. Its purpose is to detect high energy X-rays as well as to test various components in a space environment. Two CUBES will be placed on the satellite. Each CUBES consists of a printed circuit board (PCB) with three multi pixel photon counters (MPPCs). On top of these, three Gd<sub>3</sub>Al<sub>2</sub>Ga<sub>3</sub>O<sub>12</sub> (GAGG) scintillators are glued. These GAGG scintillators are of the dimension 1 × 1 × 1 cm<sup>3</sup> and are covered with PTFE tape and an opaque potting compound to prevent photons from leaving the detector. The MPPCs consists of a large amount of semi conductors operated in Geiger mode. The data is processed by an application specific integrated circuit (ASIC). In order to prepare the CUBES instrument for satellite flight, energy and thermal characterisation have been performed. The energy range was determined to be 40-1200 keV. The detector system shows linear behaviour and operates stably in a temperature range of -20 °C to +30 °C. The preparation of the boards and test results are presented in this thesis.</p>
----------------------------------------------------------------------
In diva2:1445991 
abstract is: 
<p>During 2018, the Public Transport Administration (Trafikförvaltningen) in the Stockholm region spent approximately 2.2 billion SEK on new infrastructure investments related to the public transport system, many of which were based on their public transport models. The previously used method for validating these models has lacked scientific rigour, efficiency and a systematic approach, which has led to uncertainty in decision making. Furthermore, few scientific studies have been conducted to develop validation methodologies for large-scale models, such as public transport models. For these reasons, a scientific validation methodology for public transport models has been developed in this thesis. This validation methodology has been applied on the 2014 route assignment model used by Trafikförvaltningen, for the transport modes bus, commuter train and local tram.</p><p>In the developed validation methodology, the selected validation metrics called MAPE, %RMSE and R^2 are used to compare link loads from a route assignment model with observed link loads from an Automatic Passenger Counting (APC) system. To obtain an overview of the performance of the route assignment model, eight different scenarios are set, based on whether the validation metrics meet acceptable thresholds or not.</p><p>In the application of the developed validation methodology, the average link loads for the morning rush have been validated. To adjust the developed validation methodology to system-specific factors and to set acceptable metric thresholds, discussions with model practitioners have taken place. The validation has been performed on both lines and links, and for bus entire line number series have been validated as well. The validation results show that commuter train meets the set threshold values in a higher proportion than bus and local tram do. However, Trafikförvaltningen is recommended to further calibrate the route assignment model in order to achieve a better model performance.</p><p>The developed validation methodology can be used for validation of public transport models, and can in combination with model calibration be used in an iterative process to fine-tune model parameters for optimising validation results. Finally, a number of recommendations are proposed for Trafikförvaltningen to increase the efficiency and quality of the validation process, such as synchronising model data with the observed data.</p>

corrected abstract:
<p>During 2018, the Public Transport Administration (Trafikförvaltningen) in the Stockholm region spent approximately 2.2 billion SEK on new infrastructure investments related to the public transport system, many of which were based on their public transport models. The previously used method for validating these models has lacked scientific rigour, efficiency and a systematic approach, which has led to uncertainty in decision making. Furthermore, few scientific studies have been conducted to develop validation methodologies for large-scale models, such as public transport models. For these reasons, a scientific validation methodology for public transport models has been developed in this thesis. This validation methodology has been applied on the 2014 route assignment model used by Trafikförvaltningen, for the transport modes bus, commuter train and local tram.</p><p>In the developed validation methodology, the selected validation metrics called MAPE, %RMSE and R<sup>2</sup> are used to compare link loads from a route assignment model with observed link loads from an Automatic Passenger Counting (APC) system. To obtain an overview of the performance of the route assignment model, eight different scenarios are set, based on whether the validation metrics meet acceptable thresholds or not.</p><p>In the application of the developed validation methodology, the average link loads for the morning rush have been validated. To adjust the developed validation methodology to system-specific factors and to set acceptable metric thresholds, discussions with model practitioners have taken place. The validation has been performed on both lines and links, and for bus entire line number series have been validated as well. The validation results show that commuter train meets the set threshold values in a higher proportion than bus and local tram do. However, Trafikförvaltningen is recommended to further calibrate the route assignment model in order to achieve a better model performance.</p><p>The developed validation methodology can be used for validation of public transport models, and can in combination with model calibration be used in an iterative process to fine-tune model parameters for optimising validation results. Finally, a number of recommendations are proposed for Trafikförvaltningen to increase the efficiency and quality of the validation process, such as synchronising model data with the observed data.</p>
----------------------------------------------------------------------
In diva2:1360711 
abstract is: 
<p>The Giraffe 1X is a mobile short range 3D radar from Saab used for example to detect threats and create protection in a ground based air defence system. It can also be used on naval platforms for air and surface surveillance. During the development of the radar, the system needs to be tested for both sea and mobile land applications. The most convenient place for testing is on the roof of Saab’s facility in Gothenburg. There elevators can raise the radar to the roof giving an excellent view of for example Landvetter airport and the sea. To aid future verification experiments of the radar system, this project was started in order to develop and construct a motion platform used to simulate sea- and vehicle motions. During a six month period at Saab, the work of the project was started with a thorough research of motions platforms to conduct preliminary concept studies. Furthermore the concepts were drawn as 3D-CAD models in Creo Parametric in order to visualise the different solutions and present them for suppliers. The report also covers the assembly of the produced parts, together with the development of a user interface to control the motion platform.</p><p>Lastly, the result of product development is a two-degree of freedom (DOF) motion platform influenced by the gyroscopic gimbal concept. The G1X radar is mounted on a gimbal platform which is made out of two aluminium frames, whereas the outer frame rotates around an horizontal axis while the inner frame rotates around a transversely mounted horizontal axis mounted on the outer frame. Each aluminium frame is attached to a link arm which is mounted on a motor that is used to tilt the frame. The platform can be tilted _ 22 o in pitch and _ 22 o in roll. The gimbal is supported by a steel structure to allow ground clearance and to raise the radar to a comfortable working height.</p>

corrected abstract:
<p>The Giraffe 1X is a mobile short range 3D radar from Saab used for example to detect threats and create protection in a ground based air defence system. It can also be used on naval platforms for air and surface surveillance. During the development of the radar, the system needs to be tested for both sea and mobile land applications. The most convenient place for testing is on the roof of Saab’s facility in Gothenburg. There elevators can raise the radar to the roof giving an excellent view of for example Landvetter airport and the sea. To aid future verification experiments of the radar system, this project was started in order to develop and construct a motion platform used to simulate sea- and vehicle motions. During a six month period at Saab, the work of the project was started with a thorough research of motions platforms to conduct preliminary concept studies. Furthermore the concepts were drawn as 3D-CAD models in Creo Parametric in order to visualise the different solutions and present them for suppliers. The report also covers the assembly of the produced parts, together with the development of a user interface to control the motion platform.</p><p>Lastly, the result of product development is a two-degree of freedom (DOF) motion platform influenced by the gyroscopic gimbal concept. The G1X radar is mounted on a gimbal platform which is made out of two aluminium frames, whereas the outer frame rotates around an horizontal axis while the inner frame rotates around a transversely mounted horizontal axis mounted on the outer frame. Each aluminium frame is attached to a link arm which is mounted on a motor that is used to tilt the frame. The platform can be tilted ± 22 º in pitch and ± 22 º in roll. The gimbal is supported by a steel structure to allow ground clearance and to raise the radar to a comfortable working height.</p>
----------------------------------------------------------------------
In diva2:1698174 
abstract is: 
<p>This study has investigated the conceptual feasibility of a rocket propelled kinetic energy penetrator (KEP), designed for the handheld recoilless rifle Carl-Gustaf® 84 mm calibre system, from an exterior ballistics perspective. The methodology is based upon evaluating the aerodynamic properties of different conceptual design proposals through CFD-simulations and then performing trajectory analysis to assess their exterior ballistic performance. In particular, the main focus has been to optimize the stability, velocity and spin rate of the KEP. The results of the study indicates that the final chosen KEP design retains, from an aerodynamic perspective, longitudinal stability for Mach numbers up to 4.5, regardless if the rocket motor is ignited or not. Furthermore, if using NK1384 propellant, the final chosen design in the study is, according to the calculations, able to achieve a maximum velocity of 0.7⋅v_ref and retain a minimum velocity of 0.628⋅v_ref in the horizontal range of [0.318⋅x_ref,0.648⋅x_ref] measured from the shooter. In addition, the angular spin velocity achieves a maximum value of 15.5 Hz, satisfying the performance limitation of the rocket motor which only functions properly for frequencies up to 30 Hz, while simultaneously providing a sufficiently considered spin rate in order to/ average possible thrust and mass deviations of the KEP. The results also show that if using ammonium dinitramide (ADN) propellant, the KEP is able to achieve a maximum velocity of 0.786⋅v_ref, retain a minimum velocity of 0.628⋅v_ref in the horizontal range of [0.28⋅x_ref,0.98⋅x_ref] and achieve a maximum spin rate of 17.5 Hz.</p>

corrected abstract:
<p>This study has investigated the conceptual feasibility of a rocket propelled kinetic energy penetrator (KEP), designed for the handheld recoilless rifle Carl-Gustaf® 84 𝑚𝑚 calibre system, from an exterior ballistics perspective. The methodology is based upon evaluating the aerodynamic properties of different conceptual design proposals through CFD-simulations and then performing trajectory analysis to assess their exterior ballistic performance. In particular, the main focus has been to optimize the stability, velocity and spin rate of the KEP. The results of the study indicates that the final chosen KEP design retains, from an aerodynamic perspective, longitudinal stability for Mach numbers up to 4.5, regardless if the rocket motor is ignited or not. Furthermore, if using NK1384 propellant, the final chosen design in the study is, according to the calculations, able to achieve a maximum velocity of 0.7 ⋅ 𝑣<sub>ref</sub> and retain a minimum velocity of 0.628⋅𝑣<sub>ref</sub> in the horizontal range of <span style="font-size: 1.25rem;">[</span>0.318⋅𝑥<sub>ref</sub>,0.648⋅𝑥<sub>ref</sub><span style="font-size: 1.25rem;">]</span> measured from the shooter. In addition, the angular spin velocity achieves a maximum value of 15.5 𝐻𝑧, satisfying the performance limitation of the rocket motor which only functions properly for frequencies up to 30 𝐻𝑧, while simultaneously providing a sufficiently considered spin rate in order to average possible thrust and mass deviations of the KEP. The results also show that if using ammonium dinitramide (ADN) propellant, the KEP is able to achieve a maximum velocity of 0.786⋅𝑣<sub>ref</sub>, retain a minimum velocity of 0.628⋅𝑣<sub>ref</sub> in the horizontal range of <span style="font-size: 1.25rem;">[</span>0.28⋅𝑥<sub>ref</sub>,0.98⋅𝑥<sub>ref</sub><span style="font-size: 1.25rem;">]</span> and achieve a maximum spin rate of 17.5 𝐻𝑧.</p>
----------------------------------------------------------------------
In diva2:612264 
abstract is: 
<p>Switched systems form a special class of hybrid dynamical systems, i.e. systems with both continuous and discrete dynamics. A switched system contains a family of continuous subsystems and a discrete variable that governs the switching between them. The problem of finding necessary and sufficient conditions for asymptotic stability under arbitrary switching has recently been solved for the two-dimensional focused switched system <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cdot%7Bx%7D=uAx+(1-u)Bx" /> . We consider a generalization <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cdot%7Bx%7D=uA(x-x_c)+(1-u)Bx" />  where the equilibrium points of the two subsystems have been separated, a <em>defocused</em> system. Using geometrical arguments we show that whenever the focused system is asymptotically stable, a corresponding defocused system will contain a ‘smallest’ invariant set <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" />. In the case when both the subsystems have non-real eigenvalues we are able to completely characterize <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" /> and prove that all trajectories converge to <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" />. We investigate topological properties of <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" /> and classify the possible irregularities of <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Cpartial%5COmega" />. We also build time-optimal syntheses inside <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5COmega" /> which exhibit the behavior of minimal-time trajectories. Finally we briefly look at additional phenomena that occur when real eigenvalues are present in the subsystems.</p>

corrected abstract:
<p>Switched systems form a special class of hybrid dynamical systems, i.e. systems with both continuous and discrete dynamics. A switched system contains a family of continuous subsystems and a discrete variable that governs the switching between them. The problem of finding necessary and sufficient conditions for asymptotic stability under arbitrary switching has recently been solved for the two-dimensional <em>focused</em> switched system <em>ẋ</em> = 𝑢𝐴𝑥 + (1 - 𝑢)𝐵𝑥. We consider a generalization <em>ẋ</em> = 𝑢𝐴(𝑥 - 𝑥<sub>𝑐</sub>) + (1 - 𝑢)𝐵𝑥 where the equilibrium points of the two subsystems have been separated, a <em>defocused</em> system. Using geometrical arguments we show that whenever the focused system is asymptotically stable, a corresponding defocused system will contain a ‘smallest’ invariant set Ω. In the case when both the subsystems have non-real eigenvalues we are able to completely characterize Ω and prove that all trajectories converge to Ω. We investigate topological properties of Ω and classify the possible irregularities of ∂Ω. We also build time-optimal syntheses inside Ω which exhibit the behavior of minimal-time trajectories. Finally we briefly look at additional phenomena that occur when real eigenvalues are present in the subsystems.</p>

Note the equations were simple enough that they could simply be done in unicode and HTML
----------------------------------------------------------------------
In diva2:1441946 
abstract is: 
<p>The purpose of this project is to investigate the angled nutrunner, which is a hand held torque tool often used in the industry to tighten bolted joints. The goal is to estimate parameter values for an existing model that can describe the reaction force and the angular displacement of the tool as a function of the torque transferred to the joint. The model is based on a damped mass-spring system with one degree of freedom. The short torque pulse in the tool will induce an oscillating motion of the system. A large amount of data is used that has been collected at an assembly factory. Different tightening conditions tested include tightening strategy (Quick Step and Turbo Tight), joint stiffness (hard and soft joint), tightening pace (5 and 8 tightenings per minute) and target torque. Joint torque, angular acceleration for the tool and, for the Quick Step tightenings, the reaction force is measured. The grey-box model is used in order to estimate the parameters of the model through curve fitting of the data describing the angular displacement. The reaction force on the user is also examined with different methods. The resulting mean values of the model parameters are the mass m = 2.31 kg, the dampening constant m = 103.45 kg/s and the spring stiffness k = 2314.84 N/m. The mean value of the natural frequency of the hand-arm system is f_n = 5.46 Hz. A statistical significance is found for all conditions except the tightening pace.</p>

corrected abstract:
<p>The purpose of this project is to investigate the angled nutrunner, which is a hand held torque tool often used in the industry to tighten bolted joints. The goal is to estimate parameter values for an existing model that can describe the reaction force and the angular displacement of the tool as a function of the torque transferred to the joint.</p><p>The model is based on a damped mass-spring system with one degree of freedom. The short torque pulse in the tool will induce an oscillating motion of the system.</p><p>A large amount of data is used that has been collected at an assembly factory. Different tightening conditions tested include tightening strategy (Quick Step and Turbo Tight), joint stiffness (hard and soft joint), tightening pace (5 and 8 tightenings per minute) and target torque. Joint torque, angular acceleration for the tool and, for the Quick Step tightenings, the reaction force is measured. The <em>grey-box model</em> is used in order to estimate the parameters of the model through curve fitting of the data describing the angular displacement. The reaction force on the user is also examined with different methods.</p><p>The resulting mean values of the model parameters are the mass 𝑚 = 2.31 kg, the dampening constant 𝑐 = 103.45 kg/s and the spring stiffness 𝑘 = 2314.84 N/m. The mean value of the natural frequency of the hand-arm system is 𝑓<sub>𝑛</sub> = 5.46 Hz. A statistical significance is found for all conditions except the tightening pace.</p>
----------------------------------------------------------------------
In diva2:1231299 
Note: no full text in DiVA

abstract is: 
<p>An important step in manufacturing humanoid robots is being able to imitate human movement. In the case of this project, optimal movement patterns retrieved from solving an optimal control problem serve as a substitute for human movement. A supervised learning algorithm learns to model the control signal of a mobile manipulator striking a projectile to hit a specific target. Data fed to the algorithm contains trajectories generated by solving an optimal control problem. The supervised learning algorithm applied to the problem was written in Python using the TensorFlow software library. By dividing the data in two sets, one for training and one for testing, progress is measured and overfitting estimated by calculating the relative percentage error between values predicted by the model and the corresponding values in the two data sets. A mean training accuracy of 90.7 percent and a mean validation accuracy of -226 percent. The source code can be found on https://github.com/JeremiGrosz/Optimal_control_supervised_learning</p>

corrected abstract:
<p>An important step in manufacturing humanoid robots is being able to imitate human movement. In the case of this project, optimal movement patterns retrieved from solving an optimal control problem serve as a substitute for human movement. A supervised learning algorithm learns to model the control signal of a mobile manipulator striking a projectile to hit a specific target. Data fed to the algorithm contains trajectories generated by solving an optimal control problem. The supervised learning algorithm applied to the problem was written in Python using the TensorFlow software library. By dividing the data in two sets, one for training and one for testing, progress is measured and overfitting estimated by calculating the relative percentage error between values predicted by the model and the corresponding values in the two data sets. A mean training accuracy of 90.7 percent and a mean validation accuracy of -226 percent. The source code can be found on <a href="https://github.com/JeremiGrosz/Optimal_control_supervised_learning">https://github.com/JeremiGrosz/Optimal_control_supervised_learning</a></p>
----------------------------------------------------------------------
In diva2:643302 
abstract is: 
<p>There are various technologies used for reducing fuel consumption of automobiles. Hybrid electric vehicles is one approach that has been used, which can reduce fuel consumption by 10-30% compared to conventional vehicles.</p><p>In this master thesis the minimization of fuel consumption of a power-split type HEV along a given route is considered, where the vehicle speed has been assumed to be known <em>a priori</em>. This minimization was made by first deriving a model of the HEV powertrain, followed by creating a Dynamical programming based program for finding the optimal distribution of torques.</p><p>The performance was evaluated through the commercial software GT-Suite. The resulting control from the Dynamic program could follow the reference speed in many situations. However the battery state-of-charge calculated in the Dynamic program did not update properly, resulting in a depleted battery in some cases.</p><p>The model derived could follow the dynamics of the vehicle, but there are some parts which could be improved. One of them is the dynamical model of the rotational speed for the engine <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?%5Comega_%7Be%7D" />.</p><p> The Dynamic program works for finding the controller, and can be modified to work with improved state-equations.</p>

corrected abstract:
<p>There are various technologies used for reducing fuel consumption of automobiles. Hybrid electric vehicles is one approach that has been used, which can reduce fuel consumption by 10-30% compared to conventional vehicles.</p><p>In this master thesis the minimization of fuel consumption of a power-split type HEV along a given route is considered, where the vehicle speed has been assumed to be known <em>a priori</em>. This minimization was made by first deriving a model of the HEV powertrain, followed by creating a Dynamical programming based program for finding the optimal distribution of torques.</p><p>The performance was evaluated through the commercial software GT-Suite. The resulting control from the Dynamic program could follow the reference speed in many situations. However the battery state-of-charge calculated in the Dynamic program did not update properly, resulting in a depleted battery in some cases.</p><p>The model derived could follow the dynamics of the vehicle, but there are some parts which could be improved. One of them is the dynamical model of the rotational speed for the engine <em>ω</em><sub>𝑒</sub>.</p><p> The Dynamic program works for finding the controller, and can be modified to work with improved state-equations.</p>
----------------------------------------------------------------------
In diva2:1658938 
abstract is: 
<p>In this thesis, logistic regression, random forest and statistical analysis are used to both predict and explain insurance purchases. The models are tested together with the oversampling method known as SMOTE. The result was that the random forest model together with SMOTE best predicted insurance purchases with an $F_{1}$-score of 93.2\% and ROC-AUC of 96\%. Another important discovery comes from the explanatory part where it turns out that the price of the order and the length between the order and departure date greatly influences insurance purchases. Increased values for these features increase the insurance purchase rate. For prices of the order belonging to the 90th percentile, the insurance purchase rate is approximately 2 times higher than the average and for prices of the order belonging to the 99th percentile, the insurance purchase rate is approximately 3 times higher than the average. The purchase rate is approximately 2.5 times higher for lengths longer than 8 months between order and departure compared to the length being less than one month. These are useful insights from a business perspective that can improve the travel agency’s pricing. Other more general findings include that SMOTE worked well to handle the class imbalance in this data set and that the F1-score seems to be superior to ROC-AUC as evaluation metric when it comes to the sort of problems with unbalanced data where the positive class is a minority and most important.</p>

corrected abstract:
<p>In this thesis, logistic regression, random forest and statistical analysis are used to both predict and explain insurance purchases. The models are tested together with the oversampling method known as SMOTE. The result was that the random forest model together with SMOTE best predicted insurance purchases with an 𝐹<sub>1</sub>-score of 93.2% and ROC-AUC of 96%. Another important discovery comes from the explanatory part where it turns out that the price of the order and the length between the order and departure date greatly influences insurance purchases. Increased values for these features increase the insurance purchase rate. For prices of the order belonging to the 90th percentile, the insurance purchase rate is approximately 2 times higher than the average and for prices of the order belonging to the 99th percentile, the insurance purchase rate is approximately 3 times higher than the average. The purchase rate is approximately 2.5 times higher for lengths longer than 8 months between order and departure compared to the length being less than one month. These are useful insights from a business perspective that can improve the travel agency’s pricing. Other more general findings include that SMOTE worked well to handle the class imbalance in this data set and that the F1-score seems to be superior to ROC-AUC as evaluation metric when it comes to the sort of problems with unbalanced data where the positive class is a minority and most important.</p>
----------------------------------------------------------------------
In diva2:1655639 
abstract is: 
<p>This project aims to analyze the risk measures Value-at-Risk and Conditional-Value-at-Risk for three stock portfolios with the purpose of evaluating each method's accuracy in modelling Black Swan events. This is achieved by utilizing a parametric approach in the form of a modified (C)VaR with a Cornish-Fisher expansion, a historic approach with a time series spanning ten years and a Markov Monte Carlo simulation modeled with a Brownian motion. From this, it is revealed that the parametric approach at the 99\%-level generates the most favorable results for a 30-day-(C)VaR estimation for each portfolio, followed by the historic approach and, lastly, the Markov Monte Carlo simulation. As such, it is concluded that the parametric approach may serve as a method of evaluating a portfolio's exposure to Black Swan events.</p>

corrected abstract:
<p>This project aims to analyze the risk measures Value-at-Risk and Conditional-Value-at-Risk for three stock portfolios with the purpose of evaluating each method's accuracy in modelling Black Swan events. This is achieved by utilizing a parametric approach in the form of a modified (C)VaR with a Cornish-Fisher expansion, a historic approach with a time series spanning ten years and a Markov Monte Carlo simulation modeled with a Brownian motion. From this, it is revealed that the parametric approach at the 99%-level generates the most favorable results for a 30-day-(C)VaR estimation for each portfolio, followed by the historic approach and, lastly, the Markov Monte Carlo simulation. As such, it is concluded that the parametric approach may serve as a method of evaluating a portfolio's exposure to Black Swan events.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1737350 
abstract is: 
<p>The aim of this master thesis is to develop and optimize the main wing of a T-shaped hydrofoil for a small passenger vessel called FoilCart. The project is in collaboration with KTH Maritime Robotic Lab.</p><p>The existing prototype of the hydrofoil is built by a company called ZPARQ\cite{Zparq}, but the efficiency of the main wing is not optimized. Thus, based on the genetic algorithm (GA), a 2D optimization method is applied to two 2D profile candidates to increase the efficiency (Cl/Cd). After the optimization, a parametric 3D modelling process is developed in Rhino 7 Grasshopper to create the main wing from 2D profiles and configure other components in the hydrofoil. Simulations in computational fluid dynamics (CFD) are carried out in STAR CCM+ to evaluate the optimization results in 3D.</p><p>The conclusion is that the optimization is proved to be effective, with 10.7\% and 32.59\% improvement for 3D models with optimized NACA 63-412 and optimized Eppler E836 respectively, we can say that the aims of this project are completed. Nevertheless, further model refinements need to be done and the simulation results should be verified with model tests.</p>
mc='ZPARQ\\cite{Zparq' c='ZPARQ \\cite{Zparq'

partal corrected: diva2:1737350: <p>The aim of this master thesis is to develop and optimize the main wing of a T-shaped hydrofoil for a small passenger vessel called FoilCart. The project is in collaboration with KTH Maritime Robotic Lab.</p><p>The existing prototype of the hydrofoil is built by a company called ZPARQ \cite{Zparq}, but the efficiency of the main wing is not optimized. Thus, based on the genetic algorithm (GA), a 2D optimization method is applied to two 2D profile candidates to increase the efficiency (Cl/Cd). After the optimization, a parametric 3D modelling process is developed in Rhino 7 Grasshopper to create the main wing from 2D profiles and configure other components in the hydrofoil. Simulations in computational fluid dynamics (CFD) are carried out in STAR CCM+ to evaluate the optimization results in 3D.</p><p>The conclusion is that the optimization is proved to be effective, with 10.7\% and 32.59\% improvement for 3D models with optimized NACA 63-412 and optimized Eppler E836 respectively, we can say that the aims of this project are completed. Nevertheless, further model refinements need to be done and the simulation results should be verified with model tests.</p>

corrected abstract:
<p>The aim of this master thesis is to develop and optimize the main wing of a T-shaped hydrofoil for a small passenger vessel called FoilCart. The project is in collaboration with KTH Maritime Robotic Lab.</p><p>The existing prototype of the hydrofoil is built by a company called ZPARQ[22], but the efficiency of the main wing is not optimized. Thus, based on the genetic algorithm (GA), a 2D optimization method is applied to two 2D profile candidates to increase the efficiency (Cl/Cd). After the optimization, a parametric 3D modelling process is developed in Rhino 7 Grasshopper to create the main wing from 2D profiles and configure other components in the hydrofoil. Simulations in computational fluid dynamics (CFD) are carried out in STAR CCM+ to evaluate the optimization results in 3D.</p><p>The conclusion is that the optimization is proved to be effective, with 10.7% and 32.59% improvement for 3D models with optimized NACA 63-412 and optimized Eppler E836 respectively, we can say that the aims of this project are completed. Nevertheless, further model refinements need to be done and the simulation results should be verified with model tests.</p>

Note removed the "\" before "%" and inserted the citation
----------------------------------------------------------------------
In diva2:1333887 
abstract is: 
<p>Why is it that the number of travellers in Stockholm's public transportation differs from day to day? Is the difference arbitrary or do factors such as population, temperature, weather conditions, months, or even weekdays have a significant role in this variation?</p><p>This thesis aims to explore these external variables and their effect on public transportation, as well as how this type of data driven information can result in well supported decisions. The method applied to the study was multiple linear regression and the data used was retrieved from Trafikförvaltningen, SMHI, and SCB. The study concluded that the variations in the number of travellers in Stockholm's public transportation is up to 84\% explained by population, as well as month and weekday.</p>

corrected abstract:
<p>Why is it that the number of travellers in Stockholm's public transportation differs from day to day? Is the difference arbitrary or do factors such as population, temperature, weather conditions, months, or even weekdays have a significant role in this variation?</p><p>This thesis aims to explore these external variables and their effect on public transportation, as well as how this type of data driven information can result in well supported decisions. The method applied to the study was multiple linear regression and the data used was retrieved from Trafikförvaltningen, SMHI, and SCB. The study concluded that the variations in the number of travellers in Stockholm's public transportation is up to 84% explained by population, as well as month and weekday.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1880247 
abstract is: 
<p>This thesis investigates the performance of deep learning models, specifically Resnet50 and TransUnet, in semantic image segmentation on microwell images containing tumor and natural killer (NK) cells. The main goal is to examine the effect of only using bright-field data (1-channel) as input instead of both fluorescent and brightfield data (4-channel); this is interesting since fluorescent imaging can cause damage to the cells being analyzed. The network performance is measured by Intersection over Union (IoU), the networks were trained and using manually annotated data from Onfelt Lab. TransUnet consistently outperformed the Resnet50 for both the 4-channel and 1-channel data. Moreover, the 4-channel input generally resulted in a better IoU compared to using only the bright-field channel. Furthermore, a significant decline in performance is observed when the networks are tested on the control data. For the control data, the overall IoU for the best performing 4-channel model dropped from 86.2\% to 73.9\%. The best performing 1-channel model dropped from 83.8\% to 70.8\% overall IoU.</p>

corrected abstract:
<p>This thesis investigates the performance of deep learning models, specifically Resnet50 and TransUnet, in semantic image segmentation on microwell images containing tumor and natural killer (NK) cells. The main goal is to examine the effect of only using bright-field data (1-channel) as input instead of both fluorescent and bright-field data (4-channel); this is interesting since fluorescent imaging can cause damage to the cells being analyzed. The network performance is measured by Intersection over Union (IoU), the networks were trained and using manually annotated data from Onfelt Lab. TransUnet consistently outperformed the Resnet50 for both the 4-channel and 1-channel data. Moreover, the 4-channel input generally resulted in a better IoU compared to using only the bright-field channel. Furthermore, a significant decline in performance is observed when the networks are tested on the control data. For the control data, the overall IoU for the best performing 4-channel model dropped from 86.2% to 73.9%. The best performing 1-channel model dropped from 83.8% to 70.8% overall IoU.</p>

Note removed the "\" before "%" and added hyphen to "bright-field"
----------------------------------------------------------------------
In diva2:1811793 
abstract is: 
<p>In this paper, a study is conducted to investigate the use of Curriculum Learning as an approach to address accuracy issues in a neural network caused by training on a Long-Tailed dataset. The thesis problem is presented by a Swedish e-commerce company. Currently, they are using a neural network that has been modified by them using a CORAL framework. This adaptation means that instead of having a classic binary regression model, it is an ordinal regression model. The data used for training the model has a Long-Tail distribution, which leads to inaccuracies when predicting a price distribution for items that are part of the tail-end of the data. The current method applied to remedy this problem is Re-balancing in the form of down-sampling and up-sampling. A linear training scheme is introduced, increasing in increments of $10\%$ while applying Curriculum Learning. As a method for sorting the data in an appropriate way, inspiration is drawn from Knowledge Distillation, specifically the Teacher-Student model approach. The teacher models are trained as specialists on three different subsets, and furthermore, those models are used as a basis for sorting the data before training the student model. During the training of the student model, the Curriculum Learning approach is used. The results show that for Imbalance Ratio, Kullback-Liebler divergence, Class Balance, and the Gini Coefficient, the data is clearly less Long-Tailed after dividing the data into subsets. With the correct settings before training, there is also an improvement in the training speed of the student model compared to the base model. The accuracy for both the student model and the base model is comparable. There is a slight advantage for the base model when predicting items in the head part of the data, while the student model shows improvements for items that are between the head and the tail.</p>

corrected abstract:
<p>In this paper, a study is conducted to investigate the use of Curriculum Learning as an approach to address accuracy issues in a neural network caused by training on a Long-Tailed dataset. The thesis problem is presented by a Swedish e-commerce company. Currently, they are using a neural network that has been modified by them using a CORAL framework. This adaptation means that instead of having a classic binary regression model, it is an ordinal regression model. The data used for training the model has a Long-Tail distribution, which leads to inaccuracies when predicting a price distribution for items that are part of the tail-end of the data. The current method applied to remedy this problem is Re-balancing in the form of down-sampling and up-sampling. A linear training scheme is introduced, increasing in increments of 10% while applying Curriculum Learning. As a method for sorting the data in an appropriate way, inspiration is drawn from Knowledge Distillation, specifically the Teacher-Student model approach. The teacher models are trained as specialists on three different subsets, and furthermore, those models are used as a basis for sorting the data before training the student model. During the training of the student model, the Curriculum Learning approach is used. The results show that for Imbalance Ratio, Kullback-Liebler divergence, Class Balance, and the Gini Coefficient, the data is clearly less Long-Tailed after dividing the data into subsets. With the correct settings before training, there is also an improvement in the training speed of the student model compared to the base model. The accuracy for both the student model and the base model is comparable. There is a slight advantage for the base model when predicting items in the head part of the data, while the student model shows improvements for items that are between the head and the tail.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1341581 
abstract is: 
<p>The purpose of this study is to create targeted adversarial examples for an audio classifier without access to the neural networks internal structure. Previous work in this domain has shown white box attacks that generate adversarial examples with very high measures of similarity and more noisy adversarial examples generated by black box attacks. By using an algorithm that iteratively applies noise to the audio file and selects the best candidates based on the output layer of the neural network we have managed to create new audio that is 98\% similar to the original but manages to fool the speech to text audio classifier. By evaluating the generated candidates based on different measures of similarity between the proposed candidate and the original audio file we managed to create high quality black box audio adversarial examples using genetic algorithms.</p>

corrected abstract:
<p>The purpose of this study is to create targeted adversarial examples for an audio classifier without access to the neural networks internal structure. Previous work in this domain has shown white box attacks that generate adversarial examples with very high measures of similarity and more noisy adversarial examples generated by black box attacks. By using an algorithm that iteratively applies noise to the audio file and selects the best candidates based on the output layer of the neural network we have managed to create new audio that is 98% similar to the original but manages to fool the speech to text audio classifier. By evaluating the generated candidates based on different measures of similarity between the proposed candidate and the original audio file we managed to create high quality black box audio adversarial examples using genetic algorithms.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1776799 
abstract is: 
<p>We show that with the implementation presented in this paper, the Random Forest Classification model was able to predict whether or not a stock was going to increase in value during the coming day with an accuracy higher than 50\% for all stocks included in this study. Furthermore, we show that the active trading strategy presented in this paper generated higher returns and higher risk-adjusted returns than the passive investment in the stocks underlying the strategy. Therefore, we conclude \textit{(i)} that a Random Forest Classification model can be used to provide valuable insight on publicly traded stocks, and \textit{(ii)} that it is probably possible to create a profitable trading strategy based on a Random Forest Classifier, but that this requires a more sophisticated implementation than the one presented in this paper.</p>

corrected abstract:
<p>This paper investigates the performance of the Random Forest Classification model for stock market trading. The performance is evaluated first by evaluating its ability to make predictions about the direction of future stock price changes, and secondly by evaluating simulated returns based on a classification-based trading strategy presented in this paper. The performance is evaluated using the five largest companies included in the OMXS30 index, during the period ranging from 2013-02-12 to 2022-12-30.</p>
<p>We show that with the implementation presented in this paper, the Random Forest Classification model was able to predict whether or not a stock was going to increase in value during the coming day with an accuracy higher than 50% for all stocks included in this study. Furthermore, we show that the active trading strategy presented in this paper generated higher returns and higher risk-adjusted returns than the passive investment in the stocks underlying the strategy. Therefore, we conclude <em>(i)</em> that a Random Forest Classification model can be used to provide valuable insight on publicly traded stocks, and <em>(ii)</em> that it is probably possible to create a profitable trading strategy based on a Random Forest Classifier, but that this requires a more sophisticated implementation than the one presented in this paper.</p>

Note removed the "\" before "%" and fixed the italics
----------------------------------------------------------------------
In diva2:1341400 
abstract is: 
<p>Early leakage detection is one way to make water distribution networks more efficient and sustainable. The goal of this project is to investigate the possibility to detect leakages in water distribution networks with the help of artificial neural networks. The project is based on real data collected from Stockholm water distribution network and is focusing on how to present the prediction from neural networks in an intellectual manner, by implementing and analyzing the need of a time filter. The study shows that it might be possible to detect leakages in a water distribution network with a binary accuracy of 87\%. An improvement to 98\% was achieved by implementing a time filter.</p>

corrected abstract:
<p>Early leakage detection is one way to make water distribution networks more efficient and sustainable. The goal of this project is to investigate the possibility to detect leakages in water distribution networks with the help of artificial neural networks. The project is based on real data collected from Stockholm water distribution network and is focusing on how to present the prediction from neural networks in an intellectual manner, by implementing and analyzing the need of a time filter. The study shows that it might be possible to detect leakages in a water distribution network with a binary accuracy of 87%. An improvement to 98% was achieved by implementing a time filter.</p>

Note removed the "\" before "%"
----------------------------------------------------------------------
In diva2:1698380 
abstract is: 
<p>A key component of traffic models for simulating bicycle traffic focuses on capturing the interactions between cyclists and the cycling infrastructure. One of the most relevant features of the infrastructure that has a significant impact in bicycle traffic is the gradient of a bicycle path. Bicycle traffic simulations are a rather uninvestigated topic since historically, most focus on simulations has been on cars. However, bicycle simulations are an important tool to further investigate and understand cyclist’s behaviour. Therefore, the main objective of this thesis is to investigate and simulate free-riding behavior of cyclists in connection to the gradient, particularly on downhills. To do so, trajectory data of cyclists traveling on a downhill with a maximum gradient of 5.5\% are analysed to identify the impact of gradient on the speed and acceleration. The data received needed processing in order to be useful. This included filtering of the trajectories and excluding the data from cyclists which could not to be regarded as free-riding. As a result, a linear correlation is found between pedaling power and the gradient that can be used in microscopic bicycle traffic simulation. Based on this knowledge regarding this linearity, the approach used for modeling the gradient’s effect on the pedaling power is linear regression. The model can be developed in various ways, so instead of only choosing one model, several were developed and compared against each other. These models are then used for the simulation. The results indicate that the simulation captures well the impact of downhill gradients in a population of cyclists as it reproduces similar speed profiles to the ones observed. Therefore, it can be concluded that a power-based model is suitable for simulating free-riding behaviour of cyclists traveling in downhills.</p>

corrected abstract:
<p>A key component of traffic models for simulating bicycle traffic focuses on capturing the interactions between cyclists and the cycling infrastructure. One of the most relevant features of the infrastructure that has a significant impact in bicycle traffic is the gradient of a bicycle path. Bicycle traffic simulations are a rather uninvestigated topic since historically, most focus on simulations has been on cars. However, bicycle simulations are an important tool to further investigate and understand cyclist’s behaviour. Therefore, the main objective of this thesis is to investigate and simulate free-riding behavior of cyclists in connection to the gradient, particularly on downhills. To do so, trajectory data of cyclists traveling on a downhill with a maximum gradient of 5.5% are analysed to identify the impact of gradient on the speed and acceleration. The data received needed processing in order to be useful. This included filtering of the trajectories and excluding the data from cyclists which could not to be regarded as free-riding. As a result, a linear correlation is found between pedaling power and the gradient that can be used in microscopic bicycle traffic simulation. Based on this knowledge regarding this linearity, the approach used for modeling the gradient’s effect on the pedaling power is linear regression. The model can be developed in various ways, so instead of only choosing one model, several were developed and compared against each other. These models are then used for the simulation. The results indicate that the simulation captures well the impact of downhill gradients in a population of cyclists as it reproduces similar speed profiles to the ones observed. Therefore, it can be concluded that a power-based model is suitable for simulating free-riding behaviour of cyclists traveling in downhills.</p>
----------------------------------------------------------------------
In diva2:1656056 
abstract is: 
<p>During the last few years, several stablecoins have emerged, in the form of algorithmic asset-collateralized cryptocurrencies designed to maximize price stability by expanding and contracting the circulating supply of coins. But over time as the relative prices of the underlying collateral assets change, there is a need to rebalance the underlying collateral portfolio of a stablecoin in order to maintain price stability and match the value-weighted target portfolio distribution. A critical part of the rebalancing process is therefore to find a pricing method that approximates the true prices of each collateral asset in order to accurately perform rebalancing.</p><p>For decentralized exchanges, which adopt a stablecoin as a basis for transactions, a pricing method requires a high degree of accuracy and responsiveness as well as security mechanisms in order to prevent attacks on the system from malicious actors. As the available pricing methods in the form of trusted feeds and external oracle networks have several issues related to governance, costs, complexity, and security we instead propose an internal oracle, that can estimate the true prices by only using information already available on the blockchain.</p><p>We consider a current proposal for an internal on-chain oracle and evaluate it with regards to performance and security by modeling and simulating an exchange. We identify several critical problems that need to be resolved and prove mathematically that an attacker can manipulate the pricing method without taking any trading loss. </p><p>Based on this analysis we propose new requirements for an internal oracle and formulate three new alternative pricing methods. In particular, we utilize statistical learning and regression analysis on simulated data in order to train and optimize the pricing models that minimizes the expected error. </p><p>Our results indicate that all three pricing methods significantly outperform the initial proposal with regards to security and accuracy with up to 56\%. Furthermore, an overall high system performance motivates our conclusion that an on-chain internal oracle has the potential to replace an external oracle. We conclude by noting that factors such as ownership and trading volume can be used as parameters in the pricing method, in order to reduce the risk of attacks even further.</p>

corrected abstract:
<p>During the last few years, several stablecoins have emerged, in the form of algorithmic asset-collateralized cryptocurrencies designed to maximize price stability by expanding and contracting the circulating supply of coins. But over time as the relative prices of the underlying collateral assets change, there is a need to rebalance the underlying collateral portfolio of a stablecoin in order to maintain price stability and match the value-weighted target portfolio distribution. A critical part of the rebalancing process is therefore to find a pricing method that approximates the true prices of each collateral asset in order to accurately perform rebalancing.</p><p>For decentralized exchanges, which adopt a stablecoin as a basis for transactions, a pricing method requires a high degree of accuracy and responsiveness as well as security mechanisms in order to prevent attacks on the system from malicious actors. As the available pricing methods in the form of trusted feeds and external oracle networks have several issues related to governance, costs, complexity, and security we instead propose an internal oracle, that can estimate the true prices by only using information already available on the blockchain.</p><p>We consider a current proposal for an internal on-chain oracle and evaluate it with regards to performance and security by modeling and simulating an exchange. We identify several critical problems that need to be resolved and prove mathematically that an attacker can manipulate the pricing method without taking any trading loss.</p><p>Based on this analysis we propose new requirements for an internal oracle and formulate three new alternative pricing methods. In particular, we utilize statistical learning and regression analysis on simulated data in order to train and optimize the pricing models that minimizes the expected error.</p><p>Our results indicate that all three pricing methods significantly outperform the initial proposal with regards to security and accuracy with up to 56%. Furthermore, an overall high system performance motivates our conclusion that an on-chain internal oracle has the potential to replace an external oracle. We conclude by noting that factors such as ownership and trading volume can be used as parameters in the pricing method, in order to reduce the risk of attacks even further.</p>

Note removed the "\" before "%" and removed spaced before end of paragraph
----------------------------------------------------------------------
In diva2:1795589 
abstract is: 
<p>The calibration of model parameters is a crucial step in the process of valuation of complex derivatives. It consists of choosing the model parameters that correspond to the implied market data especially the call and put prices.</p><p>We discuss in this thesis the calibration strategy for the Heston model, one of the most used stochastic volatility models for pricing complex derivatives. The main problem with this model is that the asset price does not have a known probability distribution function. Thus we use either Fourier expansions through its characteristic function or Monte Carlo simulations to have access to it. We hence discuss the approximation induced by these methods and elaborate a calibration strategy with a focus on the choice of the objective function and the choice of inputs for the calibration.</p><p>We assess that the put option prices are a better input than the call prices for the optimization function. Then through a set of experiments on simulated put prices, we find that the sum of squared error performs better choice of the objective function for the differential evolution optimization. We also establish that the put option prices where the Black Scholes delta is equal to 10\%, 25\%, 50\% 75\% and 90\% gives enough in formations on the implied volatility surface for the calibration of the Heston model. We then implement this calibration strategy on real market data of Eurostoxx50 Index and observe the same distribution of errors as in the set of experiments.</p>

corrected abstract:
<p>The calibration of model parameters is a crucial step in the process of valuation of complex derivatives. It consists of choosing the model parameters that correspond to the implied market data especially the call and put prices.</p><p>We discuss in this thesis the calibration strategy for the Heston model, one of the most used stochastic volatility models for pricing complex derivatives. The main problem with this model is that the asset price does not have a known probability distribution function. Thus we use either Fourier expansions through its characteristic function or Monte Carlo simulations to have access to it. We hence discuss the approximation induced by these methods and elaborate a calibration strategy with a focus on the choice of the objective function and the choice of inputs for the calibration.</p><p>We assess that put option prices are a better input than call prices for the Differential Evolution Optimization algorithm. Then through a set of experiments on simulated put prices, we find that the sum of squared errors performs better as objective function for the differential evolution optimization than the other tested objective functions. We also establish that put option prices where the Black Scholes delta is equal to 10%, 25%, 50% 75% and 90% give enough information about the implied volatility surface for the calibration of the Heston model. We then implement this calibration strategy on real market data of Eurostoxx50 Index and observe the same distribution of errors as in the set of experiments.</p>

Note removed the "\" before "%" - also fixed some wording differences between DiVA and original
----------------------------------------------------------------------
In diva2:1567949 - missing space in title:
"Review of ElectricRoad Systems: Both a conventional and innovative technology"
==>
"Review of Electric Road Systems: Both a conventional and innovative technology"

abstract is: 
<p>A more sustainable society can be accomplished by electrifying the road transportation sector, which the year 2016 emitted 11,9 \% of the global greenhouse emissions. To accelerate this transformation, dynamic charging via electric road systems (ERS) can be implemented, allowing electric vehicles (EV) to become cheaper and achieve lower energy consumption thanks to smaller batteries. </p><p>The purpose of this thesis is to expand the knowledge of different charging technologies and present, analyze and compare various ERSs. Based on a comprehensive literature study conductive, inductive, and capacitive supply systems are found to be the main charging technologies. After explaining the physics, the dynamic charging systems using these technologies from the side, above, underneath the vehicle, and via the wheels are presented together with my concepts. </p><p>Analyzing all researched ERSs, the overhead catenary conductive system is most mature but only useful for trucks and buses. Its high efficiency is also found in conductive rail systems which can be used by all EVs but have an impact on the road. A resonant inductive coupling under the vehicle is a very practical system that has a lower impact but also a little lower efficiency, power transfer capability, and is more expensive. Placing the coupling in the wheels results in a more complex system with worse power transfer capability due to the steel belt inside the tyre, but has great development potential. The capacitive coupling in the wheels has also been reviewed and judged to have a low power transfer due to safety reasons of radiofrequency electric fields transmission.</p>

corrected abstract:
<p>A more sustainable society can be accomplished by electrifying the road transportation sector, which the year 2016 emitted 11,9 % of the global greenhouse emissions. To accelerate this transformation, dynamic charging via electric road systems (ERS) can be implemented, allowing electric vehicles (EV) to become cheaper and achieve lower energy consumption thanks to smaller batteries.</p><p>The purpose of this thesis is to expand the knowledge of different charging technologies and present, analyze and compare various ERSs. Based on a comprehensive literature study conductive, inductive, and capacitive supply systems are found to be the main charging technologies. After explaining the physics, the dynamic charging systems using these technologies from the side, above, underneath the vehicle, and via the wheels are presented together with my concepts.</p><p>Analyzing all researched ERSs, the overhead catenary conductive system is most mature but only useful for trucks and buses. Its high efficiency is also found in conductive rail systems which can be used by all EVs but have an impact on the road. A resonant inductive coupling under the vehicle is a very practical system that has a lower impact but also a little lower efficiency, power transfer capability, and is more expensive. Placing the coupling in the wheels results in a more complex system with worse power transfer capability due to the steel belt inside the tyre, but has great development potential. The capacitive coupling in the wheels has also been reviewed and judged to have a low power transfer due to safety reasons of radiofrequency electric fields transmission.</p>

Note removed the "\" before "%" and removed spaces at end of paragraphs
----------------------------------------------------------------------
 diva2:1335380 
abstract is: 
<p>Some tasks, like recognizing digits and spoken words, are simple for humans to complete yet hard to solve for computer programs. For instance the human intuition behind recognizing the number eight, ''\textit{8}'', is to identify two loops on top of each other and it turns out this is not easy to represent as an algorithm. With machine learning one can tackle the problem in a new, easier, way where the computer program learns to recognize patterns and make conclusions from them. In this bachelor thesis a digit recognizing program is implemented and the parameters of the stochastic gradient descent optimizing algorithm are analyzed based on how their effect on the computation speed and accuracy. These parameters being the learning rate $\Delta t$ and batch size $N$. The implemented digit recognizing program yielded an accuracy of around $95$ \% when tested and the time per iteration stayed constant during the training session and increased linearly with batch size. Low learning rates yielded a slower rate of convergence while larger ones yielded faster but more unstable convergence. Larger batch sizes also improved the convergence but at the cost of more computational power.</p>
mc='\\textit{8' c='\\textit{ 8'

partal corrected: diva2:1335380: <p>Some tasks, like recognizing digits and spoken words, are simple for humans to complete yet hard to solve for computer programs. For instance the human intuition behind recognizing the number eight, ''\textit{ 8}'', is to identify two loops on top of each other and it turns out this is not easy to represent as an algorithm. With machine learning one can tackle the problem in a new, easier, way where the computer program learns to recognize patterns and make conclusions from them. In this bachelor thesis a digit recognizing program is implemented and the parameters of the stochastic gradient descent optimizing algorithm are analyzed based on how their effect on the computation speed and accuracy. These parameters being the learning rate $\Delta t$ and batch size $N$. The implemented digit recognizing program yielded an accuracy of around $95$ \% when tested and the time per iteration stayed constant during the training session and increased linearly with batch size. Low learning rates yielded a slower rate of convergence while larger ones yielded faster but more unstable convergence. Larger batch sizes also improved the convergence but at the cost of more computational power.</p>

corrected abstract:
<p>Some tasks, like recognizing digits and spoken words, are simple for humans to complete yet hard to solve for computer programs. For instance the human intuition behind recognizing the number eight, ”<em>8</em>”, is to identify two loops on top of each other and it turns out this is not easy to represent as an algorithm. With machine learning one can tackle the problem in a new, easier, way where the computer program learns to recognize patterns and make conclusions from them. In this bachelor thesis a digit recognizing program is implemented and the parameters of the stochastic gradient descent optimizing algorithm are analyzed based on how their effect on the computation speed and accuracy. These parameters being the learning rate &Delta; 𝑡 and batch size 𝑁. The implemented digit recognizing program yielded an accuracy of around 95 % when tested and the time per iteration stayed constant during the training session and increased linearly with batch size. Low learning rates yielded a slower rate of convergence while larger ones yielded faster but more unstable convergence. Larger batch sizes also improved the convergence but at the cost of more computational power.</p>

Note removed the "\" before "%" and fixed some of the math symbols
----------------------------------------------------------------------
In diva2:1817059 
abstract is: 
<p>Nowadays, the forestry industry still uses heavy machinery damaging both the forest and the soil. The start-up AirForestry is currently developing a sustainable way to thin and harvest trees. With their 6.2m wide electric drone carrying a harvesting tool, they can reach, thin, cut and carry trees without the need for access roads. Naturally, the drone needs to be as lightweight as possible to increase its endurance and operation range. Therefore, the first version of the drone was manufactured before the thesis with a carbon fibre laminate. </p><p>The purpose of this thesis is to study and optimize the composite structure of the drone. The first step is to characterize the existing design through experiments and simulations using the software ANSYS. Static bending loads, free vibrations, and forced vibrations are investigated against a set of predefined design requirements. This study shows that the contact surfaces between each arm and with the motor holders have high-stress concentrations compared to the rest of the arm. This means that most of the arm can be made thinner to lessen the weight with some extra reinforcement on those problematic areas. </p><p>The second step is to optimize the laminate to decrease the weight of the structure. A preliminary optimization was made and manufactured at the beginning of the thesis with strict limitations on the choice of the lamina or available thicknesses. Similar bending and vibration experiments and simulations are conducted on the new design to compare it with the older model. While the mass was expected to decrease by about 30 $\%$, the final measured weight of the arms indicates a drop of only 15 $\%$ of the mass. The model is then optimized further with more freedom in the design variables. Several variables are successively optimized: the material choice for the laminae, the thickness then of the laminae, and the angles of the plies. The mass of the structure with the final laminate has an expected decrease in mass of 45$\%$, saving more than 12kg in total</p>

corrected abstract:
<p>Nowadays, the forestry industry still uses heavy machinery damaging both the forest and the soil. The start-up AirForestry is currently developing a sustainable way to thin and harvest trees. With their 6.2m wide electric drone carrying a harvesting tool, they can reach, thin, cut and carry trees without the need for access roads. Naturally, the drone needs to be as lightweight as possible to increase its endurance and operation range. Therefore, the first version of the drone was manufactured before the thesis with a carbon fibre laminate.</p><p>The purpose of this thesis is to study and optimize the composite structure of the drone. The first step is to characterize the existing design through experiments and simulations using the software ANSYS. Static bending loads, free vibrations, and forced vibrations are investigated against a set of predefined design requirements. This study shows that the contact surfaces between each arm and with the motor holders have high-stress concentrations compared to the rest of the arm. This means that most of the arm can be made thinner to lessen the weight with some extra reinforcement on those problematic areas.</p><p>The second step is to optimize the laminate to decrease the weight of the structure. A preliminary optimization was made and manufactured at the beginning of the thesis with strict limitations on the choice of the lamina or available thicknesses. Similar bending and vibration experiments and simulations are conducted on the new design to compare it with the older model. While the mass was expected to decrease by about 30 %, the final measured weight of the arms indicates a drop of only 15 % of the mass. The model is then optimized further with more freedom in the design variables. Several variables are successively optimized: the material choice for the laminae, the thickness then of the laminae, and the angles of the plies. The mass of the structure with the final laminate has an expected decrease in mass of 45%, saving more than 12kg in total</p>

Note removed the "\" before "%" - not the last paragraph does not have terminal punctuation in the original
----------------------------------------------------------------------
In diva2:1375766  - document was scanned
abstract is: 
<p>In this thesis I use seven data-sets of weighted, directed graphs and present them as weighted, directed k-simplicial complexes. Then, I analyse the topological properties of each data-set in question using Flagser by \cite{flagser} and the cluster at the TU Darmstadt. I then proceed to run two versions of an alternative to the PageRank algorithm. One version contracting and deleting every node visited, and the other deleting only those visited nodes with less than 3 neighbours. I record snapshots of how the data-sets throughout the run of the algorithm, and compute the same topological properties computed before the run. I compare the changes in their homology to understand how the algorithm alters the topology of the graph. I also run several runs of the algorithms to get an idea of how the average graph looks like after the algorithm has been run. I record their new topological properties to find a correlation between the performance of the algorithm and the change in the topology of the graphs.</p>

corrected abstract:
<p>We provide a first test and overview of an alternative to Google's PageRank algorithm using persistent homology and topological data analysis. First, we present a data-set consisting of weighted, directed graphs. Then, we explain how we can transform said graphs to weighted, directed simplicial complexes. After that, we compute the persistent homology of the resulting complexes using Flagser by Daniel Luetgehetmann. Once we have the starting homological data, we introduce the two versions of the algorithm. One version contracts and deletes every node visited. The other deletes only visited nodes with less than 3 neighbours. We run the algorithm and re-compute persistent homology. Using the results, we analyse changes in the homology to determine how the algorithm changes the topological structure of our data-sets. Subsequently, we try to answer the question on whether there is a correlation between the changes in the persistent homology and the effectiveness and speed of the algorithm.</p>

Note - the DIVA abstract was very different from the original
----------------------------------------------------------------------
In diva2:1880210 
abstract is: 
<p>This thesis investigates the application of the Black-Scholes model for pricing long-maturity options, primarily utilizing historical data on S\&amp;P500 options. It compares prices computed with the Black-Scholes formula to actual market prices and critically examines the validity of the Black-Scholes model assumptions over long time frames. The assumptions mainly focused on are the constant volatility assumption, the assumption of normally distributed returns, the constant interest rate assumption and the no transaction cost assumption. The results show that the differences between computed prices and actual prices decrease as options get closer to maturity. They also show that several of the Black-Scholes model assumptions are not entirely realistic over long time frames. The conclusion of the thesis is that there are several limitations to the Black-Scholes model when it comes to pricing long-maturity options.</p>

corrected abstract:
<p>This thesis investigates the application of the Black-Scholes model for pricing long-maturity options, primarily utilizing historical data on S&amp;P500 options. It compares prices computed with the Black-Scholes formula to actual market prices and critically examines the validity of the Black-Scholes model assumptions over long time frames. The assumptions mainly focused on are the constant volatility assumption, the assumption of normally distributed returns, the constant interest rate assumption and the no transaction cost assumption. The results show that the differences between computed prices and actual prices decrease as options get closer to maturity. They also show that several of the Black-Scholes model assumptions are not entirely realistic over long time frames. The conclusion of the thesis is that there are several limitations to the Black-Scholes model when it comes to pricing long-maturity options.</p>

Note removed the "\" before "&"
----------------------------------------------------------------------
In diva2:1880272 
abstract is: 
<p>In recent decades, advancements in laser technology has made the creation of femtosecond lasers possible. This is a special type of laser where the laser beam consists of repeated high energy light bursts just a few hundred femtoseconds long as opposed to of the continuous laser beams found in every common laser pointer. The short pulse duration paired with the high energy in each burst results in a significant peak power, making the laser capable of processing materials in a way that a regular laser cannot. However, the large size and weight of the machines capable of producing femtosecond laser beams often require them to remain stationary. To utilize the laser beam for processing, precise redirection is necessary. In this report, we describe our process of converting a regular CNC machine into a laser processing station and present our findings from writing on glass, metal foil and KTP crystals. The machine is capable of following CAD instructions with micrometer precision to alter, inscribe and cut a range of materials. Processing was conducted with green ($\lambda$=514 nm) as well as infrared laser ($\lambda$=1028 nm), yielding better results for the latter. The finished laser setup could be used to repeatedly and reliably process all materials, with promising results on KTP when combined with chemical etching.</p>

corrected abstract:
<p>In recent decades, advancements in laser technology has made the creation of femtosecond lasers possible. This is a special type of laser where the laser beam consists of repeated high energy light bursts just a few hundred femtoseconds long as opposed to of the continuous laser beams found in every common laser pointer. The short pulse duration paired with the high energy in each burst results in a significant peak power, making the laser capable of processing materials in a way that a regular laser cannot. However, the large size and weight of the machines capable of producing femtosecond laser beams often require them to remain stationary. To utilize the laser beam for processing, precise redirection is necessary. In this report, we describe our process of converting a regular CNC machine into a laser processing station and present our findings from writing on glass, metal foil and KTP crystals. The machine is capable of following CAD instructions with micrometer precision to alter, inscribe and cut a range of materials. Processing was conducted with green (λ=514 nm) as well as infrared laser (λ=1028 nm), yielding better results for the latter. The finished laser setup could be used to repeatedly and reliably process all materials, with promising results on KTP when combined with chemical etching.</p>

Note replaced the LaTeX equation by the symbol for &lambda;
----------------------------------------------------------------------
In diva2:1444046 
abstract is: 
<p>Developing countermeasures dispenser systems requires many and careful tests. When it comes to testing products with pyrotechnics, testing can often be very complicated and expensive. This might lead to no testing at all due to time or resource shortages. Products to be used in the military requires further testing and even more thorough reviews to meet the strict demands placed on the products. In order to enable more tests of pyrotechnic flares in the countermeasures industry, this degree project aims to increase the ability to perform tests without the need for pyrotechnic means. This was done by designing, constructing and optimizing a recoil emulator, an apparatus that imitates the force-time curve obtained by pyrotechnic flares without the need of pyrotechnic means.</p><p>The construction of the recoil emulator was conducted at a department that develops countermeasure systems at Saab Surveillance in Järfälla. The apparatus aims to be used in the future for testing and verification of product series of countermeasures dispenser systems. The design of the apparatus was based on a test result provided by a flare manufacturer of an arbitrarily chosen flare, typical in the countermeasures industry. Based on the provided test result, three measures were chosen that together describe the fundamental and essential characteristic parts of the recoil motion behavior of pyrotechnic flares. These three measures are in the thesis called \textit{recoil measure} and defined as the Peak Recoil, the Impulse, and the Peak-Width.</p><p>To be able to verify the recoil emulator, the three recoil measures were implemented in an error model, which was based on the squares of error. In order to make the emulator imitate the desired recoil motion behavior as pleasant as possible, the error model was implemented in an optimization model. By minimizing the error of data points from each of the recoil measures obtained from the real test provided by the manufacturer with results obtained from the recoil emulator, the emulator was verified and optimized accordingly.</p><p>Results showed that the selected design of the recoil emulator resulted in a force-time curve that principally mimics the curve given by the real tests. The conclusion from the project was, therefore, that it is possible perform tests on countermeasures systems without pyrotechnics when considering the impact of recoil. Further development of this thesis could be to improve the construction of the recoil emulator and perform more research on flares and damping materials. Other future work could be to implement the emulator in existing test and validation processes at companies within the countermeasure industry.</p>

corrected abstract:
<p>Developing countermeasures dispenser systems requires many and careful tests. When it comes to testing products with pyrotechnics, testing can often be very complicated and expensive. This might lead to no testing at all due to time or resource shortages. Products to be used in the military requires further testing and even more thorough reviews to meet the strict demands placed on the products. In order to enable more tests of pyrotechnic flares in the countermeasures industry, this degree project aims to increase the ability to perform tests without the need for pyrotechnic means. This was done by designing, constructing and optimizing a recoil emulator, an apparatus that imitates the force-time curve obtained by pyrotechnic flares without the need of pyrotechnic means.</p><p>The construction of the recoil emulator was conducted at a department that develops countermeasure systems at Saab Surveillance in Järfälla. The apparatus aims to be used in the future for testing and verification of product series of countermeasures dispenser systems. The design of the apparatus was based on a test result provided by a flare manufacturer of an arbitrarily chosen flare, typical in the countermeasures industry. Based on the provided test result, three measures were chosen that together describe the fundamental and essential characteristic parts of the recoil motion behavior of pyrotechnic flares. These three measures are in the thesis called <em>recoil measure</em> and defined as the Peak Recoil, the Impulse, and the Peak-Width./p><p>To be able to verify the recoil emulator, the three recoil measures were implemented in an error model, which was based on the squares of error. In order to make the emulator imitate the desired recoil motion behavior as pleasant as possible, the error model was implemented in an optimization model. By minimizing the error of data points from each of the recoil measures obtained from the real test provided by the manufacturer with results obtained from the recoil emulator, the emulator was verified and optimized accordingly.</p><p>Results showed that the selected design of the recoil emulator resulted in a force-time curve that principally mimics the curve given by the real tests. The conclusion from the project was, therefore, that it is possible perform tests on countermeasures systems without pyrotechnics when considering the impact of recoil. Further development of this thesis could be to improve the construction of the recoil emulator and perform more research on flares and damping materials. Other future work could be to implement the emulator in existing test and validation processes at companies within the countermeasure industry.</p>

Note added the italics from the LaTeX command
----------------------------------------------------------------------
In diva2:1670637 
abstract is: 
<p>Electric buses often have batteries installed on the roof structure to have a better space utilization. This increases the height of the centre of gravity of the vehicle, affecting its roll stability. The existing vehicle setup uses an anti-roll bar to provide the roll stiffness. However, increasing the roll stiffness of the anti-roll bar for providing the required roll stability of an electric bus is limited due to the increase in weight of the anti-roll bar, its material properties and the design constraints. An alternate for the air suspension system is identified through a literature study. The identified system, an interconnected hydro-pneumatic suspension, is modelled analytically and compared to the air spring system. Multi-body simulations are performed to understand the roll performance.\par The thesis work also estimates the system's energy efficiency, and the feasibility of packaging the system within the existing vehicle architecture is studied in CAD software. The roll gradient of the vehicle is shown to improve compared to the existing air-spring system. The study also find that implementation of hydraulic-interconnected system can result in reduction of the unsprung mass of the vehicle.</p>
mc='performance.\\par' c='performance. \\par'

partal corrected: diva2:1670637: <p>Electric buses often have batteries installed on the roof structure to have a better space utilization. This increases the height of the centre of gravity of the vehicle, affecting its roll stability. The existing vehicle setup uses an anti-roll bar to provide the roll stiffness. However, increasing the roll stiffness of the anti-roll bar for providing the required roll stability of an electric bus is limited due to the increase in weight of the anti-roll bar, its material properties and the design constraints. An alternate for the air suspension system is identified through a literature study. The identified system, an interconnected hydro-pneumatic suspension, is modelled analytically and compared to the air spring system. Multi-body simulations are performed to understand the roll performance. \par The thesis work also estimates the system's energy efficiency, and the feasibility of packaging the system within the existing vehicle architecture is studied in CAD software. The roll gradient of the vehicle is shown to improve compared to the existing air-spring system. The study also find that implementation of hydraulic-interconnected system can result in reduction of the unsprung mass of the vehicle.</p>

corrected abstract:
<p>Electric buses often have batteries installed on the roof structure to have a better space utilization. This increases the height of the centre of gravity of the vehicle, affecting its roll stability. The existing vehicle setup uses an anti-roll bar to provide the roll stiffness. However, increasing the roll stiffness of the anti-roll bar for providing the required roll stability of an electric bus is limited due to the increase in weight of the anti-roll bar, its material properties and the design constraints. An alternate for the air suspension system is identified through a literature study. The identified system, an interconnected hydro-pneumatic suspension, is modelled analytically and compared to the air spring system. Multi-body simulations are performed to understand the roll performance.</p><p>The thesis work also estimates the system's energy efficiency, and the feasibility of packaging the system within the existing vehicle architecture is studied in CAD software. The roll gradient of the vehicle is shown to improve compared to the existing air-spring system. The study also find that implementation of hydraulic-interconnected system can result in reduction of the unsprung mass of the vehicle.</p>

Note replaced the \par with a </p><p>
----------------------------------------------------------------------
In diva2:1707010 
abstract is: 
<p>Density functional theory (DFT) calculations of polyethylene (PE) HVDC cable insulation have been performed for systems containing four different chemical impurities: acetophenone, cumene, $\alpha$-methyl styrene and $\alpha$-cumyl alcohol. Systems were generated by molecular dynamics (MD) equilibration at four different temperatures relevant for cable insulation applications: 277 K, 293 K, 343 K and 363 K. With the goal of gaining better measure of variations in hole and electron traps energies, four initial configurations were also stochastically generated at each temperature, which yielded four different final configurations after equilibration. The counterpoise correction scheme was implemented for DFT calculations, by distributing ghost atoms thought any empty pockets of space in between the PE chains. The PBE functional was selected for DFT simulations. The resulting band gaps were in agreement with those of earlier GGA-based studies, and thus lower by 3 eV than empirical band gaps. For all impurities, the first HOMO state and the first two LUMO states were generally located on the impurity molecule, forming one hole trap and two electron traps, but certain configurations generated increased electron trap numbers, or eliminated hole traps. No dependence could be derived between temperature and trap depth for either electron or hole traps. Mean electron trap energies were largely in agreement with results from earlier studies, they were deepest for acetophenone, and they varied by as much as 0.6 eV between different configurations. Hole traps are universally shallow and vary by up to 0.7 eV between configurations, and are similar in depth for all impurities. Results suggest that electron trap depths correlate with the presence of molecular features such as oxygen atoms and conjugated double bonds. The dependence of trap depth on the spatial configuration of the impurity molecule suggests that results could be improved by more precise quantum mechanical treatment of the dynamics of the impurity.</p>

corrected abstract:
<p>Density functional theory (DFT) calculations of polyethylene (PE) HVDC cable insulation have been performed for systems containing four different chemical impurities: acetophenone, cumene, α-methyl styrene and α-cumyl alcohol. Systems were generated by molecular dynamics (MD) equilibration at four different temperatures relevant for cable insulation applications: 277 K, 293 K, 343 K and 363 K. With the goal of gaining better measure of variations in hole and electron traps energies, four initial configurations were also stochastically generated at each temperature, which yielded four different final configurations after equilibration. The counterpoise correction scheme was implemented for DFT calculations, by distributing ghost atoms thought any empty pockets of space in between the PE chains. The PBE functional was selected for DFT simulations. The resulting band gaps were in agreement with those of earlier GGA-based studies, and thus lower by 3 eV than empirical band gaps. For all impurities, the first HOMO state and the first two LUMO states were generally located on the impurity molecule, forming one hole trap and two electron traps, but certain configurations generated increased electron trap numbers, or eliminated hole traps. No dependence could be derived between temperature and trap depth for either electron or hole traps. Mean electron trap energies were largely in agreement with results from earlier studies, they were deepest for acetophenone, and they varied by as much as 0.6 eV between different configurations. Hole traps are universally shallow and vary by up to 0.7 eV between configurations, and are similar in depth for all impurities. Results suggest that electron trap depths correlate with the presence of molecular features such as oxygen atoms and conjugated double bonds. The dependence of trap depth on the spacial configuration of the impurity molecule suggests that results could be improved by more precise quantum mechanical treatment of the dynamics of the impurity.</p>

Note replace the latex for \alpha with the character "α"
Note spelling error in original:
"spacial" should be "spatial"
----------------------------------------------------------------------
In diva2:1839766 
abstract is: 
<p>This thesis explores the spherically symmetric gravitational collapse of a massless scalar field in a minimally modified gravity theory denoted VCDM (V replaces $\Lambda$ in the $\Lambda$CDM abbreviation), a class of theories propagating the same degrees of freedom as general relativity at the expense of broken 4D diffeomorphism invariance. Numerical evolution of the equations of motion reveals that for small initial scalar profile amplitudes, no black hole forms from the collapse. However, for larger amplitudes, collapse leads to an apparent horizon's formation in finite time. Outside the horizon, the solution resembles the Schwarzschild geometry, while inside, the lapse function continues to decrease toward zero, implying the formation of a singularity/foliation breakdown. This suggests a need for a UV completion for the theory inside the horizon. Despite this, VCDM can describe the entire time evolution of the universe outside the black hole horizon without requiring knowledge of such a UV completion.</p>

corrected abstract:
<p>This thesis explores the spherically symmetric gravitational collapse of a massless scalar field in a minimally modified gravity theory denoted VCDM (V replaces Λ in the ΛCDM abbreviation), a class of theories propagating the same degrees of freedom as general relativity at the expense of broken 4D diffeomorphism invariance. Numerical evolution of the equations of motion reveals that for small initial scalar profile amplitudes, no black hole forms from the collapse. However, for larger amplitudes, collapse leads to an apparent horizon's formation in finite time. Outside the horizon, the solution resembles the Schwarzschild geometry, while inside, the lapse function continues to decrease toward zero, implying the formation of a singularity/foliation breakdown. This suggests a need for a UV completion for the theory inside the horizon. Despite this, VCDM can describe the entire time evolution of the universe outside the black hole horizon without requiring knowledge of such a UV completion.</p>

Note replace \Lambda with "Λ"
----------------------------------------------------------------------
In diva2:1817008 
abstract is: 
<p>This study investigates methods for assessing threats in space. Space services are crucial to both civilian and military capabilities, and a loss of such systems could have severe consequences. Space systems are exposed to various types of threats. To ensure the benefits of space-based applications, protect space assets, improve security, and maintain the space environment, it is crucial to assess threats in space. This thesis focuses on co-orbital antagonistic threats arising from satellites that are capable of performing precision manoeuvres. These satellites could either perform physical attacks or perform operations such as inspection, eavesdropping, or disruption on other satellites. Lambert's problem can be utilised for calculating orbital transfers. By solving the problem iteratively over a range of values of when the transfer is executed and the transfer time, it is possible to detect when a transfer is feasible. This can be used to assess when a satellite can pose a threat to a target. The calculations of orbital transfers are improved by the implementation of a genetic algorithm. The algorithm can solve for both direct transfers to the target and transfers using multiple impulses. Furthermore, a genetic algorithm, called NSGA-II, which can handle multiple objective functions is also analysed. The implemented methods show the potential of being employed to assess threats, especially for direct transfers where a single impulse is executed to transfer to a target. In this case, it is possible to identify threats based on the satellite's $\Delta v$ budget. However, when additional impulses are introduced it becomes more complicated. It is more difficult to estimate when an attack is more likely to commence. The implemented methods show potential, but further research is required in order to develop a robust method to assess co-orbital threats. </p><p>The conducted analysis has highlighted a few aspects that are crucial for assessing co-orbital threats. Information about the $\Delta v$ budget of the satellite that potentially could pose a threat must be available. Furthermore, space surveillance and tracking capabilities are essential to detect orbital changes, which can be vital to perform counter-operations in the event of an attack</p>

corrected abstract:
<p>This study investigates methods for assessing threats in space. Space services are crucial to both civilian and military capabilities, and a loss of such systems could have severe consequences. Space systems are exposed to various types of threats. To ensure the benefits of space-based applications, protect space assets, improve security, and maintain the space environment, it is crucial to assess threats in space. This thesis focuses on co-orbital antagonistic threats arising from satellites that are capable of performing precision manoeuvres. These satellites could either perform physical attacks or perform operations such as inspection, eavesdropping, or disruption on other satellites. Lambert's problem can be utilised for calculating orbital transfers. By solving the problem iteratively over a range of values of when the transfer is executed and the transfer time, it is possible to detect when a transfer is feasible. This can be used to assess when a satellite can pose a threat to a target. The calculations of orbital transfers are improved by the implementation of a genetic algorithm. The algorithm can solve for both direct transfers to the target and transfers using multiple impulses. Furthermore, a genetic algorithm, called NSGA-II, which can handle multiple objective functions is also analysed. The implemented methods show the potential of being employed to assess threats, especially for direct transfers where a single impulse is executed to transfer to a target. In this case, it is possible to identify threats based on the satellite's ∆𝑣 budget. However, when additional impulses are introduced it becomes more complicated. It is more difficult to estimate when an attack is more likely to commence. The implemented methods show potential, but further research is required in order to develop a robust method to assess co-orbital threats.</p><p>The conducted analysis has highlighted a few aspects that are crucial for assessing co-orbital threats. Information about the ∆𝑣 budget of the satellite that potentially could pose a threat must be available. Furthermore, space surveillance and tracking capabilities are essential to detect orbital changes, which can be vital to perform counter-operations in the event of an attack.</p>

Note replaced the latex commands with "∆𝑣"
----------------------------------------------------------------------
In diva2:1781237 
abstract is: 
<p>In this report, we aim to assess the sensitivity and 5$\sigma$ discovery potential of IceCube, the largest neutrino observatory on Earth, and compare it with prior findings. Our thesis will focus on a point source analysis, exploring the energy and declination dependencies, with particular emphasis on high-energy neutrinos. The primary objective is to establish the feasibility of detecting 5$\sigma$ evidence supporting the hypothesis that blazars serve as sources of neutrinos in the Southern sky, as suggested in a recent publication. Our findings indicate a substantial improvement in both discovery potential and sensitivity for the Southern sky in recent years. Furthermore, we highlight the increasing significance of investigating the origins of high-energy neutrinos in the Southern sky.</p>

corrected abstract:
<p>In this report, we aim to assess the sensitivity and 5σ discovery potential of IceCube, the largest neutrino observatory on Earth, and compare it with prior findings. Our thesis will focus on a point source analysis, exploring the energy and declination dependencies, with particular emphasis on high-energy neutrinos. The primary objective is to establish the feasibility of detecting 5σ evidence supporting the hypothesis that blazars serve as sources of neutrinos in the Southern sky, as suggested in a recent publication. Our findings indicate a substantial improvement in both discovery potential and sensitivity for the Southern sky in recent years. Furthermore, we highlight the increasing significance of investigating the origins of high-energy neutrinos in the Southern sky.</p>

Note replaced latex command with "σ"
----------------------------------------------------------------------
In diva2:1777317 
abstract is: 
<p>In chaos theory there are many different problems still unsolved. One of which is the optimization of infinite time average functionals on manifolds. To try one of the different tools to solve this problem we want to find stable manifolds in chaotic dynamical systems.In this thesis we find different manifolds for the Lorenz system when using a time dependent $\mu$ parameter and perform a sensitivity analysis on some of them. The existence of these manifolds are motivated numerically with the help of the shadowing lemma and extensive comparison of different numerical solvers.</p>

corrected abstract:
<p>In chaos theory there are many different problems still unsolved. One of which is the optimization of infinite time average functionals on manifolds. To try one of the different tools to solve this problem we want to find stable manifolds in chaotic dynamical systems. In this thesis we find different manifolds for the Lorenz system when using a time dependent µ parameter and perform a sensitivity analysis on some of them. The existence of these manifolds are motivated numerically with the help of the shadowing lemma and extensive comparison of different numerical solvers.</p>

Note replaced latex command with "µ"
----------------------------------------------------------------------
In diva2:664507 
abstract is: 
<p>  </p><p>This master thesis was performed at the section of Flight Mechanics and Performance at SAAB Aeronautics in Linköping as a part of my Master of Science in Engineering Physics at KTH, Stockholm. The aim of the thesis is to enable desktop simulations of missions from take-off to landing of JAS 39 Gripen.</p><p>The mission is set up by a series of task to be performed. Each tasks then link to a pilot model that controls the aircraft to perform the given task. The main part of the work has been to create these pilot models as an extension of the work by Ajdén and Backlund presented in [1].</p><p>The tasks that are simulated are, take-off, climb, turn, cruise, combat simulation, descent and landing at a given point. In order to perform these tasks both open and closed loop controls are used. To perform the landing first a path planing based on Dubins minimum path is calculated and then the nonlinear guidance logic presented by Park, Desyst and How in \cite{Park4} is implemented and used for trajectory tracking.</p><p>The results from a simulation of a test mission are presented and shows that mission simulations are possible and that the pilot models perform the intended tasks.</p><p>        </p>

corrected abstract:
<p>This master thesis was performed at the section of Flight Mechanics and Performance at SAAB Aeronautics in Linköping as a part of my Master of Science in Engineering Physics at KTH, Stockholm. The aim of the thesis is to enable desktop simulations of missions from take-off to landing of JAS 39 Gripen.</p><p>The mission is set up by a series of task to be performed. Each tasks then link to a pilot model that controls the aircraft to perform the given task. The main part of the work has been to create these pilot models as an extension of the work by Ajdén and Backlund presented in [1].</p><p>The tasks that are simulated are, take-off, climb, turn, cruise, combat simulation, descent and landing at a given point. In order to perform these tasks both open and closed loop controls are used. To perform the landing first a path planing based on Dubins minimum path is calculated and then the nonlinear guidance logic presented by Park, Desyst and How in [5] is implemented and used for trajectory tracking.</p><p>The results from a simulation of a test mission are presented and shows that mission simulations are possible and that the pilot models perform the intended tasks.</p>

Note put in the citation
----------------------------------------------------------------------
In diva2:1781262 
abstract is: 
<p>This thesis is the result of a literature study regarding the relationship between the Riemann zeta function and prime numbers. We introduce the $\zeta$ function, discussing its properties. Then, starting from Riemann's original series, we derive the Euler product formula and functional equation for $\zeta$. We then discuss finite-order integral functions and Hadamard products, as well as the Riemann $\xi$ function in order to determine some properties of the nontrivial zeros of $\zeta$. Then, we derive an integral function of the second Chebyshev function $\psi$, and use residue calculus in order to represent it as a sum over the zeros of $\zeta$. Lastly, we rewrite $\psi$ in order to get an estimate of $\pi$, proving the prime number theorem with a certain error term.</p>

corrected abstract:
<p>This thesis is the result of a literature study regarding the relationship between the Riemann zeta function and prime numbers. We introduce the ζ function, discussing its properties. Then, starting from Riemann's original series, we derive the Euler product formula and functional equation for ζ. We then discuss finite-order integral functions and Hadamard products, as well as the Riemann ξ function in order to determine some properties of the nontrivial zeros of ζ. Then, we derive an integral function of the second Chebyshev function ψ, and use residue calculus to represent it as a sum over the zeros of ζ. Lastly, we rewrite ψ to get an estimate of π, proving the prime number theorem with a certain error term.</p>

Note replaced the LaTeX command wtih the name symbol
----------------------------------------------------------------------
In diva2:1880221   - correct as is
----------------------------------------------------------------------
In diva2:1879615   - correct as is
----------------------------------------------------------------------
In diva2:1879416 
abstract is: 
<p>This thesis explores the process of porting a passive radar system from one SDR to another. Passive radar makes use of existing electromagnetic signals from sources such as TV and mobile phone towers to detect various objects. By leveraging consumer-grade SDRs, which have become increasingly accessible and powerful in recent years, new types of passive radar systems can be created. </p><p>The project involves adapting a passive radar system originally implemented on a KrakenSDR, to an AntSDR, which supports significantly higher bandwidth. This transition necessitates modifications to both the hardware and software setup to accommodate differences in firmware and sampling capabilities between the two SDRs. Through theoretical analysis and practical implementation, I detail required considerations for the passive radar systems' sampling, signal processing, and display.</p><p>The performance of the two systems in real-world scenarios is compared, focusing on their ability to detect and track aircraft in the vicinity of the Saab Järfälla site using a digital TV tower as the transmitter.Results demonstrate that the AntSDR system offers improved precision and detection capabilities due to its higher sampling rate, though challenges such as processing time and signal noise persist. This thesis underscores the feasibility and benefits of using modern SDRs for passive radar applications, while highlighting challenges and potential areas for further development.</p>
mc='transmitter.Results' c='transmitter. Results'

partal corrected: diva2:1879416: <p>This thesis explores the process of porting a passive radar system from one SDR to another. Passive radar makes use of existing electromagnetic signals from sources such as TV and mobile phone towers to detect various objects. By leveraging consumer-grade SDRs, which have become increasingly accessible and powerful in recent years, new types of passive radar systems can be created. </p><p>The project involves adapting a passive radar system originally implemented on a KrakenSDR, to an AntSDR, which supports significantly higher bandwidth. This transition necessitates modifications to both the hardware and software setup to accommodate differences in firmware and sampling capabilities between the two SDRs. Through theoretical analysis and practical implementation, I detail required considerations for the passive radar systems' sampling, signal processing, and display.</p><p>The performance of the two systems in real-world scenarios is compared, focusing on their ability to detect and track aircraft in the vicinity of the Saab Järfälla site using a digital TV tower as the transmitter. Results demonstrate that the AntSDR system offers improved precision and detection capabilities due to its higher sampling rate, though challenges such as processing time and signal noise persist. This thesis underscores the feasibility and benefits of using modern SDRs for passive radar applications, while highlighting challenges and potential areas for further development.</p>

corrected abstract:
<p>This thesis explores the process of porting a passive radar system from one SDR to another. Passive radar makes use of existing electromagnetic signals from sources such as TV and mobile phone towers to detect various objects. By leveraging consumer-grade SDRs, which have become increasingly accessible and powerful in recent years, new types of passive radar systems can be created.</p><p>The project involves adapting a passive radar system originally implemented on a KrakenSDR, to an AntSDR, which supports significantly higher bandwidth. This transition necessitates modifications to both the hardware and software setup to accommodate differences in firmware and sampling capabilities between the two SDRs. Through theoretical analysis and practical implementation, I detail required considerations for the passive radar systems' sampling, signal processing, and display.</p><p>The performance of the two systems in real-world scenarios is compared, focusing on their ability to detect and track aircraft in the vicinity of the Saab Järfälla site using a digital TV tower as the transmitter. Results demonstrate that the AntSDR system offers improved precision and detection capabilities due to its higher sampling rate, though challenges such as processing time and signal noise persist. This thesis underscores the feasibility and benefits of using modern SDRs for passive radar applications, while highlighting challenges and potential areas for further development.</p>

Note - only change deleted an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1878874 
abstract is: 
<p>Since their introduction in 1995, Support Vector Machines (SVM) have come to be a widely employed machine learning model for binary classification, owing to their explainable architecture, efficient forward inference, and good ability to generalize. A common desire, not only for SVMs but for machine learning classifiers in general, is to have the model do feature selection, using only a limited subset of the available attributes in its predictions. Various alterations to the SVM problem formulation exist that address this, and in this report we compare a range of such SVM models. We compare how the accuracy and feature selection compare between the models for different datasets, both real and synthetic, and we also investigate the impact of dataset size on the aforementioned quantities. </p><p>Our conclusions are that models trained to classify samples based on a smaller subset of features, tend to perform at a comparable level to dense models, with particular advantage when the dataset is small. Furthermore, as the training dataset grows in size, the number of selected features also increases, giving a more complex classifier when prompted with a larger data supply.</p>

corrected abstract:
<p>Since their introduction in 1995, Support Vector Machines (SVM) have come to be a widely employed machine learning model for binary classification, owing to their explainable architecture, efficient forward inference, and good ability to generalize. A common desire, not only for SVMs but for machine learning classifiers in general, is to have the model do feature selection, using only a limited subset of the available attributes in its predictions. Various alterations to the SVM problem formulation exist that address this, and in this report we compare a range of such SVM models. We compare how the accuracy and feature selection compare between the models for different datasets, both real and synthetic, and we also investigate the impact of dataset size on the aforementioned quantities.</p><p>Our conclusions are that models trained to classify samples based on a smaller subset of features, tend to perform at a comparable level to dense models, with particular advantage when the dataset is small. Furthermore, as the training dataset grows in size, the number of selected features also increases, giving a more complex classifier when prompted with a larger data supply.</p>

Note only change was to remove a space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1878717   - correct as is
......................................................................
In diva2:1876801   - correct as is
Note: no full text in DiVA
----------------------------------------------------------------------
In diva2:1876073   - correct as is
----------------------------------------------------------------------
In diva2:1875967   - correct as is
----------------------------------------------------------------------
In diva2:1873902 - the body of the thesis is a bitmap at under 72 dpi - so it cannot be OCR'd
abstract   - correct as is
----------------------------------------------------------------------
In diva2:1872789   - correct as is
----------------------------------------------------------------------
In diva2:1356906 
abstract is: 
<p>The vehicle sector is facing the biggest revolution of the last 70 years, whose main target is the depletion of emission and a steady decrease of fuel consumption. The focus in this report is the Green House Gas Emissions model (GEM) developed by the United States Environmental Protection Agency (EPA). The model is one of the newest and most complete simulation tool aimed to provide estimation of fuel consumption and CO2 emissions for the US market depending on the input inserted and the type of vehicle chosen. The usage of these tools is expected to increase in the next decades as it presents a more economical solution compared to chassis dynamometer and real driving test. Furthermore, all the environmental agencies that are developing these programs play also a key role in establishing new rules and regulations.</p><p>Scania, as one of the leading manufacturer companies of heavy duty vehicle worldwide, needs to learn about the new certification processes in US to be able to expand in this specific market. The aim of this work is to learn more about the GEM and to validate its results.</p><p>A theoretical review is performed to understand how GEM is working and how each parameter could possibly affect the outcomes. Several parameter variations on a long haulage HDV are performed with the GEM and output results presented. To validate the GEM the European simulation tool VECTO and an internal Scania worksheet for fuel consumption and emissions analysis are used as they were previously validated by means of the chassis dynamometer test. </p><p>GEM shows very similar results in this comparison with VECTO and the internal Scania worksheet.</p><p>Assessing the reliability of this programme on a larger scale could represent a key aspect for Original Equipment Manufacturers (OEMs) as they could use this simulation tool to easily and rapidly have some fuel economy results solely based on the specifications of the vehicle in analysis.</p>

corrected abstract:
<p>The vehicle sector is facing the biggest revolution of the last 70 years, whose main target is the depletion of emission and a steady decrease of fuel consumption. The focus in this report is the Green House Gas Emissions model (GEM) developed by the United States Environmental Protection Agency (EPA). The model is one of the newest and most complete simulation tool aimed to provide estimation of fuel consumption and CO<sub>2</sub> emissions for the US market depending on the input inserted and the type of vehicle chosen. The usage of these tools is expected to increase in the next decades as it presents a more economical solution compared to chassis dynamometer and real driving test. Furthermore, all the environmental agencies that are developing these programs play also a key role in establishing new rules and regulations.</p><p>Scania, as one of the leading manufacturer companies of heavy duty vehicle worldwide, needs to learn about the new certification processes in US to be able to expand in this specific market. The aim of this work is to learn more about the GEM and to validate its results.</p><p>A theoretical review is performed to understand how GEM is working and how each parameter could possibly affect the outcomes. Several parameter variations on a long haulage HDV are performed with the GEM and output results presented. To validate the GEM the European simulation tool VECTO and an internal Scania worksheet for fuel consumption and emissions analysis are used as they were previously validated by means of the chassis dynamometer test.</p><p>GEM shows very similar results in this comparison with VECTO and the internal Scania worksheet.</p><p>Assessing the reliability of this programme on a larger scale could represent a key aspect for Original Equipment Manufacturers (OEMs) as they could use this simulation tool to easily and rapidly have some fuel economy results solely based on the specifications of the vehicle in analysis.</p>

Note - only changes add subscript and removed space at end of a paragraph
Note that the original sets CO<sub>2</sub> as CO<span style="font-size: 0.8em;">2</span>
----------------------------------------------------------------------
In diva2:1698396 
abstract is: 
<p>Currently, the road cargo system with low or zero CO2 emission is under rapid development. Heavy-duty trucks with electrified driveline systems will be the workhorse of future freight. But developing such a brand new and very complex system and adapting it to various application scenarios, such as long-haul freight, city distribution or construction loading, is still a big problem, because there is no previous experience to refer to. There is no standard development procedure or constraint framework for uncertainty either. Simulation on a massive scale with thousands of truck agents will be of great use for developing such a road-cargo system.</p><p>System engineering will be the guiding methodology for this thesis project about developing a high-performance and multi-adaptive electrified driveline system. Referring to the classical V-shape development methodology, the complex concept will be divided into different levels of subsystems, from the large application scenarios to traffic simulation, driveline system simulation, electric motor and controller blocks development, and the system integration, performance verification and output of the results.</p><p>The massive scale of traffic simulation will be implemented in AnyLogic, which does not contain any accurate agent model with vehicle dynamic motion during simulation. Thus, a precise vehicle agent model needs to be developed and embedded into AnyLogic’s simulation scenario, so as to make the simulation very close to reality, and to be able to evaluate vehicle concepts as well. The driveline system will be developed in Matlab/Simulink while the information communication between them will be realised in the form of computational calculation functions through the C language program.</p><p>The development of the driveline model is also progressive. First, an equation-based full glider model was constructed. It simulates the scenario of a heavy-loaded truck driving on a steep slope (30% grade), decelerating from the initial 70 km/h to 0 km/h and then remaining stationary. The second model added the functionality of velocity input and output, enabling information exchange with AnyLogic. It will judge the real-time speed and the desired speed to decide whether to accelerate or decelerate and it uses the “Bang-Bang” control method of the electric motor. But this control mode results in a massive and frequent change in the electric motor output power, leading to extremely high energy consumption and in real life significantly shortened motor lifetime. So a powerful PI controller was introduced to the third Simulink model. The PI controller is embedded in the electric motor and it will replace the “Bang-Bang” control method. The “PID” control method provides a more stable power output so that the truck’s real-time speed can approach the target speed more smoothly. This control system can adapt to a variety of speed inputs and it can decide whether to output full power or partial power, depending on the speed difference.</p><p>The third version of the Simulink model with PI controller has been verified as an acceptable model through various inputs of different speeds, and it will be converted into a C language program to be embedded in AnyLogic for massive traffic simulation.</p>

corrected abstract:
<p>Currently, the road cargo system with low or zero CO<sub>2</sub> emission is under rapid development. Heavy-duty trucks with electrified driveline systems will be the workhorse of future freight. But developing such a brand new and very complex system and adapting it to various application scenarios, such as long-haul freight, city distribution or construction loading, is still a big problem, because there is no previous experience to refer to. There is no standard development procedure or constraint framework for uncertainty either. Simulation on a massive scale with thousands of truck agents will be of great use for developing such a road-cargo system.</p><p>System engineering will be the guiding methodology for this thesis project about developing a high-performance and multi-adaptive electrified driveline system. Referring to the classical V-shape development methodology, the complex concept will be divided into different levels of subsystems, from the large application scenarios to traffic simulation, driveline system simulation, electric motor and controller blocks development, and the system integration, performance verification and output of the results.</p><p>The massive scale of traffic simulation will be implemented in AnyLogic, which does not contain any accurate agent model with vehicle dynamic motion during simulation. Thus, a precise vehicle agent model needs to be developed and embedded into AnyLogic’s simulation scenario, so as to make the simulation very close to reality, and to be able to evaluate vehicle concepts as well. The driveline system will be developed in Matlab/Simulink while the information communication between them will be realised in the form of computational calculation functions through the C language program.</p><p>The development of the driveline model is also progressive. First, an equation-based full glider model was constructed. It simulates the scenario of a heavy-loaded truck driving on a steep slope (30% grade), decelerating from the initial 70 km/h to 0 km/h and then remaining stationary. The second model added the functionality of velocity input and output, enabling information exchange with AnyLogic. It will judge the real-time speed and the desired speed to decide whether to accelerate or decelerate and it uses the “Bang-Bang” control method of the electric motor. But this control mode results in a massive and frequent change in the electric motor output power, leading to extremely high energy consumption and in real life significantly shortened motor lifetime. So a powerful Proportional–Integral (PI) controller was introduced to the third Simulink model. The PI controller is embedded in the electric motor and it will replace the “Bang-Bang” control method. The “Proportional–Integral–Derivative (PID)” control method provides a more stable power output so that the truck’s real-time speed can approach the target speed more smoothly. This control system can adapt to a variety of speed inputs and it can decide whether to output full power or partial power, depending on the speed difference.</p><p>The third version of the Simulink model with PI controller has been verified as an acceptable model through various inputs of different speeds, and it will be converted into a C language program to be embedded in AnyLogic for massive traffic simulation.</p>

Note the original spelled out the acronyms and has a subscript
----------------------------------------------------------------------
In diva2:1737481 - missing subscript in title:
"Estimating CO2 emissions with satellite and traffic data: a Swedish practical case study"
==>
"Estimating CO<sub>2</sub> emissions with satellite and traffic data: a Swedish practical case study"

abstract is: 
<p>The large carbon footprint of industries is one of the main issues raised when talking about climate change. Active carbon monitoring methods need to be implemented to give transparency to the industry market and to spread awareness and information. This thesis investigates multiple CO2 emissions monitoring via satellite monitoring for four different industries in the EU. The pulp and paper industry was monitored in Sweden through smoke detection coming from the chimneys of factories. The CO2 aggregated emissions of 14 Swedish factories were calculated with a mean error of 12%. The metal ore roasting, and sintering industry were successfully monitored as well through smoke detection. In Sweden with an error 4.6%, and in the EU with an error 9.5 %. The production of lime and the calcination of dolomite were unsuccessfully monitored due to no suitable method found. Finally, coke industry emissions were monitored through burned gas monitoring. The CO2 emissions were correlated to the real emissions with a mean correlation coefficient of 0.64. This study took part in a public information campaign lead by a Swedish start-up, and some results were displayed in Stockholm, Sweden.</p>

corrected abstract:
<p>The large carbon footprint of industries is one of the main issues raised when talking about climate change. Active carbon monitoring methods need to be implemented to give transparency to the industry market and to spread awareness and information. This thesis investigates multiple CO<sub>2</sub> emissions monitoring via satellite monitoring for four different industries in the EU. The pulp and paper industry was monitored in Sweden through smoke detection coming from the chimneys’ factories. The CO<sub>2</sub> aggregated emissions of 14 Swedish factories were calculated with a mean error of 12%. The metal ore roasting and sintering industry were successfully monitored as well through smoke detection. In Sweden with an error 4.6%, and in the EU with an error 9.5%. The production of lime and the calcination of dolomite were unsuccessfully monitored due to no suitable method found. Finally, coke industry emissions were monitored through burned gas monitoring. The CO<sub>2</sub> emissions were correlated to the real emissions with a mean correlation coefficient of 0.64. This study took part in a public information campaign lead by a Swedish start-up, and some results were displayed in Stockholm, Sweden.</p>

Note there were small wording differences and the subscripts were missing in the DiVA entry
----------------------------------------------------------------------
In diva2:1757025 
abstract is: 
<p>One of the biggest topics of discussion in the last decade has been the climate change and the related global warming. This has rightfully contributed to people being more aware than ever about how to live a more sustainable life and to some extension know how they should act in their everyday life in order to reduce their carbon footprint. These are maybe the most important actions one could take in order to secure a healthy planet for generations to come. What is less known is how to invest in a sustainable way and how to understand if companies are operating sustainably. This thesis aims to solve this problem, enabling people to make more sustainable investment decisions through quickly analyzing the investment target’s financial performance. The result of the thesis is a model that can partly help investors to understand if a company is operating in a sustainable manner. The findings are that ROA, short for return on assets, is the most influential financial performance metric to look at when analyzing a company’s sustainable performance. Unfortunately, the model produced and suggested in this thesis can only explain 15.4% of the CO2 emissions produced by a company. That being said, there is still much to learn about how companies </p>

corrected abstract:
<p>One of the biggest topics of discussion in the last decade has been the climate change and the related global warming. This has rightfully contributed to people being more aware than ever about how to live a more sustainable life and to some extension know how they should act in their everyday life in order to reduce their carbon footprint. These are maybe the most important actions one could take in order to secure a healthy planet for generations to come. What is less known is how to invest in a sustainable way and how to understand if companies are operating sustainably. This thesis aims to solve this problem, enabling people to make more sustainable investment decisions through quickly analyzing the investment target’s financial performance. The result of the thesis is a model that can partly help investors to understand if a company is operating in a sustainable manner. The findings are that ROA, short for return on assets, is the most influential financial performance metric to look at when analyzing a company’s sustainable performance. Unfortunately, the model produced and suggested in this thesis can only explain 15.4% of the CO<sub>2</sub> emissions produced by a company. That being said, there is still much to learn about how companies </p>

Note added the subscript
----------------------------------------------------------------------
In diva2:1707861 
abstract is: 
<p>The impact of air travel on the climate, along with its increasing share in CO2 emissions have raised the demand for sustainable air travel solutions. The current aircraft technologies have seen significant improvement throughout the years. Although, the rate at which new aircraft technologies are developed can not keep up with the increased demand for air travel. Hence, a different approach to reduce the aviation’s impact on climate can be achieved by optimizing the vertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming.</p><p>Upon departure, an optimization of the vertical flight path is initiated and an optimal flight plan is suggested to the flight crew.</p><p> The fuel saving produced by the optimal flight plan is a potential saving that can only be fully achieved if the flight crew chose to fly according to the optimized flight path. However, restrictions from the Air Traffic Control, as well as the flight crew’s willingness to follow the optimized flight path can affect the achieved saving. Hence, a tool is developed in order to compute trip fuel consumption from post-flight data obtained from the Automatic Dependent Surveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start and end positions of cruise segments is successfully implemented. Two methods of calculating the fuel are implemented and compared. The first method is based on simulating the actual flight, which uses the same performance model as for the simulation of the operational flight plan trip and optimized trip. The second method is based on utilizing the ADS-B data to obtain the aircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft, hence the trip is not simulated. The results reveals that the simulation method produces flight trajectories that are comparable to the operational and optimized flight plans since they use the same model structure. However, using ADS-B data to obtain fuel consumption represents the actual flight trajectory more accurately.</p><p> Furthermore, an optimization algorithm based on the onboard Flight Management Computer is implemented. According to the results, the FMC optimization offers a sufficient optimization of the cruise phase, when compared to the OFP trip, however performs worse than the dynamic programming, which provides a global optimal solution.</p>

corrected abstract:
<p>The impact of air travel on the climate, along with its increasing share in CO<sub>2</sub> emissions have raised the demand for sustainable air travel solutions. The current aircraft technologies have seen significant improvement throughout the years. Although, the rate at which new aircraft technologies are developed can not keep up with the increased demand for air travel. Hence, a different approach to reduce the aviation’s impact on climate can be achieved by optimizing the vertical flight path in order to reduce the fuel consumption, i.e. using dynamic programming. Upon departure, an optimization of the vertical flight path is initiated and an optimal flight plan is suggested to the flight crew.</p><p>The fuel saving produced by the optimal flight plan is a potential saving that can only be fully achieved if the flight crew chose to fly according to the optimized flight path. However, restrictions from the Air Traffic Control, as well as the flight crew’s willingness to follow the optimized flight path can affect the achieved saving. Hence, a tool is developed in order to compute trip fuel consumption from post-flight data obtained from the Automatic Dependent Surveillance-Broadcast (ADS-B) surveillance technology. A method to identify the start and end positions of cruise segments is successfully implemented. Two methods of calculating the fuel are implemented and compared. The first method is based on simulating the actual flight, which uses the same performance model as for the simulation of the operational flight plan trip and optimized trip. The second method is based on utilizing the ADS-B data to obtain the aircraft speed which in return can be used as a parameter to obtain the fuel flow of the aircraft, hence the trip is not simulated. The results reveals that the simulation method produces flight trajectories that are comparable to the operational and optimized flight plans since they use the same model structure. However, using ADS-B data to obtain fuel consumption represents the actual flight trajectory more accurately.</p><p>Furthermore, an optimization algorithm based on the on-board Flight Management Computer is implemented. According to the results, the FMC optimization offers a sufficient optimization of the cruise phase, when compared to the OFP trip, however performs worse than the dynamic programming, which provides a global optimal solution.</p>

Note removed misplaced paragraph break, added missing hyphen, and added subscripts
----------------------------------------------------------------------
In diva2:561616 
abstract is: 
<p>Growing pressure on the packaging design to enhance the environmental and logistics performance of a packaging system stresses the packaging designers to search new design strategies that not only fulfill logistics requirements in the supply chain, but also reduce the CO</p><p>2emissions during the packaging life cycle.</p><p>This thesis focuses on the packaging design process and suggests some improvements by considering its logistics performance and CO</p><p>2emissions. A Green packaging development model was proposed for corrugated box design to explore the inter-dependencies that exist among compressive strength, waste and CO2emissions.</p><p>The verification of the proposed model unveils the significance of a holistic view of the packaging system in the packaging design process and reveals the importance of packaging design decisions on the logistics performance and CO</p><p>2 emissions. The thesis finally concluded that the packaging logistics performance should be considered in a packaging design process to explore the Green packaging design solution.</p>

corrected abstract:
<p>Growing pressure on the packaging design to enhance the environmental and logistics performance of a packaging system stresses the packaging designers to search new design strategies that not only fulfill logistics requirements in the supply chain, but also reduce the CO<sub>2</sub> emissions during the packaging life cycle.</p><p>This thesis focuses on the packaging design process and suggests some improvements by considering its logistics performance and CO<sub>2</sub> emissions. A Green packaging development model was proposed for corrugated box design to explore the interdependencies that exist among compressive strength, waste and CO<sub>2</sub> emissions.</p><p>The verification of the proposed model unveils the significance of a holistic view of the packaging system in the packaging design process and reveals the importance of packaging design decisions on the logistics performance and CO<sub>2</sub> emissions. The thesis finally concluded that the packaging logistics performance should be considered in a packaging design process to explore the Green packaging design solution.</p>

Note corrected "CO</p><p>2" to CO<sub>2</p>" and added missing instances, and removed an unnecessary hyphen.
----------------------------------------------------------------------
In diva2:1698107 
abstract is: 
<p>The automotive industry and the whole transport sector are currently facing the need to act against climate change. In fact, over the globe the passenger road vehicles emitted 3.6 Gt of CO2 in 2018 and the road freight vehicles emitted 2.4 Gt of CO2 [1]. These road and freight emissions represents 11% and 7% of the total CO2 emissions that year respectively [2]. One solution that has been chosen to limit and reduce the greenhouse gas (GHG) emissions from road transportation is to shift from internal combustion engine (ICE)-based vehicles to electric vehicles which will emit no GHG during operation. There are mainly two types of electric vehicles suitable for this purpose. The first one is the battery electric vehicles (BEVs) which is already commercially and industrially mature and already on the road (11.3 million in 2020 [3]). It uses large Li-ion batteries to store the energy on-board to then power the electric motors. The second one is the fuel cell electric vehicles (FCEVs) which is still being researched and whose number on the road is quite limited (34.8 thousand in 2020 [4]). Yet, this technology is suitable for many applications and especially the light commercial vehicles (LCVs). The stakes of this technology have been studied regarding the current market of LCVs in France and by comparing it to BEVs in terms of cost and mass. To better frame the development of new hydrogen LCVs, a tool has also been developed to calculate the range of such vehicles throughout the life of the project and the evolution of its specifications. The analysis of the market and the comparison between FCEVs and BEVs is not exhaustive and only some specific points have been dealt with, enough to give an overview of the main stakes of hydrogen LCVs. The tool developed is limited to simple input data as it aims to be used with little information at an early stage of the project.</p>

corrected abstract:
<p>The automotive industry and the whole transport sector are currently facing the need to act against climate change. In fact, over the globe the passenger road vehicles emitted 3.6 Gt of CO<sub>2</sub> in 2018 and the road freight vehicles emitted 2.4 Gt of CO<sub>2</sub> [1]. These road and freight emissions represents 11% and 7% of the total CO<sub>2</sub> emissions that year respectively [2]. One solution that has been chosen to limit and reduce the greenhouse gas (GHG) emissions from road transportation is to shift from internal combustion engine (ICE)-based vehicles to electric vehicles which will emit no GHG during operation. There are mainly two types of electric vehicles suitable for this purpose. The first one is the battery electric vehicles (BEVs) which is already commercially and industrially mature and already on the road (11.3 million in 2020 [3]). It uses large Li-ion batteries to store the energy on-board to then power the electric motors. The second one is the fuel cell electric vehicles (FCEVs) which is still being researched and whose number on the road is quite limited (34.8 thousand in 2020 [4]). Yet, this technology is suitable for many applications and especially the light commercial vehicles (LCVs). The stakes of this technology have been studied regarding the current market of LCVs in France and by comparing it to BEVs in terms of cost and mass. To better frame the development of new hydrogen LCVs, a tool has also been developed to calculate the range of such vehicles throughout the life of the project and the evolution of its specifications. The analysis of the market and the comparison between FCEVs and BEVs is not exhaustive and only some specific points have been dealt with, enough to give an overview of the main stakes of hydrogen LCVs. The tool developed is limited to simple input data as it aims to be used with little information at an early stage of the project.</p>

Note that the original sets CO<sub>2</sub> as CO<span style="font-size: 0.8em;">2</span>
----------------------------------------------------------------------
In diva2:754073 
abstract is: 
<p>In today’s fast growing and closely connected society, a reliable and energy efficient transportation system is more than ever desirable. Nowadays the significant part of the transportation sector’s energy demand is supplied by fossil fuels.</p><p>Improving energy efficiency in combustion engines will result in reduction of fuel consumption and CO2 emissions. A modern internal combustion engine has an efficiency of 30-45 %, where the most energy loss occurs as result of heat losses in the exhaust and cooling systems. By recovering and converting the thermal energy of a combustion engine to mechanical/electric power the efficiency of the combustion engine can be increased.</p><p>In this work the performance of a truck engine has been investigated with the aim to increase its efficiency by decreasing the heat losses with a Waste Heat Recovery (WHR) system. Intercooler, coolant system, EGR and exhaust systems have been studied for their heat loss potentials.</p><p>A model of an ideal Rankine process has been implemented in MATLAB with water as the working fluid. Experimental data sets from a Scania DC1306 and a Volvo D11 engines have served as inputs to the developed MATLAB model. The study shows that an efficiency gain of approximately 2 % can be achieved with a WHR-system where EGR cooler serves as a heat source and 2.5 % with the exhaust as heat source. The combination of both systems can provide an efficiency gain between 4-5 %.</p>

corrected abstract:
<p>In today’s fast growing and closely connected society, a reliable and energy efficient transportation system is more than ever desirable. Nowadays the significant part of the transportation sector’s energy demand is supplied by fossil fuels.</p><p>Improving energy efficiency in combustion engines will result in reduction of fuel consumption and CO<sub>2</sub> emissions. A modern internal combustion engine has an efficiency of 30-45 %, where the most energy loss occurs as result of heat losses in the exhaust and cooling systems. By recovering and converting the thermal energy of a combustion engine to mechanical/electric power the efficiency of the combustion engine can be increased.</p><p>In this work the performance of a truck engine has been investigated with the aim to increase its efficiency by decreasing the heat losses with a Waste Heat Recovery (WHR) system. Intercooler, coolant system, EGR and exhaust systems have been studied for their heat loss potentials.</p><p>A model of an ideal Rankine process has been implemented in MATLAB with water as the working fluid. Experimental data sets from a Scania DC1306 and a Volvo D11 engines have served as inputs to the developed MATLAB model. The study shows that an efficiency gain of approximately 2 % can be achieved with a WHR-system where EGR cooler serves as a heat source and 2.5 % whit the exhaust as heat source. The combination of both systems can provide an efficiency gain between 4-5 %.</p>

Note added subscript
Note spelling error:
"whit" should be "with" - error in original
----------------------------------------------------------------------
In diva2:1847362 
Note: no full text in DiVA

abstract is: 
<p>In the wake of an escalating environmental crisis, underscored by global warming and its profound impacts on our planet, the quest for innovative solutions has never been more critical. This urgency propels the exploration of new materials and sustainable innovations to revolutionize energy and environmental remediation technologies. This study delves into the performance of Zinc Oxide (ZnO) and Titanium Dioxide (TiO2) nanorods, alongside a Co-Fe Prussian Blue Analogue (PBA) nanocomposite, within photofenton-like reactors aimed at wastewater treatment and hydrogen gas production from cellulose. Through a detailed literature review and subsequent experiments, it becomes evident that ZnO nanorods, with their approximate length of 800 nm, and TiO2 nanorods, measuring around 1 µm, exhibit photocatalytic degradation capabilities in a near-neutral environment (pH = 6). Specifically, 1 mg/ml of TiO2-PBA was capable of decomposing 92% of 10 ppm of Rhodamine B in 40 minutes with the aid of just 0.5 mM of persulfate (Peroxydisulfate, PDS), while the photoreforming of rapeseed cellulose with 4 mM of persulfate results in the production of 90 mmol/gh of hydrogen. Furthermore, the study illuminates the distinctive Fenton-like behavior and proton conductivity of Co-Fe PBA nanocubes without the aid of noble metals like Platinum (Pt), underlining their critical role in advanced oxidation processes. This study reveals how semiconductor materials interact with reactive oxidative species to efficiently degrade pollutants and produce hydrogen, highlighting significant advancements in water purification and sustainable energy solutions. By advancing our understanding of these photocatalytic systems, this research contributes significantly to the global efforts aimed at environmental preservation and energy sustainability, marking a pivotal step towards mitigating the adverse effects of climate change through innovative technological advancements.</p>

corrected abstract:
<p>In the wake of an escalating environmental crisis, underscored by global warming and its profound impacts on our planet, the quest for innovative solutions has never been more critical. This urgency propels the exploration of new materials and sustainable innovations to revolutionize energy and environmental remediation technologies. This study delves into the performance of Zinc Oxide (ZnO) and Titanium Dioxide (TiO<sub>2</sub>) nanorods, alongside a Co-Fe Prussian Blue Analogue (PBA) nanocomposite, within photofenton-like reactors aimed at wastewater treatment and hydrogen gas production from cellulose. Through a detailed literature review and subsequent experiments, it becomes evident that ZnO nanorods, with their approximate length of 800 nm, and TiO<sub>2</sub> nanorods, measuring around 1 µm, exhibit photocatalytic degradation capabilities in a near-neutral environment (pH = 6). Specifically, 1 mg/ml of TiO<sub>2</sub>-PBA was capable of decomposing 92% of 10 ppm of Rhodamine B in 40 minutes with the aid of just 0.5 mM of persulfate (Peroxydisulfate, PDS), while the photoreforming of rapeseed cellulose with 4 mM of persulfate results in the production of 90 mmol/gh of hydrogen. Furthermore, the study illuminates the distinctive Fenton-like behavior and proton conductivity of Co-Fe PBA nanocubes without the aid of noble metals like Platinum (Pt), underlining their critical role in advanced oxidation processes. This study reveals how semiconductor materials interact with reactive oxidative species to efficiently degrade pollutants and produce hydrogen, highlighting significant advancements in water purification and sustainable energy solutions. By advancing our understanding of these photocatalytic systems, this research contributes significantly to the global efforts aimed at environmental preservation and energy sustainability, marking a pivotal step towards mitigating the adverse effects of climate change through innovative technological advancements.</p>

Note - added subscripts
----------------------------------------------------------------------
In diva2:1670559   - correct as is
----------------------------------------------------------------------
In diva2:802086 
abstract is: 
<p>Road transport emission levels are at an all-time low and post Euro VI regulations are now up for discussion. A literature study of unregulated diesel emissions in Europe; CO2, N2O, NO2, CH4 and aldehydes has been made to determine the effects and importance of the emissions in today´s heavy-duty vehicles. This work aims to give better knowledge of the emissions with fundamental information about each emission’s formation, environmental effects, health effects, measuring methods, and reduction methods. Also examined is the possibility of limiting these emissions and what policies can be enforced in any future legislative directives.</p><p>The greenhouse gas emissions, CO2, N2O and CH4, from road transport are getting a lot of attention since they are hugely responsible for an increase of the global temperature. CO2 will clearly be the focus of future regulations. It is the most abundant emission and is the main cause of global warming. Reduction is best achieved through more fuel efficient vehicles but regulations and political means will also be needed to lower CO2 levels in the atmosphere. The European Commission have therefore agreed in 2014 to come up with a plan to cut their CO2 emissions from road transport. The most important ozone depleting substance today and the most potent of the greenhouse gases is N2O which has a GWP of 298. It is mainly produced by aftertreatment systems and its formation is highly dependent on temperature. CH4 is a regulated emission for CNG but not for diesel where the levels are much lower. It has a GWP of 34 and is plays a big role in global warming. Although it is an important emission to examine, the levels of CH4 from diesel vehicles today are negligible. In modern diesel vehicles NO2 emissions come from platinum catalysed DOCs and DPFs. NO2 is used for DPF regeneration and causes respiratory problems as well as contributing heavily to ozone formation and smog pollution. By adopting a better urea dosing strategy and choosing DPF coating material with less platinum NO2 can be reduced. Aldehydes are found in low concentrations in diesel but more in alternative fuels such as ethanol, and are important to study because of their carcinogenic properties and large contribution smog pollution. Studies have shown that the most abundant forms of aldehydes are formaldehyde and acetaldehyde for almost all fuel types, and that they can be reduced with efficient catalysts and high quality fuel. Different measurement techniques are used to analyse each of these mentioned emissions but their low levels require more accurate instruments with greater level of detail for measuring the substances.</p>

corrected abstract:
<p>Road transport emission levels are at an all-time low and post Euro VI regulations are now up for discussion. A literature study of unregulated diesel emissions in Europe; CO<sub>2</sub>, N<sub>2</sub>O, NO<sub>2</sub>, CH<sub>4</sub> and aldehydes has been made to determine the effects and importance of the emissions in today´s heavy-duty vehicles. This work aims to give better knowledge of the emissions with fundamental information about each emission’s formation, environmental effects, health effects, measuring methods, and reduction methods. Also examined is the possibility of limiting these emissions and what policies can be enforced in any future legislative directives.</p><p>The greenhouse gas emissions, CO<sub>2</sub>, N<sub>2</sub>O and CH<sub>4</sub>, from road transport are getting a lot of attention since they are hugely responsible for an increase of the global temperature. CO<sub>2</sub> will clearly be the focus of future regulations. It is the most abundant emission and is the main cause of global warming. Reduction is best achieved through more fuel efficient vehicles but regulations and political means will also be needed to lower CO<sub>2</sub> levels in the atmosphere. The European Commission have therefore agreed in 2014 to come up with a plan to cut their CO<sub>2</sub> emissions from road transport. The most important ozone depleting substance today and the most potent of the greenhouse gases is N<sub>2</sub>O which has a GWP of 298. It is mainly produced by aftertreatment systems and its formation is highly dependent on temperature. CH<sub>4</sub> is a regulated emission for CNG but not for diesel where the levels are much lower. It has a GWP of 34 and is plays a big role in global warming. Although it is an important emission to examine, the levels of CH<sub>4</sub> from diesel vehicles today are negligible. In modern diesel vehicles NO<sub>2</sub> emissions come from platinum catalysed DOCs and DPFs. NO<sub>2</sub> is used for DPF regeneration and causes respiratory problems as well as contributing heavily to ozone formation and smog pollution. By adopting a better urea dosing strategy and choosing DPF coating material with less platinum NO<sub>2</sub> can be reduced. Aldehydes are found in low concentrations in diesel but more in alternative fuels such as ethanol, and are important to study because of their carcinogenic properties and large contribution smog pollution. Studies have shown that the most abundant forms of aldehydes are formaldehyde and acetaldehyde for almost all fuel types, and that they can be reduced with efficient catalysts and high quality fuel. Different measurement techniques are used to analyse each of these mentioned emissions but their low levels require more accurate instruments with greater level of detail for measuring the substances.</p>

Note added missing subscripts
----------------------------------------------------------------------
In diva2:1342310 
abstract is: 
<p>In general, mankind has a need for transports. A significant part of these transports in Sweden is executed by car. For travels and transports that are too long or too time consuming for public transport, walking and/or bicycling, the need for the car within the Swedish society will persist. Travels by any car today is associated with significant carbon dioxide equivalent emissions - therefore it is desirable to use cars with minimal emissions, with regard to the vehicles entire life cycle. This report develops a method to carrying out a life cycle analysis (LCA) for a car with regards to a few data points about the vehicle. To achieve this, a literature review is carried out in the area to obtain data on emissions in the five life stages of material production, manufacturing, use, service and recycling for a general car, the variables being certain data points. These data points are then paired with the cars data to approximate their emissions. Two passenger car categories are chosen: sedan and SUV, within each five engine types are analysed: gasoline, diesel, HEV, PHEV and BEV. Also, a petrol and a diesel version of a used car that is considered to be common in the used market is analysed. The intention is to get an indication of which type of fuel has the lowest emissions in a life cycle, and to examine whether a new or used car has the lowest emissions. According to the results in this report, at an average mileage and lifetime and for the chosen vehicles, BEV is proven to be the vehicle with the lowest CO2e emissions over its entire life cycle. This is the true both for the sedan- and the SUV-category, assuming a maximum of one battery change. Then, in ascending order regarding the emission of CO2e follows PHEV, HEV, used diesel, new diesel, used gasoline and new gasoline. At an annual mileage above the average, and with the chosen vehicles within this report, the used diesel is proven to be the vehicle with the lowest emissions. For an annual mileage higher than the average, the vehicle within the selected vehicles with the lowest emission of CO2e is instead the BEV. This applies to both categories. One of the strengths of this reports method is its simplicity; With a few data points that can easily be adapted to a specific vehicle and driving habit, an approximation of the vehicle's net emission can be produced. Its main drawback is that many simplifications is made throughout the calculations, so the approximation becomes rough. However, this can be counteracted by replacing the general data used in the function with specific data for a particular vehicle, where such data is known. One problem is that the data used is sometimes old enough to be considered not completely accurate and usually not explicitly applicable to Sweden. More research in the field, and more locally specific research, is therefore desirable. A label similar to the energy label which ranks a vehicle's climate impact in the various life-parts separately would facilitate consumers to make a more climate-smart choice based on their own driving habits and would provide a more fair verdict regarding the emissions than just, as it is today, the energy use during the vehicles use phase.</p>

corrected abstract:
<p>In general, mankind has a need to transport themselves. In Sweden a significant part of this need is met via car. For travels and transports that are too long or too time consuming for public transport, walking or bicycling, the need for the car within the Swedish society will persist. Travels by any car today is associated with significant climate effect – therefore it is desirable to use cars with minimal emissions, with regard to the vehicles’ entire life cycle.</p><p>This report develops a method to carrying out an approximation of a life cycle analysis (LCA) for a car with regards to a few data points about the vehicle. To achieve this, a literature review is carried out in the area to obtain data on emissions in the five life stages of material production, manufacturing, use, service, and recycling for a general car, the variables being certain data points. These data points are then paired with the cars data to approximate their emissions. Two passenger car categories are chosen: sedan and SUV, within each five drive-train types are analysed: gasoline, diesel, HEV, PHEV and BEV.  Included are also a petrol and a diesel version of a used car that is considered to be common in the used market. The intention is to get an indication of which type of fuel has the lowest emissions in a life cycle, and to investigate how used cars compare to newly manufactured in terms of CO2e emissions.</p><p>According to the results in this report, at an average annual driving distance of 12,110 km and a lifespan of 17 years, BEV closely followed by PHEV are the types with the lowest CO2e emissions over their entire life cycles. This is the true both for the sedan and the SUV category, assuming a maximum of one battery change. Two other cases are tested, at an annual driving distance lower than the average, typically the need for drivers in metropolitan areas, PHEV is proven to be the vehicle with the lowest CO2e emissions while BEV has the highest CO2e emissions. At an annual driving distance higher than the average, typical for the average diesel car driver, BEV is proven to be the vehicle with the lowest CO2e emissions, which applies to both categories.</p><p>One of the strengths of this report’s method is its simplicity; with a few data points that can easily be adapted to a specific vehicle and driving habit, an approximation of the vehicle's net emission can be produced. Its main drawback is that many simplifications are made throughout the calculations, so the approximation becomes rough. However, this can be counteracted by replacing the general data used in the function with specific data for a particular vehicle, where such data is known.</p><p>One problem is that the data used is sometimes old enough to be considered not completely accurate and usually not explicitly applicable to Sweden. More research in the field, and more locally specific research, is therefore desirable. A label similar to the energy label which ranks a vehicle's climate impact in the various life-parts separately would facilitate consumers to make a more climate-smart choice based on their own driving habits and would provide a more fair verdict regarding the emissions than just, as it is today, the energy use during the vehicles use phase.</p>

Note changes in wording between DiVA and original
Note that the original does not set the 2 as a subscript in "CO2e"
----------------------------------------------------------------------
In diva2:892095   - correct as is
----------------------------------------------------------------------
In diva2:1721327 
abstract is: 
<p>Developments in computer vision has sought to design deep neural networks which trained on a large set of images are able to generate high quality artificial images which share semantic qualities with the original image set. A pivotal shift was made with the introduction of the generative adversarial network (GAN) by Goodfellow et al.. Building on the work by Goodfellow more advanced models using the same idea have shown great improvements in terms of both image quality and data diversity. GAN models generate images by feeding samples from a vector space into a generative neural network. The structure of these so called latent vector samples show to correspond to semantic similarities of their corresponding generated images. In this thesis the DCGAN model is trained on a novel data set consisting of image sequences of the growth process of basil plants from germination to harvest. We evaluate the trained model by comparing the DCGAN performance on benchmark data sets such as MNIST and CIFAR10 and conclude that the model trained on the basil plant data set achieved similar results compared to the MNIST data set and better results in comparison to the CIFAR10 data set. To argue for the potential of using more advanced GAN models we compare the results from the DCGAN model with the contemporary StyleGAN2 model. We also investigate the latent vector space produced by the DCGAN model and confirm that in accordance with previous research, namely that the DCGAN model is able to generate a latent space with data specific semantic structures. For the DCGAN model trained on the data set of basil plants, the latent space is able to distinguish between images of early stage basil plants from late stage plants in the growth phase. Furthermore, utilizing the sequential semantics of the basil plant data set, an attempt at generating an artificial growth sequence is made using linear interpolation. Finally we present an unsuccessful attempt at visualising the latent space produced by the DCGAN model using a rudimentary approach at inverting the generator network function.</p>

corrected abstract:
<p>Developments in computer vision has sought to design deep neural networks which trained on a large set of images are able to generate high quality artificial images which share semantic qualities with the original image set. A pivotal shift was made with the introduction of the generative adversarial network (GAN) by Goodfellow et al. [1]. Building on the work by Goodfellow more advanced models using the same idea have shown great improvements in terms of both image quality and data diversity. GAN models generate images by feeding samples from a vector space into a generative neural network. The structure of these so called latent vector samples show to correspond to semantic similarities of their corresponding generated images [2]. In this thesis the DCGAN model [2] is trained on a novel data set consisting of image sequences of the growth process of basil plants from germination to harvest. We evaluate the trained model by comparing the DCGAN performance on benchmark data sets such as MNIST and CIFAR10 and conclude that the model trained on the basil plant data set achieved similar results compared to the MNIST data set and better results in comparison to the CIFAR10 data set. To argue for the potential of using more advanced GAN models we compare the results from the DCGAN model with the contemporary StyleGAN2 model. We also investigate the latent vector space produced by the DCGAN model and confirm that in accordance with previous research, namely that the DCGAN model is able to generate a latent space with data specific semantic structures. For the DCGAN model trained on the data set of basil plants, the latent space is able to distinguish between images of early stage basil plants from late stage plants in the growth phase. Furthermore, utilizing the sequential semantics of the basil plant data set, an attempt at generating an artificial growth sequence is made using linear interpolation. Finally we present an unsuccessful attempt at visualising the latent space produced by the DCGAN model using a rudimentary approach at inverting the generator network function.</p>

Note added missing citations.
----------------------------------------------------------------------
In diva2:919841 
abstract is: 
<p>By continued advances in mechanical strength of press hardened boron alloyed steel (22MnB5) weight savings can be achieved by the use of less material. However, with reduced material thickness stiffness problems such as buckling arises and the high strength of the material can’t fully be taken advantage of. A solution to the stiffness problem could be to create a sandwich structure using 22MnB5 steel.</p><p>To create a sandwich structure the faces have to be joined to the core. Work has been done concerning various methods of joining both coated and uncoated 22Mnb5 after being press hardened. A more direct approach would be to implement a joining process in the hot stamping line prior to forming and quenching. A joining method suitable for this task is Controlled Atmosphere Brazing (CAB), which today is used for both aluminum and steel brazing and could be implemented in a hot stamping furnace. Using a tube furnace with continuous gas flow of N2 overlap braze joints were produced on both AlSi-coated 22MnB5 (USIBOR® 1500P AS150) and plain 22MnB5. Material combination evaluated included two different brazing foil and two different flux paste in combination with various heat cycles. Both one-step braze cycles with brazing directly at austenite temperature and two-step braze cycles with brazing at a lower temperature followed by heat treatment at austenite temperature were developed. Evaluation of braze joint strength was done using tensile testing and the same was done to evaluate the coating strength of USIBOR® 1500P AS150 after heat treatment using adhesive to create an overlap joint. An adhesion pull-off test was used to determine USIBOR® 1500P  AS150 coating strength after various heat cycles. Scanning Electron Microscope (SEM) with Energy Dispersive Spectrometry (EDS) was used to investigate amount of Fe-diffusion from substrate into both coating and joint due to heat cycles and to determine phases connected to fracture location of joints. With NiCr-based brazing foil joints between plain 22MnB5 were produced with a braze time of 10 minutes at 950° C that had an avg. shear strength of 20 MPa.</p><p>Brazing above the liquidus temperature of the filler material for 5 minutes with a higher furnace temperature showed no decrease in shear strength of joints, but reduced time in furnace by 16 minutes giving a total furnace time of 9 minutes. Best results for joints between USIBOR® 1500P AS150 substrates were achieved using a two-step brazing cycle. Brazing was done at 593° C for 35 minutes with AlSi12 brazing foil and flux paste recommended for aluminum brazing. Afterwards specimens where heated for 4 minutes above austenite temperature and had an avg. a shear strength of 5,4 MPa. EDS-analysis showed that fracture in braze joints and in coating of USIBOR® 1500P AS150 was connected to the intermetallic phase Al5Fe2 as well as Fe-diffusion from substrate was higher than in as received conventionally press hardened USIBOR® 1500P AS150. Adhesion pull-off tests indicated that the heat cycles used in this study significantly reduced coating strength compared to as received conventionally press hardened USIBOR® 1500P AS150.</p>

corrected abstract:
<p>By continued advances in mechanical strength of press hardened boron alloyed steel (22MnB5) weight savings can be achieved by the use of less material. However, with reduced material thickness stiffness problems such as buckling arises and the high strength of the material can’t fully be taken advantage of. A solution to the stiffness problem could be to create a sandwich structure using 22MnB5 steel.</p><p>To create a sandwich structure the faces have to be joined to the core. Work has been done concerning various methods of joining both coated and uncoated 22Mnb5 after being press hardened. A more direct approach would be to implement a joining process in the hot stamping line prior to forming and quenching.</p><p>A joining method suitable for this task is Controlled Atmosphere Brazing (CAB), which today is used for both aluminum and steel brazing and could be implemented in a hot stamping furnace. Using a tube furnace with continuous gas flow of N<sub>2</sub> overlap braze joints were produced on both AlSi-coated 22MnB5 (USIBOR® 1500P AS150) and plain 22MnB5. Material combination evaluated included two different brazing foil and two different flux paste in combination with various heat cycles. Both one-step braze cycles with brazing directly at austenite temperature and two-step braze cycles with brazing at a lower temperature followed by heat treatment at austenite temperature were developed. Evaluation of braze joint strength was done using tensile testing and the same was done to evaluate the coating strength of USIBOR® 1500P AS150 after heat treatment using adhesive to create an overlap joint. An adhesion pull-off test was used to determine USIBOR® 1500P AS150 coating strength after various heat cycles. Scanning Electron Microscope (SEM) with Energy Dispersive Spectrometry (EDS) was used to investigate amount of Fe-diffusion from substrate into both coating and joint due to heat cycles and to determine phases connected to fracture location of joints.</p><p>With NiCr-based brazing foil joints between plain 22MnB5 were produced with a braze time of 10 minutes at 950° C that had an avg. shear strength of 20 MPa. Brazing above the liquidus temperature of the filler material for 5 minutes with a higher furnace temperature showed no decrease in shear strength of joints, but reduced time in furnace by 16 minutes giving a total furnace time of 9 minutes. Best results for joints between USIBOR® 1500P AS150 substrates were achieved using a two-step brazing cycle. Brazing was done at 593° C for 35 minutes with AlSi12 brazing foil and flux paste recommended for aluminum brazing. Afterwards specimens where heated for 4 minutes above austenite temperature and had an avg. a shear strength of 5,4 MPa. EDS-analysis showed that fracture in braze joints and in coating of USIBOR® 1500P AS150 was connected to the intermetallic phase Al5Fe2 as well as Fe-diffusion from substrate was higher than in as received conventionally press hardened USIBOR® 1500P AS150. Adhesion pull-off tests indicated that the heat cycles used in this study significantly reduced coating strength compared to as received conventionally press hardened USIBOR® 1500P AS150.</p>

Note fixed paragraph breaks and added subscript
----------------------------------------------------------------------
In diva2:1728803 
abstract is: 
<p>The fuel cladding is an essential component in the defence-in-depth strategy for nuclear safety. Its integrity and durability are therefore critical for maintaining acceptable safety conditions. However, the integrity of the cladding can be compromised during normal operation due to corrosion and hydriding. To ensure a sufficient level of safety, design and safety criteria have been established to limit oxidation and hydriding. EDF has various multiphysics software tools at its disposal to ensure that these criteria are met. One such tool, CYRANO3, uses oxide thickness measurements from the beginning of the French nuclear industry to model corrosion and hydriding. This study aims to improve CYRANO3 by expanding its validation database and improving its models.</p><p>The first part of the study focuses on improving the CYRANO3 database by providing a more comprehensive understanding of normal corrosion in a pressurized water reactor, allowing the models to be recalibrated to better represent actual corrosion behaviour. </p><p>In the second part, a deeper analysis is conducted to improve the models and increase knowledge of the parameters that influence corrosion. This analysis highlights the significance of temperature and power as input parameters, which will affect the accuracy of CYRANO3 results. Additionally, this study has identified areas for further improvement, including modifications to the implemented corrosion models and a better understanding of the assumptions made about input data.</p>

corrected abstract:
<p>The fuel cladding is an essential component in the defence-in-depth strategy for nuclear safety. Its integrity and durability are therefore critical for maintaining acceptable safety conditions. However, the integrity of the cladding can be compromised during normal operation due to corrosion and hydriding. To ensure a sufficient level of safety, design and safety criteria have been established to limit oxidation and hydriding. EDF has various multiphysics software tools at its disposal to ensure that these criteria are met. One such tool, CYRANO3, uses oxide thickness measurements from the beginning of the French nuclear industry to model corrosion and hydriding. This study aims to improve CYRANO3 by expanding its validation database and improving its models.</p><p>The first part of the study focuses on improving the CYRANO3 database by providing a more comprehensive understanding of normal corrosion in a pressurized water reactor, allowing the models to be recalibrated to better represent actual corrosion behaviour.</p><p>In the second part, a deeper analysis is conducted to improve the models and increase knowledge of the parameters that influence corrosion. This analysis highlights the significance of temperature and power as input parameters, which will affect the accuracy of CYRANO3 results. Additionally, this study has identified areas for further improvement, including modifications to the implemented corrosion models and a better understanding of the assumptions made about input data.</p>

Note - only change was to remove an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:693569   - correct as is
----------------------------------------------------------------------
In diva2:1163182   - correct as is
----------------------------------------------------------------------
In diva2:1872323 
abstract is: 
<p>This report introduces a signal discrimination framework for particle physics processes, including a novel ensemble learning method using multiple machine learning models. The framework is tested with a signal region of the Higgs boson decay channel <em>H → WW∗ → lνlν</em> with two or more jets. The final state consists of leptons with the same flavor but opposite electrical charges, and the Higgs bosons are produced by Vector-boson fusion (VBF). The background region consists of the three largest processes with the same final state without originating from the signal process. Multiple models are trained and evaluated on Monte Carlo samples corresponding to a subset of the full Run 2 dataset of proton-proton collisions recorded by the ATLAS experiment at CERN’s Large Hadron Collider (LHC). The analysis in this report shows that ensemble methods improve background rejection leading to increased discrimination between the signal and background region compared with individual machine learning models.</p>

corrected abstract:
<p>This report introduces a signal discrimination framework for particle physics processes, including a novel ensemble learning method using multiple machine learning models. The framework is tested with a signal region of the Higgs boson decay channel 𝐻 → 𝑊𝑊<sup>∗</sup> → ℓνℓν with two or more jets. The final state consists of leptons with the same flavor but opposite electrical charges, and the Higgs bosons are produced by Vector-boson fusion (VBF). The background region consists of the three largest processes with the same final state without originating from the signal process. Multiple models are trained and evaluated on Monte Carlo samples corresponding to a subset of the full Run 2 dataset of proton-proton collisions recorded by the ATLAS experiment at CERN’s Large Hadron Collider (LHC). The analysis in this report shows that ensemble methods improve background rejection leading to increased discrimination between the signal and background region compared with individual machine learning models.</p>

Note - rather than use italics, I put the formula in with unicode
----------------------------------------------------------------------
In diva2:1871652 
abstract is: 
<p>This thesis project has been conducted during a five-month research exchange visit to the Space Structure Dynamics and Control research group at University College Dublin. This report presents the design, development, and validation of a nanosatellite attitude control testbed. The testbed was designed to replicate the microgravity conditions of space by utilising an air bearing, enabling single-axis rotational motion for a 1U CubeSat-sized nanosatellite. The novel aspect of this research is the inclusion of two-degree-of-freedom, lumped-mass flexible appendages on either side of the nanosatellite, emulating a lightweight, flexible space structure. These flexible appendages were designed based on the stiffness characteristics of a deployable CubeSat solar array system found in existing literature, with exaggerated motion to amplify the measurable effects of various control approaches. The central focus of this project was the development of an avionics stack closely resembling CubeSat attitude control boards. The stack uses an STM32 microcontroller as the primary attitude control computer, and a suite of off the shelf breakout boards for sensors and wireless telemetry systems. Power, serial and I2C buses connect the attitude control board and the onboard computer board. A reaction wheel actuator controls the Euler heading attitude. The testbed was designed as an experimental platform for validating control algorithms developed through a model-based approach. Integration with the Simulink Embedded Coder toolbox allows for the compilation of Simulink models into C code, facilitating direct execution on the testbed. The testbed’s physical construction involves 3D printed ABS components, with the inclusion of load cells to measure disturbance torques from the excited flexible appendages. Results from validation experiments show that a simple PID controller causes significant excitation in the flexible appendages during a slew manoeuvre. However, the introduction of an input shaped attitude profile tailored to the natural frequency of the appendages successfully reduced the measured appendage excitation by 50%. Conversely, the force impedance wave based control approach did not show a reduction in appendage excitation, but shows promise for further developments in future work. In conclusion, the testbed has successfully achieved its predefined project objectives, albeit requiring further refinement, particularly in the telemetry down-link system. It is recommended that future work focuses on enhancement of the telemetry system, and validation of a model based approach to controller design.</p>

corrected abstract:
<p>This thesis project has been conducted during a five-month research exchange visit to the Space Structure Dynamics and Control research group at University College Dublin. This report presents the design, development, and validation of a nanosatellite attitude control testbed. The testbed was designed to replicate the microgravity conditions of space by utilising an air bearing, enabling single-axis rotational motion for a 1U CubeSat-sized nanosatellite. The novel aspect of this research is the inclusion of two-degree-of-freedom, lumped-mass flexible appendages on either side of the nanosatellite, emulating a lightweight, flexible space structure. These flexible appendages were designed based on the stiffness characteristics of a deployable CubeSat solar array system found in existing literature, with exaggerated motion to amplify the measurable effects of various control approaches. The central focus of this project was the development of an avionics stack closely resembling CubeSat attitude control boards. The stack uses an STM32 microcontroller as the primary attitude control computer, and a suite of off the shelf breakout boards for sensors and wireless telemetry systems. Power, serial and I<sup>2</sup>C buses connect the attitude control board and the onboard computer board. A reaction wheel actuator controls the Euler heading attitude. The testbed was designed as an experimental platform for validating control algorithms developed through a model-based approach. Integration with the Simulink Embedded Coder toolbox allows for the compilation of Simulink models into C code, facilitating direct execution on the testbed. The testbed’s physical construction involves 3D printed ABS components, with the inclusion of load cells to measure disturbance torques from the excited flexible appendages. Results from validation experiments show that a simple PID controller causes significant excitation in the flexible appendages during a slew manoeuvre. However, the introduction of an input shaped attitude profile tailored to the natural frequency of the appendages successfully reduced the measured appendage excitation by 50%. Conversely, the force impedance wave based control approach did not show a reduction in appendage excitation, but shows promise for further developments in future work. In conclusion, the testbed has successfully achieved its predefined project objectives, albeit requiring further refinement, particularly in the telemetry down-link system. It is recommended that future work focuses on enhancement of the telemetry system, and validation of a model based approach to controller design.</p>

Note - only change adding the missing superscript
----------------------------------------------------------------------
In diva2:1861338 
abstract is: 
<p>This thesis evaluates the efficacy of Physics-Informed Neural Networks (PINNs) in simulating fluid dynamics challenges, focusing on the Burgers' equation and the lid-driven cavity problem, to develop a robust PINN framework for nuclear engineering applications such as the Sustainable Nuclear Energy Research In Sweden (SUNRISE) project. The research compares various PINN models to traditional Computational Fluid Dynamics (CFD) simulations to enhance predictive accuracy and computational efficiency for reactor design.</p><p>The study analyses and optimises diverse PINN configurations, employing automatic and numerical differentiation techniques and their integrative approaches, while investigating the incorporation of advanced artificial viscosity methods to augment model robustness and address limitations of standalone PINN methods.</p><p>Results show that enhanced PINN strategies achieve superior accuracy in solving the Burgers' equation and the lid-driven cavity problem at increased Reynolds numbers. For the Burgers' equation, one method with artificial viscosity achieved a Mean Squared Error (MSE) of 1.19⨉10⁻³. For the lid-driven cavity problem at Re 1000, another method without artificial viscosity yielded MSEs of 2.27⨉10⁻⁴, 9.54⨉10⁻⁵, and 1.81⨉10⁻⁵ for u, v, and p, respectively. These advancements highlight the potential of PINNs in nuclear engineering applications, particularly in tackling flow-accelerated corrosion and erosion in lead-cooled fast reactors within the SUNRISE project.</p>

corrected abstract:
<p>This thesis evaluates the efficacy of Physics-Informed Neural Networks (PINNs) in simulating fluid dynamics challenges, focusing on the Burgers' equation and the lid-driven cavity problem, to develop a robust PINN framework for nuclear engineering applications such as the Sustainable Nuclear Energy Research In Sweden (SUNRISE) project. The research compares various PINN models to traditional Computational Fluid Dynamics (CFD) simulations to enhance predictive accuracy and computational efficiency for reactor design.</p><p>The study analyses and optimises diverse PINN configurations, employing automatic and numerical differentiation techniques and their integrative approaches while investigating the incorporation of advanced artificial viscosity methods to augment model robustness and address limitations of standalone PINN methods.</p><p>Results show that enhanced PINN strategies achieve superior accuracy in solving the Burgers' equation and the lid-driven cavity problem at increased Reynolds numbers. For the Burgers' equation, one method with artificial viscosity achieved a Mean Squared Error (MSE) of 1.19 × 10<sup>−3</sup>. For the lid-driven cavity problem at Re 1000, another method without artificial viscosity yielded MSEs of 2.27 × 10<sup>-4</sup>, 9.54 × 10<sup>-5</sup>, and 1.81 × 10<sup>-5</sup> for 𝑢, 𝑣, and 𝑝, respectively. These advancements highlight the potential of PINNs in nuclear engineering applications, particularly in tackling flow-accelerated corrosion and erosion in lead-cooled fast reactors within the SUNRISE project.</p>

Note set the superscript with <sup>xxxx</sup> rather than superscript characters. Changed the large "⨉" to "×" to match the original
----------------------------------------------------------------------
In diva2:1752114 
abstract is: 
<p>The proximity of Supernova 1987A provides a great opportunity to study the aftermath of the impact of the ejecta with the dense clumps (observed as ”hot spots”) located in the equatorial circumstellar ring. This thesis examines the properties of the clumps by using Hubble Space Telescope optical imaging taken with the F625W and F675W broad filters between 1994 and 2022. The centroids and widths of the spots are measured by a 2-dimensional fitting of their intensities. These are subsequently used to determine the evolution of the spots.</p><p>The analysis shows that the spots are radially expanding initially with velocities from 140 km s⁻¹ up to ∼ 1700 km s⁻¹. This wide range of velocities could imply a wide range of densities for the clumps. There is evidence that the spots are slowing down after ∼ 7000 days, which might be a consequence of the blast wave leaving the ring. The acceleration of the spots remains undetectable, indicating that it might be happening in faster times than the observational time scale of one year. Data further suggest that the brightest spots are spatially resolved with projected diameters of (3 − 5) × 10¹⁶ cm. It is hinted that the spots are increasing in size after the impact, suggesting that new gas is being swept up, though the dissolution of the spots might also be responsible for this effect. A rough mass estimate for the resolved spots of order 10⁻² M⊙ is inferred from the measured widths and an upper limit of 1.6 M⊙ on the mass of the shocked gas in the clumps in the whole ring. These estimates though depend on the geometry and composition idealizations used.</p>

corrected abstract:
<p>The proximity of Supernova 1987A provides a great opportunity to study the aftermath of the impact of the ejecta with the dense clumps (observed as ”hot spots”) located in the equatorial circumstellar ring. This thesis examines the properties of the clumps by using Hubble Space Telescope optical imaging taken with the F625W and F675W broad filters between 1994 and 2022. The centroids and widths of the spots are measured by a 2-dimensional fitting of their intensities. These are subsequently used to determine the evolution of the spots.</p><p>The analysis shows that the spots are radially expanding initially with velocities from 140 km s<sup>-1</sup> up to ∼ 1700 km s<sup>-1</sup>. This wide range of velocities could imply a wide range of densities for the clumps. There is evidence that the spots are slowing down after ∼ 7000 days, which might be a consequence of the blast wave leaving the ring. The acceleration of the spots remains undetectable, indicating that it might be happening in faster times than the observational time scale of one year. Data further suggest that the brightest spots are spatially resolved with projected diameters of (3 − 5) × 10<sup>16</sup> cm. It is hinted that the spots are increasing in size after the impact, suggesting that new gas is being swept up, though the dissolution of the spots might also be responsible for this effect. A rough mass estimate for the resolved spots of order 10<sup>−2</sup> M<sub>⊙</sub> is inferred from the measured widths and an upper limit of 1.6 M<sub>⊙</sub> on the mass of the shocked gas in the clumps in the whole ring. These estimates though depend on the geometry and composition idealizations used.</p>

Note set the superscript with <sup>xxxx</sup> rather than superscript characters. 
Also added the subscripts.
----------------------------------------------------------------------
In diva2:1861024   - correct as is
Note: no full text in DiVA
----------------------------------------------------------------------
In diva2:1852457   - correct as is
Note: no full text in DiVA
----------------------------------------------------------------------
In diva2:1640963 
abstract is: 
<p>A superconducting nanowire single-photon detector (SNSPD) is an emerging, and today commercially available technology, for photon-counting and quantum cryptography. Yet, the photon detection event is not fully understood and current modeling efforts require substantial computational resources which motivates studies of simpler models. </p><p>This thesis introduces a model for vortex dynamics in thin-layered superconductors, such as SNSPDs, using a simplified approach, which leads to a 2D Coulomb gas model where the vortices are modeled as electrostatic charges. The model is carefully constructed from the method of images to describe a wire with open boundary conditions and an applied supercurrent. Subsequently, equilibrium and non-equilibrium properties are sampled with the Metropolis-Hastings algorithm and further analyzed and discussed.</p><p>The suggested model is shown to be effective and successfully reproduces expected SNSPD behavior; most importantly critical behavior and voltage pulses which are directly measured during detection events. In conclusion, a 2D Coulomb gas model can be a preferred alternative for modeling vortex dynamics in SNSPDs at a small computational cost, motivating further development and studies.</p>

corrected abstract:
<p>A superconducting nanowire single-photon detector (SNSPD) is an emerging, and today commercially available technology, for photon-counting and quantum cryptography. Yet, the photon detection event is not fully understood and current modeling efforts require substantial computational resources which motivates studies of simpler models.</p><p>This thesis introduces a model for vortex dynamics in thin-layered superconductors, such as SNSPDs, using a simplified approach, which leads to a 2D Coulomb gas model where the vortices are modeled as electrostatic charges. The model is carefully constructed from the method of images to describe a wire with open boundary conditions and an applied supercurrent. Subsequently, equilibrium and non-equilibrium properties are sampled with the Metropolis-Hastings algorithm and further analyzed and discussed.</p><p>The suggested model is shown to be effective and successfully reproduces expected SNSPD behavior; most importantly critical behavior and voltage pulses which are directly measured during detection events. In conclusion, a 2D Coulomb gas model can be a preferred alternative for modeling vortex dynamics in SNSPDs at a small computational cost, motivating further development and studies.</p>

Note - only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:812003 
abstract is: 
<p>The purpose of this thesis is to develop an algorithm which solves the Vehicle Relocation Problem in the One-Way Car-Sharing (VRLPOWCS) as fast as possible. The problem describes the task of relocating the cars to areas with the largest demand. The chauffeurs who relocate the cars are transported by shuttle buses. Each car is assigned an individual relocation utility. The objective is to find shuttle tours that maximise in a given time the relocation utility while balancing the distribution of the cars. The VRLPOWCS is formulated as a mixed integer linear program. Since this problem is NP<em>-complete </em>we choose the branch-and-cut method to solve it. Using additional cutting planes – which exploit the structure of the VRLPOWCS – we enhance this method. Tests on real data show that this extended algorithm can solve the VRLPOWCS faster.</p>

corrected abstract:
<p>The purpose of this thesis is to develop an algorithm which solves the Vehicle Relocation Problem in the One-Way Car-Sharing (VRLPOWCS) as fast as possible. The problem describes the task of relocating the cars to areas with the largest demand. The chauffeurs who relocate the cars are transported by shuttle buses. Each car is assigned an individual relocation utility. The objective is to find shuttle tours that maximise in a given time the relocation utility while balancing the distribution of the cars. The VRLPOWCS is formulated as a mixed integer linear program. Since this problem is ℕℙ-<em>complete</em> we choose the branch-and-cut method to solve it. Using additional cutting planes – which exploit the structure of the VRLPOWCS – we enhance this method. Tests on real data show that this extended algorithm can solve the VRLPOWCS faster.</p>

Note only changes: replace "NP" with "ℕℙ" and mobe the start and end of the italics.
----------------------------------------------------------------------
In diva2:562079 - unnessary period at end of title:
"A Brief Study of the Multifractal Model of Asset Returns."
==>
"A Brief Study of the Multifractal Model of Asset Returns"

abstract   - correct as is 
----------------------------------------------------------------------
In diva2:1282822 
abstract is: 
<p>This thesis addresses the problem with minimizing the cost of labor shifts for the workforce at an airport. The cost of idle time is specifically a difficulty for the employer. With idle time means time that it is not a break at the same time as there is no work task to be performed. This originates from big variations in the traffic flow which lead to the workload to be characterized by peaks and valleys. This situation has increased the demand among airport service companies for efficient staff schedules. Even small reductions of the idle time mean considerable savings for the employer. This thesis uses authentic data from an international airport in Europe. The method used to solve the task is an algorithm based on column generation. The mathematical model used has a high flexibility and handles breaks, multi-activity, such as boarding, and non-splittable tasks, in other words tasks that has to be performed by one employee in one shift. The subproblem is a binary integer program that generates feasible shifts following certain rules and is solved using a commercial solver. The results have shown possible improvements. In the best test scenario, the idle time is reduced to 4.7 percent of the total worktime. There is room for improvement of the model and the results. One possible improvement is to reduce the running time of the program which also could lead to improved results.</p>

corrected abstract:
<p>This thesis addresses the problem with minimizing the cost of labor shifts for the workforce at an airport. The cost of idle time is specifically a difficulty for the employer. With idle time means time that it is not a break at the same time as there is no work task to be performed. This originates from big variations in the traffic flow which lead to the workload to be characterized by peaks and valleys.</p><p>This situation has increased the demand among airport service companies for efficient staff schedules. Even small reductions of the idle time mean considerable savings for the employer.</p><p>This thesis uses authentic data from an international airport in Europe.</p><p>The method used to solve the task is an algorithm based on column generation. The mathematical model used has a high flexibility and handles breaks, multi-activity, such as boarding, and non-splittable tasks, in other words tasks that has to be performed by one employee in one shift. The subproblem is a binary integer program that generates feasible shifts following certain rules and is solved using a commercial solver.</p><p>The results have shown possible improvements. In the best test scenario, the idle time is reduced to 4.7 percent of the total worktime. There is room for improvement of the model and the results. One possible improvement is to reduce the running time of the program which also could lead to improved results.</p>

Note only change was to add the missing paragraph breaks
----------------------------------------------------------------------
In diva2:1319871   - correct as is
----------------------------------------------------------------------
In diva2:1438308 
abstract is: 
<p>Which numerical methods are ideal for training a neural network? In this report four different optimization methods are analysed and compared to each other. First, the most basic method Stochastic Gradient Descent that steps in the negative gradients direction. We continue with a slightly more advanced algorithm called ADAM, often used in practice to train neural networks. Finally, we study two second order methods, the Conjugate Gradient Method which follows conjugate directions, and L-BFGS, a Quasi-Newton method which approximates the inverse of the Hessian matrix. The methods are tasked to solve a classification problem with hyperspheres acting as decision boundaries and multiple different network configurations are used. Our results indicate why first order methods are so commonly used today and that second order methods can be difficult to use effectively when the number of parameters are large.</p>

corrected abstract:
<p>Which numerical methods are ideal for training a neural network? In this report four different optimization methods are analysed and compared to each other. First, the most basic method <em>Stochastic Gradient Descent</em> that steps in the negative gradients direction. We continue with a slightly more advanced algorithm called <em>ADAM</em>, often used in practice to train neural networks. Finally, we study two second order methods, the <em>Conjugate Gradient Method</em> which follows conjugate directions, and <em>L-BFGS</em>, a Quasi-Newton method which approximates the inverse of the Hessian matrix. The methods are tasked to solve a classification problem with hyperspheres acting as decision boundaries and multiple different network configurations are used. Our results indicate why first order methods are so commonly used today and that second order methods can be difficult to use effectively when the number of parameters are large.</p>

Note - added the italics
----------------------------------------------------------------------
In diva2:747997   - correct as is
----------------------------------------------------------------------
In diva2:1849045   - correct as is
----------------------------------------------------------------------
In diva2:866670   - correct as is
----------------------------------------------------------------------
In diva2:953108 
abstract is: 
<p>This thesis deals with cut finite element methods (CutFEM) for solving partial differential equations (PDEs) on evolving interfaces. Such PDEs arise for example in the study of insoluble surfactants in multiphase flow. In CutFEM, the interface is embedded in a larger mesh which need not respect the geometry of the interface. For example, the mesh of a two dimensional space containing a curve, may be used in order to solve a PDE on the curve. Consequently, in time-dependent problems, a fixed background mesh, in which the time-dependent domain is embedded, may be used. </p><p>The cut finite element method requires a representation of the interface. Previous work on CutFEM has mostly been done using linear segments to represent the interfaces. Due to the linear interface representation the proposed methods have been of, at most, second order. Higher order methods require better than linear interface representation. In this thesis, a second order CutFEM is implemented using an explicit spline representation of the interface and the convection-diffusion equation for surfactant transport along a deforming interface is solved on a curve subject to a given velocity field. </p><p>The markers, used to explicitly represent the interface, may due to the velocity field spread out alternately cluster. This may cause the interface representation to worsen. A method for keeping the interface markers evenly spread, proposed by Hou et al., is numerically investigated in the case of convection-diffusion. The method, as implemented, is shown to not be useful.</p>

corrected abstract:
<p>This thesis deals with cut finite element methods (<em>CutFEM</em>) for solving partial differential equations (<em>PDEs</em>) on evolving interfaces. Such PDEs arise for example in the study of insoluble surfactants in multiphase flow. In CutFEM, the interface is embedded in a larger mesh which need not respect the geometry of the interface. For example, the mesh of a two dimensional space containing a curve, may be used in order to solve a PDE on the curve. Consequently, in time-dependent problems, a fixed background mesh, in which the time-dependent domain is embedded, may be used.</p><p>The cut finite element method requires a representation of the interface. Previous work on CutFEM has mostly been done using linear segments to represent the interfaces. Due to the linear interface representation the proposed methods have been of, at most, second order,  see for example [6]. Higher order methods require better than linear interface representation. In this thesis, a second order CutFEM is implemented using an explicit spline representation of the interface and the convection-diffusion equation for surfactant transport along a deforming interface is solved on a curve subject to a given velocity field.</p><p>The markers, used to explicitly represent the interface, may due to the velocity field spread out alternately cluster. This may cause the interface representation to worsen. A method for keeping the interface markers evenly spread, proposed by Hou et al. in [8], is numerically investigated in the case of convection-diffusion. The method, as implemented, is shown to not be useful.</p>

Note added missing text and citations, also eliminated the unnecessary space at the end of paragraphs; Also added italics
----------------------------------------------------------------------
In diva2:849704 - the abstract page seems to have been scanned, but it was possible to OCR it
abstract is: 
<p>As an innovation project at Ecole Central de Paris, we were to develop a SaaS platform (Software as a Service) giving the user the possibility to perform forecasts based on data coming from various posts of the income statement and/or sales data. The platform was supposed to be able to perform short-term, mid-term as well as long-term forecasts based on these data. The platform has been developed through the web application framework Django, which is based on the programming language Python. The forecasting algorithm used is based on the preceding project on the subject, however it has been modified and improved.</p>

corrected abstract:
<p>As an innovation project at Ecole Centrale Paris, we were to develop a SaaS platform (Software as a Service) giving the user the possibility to perform forecasts based on data coming from various posts of the income statement and/or sales data. The platform was supposed to be able to perform short-term, mid-term as well as long-term forecasts based on these data. The platform has been developed through the web application framework Django, which is based on the programming language Python. The forecasting algorithm used is based on the preceding project on the subject, however it has been modified and improved.</p>

Note only a minor change to correct "Central de" to "Centrale"
----------------------------------------------------------------------
In diva2:1823772 
abstract is: 
<p>The aim of this study was to explore how match-related statistics contribute to winning association football matches. This is relevant for stakeholders in the football industry to facilitate the understanding of what factors contribute to winning matches and can thus be of use when formulating match tactics. A model was constructed through the use of binary logistic regression, where winning/not winning was used as the response variable and standardized match-related statistics were used as predictor variables. Using the acquired coefficients, it was concluded that, among other variables, the home advantage and the ability of a team to finish on target has a strong correlation with winninggames. Further, the study explores the impact of a team’s ability to win football games on the financial landscape of the modern football world. The results show that some of the examined statistics are well correlated to winning a match, but that the tactical useability of these insights is low.</p>

mc='winninggames' c='winning games'

corrected abstract:
<p>The aim of this study was to explore how match-related statistics contribute to winning association football matches. This is relevant for stakeholders in the football industry to facilitate the understanding of what factors contribute to winning matches and can thus be of use when formulating match tactics. A model was constructed through the use of binary logistic regression, where winning/not winning was used as the response variable, and standardized match-related statistics were used as predictor variables. Using the acquired coefficients, it was concluded that, among other variables, the home advantage and the ability of a team to finish on target has a strong correlation with winning games. Further, the study explores the impact of a team’s ability to win football games on the financial landscape of the modern football world. The results show that some of the examined statistics are well correlated to winning a match, but that the tactical useability of these insights is low.</p>
----------------------------------------------------------------------
In diva2:932583   - correct as is
----------------------------------------------------------------------
In diva2:902335   - correct as is
----------------------------------------------------------------------
In diva2:1737164   - correct as is
----------------------------------------------------------------------
***** 'diva2:1811727' *** need to comeback and look at this as the DivA abstract is completely different from the original in the full text of the thesis
----------------------------------------------------------------------
In diva2:892101 
abstract is: 
<p>The Swedish company Strömsholmen AB, which develops and manufactures gas springs for the tool and die industry, have entered the market of heavy duty off-road vehicles where they advertise their hydropneumatic suspension system. One of their customers is the Finnish defense company Patria, which produces the eight-wheeled military vehicle Patria AMV. In order to produce a more optimized suspension system for Patria, Strömsholmen is in need of learning more about how the vehicle dynamic properties such as handling are influenced by the suspension settings. To achieve this knowledge Strömsholmen started a collaboration with the consultant company FS Dynamics and initiated this master thesis work. The aim with this master thesis is to produce a simulation model of Patria AMV and investigate the influence of the suspension system settings on vehicle dynamic properties such as handling. The thesis work has resulted in a vehicle model in Adams/Car that is verified against experimental data from two slalom maneuvers. The model shows a good correlation with the validation data, taking into account the many assumptions and estimations that had to be made during the work due to lack of vehicle parameter data. A suspension parameter study was performed investigating vehicle maneuvers such as steady-state cornering and single-lane change. The results are summarized in a lookup table which can be used during future vehicle tests. Recommendations for future work is to verify some of the estimated vehicle parameters in the vehicle model as well as correlate test drivers subjective feel of the vehicle response to the calculated objective handling measures in this work.</p>

corrected abstract:
<p>The Swedish company Strömsholmen AB, which develops and manufactures gas springs for the tool and die industry, have entered the market of heavy duty off-road vehicles where they advertise their hydropneumatic suspension system. One of their customers is the Finnish defense company Patria, which produces the eight-wheeled military vehicle Patria AMV.</p><p>In order to produce a more optimized suspension system for Patria, Strömsholmen is in need of learning more about how the vehicle dynamic properties such as handling are influenced by the suspension settings. To achieve this knowledge Strömsholmen started a collaboration with the consultant company FS Dynamics and initiated this master thesis work.</p><p>The aim with this master thesis is to produce a simulation model of Patria AMV and investigate the influence of the suspension system settings on vehicle dynamic properties such as handling.</p><p>The thesis work has resulted in a vehicle model in Adams/Car that is verified against experimental data from two slalom maneuvers. The model shows a good correlation with the validation data, taking into account the many assumptions and estimations that had to be made during the work due to lack of vehicle parameter data.</p><p>A suspension parameter study was performed investigating vehicle maneuvers such as steady-state cornering and single-lane change. The results are summarized in a lookup table which can be used during future vehicle tests.</p><p>Recommendations for future work is to verify some of the estimated vehicle parameters in the vehicle model as well as correlate test drivers subjective feel of the vehicle response to the calculated objective handling measures in this work.</p>

Note - only added the missing paragraph breaks
----------------------------------------------------------------------
In diva2:1678831   - correct as is
----------------------------------------------------------------------
In diva2:1800177 
abstract is: 
<p>This paper uses a back-propagating neural network (BPN) to predict the price movements of major crypto currencies, leveraging technical factors as well as measurements of collective sentiment derived from the micro-blogging network Twitter. Our dataset consists of daily, hourly and minutely price levels for Bitcoin, Ether and Litecoin along with 8 popular technical indicators, as well as all tweets with the currencies' cash tags during respective time periods. Insprired by previous research which suggest that artificial neural networks are superior forecasting models in this setting, we were able to create a system generating automated investment decisions on a daily, hourly and minutely time basis. The study concluded that price trends are indeed predictable, with a correct prediction rate above 50% for all models, and corrensponding profitable trading strategies for all currencies on an hourly basis when neglecting trading fees, buy-sell spreads and order delays. The overall highest predictability is obtained on the hourly trading interval for Bitcoin, yielding an accuracy of 55.74% and a cumulative return of 175.1% between October 16, 2021 and December 31, 2021.</p>


corrected abstract:
<p>This paper uses a back-propagating neural network (BPN) to predict the price movements of major crypto currencies, leveraging technical factors as well as measurements of collective sentiment derived from the micro-blogging network Twitter. Our dataset consists of daily, hourly and minutely price levels for Bitcoin, Ether and Litecoin along with 8 popular technical indicators, as well as all tweets with the currencies' cash tags during respective time periods. Inspired by previous research which suggests that artificial neural networks are superior forecasting models in this setting, we were able to create a system generating automated investment decisions on a daily, hourly and minutely time basis. The study concluded that price trends are indeed predictable, with a correct prediction rate above 50% for all models, and corresponding profitable trading strategies for all currencies on an hourly basis when neglecting trading fees, buy-sell spreads and order delays. The overall highest predictability is obtained on the hourly trading interval for Bitcoin, yielding an accuracy of 55.74% and a cumulative return of 175.1% between October 16, 2021 and December 31, 2021.</p>
----------------------------------------------------------------------
In diva2:439972   - correct as is
----------------------------------------------------------------------
In diva2:1888155   - correct as is
----------------------------------------------------------------------
In diva2:1191148 
abstract is: 
<p>The vehicle routing problem is an old and well-studied problem that arise in last mile logistics. The rapid increase of e-commerce, in particular with an increasing the demand for time scheduled home deliveries on the customer’s terms, is making the problem ever more relevant. By minimizing the cost and environmental impact, we have the setting for mathematical problem called the vehicle routing problem with time windows.</p><p>Since the problem is NP-Hard, heuristic methods are often used. In practice, they work very well and typically offer a good tradeoff between speed and quality. However, since the heuristics are often tailormade to fit the needs of the underlying problem, no known algorithm dominates the other on all problems.</p><p>One way to overcome the need for specialization is to produce heuristics that are adaptive. In this thesis, an offline learning method is proposed to generate an adaptive heuristic using local search heuristics and reinforcement learning.</p><p>The reinforcement learning agents explored in this thesis are situated in both discrete and continuous state representations. Common to all state spaces are that they are inspired by human-crafted reference models where the last action and the result of that action define the state. Four different reinforcement learning models are evaluated in the various environments.</p><p>By evaluating the models on a subset of the Solomon benchmark instances, we find that all models but one improve upon a random baseline. The average learner from each of the successful models was slightly worse than the human crafted baseline. However, the best among the generated models was an actor-critic based model which outperformed the best human baseline model.</p><p>Due to the scalar objective function, the results are not directly comparable to the Solomon benchmark results with hierarchal objectives. None the less, the results are encouraging as a proof of principle with results in line with the human crafted baseline. The results indicate two clear paths for further work. First, applying the formulation to more complex environments with more actions and more powerful state spaces. Secondly, investigate models based on stochastic policies and recurrent neural networks to cope with the inherently partially observed environment.</p>

corrected abstract:
<p>The vehicle routing problem is an old and well-studied problem that arise in last mile logistics. The rapid increase of e-commerce, in particular with an increasing the demand for time scheduled home deliveries on the customer’s terms, is making the problem ever more relevant. By minimizing the cost and environmental impact, we have the setting for mathematical problem called the vehicle routing problem with time windows.</p><p>Since the problem is NP-Hard, heuristic methods are often used. In practice, they work very well and typically offer a good tradeoff between speed and quality. However, since the heuristics are often tailormade to fit the needs of the underlying problem, no known algorithm dominates the other on all problems.</p><p>One way to overcome the need for specialization is to produce heuristics that are adaptive. In this thesis, an offline learning method is proposed to generate an adaptive heuristic using local search heuristics and reinforcement learning.</p><p>The reinforcement learning agents explored in this thesis are situated in both discrete and continuous state representations. Common to all state spaces are that they are inspired by human-crafted reference models where the last action and the result of that action define the state. Four different reinforcement learning models are evaluated in the various environments.</p><p>By evaluating the models on a subset of the Solomon benchmark instances, we find that all models but one improve upon a random baseline. The average learner from each of the successful models was slightly worse than the human crafted baseline. However, the best among the generated models was an actor-critic based model which outperformed the best human baseline model.</p><p>Due to the scalar objective function, the results are not directly comparable to the Solomon benchmark results with hierarchal objectives. None the less, the results are encouraging as a proof of principle with results in line with the human crafted baseline.</p><p>The results indicate two clear paths for further work. First, applying the formulation to more complex environments with more actions and more powerful state spaces. Secondly, investigate models based on stochastic policies and recurrent neural networks to cope with the inherently partially observed environment.</p>

Note added missing paragraph break for the last paragraph
----------------------------------------------------------------------
In diva2:706524 
abstract is: 
<p>A method for efficient analysis of variations in utter speed predictions caused by parameter variations in the mass, stiffness and aerodynamic models of a wing structure is presented. The analysis method considers a linear uncertainty formulation and performs a perturbation analysis based on computation of eigenvalue differentials of a nominal system matrix. The method is applied in a test case study in which utter speed variations caused by variations in mass and aerodynamic properties of a delta wing model is analyzed. The report is concluded with a discussion of the validity of the results and of how the applicability of the method is affected by the assumptions on which it builds.</p>

corrected abstract:
<p>A method for efficient analysis of variations in flutter speed predictions caused by parameter variations in the mass, stiffness and aerodynamic models of a wing structure is presented. The analysis method considers a linear uncertainty formulation and performs a perturbation analysis based on computation of eigenvalue differentials of a nominal system matrix. The method is applied in a test case study in which flutter speed variations caused by variations in mass and aerodynamic properties of a delta wing model is analyzed. The report is concluded with a discussion of the validity of the results and of how the applicability of the method is affected by the assumptions on which it builds.</p>

Note added the missing "fl" due to a ligrature (in flutter)
----------------------------------------------------------------------
In diva2:874841 
abstract is: 
<p>A modal Perfectly Matched Layer (PML) is constructed for the two-dimensional, non-linear, Shallow Water Equations (SWEs) in conservative variables. The result is an analytical continuation of the original equations where absorption is applied to the outgoing wave modes which are damped exponentially fast in the direction of propagation. Numerical tests are performed using a variation of the Diagonally Implicit Runge-Kutta (DIRK) integration scheme in conjunction with Lax-Wendroff’s method for smooth so- lutions and some variation of Roe’s method for discontinuous solutions.</p><p>Different absorption functions are used, i.e. the absorption function of the original PML constructed by Berenger for electromagnetic waves, and some variations of the hyperbola functions. The results clearly show that the PML is better than the characteristic boundary condition, but also that improvements through some sort of optimization should lead to better parameter choices, potentially decreasing the reflections further.</p>

corrected abstract:
<p>A modal Perfectly Matched Layer (PML) is constructed for the two-dimensional, non-linear, Shallow Water Equations (SWEs) in conservative variables. The result is an analytical continuation of the original equations where absorption is applied to the outgoing wave modes which are damped exponentially fast in the direction of propagation. Numerical tests are performed using a variation of the Diagonally Implicit Runge-Kutta (DIRK) integration scheme in conjunction with Lax-Wendroff’s method for smooth solutions and some variation of Roe’s method for discontinuous solutions. Different absorption functions are used, i.e. the absorption function of the original PML constructed by Berenger for electromagnetic waves, and some variations of the hyperbola functions. The results clearly show that the PML is better than the characteristic boundary condition, but also that improvements through some sort of optimization should lead to better parameter choices, potentially decreasing the reflections further.</p>

Note removed unnecessary hyphen and an unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1655643   - correct as is
----------------------------------------------------------------------
In diva2:1221288   - correct as is
----------------------------------------------------------------------
In diva2:1795170 
abstract is: 
<p>With an increasingly globalised market and growing asset universe, estimating the market covariance matrix becomes even more challenging. In recent years, there has been an extensive development of methods aimed at mitigating these issues. This thesis takes its starting point in the recently developed Hierarchical Principal Component Analysis, in which a priori known information is taken into account when modelling the market correlation matrix. However, while showing promising results, the current framework only allows for fairly simple hierarchies with a depth of one. In this thesis, we introduce a generalisation of the framework that allows for an arbitrary hierarchical depth. We also evaluate the method in a risk-based portfolio allocation setting with Futures contracts. </p><p>Furthermore, we introduce a shrinkage method called Hierarchical Shrinkage, which uses the hierarchical structure to further regularise the matrix. The proposed models are evaluated with respect to how well-conditioned they are, how well they predict eigenportfolio risk and portfolio performance when they are used to form the Minimum Variance Portfolio. We show that the proposed models result in sparse and easy-to-interpret eigenvector structures, improved risk prediction, lower condition numbers and longer holding periods while achieving Sharpe ratios that are at par with our benchmarks.</p>

corrected abstract:
<p>With an increasingly globalised market and growing asset universe, estimating the market covariance matrix becomes even more challenging. In recent years, there has been an extensive development of methods aimed at mitigating these issues. This thesis takes its starting point in the recently developed Hierarchical Principal Component Analysis, in which a priori known information is taken into account when modelling the market correlation matrix. However, while showing promising results, the current framework only allows for fairly simple hierarchies with a depth of one. In this thesis, we introduce a generalisation of the framework that allows for an arbitrary hierarchical depth. We also evaluate the method in a risk-based portfolio allocation setting with Futures contracts.</p><p>Furthermore, we introduce a shrinkage method called Hierarchical Shrinkage, which uses the hierarchical structure to further regularise the matrix. The proposed models are evaluated with respect to how well-conditioned they are, how well they predict eigenportfolio risk and portfolio performance when they are used to form the Minimum Variance Portfolio. We show that the proposed models result in sparse and easy-to-interpret eigenvector structures, improved risk prediction, lower condition numbers and longer holding periods while achieving Sharpe ratios that are at par with our benchmarks.</p>

Note only change was to remove the unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1334739 
abstract is: 
<p>Small and medium-sized enterprises (SMEs) have long been considered the backbone in any country’s economy for their contribution to growth and prosperity. It is therefore of great importance that the government and legislators adopt policies that optimise the success of SMEs. Recent concerns of an impending recession has made this topic even more relevant since small companies will have greater difficulty withstanding such an event. This thesis will focus on the effects of macroeconomic factors on SMEs in Sweden, with the usage of multiple linear regression. Data was collected for a 10 year period, from 2009 to 2019 at a monthly interval. The end result was a five variable model with an coefficient of determination of 98%.</p>

corrected abstract:
<p>Small and medium-sized enterprises (SMEs) have long been considered the backbone in any country’s economy for their contribution to growth and prosperity. It is therefore of great importance that the government and legislators adopt policies that optimise the success of SMEs. Recent concerns of an impending recession has made this topic even more relevant since small companies will have greater difficulty withstanding such an event.</p><p>This thesis will focus on the effects of macroeconomic factors on SMEs in Sweden, with the usage of multiple linear regression. Data was collected for a 10 year period, from 2009 to 2019, at a monthly interval. The end result was a five variable model with a coefficient of determination of 98%.</p>

Note added missing paragraph break and missing comma.
----------------------------------------------------------------------
In diva2:1222265 
abstract is: 
<p>This thesis explored the possibilities of using a Hidden Markov Model approach for multi-target localisation in an urban environment, with observations generated from Wi-Fi sensors. The area is modelled as a network of nodes and arcs, where the arcs represent sidewalks in the area and constitutes the hidden states in the model. The output of the model is the expected amount of people at each road segment throughout the day. In addition to this, two methods for analyzing the impact of events in the area are proposed. The first method is based on a time series analysis, and the second one is based on the updated transition matrix using the Baum-Welch algorithm. Both methods reveal which road segments are most heavily affected by a surge of traffic in the area, as well as potential bottleneck areas where congestion is likely to have occurred.</p>

corrected abstract:
<p>This thesis explored the possibilities of using a Hidden Markov Model approach for multi-target localisation in an urban environment, with observations generated from Wi-Fi sensors. The area is modelled as a network of nodes and arcs, where the arcs represent sidewalks in the area and constitutes the hidden states in the model. The output of the model is the expected amount of people at each road segment throughout the day. In addition to this, two methods for analysing the impact of events in the area are proposed. The first method is based on a time series analysis, and the second one is based on the updated transition matrix using the Baum-Welch algorithm. Both methods reveal which road segments are most heavily affected by a surge of traffic in the area, as well as potential bottleneck areas where congestion is likely to have occurred.</p>

Note - only change "analyzing" to "analysing" - to match the original
----------------------------------------------------------------------
In diva2:1833747   - correct as is
----------------------------------------------------------------------
In diva2:1800205 
abstract is: 
<p>This thesis investigates how neural networks can be used to produce investors' views for the Black-Litterman market model. The study uses two data sets, one with global stock market indexes and one with stock market data from the S&amp;P 500. The task of the neural networks is to produce forecasts for the returns for the next quarter and the following year. The neural network will have to predict whether the market will move up or down and determine if the market movement is less than or equal to one standard deviation, creating four different scenarios. The forecasts are used as input to the Black-Litterman model to generate new portfolios, which are backtested from 2017 until 2022. The index data set was compared to a benchmark portfolio and a portfolio with naive risk diversification, while the S&amp;P 500 data set was compared to market capitalization-weighted and naive portfolios. This resulted in eight different backtests where the neural networks obtained AUC values in the range of 0.56-0.73 and prediction accuracies in the range of 20.9% - 42.1%. The network used for yearly predictions on the index data set was the only network to outperform the benchmark portfolio. It obtained a Sharpe ratio of 1.782, a Sortino ratio of 2.165, and a maximum drawdown of -30.9% compared to the benchmark portfolio, where the corresponding metrics were 1.544, 1.879, and -32.8%.</p>

corrected abstract:
<p>This thesis investigates how neural networks can be used to produce investors' views for the Black-Litterman market model. The study uses two data sets, one with global stock market indices and one with stock market data from the S&amp;P 500. The task of the neural networks is to produce forecasts for the returns for the next quarter and the following year. The neural network will have to predict whether the market will move up or down and determine if the market movement is less than or equal to one standard deviation, creating four different scenarios. The forecasts are used as input to the Black-Litterman model to generate new portfolios, which are backtested from 2017 until 2022. The index data set was compared to a benchmark portfolio and a portfolio with naive risk diversification, while the S&amp;P 500 data set was compared to market capitalization-weighted and naive portfolios. This resulted in eight different backtests where the neural networks obtained AUC values in the range of 0.56 - 0.73 and prediction accuracies in the range of 20.9% - 42.1%. The network used for yearly predictions on the index data set was the only network to outperform the benchmark portfolio. It obtained a Sharpe ratio of 1.782, a Sortino ratio of 2.165, and a maximum drawdown of -30.9% compared to the benchmark portfolio, where the corresponding metrics were 1.544, 1.879, and -32.8%.</p>

Note change is the dashes (hypens) used.
----------------------------------------------------------------------
In diva2:1215621 
abstract is: 
<p>This study investigates a neural networks approach to portfolio choice. Linear regression models are extensively used for prediction. With the return as the output variable, one can come to understand its relation to the explanatory variables the linear regression is built upon. However, if the relationship between the output and input variables is non-linear, the linear regression model may not be a suitable choice. An Artificial Neural Network (ANN) is a non-linear statistical model that has been shown to be a “good” approximator of non-linear functions. In this study, two different ANN models are considered, Feed-forward Neural Networks (FNN) and Recurrent Neural Networks (RNN). Networks from these models are trained to predict monthly returns on asset data consisting of macroeconomic data and market data. The predicted returns are then used in a long-short portfolio strategy. The performance of these networks and their corresponding portfolios are then compared to a benchmark linear regression model. Metrics such as average hit-rate, mean squared prediction error, portfolio value and riskadjusted returns are used to evaluate the model performances. The linear regression and the feed-forward model yielded good average hit-rates and mean squared-errors, but poor portfolio performances. The recurrent neural network models yielded worse average hit-rates and mean squared prediction errors, but had outstanding portfolio performances</p>

corrected abstract:
<p>This study investigates a neural networks approach to portfolio choice. Linear regression models are extensively used for prediction. With the return as the output variable, one can come to understand its relation to the explanatory variables the linear regression is built upon. However, if the relationship between the output and input variables is non-linear, the linear regression model may not be a suitable choice. An Artificial Neural Network (ANN) is a non-linear statistical model that has been shown to be a “good” approximator of non-linear functions. In this study, two different ANN models are considered, Feed-forward Neural Networks (FNN) and Recurrent Neural Networks (RNN). Networks from these models are trained to predict monthly returns on asset data consisting of macroeconomic data and market data. The predicted returns are then used in a long-short portfolio strategy. The performance of these networks and their corresponding portfolios are then compared to a benchmark linear regression model. Metrics such as average hit-rate, mean squared prediction error, portfolio value and risk-adjusted returns are used to evaluate the model performances. The linear regression and the feed-forward model yielded good average hit-rates and mean squared-errors, but poor portfolio performances. The recurrent neural network models yielded worse average hit-rates and mean squared prediction errors, but had outstanding portfolio performances.</p>

Note added hyphen to "risk-adjusted" and added terminal period to last paragraph
----------------------------------------------------------------------
In diva2:1222440 
abstract is: 
<p>Maribot Vane is an autonomous sailboat project at KTH Royal Institute of Technology. The use of autonomous boats is being recognised all over the world as a cost-efficient alternative to traditional manned ships for oceanographic research. Vane consists of an International 2.4 mR hull propelled by a self-adjusting wing that is controlled by a flap. A self-steering mechanism is currently under development. Field testing of the boat in the summer of 2017 showed that the boat was leaking where the mast enters the deck as well as through a hatch covering the former cockpit.</p><p>This report deals with developing a new sealing solution to prevent water from entering the boat. It should be a durable and waterproof solution. Minimizing friction is of great importance to reduce interference with the self-adjusting wing. The problem is divided into two sub-problems: creating a sealing where the mast enters the boat and designing a new hatch. A housing made of 3D-printed plastic will be placed around the mast. By establishing models depicting “worst-case” scenarios calculations are done to determine how long the housing can stay submerged as well as how much impact it has to endure when being hit by a wave. Experiments are then performed on prototypes of the housing to determine how accurate the theoretical models are.</p><p>A housing that theoretically can stay submerged for approximately three seconds is developed. Analysis suggests that it is durable enough to withstand the impact from being hit by a wave. A hatch consisting of two parts is also developed. One placed in the front where the mast goes through and one in the back that should be easy to open, providing access to the inner parts of the boat even when in water.</p>

corrected abstract:
<p><em>Maribot Vane</em> is an autonomous sailboat project at KTH Royal Institute of Technology. The use of autonomous boats is being recognised all over the world as a cost-efficient alternative to traditional manned ships for oceanographic research. <em>Vane</em> consists of an International 2.4 mR hull propelled by a self-adjusting wing that is controlled by a flap. A self-steering mechanism is currently under development. Field testing of the boat in the summer of 2017 showed that the boat was leaking where the mast enters the deck as well as through a hatch covering the former cockpit.</p><p>This report deals with developing a new sealing solution to prevent water from entering the boat. It should be a durable and waterproof solution. Minimizing friction is of great importance to reduce interference with the self-adjusting wing. The problem is divided into two sub-problems: creating a sealing where the mast enters the boat and designing a new hatch. A housing made of 3D-printed plastic will be placed around the mast. By establishing models depicting “worst-case” scenarios calculations are done to determine how long the housing can stay submerged as well as how much impact it has to endure when being hit by a wave. Experiments are then performed on prototypes of the housing to determine how accurate the theoretical models are.</p><p>A housing that theoretically can stay submerged for approximately three seconds is developed. Analysis suggests that it is durable enough to withstand the impact from being hit by a wave. A hatch consisting of two parts is also developed. One placed in the front where the mast goes through and one in the back that should be easy to open, providing access to the inner parts of the boat even when in water.</p>

Note - only change was to add the italics for the name of te sailboat project
----------------------------------------------------------------------
In diva2:1438316   - correct as is
----------------------------------------------------------------------
In diva2:1644826   - correct as is
----------------------------------------------------------------------
In diva2:753569 
abstract is: 
<p>When a mammalian cell is exposed to a carcinogen, the carcinogen gives rise to new compounds. In this paper a numerical approach is taken to find the amount of these compounds overtime. The concentration of the compounds are assumed to satisfy the reaction and/or the diffusion equation, techniques used for solving these equations include the one-dimensional finite element method (FEM) and the lumped parameter approach. The mathematically derived results are compared to in-vitro measurements. The mathematical model set up in this paper will prove insufficient.</p>

corrected abstract:
<p>When a mammalian cell is exposed to a carcinogen, the carcinogen gives rise to new compounds. In this paper a numerical approach is taken to find the amount of these compounds over time. The concentration of the compounds are assumed to satisfy the reaction and/or the diffusion equation, techniques used for solving these equations include the one-dimensional finite element method (FEM) and the lumped parameter approach. The mathematically derived results are compared to in-vitro measurements. The mathematical model set up in this paper will prove insufficient.</p>

Note - only change to split "overtime" in to "over time"
----------------------------------------------------------------------
In diva2:559080 Note spelling error:
----------------------------------------------------------------------
In diva2:855388   - correct as is
----------------------------------------------------------------------
In diva2:1827745 
abstract is: 
<p>Churn refers to the discontinuation of a contract; consequently, customer churn occurs when existing customers stop being customers. Predicting customer churn is a challenging task in customer retention, but with the advancements made in the field of artificial intelligence and machine learning, the feasibility to predict customer churn has increased. Prior studies have demonstrated that machine learning can be utilized to forecast customer churn. The aim of this thesis was to develop and implement a machine learning model to predict customer churn and identify the customer features that have a significant impact on churn. This Study has been conducted in cooperation with the Swedish insurance company Bliwa, who expressed interest in gaining an increased understanding of why customers choose to leave. </p><p>Three models, Logistic Regression, Random Forest, and Gradient Boosting, were used and evaluated. Bayesian optimization was used to optimize the models. After obtaining an indication of their predictive performance during evaluation using Cross-Validation, it was concluded that LightGBM provided the best result in terms of PR-AUC, making it the most effective approach for the problem at hand.</p><p>Subsequently, a SHAP-analysis was carried out to gain insights into which customer features that have an impact on whether or not a customer churn. The outcome of the SHAP-analysis revealed specific customer features that had a significant influence on churn. This knowledge can be utilized to proactively implement measures aimed at reducing the probability of churn.</p>

corrected abstract:
<p>Churn refers to the discontinuation of a contract; consequently, customer churn occurs when existing customers stop being customers. Predicting customer churn is a challenging task in customer retention, but with the advancements made in the field of artificial intelligence and machine learning, the feasibility to predict customer churn has increased. Prior studies have demonstrated that machine learning can be utilized to forecast customer churn. The aim of this thesis was to develop and implement a machine learning model to predict customer churn and identify the customer features that have a significant impact on churn. This Study has been conducted in cooperation with the Swedish insurance company Bliwa, who expressed interest in gaining an increased understanding of why customers choose to leave.</p><p>Three models, Logistic Regression, Random Forest, and Gradient Boosting, were used and evaluated. Bayesian optimization was used to optimize the models. After obtaining an indication of their predictive performance during evaluation using Cross-Validation, it was concluded that LightGBM provided the best result in terms of PR-AUC, making it the most effective approach for the problem at hand.</p><p>Subsequently, a SHAP-analysis was carried out to gain insights into which customer features that have an impact on whether or not a customer churn. The outcome of the SHAP-analysis revealed specific customer features that had a significant influence on churn. This knowledge can be utilized to proactively implement measures aimed at reducing the probability of churn.</p>

Note - only change to delete unnecessary space at end of a paragraph
----------------------------------------------------------------------
In diva2:1143829 - missing space in title:
"A predictor corrector method for a Finite ElementMethod for the variable density Navier-Stokes equations"
==>
"A predictor corrector method for a Finite Element Method for the variable density Navier-Stokes equations"

abstract   - correct as is 
----------------------------------------------------------------------
In diva2:1894668   - correct as is
----------------------------------------------------------------------
In diva2:1589052   - correct as is
----------------------------------------------------------------------
In diva2:1655792   - correct as is
----------------------------------------------------------------------
In diva2:1795466 - missing space in title:
"A Review of Anomaly Detection Techniques forHeterogeneous Datasets"
==>
"A Review of Anomaly Detection Techniques for Heterogeneous Datasets"

abstract is: 
<p>Anomaly detection is a field of study that is closely associated with machine learning and it is the process of finding irregularities in datasets. Developing and maintaining multiple machine learning models for anomaly detection takes time and can be an expensive task. One proposed solution is to combine all datasets and create a single model. This creates a heterogeneous dataset with a wide variation in its distribution, making it difficult to find anomalies in the dataset. The objective of this thesis is then to identify a framework that is suitable for anomaly detection in heterogeneous datasets.</p><p>A selection of five methods were implemented in this project - 2 supervised learning approaches and 3 unsupervised learning approaches. These models are trained on 3 synthetic datasets that have been designed to be heterogeneous with an imbalance between the classes as anomalies are rare events. The performance of the models are evaluated with the AUC and the F1-score, aswell as observing the Precision-Recall Curve.</p><p>The results makes it evident that anomaly detection in heterogeneous datasets is a challenging task. The best performing approach was with a random forest model where the class imbalance problem had been solved by generating synthetic samples of the anomaly class by implementing a generative adversarial network.</p>

corrected abstract:
<p>Anomaly detection is a field of study that is closely associated with machine learning and it is the process of finding irregularities in datasets. Developing and maintaining multiple machine learning models for anomaly detection takes time and can be an expensive task. One proposed solution is to combine all datasets and create a single model. This creates a heterogeneous dataset with a wide variation in its distribution, making it difficult to find anomalies in the dataset. The objective of this thesis is then to identify a framework that is suitable for anomaly detection in heterogeneous datasets.</p><p>A selection of five methods were implemented in this project - 2 supervised learning approaches and 3 unsupervised learning approaches. These models are trained on 3 synthetic datasets that have been designed to be heterogeneous with an imbalance between the classes as anomalies are rare events. The performance of the models are evaluated with the AUC and the 𝐹<sub>1</sub>-score, aswell as observing the <em>Precision-Recall Curve</em>.</p><p>The results makes it evident that anomaly detection in heterogeneous datasets is a challenging task. The best performing approach was with a random forest model where the class imbalance problem had been solved by generating synthetic samples of the anomaly class by implementing a generative adversarial network.</p>

Note added "𝐹" to "F" in F1 and added the subscript and added italics
----------------------------------------------------------------------
In diva2:711139   - correct as is
----------------------------------------------------------------------
In diva2:633872 - errors in title:
"A Scenario Based Allocation Model Using Entropy Pooling for Computing the cenarioProbabilities"
==>
"A Scenario Based Allocation Model Using Entropy Pooling for Computing the Scenario Probabilities"

abstract is: 
<p>We introduce a scenario based allocation model (SBAM) that uses entropy pooling for computing scenario probabilities. Compared to most other models that allow the investor to blend historical data with subjective views about the future, the SBAM does not require the investor to quantify a level of confidence in the subjective views.</p><p> A quantitative test is performed on a simulated systematic fund offered by the fund company Informed Portfolio Management in Stockholm, Sweden. The simulated fund under study consists of four individual systematic trading strategies and the test is simulated on a monthly basis during the years 1986-2010.</p><p> We study how the selection of views might affect the SBAM portfolios, creating three systematic views and combining them in different variations creating seven SBAM portfolios. We also compare how the size of sample data affects the results. </p><p> Furthermore, the SBAM is compared to more common allocation methods, namely an equally weighted portfolio and a portfolio optimization based only on historical data.</p><p> We find that the SBAM portfolios produced higher annual returns and information ratio than the equally weighted portfolio or the portfolio optimized only on historical data.</p>

corrected abstract:
<p>We introduce a scenario based allocation model (SBAM) that uses entropy pooling for computing scenario probabilities. Compared to most other models that allow the investor to blend historical data with subjective views about the future, the SBAM does not require the investor to quantify a level of confidence in the subjective views.</p><p>A quantitative test is performed on a simulated systematic fund offered by the fund company Informed Portfolio Management in Stockholm, Sweden. The simulated fund under study consists of four individual systematic trading strategies and the test is simulated on a monthly basis during the years 1986-2010.</p><p>We study how the selection of views might affect the SBAM portfolios, creating three systematic views and combining them in different variations creating seven SBAM portfolios. We also compare how the size of sample data affects the results.</p><p>Furthermore, the SBAM is compared to more common allocation methods, namely an equally weighted portfolio and a portfolio optimization based only on historical data.</p><p>We find that the SBAM portfolios produced higher annual returns and information ratio than the equally weighted portfolio or the portfolio optimized only on historical data.</p>

Note - removed unnecessary spaces at paragraph breaks
----------------------------------------------------------------------
In diva2:1212535 
abstract is: 
<p>Bond liquidity risk is complex and something that every bond-investor needs to take into account. In this paper we investigate how well a selfnormalizing neural network (SNN) can be used to classify bonds with respect to their liquidity, and compare the results with that of a simpler logistic regression. This is done by analyzing the two algorithms' predictive capabilities on the Swedish bond market. Performing this analysis we find that the performance of the SNN and the logistic regression are broadly on the same level. However, the substantive overfitting to the training data in the case of the SNN suggests that a better performing model could be created by applying regularization techniques. As such, the conclusion is formed as such that there is need of more research in order to determine whether neural networks are the premier method to modelling liquidity.</p>

corrected abstract:
<p>Bond liquidity risk is complex and something that every bond-investor needs to take into account. In this paper we investigate how well a self-normalizing neural network (SNN) can be used to classify bonds with respect to their liquidity, and compare the results with that of a simpler logistic regression. This is done by analyzing the two algorithms' predictive capabilities on the Swedish bond market. Performing this analysis we find that the performance of the SNN and the logistic regression are broadly on the same level. However, the substantive overfitting to the training data in the case of the SNN suggests that a better performing model could be created by applying regularization techniques. As such, the conclusion is formed as such that there is need of more research in order to determine whther neural networks are the premier method to modelling liquidity.</p>

Note error in original
"whther" should be "whether"
----------------------------------------------------------------------
In diva2:1359762 
abstract is: 
<p>When designing a building, sound is one of the problems to take into account. Vibrating machines, such as ventilation fans, water pumps and compressors, generate structure-borne sound. The structure-borne sound travels up the structure of the building and generates sound in adjacent rooms. To be able to predict the sound radiated in the adjacent rooms when designing a building, a semi-analytical model has been developed. Using the incident vibrations from the floor plate where the vibrating machine is standing, the transmission loss in the junction between the floor plates and the wall plate is calculated. This can bed one in every junction in the building, creating a system of multiple junctions. The sound radiation to the adjacent rooms is later approximated using the velocity of the plates.The model is verified with measurements in two case studies. This shows that the model has good potential in predicting the normal acceleration amplitudes in the relevant plates. The two case studies have different geometric properties and different sources. The comparison between the model and the measurement gives similar results. The model analyses the output of the bending waves since this is the wave type that radiates sound, but longitudinal waves are present in the model. With only two case studies it is too early to say that the model works for all systems, but it could be used as a fist approach. The model, right now, is restricted to isotropic, homogeneous material without losses. A parametric study shows that the transmission loss is dependent on the ratio between the thicknesses of the floor plate and the wall plate. The ratio should be as large as possible to get a high transmission loss, but depends on how the junction is structured.</p>

corrected abstract:
<p>When designing a building, sound is one of the problems to take into account. Vibrating machines, such as ventilation fans, water pumps and compressors, generate structure-borne sound. The structure-borne sound travels up the structure of the building and generates sound in adjacent rooms. To be able to predict the sound radiated in the adjacent rooms when designing a building, a semi-analytical model has been developed. Using the incident vibrations from the floor plate where the vibrating machine is standing, the transmission loss in the junction between the floor plates and the wall plate is calculated. This can be done in every junction in the building, creating a system of multiple junctions. The sound radiation to the adjacent rooms is later approximated using the velocity of the plates.</p><p>The model is verified with measurements in two case studies. This shows that the model has good potential in predicting the normal acceleration amplitudes in the relevant plates. The two case studies have different geometric properties and different sources. The comparison between the model and the measurement gives similar results. The model analyses the output of the bending waves since this is the wave type that radiates sound, but longitudinal waves are present in the model. With only two case studies it is too early to say that the model works for all systems, but it could be used as a fist approach. The model, right now, is restricted to isotropic, homogeneous material without losses.</p><p>A parametric study shows that the transmission loss is dependent on the ratio between the thicknesses of the floor plate and the wall plate. The ratio should be as large as possible to get a high transmission loss, but depends on how the junction is structured.</p>

Note - fixed "bed one" to "be done" and added paragraph breaks
----------------------------------------------------------------------
In diva2:912816 
abstract is: 
<p>For an apparatus as big as the pension system, the financial stability is essential. An important feature in the existing pension system is the balance mechanism, which secures the stability of the system. The balance ratio is obtained by dividing the assets by the liabilities. When this ratio drops below 1.0000, it triggers the so-called automatic balancing.</p><p>While the existing pension system has achieved its goal of being financially stable, it has become clear that the indexation of the pensions during balancing periods has properties that are not optimal. On a short-term perspective the income pension system is exposed to the risk of reacting with a lag, or reacting unnecessarily strong. This gave rise to a new legislative proposal, issued by the government. The goal of the proposal is to obtain a smoother and more up-to-date development of the income pension, i.e. a shorter lag period, without jeopardizing the financial stability. In addition to this it is also desirable to simplify and improve the existing calculation methods. In order to compare the existing calculation methods in the pension system with the new legislative proposal, a simplified model of the existing pension system and the modified version of it, are created.</p><p>The results of this study shows that the new legislative proposal decreases the volatility in the pensions and it avoids the deepest valleys in the balance ratio. The development of the pension disbursements in the new system has a higher correlation with the development of the average pension-qualifying income than in the current system. Moreover, the results show that the new system has a shorter lag period which makes the income pension system more up- to-date with the current economic and demographic situation.</p><p>The financial stability is still contained, and the new system also handles variations in the inflation better than the current system</p>

corrected abstract:
<p>For an apparatus as big as the pension system, the financial stability is essential. An important feature in the existing pension system is the balance mechanism, which secures the stability of the system. The balance ratio is obtained by dividing the assets by the liabilities. When this ratio drops below 1.0000, it triggers the so-called automatic balancing.</p><p>While the existing pension system has achieved its goal of being financially stable, it has become clear that the indexation of the pensions during balancing periods has properties that are not optimal. On a short-term perspective the income pension system is exposed to the risk of reacting with a lag, or reacting unnecessarily strong. This gave rise to a new legislative proposal, issued by the government. The goal of the proposal is to obtain a smoother and more up-to-date development of the income pension, i.e. a shorter lag period, without jeopardizing the financial stability. In addition to this it is also desirable to simplify and improve the existing calculation methods. In order to compare the existing calculation methods in the pension system with the new legislative proposal, a simplified model of the existing pension system and the modified version of it, are created.</p><p>The results of this study shows that the new legislative proposal decreases the volatility in the pensions and it avoids the deepest valleys in the balance ratio. The development of the pension disbursements in the new system has a higher correlation with the development of the average pension-qualifying income than in the current system. Moreover, the results show that the new system has a shorter lag period which makes the income pension system more up- to-date with the current economic and demographic situation. The financial stability is still contained, and the new system also handles variations in the inflation better than the current system</p>

Note - removed unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1450189   - correct as is
----------------------------------------------------------------------
In diva2:1652587   - correct as is
----------------------------------------------------------------------
In diva2:1833673   - correct as is
----------------------------------------------------------------------
In diva2:753596   - correct as is
----------------------------------------------------------------------
In diva2:1114454   - correct as is
----------------------------------------------------------------------
In diva2:1816770   - correct as is
----------------------------------------------------------------------
In diva2:1546201   - correct as is
----------------------------------------------------------------------
In diva2:1287729   - correct as is
----------------------------------------------------------------------
In diva2:1211493   - correct as is
----------------------------------------------------------------------
In diva2:1216728 
abstract is: 
<p>Reinforcement learning has recently gained popularity due to its many successfulapplications in various fields. In this project reinforcement learning is imple- mented in a simple warehouse situation where robots have to learn to interact with each other while performing specific tasks. The aim is to study whether reinforcement learning can be used to train multiple agents. Two different meth- ods have been used to achieve this aim, Q-learning and deep Q-learning. Due to practical constraints, this paper cannot provide a comprehensive review of real life robot interactions. Both methods are tested on single-agent and multi-agent models in Python computer simulations.</p><p>The results show that the deep Q-learning model performed better in the multi- agent simulations than the Q-learning model and it was proven that agents can learn to perform their tasks to some degree. Although, the outcome of this project cannot yet be considered sufficient for moving the simulation into real- life, it was concluded that reinforcement learning and deep learning methods can be seen as suitable for modelling warehouse robots and their interactions.</p>
mc='successfulapplications' c='successful applications'

corrected abstract:
<p>Reinforcement learning has recently gained popularity due to its many successful applications in various fields. In this project reinforcement learning is implemented in a simple warehouse situation where robots have to learn to interact with each other while performing specific tasks. The aim is to study whether reinforcement learning can be used to train multiple agents. Two different methods have been used to achieve this aim, Q-learning and deep Q-learning. Due to practical constraints, this paper cannot provide a comprehensive review of real life robot interactions. Both methods are tested on single-agent and multi-agent models in Python computer simulations.</p><p>The results show that the deep Q-learning model performed better in the multiagent simulations than the Q-learning model and it was proven that agents can learn to perform their tasks to some degree. Although, the outcome of this project cannot yet be considered sufficient for moving the simulation into reallife, it was concluded that reinforcement learning and deep learning methods can be seen as suitable for modelling warehouse robots and their interactions.</p>
----------------------------------------------------------------------
In diva2:1761947 
abstract is: 
<p>This thesis provides an end-to-end picture of the modelling of interest rates and Foreign Exchange (FX) rates. We start by defining the FX rates and the interest rates. After having a good understanding of the basics, we take a deep dive into the approaches commonly used to model interest rates and FX rates respectively. In particular, we present an interest rate model and a FX rate model that I have developed for man- aging Swedbank’s Counterparty Credit Risk (CCR). In addition to the mathematical derivations, we describe the theories underlying the models, discuss the model com- parisons, and explain the model choices made in practical applications. Finally, we provide a prototype of model implementation to illustrate how theory can be put into practice.</p><p>I had some doubts about the interest rate model and the FX rate model that I have developed for managing Swedbank’s CCR. These doubts have been cleared up through this thesis work. Both the doubts and the clarifications are described in this thesis.</p>

corrected abstract:
<p>This thesis provides an end-to-end picture of the modelling of interest rates and Foreign Exchange (FX) rates. We start by defining the FX rates and the interest rates. After having a good understanding of the basics, we take a deep dive into the approaches commonly used to model interest rates and FX rates respectively. In particular, we present an interest rate model and a FX rate model that I have developed for managing Swedbank’s Counterparty Credit Risk (CCR). In addition to the mathematical derivations, we describe the theories underlying the models, discuss the model comparisons, and explain the model choices made in practical applications. Finally, we provide a prototype of model implementation to illustrate how theory can be put into practice.</p><p>I had some doubts about the interest rate model and the FX rate model that I have developed for managing Swedbank’s CCR. These doubts have been cleared up through this thesis work. Both the doubts and the clarifications are described in this thesis.</p>
----------------------------------------------------------------------
In diva2:1360578   - correct as is
----------------------------------------------------------------------
In diva2:942546 
abstract is: 
<p>This bachelor thesis in applied mathematics and industrial engineering aims to determine if and how weather affects the consumption of goods at small grocery stores. To study this, we conducted a regression analysis based on sales data from an ICA Nära. We have collected one year’s weather- and sales data and used mathematical statistics to determine how weather affects the sales for different product groups. Our belief is that weather does affect the consumption.</p><p>Several large actors in the industry have some sort of consumption of goods forecast. None of these takes weather into account when creating their sale forecast. Hopefully, this thesis will provide information aiding companies in deciding whether or not to use weather forecasts as a prediction parameter. </p><p>The results indicate a large effect on sales for some groups of products. The regression reveals how much the sale of a group increase along with an increase of one unit of the different measured weather factors. There is, most likely, not a perfect linear relation between our response variable and the explanatory variables. Therefore, one must interpret the results carefully. In addition, we discuss how a possible implementation affects the supply chain of a large grocery store company and the importance of flexibility in one’s supply chain.</p>

corrected abstract:
<p>This bachelor thesis in applied mathematics and industrial engineering aims to determine if and how weather affects the consumption of goods at small grocery stores. To study this, we conducted a regression analysis based on sales data from an ICA Nära. We have collected one year’s weather- and sales data and used mathematical statistics to determine how weather affects the sales for different product groups. Our belief is that weather does affect the consumption.</p><p>Several large actors in the industry have some sort of consumption of goods forecast. None of these takes weather into account when creating their sale forecast. Hopefully, this thesis will provide information aiding companies in deciding whether or not to use weather forecasts as a prediction parameter.</p><p>The results indicate a large effect on sales for some groups of products. The regression reveals how much the sale of a group increase along with an increase of one unit of the different measured weather factors. There is, most likely, not a perfect linear relation between our response variable and the explanatory variables. Therefore, one must interpret the results carefully. In addition, we discuss how a possible implementation affects the supply chain of a large grocery store company and the importance of flexibility in one’s supply chain.</p>

Note - only change was to remove an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1117098   - correct as is
----------------------------------------------------------------------
In diva2:1319633   - correct as is
----------------------------------------------------------------------
In diva2:433851 
abstract is: 
<p>The molten core-concrete interaction (MCCI) is treated as one of the important phenomena that may lead to the late containment failure by basemat penetration in a hypothetical severe accident of light water reactors (LWRs). The earlier research has showed that heat transfer limitation exists for the coolability of ex-vessel corium by atop water flooding due to crust formation on the melt/water interface that will isolate melt from water. However, several cooling mechanisms were identified in a series of intense investigations. A code (CORQUENCH) was developed and updated to incorporate the newly identified cooling mechanisms for the better predictions of cavity erosion and corium cooling behaviors. A description about such cooling mechanisms (i.e., bulking cooling, water ingression, eruption and crust breach) and the concrete ablation models implemented in the code is presented in this thesis.</p>
<p>The technical work in the thesis includes two parts: first, the verification and validation of the code were performed against the CCI tests from the OECD/MCCI projects; and then a reactor-scale simulation was carried out for MCCI and ex-vessel corium coolability of a reference PWR with LCS concrete. The calculations of CCI tests have a plausible agreement with the experimental data.</p>
<p>The calculation predicts an optimistic result for the reactor case, and a fast quenching achieved at about 145 minutes. In addition, a sensitivity study was also conducted on several important parameters, i.e., concrete type, corium composition, water flooding time, atmosphere pressure, concrete ablation temperature, initial temperature, decay power, cavity geometry, concrete decomposition model and melt upper heat transfer model. An attempt to explain the physics of the different predicted phenomena is presented as well.</p>
<p>Finally, comparative calculations were performed by the other codes (ASTEC and FinCCI) for the same reactor-scale configuration. Discrepancies are found in the results. Some suggestions are proposed to improve the CORQUENCH code.</p>
<p></p>

corrected abstract:
<p>The molten core-concrete interaction (MCCI) is treated as one of the important phenomena that may lead to the late containment failure by basemat penetration in a hypothetical severe accident of light water reactors (LWRs). The earlier research has showed that heat transfer limitation exists for the coolability of ex-vessel corium by atop water flooding due to crust formation on the melt/water interface that will isolate melt from water. However, several cooling mechanisms were identified in a series of intense investigations. A code (CORQUENCH) was developed and updated to incorporate the newly identified cooling mechanisms for the better predictions of cavity erosion and corium cooling behaviors. A description about such cooling mechanisms (i.e., bulking cooling, water ingression, eruption and crust breach) and the concrete ablation models implemented in the code is presented in this thesis.</p>
<p>The technical work in the thesis includes two parts: first, the verification and validation of the code were performed against the CCI tests from the OECD/MCCI projects; and then a reactor-scale simulation was carried out for MCCI and ex-vessel corium coolability of a reference PWR with LCS concrete. The calculations of CCI tests have a plausible agreement with the experimental data. The calculation predicts an optimistic result for the reactor case, and a fast quenching achieved at about 145 minutes. In addition, a sensitivity study was also conducted on several important parameters, i.e., concrete type, corium composition, water flooding time, atmosphere pressure, concrete ablation temperature, initial temperature, decay power, cavity geometry, concrete decomposition model and melt upper heat transfer model. An attempt to explain the physics of the different predicted phenomena is presented as well.</p>
<p>Finally, comparative calculations were performed by the other codes (ASTEC and FinCCI) for the same reactor-scale configuration. Discrepancies are found in the results. Some suggestions are proposed to improve the CORQUENCH code.</p>

Note - only change was to remove an unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1332815 
abstract is: 
<p>During the past few decades, social responsible investing (SRI) has rapidly grown to become a renowned investment strategy. Because of the contradictory findings on how successful this strategy is in terms of financial return, the aim of this thesis is to compare the performance of sustainable and conventional funds in four different geographical areas during the last three years. With the use of regression analysis, the correlation between the Portfolio Sustainability Score of a fund, which is a Morningstar-provided rating that represents how well a fund incorporates ESG, and its risk-adjusted return is determined.</p><p>The final results of this analysis varies among the four geographical regions. The correlation between the two variables is positive in USA and Asia ex-Japan, whereas a negative relationship is found in Europe and the Nordic region. However, the obtained findings are not of statistical significance, implying that there is no difference between the risk-adjusted returns of sustainable versus conventional funds.</p>

corrected abstract:
<p>During the past few decades, social responsible investing (SRI) has rapidly grown to become a renowned investment strategy. Because of the contradictory findings on how successful this strategy is in terms of financial return, the aim of this thesis is to compare the performance of sustainable and conventional funds in four different geographical areas during the last three years. With the use of regression analysis, the correlation between the Portfolio Sustainability Score of a fund, which is a Morningstar-provided rating that represents how well a fund incorporates ESG, and its risk-adjusted return is determined.</p><p>The final results of this analysis varies among the four geographical regions. The correlation between the two variables is positive in USA and Asia ex-Japan, whereas a negative relationship is found in Europe and the Nordic region. However, the obtained findings are either not of statistical significance or not representative of the given data, implying that there is no difference between the risk-adjusted returns of sustainable versus conventional funds.</p>

Note - added missing text
----------------------------------------------------------------------
In diva2:820906 - error in title:
"A thesis submitted in fulfilment of the requirements for the degree of Masters of Mathematics"
==>
"Bijections on Catalan Structures"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:822466   - correct as is
----------------------------------------------------------------------
In diva2:608591   - correct as is
----------------------------------------------------------------------
In diva2:1807977 
abstract is: 
<p>Acoustic focusing of microscale protein crystals with acoustophoresis technology could reduce clogs during experiments with the scientific technique serial femtosecond x-ray crystallography (SFX). SFX determines molecular structures of proteins, these structures are valuable in drug discovery and fundamental biomedical research.</p><p>Lysozyme crystals were focused in their own mother liquor and dilutions with PBS buffer. The aim of these tests were to study how the acoustic contrast factor Φ changes with the medium. Recorded experiments were analyzed using the particle tracking software Trackmate to extract velocities and radii.</p><p>The lysozyme crystals changed morphologies in large dilutions of PBS buffert, they either became rounder or broke into fragments. The changed forms are likely caused by dissolution behaviors; some dilutions were unstable, but not unstable enough to dissolve the crystals completely. </p><p>Measured velocities during focusing of the crystals had large variance. Sinusoidal fits of the velocities had significant increases in amplitudes for larger dilutions of PBS. A change in acoustic contrast factor Φ could be the cause for the increased amplitudes, but the results do not rule out other causes. There are currently major knowledge gaps about using protein crystals as particles with acoustophoresis technologies, hence many ideas for future works have been proposed in this master thesis report.</p>

corrected abstract:
<p>Acoustic focusing of microscale protein crystals with acoustophoresis technology could reduce clogs during experiments with the scientific technique serial femtosecond x-ray crystallography (SFX). SFX determines molecular structures of proteins, these structures are valuable in drug discovery and fundamental biomedical research.</p><p>Lysozyme crystals were focused in their own mother liquor and dilutions with PBS buffer. The aim of these tests were to study how the acoustic contrast factor Φ changes with the medium. Recorded experiments were analyzed using the particle tracking software Trackmate to extract velocities and radii.</p><p>The lysozyme crystals changed morphologies in large dilutions of PBS buffert, they either became rounder or broke into fragments. The changed forms are likely caused by dissolution behaviors; some dilutions were unstable, but not unstable enough to dissolve the crystals completely.</p><p>Measured velocities during focusing of the crystals had large variance. Sinusoidal fits of the velocities had significant increases in amplitudes for larger dilutions of PBS. A change in acoustic contrast factor Φ could be the cause for the increased amplitudes, but the results do not rule out other causes. There are currently major knowledge gaps about using protein crystals as particles with acoustophoresis technologies, hence many ideas for future works have been proposed in this master thesis report.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1516090   - correct as is
----------------------------------------------------------------------
In diva2:1757072 
abstract is: 
<p>Despite much research indicating that acquisitions are unsatisfactory in generating value, in terms of stock market return, their continued and growing existence highlights that acquisitions play an essential role in the corporate landscape, and will only continue doing so moving forward. </p><p>This continuous undertaking in acquisitions despite a lacking performance inspired a thesis that is focused on viewing acquisitions through an operational perspective. The aim of the thesis is to answer the question: "What impacts the difference in operational efficiency between the group of companies, that is formed after an acquisition, and the separate companies, that operate before an acquisition?" A multiple linear regression model analysis is performed with explanatory regressors that are mainly based on the target-acquirer relation.</p><p>The final model's relatively low explanatory power combined with certain model assumption violations and a challenging sample of observations harms the reliability of the significant regressors in the final model. The significant regressors are the acquirer's average annual EBIT margin growth rate before the acquisition and the cross-border regressor. The two regressors are in line with acquisition and financial accounting theory. The conclusion, however, is that analyzing acquisitions in this regard is questionable and could very well not return informative results that can be applied to future acquisitions to improve the unsatisfactory returns.</p>

corrected abstract:
<p>Despite much research indicating that acquisitions are unsatisfactory in generating value, in terms of stock market return, their continued and growing existence highlights that acquisitions play an essential role in the corporate landscape, and will only continue doing so moving forward.</p><p>This continuous undertaking in acquisitions despite a lacking performance inspired a thesis that is focused on viewing acquisitions through an operational perspective. The aim of the thesis is to answer the question: ”What impacts the difference in operational efficiency between the group of companies, that is formed after an acquisition, and the separate companies, that operate before an acquisition?” A multiple linear regression model analysis is performed with explanatory regressors that are mainly based on the target-acquirer relation.</p><p>The final model's relatively low explanatory power combined with certain model assumption violations and a challenging sample of observations harms the reliability of the significant regressors in the final model. The significant regressors are the acquirer's average annual EBIT margin growth rate before the acquisition and the cross-border regressor. The two regressors are in line with acquisition and financial accounting theory. The conclusion, however, is that analyzing acquisitions in this regard is questionable and could very well not return informative results that can be applied to future acquisitions to improve the unsatisfactory returns.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:653296 
abstract is: 
<p>This work analyzes the implementation of an active train pantograph in a full finite element model in the program Ansys. As controller design method, the H∞ method was taken in order to cope with the different uncertainties in the given system. The focus lies on the contact force between the pantograph and the catenary. The goal was the reduction of the contact force standard deviation in order to allow higher train speeds on existing lines. An additional goal is the use of multi train configurations. This means that two coupled trains with a distance between the two pantographs of 100 meters can run with high speed on existing lines. Current regulations limit the distance to 200 meters. In addition to the active solutions, different modifications of the given pantograph were investigated.</p><p>The simulations showed that the desired speed of 280 km/h is achieved on existing lines in multi train configuration. For only one train, a speed of up to 300 km/h can be reached. More important, by using an estimator, the standard deviation values for these speeds were still below the limitations and hence, it is possible to implement this solution in a real system.</p>

corrected abstract:
<p>This work analyzes the implementation of an active train pantograph in a full finite element model in the program Ansys. As controller design method, the <img src="http://www.diva-portal.org/cgi-bin/mimetex.cgi?\mathcal{H}"  title="\mathcal{H} " alt="LaTeX: \mathcal{H}" /><sub>∞</sub> method was taken in order to cope with the different uncertainties in the given system. The focus lies on the contact force between the pantograph and the catenary. The goal was the reduction of the contact force standard deviation in order to allow higher train speeds on existing lines. An additional goal is the use of multi train configurations. This means that two coupled trains with a distance between the two pantographs of 100 meters can run with high speed on existing lines. Current regulations limit the distance to 200 meters. In addition to the active solutions, different modifications of the given pantograph were investigated.</p><p>The simulations showed that the desired speed of 280 <sup>km</sup>/<sub>h</sub> is achieved on existing lines in multi train configuration. For only one train, a speed of up to 300 <sup>km</sup>/<sub>h</sub> can be reached.</p><p>More important, by using an estimator, the standard deviation values for these speeds were still below the limitations and hence, it is possible to implement this solution in a real system.</p>

Note the "<sup>km</sup>/<sub>h</sub>" and I added the mimtex to get the \mathcal{H}
----------------------------------------------------------------------
In diva2:1247182 
abstract is: 
<p>Master of Science thesis in Naval Architecture presents a study and the performance of an active seat suspension with the purpose to suppress shocks, caused by slamming in High Speed Crafts (HSCs). The system is modelled and simulated with the aid of the Mathworks software Simulink, with the main objective to evaluate if the active suspension seat has the potential to mitigate slamming impact loads to a larger extent compared to a passive suspension seat. The active suspension model is developed by adding a PD-controlled actuator in parallel with the spring and damper of a passive seat’s suspension. This paper presents the performance study of an active suspension seat where the seat is given a single impact load as input. The results are then compared to a comparable passive seat. The most promising results show that the active system can reduce the passenger seat’s acceleration response by roughly 30 %. This is achieved on the expense of an increased stroke length, from 30 mm for a comparable passive system, to 34 mm for the active system. To achieve this the actuator need to provide up to 900 N of force with a rise-time of 15 ms. During the assessment of the suspension seat performance four key performance indicators(KPI) were found to be of significance. Those are the seat response acceleration, seat displacement relative to the seat base, settling time and the zero crossing time. The seat acceleration is directly proportional to the load that the passenger is being subjected to. Hence, the acceleration is the property that needs to be reduced in order to decrease risk of injuries. The stroke length of the seat in relation to the seat base should be kept to a minimum for several reasons. One being the risk of bottoming out the suspension if the stroke length is too high, risking damage on equipment as well as injuries on passenger. Since the conditions on sea entail series of impact loads on the hull, the settling time need to be as short as possible to avoid accumulating the displacement. This is caused when the seat has not yet returned to its neutral position before next impact occurs. To define the response time of the system, the zero-crossing performance indicator was defined. Zero-crossing time is defined as the time from when the displacement of the seat starts (the suspension being compressed) until it returns and crosses the neutral position, regardless if the suspension stops at the neutral position or continue extending. A correlation was found between the zero-crossing time and the settling time. Both KPIs are dependent on the 𝑃𝐺𝑎𝑖𝑛 (the proportional part of the PD-control)and what is found is that a short zero-crossing time entail an increased overshoot, that in turn results in a longer settling time due to the seat’s oscillation about the neutral position. The active suspension seat model in this paper can be further developed and evaluated with respect to the performance indicators stated by (European Union, 2002) like VDV, RMS acceleration values etc.</p>

corrected abstract:
<p>This Master of Science thesis in Naval Architecture presents a study and the performance of an active seat suspension with the purpose to suppress shocks, caused by slamming in High Speed Crafts (HSCs). The system is modelled and simulated with the aid of the Mathworks software Simulink, with the main objective to evaluate if the active suspension seat has the potential to mitigate slamming impact loads to a larger extent compared to a passive suspension seat. The active suspension model is developed by adding a PD-controlled actuator in parallel with the spring and damper of a passive seat’s suspension.</p><p>This paper presents the performance study of an active suspension seat where the seat is given a single impact load as input. The results are then compared to a comparable passive seat. The most promising results show that the active system can reduce the passenger seat’s acceleration response by roughly 30 %. This is achieved on the expense of an increased stroke length, from 30 mm for a comparable passive system, to 34 mm for the active system. To achieve this the actuator need to provide up to 900 N of force with a rise-time of 15 ms.</p><p>During the assessment of the suspension seat performance four key performance indicators (KPI) were found to be of significance. Those are the seat response acceleration, seat displacement relative to the seat base, settling time and the zero crossing time. The seat acceleration is directly proportional to the load that the passenger is being subjected to. Hence, the acceleration is the property that needs to be reduced in order to decrease risk of injuries. The stroke length of the seat in relation to the seat base should be kept to a minimum for several reasons. One being the risk of bottoming out the suspension if the stroke length is too high, risking damage on equipment as well as injuries on passenger. Since the conditions on sea entail series of impact loads on the hull, the settling time need to be as short as possible to avoid accumulating the displacement. This is caused when the seat has not yet returned to its neutral position before next impact occurs. To define the response time of the system, the zero-crossing performance indicator was defined. Zero-crossing time is defined as the time from when the displacement of the seat starts (the suspension being compressed) until it returns and crosses the neutral position, regardless if the suspension stops at the neutral position or continue extending. A correlation was found between the zero-crossing time and the settling time. Both KPIs are dependent on the 𝑃<sub>𝐺𝑎𝑖𝑛</sub> (the proportional part of the PD-control) and what is found is that a short zero-crossing time entail an increased overshoot, that in turn results in a longer settling time due to the seat’s oscillation about the neutral position.</p><p>The active suspension seat model in this paper can be further developed and evaluated with respect to the performance indicators stated by (European Union, 2002) like VDV, RMS acceleration values etc.</p>

Note added subscript and missing paragraph breaks; also add the initial missing word
----------------------------------------------------------------------
In diva2:1816887   - correct as is
----------------------------------------------------------------------
In diva2:1259795   - correct as is
----------------------------------------------------------------------
In diva2:1780244   - correct as is
----------------------------------------------------------------------
In diva2:1843154   - correct as is
----------------------------------------------------------------------
In diva2:1356948   - correct as is
----------------------------------------------------------------------
In diva2:1066362 
abstract is: 
<p>In order for wind-farm operators to deal with challenges regarding their fleet management, it is useful for them to estimate their units’ performance for different conditions. To perform such estimations, Computational Fluid Dynamics (CFD) may be used.</p><p>This project focuses on the development of a CFD model for the aerodynamic analysis of wind turbine rotors, depending on their surface roughness. The work has been carried out in collaboration with the KTH Royal Institute of Technology and the Vattenfall AB R&amp;D department.</p><p>The open-source software OpenFOAM has been used to develop the desired model. A rigid body incompressible steady state, Reynolds-Averaged Navier-Stokes equations, <em>k – ω </em>SST CFD case has been set up. The NREL 5-MW rotor geometry has been used and the effect of four different surface roughness height values {1mm, 0.5mm, 100 μm, 30 μm} on its aerodynamic performance has been investigated for an incoming wind velocity of 10m/s. The referred roughness height values have been applied on the whole rotor surface. A 120° wedge type computational domain of unstructured mesh has been developed for the present simulations.</p><p>The results indicate that a roughness-height increase leads to earlier flow separation over the blade suction side and increases the turbulent area of the boundary layer. That leads to a decrease for the extracted <em>Torque </em>and the <em>Thrust </em>force on the wind turbine rotor. Moreover, it is concluded that the rotor aerodynamic performance is more sensitive to low roughness heights rather than to high ones.</p>

corrected abstract:
<p>In order for wind-farm operators to deal with challenges regarding their fleet management, it is useful for them to estimate their units’ performance for different conditions. To perform such estimations, Computational Fluid Dynamics (CFD) may be used.</p><p>This project focuses on the development of a CFD model for the aerodynamic analysis of wind turbine rotors, depending on their surface roughness. The work has been carried out in collaboration with the KTH Royal Institute of Technology and the Vattenfall AB R&amp;D department.</p><p>The open-source software OpenFOAM has been used to develop the desired model. A rigid body incompressible steady state, Reynolds-Averaged Navier-Stokes equations, <em>𝑘 – ω</em> SST CFD case has been set up. The NREL 5-MW rotor geometry has been used and the effect of four different surface roughness height values {1mm, 0.5mm, 100 μm, 30 μm} on its aerodynamic performance has been investigated for an incoming wind velocity of 10m/s. The referred roughness height values have been applied on the whole rotor surface. A 120° wedge type computational domain of unstructured mesh has been developed for the present simulations.</p><p>The results indicate that a roughness-height increase leads to earlier flow separation over the blade suction side and increases the turbulent area of the boundary layer. That leads to a decrease for the extracted <em>Torque</em> and the <em>Thrust</em> force on the wind turbine rotor. Moreover, it is concluded that the rotor aerodynamic performance is more sensitive to low roughness heights rather than to high ones.</p>

Note adjected the italics region and converted "k" to "𝑘"
----------------------------------------------------------------------
In diva2:757141   - correct as is
----------------------------------------------------------------------
In diva2:653238 - missing space in title:
"Aeroelastic instabilities simulations on turbofan HP compressorblisk at surge-like reversed flow conditions"
==>
"Aeroelastic instabilities simulations on turbofan HP compressor blisk at surge-like reversed flow conditions"

abstract is: 
<p>This paper presents the work done at the High Pressure Compressor design division of Safran Snecma to simulate the flutter behaviour of a compressor blisk at reversed flow conditions. The report states the preliminary phase of modelling the reversed flow on an entire compressor with a stationary simulation and the single blade aeroelastic study results obtained. One can then get interpretations on the blow down aerodynamics that can affect the mechanical behaviour of the blade. Eight simulations of a single blade model have been done on two aeroelastic configurations, each one on a rotor from two different compressor designs. By investigating the airflow around the blade profiles, it becomes possible to get values on the aerodynamic damping on the blade. As presented below, even with strong surge stresses, the compressor stage will remain flutter-free.</p>

corrected abstract:
<p>This paper presents the work done at the High Pressure Compressor design division of <em>Safran Snecma</em> to simulate the flutter behaviour of a compressor blisk at reversed flow conditions. The report states the preliminary phase of modelling the reversed flow on an entire compressor with a stationary simulation and the single blade aeroelastic study results obtained. One can then get interpretations on the blow down aerodynamics that can affect the mechanical behaviour of the blade. Eight simulations of a single blade model have been done on two aeroelastic configurations, each one on a rotor from two different compressor designs. By investigating the airflow around the blade profiles, it becomes possible to get values on the aerodynamic damping on the blade. As presented below, even with strong surge stresses, the compressor stage will remain flutter-free.</p>

Note - added italics
----------------------------------------------------------------------
In diva2:398320   - correct as is
----------------------------------------------------------------------
In diva2:1578571 
abstract is: 
<p>Growing environmental concerns are causing a large transformation within the energy industry. Within the gas turbine industry, there is a large drive to develop improved modern dry-low emission combustion systems. The aim is to enable gas turbines to run on green fuels like hydrogen, while still keeping emission as NOx down. To design these systems, a thorough understanding of the aerothermal and kinetic processes within the combustion system of a gas turbine is essential.</p><p>The goal of the thesis was to develop a one-dimensional general network model of the combustion system of Siemens Energy SGT-700, which accurately could predict pressure losses, mass flows, key temperatures, and emissions.</p><p>Three models were evaluated and a code that emulated some aspects of the control system was developed. The models and the code were evaluated and compared to each other and to test data from earlier test campaigns performed on SGT-700 and SGT-600. Simulations were also carried out with hydrogen as the fuel. </p><p>In the end, a model of the SGT-700 combustion chamber was developed and delivered to Siemens Energy. The model had been verified against test data and predictions made by other Siemens Energy thermodynamic calculation software, for a range of load conditions. The preforms of the model, when hydrogen was introduced into the fuel mixture, were also tested and compared to test data</p>

corrected abstract:
<p>Growing environmental concerns are causing a large transformation within the energy industry. Within the gas turbine industry, there is a large drive to develop improved modern dry-low emission combustion systems. The aim is to enable gas turbines to run on green fuels like hydrogen, while still keeping emission as NOx down. To design these systems, a thorough understanding of the aerothermal and kinetic processes within the combustion system of a gas turbine is essential.</p><p>The goal of the thesis was to develop a one-dimensional general network model of the combustion system of Siemens Energy SGT-700, which accurately could predict pressure losses, mass flows, key temperatures, and emissions.</p><p>Three models were evaluated and a code that emulated some aspects of the control system was developed. The models and the code were evaluated and compared to each other and to test data from earlier test campaigns performed on SGT-700 and SGT-600. Simulations were also carried out with hydrogen as the fuel.</p><p>In the end, a model of the SGT-700 combustion chamber was developed and delivered to Siemens Energy. The model had been verified against test data and predictions made by other Siemens Energy thermodynamic calculation software, for a range of load conditions. The preforms of the model, when hydrogen was introduced into the fuel mixture, were also tested and compared to test data.</p>

Note - small changes in text
----------------------------------------------------------------------
In diva2:1757040 
abstract is: 
<p>The purpose of this report is to, together with a Swedish bank, evaluate whether publicly owned companies within the Oil &amp; Gas industry are valued differently based on their ESG (Environmental, Social and Governance) performance. To examine if there is a correlation between ESG-factors and valuation, a mathematical model will be built with linear regression analysis. </p><p>The model that best describes the data does not give a satisfactory enough correlation to conclude that ESG-factors generally impact valuation of companies and it is therefore not possible to, with significance, conclude that companies with strong ESG-profiles enjoy a higher valuation than its peers. However, there is a significant correlation between one of the regressors in the model and valuation which could imply the existence of a correlation but it is not significant enough to model accurately. There is also the problem of lack of data, many companies do not report ESG-factors which complicates modeling. There seems to be a difference in ESG-reporting based on region and company size which could be used as motivation for future studies of even more homogenous companies. Finally, potential improvements of the model are discussed </p>

corrected abstract:
<p>The purpose of this report is to, together with a Swedish bank, evaluate whether publicly owned companies within the Oil &amp; Gas industry are valued differently based on their ESG (Environmental, Social and Governance) performance. To examine if there is a correlation between ESG-factors and valuation, a mathematical model will be built with linear regression analysis.</p><p>The model that best describes the data does not give a satisfactory enough correlation to conclude that ESG-factors generally impact valuation of companies and it is therefore not possible to, with significance, conclude that companies with strong ESG-profiles enjoy a higher valuation than its peers. However, there is a significant correlation between one of the regressors in the model and valuation which could imply the existence of a correlation but it is not significant enough to model accurately. There is also the problem of lack of data, many companies do not report ESG-factors which complicates modeling. There seems to be a difference in ESG-reporting based on region and company size which could be used as motivation for future studies of even more homogenous companies. Finally, potential improvements of the model are discussed.</p>

Note miniro adjustments to the text.
----------------------------------------------------------------------
In diva2:1450570   - correct as is
----------------------------------------------------------------------
In diva2:1449911   - correct as is
----------------------------------------------------------------------
In diva2:802097 
abstract is: 
<p>The condition of the roads is a factor that may not only affect the wear of a vehicle, car or truck, but as well may reduce fuel consumption, increase comfort, lower noise and maybe most importantly increase traffic safety. This gives a need of a system that can measure road quality and detect potholes, which could be of interest to haulers and to local road authorities that would get valuable information of road sections that are in need of maintenance.</p><p>In this Master Thesis different algorithms were developed, and tested, that could automatically detect different kind of road anomalies using only an three-axis accelerometer mounted on the chassis of heavy duty trucks from Scania. Data collection was performed using two different trucks and the road anomalies were noted by the co-driver using the keyboard of a laptop. This Master Thesis also explored the correlation between the acceleration levels on the chassis and high elongation values on the front leaf spring.</p><p>Using a developed evaluation framework, the anomaly detections from the different algorithms were compared to the test oracle to determine if the anomaly detection given by the algorithm was a true positive hit or a false positive. A great advantage of the developed evaluation framework is that additional algorithms could easily be added for evaluation. For the evaluation of the algorithms the statistical F-measure, which is the harmonic mean of the precision and sensitivity, was used for the test’s accuracy of the algorithms.</p><p>The two algorithms that had the best performance results regarding detection of road anomalies were Algorithm – T and Algorithm – SDT. These two algorithms had a F-measure score of 65% and 64% respectively when the precision and sensitivity were equally weighted.</p><p>For the correlation between acceleration levels and high elongation levels, Algorithm – SDT scored the highest F-measure value of 14%. This value is far from satisfying and a reason for the low value is that the algorithms were primarily developed for detection of road anomalies.</p>

corrected abstract:
<p>The condition of the roads is a factor that may not only affect the wear of a vehicle, car or truck, but as well may reduce fuel consumption, increase comfort, lower noise and maybe most importantly increase traffic safety. This gives a need of a system that can measure road quality and detect potholes, which could be of interest to haulers and to local road authorities that would get valuable information of road sections that are in need of maintenance.</p><p>In this Master Thesis different algorithms were developed, and tested, that could automatically detect different kind of road anomalies using only an three-axis accelerometer mounted on the chassis of heavy duty trucks from Scania. Data collection was performed using two different trucks and the road anomalies were noted by the co-driver using the keyboard of a laptop. This Master Thesis also explored the correlation between the acceleration levels on the chassis and high elongation values on the front leaf spring.</p><p>Using a developed evaluation framework, the anomaly detections from the different algorithms were compared to the test oracle to determine if the anomaly detection given by the algorithm was a true positive hit or a false positive. A great advantage of the developed evaluation framework is that additional algorithms could easily be added for evaluation. For the evaluation of the algorithms the statistical F-measure, which is the harmonic mean of the precision and sensitivity, was used for the test’s accuracy of the algorithms.</p><p>The two algorithms that had the best performance results regarding detection of road anomalies were Algorithm – T and Algorithm – SDT. These two algorithms had a F-measure score of 65% and 64% respectively when the precision and sensitivity were equally weighted. For the correlation between acceleration levels and high elongation levels, Algorithm – SDT scored the highest F-measure value of 14%. This value is far from satisfying and a reason for the low value is that the algorithms were primarily developed for detection of road anomalies.</p>

Note remove the unneeded paragraph break
----------------------------------------------------------------------
In diva2:1319888 
abstract is: 
<p>The equity option expiration effect is a well observed phenomenon and is explained by delta hedge rebalancing and pinning risk, which makes the strike price of an option work as a magnet for the underlying price. The FX option expiration effect has not previously been explored to the same extent. In this paper the FX option expiration effect is investigated with the aim of finding out whether it provides valuable information for predicting FX rate movements. New models are created based on the concept of the option relevance coefficient that determines which options are at higher risk of being in the money or out of the money at a specified future time and thus have an attraction effect. An algorithmic trading strategy is created to evaluate these models. The new models based on the FX option expiration effect strongly outperform time series models used as benchmarks. The best results are obtained when the information about the FX option expiration effect is included as an exogenous variable in a GARCH-X model. However, despite promising and consistent results, more scientific research is required to be able to draw significant conclusions.</p>

corrected abstract:
<p>The equity option expiration effect is a well observed phenomenon and is explained by delta hedge rebalancing and pinning risk, which makes the strike price of an option work as a magnet for the underlying price. The FX option expiration effect has not previously been explored to the same extent. In this paper the FX option expiration effect is investigated with the aim of finding out whether it provides valuable information for predicting FX rate movements. New models are created based on the concept of the <em>option relevance coefficient</em> that determines which options are at higher risk of being in the money or out of the money at a specified future time and thus have an attraction effect. An algorithmic trading strategy is created to evaluate these models. The new models based on the FX option expiration effect strongly outperform time series models used as benchmarks. The best results are obtained when the information about the FX option expiration effect is included as an exogenous variable in a GARCH-X model. However, despite promising and consistent results, more scientific research is required to be able to draw significant conclusions.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1219121 
abstract is: 
<p>Strassen’s algorithm was one of the breakthroughs in matrix analysis in 1968. In this report the thesis of Volker Strassen’s algorithm for matrix multipli- cations along with theories about precisions will be shown. The benefits of using this algorithm compared to naive matrix multiplication and its implica- tions, how its performance compare to the naive algorithm, will be displayed. Strassen’s algorithm will also be assessed on how the output differ when the matrix sizes grow larger, as well as how the theoretical complexity of the al- gorithm differs from the achieved complexity.</p><p>The studies found that Strassen’s algorithm outperformed the naive matrix multiplication at matrix sizes 1024 1024 and above. The achieved complex- ity was a little higher compared to Volker Strassen’s theoretical. The optimal precision for this case were the double precision, Float64.</p><p>How the algorithm is implemented in code matters for its performance. A number of techniques need to be considered in order to improve Strassen’s algorithm, optimizing its termination criterion, the manner by which it is padded in order to make it more usable for recursive application and the way it is implemented e.g. parallel computing. Even tough it could be proved that Strassen’s algorithm outperformed the Naive after reaching a certain matrix size, it is still not the most efficient one; e.g. as shown with Strassen-Winograd. One need to be careful of how the sub-matrices are being allocated, to not use unnecessary memory. For further reading one can study cache-oblivious and cache-aware algorithms.</p>

corrected abstract:
<p>Strassen’s algorithm was one of the breakthroughs in matrix analysis in 1968. In this report the thesis of Volker Strassen’s algorithm for matrix multiplications along with theories about precisions will be shown. The benefits of using this algorithm compared to naive matrix multiplication and its implications, how its performance compare to the naive algorithm, will be displayed. Strassen’s algorithm will also be assessed on how the output differ when the matrix sizes grow larger, as well as how the theoretical complexity of the algorithm differs from the achieved complexity.</p><p>The studies found that Strassen’s algorithm outperformed the naive matrix multiplication at matrix sizes 1024 × 1024 and above. The achieved complexity was a little higher compared to Volker Strassen’s theoretical. The optimal precision for this case were the double precision, Float64.</p><p>How the algorithm is implemented in code matters for its performance. A number of techniques need to be considered in order to improve Strassen’s algorithm, optimizing its termination criterion, the manner by which it is padded in order to make it more usable for recursive application and the way it is implemented e.g. parallel computing. Even tough it could be proved that Strassen’s algorithm outperformed the Naive after reaching a certain matrix size, it is still not the most efficient one; e.g. as shown with Strassen-Winograd. One need to be careful of how the sub-matrices are being allocated, to not use unnecessary memory. For further reading one can study cache-oblivious and cache-aware algorithms.</p>

Note added missing "×" and remove unnecessary hyphens
----------------------------------------------------------------------
In diva2:696780 
abstract is: 
<p>We use regime switching and regression tree methods to evaluate performance in the risk premia strategies provided by Deutsche Bank and constructed from U.S. research data from the Fama French library. The regime switching method uses the Baum-Welch algorithm at its core and splits return data into a normal and a turbulent regime. Each regime is independently evaluated for risk and the estimates are then weighted together according to the expected value of the proceeding regime. The regression tree methods identify macro-economic states in which the risk premia perform well or poorly and use these results to allocate between risk premia strategies. The regime switching method proves to be mostly unimpressive but has its results boosted by investing less into risky assets as the probability of an upcoming turbulent regime becomes larger. This proves to be highly effective for all time periods and for both data sources. The regression tree method proves the most effective when making the assumption that we know all macro-economic data the same month as it is valid for. Since this is an unrealistic assumption the best method seems to be to evaluate the performance of the risk premia strategy using macro-economic data from the previous quarter.</p>

corrected abstract:
<p>We use regime switching and regression tree methods to evaluate performance in the risk premia strategies provided by Deutsche Bank and constructed from U.S. research data from the Fama French library. The regime switching method uses the Baum-Welch algorithm at its core and splits return data into a normal and a turbulent regime. Each regime is independently evaluated for risk and the estimates are then weighted together according to the expected value of the proceeding regime. The regression tree methods identify macro-economic states in which the risk premia perform well or poorly and use these results to allocate between risk premia strategies.</p><p>The regime switching method proves to be mostly unimpressive but has its results boosted by investing less into risky assets as the probability of an upcoming turbulent regime becomes larger. This proves to be highly effective for all time periods and for both data sources. The regression tree method proves the most effective when making the assumption that we know all macro-economic data the same month as it is valid for. Since this is an unrealistic assumption the best method seems to be to evaluate the performance of the risk premia strategy using macro-economic data from the previous quarter.</p>

Note add missing paragraph break
----------------------------------------------------------------------
In diva2:1699856   - correct as is
----------------------------------------------------------------------
In diva2:1319481 
abstract is: 
<p>The aim of this thesis is to optimize hydro power plants with data generated from observations and field tests at the plants. The output is optimal production tables and curves in order to operate and plan hydro power plants in an optimized way concerning power output, efficiency and distribution of water. The thesis is performed in collaboration with Vattenfall AB, which currently use an internal optimization program called SEVAP. Two alternative methods have been selected, employed and compared with the current optimization program, these are Interior-Point Method and Sequential Quadratic Programming. Three start-point strategies are created to increase the probability of finding a global optima. A heuristic rule is used for selection of strategy in order to prevent rapid changes in load distribution for small variations in dispatched water. The optimization is performed at three plants in Sweden with different size and setup. The results of this evaluation showed marginally better results for the employed methods in comparison to the currently used optimization. Further, the developed program is more flexible and compatible to integrate with future digitalization projects.</p>

corrected abstract:
<p>The aim of this thesis is to optimize hydro power plants with data generated from observations and field tests at the plants. The output is optimal production tables and curves in order to operate and plan hydro power plants in an optimized way concerning power output, efficiency and distribution of water. The thesis is performed in collaboration with Vattenfall AB, which currently use an internal optimization program called SEVAP.</p><p>Two alternative methods have been selected, employed and compared with the current optimization program, these are Interior-Point Method and Sequential Quadratic Programming. Three start-point strategies are created to increase the probability of finding a global optima. A heuristic rule is used for selection of strategy in order to prevent rapid changes in load distribution for small variations in dispatched water. The optimization is performed at three plants in Sweden with different size and setup.</p><p>The results of this evaluation showed marginally better results for the employed methods in comparison to the currently used optimization. Further, the developed program is more flexible and compatible to integrate with future digitalization projects.</p>

Note - only change to add the missing paragraph breaks
----------------------------------------------------------------------
In diva2:942567   - correct as is
----------------------------------------------------------------------
In diva2:1078803   - correct as is
----------------------------------------------------------------------
In diva2:676730 - missing space in title:
"An Aeroelastic Implementation for YachtSails and Rigs"
==>
"An Aeroelastic Implementation for Yacht Sails and Rigs"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1211062 - missing space in title:
"An analysis of how variables andhome styling affect housing prices"
==>
"An analysis of how variables and home styling affect housing prices"

abstract is: 
<p>Based on the growing interest for home styling and earlier psychological scientific evidence, this study examines how home styling and other variables affect the final price of condominiums in Uppsala. Using multiple linear regression and different statistics, seven different models are analyzed in order to determine whether or not home styling is an influencing factor. To obtain a reliable result, nine other variables such as starting price, living area and floor level etc. are included in the initial model. In addition, these models are investigated statistically to determine if near linear dependence among the regressor variables exists or not. The results show that home styling have a positive impact on the final price of a condominium. The different analytical methods do not always agree, but if looking at the regression result and confidence interval it is obvious that home styling can help increase the final price. Using variable selection, home styling is only included in the model when allowing seven or more variables. The results and analysis from this report is not enough to determine exactly how much home styling affects the final price; since home styling is converted to a dummy variable in the study. The conclusion is that there is a correlation between the response, final price, and the regressor variable, home styling.</p>

corrected abstract:
<p>Based on the growing interest for home styling and earlier psychological scientific evidence, this study examines how home styling and other variables affect the final price of condominiums in Uppsala. Using multiple linear regression and different statistics, seven different models are analyzed in order to determine whether or not home styling is an influencing factor. To obtain a reliable result, nine other variables such as starting price, living area and floor level etc. are included in the initial model. In addition, these models are investigated statistically to determine if near linear dependence among the regressor variables exists or not. The results show that home styling have a positive impact on the final price of a condominium. The different analytical methods do not always agree, but if looking at the regression result and confidence interval it is obvious that home styling can help increase the final price. Using variable selection, home styling is only included in the model when allowing seven or more variables. The results and analysis from this report is not enough to determine exactly how much home styling affects the final price; since home styling is converted to a dummy variable in the study. The conclusion is that there is a correlation between the response, final price, and the regressor varible, home styling.</p>

Note spelling error:
"varible" should be "variable"
----------------------------------------------------------------------
In diva2:1450550 
abstract is: 
<p>The purpose of this thesis is to define which variables affect the average credit spread on the Swedish bond market. The study is conducted via the help of Enter Fonder, who contributes with data and insight into the Swedish corporate bond market. Earlier research has put a lot of weight on the connection between default risk and credit spread. The exact effect is however still debated and it is unclear which variables best describe the default risk. A multilinear regression analysis is conducted, studying the effect on the average credit spread in the NOMX-index (NOMXCRSP) with the following predictor variables: Treasury rate, Predicted EPS amongst OMXS30-companies, Change in net asset under management (AUM) of Swedish corporate bonds, The average credit spread on two European and two American counterparts to NOMX, D/E-ratio and EBITDA-margin amongst OMXS30-companies and finally PMI-index from both the industry and service sector. The regression analysis is based on 89 data points which were aggregated into an equivalent interval on a monthly basis. The final results presents a model of seven variables consisting of all the four international indexes, treasury rate, predicted EPS and change in net AUM, and was able to explain around 87% of the variance in the data.</p>

corrected abstract:
<p>The purpose of this thesis is to define which variables affect the average credit spread on the Swedish bond market. The study is conducted via the help of Enter Fonder, who contributes with data and insight into the Swedish corporate bond market. Earlier research has put a lot of weight on the connection between default risk and credit spread. The exact effect is however still debated and it is unclear which variables best describe the default risk. A multilinear regression analysis is conducted, studying the effect on the average credit spread in the NOMX-index (NOMXCRSP) with the following predictor variables: <em>Treasury rate</em>, <em>Predicted EPS amongst OMXS30-companies</em>, <em>Change in net asset under management (AUM) of Swedish corporate bonds</em>, <em>The average credit spread on two European and two American counterparts to NOMX</em>, <em>D/E-ratio and EBITDA-margin amongst OMXS30-companies<em> and finally <em>PMI-index from both the industry and service sector<em>. The regression analysis is based on 89 data points which were aggregated into an equivalent interval on a monthly basis. The final results presents a model of seven variables consisting of all the four international indexes, treasury rate, predicted EPS and change in net AUM, and was able to explain around 87% of the variance in the data.</p>

Note added italics to match original
----------------------------------------------------------------------
In diva2:942570 --- title does not match that of thesis:
"An analysis on the prices of French wines and on a business plan of creating a wine app"
==>
"An analysis on the precis of French wines and on a business plan of creating a wine app"

abstract is: 
<p>The purpose of this thesis is to evaluate the influences behind the pricing of French wines by the use of Applied Mathematics and statistical analysis. A regression analysis, based on a literature review of a mathematical compendium, was executed to estimate the coefficients of 21 characteristics of French wines. To perform such analysis, 490 bottles of wines were picked independently from the most famous wine guide of France, the Guide Hachette des Vins. The data was collected in order to make a representation of the French wine production distribution. The final regression equation was reduced to 11 variables and acquired a low goodness of fit. The result may be adequate but not capable of an accurate prediction of the average price of a bottle of wine. The result can be used as an example on the presence of influences behind the pricing of a wine.</p><p>By the use of regression analysis, a second research was shaped: the impact of a regression equation in the creation of a mobile wine application. With knowledge in Industrial Engineering and a literature study in the fields of e-business, SWOT-analysis, business planning and innovation strategy; a covering analysis was put together in hope to achieve a profitable business strategy. Enhancing a search engine in a wine app by the use of regression gives an edge against existing competitors. The rewards of a wine app are greater than the risks. Henceforth, the creation of a wine app is recommended. </p>

corrected abstract:
<p>The purpose of this thesis is to evaluate the influences behind the pricing of French wines by the use of Applied Mathematics and statistical analysis. A regression analysis, based on a literature review of a mathematical compendium, was executed to estimate the coefficients of 21 characteristics of French wines. To perform such analysis, 490 bottles of wines were picked independently from the most famous wine guide of France, the <em lang="fr">Guide Hachette des Vins</em>. The data was collected in order to make a representation of the French wine production distribution. The final regression equation was reduced to 11 variables and acquired a low goodness of fit. The result may be adequate but not capable of an accurate prediction of the average price of a bottle of wine. The result can be used as an example on the presence of influences behind the pricing of a wine.</p><p>By the use of regression analysis, a second research was shaped: the impact of a regression equation in the creation of a mobile wine application. With knowledge in Industrial Engineering and a literature study in the fields of e-business, SWOT-analysis, business planning and innovation strategy; a covering analysis was put together in hope to achieve a profitable business strategy. Enhancing a search engine in a wine app by the use of regression gives an edge against existing competitors. The rewards of a wine app are greater than the risks. Henceforth, the creation of a wine app is recommended.</p>

Note - removed the unnecessary space at the end of the final paragraph and added italics (and language tag)
----------------------------------------------------------------------
In diva2:1609976   - correct as is
----------------------------------------------------------------------
In diva2:1246222 
abstract is: 
<p>The demand for higher velocities and heavier axle loads for freight trains leads to higher forces on the railway wheels which in turn lead to an increase in stresses on and below the surface of the wheel-rail contact. By time, this induces wear on the wheels which consequently lead to higher maintenance costs and in some cases accidents. The ability to predict the evolution of wheel profiles due to uniform wear has been demonstrated with a rather accurate precision in most operational conditions. These wear models are based on wear coefficients and since they are not usually valid for real operational conditions, the models are generally calibrated against real-life scenarios in order to adjust the coefficients from test conditions to real-life lubrication conditions. This engineering approach can be useful in prediction of wear in systems where the materials and contact conditions do not vary. However, when addressing material development focused on reducing specific damage modes, the approach is of limited use because the obtained wear coefficients are not directly related to material properties. Therefore, attempts towards developing physical fracture propagation models that relates to the contact conditions and material properties have been made. The purpose has been to retrieve vital information about where a fracture initiates and how it propagates. In the long run, it is of great interest to be able to attain information about how a material particle is removed from the contact surface. Studies for this type of model was done in the 70’s and 80’s mainly with pin-disk experiments but has not been utilized in the specific field of wheel-rail contact. The thesis is part of the FR8RAIL project arranged by the European rail initiative Shift2Rail. Literature studies have been the basis for the thesis in order to gain vital insights into fracture mechanics and other related fields. The physical fracture propagation models have been constructed in the FE software Abaqus with the implementation of the XFEM. For the 2D model, the fracture initiates at the top of the implanted inclusion when the friction coefficient is  and propagates upwards a few elements. For , the fracture initiates at the right surface boundary where the pressure distribution and traction is applied. The fracture propagation angle increases relative to the surface as the friction coefficient value is increased. The fracture for the 3D model extends broader compared to the 2D model at the top of the inclusion in the case of . The fracture initiates at the same surface location as for the 2D model for . The fracture propagation is however non-existent due to convergence problems. The FE-models constructed are initial steps towards analysing the fracture propagation and closely related phenomena for a railway freight wheel in detail. At the end of the thesis, the simplified models give mainly information about the fracture initiation, propagation and its patterns. From this first phase, further adjustments and improvements can take place in order to eliminate the margins of error. In the long run, fully integrated models with further implementations such as detailed microstructure for the contact conditions, plastic behaviour for the material, and complete three-dimensional models can finally be employed.</p>

corrected abstract:
<p>The demand for higher velocities and heavier axle loads for freight trains leads to higher forces on the railway wheels which in turn lead to an increase in stresses on and below the surface of the wheel-rail contact. By time, this induces wear on the wheels which consequently lead to higher maintenance costs and in some cases accidents. The ability to predict the evolution of wheel profiles due to uniform wear has been demonstrated with a rather accurate precision in most operational conditions. These wear models are based on wear coefficients and since they are not usually valid for real operational conditions, the models are generally calibrated against real-life scenarios in order to adjust the coefficients from test conditions to real-life lubrication conditions. This engineering approach can be useful in prediction of wear in systems where the materials and contact conditions do not vary. However, when addressing material development focused on reducing specific damage modes, the approach is of limited use because the obtained wear coefficients are not directly related to material properties. Therefore, attempts towards developing physical fracture propagation models that relates to the contact conditions and material properties have been made. The purpose has been to retrieve vital information about where a fracture initiates and how it propagates. In the long run, it is of great interest to be able to attain information about how a material particle is removed from the contact surface. Studies for this type of model was done in the 70’s and 80’s mainly with pin-disk experiments but has not been utilized in the specific field of wheel-rail contact. The thesis is part of the FR8RAIL project arranged by the European rail initiative Shift2Rail. Literature studies have been the basis for the thesis in order to gain vital insights into fracture mechanics and other related fields. The physical fracture propagation models have been constructed in the FE software Abaqus with the implementation of the XFEM. For the 2D model, the fracture initiates at the top of the implanted inclusion when the friction coefficient is µ &le; 0.2 and propagates upwards a few elements. For µ &gt; 0.2, the fracture initiates at the right surface boundary where the pressure distribution and traction is applied. The fracture propagation angle increases relative to the surface as the friction coefficient value is increased. The fracture for the 3D model extends broader compared to the 2D model at the top of the inclusion in the case of µ &le; 0.2. The fracture initiates at the same surface location as for the 2D model for µ &gt; 0.2. The fracture propagation is however non-existent due to convergence problems. The FE-models constructed are initial steps towards analysing the fracture propagation and closely related phenomena for a railway freight wheel in detail. At the end of the thesis, the simplified models give mainly information about the fracture initiation, propagation and its patterns. From this first phase, further adjustments and improvements can take place in order to eliminate the margins of error. In the long run, fully integrated models with further implementations such as detailed microstructure for the contact conditions, plastic behaviour for the material, and complete three-dimensional models can finally be employed.</p>

Note - added the missing equations
----------------------------------------------------------------------
In diva2:1432664 
abstract is: 
<p>Mean variance optimization has shortcomings making the strategy far from optimal from an investor’s perspective. The purpose of the study is to conduct an empirical investigation as to how modern methods of portfolio optimization address the shortcomings associated with mean variance optimization. Equal risk contribution, the Most diversified portfolioand a modification of the Minimum variance portfolio are considered as alternatives to the mean variance model. Portfolio optimization models introduced are explained in detail and solved using the optimization algorithms Cyclical coordinate descent and Alternating direction method of multipliers. Through implementation and backtesting using a diverse set of indices representing various asset classes, the study shows that the mean variance model suffers from high turnover and sensitivity to input parameters in comparison to the modern alternatives. The sophisticated asset allocation models equal risk contribution and the most diversified portfolio do not rely on expected return as an input parameter, which is seen as an advantage, and are not affected to the same extent by the shortcomings associated with mean variance optimization. The paper concludes by discussing the findings critically and suggesting ideas for further research.</p>

corrected abstract:
<p><em>Mean variance optimization</em> has shortcomings making the strategy far from optimal from an investor’s perspective. The purpose of the study is to conduct an empirical investigation as to how modern methods of portfolio optimization address the shortcomings associated with mean variance optimization. <em>Equal risk contribution</em>, the <em>Most diversified portfolio</em> and a modification of the <em>Minimum variance portfolio</em> are considered as alternatives to the mean variance model. Portfolio optimization models introduced are explained in detail and solved using the optimization algorithms <em>Cyclical coordinate descent</em> and <em>Alternating direction method of multipliers</em>. Through implementation and backtesting using a diverse set of indices representing various asset classes, the study shows that the mean variance model suffers from high turnover and sensitivity to input parameters in comparison to the modern alternatives. The sophisticated asset allocation models equal risk contribution and the most diversified portfolio do not rely on expected return as an input parameter, which is seen as an advantage, and are not affected to the same extent by the shortcomings associated with mean variance optimization. The paper concludes by discussing the findings critically and suggesting ideas for further research.</p>

Note added italics to match original
----------------------------------------------------------------------
In diva2:1431031   - correct as is
----------------------------------------------------------------------
In diva2:1360615 
abstract is: 
<p>This master thesis report presents the modelling process of key elements of the Laser Interferometer Space Antenna mission (LISA mission) in a Modelbased systems engineering (MBSE) approach with SysML (Systems Modeling Language). The model implements a selected set of functions of the mission through executable graphical representations, called diagrams. It is shown how such diagrams can benefit the mission, by comparing this mean of information exchange to the traditional text- based systems engineering. The model represents the mission structure and behaviour through a system of nested layers. The deeper the layer is, the more it gives details on a system part. Each layer can be seen from different point of views, either focusing on the structure, the behaviour, or the performance of related system part.</p>

corrected abstract:
<p>This master thesis report presents the modelling process of key elements of the Laser Interferometer Space Antenna mission (LISA mission) in a Modelbased systems engineering (MBSE) approach with SysML (Systems Modeling Language). The model implements a selected set of functions of the mission through executable graphical representations, called diagrams. It is shown how such diagrams can benefit the mission, by comparing this mean of information exchange to the traditional text-based systems engineering. The model represents the mission structure and behaviour through a system of nested layers. The deeper the layer is, the more it gives details on a system part. Each layer can be seen from different point of views, either focusing on the structure, the behaviour, or the performance of related system part.</p>

Note - only change to remove the space after a hyphen
----------------------------------------------------------------------
In diva2:1320415 
abstract is: 
<p>In this thesis, various famous models have been investigated and compared to a custom model for people detection in low resolution video feeds. YOLOv3 and SSD in particular are famous models which have, at their time, produced state of the art results on competitions such as ImageNet and COCO. The performance of all models have been compared on speed and accuracy where it was found that YOLOv3 was the slowest and SSD was the fastest. The proposed model was superior in accuracy to both of the aforementioned architectures which can be attributed to addition of newer techniques from research such as leaving activations out and having a carefully balanced loss function. The results seem to suggest that the proposed model is implementable for real-time inference using cheap hardware such as a raspberry pi 3B+ coupled with one or more AI accelerator stickssuch as the Intel Neural Compute Stick 2 and that the networks are usable for detection even in bad video streams.</p>

corrected abstract:
<p>In this thesis, various famous models have been investigated and compared to a custom model for people detection in low resolution video feeds. YOLOv3 and SSD in particular are famous models which have, at their time, produced state of the art results on competitions such as ImageNet and COCO. The performance of all models have been compared on speed and accuracy where it was found that YOLOv3 was the slowest and SSD was the fastest. The proposed model was superior in accuracy to both of the aforementioned architectures which can be attributed to addition of newer techniques from research such as leaving activations out and having a carefully balanced loss function. The results seem to suggest that the proposed model is implementable for real-time inference using cheap hardware such as a raspberry pi 3B+ coupled with one or more AI accelerator sticks such as the Intel Neural Compute Stick 2 and that the networks are usable for detection even in bad videostreams.</p>

Note spelling error:
mc='stickssuch' c='sticks such'
"videostreams" is one word in the original
----------------------------------------------------------------------
In diva2:1130084   - correct as is
Note that 'diva2:1109070' is a duplicate
----------------------------------------------------------------------
In diva2:1391482   - correct as is
----------------------------------------------------------------------
In diva2:1183207 
abstract is: 
<p>With more and more digital text-valued data available, the need to be able to cluster, classify and study them arises. We develop in this thesis statistical tools to perform null hypothesis testing and clustering or classification on text-valued data in the framework of Object-Oriented Data Analysis.</p><p>The project includes research on semantic methods to represent texts, comparisons between representations, distances for such representations and performance of permutation tests. Main methods compared are Vector Space Model and topic model. More precisely, this thesis will provide an algorithm to compute permutation tests at document or sentence level to study the equality in terms of distribution of two texts for different representations and distances. Lastly, we describe the study of texts regarding a syntactic point of view and its structure with a tree representation.</p>

corrected abstract:
<p>With more and more digital text-valued data available, the need to be able to cluster, classify and study them arises. We develop in this thesis statistical tools to perform null hypothesis testing and clustering or classification on text-valued data in the framework of Object-Oriented Data Analysis.</p><p>The project includes research on semantic methods to represent texts, comparisons between representations, distances for such representations and performance of permutation tests. Main methods compared are Vector Space Model and topic model. More precisely, this thesis will provide an algorithm to compute permutation tests at document or sentence level to study the equality in terms of distribution of two texts for different representations and distances.</p><p>Lastly, we describe the study of texts regarding a syntactic point of view and its structure with a tree representation.</p>

Note - only change was to added the missing paragraph break
----------------------------------------------------------------------
In diva2:1120349   - correct as is
----------------------------------------------------------------------
In diva2:1751455   - correct as is
----------------------------------------------------------------------
In diva2:813411 
abstract is: 
<p>In this study a multiple linear regression was carried out in the interest of analysing a number of variables effect on the final prices of apartments in Stockholm’s inner districts. The result may be employed to predict and observe percentage changes on the final price of apartments in Stockholm in the future. Five models were constructed after which they were analysed and compared. The construction of these models were supported by data from the real estate agency <em>Erik Olsson</em>. The result of this study displays that living space have the highest positive influence on the final prices. Among all the inner city districts, Östermalm is the district that contributes the most to the final price growth. All five models had a coefficient of determination between 89%-94%.</p>

corrected abstract:
<p>In this study a multiple linear regression was carried out in the interest of analysing a number of variables effect on the final prices of apartments in Stockholm’s inner districts. The result may be employed to predict and observe percentage changes on the final price of apartments in Stockholm in the future. Five models were constructed after which they were analysed and compared. The construction of these models were supported by data from the real estate agency <em>Erik Olsson</em>.</p><p>The result of this study displays that living space have the highest positive influence on the final prices. Among all the inner city districts, Östermalm is the district that contributes the most to the final price growth. All five models had a coefficient of determination between 89% &mdash; 94%.</p>

Note - added paragraph break and changed the dash to a longer em-dash
----------------------------------------------------------------------
In diva2:1655678   - correct as is
Note spelling error:
w='degress' val={'c': 'degrees', 's': 'diva2:1655678', 'n': 'error in original'}
----------------------------------------------------------------------
In diva2:754040   - correct as is
----------------------------------------------------------------------
In diva2:408791   - correct as is
----------------------------------------------------------------------
In diva2:760099 
abstract is: 
<p>American football is a well-known sport in America. In addition to the design and rules of the game and the surface it is played on, there are also other differences that are not directly visible. One difference noted in this study is how the teams in the game provide players. Each team has a board of directors together with the team's leadership and they will determine which players are most suitable for the team. These players are recruited to the team after graduating from a U.S. college. This makes it possible to recruit new and young players to the squad. If the team wants other types of new players, there is also an opportunity to replace one or more players. This is seen as a trading, where players are replaced with other players between the two teams. These two opportunities to recruit new players to the team are called drafting, which is a process of recruiting players.</p><p>This study focuses on the recruitment of players from college. In America, it is a system used to recruit both new and old players to their respective teams in the National Football League, NFL. This study evaluates and analyzes the factors based on how each team in the NFL recruits players from the U.S. college league (NCAA). The factors analyzed were for instance each individual player’s performance during the college league, such as age, passing touchdowns and tackles. By using logistic analysis, these factors could then be determined by analyzing the odds and its change for each player's performance based on their positions in the team. This study shows that in every way a positional player performs during the college league, there will be important factors that contribute to recruitment to the NFL.</p>

corrected abstract:
<p>American football is a well-known sport in America. In addition to the design and rules of the game and the surface it is played on, there are also other differences that are not directly visible. One difference noted in this study is how the teams in the game provide players. Each team has a board of directors together with the team's leadership and they will determine which players are most suitable for the team. These players are recruited to the team after graduating from a U.S. college. This makes it possible to recruit new and young players to the squad. If the team wants other types of new players, there is also an opportunity to replace one or more players. This is seen as a trading, where players are replaced with other players between the two teams. These two opportunities to recruit new players to the team are called <em>drafting</em>, which is a process of recruiting players. This study focuses on the recruitment of players from college. In America, it is a system used to recruit both new and old players to their respective teams in the National Football League, NFL.</p><p>This study evaluates and analyzes the factors based on how each team in the NFL recruits players from the U.S. college league (NCAA). The factors analyzed were for instance each individual player’s performance during the college league, such as age, passing touchdowns and tackles. By using logistic analysis, these factors could then be determined by analyzing the odds and its change for each player's performance based on their positions in the team. This study shows that in every way a positional player performs during the college league, there will be important factors that contribute to recruitment to the NFL.</p>

Note moved the paragraph break to where it is in the original and added italics
----------------------------------------------------------------------
In diva2:838515   - correct as is
----------------------------------------------------------------------
In diva2:849700   - correct as is
----------------------------------------------------------------------
In diva2:1249683 
abstract is: 
<p>In this study the ISDA SIMM and the initial margin requirements for non-centrally cleared over the counter derivatives is investigated and compared with the traditional risk measure value-at-risk.</p><p>The empirical results suggest that for swap portfolios the ISDA SIMM achieves its set out purpose of being less volatile and more transparent than the 10-day value-at-risk on a 99% conﬁdence level. However, the SIMM framework will re-quire market participants to be continuously updated and provided calibration parameters which reﬂect current market conditions from ISDA.</p>

corrected abstract:
<p>In this study the ISDA SIMM and the initial margin requirements for non-centrally cleared over the counter derivatives is investigated and compared with the traditional risk measure value-at-risk.</p><p>The empirical results suggest that for swap portfolios the ISDA SIMM achieves its set out purpose of being less volatile and more transparent than the 10-day value-at-risk on a 99% confidence level. However, the SIMM framework will require market participants to be continuously updated and provided calibration parameters which reflect current market conditions from ISDA.</p>

Note - only change deleted an unnecessary hyphen in "re-quire"
----------------------------------------------------------------------
In diva2:1757070 
abstract is: 
<p>This study aims to find the best model to predict the outcome of football (1,X,2 - Home Win, Draw, Away Win) games by looking at match data. The data used is put together from the three highest football divisions in England and go back to the year 2005. Multinomial logistic regression is used to model the response variable from the regressors. A best subset regression is used to find the models with the lowest Akaike Information Criterion (AIC). By doing a multicollinearity analysis these models are further examined and the best one is chosen. </p><p>The results show both expected and unexpected effects that create foundation for future studies. Areas for model improvement include more variables, comparison with the bookmaker’s odds and tests on new test data. The application of the model is in sports betting where it can be used to value multi bets and live odds.</p>

corrected abstract:
<p>This study aims to find the best model to predict the outcome of football (<em>1,X,2 - Home Win, Draw, Away Win</em>) games by looking at match data. The data used is put together from the three highest football divisions in England and go back to the year 2005. Multinomial logistic regression is used to model the response variable from the regressors. A best subset regression is used to find the models with the lowest <em>Akaike Information Criterion (AIC)</em>. By doing a multicollinearity analysis these models are further examined and the best one is chosen.</p><p>The results show both expected and unexpected effects that create foundation for future studies. Areas for model improvement include more variables, comparison with the bookmaker’s odds and tests on new test data. The application of the model is in sports betting where it can be used to value multi bets and live odds.</p>

Note removed an unnecessary space at the end of a paragraph and added italics
----------------------------------------------------------------------
In diva2:1145323   - correct as is
----------------------------------------------------------------------
In diva2:813123 
abstract is: 
<p>This is a project that describes a part of the development of a tool intended for wheelchair users in their learning process of new and important movements in everyday life and in sports. The process began with an investigation, with the help of people with wheelchair experience, of which movements that were considered important and which were considered useful in sports and in everyday life. The project was first focused on improving the technical skills of athletes practicing wheelchair sports. After studies and discussions it however changed focus, to also deal with obstacles in everyday life and to help people using wheelchairs to learn more everyday movements. After practical workshops, a mechanical analysis of the wheelchair was made. Some types of movement patterns, such as balancing on the rear wheels and turning, was considered essential to study and the goal was also to get information about for example the speed and acceleration of the wheelchair. This was to be analysed in real time and then sonified in later steps. We developed a code torecognize and assess these movements, which is the product of the project.</p>

mc='torecognize' c='to recognize'

corrected abstract:
<p>This is a project that describes a part of the development of a tool intended for wheelchair users in their learning process of new and important movements in everyday life and in sports. The process began with an investigation, with the help of people with wheelchair experience, of which movements that were considered important and which were considered useful in sports and in everyday life. The project was first focused on improving the technical skills of athletes practicing wheelchair sports. After studies and discussions it however changed focus, to also deal with obstacles in everyday life and to help people using wheelchairs to learn more everyday movements. After practical workshops, a mechanical analysis of the wheelchair was made. Some types of movement patterns, such as balancing on the rear wheels and turning, was considered essential to study and the goal was also to get information about for example the speed and acceleration of the wheelchair. This was to be analysed in real time and then sonified in later steps. We developed a code to recognize and assess these movements, which is the product of the project.</p>
----------------------------------------------------------------------
In diva2:1701376 
abstract is: 
<p>Cell differentiation is the process of a cell developing from one cell type to another. It is of interest to analyse the differentiation from stem cells to different types of mature cells, and discover what genes are involved in regulating the differentiation to specific cells, for instance to get insights to what is causing certain diseases and find potential treatments. </p><p>In this project, two mathematical models are developed for analysing blood cell differentiation (haematopoiesis) with methods based on optimal transportation. Optimal transportation is about moving one mass distribution to another at minimal cost. Modelling a sample of cells as point masses placed in a space based on the cells' gene expressions, accessed by single-cell RNA sequencing, optimal transportation is used to find transitions between cells that costs the least in terms of changes in gene expression. With this, cell-to-cell trajectories, from haematopoietic stem cells to mature blood cells, are obtained.</p><p>With the first model, cells are divided into groups based on their maturity, which is determined by using diffusion pseudotime, and optimal transportation is preformed between groups. The resulting trajectories suggest that haematopoietic stem cells possibly can develop into the same mature cell type in different ways, and that the cell fate for some cell types is decided late on in development. In future work, the gene regulation along the obtained trajectories can be analysed. The second model is developed to be more general than the first, and not be dependent on a group division before preforming optimal transportation.</p>

corrected abstract:
<p>Cell differentiation is the process of a cell developing from one cell type to another. It is of interest to analyse the differentiation from stem cells to different types of mature cells, and discover what genes are involved in regulating the differentiation to specific cells, for instance to get insights to what is causing certain diseases and find potential treatments.</p><p>In this project, two mathematical models are developed for analysing blood cell differentiation (haematopoiesis) with methods based on optimal transportation. Optimal transportation is about moving one mass distribution to another at minimal cost. Modelling a sample of cells as point masses placed in a space based on the cells' gene expressions, accessed by single-cell RNA sequencing, optimal transportation is used to find transitions between cells that costs the least in terms of changes in gene expression. With this, cell-to-cell trajectories, from haematopoietic stem cells to mature blood cells, are obtained.</p><p>With the first model, cells are divided into groups based on their maturity, which is determined by using diffusion pseudotime, and optimal transportation is preformed between groups. The resulting trajectories suggest that haematopoietic stem cells possibly can develop into the same mature cell type in different ways, and that the cell fate for some cell types is decided late on in development. In future work, the gene regulation along the obtained trajectories can be analysed. The second model is developed to be more general than the first, and not be dependent on a group division before preforming optimal transportation.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1570309 
abstract is: 
<p>This thesis analyses four different optimization algorithms for training a convolutional neural network (CNN) using three different datasets. The algorithms studied were stochastic gradient descent (SGD), Polyak momentum, Nesterov momentum and Adaptive Moment Estimation (ADAM). The data sets used to train the algorithms were two image based datasets called Fashion MNIST and Cifar-10 as well as a multivariate data set containing information about flower species called Iris. The algorithm that reached the highest accuracy across all three data sets was the Adaptive Moment Estimation, therefore making it the most suitable algorithm for these datasets.</p>

corrected abstract:
<p>This thesis analyses four different optimization algorithms for training a convolutional neural network (CNN) using three different datasets. The algorithms studied were stochastic gradient descent (SGD), Polyak momentum, Nesterov momentum and Adaptive Moment Estimation (ADAM). The data sets used to train the algorithms were two image based data sets called Fashion MNIST and Cifar-10 as well as a multivariate data set containing information about flower species called Iris. The algorithm that reached the highest accuracy across all three data sets was the Adaptive Moment Estimation, therefore making it the most suitable algorithm for these datasets.</p>

Note - only change is to make "datasets" into "data sets" - as per the original
----------------------------------------------------------------------
In diva2:1800559   - correct as is
----------------------------------------------------------------------
In diva2:1268646 
abstract is: 
<p>The main aim of this thesis was to understand the mechanism behind the classical transition scenario inside the boundary layer over an airfoil and eventually attempting to control this transition utilizing passive devices for transition delay. The initial objective of analyzing the transition phenomenon based on TS wave disturbance growth was conducted at 90 Hz using LDV and CTA measurement techniques at two different angles of attack. This was combined with the studies performed on two other frequencies of 100 and 110 Hz, in order to witness its impact on the neutral stability curve behavior. The challenges faced in the next phase of the thesis while trying to control the transition location, was to understand and encompass the effect of adverse pressure gradient before setting up the passive control devices, which in this case was miniature vortex generators. Consequently, several attempts were made to optimize the parameters of the miniature vortex generators depending upon the streak strength and stability. Finally, for 90 Hz a configuration of miniature vortex generators have been found to successfully stabilize the TS wave disturbances below a certain forcing amplitude, which also led to transition delay.</p>
corrected abstract:
<p>The main aim of this thesis was to understand the mechanism behind the classical transition scenario inside the boundary layer over an airfoil and eventually attempting to control this transition utilizing passive devices for transition delay. The initial objective of analyzing the transition phenomenon based on TS wave disturbance growth was conducted at 90 Hz using LDV and CTA measurement techniques at two different angles of attack. This was combined with the studies performed on two other frequencies of 100 and 110 Hz, in order to witness its impact on the neutral stability curve behaviour.</p><p>The challenges faced in the next phase of the thesis while trying to control the transition location, was to understand and encompass the effect of adverse pressure gradient before setting up the passive control devices, which in this case was miniature vortex generators. Consequently, several attempts were made to optimize the parameters of the miniature vortex generators depending upon the streak strength and stability. Finally, for 90 Hz a configuration of miniature vortex generators have been found to successfully stabilize the TS wave disturbances below a certain forcing amplitude, which also led to transition delay.</p>

Note "behavior" should eb "behaviour" - as in the original
----------------------------------------------------------------------
In diva2:509618 
abstract is: 
<p>In this work, a flow reversal resonator fitted with a short-circuit duct connecting the inlet and outlet is analysed and used as a tuneable muffler element, aimed to be used use in a semi-active muffler on an IC-engine. The work done can be divided into 3 main parts. 1), a study of what type of valve that could be used to change the acoustical properties of the short-circuit duct. 2), Design of a flow reversal resonator with a controllable valve as the short-circuit. 3), experimental validation in a flow acoustic test rig. The flow reversal resonator with a controllable valve as short-circuit is successfully validated to work as tuneable muffler element during laboratory conditions. The same valve concept is simulated in a full scale concept but not validated experimentally on a running IC-engine. The theory used to describe the acoustics of a flow duct element is also presented together with three simulation techniques and the two microphone technique used to determine the acoustic properties of the investigated flow reversal resonator.</p>

corrected abstract:
<p>In this work, a flow reversal resonator fitted with a short-circuit duct connecting the inlet and outlet is analysed and used as a tuneable muffler element, aimed to be used use in a semi-active muffler on an IC-engine. The work done can be divided into 3 main parts. 1), a study of what type of valve that could be used to change the acoustical properties of the short-circuit duct. 2), Design of a flow reversal resonator with a controllable valve as the short-circuit. 3), experimental validation in a flow acoustic test rig.</p><p>The flow reversal resonator with a controllable valve as short-circuit is successfully validated to work as tuneable muffler element during laboratory conditions. The same valve concept is simulated in a full scale concept but not validated experimentally on a running IC-engine.</p><p>The theory used to describe the acoustics of a flow duct element is also presented together with three simulation techniques and the two microphone technique used to determine the acoustic properties of the investigated flow reversal resonator.</p>

Note - added the missing paragraph breaks
----------------------------------------------------------------------
In diva2:633167 - missing space in title:
"Analysis and implementation of anefficient solver for large-scalesimulations of neuronal systems"
==>
"Analysis and implementation of an efficient solver for large-scale simulations of neuronal systems"


abstract is: 
<p>Numerical integration methods exploiting the characteristics of neuronal equation systems were investigated. The main observations was a high stiffness and a quasi-linearity of the system. The latter allowed for decomposition into two smaller systems by using a block diagonal Jacobian approximation. The popular backwards differentiation formulas methods (BDF) showed performance degradation for this during first experiments. Linearly implicit peer methods (PeerLI), a new class of methods, did not show this degradation. Parameters for PeerLI were optimized by experimental means and then compared in performance to BDF. Models were simulated in both Matlab and NEURON, a neuron modelling package. For small models PeerLI was competitive with BDF, especially with a block diagonal Jacobian. In NEURON the performance of the block diagonal Jacobian did no longer degrade for BDF, but instead showed degradation for PeerLI, especially for large models. With full Jacobian PeerLI was competitive with BDF, but with block diagonal Jacobian an increase of ca.50% was seen in simulation time. Overall PeerLI methods were competitive for certain problems, but did not give the desired performance gain for block diagonal Jacobian for large problems. There is, however, still a lot of room for improvement, since parameters were only determined experimentally and tuned to small problems.</p>

corrected abstract:
<p>Numerical integration methods exploiting the characteristics of neuronal equation systems were investigated. The main observations was a high stiffness and a quasi-linearity of the system. The latter allowed for decomposition into two smaller systems by using a block diagonal Jacobian approximation. The popular backwards differentiation formulas methods (BDF) showed performance degradation for this during first experiments. Linearly implicit peer methods (PeerLI), a new class of methods, did not show this degradation. Parameters for PeerLI were optimized by experimental means and then compared in performance to BDF. Models were simulated in both Matlab and NEURON, a neuron modelling package. For small models PeerLI was competitive with BDF, especially with a block diagonal Jacobian. In NEURON the performance of the block diagonal Jacobian did no longer degrade for BDF, but instead showed degradation for PeerLI, especially for large models. With full Jacobian PeerLI was competitive with BDF, but with block diagonal Jacobian an increase of ca. 50% was seen in simulation time. Overall PeerLI methods were competitive for certain problems, but did not give the desired performance gain for block diagonal Jacobian for large problems. There is, however, still a lot of room for improvement, since parameters were only determined experimentally and tuned to small problems.</p>

Note added space after "ca." and before "50%"
----------------------------------------------------------------------
In diva2:1679315   - correct as is
----------------------------------------------------------------------
In diva2:1380194 
abstract is: 
<p>Aircraft design is an act of art requiring dedication and careful work to ensure good results. An essential tool in that work is a flight mechanics simulator. Such simulators are often built up of modules/models that are executed in a sequential order in each time iteration. This project aims to analyze potential improvements to the model execution order based on the dependency structure of one such simulator. The analysis method Design Structure Matrix (DSM), was used to define/map the dependencies and then Binary Linear Programming (BLP) was utilized to find five new potentially improved model orders to minimize the number of feedbacks from one iteration to the next one. Those five proposed execution orders were next compared and evaluated. The result is a model order that reduce the number of models receiving feedbacks from the previous iteration from 13 to 6, with insignificant changes in the precision of the simulator.</p>

corrected abstract:
<p>Aircraft design is an act of art requiring dedication and careful work to ensure good results. An essential tool in that work is a flight mechanics simulator. Such simulators are often built up of modules/models that are executed in a sequential order in each time iteration. This project aims to analyze potential improvements to the model execution order based on the dependency structure of one such simulator. The analysis method <em>Design Structure Matrix (DSM)</em>, was used to define/map the dependencies and then <em>Binary Linear Programming (BLP)</em> was utilized to find five new potentially improved model orders to minimize the number of feedbacks from one iteration to the next one. Those five proposed execution orders were next compared and evaluated. The result is a model order that reduce the number of models receiving feedbacks from the previous iteration from 13 to 6, with insignificant changes in the precision of the simulator.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1289344 
abstract is: 
<p>One of the tasks for the Swedish Transport Agency, Traﬁkverket, is to provide traﬃc forecasts. To do this, a number of diﬀerent forecast models are used, where Samgods is a nationally estimated model, where the quality of the results gets more unstable the more disaggregated level you are looking at. For rail this is handled with a model called Bangods. However, in Bangods the diﬀerence in geographical growth within each commodity group is lost.</p><p>This thesis examines whether it is possible to replace the national growth rates from Samgods with geographical disaggregated growth rates. The growth rates are calculated with a mathematical model based on the pivot point method (PPM). The model has been implemented in Python and is used to disaggregate the growth rates from Samgods to maintain the geographical growth. However, the data to the model comes from diﬀerent systems and models that use diﬀerent link formats. Therefore a link matching method is required that converts links from one system to another before using PPM.</p><p>The growth rates from the PPM and the link matching-method has been modelled for twelve commodity groups, 8 or 1417 geographic regions and with or without a train division with four train types. The best result was to used 96 growth rates divided into twelve commodity groups and eight geographical regions.</p>


corrected abstract:
<p>One of the tasks for the Swedish Transport Agency, Trafikverket, is to provide traffic forecasts. To do this, a number of different forecast models are used, where Samgods is a nationally estimated model, where the quality of the results gets more unstable the more disaggregated level you are looking at. For rail this is handled with a model called Bangods. However, in Bangods the difference in geographical growth within each commodity group is lost.</p><p>This thesis examines whether it is possible to replace the national growth rates from Samgods with geographical disaggregated growth rates. The growth rates are calculated with a mathematical model based on the pivot point method (PPM). The model has been implemented in Python and is used to disaggregate the growth rates from Samgods to maintain the geographical growth. However, the data to the model comes from different systems and models that use different link formats. Therefore a link matching method is required that converts links from one system to another before using PPM.</p><p>The growth rates from the PPM and the link matching-method has been modelled for twelve commodity groups, 8 or 1417 geographic regions and with or without a train division with four train types. The best result was to used 96 growth rates divided into twelve commodity groups and eight geographical regions.</p>


Note replaced ligatures with their expended evrsion
----------------------------------------------------------------------
In diva2:1578526 
abstract is: 
<p>In this study a finite element model has been created in ABAQUS/Standard in order to replicate impact tests performed by SSAB and to determine residual stresses in a plate. Uniaxial tensile tests have been done in order to determine the plastic behaviour of a high strength steel from the manufacturer SSAB and to determine whether the material can be assumed to be isotropic. Test specimens in the rolling and cross-rolling direction were cut out from an undeformed plate. The data was fit to a Voce hardening law which showed that the material is not isotropic due to different initial yield stresses. However, the hardening curve can be regarded as isotropic due to small differences between the rolling and cross-rolling direction.</p><p>Vickers hardness tests have been done on three test specimens cut out from a deformed plate which has been used in the impact tests at SSAB. These Vickers hardness tests have been done in order to verify the finite element model. The Vickers hardness determined from experiments was compared to the calculated Vickers hardness from the finite element simulation. The comparison showed good agreement between experiments and simulations seeing that they follow the same trend. This indicate that the finite element model is a good numerical method to determine residual stresses in high strength steels subjected to significant plastic deformation.</p>

corrected abstract:
<p>In this study a finite element model has been created in ABAQUS/Standard in order to replicate impact tests performed by SSAB and to determine residual stresses in a plate. Uniaxial tensile tests have been done in order to determine the plastic behaviour of a high strength steel from the manufacturer SSAB and to determine whether the material can be assumed to be isotropic. Test specimens in the rolling and cross-rolling direction were cut out from an undeformed plate. The data was fit to a Voce hardening law which showed that the material is not isotropic due to different initial yield stresses. However, the hardening curve can be regarded as isotropic due to small differences between the rolling and cross-rolling direction.</p><p>Vickers hardness tests have been done on three test specimens cut out from a deformed plate which has been used in the impact tests at SSAB. These Vickers hardness tests have been done in order to verify the finite element model. The Vickers hardness determined from experiments was compared to the calculated Vickers hardness from the finite element simulation. The comparison showed good agreement between experiments and simulations seeing that they follow the same trend. This indicate that the finite element model is a good numerical method to determine residual stresses in high strength steels subjected to significant plastic deformation.</p>

Note change in hyphens from "0x1d" to "0x2d"
----------------------------------------------------------------------
In diva2:1817060 
abstract is: 
<p>A reduction in the global use of fossil fuels is necessary when striving for a more sustainable future. One key strategy in the transition from fossil fuels is electrification. This strategy is particularly prominent within the transport sector, where more efficient ways to store electric energy are being pursued. Structural battery composites represent a promising technology. Being based on multifunctional composite materials that can carry mechanical loads and store electrical energy at the same time, they provide a ‘mass-less’ energy storage.</p><p>This work aims to develop a shape morphing structural battery capable of bending upwards and downwards in a cantilever setup. The structural battery is made from several constituents. Two outer layers of carbon fibers act as negative electrodes and a middle layer of aluminium foil coated with NMC622 on both sides acts as the positive electrodes. Additionally, a glass veil layer and a ceramic separator separate the positive and negative electrodes. A structural battery electrolyte is used to embed the laminate in order to provide load transfer and ion transfer. From this setup, it is possible to control the lithiation/delithiation of each carbon fiber layer independently and thereby bend the laminate in the desired direction. Subsequently, the system is modeled both analytically using Matlab and numerically using Comsol Multiphysics 6.1. </p><p>From the models it is found that the system is in theory capable of large deformations, showing promising results. However, the experimental laminates show low capacity upon cycling which would cause near to zero deformations. The poor performance of the system could be linked to incompatibility between the structural battery electrolyte and the NMC622.</p>

corrected abstract:
<p>A reduction in the global use of fossil fuels is necessary when striving for a more sustainable future. One key strategy in the transition from fossil fuels is electrification. This strategy is particularly prominent within the transport sector, where more efficient ways to store electric energy are being pursued. Structural battery composites represent a promising technology. Being based on multifunctional composite materials that can carry mechanical loads and store electrical energy at the same time, they provide a ‘mass-less’ energy storage.</p><p>This work aims to develop a shape morphing structural battery capable of bending upwards and downwards in a cantilever setup. The structural battery is made from several constituents. Two outer layers of carbon fibers act as negative electrodes and a middle layer of aluminium foil coated with NMC622 on both sides acts as the positive electrodes. Additionally, a glass veil layer and a ceramic separator separate the positive and negative electrodes. A structural battery electrolyte is used to embed the laminate in order to provide load transfer and ion transfer. From this setup, it is possible to control the lithiation/delithiation of each carbon fiber layer independently and thereby bend the laminate in the desired direction. Subsequently, the system is modeled both analytically using Matlab and numerically using Comsol Multiphysics 6.1.</p><p>From the models it is found that the system is in theory capable of large deformations, showing promising results. However, the experimental laminates show low capacity upon cycling which would cause near to zero deformations. The poor performance of the system could be linked to incompatibility between the structural battery electrolyte and the NMC<sub>622</sub>.</p>

Note eliminated an unnecessary space at the end of a paragraph and added subscripts
----------------------------------------------------------------------
In diva2:1279807 
abstract is: 
<p>In a gas turbine rotor, the Blade-Disc attachment is often a critical part. This part needs to endure both high contact stress and high temperature. Thus, the prediction of the life of the attachment becomes important.  Furthermore, the refinement of the Blade-Disc attachment is worth to study, to extend the lifetime of the turbine rotor. In this work, the Blade-Disc attachment is belonging to the gas turbine section.   </p><p>Geometry modifications based on a 2D FE-model of a blade-disc attachment are performed. Besides of flat contact surface, an alternative curved contact form which creates a barrel on the contact surface is studied. The effect of the different geometries, is studied with respect to high local stresses. The results are evaluated using different methods; deterministic LCF life using EVAL, probabilistic LCF life using ProbLCF and fretting damage parameter evaluation  using the Ruiz-Chen Model.   </p><p>During this work, geometry modifications based on a 2D Blade-Disc attachment is performed first to study the effect of the different modification results. Then a fretting damage parameter evaluation is carried out, to discuss the crack initiation position of the models with respect to different modification cases. Finally, low cycle fatigue tests are performed to analyse the life cycle of the models.   </p><p>The result of the work shows at manufactory tolerances can cause a maximum stress increase by at least 60% in the model, especially in the contact surfaces. Also, comparison results shows apply a barrel to the contact surface can effectively decrease the maximum stress in the model, and extend the cyclic life of the attachment. The most suitable barrel height for this work is between 0.05[mm] and 0.015 [mm].</p>

corrected abstract:
<p>In a gas turbine rotor, the Blade-Disc attachment is often a critical part. This part needs to endure both high contact stress and high temperature. Thus, the prediction of the life of the attachment becomes important.  Furthermore, the refinement of the Blade-Disc attachment is worth to study, to extend the lifetime of the turbine rotor. In this work, the Blade-Disc attachment is belonging to the gas turbine section.</p><p>Geometry modifications based on a 2D FE-model of a blade-disc attachment are performed. Besides of flat contact surface, an alternative curved contact form which creates a barrel on the contact surface is studied. The effect of the different geometries, is studied with respect to high local stresses. The results are evaluated using different methods; deterministic LCF life using EVAL, probabilistic LCF life using ProbLCF and fretting damage parameter evaluation using the Ruiz-Chen Model.</p><p>During this work, geometry modifications based on a 2D Blade-Disc attachment is performed first to study the effect of the different modification results. Then a fretting damage parameter evaluation is carried out, to discuss the crack initiation position of the models with respect to different modification cases. Finally, low cycle fatigue tests are performed to analyse the life cycle of the models.</p><p>The result of the work shows at manufactory tolerances can cause a maximum stress increase by at least 60% in the model, especially in the contact surfaces. Also, comparison results shows apply a barrel to the contact surface can effectively decrease the maximum stress in the model, and extend the cyclic life of the attachment. The most suitable barrel height for this work is between 0.05[mm] and 0.015 [mm].</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph and eliminate an unnecessary space
----------------------------------------------------------------------
In diva2:447365 
abstract is: 
<p>It is well known that boiling water reactors can experience inadvertent power oscillations. When such instability occurs the core can oscillate in two different modes (in phase mode and out of phase mode). In the late 90’s a stability benchmark was created using the stability data obtained from the experiments at the Swedish nuclear power plant of Ringhals-1. Data was collected from the cycles 14, 15 , 16 and 17. Later on, this data was used to validate the various models and codes with the aim of predicting the instability behavior of the core and understand the triggers of such oscillations. The current trend of increasing reactor power density and relying on natural circulation for core cooling may have consequences for the stability of modern BWR’s designs. The objective of this work is to find the most important parameters affecting the stability of the BWRs and propose alternative stability maps. For this purpose a TRACE/PARCS model of the Ringhals-1 NPP will be used. Afterwards a selection of possible parameters and dimensionless numbers will be made to study its effect on stability. Once those parameters are found they will be included in the stability maps to make them more accurate.</p>

corrected abstract:
<p>It is well known that boiling water reactors can experience inadvertent power oscillations. When such instability occurs the core can oscillate in two different modes (in phase mode and out of phase mode). In the late 90’s a stability benchmark was created using the stability data obtained from the experiments at the Swedish nuclear power plant of Ringhals-1. Data was collected from the cycles 14, 15 , 16 and 17. Later on, this data was used to validate the various models and codes with the aim of predicting the instability behavior of the core and understand the triggers of such oscillations. The current trend of increasing reactor power density and relying on natural circulation for core cooling may have consequences for the stability of modern BWR’s designs. The objective of this work is to find the most important parameters affecting the stability of the BWRs and propose alternative stability maps. For this purpose a TRACE/PARCS model of the Ringhals-1 NPP will be used. Afterwards a selection of possible parameters and dimensionless numbers will be made to study its effect on stability. Once those parameters are found they will be included in the stability maps to make them more accurate.</p>
----------------------------------------------------------------------
In diva2:810154 
abstract is: 
<p>Hydro power is the largest source for generation of electricity in the Nordic region today.</p><p>This production is heavily dependent on the weather since it dictates the terms for the availability and the amount of power to be produced. Vattenfall as a company has an incentive to avoid volatile revenue streams as it facilitates economic planning and induces a positive effect on its credit rating, thus also on its bottom line. Vattenfall is a large producer of hydro power with a possibility to move the power market which adds further complexity to the problem. In this thesis the authors develop new hedging strategies which will hedge more efficiently. With efficiency is meant the same risk, or standard deviation, at a lower cost or alternatively formulated lower risk for the same cost. In order to enable comparison and make claims about efficiency, a reference solution is developed that should reflect their current hedging strategy. To achieve higher efficiency we focus on finding dynamic hedging strategies. First a prototype model is suggested to facilitate the construction of the solution methods and if it is worthwhile to pursue a further investigation. As this initial prototype model results showed that there were substantial room for efficiency improvement, a larger main model with parameters estimated from data is constructed which encapsulate the real world scenario much better. Four different solutions methods are developed and applied to this main model setup. The results are then compared to reference strategy. We find that even though the efficiency was less then first expected from the prototype model results, using these new hedging strategies could reduce costs by 1.5 % - 5%. Although the final choice of the hedging strategy might be down to the end user we suggest the strategy called BW to reduce costs and improve efficiency. The paper also discusses among other things; the solution methods and hedging strategies, the term optimality and the impact of parameters in the model.</p>

corrected abstract:
<p>Hydro power is the largest source for generation of electricity in the Nordic region today. This production is heavily dependent on the weather since it dictates the terms for the availability and the amount of power to be produced. Vattenfall as a company has an incentive to avoid volatile revenue streams as it facilitates economic planning and induces a positive effect on its credit rating, thus also on its bottom line. Vattenfall is a large producer of hydro power with a possibility to move the power market which adds further complexity to the problem. In this thesis the authors develop new hedging strategies which will hedge more efficiently. With efficiency is meant the same risk, or standard deviation, at a lower cost or alternatively formulated lower risk for the same cost. In order to enable comparison and make claims about efficiency, a reference solution is developed that should reflect their current hedging strategy. To achieve higher efficiency we focus on finding dynamic hedging strategies. First a prototype model is suggested to facilitate the construction of the solution methods and if it is worthwhile to pursue a further investigation. As this initial prototype model results showed that there were substantial room for efficiency improvement, a larger main model with parameters estimated from data is constructed which encapsulate the real world scenario much better. Four different solutions methods are developed and applied to this main model setup. The results are then compared to reference strategy. We find that even though the efficiency was less then first expected from the prototype model results, using these new hedging strategies could reduce costs by 1.5 % - 5%. Although the final choice of the hedging strategy might be down to the end user we suggest the strategy called <em>BW</em> to reduce costs and improve efficiency. The paper also discusses among other things; the solution methods and hedging strategies, the term optimality and the impact of parameters in the model.</p>

Note removed the unnecessary paragraph break and added italics
----------------------------------------------------------------------
In diva2:1449088 
abstract is: 
<p>The performance of paperboard materials in packaging application has been investigated and evaluated for a long time. This is because it plays a decisive role for product protection and decoration in packaging applications. Potential damages during transportation sometimes affect the consistency of the performance. Therefore, the capability of the material to resist these external disturbances was of interest.</p><p>A multiply paperboard was chosen as the experimental material. The analysis conducted in this thesis aimed to reveal the tensile behavior in the cross-machine direction (CD) of the material against various kinds of local or global changes. The changes included global and local climate variations, cutouts, and regional weakening and strengthening, which were applied during the intervals between preloading and reloading. The digital image correlation (DIC) analysis computed the time-varying strain fields from the gray level information contained in the recorded videos of loading processes. </p><p>The generated strain fields were imported to post analysis. Comparison between comparable stages (two stages with the same average strain value from different loading sections) was considered as the scheme of isolating the influences of the changes and investigating them individually. The cosine image similarity method and the eigenface algorithm were used to validate this scheme, while the directional average calculation and the strain field compensation method were introduced to realize the isolation.</p><p>The differences between the front and back outer plies of the paperboard sheets were detected as individual. Moreover, both global and local climate changes were affecting the strain distributions of the specimens proportionally on account of the moisture ratio within the material. In addition, the invisible mechanical weakening and strengthening were captured evidently with the analysis, which caused strain concentrations due to the uneven distribution of expansion capability. The relaxation and bending in unloading processes were two of the primary disturbing factors within all the deformed specimens, which were related to time and bending direction, correspondingly.</p>

corrected abstract:
<p>The performance of paperboard materials in packaging application has been investigated and evaluated for a long time. This is because it plays a decisive role for product protection and decoration in packaging applications. Potential damages during transportation sometimes affect the consistency of the performance. Therefore, the capability of the material to resist these external disturbances was of interest.</p><p>A multiply paperboard was chosen as the experimental material. The analysis conducted in this thesis aimed to reveal the tensile behavior in the cross-machine direction (CD) of the material against various kinds of local or global changes. The changes included global and local climate variations, cutouts, and regional weakening and strengthening, which were applied during the intervals between preloading and reloading. The digital image correlation (DIC) analysis computed the time-varying strain fields from the gray level information contained in the recorded videos of loading processes.</p><p>The generated strain fields were imported to post analysis. Comparison between comparable stages (two stages with the same average strain value from different loading sections) was considered as the scheme of isolating the influences of the changes and investigating them individually. The cosine image similarity method and the eigenface algorithm were used to validate this scheme, while the directional average calculation and the strain field compensation method were introduced to realize the isolation.</p><p>The differences between the front and back outer plies of the paperboard sheets were detected as individual. Moreover, both global and local climate changes were affecting the strain distributions of the specimens proportionally on account of the moisture ratio within the material. In addition, the invisible mechanical weakening and strengthening were captured evidently with the analysis, which caused strain concentrations due to the uneven distribution of expansion capability. The relaxation and bending in unloading processes were two of the primary disturbing factors within all the deformed specimens, which were related to time and bending direction, correspondingly.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:708226   - correct as is
----------------------------------------------------------------------
In diva2:942695 
abstract is: 
<p>Student debt in the U.S has grown rapidly over the last decades. A common practice among lenders is to pool the loans into securities that are sold off and traded between institutional investors. Since these securities have no market price this thesis aims to develop a valuation model. A time discrete approach is used, based on the Hull-White short-rate model to create a trinomial interest rate tree. This tree serves as a basis for the discounting of future cash flows generated from a specific student loan asset-backed security. In order to assess the credit risk, the student loan market and potential speculative bubbles are discussed.</p><p>The model is applied on the ”Navient Student Loan Trust 2015-2” and each tranche’s intrinsic value and yield to maturity is calculated. Since the model lacks proper quantification of the credit risk, the result is a valuation model that is best used when valuing asset-backed securities that can be deemed risk- free.</p>

corrected abstract:
<p>Student debt in the U.S has grown rapidly over the last decades. A common practice among lenders is to pool the loans into securities that are sold off and traded between institutional investors. Since these securities have no market price this thesis aims to develop a valuation model. A time discrete approach is used, based on the Hull-White short-rate model to create a trinomial interest rate tree. This tree serves as a basis for the discounting of future cash flows generated from a specific student loan asset-backed security. In order to assess the credit risk, the student loan market and potential speculative bubbles are discussed.</p><p>The model is applied on the ”Navient Student Loan Trust 2015-2” and each tranche’s intrinsic value and yield to maturity is calculated. Since the model lacks proper quantification of the credit risk, the result is a valuation model that is best used when valuing asset-backed securities that can be deemed risk-free.</p>

Note - removed space after hypehn
----------------------------------------------------------------------
In diva2:1503976 
abstract is: 
<p>The electrification of the truck is crucial to meet the strategic vision of the European Union (EU) to contribute to net-zero greenhouse gas emissions for all sectors of the economy and society. The battery-electric truck is very efficient to reduce the emissions and has also a lower Total Cost of Ownership (TCO) compared to diesel trucks. Thus, the energy consumption of the battery-electric truck needs to be analysed in detail, and the differences in the conventional powertrain, recuperation by regenerative braking during driving and charging during standing, need to be considered. This master thesis aims to analyse the energy consumption of the battery-electric truck during driving and standing charging. For driving cycle simulation the Vehicle Energy Consumption calculation TOol (VECTO) and MATLAB are used. Different variations, such as payload, rolling resistance, air drag, and Power Take Off (PTO), are considered in the driving cycle simulation. The driving cycle simulation is verified by calculating the energy balance and compared with the on-road test results. For the standing charging simulation, MATLAB is used to analyse the charging loss with different battery packs and charging speeds. The results are shown with the Sankey diagram and other illustrative tools. Seen from the simulation results, the usable energy of the battery pack is enough for the truck to complete the designed driving cycle. The main loss in the powertrain is the Power Electronic Converter (PEC) and the electric machine. To increase the range and reduce energy loss, using a higher efficiency PEC and electric machine is an efficient method. For the charging simulation, the current Combined Charging System (CCS) standard charging station can charge the battery-electric truck with adequate voltage and reasonable charging time. The main loss during the charging comes from the charging station.</p>

corrected abstract:
<p>The electrification of the truck is crucial to meet the strategic vision of the European Union (EU) to contribute to net-zero greenhouse gas emissions for all sectors of the economy and society. The battery-electric truck is very efficient to reduce the emissions and has also a lower Total Cost of Ownership (TCO) compared to diesel trucks. Thus, the energy consumption of the battery-electric truck needs to be analysed in detail, and the differences in the conventional powertrain, recuperation by regenerative braking during driving and charging during standing, need to be considered.</p><p>This master thesis aims to analyse the energy consumption of the battery-electric truck during driving and standing charging. For driving cycle simulation the Vehicle Energy Consumption calculation TOol (VECTO) and MATLAB are used. Different variations, such as payload, rolling resistance, air drag, and Power Take Off (PTO), are considered in the driving cycle simulation. The driving cycle simulation is verified by calculating the energy balance and compared with the on-road test results. For the standing charging simulation, MATLAB is used to analyse the charging loss with different battery packs and charging speeds. The results are shown with the Sankey diagram and other illustrative tools.</p><p>Seen from the simulation results, the usable energy of the battery pack is enough for the truck to complete the designed driving cycle. The main loss in the powertrain is the Power Electronic Converter (PEC) and the electric machine. To increase the range and reduce energy loss, using a higher efficiency PEC and electric machine is an efficient method. For the charging simulation, the current Combined Charging System (CCS) standard charging station can charge the battery-electric truck with adequate voltage and reasonable charging time. The main loss during the charging comes from the charging station.</p>

Note added missing paragrap breaks
----------------------------------------------------------------------
In diva2:1351514 
abstract is: 
<p>In this study 3D CFD was used to study the mass flow distribution in a reheater in a nuclear power plant. The aim was to see if 3D-modeling provide different results than traditionally used 1D-analysis.</p><p>Models with detailed geometry were created for a section of the tube package in the reheater to obtain pressure drop coefficients. The set up of the model for pure cross-flow over the tubes was first validated against an experiment by Ward [33]. The model showed good agreement with the experimental data for pressure drop and heat transfer, with both unsteady simulations using LES and steady simulations using the SST <em>k </em><em>- w</em> model. It also showed that using the empirical correlations by Ward and Young [32] and Rabas et al. [22], without any experimental data for the tube bank, gave an overestimation of the pressure drop.</p><p>The pressure drop coefficients obtained from the tube package simulations were used to model the tube package in the reheater as a porous medium. A set of perforated plates in the reheater needed to be modeled as a porous medium as well. These plates were originally meant to be mounted on the inlet side of the tube package to even out the mass flow distribution. However, due to a manufacturing error they were now mounted on the outlet side. A set of simulations using the SST <em>k </em><em>- w</em> for a part of the plate geometry gave the required pressure drop coefficients.</p><p>With all coefficients for the porous medium obtained a full scale model of the reheater was created. In the 3D-model the IAPWS-IF97 formulation was used to model the pressure and temperature dependent properties of the steam. Both the SST <em>k </em><em>- w</em> model and DES were used for turbulence closure. A 1D-simulation of the reheater was also performed using RELAP5.</p><p>The 3D-simulations showed a larger difference in the mass flow distribution between the upper and lower part of the tube package than the 1D-model. The 1D-model showed a clear connection between the pressure drop over the perforated plates and the mass flow distribution. However, in the 3D-model the mass flow distribution appeared to also be affected by other properties.</p>

corrected abstract:
<p>In this study 3D CFD was used to study the mass flow distribution in a reheater in a nuclear power plant. The aim was to see if 3D-modeling provide different results than traditionally used 1D-analysis.</p><p>Models with detailed geometry were created for a section of the tube package in the reheater to obtain pressure drop coefficients. The set up of the model for pure cross-flow over the tubes was first validated against an experiment by Ward [33]. The model showed good agreement with the experimental data for pressure drop and heat transfer, with both unsteady simulations using LES and steady simulations using the SST 𝑘-<em>ω</em> model. It also showed that using the empirical correlations by Ward and Young [32] and Rabas et al. [22], without any experimental data for the tube bank, gave an overestimation of the pressure drop.</p><p>The pressure drop coefficients obtained from the tube package simulations were used to model the tube package in the reheater as a porous medium. A set of perforated plates in the reheater needed to be modeled as a porous medium as well. These plates were originally meant to be mounted on the inlet side of the tube package to even out the mass flow distribution. However, due to a manufacturing error they were now mounted on the outlet side. A set of simulations using the SST 𝑘-<em>ω</em> for a part of the plate geometry gave the required pressure drop coefficients.</p><p>With all coefficients for the porous medium obtained a full scale model of the reheater was created. In the 3D-model the IAPWS-IF97 formulation was used to model the pressure and temperature dependent properties of the steam. Both the SST 𝑘-<em>ω</em> model and DES were used for turbulence closure. A 1D-simulation of the reheater was also performed using RELAP5.</p><p>The 3D-simulations showed a larger difference in the mass flow distribution between the upper and lower part of the tube package than the 1D-model. The 1D-model showed a clear connection between the pressure drop over the perforated plates and the mass flow distribution. However, in the 3D-model the mass flow distribution appeared to also be affected by other properties.</p>

Note added paragraph breaks and fixed the 𝑘-ω
----------------------------------------------------------------------
In diva2:1658933 - note the double colon is probably unnecessary in the title
abstract is: 
<p>The purpose of this study is to identify what company specific parameters prior to an IPO have significant impact on share price performance one year after listing. This is done by analysing listings on the Stockholm Stock Exchange in the period 2014-2019. </p><p>The method which has been used is a multiple linear regression with adjusted share price as response variable and 7 specific company data points as independent variables. The share price development of companies is adjusted to the SIX Return Index and the 7 company variables cover size, growth, profitability and ownership.</p><p>The results from the study imply that the independent variables covering size and profitability have the highest impact on share price performance after listing and that ownership had the least impact. The final model with the independent variables that had the highest relevance still only display a small significant correlation with an adjusted R2 = 0.09, which is understandable due to the nature of share prices not being able to be predicted one year into the future. Furthermore, the stock market is a large and complex system of many unknowns, which aggravates the process of simplifying and quantifying data of only one source into a regression model with high predictability.</p>

corrected abstract:
<p>The purpose of this study is to identify what company specific parameters prior to an IPO have significant impact on share price performance one year after listing. This is done by analysing listings on the Stockholm Stock Exchange in the period 2014-2019.</p><p>The method which has been used is a multiple linear regression with adjusted share price as response variable and 7 specific company data points as independent variables. The share price development of companies is adjusted to the SIX Return Index and the 7 company variables covers size, growth, profitability and ownership.</p><p>The results from the study imply that the independent variables covering size and profitability have the highest impact on share price performance after listing and that ownership had the least impact. The final model with the independent variables that had the highest relevance still only display a small significant correlation with an adjusted R2 = 0.09, which is understandable due to the nature of share prices not being able to be predicted one year into the future. Furthermore, the stock market is a large and complex system of many unknowns, which aggravate the process of simplifying and quantifying data of only one source into a regression model with high predictability.</p>

Note small changes in text, also note that "R2" should be "R<sup>2</sup>" - error in original
----------------------------------------------------------------------
In diva2:1416550 
abstract is: 
<p>The tyre is an essential part of a road vehicle. It is in the contact between road and tyre that the forces that create the possibility for the driver to control the vehicle are generated. Tyres, however, wear down, which leads to both unhealthy wear particles and disposal of old tyres, both of which are harmful to the environment. If one could learn more about what causes wear, it might be possible to reduce tyre wear, which would be beneficial from both an economic and an ecological point of view. The aim of this thesis work is to develop a tyre model that can simulate tyre wear and take temperature, pressure and vehicle settings into account. Based on tyre brush theory, a tyre wear model has been developed which includes a thermal model, a pressure model and a friction model. Simulations and analysis of different cases has been performed. From the results, one can conclude the following: the tyre temperature and inflation pressure change with the distance the vehicle travels at the beginning and later become steady; higher external temperature will decrease tyre wear rate since the inflation pressure increases with the external temperature and the sliding friction decreases; higher vehicle speed leads to a higher tyre wear rate; the tyre temperature increases with increasing vehicle speed; the amount of tyre wear increases linearly with the normal load on the tyre; the tyre wear increases with the slip ratio exponentially due to both the siding distance and the sliding friction increasing with the slip ratio; the tyre wear increases exponentially with the slip angle. The complete model can estimate the tyre wear with different vehicle settings and external factors. More experiments are needed in the future to validate the complete model. In addition, since the heat transfer coefficient is changeable with temperature, the thermal model can be improved by introducing dynamic heat transfer coefficients. The Savkoor friction model used in the report can also be improved by tuning its parameters using more experimental data.</p>

corrected abstract:
<p>The tyre is an essential part of a road vehicle. It is in the contact between road and tyre that the forces that create the possibility for the driver to control the vehicle are generated. Tyres, however, wear down, which leads to both unhealthy wear particles and disposal of old tyres, both of which are harmful to the environment. If one could learn more about what causes wear, it might be possible to reduce tyre wear, which would be beneficial from both an economic and an ecological point of view.</p><p>The aim of this thesis work is to develop a tyre model that can simulate tyre wear and take temperature, pressure and vehicle settings into account.</p><p>Based on tyre brush theory, a tyre wear model has been developed which includes a thermal model, a pressure model and a friction model. Simulations and analysis of different cases has been performed. From the results, one can conclude the following: the tyre temperature and inflation pressure change with the distance the vehicle travels at the beginning and later become steady; higher external temperature will decrease tyre wear rate since the inflation pressure increases with the external temperature and the sliding friction decreases; higher vehicle speed leads to a higher tyre wear rate; the tyre temperature increases with increasing vehicle speed; the amount of tyre wear increases linearly with the normal load on the tyre; the tyre wear increases with the slip ratio exponentially due to both the siding distance and the sliding friction increasing with the slip ratio; the tyre wear increases exponentially with the slip angle.</p><p>The complete model can estimate the tyre wear with different vehicle settings and external factors. More experiments are needed in the future to validate the complete model. In addition, since the heat transfer coefficient is changeable with temperature, the thermal model can be improved by introducing dynamic heat transfer coefficients. The Savkoor friction model used in the report can also be improved by tuning its parameters using more experimental data.</p>

Note added paragraph breaks
----------------------------------------------------------------------
In diva2:1442611 
abstract is: 
<p>Due to their large torque-speed ratio and transmission efficiency, planetary gears are widely used in the automotive industry. However, high amplitude vibrations remain their critical weakness, which limits their usage especially when new strict noise legislations come into action.</p><p>A new approach to handle the instability problems of planetary gears encountered in real industrial context is presented in this work. First, the dynamic response of a planetary gear failing to pass the noise regulations is theoretically investigated through an analytical model. The equations of motion were solved using the Spectral Iterative Method. The observed experimental results correlated well with those from the developed model. In order to limit the resonance phenomena, impacts of different macro and micro-geometry modifications were analytically investigated: quadratic teeth profile, different planets positioning, different number of teeth and number of planets. Optimum modifications were retrieved and are expected to be tested experimentally on a test bench and on the truck.</p><p>Finally, the analytical model’s limits and sensitivity to different parameters were investigated in order to certify its reliability, and suggestions for improvements were presented.</p>

corrected abstract:
<p>Due to their large torque-speed ratio and transmission efficiency, planetary gears are widely used in the automotive industry. However, high amplitude vibrations remain their critical weakness, which limits their usage especially when new strict noise legislations come into action.</p><p>A new approach to handle the instability problems of planetary gears encountered in real industrial context is presented in this work. First, the dynamic response of a planetary gear failing to pass the noise regulations is theoretically investigated through an analytical model. The equations of motion were solved using the Spectral Iterative Method. The observed experimental results correlated well with those from the developed model. In order to limit the resonance phenomena, impacts of different macro and micro-geometry modifications were analytically investigated: quadratic teeth profile, different planets positioning, different number of teeth and number of planets. Optimum modifications were retrieved and are expected to be tested experimentally on a test bench and on the truck.</p><p>Finally, the analytical model’s limits and sensitivity to different parameters were investigated in order to certify its reliability, and suggestions for improvements were presented.</p>
----------------------------------------------------------------------
In diva2:737807   - correct as is
----------------------------------------------------------------------
In diva2:1232457 
abstract is: 
<p>Classical mechanics is the branch of physics concerned with describing the motion of bodies. The subject is based on three simple axioms relating forces and movement. These axioms were first postulated by Newton in the 17th century and are known as his three laws of motion.</p><p>Lagrangian mechanics is a restatement of the Newtonian formulation. It deals with energy quantities and paths-of-motion instead of forces. This often makes it simpler to use when working with non-trivial mechanical systems. In this thesis, we use the Lagrangian method to model two such systems; A rotating torus and a variant of the classical double pendulum.</p><p>It soon becomes clear that the complexity of these systems make them difficult to attack by hand. For this reason, we take a computer-based approach. We use a software-package called Sophia which is a plug-in to the computer algebra system Maple. Sofia was developed at the Department of Mechanics at KTH for the specific purpose of modeling mechanical problems using Lagrange’s method. We demonstrate that this method can be successfully applied to the analysis of motion of complex mechanical systems. The complete equations of motion are derived in a symbolic form and then integrated numerically. The motion of the system is finally visualized by means of 3D graphics software Blender.</p>

corrected abstract:
<p>Classical mechanics is the branch of physics concerned with describing the motion of bodies. The subject is based on three simple axioms relating forces and movement. These axioms were first postulated by Newton in the 17th century and are known as his three laws of motion.</p><p>Lagrangian mechanics is a restatement of the Newtonian formulation. It deals with energy quantities and paths-of-motion instead of forces. This often makes it simpler to use when working with non-trivial mechanical systems. In this thesis, we use the Lagrangian method to model two such systems; A rotating torus and a variant of the classical double pendulum.</p><p>It soon becomes clear that the complexity of these systems make them difficult to attack by hand. For this reason, we take a computer-based approach. We use a software-package called Sophia which is a plug-in to the computer algebra system Maple™. Sofia was developed at the Department of Mechanics at KTH for the specific purpose of modeling mechanical problems using Lagrange’s method. We demonstrate that this method can be successfully applied to the analysis of motion of complex mechanical systems. The complete equations of motion are derived in a symbolic form and then integrated numerically. The motion of the system is finally visualized by means of 3D graphics software Blender™.</p>

Note added the ™ markup
----------------------------------------------------------------------
In diva2:1186356   - correct as is
----------------------------------------------------------------------
In diva2:1349671   - correct as is
----------------------------------------------------------------------
In diva2:1210809 
abstract is: 
<p>The Tobii Pro Glasses 2 are used to record gaze data that is used for market research or scientific experiments. To make extraction of relevant statistics more efficient, the gaze points in the recorded video are mapped to a static snapshot with areas of interests (AOIs). The most important statistics revolve around fixations. A fixation is when a person is keeping his or her vision still for a short period of time. The method most used today is to manually map the gaze points. However, a faster method is automated mapping using the Real World Mapping (RWM) tool. In order to examine the reliability of RWM, the fixations from different recordings and projects were analyzed using Decision Trees. Further, a Random Forest (RF) model was constructed in order to predict if a gaze point was correctly or incorrectly mapped. It was shown that fixation classification on data from RWM performed significantly worse than when the same fixation classification on manually mapped data was run. It was shown that RWM works better when head movement is low and AOIs are set appropriately. This can guide researchers in set- ting up experiments, although major improvements of RWM is needed. The RF classifier showed promising results on several test sets for mapped gaze points. It also showed promising results for gaze points that were not mapped and were close in time to being mapped. In conclusion, the RF should replace current methods of estimating the quality of RWM gaze points. Gaze points that are classified as badly mapped can be manually remapped. If RWM fails to map large segments of gaze points to a snapshot, visually classifying these to be remapped is the preferred method.</p>

corrected abstract:
<p>The Tobii Pro Glasses 2 are used to record gaze data that is used for market research or scientific experiments. To make extraction of relevant statistics more efficient, the gaze points in the recorded video are mapped to a static snapshot with areas of interests (AOIs). The most important statistics revolve around fixations. A fixation is when a person is keeping his or her vision still for a short period of time. The method most used today is to manually map the gaze points. However, a faster method is automated mapping using the Real World Mapping (RWM) tool. In order to examine the reliability of RWM, the fixations from different recordings and projects were analyzed using Decision Trees. Further, a Random Forest (RF) model was constructed in order to predict if a gaze point was correctly or incorrectly mapped. It was shown that fixation classification on data from RWM performed significantly worse than when the same fixation classification on manually mapped data was run. It was shown that RWM works better when head movement is low and AOIs are set appropriately. This can guide researchers in setting up experiments, although major improvements of RWM is needed. The RF classifier showed promising results on several test sets for mapped gaze points. It also showed promising results for gaze points that were not mapped and were close in time to being mapped. In conclusion, the RF should replace current methods of estimating the quality of RWM gaze points. Gaze points that are classified as badly mapped can be manually remapped. If RWM fails to map large segments of gaze points to a snapshot, visually classifying these to be remapped is the preferred method.</p>


Note - eliminated an unnecessary hyphen
----------------------------------------------------------------------
In diva2:1347995 
abstract is: 
<p>In this thesis, the use of unsupervised and semi-supervised machine learning techniques was analyzed as potential tools for anomaly detection in the sensor network that the electrical system in a Scania truck is comprised of. The experimentation was designed to analyse the need for both point and contextual anomaly detection in this setting. For the point anomaly detection the method of Isolation Forest was experimented with and for contextual anomaly detection two different recurrent neural network architectures using Long Short Term Memory units was relied on. One model was simply a many to one regression model trained to predict a certain signal, while the other was an encoder-decoder network trained to reconstruct a sequence. Both models were trained in an semi-supervised manner, i.e. on data that only depicts normal behaviour, which theoretically should lead to a performance drop on abnormal sequences resulting in higher error terms. In both setting the parameters of a Gaussian distribution were estimated using these error terms which allowed for a convenient way of defining a threshold which would decide if the observation would be flagged as anomalous or not. Additional experimentation's using an exponential weighted moving average over a number of past observations to filter the signal was also conducted. The models performance on this particular task was very different but the regression model showed a lot of promise especially when combined with a filtering preprocessing step to reduce the noise in the data. However the model selection will always be governed by the nature the particular task at hand so the other methods might perform better in other settings.</p>

corrected abstract:
<p>In this thesis, unsupervised and semi-supervised machine learning techniques are analyzed as potential tools for anomaly detection in Scania truck sensor networks.  The thesis investigates the need for both point and contextual anomaly detection in this setting. For the point anomaly detection the method of Isolation forest was applied and for contextual anomaly detection two different recurrent neural network architectures using Long Short Term Memory units were used. One model was simply a many-to-one regression model trained to predict a certain signal, while the other was an encoder-decoder network trained to reconstruct a sequence. Both models were trained in an semi-supervised manner, i.e. on data that only depicts normal behaviour, which theoretically should lead to a performance drop on abnormal sequences resulting in higher error terms. In both settings the parameters of a Gaussian distribution were estimated using these error terms, which allowed for a convenient way of defining a threshold which would decide if the observation would be flagged as anomalous or not. Additional experiments using an exponential weighted moving average over a number of past observations to filter the signal was also conducted. The methods performance on this particular task were very different, but the regression model showed a lot of promise especially when combined with a filtering preprocessing step to reduce the noise in the data. However, the model selection will always be governed by the nature of the particular task at hand, so the other methods might perform better in other settings.</p>

Note there is a major difference between the DiVA and original abstract
----------------------------------------------------------------------
In diva2:1431644   - correct as is
----------------------------------------------------------------------
In diva2:1216876   - correct as is
----------------------------------------------------------------------
In diva2:1156318 - missing space in title:
"Ant Colony Algorithms andits applications to Autonomous Agents Systems"
==>
"Ant Colony Algorithms and its applications to Autonomous Agents Systems"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:233558
Note: no full text in DiVA

The abstract (currently) shown in DiVA does not match the txe that I earlier found for this DiVA_ID.
----------------------------------------------------------------------
In diva2:420111   - correct as is
----------------------------------------------------------------------
In diva2:911535   - correct as is
----------------------------------------------------------------------
In diva2:1320414   - correct as is
----------------------------------------------------------------------
In diva2:1212533   - correct as is
----------------------------------------------------------------------
In diva2:1212533 
abstract is: 
<p>This thesis is an implementation project of a portfolio optimization model, with the purpose of creating a decision support tool. It aims to provide quantitative input to the portfolio construction process at Handelsbanken Fonder, by applying Konno &amp; Yamazaki’s Mean Absolute Deviation method, with a Feinstein &amp; Thapa modification. Additionally, the Black-Litterman model is implemented to approximate the input of expected return. The linear optimization problem was then solved by the Simplex algorithm. The main deliverable is a model that can assist portfolio managers in making investment decisions. Back-testing of the model showed that it did not outperform the benchmark portfolios, which is likely a result of only allowing long positions in the model. Nevertheless, the model provides value by giving the user a second opinion on the efficient frontier, for any given investment decision.</p>

corrected abstract:
<p>This thesis is an implementation project of a portfolio optimization model, with the purpose of creating a decision support tool. It aims to provide quantitative input to the portfolio construction process at Handelsbanken Fonder, by applying Konno &amp; Yamazaki’s Mean Absolute Deviation method, with a Feinstein &amp; Thapa modification. Additionally, the Black-Litterman model is implemented to approximate the input of expected return. The linear optimization problem was then solved by the Simplex algorithm. The main deliverable is a model that can assist portfolio managers in making investment decisions. Back-testing of the model showed that it did not outperform the benchmark portfolios, which is likely a result of only allowing long positions in the model. Nevertheless, the model provides value by giving the user a second opinion on the efficient frontier, for any given investment decision.</p>
----------------------------------------------------------------------
In diva2:1298366   - correct as is
----------------------------------------------------------------------
In diva2:1442067 
abstract is: 
<p>Graph theory is a mathematical study of objects and their pairwise relations, known as nodes and edges respectively. The birth of graph theory is often considered to take place in 1736 when the Swiss mathematician Leonhard Euler tried to solve a routing problem involving seven bridges of Königsberg in Prussia. In more recent times, graph theory has caught the attention of companies from all types of industries due to its power of modelling and analysing exceptionally large networks.</p><p>This thesis investigates the usage of graph theory in the energy sector for a utility company, in particular Fortum whose activities consist of, but not limited to, production and distribution of electricity and heat. The output of the thesis is a wide overview of graph-theoretic concepts and their practical applications, as well as a study of a use-case where some concepts are put into deeper analysis. The chosen use-case within the scope of this thesis is feature selection - a process for reducing the number of features, also known as input variables, typically before a regression model is built to avoid overfitting and increase model interpretability.</p><p>Five graph-based feature selection methods with different points of view are studied. Experiments are conducted on realistic data sets with many features to verify the legitimacy of the methods. One of the data sets is owned by Fortum and used for forecasting the electricity price, among other important quantities. The obtained results look promising according to several evaluation metrics and can be used by Fortum as a support tool to develop prediction models. In general, a utility company can likely take advantage graph theory in many ways and add value to their business with enriched mathematical knowledge.</p>

corrected abstract:
<p>Graph theory is a mathematical study of objects and their pairwise relations, known as <em>nodes</em> and <em>edges</em> respectively. The birth of graph theory is often considered to take place in 1736 when the Swiss mathematician Leonhard Euler tried to solve a routing problem involving seven bridges of Königsberg in Prussia. In more recent times, graph theory has caught the attention of companies from all types of industries due to its power of modelling and analysing exceptionally large networks.</p><p>This thesis investigates the usage of graph theory in the energy sector for a utility company, in particular Fortum whose activities consist of, but not limited to, production and distribution of electricity and heat. The output of the thesis is a wide overview of graph-theoretic concepts and their practical applications, as well as a study of a use-case where some concepts are put into deeper analysis. The chosen use-case within the scope of this thesis is feature selection - a process for reducing the number of features, also known as input variables, typically before a regression model is built to avoid overfitting and increase model interpretability.</p><p>Five graph-based feature selection methods with different points of view are studied. Experiments are conducted on realistic data sets with many features to verify the legitimacy of the methods. One of the data sets is owned by Fortum and used for forecasting the electricity price, among other important quantities. The obtained results look promising according to several evaluation metrics and can be used by Fortum as a support tool to develop prediction models. In general, a utility company can likely take advantage graph theory in many ways and add value to their business with enriched mathematical knowledge.</p>

Note added italics
----------------------------------------------------------------------
In diva2:1799803 
abstract is: 
<p>This thesis investigates applying the semiparametric method Peaks-Over-Threshold on data generated from a Monte Carlo simulation when estimating the financial risk measures Value-at-Risk and Expected Shortfall. The goal is to achieve a faster convergence than a Monte Carlo simulation when assessing extreme events that symbolise the worst outcomes of a financial portfolio. Achieving a faster convergence will enable a reduction of iterations in the Monte Carlo simulation, thus enabling a more efficient way of estimating risk measures for the portfolio manager. </p><p>The financial portfolio consists of US life insurance policies offered on the secondary market, gathered by our partner RessCapital. The method is evaluated on three different portfolios with different defining characteristics. </p><p>In Part I an analysis of selecting an optimal threshold is made. The accuracy and precision of Peaks-Over-Threshold is compared to the Monte Carlo simulation with 10,000 iterations, using a simulation of 100,000 iterations as the reference value. Depending on the risk measure and the percentile of interest, different optimal thresholds are selected. </p><p>Part II presents the result with the optimal thresholds from Part I. One can conclude that Peaks-Over-Threshold performed significantly better than a Monte Carlo simulation for Value-at-Risk with 10,000 iterations. The results for Expected Shortfall did not achieve a clear improvement in terms of precision, but it did show improvement in terms of accuracy. </p><p>Value-at-Risk and Expected Shortfall at the 99.5th percentile achieved a greater error reduction than at the 99th. The result therefore aligned well with theory, as the more "rare" event considered, the better the Peaks-Over-Threshold method performed. </p><p>In conclusion, the method of applying Peaks-Over-Threshold can be proven useful when looking to reduce the number of iterations since it do increase the convergence of a Monte Carlo simulation. The result is however dependent on the rarity of the event of interest, and the level of precision/accuracy required.</p>

corrected abstract:
<p>This thesis investigates applying the semiparametric method Peaks-Over-Threshold on data generated from a Monte Carlo simulation when estimating the financial risk measures Value-at-Risk and Expected Shortfall. The goal is to achieve a faster convergence than a Monte Carlo simulation when assessing extreme events that symbolise the worst outcomes of a financial portfolio. Achieving a faster convergence will enable a reduction of iterations in the Monte Carlo simulation, thus enabling a more efficient way of estimating risk measures for the portfolio manager.</p><p>The financial portfolio consists of US life insurance policies offered on the secondary market, gathered by our partner RessCapital. The method is evaluated on three different portfolios with different defining characteristics.</p><p>In Part I an analysis of selecting an optimal threshold is made. The accuracy and precision of Peaks-Over-Threshold is compared to the Monte Carlo simulation with 10,000 iterations, using a simulation of 100,000 iterations as the reference value. Depending on the risk measure and the percentile of interest, different optimal thresholds are selected.</p><p>Part II presents the result with the optimal thresholds from Part I. One can conclude that Peaks-Over-Threshold performed significantly better than a Monte Carlo simulation for Value-at-Risk with 10,000 iterations. The results for Expected Shortfall did not achieve a clear improvement in terms of precision, but it did show improvement in terms of accuracy.</p><p>Value-at-Risk and Expected Shortfall at the 99.5th percentile achieved a greater error reduction than at the 99th. The result therefore aligned well with theory, as the more ”rare” event considered, the better the Peaks-Over-Threshold method performed.</p><p>In conclusion, the method of applying Peaks-Over-Threshold can be proven useful when looking to reduce the number of iterations since it do increase the convergence of a Monte Carlo simulation. The result is however dependent on the rarity of the event of interest, and the level of precision/accuracy required.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph and fixing double quotes
----------------------------------------------------------------------
In diva2:1800537   - correct as is
----------------------------------------------------------------------
In diva2:1757027 
abstract is: 
<p>Investing in public real estate stocks can diversify a stock portfolio due to the nature of these companies. The industry is generally less sensitive to economic downturns and spikes in inflation are offset by increased real estate property and rent prices. Nevertheless, measures of the wider economy could be used as predictors of the real estate stock market. </p><p>This thesis attempts to model the Swedish real estate stock market with the index SX35PI (Stockholm Real Estate PI) using the fundamental economic factors and repo rate. Data was collected and formatted to a monthly interval for the period February 2012 to December 2021. This resulted in an exponential multiple regression model that used all the regressors that explained 95.7% of the variation in SX35PI, and an alternative autoregressive forecasting model that explained 82.3% of the variation in SX35PI.</p>

corrected abstract:
<p>Investing in public real estate stocks can diversify a stock portfolio due to the nature of these companies. The industry is generally less sensitive to economic downturns and spikes in inflation are offset by increased real estate property and rent prices. Nevertheless, measures of the wider economy could be used as predictors of the real estate stock market.</p><p>This thesis attempts to model the Swedish real estate stock market with the index SX35PI (Stockholm Real Estate PI) using the fundamental economic factors and repo rate. Data was collected and formatted to a monthly interval for the period February 2012 to December 2021. This resulted in an exponential multiple regression model that used all the regressors that explained 95.7% of the variation in SX35PI, and an alternative autoregressive forecasting model that explained 82.3% of the variation in SX35PI.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1871604   - correct as is
----------------------------------------------------------------------
In diva2:1851009   - correct as is
----------------------------------------------------------------------
In diva2:1450579   - correct as is
----------------------------------------------------------------------
In diva2:1440755 
abstract is: 
<p>This report sets out to implement and asses a one-dimensional loss model for centrifugal compressors. The loss model which has been evaluated is the model by Oh et al. (1997) [1]. The implementation was completed using iterative methods implemented in Matlab. It was shown that the performance prediction using the Oh et al method produced usable estimates in the higher rotational speed region used in this work, at design conditions. The pressure ratio estimates had a difference compared to the measured reference data of 5-10% while the isentropic efficiency had slightly higher differences. Furthermore, it was shown that the best estimates of the pressure ratio came from the lowest rotational speeds, and successively flattened out from the expected curvature as the speed increased. The isentropic efficiency did not have the same property, giving the best consistent estimate at a higher rotational speed of 143 kRPM. The conclusion which was drawn from this work was that the model by Oh et al. is a useful model for prediction of performance at design points in the lower RPM region, while requiring complementary calculations at off-design conditions at high RPM. It was also pointed out that there are several areas that require further work within performance prediction to make it more implementation-friendly.</p>

corrected abstract:
<p>This report sets out to implement and asses a one-dimensional loss model for centrifugal compressors. The loss model which has been evaluated is the model by Oh et al. (1997) [1]. The implementation was completed using iterative methods implemented in Matlab. It was shown that the performance prediction using the Oh et al method produced usable estimates in the higher rotational speed region used in this work, at design conditions. The pressure ratio estimates had a difference compared to the measured reference data of 5-10% while the isentropic efficiency had slightly higher differences. Furthermore, it was shown that the best estimates of the pressure ratio came from the lowest rotational speeds, and successively flattened out from the expected curvature as the speed increased. The isentropic efficiency did not have the same property, giving the best consistent estimate at a higher rotational speed of 143 kRPM. The conclusion which was drawn from this work was that the model by Oh et al. is a useful model for prediction of performance at design points in the lower RPM region, while requiring complementary calculations at off-design conditions at high RPM.</p><p>It was also pointed out that there are several areas that require further work within performance prediction to make it more implementation-friendly.</p>

Note added the missing paragraph break
----------------------------------------------------------------------
In diva2:1351695   - correct as is
----------------------------------------------------------------------
In diva2:1596353   - correct as is
----------------------------------------------------------------------
In diva2:1450000 
abstract is: 
<p>With the increasing demand for renewable energy sources, new systems are being developed to sustain future infrastructure, accommodating these new energy sources. One of the proposed solutions is to incorporate distributed energy resources to different households in order to provide local energy demands effectively. To enable large-scale integration of flexible energy resources, it is crucial to reduce end-user energy and power costs, which can be done by designing an optimization model objected to minimize the total electricity bill. In the scope of this Master thesis, the interest lies in investigating a control strategy to operate batteries, heat pumps, and other assets by producing the optimal setpoints using the designed optimization algorithm that takes, amongst others, market and weather data as well as customer behavior into account. The applied method for producing these setpoints is sensitivity analysis in linear programming, and heat pump scheduling has been investigated for performance evaluation of this technique.</p><p>The results show that applying this method produces the optimal setpoints over the non-controllable electricity load range by utilizing a low number of optimizations, i.e. high computation-efficiency, and high accuracy. Consequently, the controller by having the given setpoints as the input can easily adjust the heat pump output power based on the real-time non-controllable electricity load without creating any peaks and extra costs for the customers.</p>

corrected abstract:
<p>With increasing demand for renewable energy sources, new systems are being developed to sustain future infrastructure, accommodating these new energy sources. One of the proposed solutions is to incorporate distributed energy resources to different households in order to provide local energy demands effectively. To enable large-scale integration of flexible energy resources, it is crucial that to reduce end-user energy and power costs, which can be done by designing an optimization model objected to minimize the total electricity bill. In the scope of this Master thesis, the interest lies in investigating a control strategy to operate batteries, heat pumps, and other assets by producing the optimal setpoints using the designed optimization algorithm that takes, amongst others, market and weather data as well as customer behavior into account. The applied method for producing these setpoints is sensitivity analysis in linear programming, and heat pump scheduling has been investigated for performance evaluation of this technique.</p><p>The results show that applying this method produces the optimal setpoints over the non-controllable electricity load range by utilizing a low number of optimizations, i.e. high computation-efficiency, and high accuracy. Consequently, the controller by having the given setpoints as the input can easily adjust the heat pump output power based on the real-time non-controllable electricity load without creating any peaks and extra costs for the customers.</p>

Note small changes in the text
----------------------------------------------------------------------
In diva2:828122 
abstract is: 
<p>The electric vehicle (EV) fleet is expected to continue growing in the near future. The increasing electrification of the transportation sector is a promising solution to the global dependency on oil and is expected to drive investments in renewable and intermittent energy sources.</p><p>In order to facilitate the integration, utilize the potential of a growing EV fleet and to avoid unwanted effects on the electric grid, smart charging strategies will be necessary. The aspect of smart EV charging investigated in this work is the profitability of bidirectional energy transfer, often referred to as vehicle-to-grid (V2G), i.e. the possibility of using aggregated EV batteries as storage for energy which can be injected back to the grid.</p><p>A mixed integer linear problem (MILP) for minimizing energy costs and battery ageing costs for EV owners is formulated. The battery degradation due to charging and discharging is accounted for in the model used. A realistic case study of overnight charging of EVs in Sweden is constructed, and the results show that given current energy prices and battery costs, V2G is not profitable for EV owners. Further, a hypothetical case for lower battery costs is formulated to demonstrate the ability of our MILP model to treat a number of charging scenarios</p>

corrected abstract:
<p>The electric vehicle (EV) fleet is expected to continue growing in the near future. The increasing electrification of the transportation sector is a promising solution to the global dependency on oil and is expected to drive investments in renewable and intermittent energy sources.</p><p>In order to facilitate the integration, utilize the potential of a growing EV fleet and to avoid unwanted effects on the electric grid, smart charging strategies will be necessary.</p><p>The aspect of smart EV charging investigated in this work is the profitability of bidirectional energy transfer, often referred to as vehicle-to-grid (V2G), i.e. the possibility of using aggregated EV batteries as storage for energy which can be injected back to the grid.</p><p>A mixed integer linear problem (MILP) for minimizing energy costs and battery ageing costs for EV owners is formulated. The battery degradation due to charging and discharging is accounted for in the model used. A realistic case study of overnight charging of EVs in Sweden is constructed, and the results show that given current energy prices and battery costs, V2G is not profitable for EV owners. Further, a hypothetical case for lower battery costs is formulated to demonstrate the ability of our MILP model to treat a number of charging scenarios.</p>

Note added missing paragraph break and missing terminal period in last sentence
----------------------------------------------------------------------
In diva2:1149189 
abstract is: 
<p>In recent years, new regulations and stronger competition have further increased the importance of stochastic asset-liability management (ALM) models for life insurance firms. However, the often complex nature of life insurance contracts makes modeling to a challenging task, and insurance firms often struggle with models quickly becoming too complicated and inefficient. There is therefore an interest in investigating if, in fact, certain traits of financial ratios could be exposed through a more efficient model.</p><p>In this thesis, a discrete time stochastic model framework, for the simulation of simplified balance sheets of life insurance products, is proposed. The model is based on a two-factor stochastic capital market model, supports the most important product characteristics, and incorporates a reserve-dependent bonus declaration. Furthermore, a first approach to endogenously model customer transitions is proposed, where realized policy returns are used for assigning transition probabilities.</p><p>The model's sensitivity to different input parameters, and ability to capture the most important behaviour patterns, are demonstrated by the use of scenario and sensitivity analyses. Furthermore, based on the findings from these analyses, suggestions for improvements and further research are also presented.</p>

corrected abstract:
<p>In recent years, new regulations and stronger competition have further increased the importance of stochastic asset-liability management (ALM) models for life insurance firms. However, the often complex nature of life insurance contracts makes modeling to a challenging task, and insurance firms often struggle with models quickly becoming too complicated and inefficient. There is therefore an interest in investigating if, in fact, certain traits of financial ratios could be exposed through a more efficient model.</p><p>In this thesis, a discrete time stochastic model framework, for the simulation of simplified balance sheets of life insurance products, is proposed. The model is based on a two-factor stochastic capital market model, supports the most important product characteristics, and incorporates a reserve-dependent bonus declaration. Furthermore, a first approach to endogenously model customer transitions is proposed, where realized policy returns are used for assigning transition probabilities.</p><p>The model's sensitivity to different input parameters, and ability to capture the most important behaviour patterns, are demonstrated by the use of scenario and sensitivity analyses. Furthermore, based on the findings from these analyses, suggestions for improvements and further research are also presented.</p>
In diva2:1149189 
abstract is: 
<p>In recent years, new regulations and stronger competition have further increased the importance of stochastic asset-liability management (ALM) models for life insurance firms. However, the often complex nature of life insurance contracts makes modeling to a challenging task, and insurance firms often struggle with models quickly becoming too complicated and inefficient. There is therefore an interest in investigating if, in fact, certain traits of financial ratios could be exposed through a more efficient model.</p><p>In this thesis, a discrete time stochastic model framework, for the simulation of simplified balance sheets of life insurance products, is proposed. The model is based on a two-factor stochastic capital market model, supports the most important product characteristics, and incorporates a reserve-dependent bonus declaration. Furthermore, a first approach to endogenously model customer transitions is proposed, where realized policy returns are used for assigning transition probabilities.</p><p>The model's sensitivity to different input parameters, and ability to capture the most important behaviour patterns, are demonstrated by the use of scenario and sensitivity analyses. Furthermore, based on the findings from these analyses, suggestions for improvements and further research are also presented.</p>

corrected abstract:
<p>In recent years, new regulations and stronger competition have further increased the importance of stochastic asset-liability management (ALM) models for life insurance firms. However, the often complex nature of life insurance contracts makes modelling to a challenging task, and insurance firms often struggle with models quickly becoming too complicated and inefficient. There is therefore an interest in investigating if, in fact, certain traits of financial ratios could be exposed through a more efficient model.</p><p>In this thesis, a discrete time stochastic model framework, for the simulation of simplified balance sheets of life insurance products, is proposed. The model is based on a two-factor stochastic capital market model, supports the most important product characteristics, and incorporates a reserve-dependent bonus declaration. Furthermore, a first approach to endogenously model customer transitions is proposed, where realized policy returns are used for assigning transition probabilities.</p><p>The model's sensitivity to different input parameters, and ability to capture the most important behaviour patterns, are demonstrated by the use of scenario and sensitivity analyses. Furthermore, based on the findings from these analyses, suggestions for improvements and further research are also presented.</p>


Note - only one change "modeling" to "modelling" (as per the original)
-------------------------------------------------------------------------------
In diva2:1699780 
abstract is: 
<p>In this master thesis, different neural networks have investigated annotating objects in video streams with partially annotated data as input. Annotation in this thesis is referring to bounding boxes around the targeted objects. Two different methods have been used ROLO and GOTURN, object detection with tracking respective object tracking with pixels. The data set used for validation is surveillance footage consists of varying image resolution, image size and sequence length. Modifications of the original models have been executed to fit the test data. </p><p>Promising results for modified GOTURN were shown, where the partially annotated data was used as assistance in tracking. The model is robust and provides sufficiently accurate object detections for practical use. With the new model, human resources for image annotation can be reduced by at least half.</p>

corrected abstract:
<p>In this master thesis, different neural networks have investigated annotating objects in video streams with partially annotated data as input. Annotation in this thesis is referring to bounding boxes around the targeted objects. Two different methods have been used ROLO and GOTURN, object detection with tracking respective object tracking with pixels. The data set used for validation is surveillance footage consists of varying image resolution, image size and sequence length. Modifications of the original models have been executed to fit the test data.</p><p>Promising results for modified GOTURN were shown, where the partially annotated data was used as assistance in tracking. The model is robust and provides sufficiently accurate object detections for practical use. With the new model, human resources for image annotation can be reduced by at least half.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1149609   - correct as is
----------------------------------------------------------------------
In diva2:1341574 
abstract is: 
<p>We cover in this report the implementation of a reinforcement learning (RL) algorithm capable of learning how to play the game 'Breakout' on the Atari Learning Environment (ALE). The non-human player (agent) is given no prior information of the game and must learn from the same sensory input that a human would typically receive when playing the game. The aim is to reproduce previous results by optimizing the agent driven control of 'Breakout' so as to surpass a typical human score. To this end, the problem is formalized by modeling it as a Markov Decision Process. We apply the celebrated Deep Q-Learning algorithm with action masking to achieve an optimal strategy. We find our agent's average score to be just below the human benchmarks: achieving an average score of 20, approximately 65% of the human counterpart. We discuss a number of implementations that boosted agent performance, as well as further techniques that could lead to improvements in the future.</p>

corrected abstract:
<p>We cover in this report the implementation of a reinforcement learning (RL) algorithm capable of learning how to play the game <em>Breakout</em> on the Atari Learning Environment (ALE). The non-human player (agent) is given no prior information of the game and must learn from the same sensory input that a human would typically receive when playing the game.</p><p>The aim is to reproduce previous results by optimizing the agent driven control of <em>Breakout</em> so as to surpass a typical human score. To this end, the problem is formalized by modeling it as a Markov Decision Process. We apply the celebrated Deep Q-Learning algorithm with action masking to achieve an optimal strategy.</p><p>We find our agent's average score to be just below the human benchmarks: achieving an average score of 20, approximately 65% of the human counterpart. We discuss a number of implementations that boosted agent performance, as well as further techniques that could lead to improvements in the future.</p>

Note "Breakout" was not set in quotes but rather in italics. Also added the missing paragraph breaks
----------------------------------------------------------------------
In diva2:1499840 
abstract is: 
<p>The study of structure-borne vibrations caused by railway traffic is becoming increasingly relevant along with the expansions of cities, as building projects are forced towards more sensitive areas, e.g closer to railway tracks. With high enough vibration levels, there is a risk for human annoyance inside buildings. Knowledge of the attenuation of vibrations in the ground caused by railway traffic is in such cases of great importance. Where a railway switch is present, higher levels are likely to be generated as the train passes. In this study, measurements of vibrations from railway traffic as trains pass a railway switch was performed. Four accelerometers were placed at different distances away from the track to capture the attenuation. In the analysis of the measurements, the attenuation of train induced vibrations and the dominating response due to the presence of the railway switch is found. From the findings in the measurements, a model based on superpositioning of incoming waves from point sources is developed, that predicts the attenuation of vibrations over distance from a train passing over a railway switch.</p>

corrected abstract:
<p>The study of structure-borne vibrations caused by railway traffic is becoming increasingly relevant along with the expansions of cities, as building projects are forced towards more sensitive areas, e.g closer to railway tracks. With high enough vibration levels, there is a risk for human annoyance inside buildings. Knowledge of the attenuation of vibrations in the ground caused by railway traffic is in such cases of great importance. Where a railway switch is present, higher levels are likely to be generated as the train passes.</p><p>In this study, measurements of vibrations from railway traffic as trains pass a railway switch were performed. Four accelerometers were placed at different distances away from the track to capture the attenuation. In the analysis of the measurements, the attenuation of train induced vibrations and the dominating response due to the presence of the railway switch is found.</p><p>From the findings in the measurements, a model based on superpositioning of incoming waves from point sources is developed, that predicts the attenuation of vibrations over distance from a train passing over a railway switch.</p>

Note "was" to "were" and added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1653920 
abstract is: 
<p>This Master thesis report contains a complete attitude and orbit control analysis for solar sail spacecraft in the Circular-Restricted-3-Body-Reference frame. The overall goal of the project is to drive one solar sail from the edge of the Earth Sphere of Influence to a vicinity of the sub-Lagrange1 point. First, a trajectory control optimization has been performed using a direct transcription method to find an optimal control to guide the solar sail to a neighborhood of the sub-Lagrange 1 point. Then, the stability of the artificial equilibrium points in the Sun-Earth system has beeninvestigated. Two options are detailed in the following study: Halo orbits which correspond to periodic motion, and the implementation of a PD-controller on the solar sail normal vector to make any artificial equilibrium point asymptotically stable. An attitude control analysis study has been performed to estimate the needed torques for all the mission phases. This report is one of the first step for Space sunshade missions in the vicinity of the sub-Lagrange 1 point.</p>

corrected abstract:
<p>This Master thesis report contains a complete attitude and orbit control analysis for solar sail spacecraft in the Circular-Restricted-3-Body-Reference frame. The overall goal of the project is to drive one solar sail from the edge of the Earth Sphere of Influence to a vicinity of the sub-Lagrange 1 point. First, a trajectory control optimization has been performed using a direct transcription method to find an optimal control to guide the solar sail to a neighborhood of the sub-Lagrange 1 point. Then, the stability of the artificial equilibrium points in the Sun-Earth system has been investigated. Two options are detailed in the following study: Halo orbits which correspond to periodic motion, and the implementation of a PD-controller on the solar sail normal vector to make any artificial equilibrium point asymptotically stable. An attitude control analysis study has been performed to estimate the needed torques for all the mission phases. This report is one of the first step for Space sunshade missions in the vicinity of the sub-Lagrange 1 point.</p>

Note changes:
mc='beeninvestigated' c='been investigated'
"sub-Lagrange1" to "sub-Lagrange 1"
----------------------------------------------------------------------
In diva2:1227832   - correct as is
----------------------------------------------------------------------
In diva2:1816796 
abstract is: 
<p>This thesis deals with the scheduling problem for a constellation of Earth observation satellites, focusing on modelling the attitude dynamics to assess the tasking capabilities. A target selection algorithm is developed considering the time dependent manoeuvres between targets and the time-dependent value of the observed targets. Further, a closed-loop dynamics simulation is carried out to assess the agility of the 6U platform and verify the results of the algorithm. The work does not intend to present definitive numerical results, rather the goal is to develop a holistic framework that allows appraising the performance of a platform and the fulfilment of the mission objectives, aiming to maximise the collective value of the observed targets. Given the inputs in terms of platform, sensor, orbit and list of targets, this work serves to simulate the target selection and imaging at an arbitrary day and time for a chosen observation window.</p>

corrected abstract:
<p>This thesis deals with the scheduling problem for a constellation of Earth observation satellites, focusing on modelling the attitude dynamics to assess the tasking capabilities. A target selection algorithm is developed considering the time-dependent manoeuvres between targets and the time-dependent value of the observed targets. Further, a closed-loop dynamics simulation is carried out to assess the agility of the 6U platform and verify the results of the algorithm. The work does not intend to present definitive numerical results, rather the goal is to develop a holistic framework that allows appraising the performance of a platform and the fulfilment of the mission objectives, aiming to maximise the collective value of the observed targets. Given the inputs in terms of platform, sensor, orbit and list of targets, this work serves to simulate the target selection and imaging at an arbitrary day and time for a chosen observation window.</p>

Note added a hyphen in "time-dependent"
----------------------------------------------------------------------
In diva2:755070   - correct as is
----------------------------------------------------------------------
In diva2:753869   - correct as is
----------------------------------------------------------------------
In diva2:1215870 
abstract is: 
<p>In bridge design, a set of piles is referred to as a pile group. The design process of pile groups employed by many firms is currently manual, time consuming, and produces pile groups that are not robust against placement errors.</p><p>This thesis applies the metaheuristic method Genetic Algorithm to automate and improve the design of pile groups for bridge column foundations. A software is developed and improved by implementing modifications to the Genetic Algorithm. The algorithm is evaluated by the pile groups it produces, using the Monte Carlo method to simulate errors for the purpose of testing the robustness. The results are compared with designs provided by the consulting firm Tyrens AB.</p><p>The software is terminated manually, and generally takes less than half an hour to produce acceptable pile groups. The developed Genetic Algorithm Software produces pile groups that are more robust than the manually designed pile groups to which they are compared, using the Monte Carlo method. However, due to the visually disorganized designs, the pile groups produced by the algorithm may be di cult to get approved by Trafikverket. The software might require further modifications addressing this problem before it can be of practical use.</p>

corrected abstract:
<p>In bridge design, a set of piles is referred to as a pile group. The design process of pile groups employed by many firms is currently manual, time consuming, and produces pile groups that are not robust against placement errors.</p><p>This thesis applies the metaheuristic method Genetic Algorithm to automate and improve the design of pile groups for bridge column foundations. A software is developed and improved by implementing modifications to the Genetic Algorithm. The algorithm is evaluated by the pile groups it produces, using the Monte Carlo method to simulate errors for the purpose of testing the robustness. The results are compared with designs provided by the consulting firm Tyréns AB.</p><p>The software is terminated manually, and generally takes less than half an hour to produce acceptable pile groups. The developed Genetic Algorithm Software produces pile groups that are more robust than the manually designed pile groups to which they are compared, using the Monte Carlo method. However, due to the visually disorganized designs, the pile groups produced by the algorithm may be difficult to get approved by Trafikverket. The software might require further modifications addressing this problem before it can be of practical use.</p>

Note fixed the access in "Tyréns" and added the missing ligrature in "difficult"
----------------------------------------------------------------------
In diva2:1679036   - correct as is
----------------------------------------------------------------------
In diva2:1139765 
abstract is: 
<p>This paper is about automatizing parameter estimation of GARCH type conditional volatility models for the sake of using it in an automated risk monitoring system. Many challenges arise with this task such as guaranteeing convergence, being able to yield reasonable results regardless of the quality of the data, accuracy versus speed of the algorithm to name a few. These problems are investigated and a robust framework for an algorithm is proposed, containing dimension reducing and constraint relaxing parameter space transformations with robust initial values. The algorithm is implemented in java with two models, namely the GARCH and gjr-GARCH model. By using real market data, performance of the algorithm are tested with various in-sample and out-of-sample measures, including backtesting of the widely used risk measure Value-at-Risk. The empirical studies conclude that the more complex gjr-sGARCH model with the conditional student’s t distribution was found to yield the most accurate results. However for the purpose of this paper the GARCH orgjr-GARCH seems more appropriate.</p>

corrected abstract:
<p>This paper is about automatizing parameter estimation of GARCH type conditional volatility models for the sake of using it in an automated risk monitoring system. Many challenges arise with this task such as guaranteeing convergence, being able to yield reasonable results regardless of the quality of the data, accuracy versus speed of the algorithm to name a few. These problems are investigated and a robust framework for an algorithm is proposed, containing dimension reducing and constraint relaxing parameter space transformations with robust initial values. The algorithm is implemented in java with two models, namely the GARCH and gjr-GARCH model. By using real market data, performance of the algorithm are tested with various in-sample and out-of-sample measures, including backtesting of the widely used risk measure Value-at-Risk. The empirical studies conclude that the more complex gjr-sGARCH model with the conditional student’s t distribution was found to yield the most accurate results. However for the purpose of this paper the GARCH or gjr-GARCH seems more appropriate.</p>

Note spelling error:
mc='orgjr-GARCH' c='or gjr-GARCH'
----------------------------------------------------------------------
In diva2:1219103 
abstract is: 
<p> An important step towards making road transportation safer around the world is the development of autonomous vehicles. In this paper a controller for performing autonomous overtaking at highway speeds using Model Predictive Control (MPC) is designed. The MPC framework is designed and tested in a simulated environment in order to evaluate the performance of the controller. Four different MPC frameworks are developed for generating paths for autonomous overtaking and a Proportional Integral Derivative (PID) controller is formulated for a general comparison. The four MPC frameworks all plan trajectories by introducing constraints; they differ in the way they formulate said constraints. From the simulations we conclude that MPC is a better controller choice than PID for the application of controlling autonomous vehicles because of the usability of MPC, even though they might be equally fast. The benefits and drawbacks of the MPC implementations and their characteristics are discussed, and we conclude that the preferred implementation is a linear sloped edge dynamic constraint where a disallowed area around the leading vehicle is explicitly defined outside of the MPC framework.</p>

corrected abstract:
<p>An important step towards making road transportation safer around the world is the development of autonomous vehicles. In this paper a controller for performing autonomous overtaking at highway speeds using Model Predictive Control (MPC) is designed. The MPC framework is designed and tested in a simulated environment in order to evaluate the performance of the controller. Four different MPC frameworks are developed for generating paths for autonomous overtaking and a Proportional Integral Derivative (PID) controller is formulated for a general comparison. The four MPC frameworks all plan trajectories by introducing constraints; they differ in the way they formulate said constraints. From the simulations we conclude that MPC is a better controller choice than PID for the application of controlling autonomous vehicles because of the usability of MPC, even though they might be equally fast. The benefits and drawbacks of the MPC implementations and their characteristics are discussed, and we conclude that the preferred implementation is a linear sloped edge dynamic constraint where a disallowed area around the leading vehicle is explicitly defined outside of the MPC framework.</p>

Note simply eliminate the initial unnecessary space.
----------------------------------------------------------------------
In diva2:1441967 
abstract is: 
<p>The technical development has during the last few decades moved forward in a very high pace and modern technical solutions have become an even larger part of everyday life. Technical systems are becoming more and more advanced, and society is more influenced by autonomous solutions than before. The vehicle industry is no exception. One of the most important goals in development of autonomous vehicles is to reduce human errors, because most accidents are caused by humans. The objective in this thesis is to survey how far the development of autonomous vehicles have come and also in which areas this is happening. Based on a literature study of previous research on the subject, a judgement of future development in autonomous vehicles will be performed. The areas which are analyzed in this thesis are private cars, public transports, as well as warehouse and logistics. This thesis is first and foremost focused on how the situation in Sweden look. There are three major aspects of the development of autonomous vehicles that are analyzed namely, technical, legal and ethical. The analysis for future development is based on a SWOT-analysisto map out the different strengths, weaknesses, opportunities, and threats to this technology. The three aspects mentioned above constitutes the SWOT-analysis. This thesis shows that the development of autonomous vehicles is moving forward in all of the areas analyzed, where warehouse and logistics has the most developed vehicles. Public transport is also on its way to get implemented in real traffic, while private cars have a long way to go before commercial usage.</p>
mc='analysisto' c='analysis to'
mc='SWOT-analysisto' c='SWOT-analysis to'

partal corrected: diva2:1441967: <p>The technical development has during the last few decades moved forward in a very high pace and modern technical solutions have become an even larger part of everyday life. Technical systems are becoming more and more advanced, and society is more influenced by autonomous solutions than before. The vehicle industry is no exception. One of the most important goals in development of autonomous vehicles is to reduce human errors, because most accidents are caused by humans. The objective in this thesis is to survey how far the development of autonomous vehicles have come and also in which areas this is happening. Based on a literature study of previous research on the subject, a judgement of future development in autonomous vehicles will be performed. The areas which are analyzed in this thesis are private cars, public transports, as well as warehouse and logistics. This thesis is first and foremost focused on how the situation in Sweden look. There are three major aspects of the development of autonomous vehicles that are analyzed namely, technical, legal and ethical. The analysis for future development is based on a SWOT-analysis to map out the different strengths, weaknesses, opportunities, and threats to this technology. The three aspects mentioned above constitutes the SWOT-analysis. This thesis shows that the development of autonomous vehicles is moving forward in all of the areas analyzed, where warehouse and logistics has the most developed vehicles. Public transport is also on its way to get implemented in real traffic, while private cars have a long way to go before commercial usage.</p>

corrected abstract:
<p>The technical development has during the last few decades moved forward in a very high pace and modern technical solutions have become an even larger part of everyday life. Technical systems are becoming more and more advanced, and society is more influenced by autonomous solutions than before. The vehicle industry is no exception. One of the most important goals in development of autonomous vehicles is to reduce human errors, because most accidents are caused by humans.</p><p>The objective in this thesis is to survey how far the development of autonomous vehicles have come and also in which areas this is happening. Based on a literature study of previous research on the subject, a judgement of future development in autonomous vehicles will be performed. The areas which are analyzed in this thesis are private cars, public transports, as well as warehouse and logistics. This thesis is first and foremost focused on how the situation in Sweden look. There are three major aspects of the development of autonomous vehicles that are analyzed namely, technical, legal and ethical.</p><p>The analysis for future development is based on a SWOT-analysis to map out the different strengths, weaknesses, opportunities, and threats to this technology. The three aspects mentioned above constitutes the SWOT-analysis.</p><p>This thesis shows that the development of autonomous vehicles is moving forward in all of the areas analyzed, where warehouse and logistics has the most developed vehicles. Public transport is also on its way to get implemented in real traffic, while private cars have a long way to go before commercial usage.</p>

Note added the missigng paragraph breaks.
----------------------------------------------------------------------
In diva2:1583473 
abstract is: 
<p>Many satellites have an orbit of reference defined according to their mission. The satellites need therefore to navigate as close as possible to their reference orbit. However, due to external forces, the trajectory of a satellite is disturbed and actions need to be taken. For now, the trajectories of the satellites are monitored by the operations of satellites department which gives appropriate instructions of navigation to the satellites. These steps require a certain amount of time and involvement which could be used for other purposes. A solution could be to make the satellites autonomous. The satellites would take their own decisions depending on their trajectory. The navigation control would be therefore much more efficient, precise and quicker. Besides, the autonomous orbit control could be coupled with an avoidance collision risk management. The satellites would decide themselves if an avoidance maneuver needs to be considered. The alerts of collisions would be given by the ground segment. In order to advance in this progress, this internship enables to analyse the feasibility of the implementation of the two concepts by testing them on an experiments satellite. To do so, tests plans were defined, tests procedures were executed and post-treatment tools were developed for analysing the results of the tests. Critical computational cases were considered as well. The tests were executed in real operations conditions.</p>

corrected abstract:
<p>Many satellites have an orbit of reference defined according to their mission. The satellites need therefore to navigate as close as possible to their reference orbit. However, due to external forces, the trajectory of a satellite is disturbed and actions need to be taken. For now, the trajectories of the satellites are monitored by the operations of satellites department which gives appropriate instructions of navigation to the satellites. These steps require a certain amount of time and involvement which could be used for other purposes.</p><p>A solution could be to make the satellites autonomous. The satellites would take their own decisions depending on their trajectory. The navigation control would be therefore much more efficient, precise and quicker. Besides, the autonomous orbit control could be coupled with an avoidance collision risk management. The satellites would decide themselves if an avoidance maneuver needs to be considered. The alerts of collisions would be given by the ground segment. In order to advance in this progress, this internship enables to analyse the feasibility of the implementation of the two concepts by testing them on an experiments satellite.</p><p>To do so, tests plans were defined, tests procedures were executed and post-treatment tools were developed for analysing the results of the tests. Critical computational cases were considered as well. The tests were executed in real operations conditions.</p>
----------------------------------------------------------------------
In diva2:1442071 
abstract is: 
<p>We review recent research into trajectory planning for autonomous overtaking to understand existing challenges. Then, the recently developed framework Learning Model Predictive Control (LMPC) is presented as a suitable method to iteratively improve an overtaking manoeuvre each time it is performed. We present recent extensions to the LMPC framework to make it applicable to overtaking. Furthermore, we also present two alternative modelling approaches with the intention of reducing computational complexity of the optimization problems solved by the controller. All proposed frameworks are built from scratch in Python3 and simulated for evaluation purposes. Optimization problems are modelled and solved using the Gurobi 9.0 Python API gurobipy. The results show that LMPC can be successfully applied to the overtaking problem, with improved performance at each iteration. However, the first proposed alternative modelling approach does not improve computational times as was the intention. The second one does but fails in other areas.</p>

corrected abstract:
<p>We review recent research into trajectory planning for autonomous overtaking to understand existing challenges. Then, the recently developed framework <em>Learning Model Predictive Control</em> (LMPC) from [1] is presented as a suitable method to iteratively improve an overtaking manoeuvre each time it is performed. We present extensions to the LMPC framework from [2] and [3] to make it applicable to overtaking. Furthermore, we also present two alternative modelling approaches with the intention of reducing computational complexity of the optimization problems solved by the controller. All proposed frameworks are built from scratch in <span style="font-variant: small-caps;">Python3</span> and simulated for evaluation purposes. Optimization problems are modelled and solved using the <span style="font-variant: small-caps;">Gurobi 9.0</span> Python API <span style="font-variant: small-caps;">gurobipy</span>. The results show that LMPC can be successfully applied to the overtaking problem, with improved performance at each iteration. However, the first proposed alternative modelling approach does not improve computational times as was the intention. The second one does but fails in other areas.</p>

Note change in wording and added italics and small caps
----------------------------------------------------------------------
In diva2:1056983 
abstract is: 
<p>How to measure risk is an important question in finance and much work has been done on how to quantitatively measure risk. An important part of this measurement is evaluating the measurements against the outcomes a procedure known as backtesting. A common risk measure is Expected shortfall for which how to backtest has been debated. In this thesis we will compare four different proposed backtests and see how they perform in a realistic setting. The main finding in this thesis is that it is possible to find backtests that perform well but it is important to investigate them thoroughly as small errors in the model can lead to large errors in the outcome of the</p><p>backtest</p>

corrected abstract:
<p>How to measure risk is an important question in finance and much work has been done on how to quantitatively measure risk. An important part of this measurement is evaluating the measurements against the outcomes a procedure known as backtesting. A common risk measure is Expected shortfall for which how to backtest has been debated. In this thesis we will compare four different proposed backtests and see how they perform in a realistic setting. The main finding in this thesis is that it is possible to find backtests that perform well but it is important to investigate them thoroughly as small errors in the model can lead to large errors in the outcome of the >backtest</p>

Note remove the unnecessary paragraph break
----------------------------------------------------------------------
In diva2:848996   - correct as is
----------------------------------------------------------------------
In diva2:449167   - correct as is
----------------------------------------------------------------------
In diva2:1162961 
abstract is: 
<p>This report has been written during my internship/master thesis at Thales Alenia Space, Cannes, FRANCE. The subject of the thesis is ball bearing design, and is focused on the software RBSDyn. This software has been developed by CNES, the French Center for Space Studies, and is used to simulate bearings behaviors under various conditions. My mission was to verify, test and implement this software for the company. In order to do so, the first step was to understand the bearing theory, which is the first part of this report. The second step was to use the software and verify its results, which is presented in the second section. Eventually, the final goal of this internship was to create a sequence to help Thales</p><p>Alenia Space engineers to design and select bearings, using this software and an Excel tool that needed to be created. Note that for confidentiality reasons, the values and names used for internal TAS mechanisms have been removed of this document.</p>

corrected abstract:
<p>This report has been written during my internship/master thesis at Thales Alenia Space, Cannes, FRANCE. The subject of the thesis is ball bearing design, and is focused on the software RBSDyn. This software has been developed by CNES, the French Center for Space Studies, and is used to simulate bearings behaviors under various conditions. My mission was to verify, test and implement this software for the company. In order to do so, the first step was to understand the bearing theory, which is the first part of this report. The second step was to use the software and verify its results, which is presented in the second section. Eventually, the final goal of this internship was to create a sequence to help Thales Alenia Space engineers to design and select bearings, using this software and an Excel tool that needed to be created.</p><p>Note that for confidentiality reasons, the values and names used for internal TAS mechanisms have been removed of this document.</p>

Note removed unnecessary paragraph break
----------------------------------------------------------------------
In diva2:1219078   - correct as is
----------------------------------------------------------------------
In diva2:1319947   - correct as is
----------------------------------------------------------------------
In diva2:818932 
abstract is: 
<p>This report presents a novel algorithm for hierarchical clustering called <em>Bayesian Sample Clustering </em>(BSC). BSC is a <em>single linkage </em>algorithm that uses data samples to produce a <em>predictive distribution </em>for each sample. The predictive distributions are compared using the <em>Chan-Darwiche distance</em>, a metric for finite probability distributions, to produce a hierarchy of samples. The implemented version of BSC is found at <em>https:</em><em>//</em><em>github.com</em><em>/</em><em>Skjulet</em><em>/</em><em>Bayesian Sample Clustering.</em></p><p><em> </em></p>

corrected abstract:
<p>This report presents a novel algorithm for hierarchical clustering called <em>Bayesian Sample Clustering</em> (BSC). BSC is a <em>single linkage</em> algorithm that uses data samples to produce a <em>predictive distribution</em> for each sample. The predictive distributions are compared using the <em>Chan-Darwiche distance</em>, a metric for finite probability distributions, to produce a hierarchy of samples. The implemented version of BSC is found at<br> <a href="https://github.com/Skjulet/Bayesian Sample Clustering">https://github.com/Skjulet/Bayesian Sample Clustering</a>.</p>

Note lots of changes
----------------------------------------------------------------------
In diva2:820529 
abstract is: 
<p>Under the Advanced Measurement Approach (AMA), banks must use four different sources of information to assess their operational risk capital requirement. The three main quantitative sources available to build the future loss distribution are internal loss data, external loss data and scenario analysis. The fourth source, business environment and internal control factors, is treated as an ex-post update to capital calculations and is not a subject of this thesis. Ap- proaches from Extreme Value Theory (EVT) have gained popularity in the area of operational risk in recent years, with its focus on the behaviour of processes at extreme levels making it a natural candidate for operational risk modelling. However, the adoption of EVT in operational risk modelling has encountered several obstacles with the main one being the scarcity of data leading to substantial statistical uncertainty for both parameter and capital estimates. This Master thesis evaluates Bayesian Inference approaches to extreme value estimation and implements a method to reduce these uncertainties. The results indicate that the Bayesian Inference approaches gives a significant reduction of the statistical uncertainties compared to more traditional estimators and also performs well when applied on real-world data sets. </p>

corrected abstract:
<p>Under the Advanced Measurement Approach (AMA), banks must use four different sources of information to assess their operational risk capital requirement. The three main quantitative sources available to build the future loss distribution are internal loss data, external loss data and scenario analysis. The fourth source, business environment and internal control factors, is treated as an ex-post update to capital calculations and is not a subject of this thesis. Approaches from Extreme Value Theory (EVT) have gained popularity in the area of operational risk in recent years, with its focus on the behaviour of processes at extreme levels making it a natural candidate for operational risk modelling. However, the adoption of EVT in operational risk modelling has encountered several obstacles with the main one being the scarcity of data leading to substantial statistical uncertainty for both parameter and capital estimates. This Master thesis evaluates Bayesian Inference approaches to extreme value estimation and implements a method to reduce these uncertainties. The results indicate that the Bayesian Inference approaches gives a significant reduction of the statistical uncertainties compared to more traditional estimators and also performs well when applied on real-world data sets.</p>

Note remove unnecessary hyphen
----------------------------------------------------------------------
In diva2:1320142 
abstract is: 
<p>Neural networks are powerful tools for modelling complex non-linear mappings, but they often suffer from overfitting and provide no measures of uncertainty in their predictions. Bayesian techniques are proposed as a remedy to these problems, as these both regularize and provide an inherent measure of uncertainty from their posterior predictive distributions. By quantifying predictive uncertainty, we attempt to improve a systematic trading strategy by scaling positions with uncertainty. Exact Bayesian inference is often impossible, and approximate techniques must be used. For this task, this thesis compares dropout, variational inference and Markov chain Monte Carlo. We find that dropout and variational inference provide powerful regularization techniques, but their predictive uncertainties cannot improve a systematic trading strategy. Markov chain Monte Carlo provides powerful regularization as well as promising estimates of predictive uncertainty that are able to improve a systematic trading strategy. However, Markov chain Monte Carlo suffers from an extreme computational cost in the high-dimensional setting of neural networks.</p>

corrected abstract:
<p>Neural networks are powerful tools for modelling complex non-linear mappings, but they often suffer from overfitting and provide no measures of uncertainty in their predictions. Bayesian techniques are proposed as a remedy to these problems, as these both regularize and provide an inherent measure of uncertainty from their posterior predictive distributions.</p><p>By quantifying predictive uncertainty, we attempt to improve a systematic trading strategy by scaling positions with uncertainty. Exact Bayesian inference is often impossible, and approximate techniques must be used. For this task, this thesis compares dropout, variational inference and Markov chain Monte Carlo. We find that dropout and variational inference provide powerful regularization techniques, but their predictive uncertainties cannot improve a systematic trading strategy. Markov chain Monte Carlo provides powerful regularization as well as promising estimates of predictive uncertainty that are able to improve a systematic trading strategy. However, Markov chain Monte Carlo suffers from an extreme computational cost in the high-dimensional setting of neural networks.</p>

Note add missing paragraph
----------------------------------------------------------------------
In diva2:1752040   - correct as is
----------------------------------------------------------------------
In diva2:1263411 
abstract is: 
<p>The lighter an object is the easier it is to send into space. This principle is what drives the never ending hunt for lighter structures in the space industry. One way to reduce weight is to replace existing materials with lighter ones. Polymer matrix composites are such materials, as their density is lower than both steel and aluminium. The company RUAG Space produces a payload separating system that operates by clamping the payload using a clamp band to the rocket and releasing the payload by releasing the tension in the band. The current band is made in aluminium but RUAG seeks to build them using carbon fiber reinforced epoxy instead. Earlier projects have shown that carbon fiber fulfills the basic requirements, but has insufficient bearing strength to handle the loads at the bolted joints to the release mechanism. Research suggests that making the individual layers of carbon fiber thinner will increase the bearing strength and so in this project test specimen have been manufactured using thick and thin carbon fiber layers. These specimen were then subjected to bearing loads and the response was observed. The result showed that the ultimate bearing strength only increased a small amount with thin plies, but the onset of damage came at 47% higher stress levels compared to thick plies, suggesting a more brittle behavior. Since the onset of damage is the most important factor for RUAG the use of thin plies produced very positive results and could be a viable solution to increase the bearing strength in the clamp band.</p>

corrected abstract:
<p>The lighter an object is the easier it is to send into space. This principle is what drives the never ending hunt for lighter structures in the space industry. One way to reduce weight is to replace existing materials with lighter ones. Polymer matrix composites are such materials, as their density is lower than both steel and aluminium. The company RUAG Space produces a payload separating system that operates by clamping the payload using a clamp band to the rocket and releasing the payload by releasing the tension in the band. The current band is made in aluminium but RUAG seeks to build them using carbon fiber reinforced epoxy instead. Earlier projects have shown that carbon fiber fulfills the basic requirements, but has insufficient bearing strength to handle the loads at the bolted joints to the release mechanism. Research suggests that making the individual layers of carbon fiber thinner will increase the bearing strength and so in this project test specimen have been manufactured using thick and thin carbon fiber layers. These specimen were then subjected to bearing loads and the response was observed. The result showed that the ultimate bearing strength only increased a small amount with thin plies, but the onset of damage came at 47% higher stress levels compared to thick plies, suggesting a more brittle behavior. Since the onset of damage is the most important factor for RUAG the use of thin plies produced very positive results and could be a viable solution to increase the bearing strength in the clamp band.</p>
----------------------------------------------------------------------
In diva2:1115832 
abstract is: 
<p>In this thesis various portfolio weighting strategies are tested. Their performance is determined by their average annual return, Sharpe ratio, tracking error, information ratio and annual standard deviation. The data used is provided by Öhman from Bloomberg and consists of monthly data between 1996-2016 of all stocks that were in the MSCI USA Index at any time between 2002-2016.For any given month we use the last five years of data as a basis for the analysis. Each time the MSCI USA Index changes portfolio constituents we update which constituents are in our portfolio.</p><p>The traditional weighting strategies used in this thesis are market capitalization, equal, risk-adjusted alpha, fundamental and minimum variance weighting. On top of that, the weighting strategies are used in a cluster framework where the clusters are constructed by using K-means clustering on the stocks each month. The clusters are assigned equal weight and then the traditional weighting strategies are applied within each cluster. Additionally, a GARCH-estimated covariance matrix of the clusters is used to determine the minimum variance optimized weights of the clusters where the constituents within each cluster are equally weighted.</p><p>We conclude in this thesis that the market capitalization weighting strategy is the one that earns the least of all traditional strategies. From the results we can conclude that there are weighting strategies with higher Sharpe ratio and lower standard deviation. The risk-adjusted alpha in a traditional framework performed best out of all strategies. All cluster weighting strategies with the exception of risk-adjusted alpha outperform their traditional counterpart in terms of return.</p>

corrected abstract:
<p>In this thesis various portfolio weighting strategies are tested. Their performance is determined by their average annual return, Sharpe ratio, tracking error, information ratio and annual standard deviation. The data used is provided by Öhman from Bloomberg and consists of monthly data between 1996-2016 of all stocks that were in the MSCI USA Index at any time between 2002-2016. For any given month we use the last five years of data as a basis for the analysis. Each time the MSCI USA Index changes portfolio constituents we update which constituents are in our portfolio.</p><p>The traditional weighting strategies used in this thesis are market capitalization, equal, risk-adjusted alpha, fundamental and minimum variance weighting. On top of that, the weighting strategies are used in a cluster framework where the clusters are constructed by using 𝐾-means clustering on the stocks each month. The clusters are assigned equal weight and then the traditional weighting strategies are applied within each cluster. Additionally, a GARCH-estimated covariance matrix of the clusters is used to determine the minimum variance optimized weights of the clusters where the constituents within each cluster are equally weighted.</p><p>We conclude in this thesis that the market capitalization weighting strategy is the one that earns the least of all traditional strategies. From the results we can conclude that there are weighting strategies with higher Sharpe ratio and lower standard deviation. The risk-adjusted alpha in a traditional framework performed best out of all strategies. All cluster weighting strategies with the exception of risk-adjusted alpha outperform their traditional counterpart in terms of return.</p>

Note replaced "K" with "𝐾"
----------------------------------------------------------------------
In diva2:1381279 
abstract is: 
<p>High-speed planing craft is designed to overcome conventional hull’s speed barrier associated with wave making resistance and high frictional forces. Despite being able to reach high speeds, some planing hull forms will develop large volumes of spray attached to the hull surface, which can account for a large proportion of the total resistance. In this study, an experimental evaluation of the novel spray deflector technology proposed by Petestep AB is carried out in model scale at the Davidson Laboratory towing tank. The spray deflectors are compared against a time-proven spray rails technology and bare hull configuration. A modular hull design was developed that allows for rapid conversion between the three hull configurations and for future modifications to the design. The calm water resistance tests have shown up to 9% resistance reduction for spray rails and up to 25.75% reduction for spray deflectors as compared to the bare hull configuration. The running position of the spray deflector configuration was affected by the selected deflector design and differed from the spray rail and bare hull configuration, making the direct comparison of the technologies inapplicable. The Irregular waves tests have shown that for the current deflector design, the significant accelerations are approximately the same for the spray rail and spray deflector configurations. Both the technologies have led to increased accelerations at the center of gravity as compared to the bare hull. The spray deflector configuration, however, experienced lower accelerations in the bow area. A number of improvements to the current model design were proposed for the next series of experiments.</p>

corrected abstract:
<p>High-speed planing craft is designed to overcome conventional hull’s speed barrier associated with wave making resistance and high frictional forces. Despite being able to reach high speeds, some planing hull forms will develop large volumes of spray attached to the hull surface, which can account for a large proportion of the total resistance. In this study, an experimental evaluation of the novel spray deflector technology proposed by Petestep AB is carried out in model scale at the Davidson Laboratory towing tank. The spray deflectors are compared against a time-proven spray rails technology and bare hull configuration. A modular hull design was developed that allows for rapid conversion between the three hull configurations and for future modifications to the design.</p><p>The calm water resistance tests have shown up to 9% resistance reduction for spray rails and up to 25.75% reduction for spray deflectors as compared to the bare hull configuration. The running position of the spray deflector configuration was affected by the selected deflector design and differed from the spray rail and bare hull configuration, making the direct comparison of the technologies inapplicable. The Irregular waves tests have shown that for the current deflector design, the significant accelerations are approximately the same for the spray rail and spray deflector configurations. Both the technologies have led to increased accelerations at the center of gravity as compared to the bare hull. The spray deflector configuration, however, experienced lower accelerations in the bow area. A number of improvements to the current model design were proposed for the next series of experiments.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1380187 
abstract is: 
<p>This report presents the bending modes study conducted on a heavy launcher. The controller of the launcher takes as inputs the attitude and attitude rate measurements given by the Inertial Measurement Unit (IMU). Since the bending modes generate measurement errors at the IMU location, the study of deformations due to these bending modes is critical to assess the stability of the launcher during the atmospheric flight phase. The goal of this master thesis project is to detect and then select the most excitable bending modes among the large number of modes provided by a detailed structural analysis of the launcher. Only these relevant modes will be later used to generate reduced dynamical models of the launcher in order to efficiently design an appropriate controller. Indeed, considering all the bending modes will dramatically increase the calculation time and will not significantly improve the representativeness of the model at the control law frequency range of interest. To reach this objective, an extended excitability (the maximum of the module of the transfer function between the effective deflection and the considered mode generalized coordinate transported at the IMU location) is defined and computed for each mode. A criterion has been implemented to choose only the relevant modes. The sensitivity study conducted during this master thesis project has shown that with around 20 modes over 200, one can reproduce the dynamic behavior of the complete system. </p>

corrected abstract:
<p>This report presents the bending modes study conducted on a heavy launcher. The controller of the launcher takes as inputs the attitude and attitude rate measurements given by the Inertial Measurement Unit (IMU). Since the bending modes generate measurement errors at the IMU location, the study of deformations due to these bending modes is critical to assess the stability of the launcher during the atmospheric flight phase. The goal of this master thesis project is to detect and then select the most excitable bending modes among the large number of modes provided by a detailed structural analysis of the launcher. Only these relevant modes will be later used to generate reduced dynamical models of the launcher in order to efficiently design an appropriate controller. Indeed, considering all the bending modes will dramatically increase the calculation time and will not significantly improve the representativeness of the model at the control law frequency range of interest. To reach this objective, an extended excitability (the maximum of the module of the transfer function between the effective deflection and the considered mode generalized coordinate transported at the IMU location) is defined and computed for each mode. A criterion has been implemented to choose only the relevant modes. The sensitivity study conducted during this master thesis project has shown that with around 20 modes over 200, one can reproduce the dynamic behavior of the complete system.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1701325 
abstract is: 
<p>Benford’s Law describes a profound behavior that the leading digits of many quantities arising from mathematics, physics, ﬁnance, and engineering exhibit. In this text we prove Benford’s Law for the absolute value of the characteristic polynomial det (U-λI) of the CUE(N) as N →∞.  Our analysis produces an integrable bound for the characteristic function of log | det (U - λI|.</p>

corrected abstract:
<p>Benford’s Law describes a profound behavior that the leading digits of many quantities arising from mathematics, physics, finance, and engineering exhibit. In this text we prove Benford’s Law for the absolute value of the characteristic polynomial det(𝑈-&#x1D706;𝐼) of the CUE(𝑁) as 𝑁 → ∞. Our analysis produces an integrable bound for the characteristic function of log|det(𝑈 - &#x1D706;𝐼)|.</p>

Note set equations using math symbols and added missing right parenthesis to last equation
----------------------------------------------------------------------
In diva2:813360 - original does not have an abstract in English
abstract is: 
<p>In a world with increasing energy consumption, and where a large part of the energy is based on non-environmental sources, renewable energy is a solution that could help to satisfy all needs. One kind of renewable energy is solar energy, but to obtain economic viability for solar plants in countries such as Sweden can be hard. This project is part of a larger project aiming to perform a cost-effectiveness analysis of an imaginary solar plant in Stockholm. The goal of this sub-project is to investigate how much electricity a solar plant at the Royal Tennis Hall in Stockholm can produce assuming that the plant is 100 % reliable. The sub-project also compares the potential revenue if the electricity is sold on the Nord Pool Spot with the potential savings from not having to buy the produced amount from electric companies. In the economic analysis, possibilities for hedging, uncertainties in future electricity market and various grants are studied. In order to estimate production and profit a literature study has been conducted. An overall cost benefit analysis has been performed together with the other sub-projects. The annual production of a plant with an installed capacity of 46.64 kW has been estimated to 68.6 MWh. If all the electricity produced is consumed instead of being sold, the electricity costs would be reduced by 57,150 SEK annually.</p>

corrected abstract:
<p>In a world with increasing energy consumption, and where a large part of the energy is based on non-environmental sources, renewable energy is a solution that could help to satisfy all needs. One kind of renewable energy is solar energy, but to obtain economic viability for solar plants in countries such as Sweden can be hard. This project is part of a larger project aiming to perform a cost-effectiveness analysis of an imaginary solar plant in Stockholm. The goal of this sub-project is to investigate how much electricity a solar plant at the Royal Tennis Hall in Stockholm can produce assuming that the plant is 100 % reliable. The sub-project also compares the potential revenue if the electricity is sold on the Nord Pool Spot with the potential savings from not having to buy the produced amount from electric companies. In the economic analysis, possibilities for hedging, uncertainties in future electricity market and various grants are studied. In order to estimate production and profit a literature study has been conducted. An overall cost benefit analysis has been performed together with the other sub-projects. The annual production of a plant with an installed capacity of 46.64 kW has been estimated to 68.6 MWh. If all the electricity produced is consumed instead of being sold, the electricity costs would be reduced by 57,150 SEK annually.</p>

Note that the English does not exactly match  translation of the Sweden:
----------------------------------------------------------------------
In diva2:612289 
abstract is: 
<p>This study treats the design of secondary structures for wheel-loaded decks. It concludes that significant savings in structural weight, overall cost and environmental impact can be obtained by an improved design. The rules of three classification societies are examined and their principle differences are discussed. Weight and cost optimal solutions of rule-based design are identified for a deck of a typical short-sea RoRo-vessel. The rule-optimal designs are assessed and further improved on the basis of FEcalculations and the economic and environmental benefits associated with the best solutions are approximated.</p>
mc='FEcalculations' c='FE calculations'

partal corrected: diva2:612289: <p>This study treats the design of secondary structures for wheel-loaded decks. It concludes that significant savings in structural weight, overall cost and environmental impact can be obtained by an improved design. The rules of three classification societies are examined and their principle differences are discussed. Weight and cost optimal solutions of rule-based design are identified for a deck of a typical short-sea RoRo-vessel. The rule-optimal designs are assessed and further improved on the basis of FE calculations and the economic and environmental benefits associated with the best solutions are approximated.</p>

corrected abstract:
<p>This study treats the design of secondary structures for wheel-loaded decks. It concludes that significant savings in structural weight, overall cost and environmental impact can be obtained by an improved design. The rules of three classification societies are examined and their principle differences are discussed. Weight and cost optimal solutions of rule-based design are identified for a deck of a typical short-sea RoRo-vessel. The rule-optimal designs are assessed and further improved on the basis of FE-calculations and the economic and environmental benefits associated with the best solutions are approximated.</p>

Note added hyphen to "FE-calculations"
----------------------------------------------------------------------
In diva2:1319939   - correct as is
----------------------------------------------------------------------
In diva2:813076 -- no English abstract in the full text
----------------------------------------------------------------------
In diva2:1374912   - correct as is
----------------------------------------------------------------------
In diva2:1057190 
abstract is: 
<p>Airplanes have a risk of encounter birds while flying, taking off or landing and to ensure a safe flight the engines have to sustain functionality after one or several bird strikes; one vital part in the engine is the first stage rotor and hence it has to withstand bird strikes. Commercial finite element codes with explicit time marching techniques are commonly used today but they can be time-consuming and therefore a faster analytical method is sought. An analytical method is suitable for comparison of rotor blades in an early design process.</p><p>The force during the bird strike was divided into two parts in accordance with a paper by Sinha et al. [1], a slicing force and a travelling force. Once the force was obtained it was transformed to a pressure over an area. The pressure and area was then used to perform a transient analysis in ANSYS. The analytical based results are then compared to results obtained in a simulation done in LS-DYNA. The force exerted on a rotor blade, analytical and in LS-DYNA, shows good agreement with respect to maximum force. Comparing the displacement of the rotor blade in ANSYS and LS-DYNA shows that the magnitude agrees well but the phase does not agree as well.</p><p>It is possible to make automated scripts with a minimum of input data required to perform the analysis. The slicing force shows a good estimate of the maximum force exerted on a single blade during a bird strike. The impulse of the slicing force can also be seen as a lower limit of the total impulse felt by a single blade. The travelling force based on the paper by Sinha et al. was found to have some insufficiency and the recommendation is therefore to exclude the travelling force, in current state, from the analysis.</p>

corrected abstract:
<p>Airplanes have a risk of encounter birds while flying, taking off or landing and to ensure a safe flight the engines have to sustain functionality after one or several bird strikes; one vital part in the engine is the first stage rotor and hence it has to withstand bird strikes. Commercial finite element codes with explicit time marching techniques are commonly used today but they can be time-consuming and therefore a faster analytical method is sought. An analytical method is suitable for comparison of rotor blades in an early design process.</p><p>The force during the bird strike was divided into two parts in accordance with a paper by Sinha et al. [1], a slicing force and a travelling force. Once the force was obtained it was transformed to a pressure over an area. The pressure and area was then used to perform a transient analysis in ANSYS. The analytical based results are then compared to results obtained in a simulation done in LS-DYNA.</p><p>The force exerted on a rotor blade, analytical and in LS-DYNA, shows good agreement with respect to maximum force. Comparing the displacement of the rotor blade in ANSYS and LS-DYNA shows that the magnitude agrees well but the phase does not agree as well.</p><p>It is possible to make automated scripts with a minimum of input data required to perform the analysis. The slicing force shows a good estimate of the maximum force exerted on a single blade during a bird strike. The impulse of the slicing force can also be seen as a lower limit of the total impulse felt by a single blade. The travelling force based on the paper by Sinha et al. was found to have some insufficiency and the recommendation is therefore to exclude the travelling force, in current state, from the analysis.</p>

Note added missing paragraph break
----------------------------------------------------------------------
In diva2:1777333   - correct as is
----------------------------------------------------------------------
In diva2:558015 
abstract is: 
<p>We study blow-ups and their relation to orders of vanishing in algebraic geometry. In particular, we study the exceptional divisor and the strict transform of a blow-up. We use the order of vanishing to measure the severity of singularities, and show that if we blow up a closed point on a hypersurface we obtain points of equal order above it.</p>

corrected abstract:
<p>We study blow-ups and their relation to orders of vanishing in algebraic geometry. In particular, we study the exceptional divisor and the strict transform of a blow-up. We use the order of vanishing to measure the severity of singularities, and show that if we blow up a closed point on a hypersurface we obtain points of equal or lower order above it.</p>

Note added missing words "or lower"
----------------------------------------------------------------------
In diva2:1630882   - correct as is
----------------------------------------------------------------------
In diva2:1680272   - correct as is
----------------------------------------------------------------------
In diva2:1776722   - correct as is
----------------------------------------------------------------------
In diva2:1145353 
abstract is: 
<p>Using a vessel for public transport can possibly save large amounts of time in a city as Stockholm. The transport is easy during the period of the year where there is no ice cover on the waters, however during the time when there is ice, the vessels used face more extreme conditions. Swedish Steel Yachts (SSY) now wants to have a design for their “Shuttle Ferry Concept” intended for operation all year round. SSY has developed a special way of designing a ship’s hull structure, using this design together with the super duplex stainless steel alloy, SAF2507, SSY hopes to revolutionize the ship building industry. The aim of this thesis is to deliver a bow design that is able to combine operation in brash ice with good performance in open water using the special SSY design together with the super duplex stainless steel.</p><p>This thesis presents to you basic knowledge regarding operation in ice, ice theory, the SSY design concept more in detail and finally a design development of a suitable structure. The results from the thesis are shown, mainly as preferred outer geometry and expected load cases from the ice in the operational area.</p>

corrected abstract:
<p>Using a vessel for public transport can possibly save large amounts of time in a city as Stockholm. The transport is easy during the period of the year where there is no ice cover on the waters, however during the time when there is ice, the vessels used face more extreme conditions. Swedish Steel Yachts (SSY) now wants to have a design for their “Shuttle Ferry Concept” intended for operation all year round.</p><p>SSY has developed a special way of designing a ship’s hull structure, using this design together with the super duplex stainless steel alloy, SAF2507, SSY hopes to revolutionize the ship building industry. The aim of this thesis is to deliver a bow design that is able to combine operation in brash ice with good performance in open water using the special SSY design together with the super duplex stainless steel.</p><p>This thesis presents to you basic knowledge regarding operation in ice, ice theory, the SSY design concept more in detail and finally a design development of a suitable structure.</p><p>The results from the thesis are shown, mainly as preferred outer geometry and expected load cases from the ice in the operational area.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1229270 
abstract is: 
<p>When leaving classical physics and entering the realm of quantum physics, there are many new concepts being introduced. One of the most fundamental ideas in quantum mechanics is that particles no longer have exact known positions, but instead expected values and prob- abilities. This leads to the phenomena of truly identical particles, since they no longer can be distinguished simply by their positions. An important property differentiating different kinds of particles is how a system behaves when two such identical particles are exchanged. Historically, this divided particles into bosons and fermions, corresponding to symmetry and antisymmetry under an exchange.</p><p>However, in two dimensions a new type of particle appears. These particles are called anyons, and behave differently when particles are exchanged. Anyons can be further divided into abelian and non-abelian anyons, of which this thesis will focus on the latter. The ex- changes can then be represented by the fundamental group of the configuration space of the particles, and in two dimensions this fundamental group is the braid group. Using rotors from a Clifford algebra and studying excitations of Majorana fermions, this thesis will show a way to calculate the exchange matrices of non-abelian anyons, and their corresponding eigenvalues. Furthermore, suggestions on a generalization of this framework along with areas where it can be applied are given.</p>

corrected abstract:
<p>When leaving classical physics and entering the realm of quantum physics, there are many new concepts being introduced. One of the most fundamental ideas in quantum mechanics is that particles no longer have exact known positions, but instead expected values and probabilities. This leads to the phenomena of truly identical particles, since they no longer can be distinguished simply by their positions. An important property differentiating different kinds of particles is how a system behaves when two such identical particles are exchanged. Historically, this divided particles into bosons and fermions, corresponding to symmetry and antisymmetry under an exchange.</p><p>However, in two dimensions a new type of particle appears. These particles are called anyons, and behave differently when particles are exchanged. Anyons can be further divided into abelian and non-abelian anyons, of which this thesis will focus on the latter. The exchanges can then be represented by the fundamental group of the configuration space of the particles, and in two dimensions this fundamental group is the braid group. Using rotors from a Clifford algebra and studying excitations of Majorana fermions, this thesis will show a way to calculate the exchange matrices of non-abelian anyons, and their corresponding eigenvalues. Furthermore, suggestions on a generalization of this framework along with areas where it can be applied are given.</p>

Note removed unnecessary hyphens
----------------------------------------------------------------------
In diva2:1695468 
abstract is: 
<p>Technological advancements have opened up the possibility of digitizing the pathological landscape, enabling deep learning-based methods to analyze digitized tissue samples, i.e., whole slide images (WSIs). Attention has recently shifted toward modeling WSIs as graphs since graph representations can capture dynamic relationships. This thesis investigates different graph construction techniques in conjunction with graph-based deep learning to classify WSIs as breast cancer histological grade 1 versus histological grade 3. To that extent, multiple graph representation techniques and two graph convolutional networks, GCN and GraphSAGE, were utilized. Finally, by evaluating the proposed models on an external test set originating from a separate cohort, it is clear that both models have the capacity for binary histological grading, yielding AUC scores of 0.791 (95% CI 0.756 − 0.825) and 0.838 (95% CI 0.808 − 0.869) for the GCN and GraphSAGE models. Modeling WSIs as graphs is an exciting and emerging field; however, further work is needed to evaluate alternative graph representation techniques and graph convolutional networks.</p>

corrected abstract:
<p>Technological advancements have opened up the possibility of digitizing the pathological landscape, enabling deep learning-based methods to analyze digitized tissue samples, i.e., whole slide images (WSIs). Attention has recently shifted toward modeling WSIs as graphs since graph representations can capture dynamic relationships. This thesis investigates different graph construction techniques in conjunction with graph-based deep learning to classify WSIs as breast cancer histological grade 1 versus histological grade 3. To that extent, multiple graph representation techniques and two graph convolutional networks, GCN and GraphSAGE, were utilized. Finally, by evaluating the proposed models on an external test set originating from a separate cohort, it is clear that both models have the capacity for binary histological grading, yielding AUC scores of 0.791 (95% CI 0.756 &ndash; 0.825) and 0.838 (95% CI 0.808 &ndash; 0.869) for the GCN and GraphSAGE models. Modeling WSIs as graphs is an exciting and emerging field; however, further work is needed to evaluate alternative graph representation techniques and graph convolutional networks.</p>

Note use an en dash  for the ranges
----------------------------------------------------------------------
In diva2:1880371   - correct as is
----------------------------------------------------------------------
In diva2:878311   - correct as is
----------------------------------------------------------------------
In diva2:1115298   - correct as is
----------------------------------------------------------------------
In diva2:1380062 
abstract is: 
<p>Since the world’s first fixed-wing scheduled aircraft took-off in 1914, with the development on commercial aircraft, the aviation industry has improved constantly in the following 104 years [1]. In 2017, over 4.1 billion of passengers were carried by about 36.8 million of flights by the world’s airlines. Statistic number also shows that about 2% of human-induced carbon dioxide emission should be responsible by the aviation industry [2].To protect the environment and reduce carbon dioxide emission, one important way is to reduce jet fuel consumption. Aircraft manufacturers has already employed many fuel saving methods such as improving aircraft aerodynamics and engine efficiency, and apply composite materials to reduce aircraft weight in recent years. For airlines, a suitable and economical flight plan is helpful to reduce fuel consumption. However, in addition to fuel consumption, time is another equally important factor for airlines at the same time.This thesis starts from the flight management point of view, based on dynamic programming method, establish a numerical simulation to calculate the most optimal vertical flight trajectory under ATC (Air Traffic Control) constrains and up-to-date high-resolution weather information.</p>

corrected abstract:
<p>Since the world’s first fixed-wing scheduled aircraft took-off in 1914, with the development on commercial aircraft, the aviation industry has improved constantly in the following 104 years [1]. In 2017, over 4.1 billion of passengers were carried by about 36.8 million of flights by the world’s airlines. Statistic number also shows that about 2% of human-induced carbon dioxide emission should be responsible by the aviation industry [2].</p><p>To protect the environment and reduce carbon dioxide emission, one important way is to reduce jet fuel consumption. Aircraft manufacturers has already employed many fuel saving methods such as improving aircraft aerodynamics and engine efficiency, and apply composite materials to reduce aircraft weight in recent years. For airlines, a suitable and economical flight plan is helpful to reduce fuel consumption. However, in addition to fuel consumption, time is another equally important factor for airlines at the same time.</p><p>This thesis starts from the flight management point of view, based on dynamic programming method, establish a numerical simulation to calculate the most optimal vertical flight trajectory under ATC (Air Traffic Control) constrains and up-to-date high-resolution weather information.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1147858   - correct as is
----------------------------------------------------------------------
In diva2:1334688 
abstract is: 
<p>During the last three years the Swedish stock market has showed a strong upwards movement from the lows of 2016. At the same time the IPO activity has been large and a lot of the offerings have had a positive return during the first day of trading in the market.</p><p>The goal of this study is to analyze if there is any particular IPO specific data that has a correlation with the first day return and if it can be used to predict the first day return for future IPO’s. If any regressors were shown to have correlation with the first day return, the goal is also to find a subset of regressors with even higher predictability. Then to classify which regressors show the highest correlation with a large positive return. The method which has been used is a multiple linear regression with IPO-data from the period 2017-2018.</p><p>The results from the study imply that none of the chosen regressors show any significant correlation with the first day return. It is a complicated process which might be difficult to simplify and quantify into a regression model, but further studies are needed to draw a conclusion if there are any other qualitative factors which correlate with the first day return.</p>

corrected abstract:
<p>During the last three years the Swedish stock market has showed a strong upwards movement from the lows of 2016. At the same time the IPO activity has been large and a lot of the offerings have had a positive return during the first day of trading in the market.</p><p>The goal of this study is to analyze if there is any particular IPO specific data that has a correlation with the first day return and if it can be used to predict the first day return for future IPO’s. If any regressors were shown to have any correlation with the first day return, the goal is also to find a subset of regressors with even higher predictability. Then to classify which regressors show the highest correlation with a large positive return. The method which has been used is a multiple linear regression with IPO-data from the period 2017-2018.</p><p>The results from the study imply that none of the chosen regressors show any significant correlation with the first day return. It is a complex process which might be difficult to simplify and quantify into a regression model, hence further studies are needed to draw a conclusion if there are any other qualitative factors which correlate with the first day return.</p>

Note corrected the text to match that in the original
----------------------------------------------------------------------
In diva2:1334677 - full text document is scanned
abstract is: 
<p>In this thesis a Markov chain model, which can be used for analysing students’ performance and their academic progress, is developed. Being able to evaluate students progress is useful for any educational system. It gives a better understanding of how students resonates and it can be used as support for important decisions and planning. Such a tool can be helpful for managers of the educational institution to establish a more optimal educational policy, which ensures better position in the educational market. To show that it is reasonable to use a Markov chain model for this purpose, a test for how well data fits such a model is created and used. The test shows that we cannot reject the hypothesis that the data can be fitted to a Markov chain model.</p>

corrected abstract:
<p>In this thesis a Markov chain model, which can be used for analysing students’ performance and their academic progress, is developed. Being able to evaluate students progress is useful for any educational system. It gives a better understanding of how students resonates and it can be used as support for important decisions and planning. Such a tool can be helpful for managers of the educational institution to establish a more optimal educational policy, which ensures better position in the educational market.</p><p>To show that it is reasonable to use a Markov chain model for this purpose, a test for how well data fits such a model is created and used. The test shows that we cannot reject the hypothesis that the data can be fitted to a Markov chain model.</p><p>The data used for the thesis contains information about 22551 students from LTH between the years 1993 &ndash; 2016 from 15 different programs, i.e all master of engineering programs offered.</p><p>The thesis will also contain an Industrial Engineering and Management part which looks into how management in educational organisations matter for educational performance.</p>

Note added missing final two paragraphs
----------------------------------------------------------------------
In diva2:1879553 - the English abstract - is actually in Swedish; there is no English abstract in the full texxt
The English title is present in DiVA, but not the Swedish title that is in the report.

Note as this thesis is from 2024 - it should have both an English and Swedish abstract in DiVA
----------------------------------------------------------------------
In diva2:605178 
abstract is: 
<p>The project has as task, if possible, to predict the stock market by assuming that it behaves like a Markov chain. To accomplish this, Markov chains of order one, two and three are used. All the observed stocks are considered as up- or downgoing at closing. This leaves the Markov chain with two possible states. Both daily and weekly reports occur. A program is constructed to be able to generate hints on how the stock price is going to change in the future. The program which does these calculations also tests itself to see the proportion of correct hints, i.e. how efficient the program is. The efficiency is measured in two made-up constants, the</p><p><em>GJ </em>constant and the <em>J </em>constant. The program in use is Matlab.</p><p>Keywords: Markov chain, higher order, two states, self-test, Matlab.</p>
mc='downgoing' c='down going'


corrected abstract:
<p>The project has as task, if possible, to predict the stock market by assuming that it behaves like a Markov chain. To accomplish this, Markov chains of order one, two and three are used. All the observed stocks are considered as up- or downgoing at closing. This leaves the Markov chain with two possible states. Both daily and weekly reports occur. A program is constructed to be able to generate hints on how the stock price is going to change in the future. The program which does these calculations also tests itself to see the proportion of correct hints, i.e. how efficient the program is. The efficiency is measured in two made-up constants, the 𝐺𝐽 constant and the 𝐽 constant. The program in use is Matlab.</p>

Note - removed keywords from abstract; added the math italic characters for the constants
----------------------------------------------------------------------
In diva2:1827834 
abstract is: 
<p>Åre ski resort is the largest and most renowned ski resort in Sweden, offering excellent skiing opportunities, restaurants, and nightlife in a prime location. Meanwhile, it is often subject to heavy traffic during the peak season and has earned a bad reputation for struggling with long lift queues. To address the issue, this paper aimed at analyzing the current capacity of the ski resort with the purpose of identifying areas and cost-efficient measures for improvement. It was done by modeling the ski system as a Jackson Network based on queuing theory, with relevant parameters extracted from actual skier data provided by the operating company Skistar. Several models were constructed to capture varying skiing patterns throughout the day and under different weather conditions. The models suggested that the lift queues first start to form at the lifts VM 8:an, Sadelexpressen, and Bräckeliften when the number of skiers in the system ranges from 3,700 to 6,200. Recommendations were then proposed to Skistar on how to resolve the identified bottlenecks and increase the resort’s capacity to a range of 6,000 to 8,400 skiers. Lastly, the models estimated that the resort could </p>

corrected abstract:
<p>Åre ski resort is the largest and most renowned ski resort in Sweden, offering excellent skiing opportunities, restaurants, and nightlife in a prime location. Meanwhile, it is often subject to heavy traffic during the peak season and has earned a bad reputation for struggling with long lift queues. To address the issue, this paper aimed at analyzing the current capacity of the ski resort with the purpose of identifying areas and cost-efficient measures for improvement. It was done by modeling the ski system as a Jackson Network based on queuing theory, with relevant parameters extracted from actual skier data provided by the operating company Skistar. Several models were constructed to capture varying skiing patterns throughout the day and under different weather conditions. The models suggested that the lift queues first start to form at the lifts VM 8:an, Sadelexpressen, and Bräckeliften when the number of skiers in the system range from 3,700 to 6,200. Recommendations were then proposed to Skistar on how to resolve the identified bottlenecks and increase the resort’s capacity to a range of 6,000 to 8,400 skiers. Lastly, the models estimated that the resort could reach a maximum capacity of 14,000 skiers by optimally utilizing all of its lifts.</p>

Note added missing text and fixed "ranges" to "range"
----------------------------------------------------------------------
In diva2:1645391 
abstract is: 
<p>The automotive and heavy-duty trucking industries are heading towards research and development of alternative powertrain solutions to meet the United Nations sustainability goals and cleaner solutions to aid climate change actions. This thesis project aligns with the vision of finding greener and sustainable modes of transport in the heavy long haulage trucking industry. This project aims to find and develop a method for creating drive cycles, getting the vehicular power requirements to drive on these selected routes and finally calculating the TCO of a vehicle.</p><p>The scripts for these mentioned steps are developed in MATLAB. The approach used in this work could help both the vehicle manufacturer and the vehicle operator to predict or cater to upcoming customer demand on, in our case, routes pan EU, to receive information about energy, power and vehicular configuration needed to fulfil the mission, and also, optimize the powertrain configuration in collaboration with a parallel thesis work done here at Scania, and finally calculate a somewhat simplified TCO of the vehicle. </p><p>In this work, two different driving conditions has been used; summer or winter, and two different payload conditions, as well as two types of vehicle powertrains; FCEV and BEV. Finally, a comparison regarding TCO for FCEV and BEV has been done.</p>

corrected abstract:
<p>The automotive and heavy-duty trucking industries are heading towards research and development of alternative powertrain solutions to meet the United Nations sustainability goals and cleaner solutions to aid climate change actions. This thesis project aligns with the vision of finding greener and sustainable modes of transport in the heavy long haulage trucking industry. This project aims to find and develop a method for creating drive cycles, getting the vehicular power requirements to drive on these selected routes and finally calculating the Total Cost of Ownership (TCO) of a vehicle.</p><p>The scripts for these mentioned steps are developed in MATLAB. The approach used in this work could help both the vehicle manufacturer and the vehicle operator to predict or cater to upcoming customer demand on, in our case, routes pan EU, to receive information about energy, power and vehicular configuration needed to fulfil the mission, and also, optimize the powertrain configuration in collaboration with a parallel thesis work done here at Scania [15], and finally calculate a somewhat simplified TCO of the vehicle.</p><p>In this work, two different driving conditions has been used; summer or winter, and two different payload conditions, as well as two types of vehicle powertrains; Fuel Cell Electric Vehicle (FCEV) and Battery Electric Vehicle (BEV). Finally, a comparison regarding TCO for FCEV and BEV has been done.</p>

Note added the missing text
----------------------------------------------------------------------
In diva2:1431032   - correct as is
----------------------------------------------------------------------
In diva2:1663244   - correct as is
----------------------------------------------------------------------
In diva2:1663200 
abstract is: 
<p>Despite having a philosophical grounding from empiricism that spans some centuries, the algorithmization of causal discovery started only a few decades ago. This formalization of studying causal relationships relies on connections between graphs and probability distributions. In this setting, the task of causal discovery is to recover the graph that best describes the causal structure based on the available data. A particular class of causal discovery algorithms, called constraint-based methods rely on Directed Acyclic Graphs (DAGs) as an encoding of Conditional Independence (CI) relations that carry some level of causal information. However, a CI relation such as X and Y being independent conditioned on Z assumes the independence holds for all possible values Z can take, which can tend to be unrealistic in practice where causal relations are often context-specific. In this thesis we aim to develop constraint-based algorithms to learn causal structure from Context-Specific Independence (CSI) relations within the discrete setting, where the independence relations are of the form X and Y being independent given Z and C = a for some a. This is done by using Context-Specific trees, or CStrees for short, which can encode CSI relations.</p>

corrected abstract:
<p>Despite having a philosophical grounding from empiricism that spans some centuries, the algorithmization of causal discovery started only a few decades ago. This formalization of studying causal relationships relies on connections between graphs and probability distributions. In this setting, the task of causal discovery is to recover the graph that best describes the causal structure based on the available data. A particular class of causal discovery algorithms, called constraint-based methods rely on Directed Acyclic Graphs (DAGs) as an encoding of Conditional Independence (CI) relations that carry some level of causal information. However, a CI relation such as 𝑋 and 𝑌 being independent conditioned on 𝑍 assumes the independence holds for all possible values 𝑍 can take, which can tend to be unrealistic in practice where causal relations are often context-specific. In this thesis we aim to develop constraint-based algorithms to learn causal structure from Context-Specific Independence (CSI) relations within the discrete setting, where the independence relations are of the form 𝑋 and 𝑌 being independent given 𝑍 and 𝐶 = 𝑎 for some 𝑎. This is done by using Context-Specific trees, or CStrees for short, which can encode CSI relations.</p>

Note changes equations to use math italic font
----------------------------------------------------------------------
In diva2:1040644 
abstract is: 
<p>Computing the loads for a train passing a tunnel requires to predict both the external and the internal pressure variations in time, both of which are strong and quick for a non-pressure tight train.</p><p>The key achievement of this work has been the development of 3D CFD Star-CCM+ overset mesh simulations capable of simulating the single train tunnel entry and tunnel passage as well as the two trains crossing inside the tunnel. Unfortunately it is not affordable to execute a 3D CFD study for the tunnel passage of each new train model, so 1D CFD codes have been employed, simplified predictive models have been developed and both have been compared to the 3D CFD results.</p><p>An important result has been identifying the influence of several parameters on the loads caused both by the travelling pressure waves generated when the train enters the tunnel and by the pressure disturbances due to train crossing inside the tunnel, using Star-CCM+ parameters sweeps simulations over train speed and train and tunnel geometrical parameters.</p><p>The main conclusion is that the internal pressure variation is particularly important to compute the loads, especially for non-tight trains. For this reason it is necessary to take into account both the carriage free length and the position along the carriage on which the loads are needed.</p>

corrected abstract:
<p>Computing the loads for a train passing a tunnel requires to predict both the external and the internal pressure variations in time, both of which are strong and quick for a non-pressure tight train.</p><p>The key achievement of this work has been the development of 3D CFD Star-CCM+ overset mesh simulations capable of simulating the single train tunnel entry and tunnel passage as well as the two trains crossing inside the tunnel. Unfortunately it is not affordable to execute a 3D CFD study for the tunnel passage of each new train model, so 1D CFD codes have been employed, simplified predictive models have been developed and both have been compared to the 3D CFD results.</p><p>An important result has been identifying the influence of several parameters on the loads caused both by the travelling pressure waves generated when the train enters the tunnel and by the pressure disturbances due to train crossing inside the tunnel, using Star-CCM+ parameters sweeps simulations over train speed and train and tunnel geometrical parameters.</p><p>The main conclusion is that the internal pressure variation is particularly important to compute the loads, especially for non-tight trains. For this reason it is necessary to take into account both the carriage free length and the position along the carriage on which the loads are needed.</p>
----------------------------------------------------------------------
In diva2:1306995 
abstract is: 
<p>High speed planing hulls are currently widely used for example in recreational and emergency vessel applications. However, very little CFD research has been done for planing vessels, especially for those with stepped hulls. A validated CFD method for planing stepped hulls could be a valuable improvement for the design phase of such hulls. In this thesis, a CFD method for stepped hulls, with a primary focus on two-step hulls, is developed using STAR-CCM+. As a secondary objective, porpoising instability of two-step hulls is investigated. The simulations</p><p>are divided into two parts: In the first part a method is developed and validated with existing experimental and numerical data for a simple model scale planing hull with one step. In the second part the method is applied for two two-step hulls provided with Hydrolift AS. A maximum two degrees of freedom, trim and heave, are used, as well as RANS based k-w SST turbulence model and Volume of Fluid (VOF) as a free surface model. The results for the one-step hull mostly corresponded well with the validation data. For the two-step hulls, validation data did not exists and they were first simulated with a fixed trim and sinkage and compered between each other. In the simulations with free trim and heave both hulls experienced unstable porpoising behavior.</p>

corrected abstract:
<p>High speed planing hulls are currently widely used for example in recreational and emergency vessel applications. However, very little CFD research has been done for planing vessels, especially for those with stepped hulls. A validated CFD method for planing stepped hulls could be a valuable improvement for the design phase of such hulls.</p><p>In this thesis, a CFD method for stepped hulls, with a primary focus on two-step hulls, is developed using STAR-CCM+. As a secondary objective, porpoising instability of two-step hulls is investigated. The simulations are divided into two parts: In the first part a method is developed and validated with existing experimental and numerical data for a simple model scale planing hull with one step. In the second part the method is applied for two two-step hulls provided with Hydrolift AS. A maximum two degrees of freedom, trim and heave, are used, as well as RANS based 𝑘-ω SST turbulence model and Volume of Fluid (VOF) as a free surface model.</p><p>The results for the one-step hull mostly corresponded well with the validation data. For the two-step hulls, validation data did not exists and they were first simulated with a fixed trim and sinkage and compered between each other. In the simulations with free trim and heave both hulls experienced unstable porpoising behavior.</p>

Note added missing paragraph breaks and fixed "k" to "𝑘" and "w" to "ω"
----------------------------------------------------------------------
In diva2:1788302   - correct as is
----------------------------------------------------------------------
In diva2:538302 
abstract is: 
<p>The objective of this project is to design, using computational fluid dynamics (CFD), a set of retention aid dosage nozzles that minimize shear levels during their operation. This includes the effect of dosage nozzle size, contour and dosage velocity - absolute and relative to the stock flow. As a starting point, the three different dosage nozzles currently implemented on the Innventia FEX paper-machine have been studied using CFD. Problem areas, defined as regions of high viscous and/or turbulent shear, with these designs should be identified, and solutions to their improvement have been realized. The computational models considered here include non Newtonian models of the retention aid solution, as well as turbulent modeling of the stock flow. Novel configurations have been implemented which attempt to minimize the strain rate and shear stress during dosage and at the same time improve the mixing quality of the retention aid polymers. While the velocity of the side jet is determined to be the main cause of high strain rates and shear stresses, a good mixing can be reached by varying the position of the nozzles and the diameter penetrating the stock flow. The best compromise of mixing and shear stress has been reached with a triple side-wall nozzles configuration.</p>

corrected abstract:
<p>The objective of this project is to design, using computational fluid dynamics (CFD), a set of retention aid dosage nozzles that minimize shear levels during their operation. This includes the effect of dosage nozzle size, contour and dosage velocity - absolute and relative to the stock flow. As a starting point, the three different dosage nozzles currently implemented on the Innventia FEX paper-machine have been studied using CFD. Problem areas, defined as regions of high viscous and/or turbulent shear, with these designs should be identified, and solutions to their improvement have been realized. The computational models considered here include non-Newtonian models of the retention aid solution, as well as turbulent modeling of the stock flow. Novel configurations have been implemented which attempt to minimize the strain rate and shear stress during dosage and at the same time improve the mixing quality of the retention aid polymers. While the velocity of the side jet is determined to be the main cause of high strain rates and shear stresses, a good mixing can be reached by varying the position of the nozzles and the diameter penetrating the stock flow. The best compromise of mixing and shear stress has been reached with a triple side-wall nozzles configuration.</p>

Note added hyphen in "non-Newtonian"
----------------------------------------------------------------------
In diva2:1720147   - correct as is
----------------------------------------------------------------------
In diva2:1392095 - note that the registered trademark symbol is missing from title:
"CFD Simulations of Unsteady LHA Ship Airwake in OpenFOAM"
==>
"CFD Simulations of Unsteady LHA Ship Airwake in OpenFOAM®"

abstract   - correct as is
----------------------------------------------------------------------
In diva2:1218975 
abstract is: 
<p>The investing market can be a cold ruthless place for the layman. In order to get the chance of making money in this business one must place countless hours on research, with many different parameters to handle in order to reach success. To reduce the risk, one must look to many different companies operating in multiple fields and industries. In other words, it can be a hard task to manage this feat. With modern technology, there is now lots of potential to handle this tedious analysis autonomously using machine learning and clever algorithms. With this approach, the amount of analyzes is only limited by the capacity of the computer. Resulting in a number far greater than if done by hand. This study aims at exploring the possibilities to modify and implement efficient algorithms in the field of finance. The study utilizes the power of kernel methods in order to algorithmically analyze the patterns found in financial data efficiently. By combining the powerful tools of change point detection and nonlinear regression the computer can classify the different trends and moods in the market. The study culminates to a tool for analyzing data from the stock market in a way that minimizes the influence from short spikes and drops, and instead is influenced by the underlying pattern. But also, an additional tool for predicting future movements in the price.</p>

corrected abstract:
<p>The investing market can be a cold ruthless place for the layman. In order to get the chance of making money in this business one must place countless hours on research, with many different parameters to handle in order to reach success. To reduce the risk, one must look to many different companies operating in multiple fields and industries. In other words, it can be a hard task to manage this feat.</p><p>With modern technology, there is now lots of potential to handle this tedious analysis autonomously using machine learning and clever algorithms. With this approach, the amount of analyzes is only limited by the capacity of the computer. Resulting in a number far greater than if done by hand.</p><p>This study aims at exploring the possibilities to modify and implement efficient algorithms in the field of finance. The study utilizes the power of kernel methods in order to algorithmically analyze the patterns found in financial data efficiently. By combining the powerful tools of change point detection and nonlinear regression the computer can classify the different trends and moods in the market.</p><p>The study culminates to a tool for analyzing data from the stock market in a way that minimizes the influence from short spikes and drops, and instead is influenced by the underlying pattern. But also, an additional tool for predicting future movements in the price.</p>

Note added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1561661 
abstract is: 
<p>Pentameric ligand-gated ion channels (pLGICs) are membrane receptors that play a crucial role in every living organism. The pLGIC protein structure forms a pore through the membrane of a cell that can let specific ions pass through, upon activation by endogenous agonists. pLGICs are allosterically modulated by ligands binding at allosteric sites, that either stabilize a certain conformation or change the binding affinity of the endogenous agonist. However, much remains unknown about the exact way in which these modulators bind to and affect pLGICs. An increased understanding could help in the search for novel and/or more effective target drugs. With this masters thesis, I hope to contribute by investigating the modulatory effect of ethanol on the bacterial Gloeobacter ligand-gated ion channel (GLIC). This has been done by performing oocyte electrophysiology recordings and analysis of molecular dynamics simulations, both with and without ethanol, and of four separate variants of GLIC that are either potentiated or inhibited by ethanol. Two possible allosteric sites were discovered in a transmembraneintrasubunit pocket: a potentiating allosteric site close to the M2 helix and residue V242, as well as an inhibitory membrane- and M4 helix-close intrasubunit site. Finally, evidence was found that could support a previously suggested inhibitory allosteric site in the pore around the 9’ hydrophobic gate.</p>

corrected abstract:
<p>Pentameric ligand-gated ion channels (pLGICs) are membrane receptors that play a crucial role in every living organism. The pLGIC protein structure forms a pore through the membrane of a cell that can let specific ions pass through, upon activation by endogenous agonists. pLGICs are allosterically modulated by ligands binding at allosteric sites, that either stabilize a certain conformation or change the binding affinity of the endogenous agonist. However, much remains unknown about the exact way in which these modulators bind to and affect pLGICs. An increased understanding could help in the search for novel and/or more effective target drugs. With this masters thesis, I hope to contribute by investigating the modulatory effect of ethanol on the bacterial <em>Gloeobacter</em> ligand-gated ion channel (GLIC). This has been done by performing oocyte electrophysiology recordings and analysis of molecular dynamics simulations, both with and without ethanol, and of four separate variants of GLIC that are either potentiated or inhibited by ethanol. Two possible allosteric sites were discovered in a transmembrane intrasubunit pocket: a potentiating allosteric site close to the M2 helix and residue V242, as well as an inhibitory membrane- and M4 helix-close intrasubunit site. Finally, evidence was found that could support a previously suggested inhibitory allosteric site in the pore around the 9’ hydrophobic gate.</p>

Note - added missing italics
----------------------------------------------------------------------
In diva2:549834   - correct as is
----------------------------------------------------------------------
In diva2:1817011   - correct as is
----------------------------------------------------------------------
In diva2:1703975   - correct as is
----------------------------------------------------------------------
In diva2:1288436 
abstract is: 
<p>This report describes the structural design of a wing for a Vertical Take Off  and Landing drone, in which all the structure will be built by fused deposition modeling of polylactic acid (PLA). To perform this de-sign, the material used is first characterized in different orientations using tensile stress tests, Image Correlation and MATLAB. These properties are then input in a MATLAB program specially developed for this project to obtain the optimum skin and spar thickness in the wing for certain fight conditions. Results are finally verified with a 3D model in CAD and scaled wings in bending tests.</p>

corrected abstract:
<p>This report describes the structural design of a wing for a Vertical Take Off and Landing drone, in which all the structure will be built by fused deposition modeling of polylactic acid (PLA). To perform this design, the material used is first characterized in different orientations using tensile stress tests, Image Correlation and MATLAB. These properties are then input in a MATLAB program specially developed for this project to obtain the optimum skin and spar thickness in the wing for certain flight conditions. Results are finally verified with a 3D model in CAD and scaled wings in bending tests.</p>

Note - removed unnecessary hyphen and corrceed spelling of "flight"
----------------------------------------------------------------------
In diva2:550442   - perhaps a missing colon separating title and subtitle:
"Characterization of Track Irregularities With respect to vehicle response"
==>
"Characterization of Track Irregularities: With respect to vehicle response"

abstract correct as is
----------------------------------------------------------------------
In diva2:1082703 
abstract is: 
<p>The demands on fuel efficiency and environmental friendliness of cars have driven the automotive industry towards composite materials which reduce the weight compared to the traditional aluminum and steel solutions. The purpose of this master thesis is to evaluate the possibility and feasibility of redesigning a high volume metal chassis part in composite materials.  To accomplish this the thesis work was divided into two parts. The first part consists of a composite study which explores the available composite technologies in the industry such as implemented chassis components and available manufacturing methods. The composite study shows that almost no high volume chassis component in the market are made out of composites, with exception to leaf springs. In the industry there are many different composite manufacturing methods but in general the most ready for high volume production are Injection molding, compression molding and RTM. A method was also explored to efficiently evaluate different material and manufacturing methods against each other. By knowing the critical requirement both materials and manufacturing methods can be evaluated separately against each other. The second part consists of a design phase where the knowledge from the composite study was used to choose and redesign a chassis component in composite. A motor mount was chosen and redesigned using injection molding. The new design shows that a weight decrease of at least 38% is possible without significant cost differences. </p>

corrected abstract:
<p>The demands on fuel efficiency and environmental friendliness of cars have driven the automotive industry towards composite materials which reduce the weight compared to the traditional aluminum and steel solutions. The purpose of this master thesis is to evaluate the possibility and feasibility of redesigning a high volume metal chassis part in composite materials.  To accomplish this the thesis work was divided into two parts. The first part consists of a composite study which explores the available composite technologies in the industry such as implemented chassis components and available manufacturing methods. The composite study shows that almost no high volume chassis component in the market are made out of composites, with exception to leaf springs. In the industry there are many different composite manufacturing methods but in general the most ready for high volume production are Injection molding, compression molding and RTM. A method was also explored to efficiently evaluate different material and manufacturing methods against each other. By knowing the critical requirement both materials and manufacturing methods can be evaluated separately against each other. The second part consists of a design phase where the knowledge from the composite study was used to choose and redesign a chassis component in composite. A motor mount was chosen and redesigned using injection molding. The new design shows that a weight decrease of at least 38% is possible without significant cost differences.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1081928 
abstract is: 
<p>Autonomous driving might increase safety and profitability of trucks in many applications. The mining industry, with its enclosed and controlled areas, is ideal for early implementation of autonomous solutions. The possibility of increased productivity, profitability and safety for the mining industry and the mining area as a ground for development could, through collaboration, result in many benefits for both mining companies and truck manufactures. Scania must investigate how these autonomous vehicles should be constructed. The project goal is thereby to develop a chassis layout concept for an autonomous truck. The concept should improve profitability and safety for transportation of materials within the mining industry while minimizing the introduction of new components to Scania.</p><p>The chosen approach is based on the Ulrich &amp; Eppinger method for product development including generation and selection of concepts. Product requirements were specified from identified customer needs. The generated concepts were evaluated against these requirements and comparisons were performed with weighted matrices. Some benefits of the final chassis layout concept are a higher load carrying capacity, more robust component placement and higher ground clearance. The vehicle concept would also be able to operate in underground mines with low roof clearance which could open new market segments for Scania. However, the concept requires development to gain higher performance on load carrying components in the chassis front.</p><p>The suggested concept shows that Scania could build and deliver autonomous mining vehicles with optimized chassis layouts based on Scania’s existing components within a near future.</p>

corrected abstract:
<p>Autonomous driving might increase safety and profitability of trucks in many applications. The mining industry, with its enclosed and controlled areas, is ideal for early implementation of autonomous solutions. The possibility of increased productivity, profitability and safety for the mining industry and the mining area as a ground for development could, through collaboration, result in many benefits for both mining companies and truck manufactures.</p><p>Scania must investigate how these autonomous vehicles should be constructed. The project goal is thereby to develop a chassis layout concept for an autonomous truck. The concept should improve profitability and safety for transportation of materials within the mining industry while minimizing the introduction of new components to Scania.</p><p>The chosen approach is based on the Ulrich &amp; Eppinger method for product development including generation and selection of concepts. Product requirements were specified from identified customer needs. The generated concepts were evaluated against these requirements and comparisons were performed with weighted matrices.</p><p>Some benefits of the final chassis layout concept are a higher load carrying capacity, more robust component placement and higher ground clearance. The vehicle concept would also be able to operate in underground mines with low roof clearance which could open new market segments for Scania. However, the concept requires development to gain higher performance on load carrying components in the chassis front.</p><p>The suggested concept shows that Scania could build and deliver autonomous mining vehicles with optimized chassis layouts based on Scania’s existing components within a near future.</p>

Note - added missing paragrap breaks
----------------------------------------------------------------------
In diva2:1379746   - correct as is
----------------------------------------------------------------------
In diva2:1348448 
abstract is: 
<p>The divisible cross-country ski, an innovation by Cityski, created to facilitate practicing the sport in an urban setting. The parts are attached by a cone-shaped coupling in a fitted sleeve made of composite material and fastened with a lock. The aim of this thesis is to analyze the coupling and lock with mechanical models to calculate loads, deformations and stresses that may occur when the ski is used as well as giving suggestions for further improvement. The study concludes that the coupling in current state and material cannot be expected to stand anticipated loads during usage as the stresses exceed the tensile strength of the material with a factor of four. A study of materials showed that by using aluminum as an option to composite could reduce the factor considerably. Further, the result also showed that over 90 percent of the overall stresses the cone was subjected to were represented by bending stresses. To account for the stresses the coupling has been subjected to a parameter study in order to relate the geometry of the cone to the stresses. From this it was shown that by simply adjusting the placement of the cone from its centered position downwards the bending stresses could be reduced by approximately 9 percent. This combined with additional adjustments such as a more rigid locking mechanism and tighter lock could contribute to a maximum 26 percent reduction. To further establish the result from the mechanical modelling a numerical analysis with finite element method was conducted using the software ANSYS MECHANICAL. The outcomes confirmed earlier results by showing similar levels of stresses, even to a certain degree higher due to concentration in the area where the cone is mounted. This is a known weak zone from earlier testing of the ski. To enable suggestion for strengthening the area a method for topology optimization was done. Based on this a new model which make use of the material more effectively was designed. Additional suggestions for improvements discussed but not analyzed in depth are the possibility of a plate of a more rigid material on the bottom side of the cone to allow for a higher load before break or make the ski slightly higher at the coupling to lower the effects of bending stresses.</p>

corrected abstract:
<p>The divisible cross-country ski, an innovation by Cityski, created to facilitate practicing the sport in an urban setting. The parts are attached by a cone-shaped coupling in a fitted sleeve made of composite material and fastened with a lock. The aim of this thesis is to analyze the coupling and lock with mechanical models to calculate loads, deformations and stresses that may occur when the ski is used as well as giving suggestions for further improvement. The study concludes that the coupling in current state and material cannot be expected to stand anticipated loads during usage as the stresses exceed the tensile strength of the material with a factor of four. A study of materials showed that by using aluminum as an option to composite could reduce the factor considerably.</p><p>Further, the result also showed that over 90 percent of the overall stresses the cone was subjected to were represented by bending stresses. To account for the stresses the coupling has been subjected to a parameter study in order to relate the geometry of the cone to the stresses. From this it was shown that by simply adjusting the placement of the cone from its centered position downwards the bending stresses could be reduced by approximately 9 percent. This combined with additional adjustments such as a more rigid locking mechanism and tighter lock could contribute to a maximum 26 percent reduction.</p><p>To further establish the result from the mechanical modelling a numerical analysis with finite element method was conducted using the software ANSYS MECHANICAL. The outcomes confirmed earlier results by showing similar levels of stresses, even to a certain degree higher due to concentration in the area where the cone is mounted. This is a known weak zone from earlier testing of the ski. To enable suggestion for strengthening the area a method for topology optimization was done. Based on this a new model which make use of the material more effectively was designed. Additional suggestions for improvements discussed but not analyzed in depth are the possibility of a plate of a more rigid material on the bottom side of the cone to allow for a higher load before break or make the ski slightly higher at the coupling to lower the effects of bending stresses.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:852715 
abstract is: 
<p>Three methods for claims reserving are compared on two data sets. The first two methods are the commonly used chain ladder method that uses aggregated payments and the relatively new method, double chain ladder, that apart from the payments data also uses the number of reported claims. The third method is more advanced, data on micro-level is needed such as the reporting delay and the number of payment periods for every single claim. The two data sets that are used consist of claims with typically shorter and longer settlement time, respectively. The questions considered are if you can gain anything from using a method that is more advanced than the chain ladder method and if the gain differs from the two data sets. The methods are compared by simulating the reserves distributions as well as comparing the point estimates of the reserve with the real out-of-sample reserve. The results show that there is no gain in using the micro-level method considered. The double chain lad- der method on the other hand performs better than the chain ladder method. The difference between the two data sets is that the reserve in the data set with longer settlement times is harder to estimate, but no difference can be seen when it comes to method choice.</p>

corrected abstract:
<p>Three methods for claims reserving are compared on two data sets. The first two methods are the commonly used chain ladder method that uses aggregated payments and the relatively new method, double chain ladder, that apart from the payments data also uses the number of reported claims. The third method is more advanced, data on micro-level is needed such as the reporting delay and the number of payment periods for every single claim. The two data sets that are used consist of claims with typically shorter and longer settlement time, respectively. The questions considered are if you can gain anything from using a method that is more advanced than the chain ladder method and if the gain differs from the two data sets. The methods are compared by simulating the reserves distributions as well as comparing the point estimates of the reserve with the real out-of-sample reserve. The results show that there is no gain in using the micro-level method considered. The double chain ladder method on the other hand performs better than the chain ladder method. The difference between the two data sets is that the reserve in the data set with longer settlement times is harder to estimate, but no difference can be seen when it comes to method choice.</p>

Note - removed unnecessary hyphen
----------------------------------------------------------------------
In diva2:1215659   - correct as is
----------------------------------------------------------------------
In diva2:1595248 - PDF appears to have been printed as a bitmap
abstract is: 
<p>The primary objective of this thesis is to evaluate the prospect of machine learning methods being used to classify flying qualities based on simulator data (with the focus being on pitch maneuvers). If critical flying qualities could be identified earlier in the verification process, they can be further invested in and focused on with less cost for design changes of the flight control system.</p><p>Information from manned simulations with given flying quality levels are used to create a replication of the performed pitch maneuver in a desktop simulator. The generated flight data is represented by different measures in the classification to separately train and test the machine learning models against the given flying quality level. The models used are Logistic Regression, Support Vector Machines with radial basis functions (RBF), linear and polynomial kernels along with Artificial Neural Networks. </p><p>The results show that the classifiers correctly identify at least 80% of cases with critical flying qualities. The classification shows that the statistical measures of the time signals and first order time derivatives of pitch, roll and yaw rates are enough for classification within the scope of this thesis. The different machine learning models show no significant difference in performance in the scope of this thesis. In conclusion, machine learning methods show good potential for classification of flying qualities, and could become an important tool for evaluating flying qualities of large amounts of simulations, in addition to manned simulations.</p>

corrected abstract:
<p>The primary objective of this thesis is to evaluate the prospect of machine learning methods being used to classify flying qualities based on simulator data (with the focus being on pitch maneuvers). If critical flying qualities could be identified earlier in the verification process, they can be further invested in and focused on with less cost for design changes of the flight control system.</p><p>Information from manned simulations with given flying quality levels are used to create a replication of the performed pitch maneuver in a desktop simulator. The generated flight data is represented by different measures in the classification to separately train and test the machine learning models against the given flying quality level. The models used are Logistic Regression, Support Vector Machines with radial basis functions (RBF), linear and polynomial kernels along with Artificial Neural Networks.</p><p>The results show that the classifiers correctly identify at least 80% of cases with critical flying qualities. The classification shows that statistical measures of the time signals and first order time derivatives of pitch, roll and yaw rates are enough for classification within the scope of this thesis. The different machine learning models show no significant difference in performance in the scope of this thesis. In conclusion, machine learning methods shows good potential for classification of flying qualities, and could become an important tool for evaluating flying qualities of large amounts of simulations, in addition to manned simulations.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph and minor spelling corrceions
("the statistical" should eb "statistical" and "show" to "shows")
----------------------------------------------------------------------
In diva2:575902   - correct as is
----------------------------------------------------------------------
In diva2:1850032 
abstract is: 
<p>The Maximum Likelihood Degree (ML degree) of a statistical model is the number of complex critical points of the likelihood function. In this thesis we study this on Colored Gaussian Graphical Models, classifying the ML degree of colored graphs of order up to three. We do this by calculating the rational function degree of the gradient of the log- likelihood. Moreover we find that coloring a graph can lower the ML degree. Finally we calculate solutions to the homaloidal partial differential equation developed by Améndola et al. The code developed for these calculations can be used on graphs of higher orders.</p>

corrected abstract:
<p>The Maximum Likelihood Degree (ML degree) of a statistical model is the number of complex critical points of the likelihood function. In this thesis we study this on Colored Gaussian Graphical Models, classifying the ML degree of colored graphs of order up to three. We do this by calculating the rational function degree of the gradient of the log-likelihood. Moreover we find that coloring a graph can lower the ML degree. Finally we calculate solutions to the homaloidal partial differential equation developed by Améndola et al. The code developed for these calculations can be used on graphs of higher orders.</p>

Note - removed unnecessary space after hypen
----------------------------------------------------------------------
In diva2:1849053   - correct as is
----------------------------------------------------------------------
In diva2:1165806   - correct as is
----------------------------------------------------------------------
In diva2:1307352   - correct as is
----------------------------------------------------------------------
In diva2:1375293   - correct as is
----------------------------------------------------------------------
In diva2:1431629 
abstract is: 
<p>With the help of new technology it has become much easier to apply for a job. Reaching out to a larger audience also results in a lot of more applications to consider when hiring for a new position. This has resulted in that many big companies uses statistical learning methods as a tool in the first step of the recruiting process. Smaller companies that do not have access to the same amount of historical and big data sets do not have the same opportunities to digitalise their recruitment process. Using topological data analysis, this thesis explore how clustering methods can be used on smaller data sets in the early stages of the recruitment process. It also studies how the level of abstraction in data representation affects the results. The methods seem to perform well on higher level job announcements but struggles on basic level positions. It also shows that the representation of candidates and jobs has a huge impact on the results.</p>

corrected abstract:
<p>With the help of new technology it has become much easier to apply for a job. Reaching out to a larger audience also results in a lot of more applications to consider when hiring for a new position. This has resulted in that many big companies uses statistical learning methods as a tool in the first step of the recruiting process. Smaller companies that do not have access to the same amount of historical and big data sets do not have the same opportunities to digitalise their recruitment process. Using topological data analysis, this thesis explore how clustering methods can be used on smaller data sets in the early stages of the recruitment process. It also studies how the level of abstraction in data representation affects the results. The methods seem to perform well on higher level job announcements but struggles on basic level positions. It also shows that the choice of representation of candidates and jobs has a huge impact on the results.</p>

Note added missing word "choice"
----------------------------------------------------------------------
In diva2:1319859 
abstract is: 
<p>In this thesis a clustering of the Stockholm county housing market has been performed using different clustering methods. Data has been derived and different geographical constraints have been used. DeSO areas (Demographic statistical areas), developed by SCB, have been used to divide the housing market in to smaller regions for which the derived variables have been calculated. Hierarchical clustering methods, SKATER and Gaussian mixture models have been applied. Methods using different kinds of geographical constraints have also been applied in an attempt to create more geographically contiguous clusters. The different methods are then compared with respect to performance and stability. The best performing method is the Gaussian mixture model EII, also known as the K-means algorithm. The most stable method when applied to bootstrapped samples is the ClustGeo-method.</p>

corrected abstract:
<p>In this thesis a clustering of the Stockholm county housing market has been performed using different clustering methods. Data has been derived and different geographical constraints have been used. DeSO areas (Demographically statistical areas), developed by SCB, has been used to divide the housing market in to smaller regions for which the derived variables have been calculated. Hierarchical clustering methods, <em>SKATER</em> and Gaussian mixture models have been applied. Methods using different kinds of geographical constraints have also been applied in an attempt to create more geographically contiguous clusters. The different methods are then compared with respect to performance and stability. The best performing method is the Gaussian mixture model <em>EII</em>, also known as the 𝐾-<em>means</em> algorithm. The most stable method when applied to bootstrapped samples is the <em>ClustGeo-method</em>.</p>

Note added missing text, corrected some text, added italics and changed "K" to "𝐾"
----------------------------------------------------------------------
In diva2:633890 
abstract is: 
<p>Adaptive mesh refinement and coarsening methods are effective techniques to reduce the computation time of finite element based solvers. Parallel imple- mentations of such adaption routines, suitable for large scale computations on distributed memory machines, need additional care. In this thesis, a coarsening technique based on edge collapses is presented, its implementation and opti- mization for parallel computations explained and it is analyzed with respect to coarsening efficiency and performance. As a possible application the use of mesh coarsening in adaptive flow simulations is demonstrated</p>

corrected abstract:
<p>Adaptive mesh refinement and coarsening methods are effective techniques to reduce the computation time of finite element based solvers. Parallel implementations of such adaption routines, suitable for large scale computations on distributed memory machines, need additional care. In this thesis, a coarsening technique based on edge collapses is presented, its implementation and optimization for parallel computations explained and it is analyzed with respect to coarsening efficiency and performance. As a possible application the use of mesh coarsening in adaptive flow simulations is demonstrated.</p>

Note - removed unnecessary hpyehns and added terminal period to last sentence (as it is in the original)
----------------------------------------------------------------------
In diva2:1086408   - correct as is
----------------------------------------------------------------------
In diva2:1596326   - correct as is --- diva2:1656340 seems to be a duplicate
----------------------------------------------------------------------
In diva2:1739340   - correct as is
----------------------------------------------------------------------
In diva2:1441558   - correct as is
----------------------------------------------------------------------
In diva2:1796647 
abstract is: 
<p>This research study aims to investigate the capacity of single photons to carry information through polarization and time ordering and proposes a protocol called Beyond Pulse Position Modulation (BPPM) to improve photon-based communication reliability over longer distances with limited power. Such a protocol may be used in any communication scenario where energy efficiency is important, e.g., in satellite communication or where pulse position modulation (PPM) typically is used. The study compares various metrics such as information bits per symbol, photon, and time bin to evaluate the system’s efficiency and conducts a comparative analysis of BPPM, Pulse Position Modulation (PPM), On-Off Keying (OOK), andGeneral protocol’s effectiveness. (The simulations were conducted using the Python programming language with Visual Studio Code IDE.)</p>

mc='andGeneral' c='and General'

corrected abstract:
<p>This research study aims to investigate the capacity of single photons to carry information through polarization and time ordering and proposes a protocol called Beyond Pulse Position Modulation (BPPM) to improve photon-based communication reliability over longer distances with limited power. Such a protocol may be used in any communication scenario where energy efficiency is important, e.g., in satellite communication or where pulse position modulation (PPM) typically is used. The study compares various metrics such as information bits per symbol, photon, and time bin to evaluate the system’s efficiency and conducts a comparative analysis of BPPM, Pulse Position Modulation (PPM), On-Off Keying (OOK), and General protocol’s effectiveness. (The simulations were conducted using the Python programming language with Visual Studio Code IDE.)</p>

Note - separated the merged words
----------------------------------------------------------------------
In diva2:1255173   - correct as is

Note error in original "in-situ" should be "<em>in situ</em>"
----------------------------------------------------------------------
In diva2:872164   - correct as is
----------------------------------------------------------------------
In diva2:1107306 
abstract is: 
<p>This thesis compares two groups of features for short-term price predictions of futures contracts; fast- and slow-acting features. The fast-acting group are based on limit order book derived features and technical indicators that reacts to changes in price quickly. The slow-acting features constitute of technical indicators that reacts to changes in price slowly.</p><p>The comparison is done through two methods, group importance and a mean cost calculation. This is evaluated for different forecast horizons and contracts. Furthermore, two years of data was provided to do the analysis. Moreover, the comparison is modelled with an ensemble method called random forest. The response is constructed using rolling quantiles and a volume weighted price. </p><p>The finding implies that fast-acting features are superior at predicting price changes on smaller time scales, while long-acting features are better at predicting prices changes on larger time scales. Furthermore, the multivariate model results were similar to the univariate ones. However, the results are not clear-cut and more investigation ought to be done in order to confirm these results.</p>

corrected abstract:
<p>This thesis compares two groups of features for short-term price predictions of futures contracts; fast- and slow-acting features. The fast-acting group are based on limit order book derived features and technical indicators that reacts to changes in price quickly. The slow-acting features constitute of technical indicators that reacts to changes in price slowly.</p><p>The comparison is done through two methods, group importance and a mean cost calculation. This is evaluated for different forecast horizons and contracts. Furthermore, two years of data was provided to do the analysis. Moreover, the comparison is modelled with an ensemble method called random forest. The response is constructed using rolling quantiles and a volume weighted price.</p><p>The finding implies that fast-acting features are superior at predicting price changes on smaller time scales, while long-acting features are better at predicting prices changes on larger time scales. Furthermore, the multivariate model results were similar to the univariate ones. However, the results are not clear-cut and more investigation ought to be done in order to confirm these results.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1766767   - correct as is
----------------------------------------------------------------------
In diva2:917945   - correct as is
----------------------------------------------------------------------
In diva2:1528156 
abstract is: 
<p>Formation flying of satellites describes a mission in which a set of satellites arrange their position with respect to one another. In this paper, satellite formation flying guidance and control algorithms are investigated in terms of required velocity increment Delta-v, and tracking error for a Chief/Deputy satellite system. Different control methods covering continuous and impulsive laws are implemented and tested for Low Earth Orbit (LEO). Sliding Mode, Feedback Linearization and Model Predictive Controllers are compared to an Impulsive Feedback Law which tracks the mean orbital element differences. Sliding Mode and Feedback Linearization controllers use the same dynamic model which includes Earth Oblateness perturbations. On the other hand, Model Predictive Control with Multi-Objective Cost Function is based on the Clohessy–Wiltshire equations, which do not account for any perturbation and do not cover the eccentricity of the orbit. The comparison was done for two different missions both including Earth Oblateness effects only. A relative orbit mission, which was based on the Prisma Satellite Mission and a rendezvous mission, was implemented. The reference trajectory for the controllers was generated with Yamanaka and Ankersen’s state transition matrix, while a separate method was used for the Impulsive Law. In both of the missions, it was observed that the implemented Impulsive Law outperformed in terms of Delta-v, 1.2 to 3.5 times smaller than the continuous control approaches, while the continuous controllers had a smaller tracking error, 2 to 8.3 times less, both in terms of root mean square error and maximum error in the steady state. Finally, this study shows that the tracking error and Delta-v has inversely proportional relationship.</p>

corrected abstract:
<p>Formation flying of satellites describes a mission in which a set of satellites arrange their position with respect to one another. In this paper, satellite formation flying guidance and control algorithms are investigated in terms of required velocity increment ∆𝑣, and tracking error for a Chief/Deputy satellite system. Different control methods covering continuous and impulsive laws are implemented and tested for Low Earth Orbit (LEO). Sliding Mode, Feedback Linearization and Model Predictive Controllers are compared to an Impulsive Feedback Law which tracks the mean orbital element differences. Sliding Mode and Feedback Linearization controllers use the same dynamic model which includes Earth Oblateness perturbations. On the other hand, Model Predictive Control with Multi-Objective Cost Function is based on the Clohessy–Wiltshire equations, which do not account for any perturbation and do not cover the eccentricity of the orbit. The comparison was done for two different missions both including Earth Oblateness effects only. A relative orbit mission, which was based on the Prisma Satellite Mission and a rendezvous mission, was implemented. The reference trajectory for the controllers was generated with Yamanaka and Ankersen’s state transition matrix, while a separate method was used for the Impulsive Law. In both of the missions, it was observed that the implemented Impulsive Law outperformed in terms of ∆𝑣, 1.2 to 3.5 times smaller than the continuous control approaches, while the continuous controllers had a smaller tracking error, 2 to 8.3 times less, both in terms of root mean square error and maximum error in the steady state. Finally, this study shows that the tracking error and ∆𝑣 has inversely proportional relationship.</p>

Note - replaced "Delta-v" with "∆𝑣"
----------------------------------------------------------------------
In diva2:1567721   - correct as is
----------------------------------------------------------------------
In diva2:1429548 
abstract is: 
<p>The use of composite materials has been common in small craft boat building for a long time. In recent years, there has been a huge push in the development of diﬀerent types of appendages such as hydrofoils. These hydrofoils are commonly manufactured in carbon ﬁbre composites, due to high requirements in weight and stiﬀness. These appendages can be diﬃcult to develop and complex to manufacture since manufacturing methods for composites are complex. KTH Royal Institute of Technology is developing a hydrofoil concept for a small autonomous vessel. The hydrofoil is designed to be built in carbon ﬁbre composite. It requires to have control surfaces in order to maintain a stable ﬂight and the electrical propulsion is located on it as well. This makes this hydrofoil one of a kind and the parts that build up the hydrofoil have more speciﬁcations than just to be designed from a hydrodynamic and structural point of view like a conventional hydrofoil. This thesis investigated what manufacturing methods should be used when building a hydrofoil like this.</p><p>Existing manufacturing methods such as vacuum infusion and diﬀerent types of prepreg moulding have been reviewed and are presented early in the report. The methods have been analyzed from the perspective of the components of the hydrofoil, resulting in an initial manufacturing strategy for the diﬀerent components. The strategy includes everything from a 3D-model of the part to a ﬁnished product, including sheet design, mould manufacturing and moulding of the part.</p><p>Several tests were conducted before a component was successfully manufactured. Each test was evaluated and presented in such a manner that the reader can understand what is needed to be improved and why. The conclusions of each test lead to an improvement of the manufacturing technique and a new test until the ﬁnal result was acquired. The tests were examined with a microscope to verify the quality of the part. Then a weight fraction analysis was made on these parts. The ﬁnal conclusions of the thesis gave successful methods to manufacture the diﬀerent parts of the hydrofoil. A fast manufacturing method for product development of complex parts was achieved. The resulting parts from the tests show good quality from the analysis.</p>

corrected abstract:
<p>The use of composite materials has been common in small craft boat building for a long time. In recent years, there has been a huge push in the development of different types of appendages such as hydrofoils. These hydrofoils are commonly manufactured in carbon fibre composites, due to high requirements in weight and stiffness. These appendages can be difficult to develop and complex to manufacture since manufacturing methods for composites are complex. KTH Royal Institute of Technology is developing a hydrofoil concept for a small autonomous vessel. The hydrofoil is designed to be built in carbon fibre composite. It requires to have control surfaces in order to maintain a stable flight and the electrical propulsion is located on it as well. This makes this hydrofoil one of a kind and the parts that build up the hydrofoil have more specifications than just to be designed from a hydrodynamic and structural point of view like a conventional hydrofoil. This thesis investigated what manufacturing methods should be used when building a hydrofoil like this.</p><p>Existing manufacturing methods such as vacuum infusion and different types of prepreg moulding have been reviewed and are presented early in the report. The methods have been analyzed from the perspective of the components of the hydrofoil, resulting in an initial manufacturing strategy for the different components. The strategy includes everything from a 3D-model of the part to a finished product, including sheet design, mould manufacturing and moulding of the part.</p><p>Several tests were conducted before a component was successfully manufactured. Each test was evaluated and presented in such a manner that the reader can understand what is needed to be improved and why. The conclusions of each test lead to an improvement of the manufacturing technique and a new test until the final result was acquired. The tests were examined with a microscope to verify the quality of the part. Then a weight fraction analysis was made on these parts. The final conclusions of the thesis gave successful methods to manufacture the different parts of the hydrofoil. A fast manufacturing method for product development of complex parts was achieved. The resulting parts from the tests show good quality from the analysis.</p>

Note the corrected version has replaced the ligatures byt their equivalents
----------------------------------------------------------------------
In diva2:1693894 
abstract is: 
<p>The master thesis subject takes place in the automotive industry and specifically in the internal combustion engine area. The need of improving the efficiency of the engines leads to develop new technologies like turbo compressors. Some of the challenges to overcome are high rotational speed difficulties or extreme load and fatigue in the rotors. By design they are also prone to aerodynamic instabilities like compressor surge. These off design behaviors are not often studied by the manufacturers and therefore not so well known. </p><p>The aims are to understand, analyze and possible ameliorate the sources of compressor surge; to identify surge causes; to create a way to reproduce the phenomena with robustness and precision; to be able to study potential solutions to eliminate surge noises. A literature review has been carried out. This would give good metrics to identify surge cycles.</p><p>Based on the theory developed by Fink et al. (1992) a simulation model has been generated, followed by a process of calibration carried out using data acquired during field experiments. This method uses a fully modifiable simulation model in order to be able to be adapted to a wide range of turbo compressors.</p><p>The predicted data by the model shows a reasonable agreement with the experimental data. This allows to test control laws with a surge valve or a high pressure gas recirculating valve. The knowledge alongside the simulation would help the team to better apprehend the problem on the future engine generations and have means to avoid the unwanted surge phenomena to occur.</p>

corrected abstract:
<p>The master thesis subject takes place in the automotive industry and specifically in the internal combustion engine area. The need of improving the efficiency of the engines leads to develop new technologies like turbo compressors. Some of the challenges to overcome are high rotational speed difficulties or extreme load and fatigue in the rotors. By design they are also prone to aerodynamic instabilities like compressor surge. These off design behaviors are not often studied by the manufacturers and therefore not so well known.</p><p>The aims are to understand, analyze and possible ameliorate the sources of compressor surge; to identify surge causes; to create a way to reproduce the phenomena with robustness and precision; to be able to study potential solutions to eliminate surge noises. A literature review has been carried out. This would give good metrics to identify surge cycles.</p><p>Based on the theory developed by <em>Fink et al.</em> (1992) a simulation model has been generated, followed by a process of calibration carried out using data acquired during field experiments. This method uses a fully modifiable simulation model in order to be able to be adapted to a wide range of turbo compressors.</p><p>The predicted data by the model shows a reasonable agreement with the experimental data. This allows to test control laws with a surge valve or a high pressure gas recirculating valve. The knowledge alongside the simulation would help the team to better apprehend the problem on the future engine generations and have means to avoid the unwanted surge phenomena to occur.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph and added italics
----------------------------------------------------------------------
In diva2:604474 
abstract is: 
<p>This thesis details the refinement and numerical solution of a preexisting model for predicting the strengths and positions of so-called wake-vortices that are generated from the lift of heavy aircraft. The ultimate objective is to implement a numerical scheme for the model that is fast enough</p><p>to allow for probabilistic methods, such as Monte Carlosimulations, in order to deal with the inherent uncertainty in input parameters for wake-vortex predictions.</p><p>The differential equation system of the wake-vortex model is stated clearly, which has not been done before. The refinement consists in reducing the number of necessary state variables in the differential equation system.</p><p>A numerical algorithm based on the mathematical properties of the model is implemented and different ways of optimizing the computations are considered, e.g. through</p><p>parallelization.</p><p>Finally, a study will be made trying to assess the validity of the results through analyses of the accuracy and of the model’s sensitivity to small input parameter variations.</p>

mc='Carlosimulations' c='Carlo simulations'

corrected abstract:
<p>This thesis details the refinement and numerical solution of a preexisting model for predicting the strengths and positions of so-called wake-vortices that are generated from the lift of heavy aircraft. The ultimate objective is to implement a numerical scheme for the model that is fast enough to allow for probabilistic methods, such as Monte Carlo-simulations, in order to deal with the inherent uncertainty in input parameters for wake-vortex predictions.</p><p>The differential equation system of the wake-vortex model is stated clearly, which has not been done before. The refinement consists in reducing the number of necessary state variables in the differential equation system.</p><p>A numerical algorithm based on the mathematical properties of the model is implemented and different ways of optimizing the computations are considered, e.g. through parallelization.</p><p>Finally, a study will be made trying to assess the validity of the results through analyses of the accuracy and of the model’s sensitivity to small input parameter variations.</p>

Note - removed unnecessary paragraph breaks and added hypen
----------------------------------------------------------------------
In diva2:1705466 
abstract is: 
<p>Numerical simulations of large complex systems such as biomolecules often suffer from the full description of the system having too many dimensions for direct numerical calculations and Monte Carlo methods having trouble overcoming energy barriers. It is therefore desirable to formulate a description in lower dimension which captures the system’s macroscopic behaviour. Recently, Lindahl et al [1] proposed a metric, g(λ), on the extended space Λ based on the dynamics of the system to optimize Monte Carlo sampling within extended ensemble formalism. In this thesis, we formulate a low-dimensional effective coarse-grained dynamic on Λ as a diffusion process and ask if it is possible to use this metric to calculate thelocal effective diffusion matrix as D(λ) = g−1(λ). By testing various scenarios we conclude that computing D(λ) in this manner indeed gives a correct effective dynamic in most cases, where the scale of coarse-graining can be tuned. However, an incorrect dynamic is received for example when the scale of coarse-graining is comparable to the size of oscillations in the energy landscape.</p>

mc='thelocal' c='the local'

corrected abstract:
<p>Numerical simulations of large complex systems such as biomolecules often suffer from the full description of the system having too many dimensions for direct numerical calculations and Monte Carlo methods having trouble overcoming energy barriers. It is therefore desirable to formulate a description in lower dimension which captures the system’s macroscopic behaviour. Recently, Lindahl <em>et al</em> [1] proposed a metric, 𝑔(&#x1D706;), on the extended space Λ based on the dynamics of the system to optimize Monte Carlo sampling within extended ensemble formalism. In this thesis, we formulate a low-dimensional effective coarse-grained dynamic on Λ as a diffusion process and ask if it is possible to use this metric to calculate the local effective diffusion matrix as 𝐷(λ) = 𝑔<sup>−1</sup>(&#x1D706;). By testing various scenarios we conclude that computing 𝐷(&#x1D706;) in this manner indeed gives a correct effective dynamic in most cases, where the scale of coarse-graining can be tuned. However, an incorrect dynamic is received for example when the scale of coarse-graining is comparable to the size of oscillations in the energy landscape.</p>

Note error in oroginal, "et al" is missing a period and added math italics and superscript for equations
----------------------------------------------------------------------
In diva2:1438246 
abstract is: 
<p>In this thesis the eigenmodes and eigenvalues of three dimensional structures are analyzed using the Python environment FEniCS in combination with an implementation of the Arnoldi method in MATLAB for calculation of eigenpairs. This is done by considering separable solutions of the wave equation and subsequently expressing these as the solutions to an eigenvalue problem. The eigenvalue problem is then solved on two geometries inspired by two objects from Star Wars; the Death Star and a TIE fighter. To do this, the eigenvalue problem obtained from the wave equation is expressed in its weak form, also known as its variational form, and then discretized into a generalized eigenvalue problem. The eigenvalue problem is then solved approximately using the Arnoldi method, a method that can be used for finding approximate solutions to large and sparse eigenvalue problems. The main results are the plots of the eigenmodes of the two structures which are produced using the Python library vtkplotter.</p>

corrected abstract:
<p>In this thesis the eigenmodes and eigenvalues of three dimensional structures are analyzed using the Python environment FEniCS in combination with an implementation of the Arnoldi method in MATLAB for calculation of eigenpairs. This is done by considering separable solutions of the wave equation and subsequently expressing these as the solutions to an eigenvalue problem. The eigenvalue problem is then solved on two geometries inspired by two objects from Star Wars; the Death Star and a TIE fighter. To do this, the eigenvalue problem obtained from the wave equation is expressed in its weak form, also known as its variational form, and then discretized into a generalized eigenvalue problem. The eigenvalue problem is then solved approximately using the Arnoldi method, a method that can be used for finding approximate solutions to large and sparse eigenvalue problems. The main results are the plots of the eigenmodes of the two structures which are produced using the Python library vtkplotter.</p>
----------------------------------------------------------------------
In diva2:1435827 
abstract is: 
<p>The Bose-Einstein condensate is a phase of matter that arises when cooling gases of bosons to extremely low temperatures. When studying these condensates one may use the Gross-Pitaevskii equation, which is a non-linear variant of the Schrödinger equation. An interesting phenomenon that arises when rotating a Bose-Einstein condensate is the appearance of vortices. We implement a semi-implicit Euler scheme using spectral methods proposed in [1] to numerically calculate the ground state of a rotating Bose-Einstein condensate. We start with implementing a simpler iterative fixed-point method to solve the Euler scheme but show that this method fails to converge for large rotations. Because of this we implement multiple Krylov subspace solvers that in fact do converge for large rotations and show that the Preconditioned Conjugate Gradient method has better performance than the BiConjugate Gradient Stabilized method. After the implementation we briefly look at the performance of the method and improve it with simple tricks that do not compromise the accuracy or robustness and which reduce the computation time slightly. Lastly we look at the formation of vortices in 2-dimensional and 3-dimensional Bose-Einstein condensates. We show that the number of vortices increases exponentially for increasing angular velocity in 2D until the condensate breaks apart, but in 3D we ultimately find that the required computation time and RAM storage is too large to be able to analyze the vortices in a similar way on our personal computers.</p>

corrected abstract:
<p>The Bose-Einstein condensate is a phase of matter that arises when cooling gases of bosons to extremely low temperatures. When studying these condensates one may use the Gross-Pitaevskii equation, which is a non-linear variant of the Schrödinger equation. An interesting phenomenon that arises when rotating a Bose-Einstein condensate is the appearance of vortices.</p><p>We implement a semi-implicit Euler scheme using spectral methods proposed in [1] to numerically calculate the ground state of a rotating Bose-Einstein condensate. We start with implementing a simpler iterative fixed-point method to solve the Euler scheme but show that this method fails to converge for large rotations. Because of this we implement multiple Krylov subspace solvers that in fact do converge for large rotations and show that the Preconditioned Conjugate Gradient method has better performance than the BiConjugate Gradient Stabilized method. After the implementation we briefly look at the performance of the method and improve it with simple tricks that do not compromise the accuracy or robustness and which reduce the computation time slightly.</p><p>Lastly we look at the formation of vortices in 2-dimensional and 3-dimensional Bose-Einstein condensates. We show that the number of vortices increases exponentially for increasing angular velocity in 2D until the condensate breaks apart, but in 3D we ultimately find that the required computation time and RAM storage is too large to be able to analyze the vortices in a similar way on our personal computers.</p>

Note - added missing paragraph breaks
----------------------------------------------------------------------
In diva2:1741196 
abstract is: 
<p>Planing hull is a common hull design concept which decreases the resistance while speed is increasing for a specific speed range. It is also suitable for hull modifications to achieve higher efficiency. Spray deflector is a promising hull modification which offers extra resistance decreasing and less vertical acceleration for planing hulls. Spray deflector technology can decrease the resistance up to 28% compared to the bare hull. However, the information on spray deflector design is strongly limited. In this study, there are two different types of spray deflector designs compared via CFD to achieve better design. Star CCM+ software was used to create CFD models with given numerical settings: 3-Dimensional, implicit unsteady, multiphase VOF, RANS based SST K-Omega turbulences model, all y+ Hybrid Wall Treatment while only considering heave and trim. Froude Number of the simulations ranges from 2 to 2.6. To improve the value of CFD models, mesh sensitivity study, time step study, y+ study, and alteration of prism layer number were conducted. The experimental base of this thesis is Molchanov’s “Experimental validation of spray deflectors' impact on the performance of high-speed planing” study from 2018. All CFD outcomes were evaluated according to these experiments. </p><p>There is a problem named numerical ventilation which downgrades the value of outcomes. Thus, three different methods were evaluated against numerical ventilation additionally to the spray deflector comparison. These methods are “Phase Replacement”, “Modified High-Resolution Interface Capturing Scheme”, and “Volume Fraction Source Term”. Application of Volume Fraction Source Term method gave the best achievements for the calculation of resistance with 0.35% error ratio, and trim angle 17% error ratio while causing 16% error ratio for heave. The modified HRIC scheme achieved a 1.4% error ratio for heave, 12.5% error ratio for resistance, and 20.4% error ratio for trim angle. The restrictions of these methods and their application ways are specified in this thesis.</p>

corrected abstract:
<p>Planing hull is a common hull design concept which decreases the resistance while speed is increasing for a specific speed range. It is also suitable for hull modifications to achieve higher efficiency. Spray deflector is a promising hull modification which offers extra resistance decreasing and less vertical acceleration for planing hulls. Spray deflector technology can decrease the resistance up to 28% compared to the bare hull. However, the information on spray deflector design is strongly limited. In this study, there are two different types of spray deflector designs compared via CFD to achieve better design. Star CCM+ software was used to create CFD models with given numerical settings: 3-Dimensional, implicit unsteady, multiphase VOF, RANS based SST K-Omega turbulences model, all y+ Hybrid Wall Treatment while only considering heave and trim. Froude Number of the simulations ranges from 2 to 2.6. To improve the value of CFD models, mesh sensitivity study, time step study, y+ study, and alteration of prism layer number were conducted. The experimental base of this thesis is Molchanov’s “Experimental validation of spray deflectors' impact on the performance of high-speed planing” study from 2018. All CFD outcomes were evaluated according to these experiments.</p><p>There is a problem named numerical ventilation which downgrades the value of outcomes. Thus, three different methods were evaluated against numerical ventilation additionally to the spray deflector comparison. These methods are “Phase Replacement”, “Modified High-Resolution Interface Capturing Scheme”, and “Volume Fraction Source Term”. Application of Volume Fraction Source Term method gave the best achievements for the calculation of resistance with 0.35% error ratio, and trim angle 17% error ratio while causing 16% error ratio for heave. The modified HRIC scheme achieved a 1.4% error ratio for heave, 12.5% error ratio for resistance, and 20.4% error ratio for trim angle. The restrictions of these methods and their application ways are specified in this thesis.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1720652   - correct as is
----------------------------------------------------------------------
In diva2:1127926   - correct as is
----------------------------------------------------------------------
In diva2:1572092 
abstract is: 
<p>In this report, the conceptual design of an unmanned aerial vehicle (UAV) and aerodynamic analysis is treated. The project was split into two groups, one group would do the aerodynamic analysis, and the other group would do the performance analysis. The plan was to create a UAV capable of surveying life stock, large farmlands, wildlife, and also reindeer husbandry. This demanded that the aircraft had to be able to easily launch from all types of locations. To solve this the plane was designed for vertical take-off and landing capabilities (VTOL). </p><p>The study includes the selection and performance testing of an airfoil, aerodynamic performance of the wing, and the wing's geometry. It also includes stability analysis, structural design, and CAD creation. The majority of this work was done by combining the usage of the design tool XFLR5 with CAD from Solid Edge and equations done in MATLAB. The aircraft accomplishes our goals to have it be VTOL functional. It has a flight time of over 2 hours and weighs less than 5 kg. Its cruise speed lies at 12 m/s. It is also possible to create a detailed design and to produce the aircraft with relative ease and low cost. Its dynamic stability is however not optimized and further work is needed if optimized stability is desired.</p>

corrected abstract:
<p>In this report, the conceptual design of an unmanned aerial vehicle (UAV) and aerodynamic analysis is treated. The project was split into two groups, one group would do the aerodynamic analysis, and the other group would do the performance analysis. The plan was to create a UAV capable of surveying life stock, large farmlands, wildlife, and also reindeer husbandry. This demanded that the aircraft had to be able to easily launch from all types of locations. To solve this the plane was designed for vertical take-off and landing capabilities (VTOL).</p><p>The study includes the selection and performance testing of an airfoil, aerodynamic performance of the wing, and the wing's geometry. It also includes stability analysis, structural design, and CAD creation. The majority of this work was done by combining the usage of the design tool XFLR5 with CAD from Solid Edge and equations done in MATLAB. The aircraft accomplishes our goals to have it be VTOL functional. It has a flight time of over 2 hours and weighs less than 5 kg. Its cruise speed lies at 12 m/s. It is also possible to create a detailed design and to produce the aircraft with relative ease and low cost. Its dynamic stability is however not optimized and further work is needed if optimized stability is desired.</p>

Note only change was to eliminate an unnecessary space at the end of a paragraph
----------------------------------------------------------------------
In diva2:1572108   - correct as is
----------------------------------------------------------------------
In diva2:1130018   - correct as is
----------------------------------------------------------------------
In diva2:1599577 
abstract is: 
<p>The objective of this study was to design a launch vehicle capable of deploying a nanosatellite into a Sun-synchronous orbit at 500 km orbital altitude from the JAS 39E/F Gripen fighter aircraft. This was achieved by first performing theoretical calculations for the required nozzles and solid propellant grain configurations for the first two solid stages, followed by the necessary liquid propellant configuration for the third stage. Lastly, two methods were investigated in solving the trajectory ascent problem for the launch vehicle design. First, by stating the trajectory problem as an initial value problem while guessing a Sigmoidal steering law. Secondly, by stating the trajectory problem as a boundary value problem. The latter was solved by transcribing the trajectory problem into a nonlinear program where a parametric steering law was derived using a Sequential quadratic programming algorithm.Ultimately, resulting in a launch vehicle design with a gross lift-off mass of 1,289 kg, capable of launching an 8.4 kg payload into the targeted orbit, with suggested modifications to increase the possible payload mass to 12.9 kg.</p>

mc='algorithm.Ultimately' c='algorithm. Ultimately'

corrected abstract:
<p>The objective of this study was to design a launch vehicle capable of deploying a nanosatellite into a Sun-synchronous orbit at 500 km orbital altitude from the JAS 39E/F Gripen fighter aircraft. This was achieved by first performing theoretical calculations for the required nozzles and solid propellant grain configurations for the first two solid stages, followed by the necessary liquid propellant configuration for the third stage. Lastly, two methods were investigated in solving the trajectory ascent problem for the launch vehicle design. First, by stating the trajectory problem as an initial value problem while guessing a Sigmoidal steering law. Secondly, by stating the trajectory problem as a boundary value problem. The latter was solved by transcribing the trajectory problem into a nonlinear program where a parametric steering law was derived using a Sequential quadratic programming algorithm. Ultimately, resulting in a launch vehicle design with a gross lift-off mass of 1,289 kg, capable of launching an 8.4 kg payload into the targeted orbit, with suggested modifications to increase the possible payload mass to 12.9 kg.</p>

Note change due to unmerging words
----------------------------------------------------------------------
In diva2:1740138   - correct as is
----------------------------------------------------------------------
In diva2:1360752 
abstract is: 
<p>The Swedish Navy has been using the AUV62-AT for submarine hunting training successfully for a while and it has been found that the biggest drawback is the transport which is very resource draining. The Swedish Defence Materiel Administration was given the task to solve this and a hydrofoiling-trailer was seen as a potential solution. This thesis will present the design process of the foil-trailer as well as issues found and how theses were mitigated, to produce a viable design. To develop the final design both current use of hydrofoil vessels and further adaptions needed for this applications were investigated. The project was done in collaboration with Simon Källerfelt Korall, who has during the project investigated in detail the foil-trailer’s roll stability, presents this in "The development of a foiling trailer for transport of the AUV62-AT"[1], which deals with how the experimental- and model results lines up. The result is a final design of a foil-trailer which is constituted of several sub-systems that has been found to improve the overall concept. It was concluded that the concept was viable and if further developed it can be used as a great method of saving time when deploying the AUV62-AT.</p>

corrected abstract:
<p>The Swedish Navy has been using the AUV62-AT for submarine hunting training successfully for a while and it has been found that the biggest drawback is the transport which is very resource draining. The Swedish Defence Materiel Administration was given the task to solve this and a hydrofoiling-trailer was seen as a potential solution.</p><p>This thesis will present the design process of the foil-trailer as well as issues found and how theses were mitigated, to produce a viable design. To develop the final design both current use of hydrofoil vessels and further adaptions needed for this applications were investigated. The project was done in collaboration with Simon Källerfelt Korall, who has during the project investigated in detail the foil-trailer’s roll stability, presents this in "The development of a foiling trailer for transport of the AUV62-AT"[1], which deals with how the experimental- and model results lines up.</p><p>The result is a final design of a foil-trailer which is constituted of several sub-systems that has been found to improve the overall concept. It was concluded that the concept was viable and if further developed it can be used as a great method of saving time when deploying the AUV62-AT.</p>

Note - added missing paragrap breaks
----------------------------------------------------------------------
In diva2:1499358   - correct as is
----------------------------------------------------------------------
In diva2:550530 - note title is missing registered trademark symbol:
"Conceptual Simulator Implementation of Flapping Wing Micro Air Vehicle Using FLAMES"
==>
"Conceptual Simulator Implementation of Flapping Wing Micro Air Vehicle Using FLAMES®"

abstract is: 
<p>The interest for Flapping Wing Micro Air Vehicles (FWMAV) is growing. With this comes a need for future users to test and evaluate these vehicles with simulations. This study presents a first iteration of an implementation of a FWMAV in FLAMES Simulation Framework. An aerodynamic model based on the widely used quasi-steady blade element approach is presented and a first linearised version is implemented in FLAMES. The simulation model is capable of both manual and autonomous flight. With the first person view a pilot can investigate buildings and objects. The study gives an idea of how FLAMES can be used for this type of simulations. FLAMES provides a good environment for testing the vehicle both individually and in a context with other units.</p>

corrected abstract:
<p>The interest for Flapping Wing Micro Air Vehicles (FWMAV) is growing. With this comes a need for future users to test and evaluate these vehicles with simulations. This study presents a first iteration of an implementation of a FWMAV in FLAMES Simulation Framework. An aerodynamic model based on the widely used quasi-steady blade element approach is presented and a first linearised version is implemented in FLAMES. The simulation model is capable of both manual and autonomous flight. With the first person view a pilot can investigate buildings and objects. The study gives an idea of how FLAMES can be used for this type of simulations. FLAMES provides a good environment for testing the vehicle both individually and in a context with other units.</p>
----------------------------------------------------------------------
In diva2:1900954 
abstract is: 
<p>This study presents a preliminary launch abort concept for the Nyx Earth Crew spacecraft developed at The Exploration Company. A requirements study of general human mission certification focusing on abort requirements is followed by a more detailed assessment of abort needs, interfaces and performance. Assuming launch with the Ariane 6 from French Guiana, a total of nine abort modes, three major abort criteria, seven success parameters, a layout of an abort control system and a mission sequence are defined, including an assessment of the most substantial risks during an abort manoeuvre. Five driving abort requirements were identified in the areas of safety, performance, reusability, in-house development and system complexity. Based on this extensive analysis, three different abort architectures with two design variations each are proposed: A "puller tower" similar to the one used on the Orion spacecraft, a "pusher engine" similar to the one used on the Crew Dragon spacecraft and an architecture using solid rocket boosters which does not yet exist for abort purposes. An architecture trade-off is performed by means of studying the components, accommodation, propulsion system and driving requirements of each architecture. The new hybrid booster concept is deemed the most feasible as it combines several advantages of both the pusher and puller system, namely a lightweight, jettisoned and simple solid propulsion system. Nonetheless, given that this concept has no abort heritage, several analyses have to be conducted to determine whether this concept is technically feasible, in particular in terms of continuous abort capability, booster jettison and potential reusability. The most feasible alternative to be further investigated was identified as the pusher engine, given potential synergies with the main propulsion system and advantages in terms of reusability. The puller tower is proposed to be discarded as a potential launch abort system for Nyx Earth Crew given its excessive mass and high sequential complexity during an abort. Based on this preliminary trade-off, a development plan is presented which spans six years from kick-off in 2025 to maiden launch end of 2030. The work breakdown structure includes preliminary design focusing on propellants and nozzles, detailed design with an engineering model containing all subsystems and qualification with two models used for pad and in-flight abort testing, leading up to the flight model in 2030. A cost analysis of this development cycle resulted in 180 - 200M € non-recurring development costs driven by qualification testing and 25 - 50M € recurring flight costs driven by the added launch mass.</p>


corrected abstract:
<p>This study presents a preliminary launch abort concept for the Nyx Earth Crew spacecraft developed at The Exploration Company. A requirements study of general human mission certification focusing on abort requirements is followed by a more detailed assessment of abort needs, interfaces and performance. Assuming launch with the Ariane 6 from French Guiana, a total of nine abort modes, three major abort criteria, seven success parameters, a layout of an abort control system and a mission sequence are defined, including an assessment of the most substantial risks during an abort manoeuvre. Five driving abort requirements were identified in the areas of safety, performance, reusability, in-house development and system complexity. Based on this extensive analysis, three different abort architectures with two design variations each are proposed: A ”puller tower” similar to the one used on the Orion spacecraft, a ”pusher engine” similar to the one used on the Crew Dragon spacecraft and an architecture using solid rocket boosters which does not yet exist for abort purposes. An architecture trade-off is performed by means of studying the components, accommodation, propulsion system and driving requirements of each architecture. The new hybrid booster concept is deemed the most feasible as it combines several advantages of both the pusher and puller system, namely a lightweight, jettisoned and simple solid propulsion system. Nonetheless, given that this concept has no abort heritage, several analyses have to be conducted to determine whether this concept is technically feasible, in particular in terms of continuous abort capability, booster jettison and potential reusability. The most feasible alternative to be further investigated was identified as the pusher engine, given potential synergies with the main propulsion system and advantages in terms of reusability. The puller tower is proposed to be discarded as a potential launch abort system for Nyx Earth Crew given its excessive mass and high sequential complexity during an abort. Based on this preliminary trade-off, a development plan is presented which spans six years from kick-off in 2025 to maiden launch end of 2030. The work breakdown structure includes preliminary design focusing on propellants and nozzles, detailed design with an engineering model containing all subsystems and qualification with two models used for pad and in-flight abort testing, leading up to the flight model in 2030. A cost analysis of this development cycle resulted in 180 - 200M € non-recurring development costs driven by qualification testing and 25 - 50M € recurring flight costs driven by the added launch mass.</p>

Note - only change was to make the double quotes match those of the original
----------------------------------------------------------------------
In diva2:1436882 
abstract is: 
<p>The Functional Materials Division of KTH conducts research on CDI technology for water desalination. A control unit is being designed and requires measurement of salinity which relates to electrolytic conductivity. Hence, a sensor developed by Innovative Sensor Technology (iST) for electrolytic conductivity was studied. In this report a starting point for integration of the sensor to the control unit is examined. The main focuses are sensor input voltage, output current, and circuit evaluation. Tests have been made in sodium chloride solution in salinity ranges of 700-2200 mg/L. The output current range was 590-2200 μA RMS with 15 % error. Reasonable input voltages could not be verified in tests. Exemplary circuits from iST were built and tested experimentally. They were demonstrated to work when the sensor was not connected. The circuit parts that were tested for accuracy, showed a signal transmission accuracy of around 95 %, within certain input ranges. Valid future work and further improvements are suggested.</p>

corrected abstract:
<p>The Functional Materials Division of KTH conducts research on CDI technology for water desalination. A control unit is being designed and requires measurement of salinity which relates to electrolytic conductivity. Hence, a sensor developed by Innovative Sensor Technology (iST) for electrolytic conductivity was studied.</p><p>In this report a starting point for integration of the sensor to the control unit is examined. The main focuses are sensor input voltage, output current, and circuit evaluation.</p><p>Tests have been made in sodium chloride solution in salinity ranges of 700-2200 mg/L. The output current range was 590-2200 µA RMS with 15 % error. Reasonable input voltages could not be verified in tests.</p><p>Exemplary circuits from iST were built and tested experimentally. They were demonstrated to work when the sensor was not connected. The circuit parts that were tested for accuracy, showed a signal transmission accuracy of around 95 %, within certain input ranges. Valid future work and further improvements are suggested.</p>

Note - added missing paragrap h breaks and change to use the "µ" in the original (i.e., the micro sign)
----------------------------------------------------------------------
In diva2:766649   - correct as is
----------------------------------------------------------------------
In diva2:1663235   - correct as is
----------------------------------------------------------------------
In diva2:1867423 
abstract is: 
<p>Ligand-gated ion channels play an important role in electrochemical signal transduction across diverse organisms, yet their structural and functional intricacies are not fully understood. Particularly lacking is the knowledge of their response to variations in pH, an aspect necessary for understanding their physiological relevance and potential therapeutic targeting in neurological diseases. In this thesis project, I have investigated the mechanistic response of sTeLIC, a recently reported prokaryotic member of the pentameric ligand-gated ion channel family, to different environmental conditions. Using molecular dynamics simulations, a total of 16 different environmental conditions have been explored including variations in pH (neutral and alkaline), the presence and absence of calcium, and the inclusion of an electric field acting as an external driving force on charged atoms. The results reveal a comprehensive pH-sensing and gating mechanism involving key residues, notably E106 (on the β6 strand) and E160 (on loop F), and their local microenvironments. Additionally, an inhibitory mechanism for calcium is proposed, with E160 playing an important role. The simulations including an electric field has provided support for a non-conventional ion pathway through the pore. Collectively, these results offer insights into a mechanistic framework that may extend to other physiologically relevant systems, providing a foundation for further investigations and potential future therapeutic intervention.</p>

corrected abstract:
<p>Ligand-gated ion channels play an important role in electrochemical signal transduction across diverse organisms, yet their structural and functional intricacies are not fully understood. Particularly lacking is the knowledge of their response to variations in pH, an aspect necessary for understanding their physiological relevance and potential therapeutic targeting in neurological diseases. In this thesis project, I have investigated the mechanistic response of sTeLIC, a recently reported prokaryotic member of the pentameric ligand-gated ion channel family, to different environmental conditions. Using molecular dynamics simulations, a total of 16 different environmental conditions have been explored including variations in pH (neutral and alkaline), the presence and absence of calcium, and the inclusion of an electric field acting as an external driving force on charged atoms. The results reveal a comprehensive pH-sensing and gating mechanism involving key residues, notably E106 (on the β6 strand) and E160 (on loop F), and their local microenvironments. Additionally, an inhibitory mechanism for calcium is proposed, with E160 playing an important role. The simulations including an electric field has provided support for a non-conventional ion pathway through the pore. Collectively, these results offer insights into a mechanistic framework that may extend to other physiologically relevant systems, providing a foundation for further investigations and potential future therapeutic interventions.</p>


Note spelling error:
'intervention' = => 'interventions'
----------------------------------------------------------------------
In diva2:1800496 - no absract in thesis! I'm not sure where the abstract in DiVA came from
----------------------------------------------------------------------
In diva2:1678955 
abstract is: 
<p>Dark matter, the unknown matter that constitutes 85% of all matter in the universe, is one of the greatest mysteries in fundamental physics. One theory that might explain dark matter predicts that there are long-lived particles known as dark pions. If these were created in a particle accelerator, they could decay inside the detector, resulting in particles that seemingly "emerge" from nothing. This phenomenon is known as emerging jets. In this study, emerging jets are simulated with various values of the dark pion average lifetime, dark pion mass, and mediator particle mass. These simulations are compared with a search for displaced vertices conducted by the ATLAS collaboration, allowing one to reinterpret the ATLAS results to constrain the parameter values that the emerging-jets model can have. This study simulates and constrains the allowed values for the dark pion mass, dark pion average life time and mediator mass with 95% confidence level. This is the first study to use results from the ATLAS experiment to constrain the emerging-jets model, as well as the first study to exclude this region of the parameter space.</p>

corrected abstract:
<p>Dark matter, the unknown matter that constitutes 85% of all matter in the universe, is one of the greatest mysteries in fundamental physics. One theory that might explain dark matter predicts that there are long-lived particles known as dark pions. If these were created in a particle accelerator, they could decay inside the detector, resulting in particles that seemingly “emerge” from nothing. This phenomenon is known as emerging jets. In this study, emerging jets are simulated with various values of the dark pion average lifetime, dark pion mass, and mediator particle mass. These simulations are compared with a search for displaced vertices conducted by the ATLAS collaboration, allowing one to reinterpret the ATLAS results to constrain the parameter values that the emerging-jets model can have. This study simulates and constrains the allowed values for the dark pion mass, dark pion average life time and mediator mass with 95% confidence level. This is the first study to use results from the ATLAS experiment to constrain the emerging-jets model, as well as the first study to exclude this region of the parameter space.</p>

Note - only change was to make the double quotation marks match the original
----------------------------------------------------------------------
In diva2:1658894 
abstract is: 
<p>Hedge funds use a variety of different financial instruments in order to try to achieve over-average returns without taking on excessive risk - options being one of the most common of these instruments. Basket options is a type of option that is written on several underlying assets that can be used to hedge risky positions. This project has been working together with the hedge fund Proxy P to develop software to construct basket options and to analyze their use as a hedging strategy.</p><p>Construction of basket options can be performed through the use of several different mathematical models. These models range from complex continuous models, such as Monte Carlo simulations, to simple discrete models, such as the binomial option pricing model. In this project, the binomial option pricing model was chosen as the main tool to determine some quantities of basket options. It can conveniently handle both European and American options, independently of whether these are put or call options. The quantities calculated, the option price and option Delta, are dependent on the volatility and the initial price of the underlying. When evaluating the basket option there are two key assumptions that need to be studied. These key assumptions are if the weights and the initial price of the underlying change with each time step, or if they are held constant. It was found that both the weights and the price of the underlying should change dynamically with each time step.</p><p>Furthermore, in order to evaluate the performance of the basket options used as a hedge, the project used historical data and measured how the options neutralized negative movements in the underlying. This was done through the use of the option Delta and the hedge ratio. What could be concluded was that the put basket option can serve as a relatively inexpensive hedge and minimize the risk on the downside in a sufficient matter.</p>

corrected abstract:
<p>Hedge funds use a variety of different financial instruments in order to try to achieve overaverage returns without taking on excessive risk - options being one of the most common of these instruments. Basket options is a type of option that is written on several underlying assets that can be used to hedge risky positions. This project has been working together with the hedge fund Proxy P to develop software to construct basket options and to analyze their use as a hedging strategy.</p><p>Construction of basket options can be performed through the use of several different mathematical models. These models range from complex continuous models, such as Monte Carlo simulations, to simple discrete models, such as the binomial option pricing model. In this project, the binomial option pricing model was chosen as the main tool to determine some quantities of basket options. It can conveniently handle both European and American options, independently of whether these are put or call options. The quantities calculated, the option price and option Delta, are dependent on the volatility and the initial price of the underlying. When evaluating the basket option there are two key assumptions that need to be studied. These key assumptions are if the weights and the initial price of the underlying change with each time step, or if they are held constant. It was found that both the weights and the price of the underlying should change dynamically with each time step.</p><p>Furthermore, in order to evaluate the performance of the basket options used as a hedge, the project used historical data and measured how the options neutralized negative movements in the underlying. This was done through the use of the option Delta and the hedge ratio. What could be concluded was that the put basket option can serve as a relatively inexpensive hedge and minimize the risk on the downside in a sufficient matter.</p>

Note handle word split at end of line with hyphenation:
'over-average' ==> 'overaverage'
----------------------------------------------------------------------
In diva2:1878793   - correct as is
----------------------------------------------------------------------
In diva2:1145360   - correct as is
----------------------------------------------------------------------
In diva2:448845 
abstract is: 
<p>Abstrakt:</p><p>This report examines different control design methods, linear as well as nonlinear, for a suborbital reusable launch vehicle. An investigation of the natural vehicle behavior is made, after which a baseline linear controller is constructed to fulfill certain handling quality criteria. Thereafter the nonlinear cascade control methods block backstepping and nonlinear dynamic inversion via time scale separation are examined and used to construct two nonlinear controllers for the vehicle. Optimal controllers, in terms of three different criteria, are found using the genetic optimization algorithm differential evolution. The optimal controllers are compared, and it is found that nonlinear dynamic inversion via time scale separation performs better than block backstepping with respect to the cases investigated. The results suggest control design by global optimization is a viable and promising complement to classical methods.</p>

corrected abstract:
<p>This report examines different control design methods, linear as well as nonlinear, for a suborbital reusable launch vehicle. An investigation of the natural vehicle behavior is made, after which a baseline linear controller is constructed to fulfill certain handling quality criteria. Thereafter the nonlinear cascade control methods block backstepping and nonlinear dynamic inversion via time scale separation are examined and used to construct two nonlinear controllers for the vehicle. Optimal controllers, in terms of three different criteria, are found using the genetic optimization algorithm differential evolution. The optimal controllers are compared, and it is found that nonlinear dynamic inversion via time scale separation performs better than block backstepping with respect to the cases investigated. The results suggest control design by global optimization is a viable and promising complement to classical methods.</p>

Note - only change was to remove "<p>Abstrakt:</p>"
----------------------------------------------------------------------
In diva2:1658831 
abstract is: 
<p>A numerical method for solving the buoyancy-driven magneto-convection equations in a rapidly rotating spherical shell is presented. The method is implemented through a FORTRAN 90 program, based on a FORTRAN 66 code written by Hollerbach [International Journal for Numerical Methods in Fluids, 32 (2000)] and partially translated in FORTRAN 90 by Riquier. The program uses the pseudo-spectral method and computes velocity as well as temperature fields in a rapidly rotating spherical shell. The code has been validated through comparisons with previous studies and parallelized using OpenMP. Comparisons with Hollerbach's method have been carried out and showed improvements in stability.</p>

corrected abstract:
<p>A numerical method for solving the buoyancy-driven magneto-convection equations in a rapidly rotating spherical shell is presented. The method is implemented through a FORTRAN 90 program, based on a FORTRAN 66 code written by Hollerbach [<em>International Journal for Numerical Methods in Fluids</em>, 32 (2000)] and partially translated in FORTRAN 90 by Riquier. The program uses the pseudo-spectral method and computes velocity as well as temperature fields in a rapidly rotating spherical shell. The code has been validated through comparisons with previous studies and parallelized using OpenMP. Comparisons with Hollerbach's method have been carried out and showed improvements in stability.</p>

Note - added italics
----------------------------------------------------------------------
In diva2:1431643 
abstract is: 
<p>This thesis evaluates how the evolutionary algorithm CMA-ES (Covariance Matrix Adaption Evolution Strategy) can be used for optimizing the total initial margin for a network of banks trading bilateral OTC derivatives. The algorithm is a stochastic method for optimization of non-linear and, but not limited to, non-convex functions. The algorithm searches for an optimum by generating normally distributed samples and iteratively updating the mean and covariance matrix of the search distribution using the best candidate solutions in the sampled population. In this thesis, feasible solutions are represented by the null space obtained from the constraint of keeping all banks' market exposure unchanged throughout the optimization, and the generated samples for each iteration correspond to linear combinations of the base vectors spanning this null space. In particular, this thesis investigates how different representations of this null space affect the convergence speed of the algorithm. By applying the algorithm to problems of varying sizes, using several different null space representations coming from different matrix decomposition methods, it is found that as long as an orthonormal representation is used it does not matter which matrix decomposition method it comes from. This is found to be because, given any orthonormal null space representation, the algorithm will at start generate a rotationally invariant sample space in its search for the optimal solution, independent of the specific null space representation. If the representation is not orthogonal, the initial sample will in contrast be in the shape of an ellipsoid and thus biased in certain directions, which in general affects the performance negatively. A non-orthonormal representation can converge faster in specific optimization problems, if the direction of the solution is known in advance and the sample space is pointed towards that direction. However, the benefit of this aspect is limited in a realistic scenario and an orthonormal representation is recommended. Furthermore, as it is shown that different orthonormal representations perform equally, it is implied that other characteristics can be considered when deciding which matrix decomposition method to use; such as the importance of fast computation or desire for a sparse representation.</p>

corrected abstract:
<p>This thesis evaluates how the evolutionary algorithm CMA-ES (<em>Covariance Matrix Adaption Evolution Strategy</em>) can be used for optimizing the total initial margin for a network of banks trading bilateral OTC derivatives. The algorithm is a stochastic method for optimization of non-linear and, but not limited to, non-convex functions. The algorithm searches for an optimum by generating normally distributed samples and iteratively updating the mean and covariance matrix of the search distribution using the best candidate solutions in the sampled population. In this thesis, feasible solutions are represented by the null space obtained from the constraint of keeping all banks' market exposure unchanged throughout the optimization, and the generated samples for each iteration correspond to linear combinations of the base vectors spanning this null space. In particular, this thesis investigates how different representations of this null space affect the convergence speed of the algorithm. By applying the algorithm to problems of varying sizes, using several different null space representations coming from different matrix decomposition methods, it is found that as long as an orthonormal representation is used it does not matter which matrix decomposition method it comes from. This is found to be because, given any orthonormal null space representation, the algorithm will at start generate a rotationally invariant sample space in its search for the optimal solution, independent of the specific null space representation. If the representation is not orthogonal, the initial sample will in contrast be in the shape of an ellipsoid and thus biased in certain directions, which in general affects the performance negatively. A non-orthonormal representation can converge faster in specific optimization problems, if the direction of the solution is known in advance and the sample space is pointed towards that direction. However, the benefit of this aspect is limited in a realistic scenario and an orthonormal representation is recommended. Furthermore, as it is shown that different orthonormal representations perform equally, it is implied that other characteristics can be considered when deciding which matrix decomposition method to use; such as the importance of fast computation or desire for a sparse representation.</p>

Note - added italics
----------------------------------------------------------------------
In diva2:1595241 
abstract is: 
<p>The thesis work described in this report is about simulation of the cooling of an electrical machine rotor. Limitations and simplifications were made on the CAD model of the rotor with the purpose of reducing the simulation time, for it to then be used for CFD-simulations using STAR-CCM+. This was done to see the temperature, as well as its distribution, in the model. By changing various parameters, one at the time whilst the rest were kept at their assigned standard values, the changes could be analysed and thereafter compared. The tests include smaller geometry changes, parameters of the coolant and its flow, parameters for the airgap and the materials in the laminates and the material around the magnets, as well as changes in loss values. The simulations for geometry changes involving the magnets and their surrounding material resulted in minor temperature increases. An inner rotor radius increase gave a relatively large temperature decrease (although this change would be more difficult to make in practice). Most of the mean values of the temperature changes in the regions of the model were within 10% from the standard simulation used. Increased thermal contact resistance between the Bakelite and the laminates, and increased losses had the worst impact on the cooling. Meanwhile the changes in coolant parameters (as well as the its inlet temperature and mass flow) and reduced losses had the best impact on the cooling. Generally, the temperature distributions looked similar for the different simulations. There were more differences in the distributions for the simulations with changed material properties or thermal contact resistance.</p>

corrected abstract:
<p>The thesis work described in this report is about simulation of the cooling of an electrical machine rotor. Limitations and simplifications were made on the CAD model of the rotor with the purpose of reducing the simulation time, for it to then be used for CFD-simulations using STAR-CCM+. This was done to see the temperature, as well as its distribution, in the model. By changing various parameters, one at the time whilst the rest were kept at their assigned standard values, the changes could be analysed and thereafter compared. The tests include smaller geometry changes, parameters of the coolant and its flow, parameters for the airgap and the materials in the laminates and the material around the magnets, as well as changes in loss values. The simulations for geometry changes involving the magnets and their surrounding material resulted in minor temperature increases. An inner rotor radius increase gave a relatively large temperature decrease (although this change would be more difficult to make in practice). Most of the mean values of the temperature changes in the regions of the model were within 10% from the standard simulation used. Increased thermal contact resistance between the Bakelite and the laminates, and increased losses had the worst impact on the cooling. Meanwhile the changes in coolant parameters (as well as the oil inlet temperature and mass flow) and reduced losses had the best impact on the cooling. Generally, the temperature distributions looked similar for the different simulations. There were a bit more differences in the distributions for the simulations with changed material properties or contact resistance.</p>

Note - some changes in wording to match the original
----------------------------------------------------------------------
